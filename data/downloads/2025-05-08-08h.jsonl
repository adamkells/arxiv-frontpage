{"created":"2025-05-07 17:58:25","title":"Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait","abstract":"We address the problem of whole-body person recognition in unconstrained environments. This problem arises in surveillance scenarios such as those in the IARPA Biometric Recognition and Identification at Altitude and Range (BRIAR) program, where biometric data is captured at long standoff distances, elevated viewing angles, and under adverse atmospheric conditions (e.g., turbulence and high wind velocity). To this end, we propose FarSight, a unified end-to-end system for person recognition that integrates complementary biometric cues across face, gait, and body shape modalities. FarSight incorporates novel algorithms across four core modules: multi-subject detection and tracking, recognition-aware video restoration, modality-specific biometric feature encoding, and quality-guided multi-modal fusion. These components are designed to work cohesively under degraded image conditions, large pose and scale variations, and cross-domain gaps. Extensive experiments on the BRIAR dataset, one of the most comprehensive benchmarks for long-range, multi-modal biometric recognition, demonstrate the effectiveness of FarSight. Compared to our preliminary system, this system achieves a 34.1% absolute gain in 1:1 verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set identification (Rank-20), and a 34.3% reduction in open-set identification errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE Face in Video Evaluation (FIVE), which conducts standardized face recognition testing on the BRIAR dataset. These results establish FarSight as a state-of-the-art solution for operational biometric recognition in challenging real-world conditions.","sentences":["We address the problem of whole-body person recognition in unconstrained environments.","This problem arises in surveillance scenarios such as those in the IARPA Biometric Recognition and Identification at Altitude and Range (BRIAR) program, where biometric data is captured at long standoff distances, elevated viewing angles, and under adverse atmospheric conditions (e.g., turbulence and high wind velocity).","To this end, we propose FarSight, a unified end-to-end system for person recognition that integrates complementary biometric cues across face, gait, and body shape modalities.","FarSight incorporates novel algorithms across four core modules: multi-subject detection and tracking, recognition-aware video restoration, modality-specific biometric feature encoding, and quality-guided multi-modal fusion.","These components are designed to work cohesively under degraded image conditions, large pose and scale variations, and cross-domain gaps.","Extensive experiments on the BRIAR dataset, one of the most comprehensive benchmarks for long-range, multi-modal biometric recognition, demonstrate the effectiveness of FarSight.","Compared to our preliminary system, this system achieves a 34.1% absolute gain in 1:1 verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set identification (Rank-20), and a 34.3% reduction in open-set identification errors (FNIR@1% FPIR).","Furthermore, FarSight was evaluated in the 2025 NIST RTE Face in Video Evaluation (FIVE), which conducts standardized face recognition testing on the BRIAR dataset.","These results establish FarSight as a state-of-the-art solution for operational biometric recognition in challenging real-world conditions."],"url":"http://arxiv.org/abs/2505.04616v1"}
{"created":"2025-05-07 17:53:47","title":"WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales","abstract":"Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria,'' such as data shifts that violate certain exchangeability assumptions, or do not allow for online adaptation in response to shifts. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that accommodate online adaptation to mild covariate shifts (in the marginal input distribution) while raising alarms in response to more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.","sentences":["Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior.","Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task.","However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria,'' such as data shifts that violate certain exchangeability assumptions, or do not allow for online adaptation in response to shifts.","In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms.","For practical applications, we propose specific WCTM algorithms that accommodate online adaptation to mild covariate shifts (in the marginal input distribution) while raising alarms in response to more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to.","On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines."],"url":"http://arxiv.org/abs/2505.04608v1"}
{"created":"2025-05-07 17:50:15","title":"Testing Juntas Optimally with Samples","abstract":"We prove tight upper and lower bounds of $\\Theta\\left(\\tfrac{1}{\\epsilon}\\left( \\sqrt{2^k \\log\\binom{n}{k} } + \\log\\binom{n}{k} \\right)\\right)$ on the number of samples required for distribution-free $k$-junta testing. This is the first tight bound for testing a natural class of Boolean functions in the distribution-free sample-based model. Our bounds also hold for the feature selection problem, showing that a junta tester must learn the set of relevant variables. For tolerant junta testing, we prove a sample lower bound of $\\Omega(2^{(1-o(1)) k} + \\log\\binom{n}{k})$ showing that, unlike standard testing, there is no large gap between tolerant testing and learning.","sentences":["We prove tight upper and lower bounds of $\\Theta\\left(\\tfrac{1}{\\epsilon}\\left( \\sqrt{2^k \\log\\binom{n}{k} } + \\log\\binom{n}{k} \\right)\\right)$ on the number of samples required for distribution-free $k$-junta testing.","This is the first tight bound for testing a natural class of Boolean functions in the distribution-free sample-based model.","Our bounds also hold for the feature selection problem, showing that a junta tester must learn the set of relevant variables.","For tolerant junta testing, we prove a sample lower bound of $\\Omega(2^{(1-o(1))","k} + \\log\\binom{n}{k})$ showing that, unlike standard testing, there is no large gap between tolerant testing and learning."],"url":"http://arxiv.org/abs/2505.04604v1"}
{"created":"2025-05-07 17:48:35","title":"OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning","abstract":"OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.","sentences":["OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models.","Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released.","This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA.","OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models.","By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments."],"url":"http://arxiv.org/abs/2505.04601v1"}
{"created":"2025-05-07 17:37:23","title":"MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection","abstract":"Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.","sentences":["Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space.","While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability.","Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs.","First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features.","Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next.","Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones.","Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets."],"url":"http://arxiv.org/abs/2505.04594v1"}
{"created":"2025-05-07 17:27:51","title":"Active Sampling for MRI-based Sequential Decision Making","abstract":"Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling","sentences":["Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity.","To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies.","Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples.","Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired.","We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data.","Our approach during inference actively adapts to sequential decisions to optimally sample.","To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function.","We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment.","Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples.","Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device.","Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling"],"url":"http://arxiv.org/abs/2505.04586v1"}
{"created":"2025-05-07 17:12:15","title":"Componential Prompt-Knowledge Alignment for Domain Incremental Learning","abstract":"Domain Incremental Learning (DIL) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. KA-Prompt operates in two phases: (1) Initial Componential Structure Configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) Online Alignment Preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. Extensive experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt. Our source code is available at https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt","sentences":["Domain Incremental Learning (DIL) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge.","Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions.","This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.","To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model.","KA-Prompt operates in two phases: (1) Initial Componential Structure Configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts.","(2) Online Alignment Preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve.","Extensive experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.","Our source code is available at https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt"],"url":"http://arxiv.org/abs/2505.04575v1"}
{"created":"2025-05-07 16:58:18","title":"Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data","abstract":"This paper presents a multitask learning approach based on long-short-term memory (LSTM) networks for the joint prediction of arboviral outbreaks and case counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging historical public health data from DataSUS (2017-2023), the proposed model concurrently performs binary classification (outbreak detection) and regression (case forecasting) tasks. A sliding window strategy was adopted to construct temporal features using varying input lengths (60, 90, and 120 days), with hyperparameter optimization carried out using Keras Tuner. Model evaluation used time series cross-validation for robustness and a held-out test from 2023 for generalization assessment. The results show that longer windows improve dengue regression accuracy, while classification performance peaked at intermediate windows, suggesting an optimal trade-off between sequence length and generalization. The multitask architecture delivers competitive performance across diseases and tasks, demonstrating the feasibility and advantages of unified modeling strategies for scalable epidemic forecasting in data-limited public health scenarios.","sentences":["This paper presents a multitask learning approach based on long-short-term memory (LSTM) networks for the joint prediction of arboviral outbreaks and case counts of dengue, chikungunya, and Zika in Recife, Brazil.","Leveraging historical public health data from DataSUS (2017-2023), the proposed model concurrently performs binary classification (outbreak detection) and regression (case forecasting) tasks.","A sliding window strategy was adopted to construct temporal features using varying input lengths (60, 90, and 120 days), with hyperparameter optimization carried out using Keras Tuner.","Model evaluation used time series cross-validation for robustness and a held-out test from 2023 for generalization assessment.","The results show that longer windows improve dengue regression accuracy, while classification performance peaked at intermediate windows, suggesting an optimal trade-off between sequence length and generalization.","The multitask architecture delivers competitive performance across diseases and tasks, demonstrating the feasibility and advantages of unified modeling strategies for scalable epidemic forecasting in data-limited public health scenarios."],"url":"http://arxiv.org/abs/2505.04566v1"}
{"created":"2025-05-07 16:57:51","title":"Hierarchical Task Decomposition for Execution Monitoring and Error Recovery: Understanding the Rationale Behind Task Demonstrations","abstract":"Multi-step manipulation tasks where robots interact with their environment and must apply process forces based on the perceived situation remain challenging to learn and prone to execution errors. Accurately simulating these tasks is also difficult. Hence, it is crucial for robust task performance to learn how to coordinate end-effector pose and applied force, monitor execution, and react to deviations. To address these challenges, we propose a learning approach that directly infers both low- and high-level task representations from user demonstrations on the real system. We developed an unsupervised task segmentation algorithm that combines intention recognition and feature clustering to infer the skills of a task. We leverage the inferred characteristic features of each skill in a novel unsupervised anomaly detection approach to identify deviations from the intended task execution. Together, these components form a comprehensive framework capable of incrementally learning task decisions and new behaviors as new situations arise. Compared to state-of-the-art learning techniques, our approach significantly reduces the required amount of training data and computational complexity while efficiently learning complex in-contact behaviors and recovery strategies. Our proposed task segmentation and anomaly detection approaches outperform state-of-the-art methods on force-based tasks evaluated on two different robotic systems.","sentences":["Multi-step manipulation tasks where robots interact with their environment and must apply process forces based on the perceived situation remain challenging to learn and prone to execution errors.","Accurately simulating these tasks is also difficult.","Hence, it is crucial for robust task performance to learn how to coordinate end-effector pose and applied force, monitor execution, and react to deviations.","To address these challenges, we propose a learning approach that directly infers both low- and high-level task representations from user demonstrations on the real system.","We developed an unsupervised task segmentation algorithm that combines intention recognition and feature clustering to infer the skills of a task.","We leverage the inferred characteristic features of each skill in a novel unsupervised anomaly detection approach to identify deviations from the intended task execution.","Together, these components form a comprehensive framework capable of incrementally learning task decisions and new behaviors as new situations arise.","Compared to state-of-the-art learning techniques, our approach significantly reduces the required amount of training data and computational complexity while efficiently learning complex in-contact behaviors and recovery strategies.","Our proposed task segmentation and anomaly detection approaches outperform state-of-the-art methods on force-based tasks evaluated on two different robotic systems."],"url":"http://arxiv.org/abs/2505.04565v1"}
{"created":"2025-05-07 16:56:52","title":"Optimal Deterministic Rendezvous in Labeled Lines","abstract":"In a rendezvous task, a set of mobile agents dispersed in a network have to gather at an arbitrary common site. We consider the rendezvous problem on the infinite labeled line, with $2$ initially asleep agents, without communication, and a synchronous notion of time. Nodes are labeled with unique positive integers. The initial distance between the two agents is denoted by $D$. Time is divided into rounds. We count time from when an agent first wakes up, and denote by $\\tau$ the delay between the agents' wake up times. If awake in a given round $T$, an agent has three options: stay at its current node $v$, take port $0$, or take port $1$. If it decides to stay, the agent is still at node $v$ in round $T+1$. Otherwise, it is at one of the two neighbors of $v$ on the line, based on the port it chose. The agents achieve rendezvous in $T$ rounds if they are at the same node in round $T$. We aim for a deterministic algorithm for this task.   The problem was recently considered by Miller and Pelc [DISC 2023]. With $\\ell_{\\max}$ the largest label of the two starting nodes, they showed that no algorithm can guarantee rendezvous in $o(D \\log^* \\ell_{\\max})$ rounds. The lower bound follows from a connection with the LOCAL model of distributed computing, and holds even if the agents are guaranteed simultaneous wake-up ($\\tau = 0$) and are given $D$ as advice. Miller and Pelc also gave an algorithm of optimal matching complexity $O(D \\log^* \\ell_{\\max})$ when $D$ is known to the agents, but only obtained the higher bound of $O(D^2 (\\log^* \\ell_{\\max})^3)$ when $D$ is unknown.   We improve this second complexity to a tight $O(D \\log^* \\ell_{\\max})$. In fact, our algorithm achieves rendezvous in $O(D \\log^* \\ell_{\\min})$ rounds, where $\\ell_{\\min}$ is the smallest label within distance $O(D)$ of the two starting positions.","sentences":["In a rendezvous task, a set of mobile agents dispersed in a network have to gather at an arbitrary common site.","We consider the rendezvous problem on the infinite labeled line, with $2$ initially asleep agents, without communication, and a synchronous notion of time.","Nodes are labeled with unique positive integers.","The initial distance between the two agents is denoted by $D$. Time is divided into rounds.","We count time from when an agent first wakes up, and denote by $\\tau$ the delay between the agents' wake up times.","If awake in a given round $T$, an agent has three options: stay at its current node $v$, take port $0$, or take port $1$. If it decides to stay, the agent is still at node $v$ in round $T+1$. Otherwise, it is at one of the two neighbors of $v$ on the line, based on the port it chose.","The agents achieve rendezvous in $T$ rounds if they are at the same node in round $T$. We aim for a deterministic algorithm for this task.   ","The problem was recently considered by Miller and Pelc","[DISC 2023].","With $\\ell_{\\max}$ the largest label of the two starting nodes, they showed that no algorithm can guarantee rendezvous in $o(D \\log^* \\ell_{\\max})$ rounds.","The lower bound follows from a connection with the LOCAL model of distributed computing, and holds even if the agents are guaranteed simultaneous wake-up ($\\tau = 0$) and are given $D$ as advice.","Miller and Pelc also gave an algorithm of optimal matching complexity $O(D \\log^* \\ell_{\\max})$ when $D$ is known to the agents, but only obtained the higher bound of $O(D^2 (\\log^* \\ell_{\\max})^3)$ when $D$ is unknown.   ","We improve this second complexity to a tight $O(D \\log^* \\ell_{\\max})$.","In fact, our algorithm achieves rendezvous in $O(D \\log^* \\ell_{\\min})$ rounds, where $\\ell_{\\min}$ is the smallest label within distance $O(D)$ of the two starting positions."],"url":"http://arxiv.org/abs/2505.04564v1"}
{"created":"2025-05-07 16:31:21","title":"Fast Pattern Matching with Epsilon Transitions","abstract":"In the String Matching in Labeled Graphs (SMLG) problem, we need to determine whether a pattern string appears on a given labeled graph or a given automaton. Under the Orthogonal Vectors hypothesis, the SMLG problem cannot be solved in subquadratic time [ICALP 2019]. In typical bioinformatics applications, pattern matching algorithms should be both fast and space-efficient, so we need to determine useful classes of graphs on which the SLMG problem can be solved efficiently.   In this paper, we improve on a recent result [STACS 2024] that shows how to solve the SMLG problem in linear time on the compressed representation of Wheeler generalized automata, a class of string-labeled automata that extend de Bruijn graphs. More precisely, we show how to remove the assumption that the automata contain no $ \\epsilon $-transitions (namely, edges labeled with the empty string), while retaining the same time and space bounds. This is a significant improvement because $ \\epsilon $-transitions add considerable expressive power (making it possible to jump to multiple states for free) and capture the complexity of regular expressions (through Thompson's construction for converting a regular expression into an equivalent automaton). We prove that, to enable $ \\epsilon $-transitions, we only need to store two additional bitvectors that can be constructed in linear time.","sentences":["In the String Matching in Labeled Graphs (SMLG) problem, we need to determine whether a pattern string appears on a given labeled graph or a given automaton.","Under the Orthogonal Vectors hypothesis, the SMLG problem cannot be solved in subquadratic time [ICALP 2019].","In typical bioinformatics applications, pattern matching algorithms should be both fast and space-efficient, so we need to determine useful classes of graphs on which the SLMG problem can be solved efficiently.   ","In this paper, we improve on a recent result [STACS 2024] that shows how to solve the SMLG problem in linear time on the compressed representation of Wheeler generalized automata, a class of string-labeled automata that extend de Bruijn graphs.","More precisely, we show how to remove the assumption that the automata contain no $ \\epsilon $-transitions (namely, edges labeled with the empty string), while retaining the same time and space bounds.","This is a significant improvement because $ \\epsilon $-transitions add considerable expressive power (making it possible to jump to multiple states for free) and capture the complexity of regular expressions (through Thompson's construction for converting a regular expression into an equivalent automaton).","We prove that, to enable $ \\epsilon $-transitions, we only need to store two additional bitvectors that can be constructed in linear time."],"url":"http://arxiv.org/abs/2505.04549v1"}
{"created":"2025-05-07 16:17:54","title":"Registration of 3D Point Sets Using Exponential-based Similarity Matrix","abstract":"Point cloud registration is a fundamental problem in computer vision and robotics, involving the alignment of 3D point sets captured from varying viewpoints using depth sensors such as LiDAR or structured light. In modern robotic systems, especially those focused on mapping, it is essential to merge multiple views of the same environment accurately. However, state-of-the-art registration techniques often struggle when large rotational differences exist between point sets or when the data is significantly corrupted by sensor noise. These challenges can lead to misalignments and, consequently, to inaccurate or distorted 3D reconstructions. In this work, we address both these limitations by proposing a robust modification to the classic Iterative Closest Point (ICP) algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP), integrates a Gaussian-inspired exponential weighting scheme to construct a similarity matrix that dynamically adapts across iterations. This matrix facilitates improved estimation of both rotational and translational components during alignment. We demonstrate the robustness of ESM-ICP in two challenging scenarios: (i) large rotational discrepancies between the source and target point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show that ESM-ICP outperforms traditional geometric registration techniques as well as several recent learning-based methods. To encourage reproducibility and community engagement, our full implementation is made publicly available on GitHub. https://github.com/aralab-unr/ESM_ICP","sentences":["Point cloud registration is a fundamental problem in computer vision and robotics, involving the alignment of 3D point sets captured from varying viewpoints using depth sensors such as LiDAR or structured light.","In modern robotic systems, especially those focused on mapping, it is essential to merge multiple views of the same environment accurately.","However, state-of-the-art registration techniques often struggle when large rotational differences exist between point sets or when the data is significantly corrupted by sensor noise.","These challenges can lead to misalignments and, consequently, to inaccurate or distorted 3D reconstructions.","In this work, we address both these limitations by proposing a robust modification to the classic Iterative Closest Point (ICP) algorithm.","Our method, termed Exponential Similarity Matrix ICP (ESM-ICP), integrates a Gaussian-inspired exponential weighting scheme to construct a similarity matrix that dynamically adapts across iterations.","This matrix facilitates improved estimation of both rotational and translational components during alignment.","We demonstrate the robustness of ESM-ICP in two challenging scenarios: (i) large rotational discrepancies between the source and target point clouds, and (ii) data corrupted by non-Gaussian noise.","Our results show that ESM-ICP outperforms traditional geometric registration techniques as well as several recent learning-based methods.","To encourage reproducibility and community engagement, our full implementation is made publicly available on GitHub.","https://github.com/aralab-unr/ESM_ICP"],"url":"http://arxiv.org/abs/2505.04540v1"}
{"created":"2025-05-07 16:13:54","title":"Light Spanners with Small Hop-Diameter","abstract":"Lightness, sparsity, and hop-diameter are the fundamental parameters of geometric spanners. Arya et al. [STOC'95] showed in their seminal work that there exists a construction of Euclidean $(1+\\varepsilon)$-spanners with hop-diameter $O(\\log n)$ and lightness $O(\\log n)$. They also gave a general tradeoff of hop-diameter $k$ and sparsity $O(\\alpha_k(n))$, where $\\alpha_k$ is a very slowly growing inverse of an Ackermann-style function. The former combination of logarithmic hop-diameter and lightness is optimal due to the lower bound by Dinitz et al. [FOCS'08]. Later, Elkin and Solomon [STOC'13] generalized the light spanner construction to doubling metrics and extended the tradeoff for more values of hop-diameter $k$. In a recent line of work [SoCG'22, SoCG'23], Le et al. proved that the aforementioned tradeoff between the hop-diameter and sparsity is tight for every choice of hop-diameter $k$. A fundamental question remains: What is the optimal tradeoff between the hop-diameter and lightness for every value of $k$?   In this paper, we present a general framework for constructing light spanners with small hop-diameter. Our framework is based on tree covers. In particular, we show that if a metric admits a tree cover with $\\gamma$ trees, stretch $t$, and lightness $L$, then it also admits a $t$-spanner with hop-diameter $k$ and lightness $O(kn^{2/k}\\cdot \\gamma L)$. Further, we note that the tradeoff for trees is tight due to a construction in uniform line metric, which is perhaps the simplest tree metric. As a direct consequence of this framework, we obtain a tight tradeoff between lightness and hop-diameter for doubling metrics in the entire regime of $k$.","sentences":["Lightness, sparsity, and hop-diameter are the fundamental parameters of geometric spanners.","Arya et al.","[STOC'95] showed in their seminal work that there exists a construction of Euclidean $(1+\\varepsilon)$-spanners with hop-diameter $O(\\log n)$ and lightness $O(\\log n)$. They also gave a general tradeoff of hop-diameter $k$ and sparsity $O(\\alpha_k(n))$, where $\\alpha_k$ is a very slowly growing inverse of an Ackermann-style function.","The former combination of logarithmic hop-diameter and lightness is optimal due to the lower bound by Dinitz et al.","[FOCS'08].","Later, Elkin and Solomon [STOC'13] generalized the light spanner construction to doubling metrics and extended the tradeoff for more values of hop-diameter $k$. In a recent line of work","[SoCG'22, SoCG'23], Le et al. proved that the aforementioned tradeoff between the hop-diameter and sparsity is tight for every choice of hop-diameter $k$. A fundamental question remains: What is the optimal tradeoff between the hop-diameter and lightness for every value of $k$?   ","In this paper, we present a general framework for constructing light spanners with small hop-diameter.","Our framework is based on tree covers.","In particular, we show that if a metric admits a tree cover with $\\gamma$ trees, stretch $t$, and lightness $L$, then it also admits a $t$-spanner with hop-diameter $k$ and lightness $O(kn^{2/k}\\cdot \\gamma L)$. Further, we note that the tradeoff for trees is tight due to a construction in uniform line metric, which is perhaps the simplest tree metric.","As a direct consequence of this framework, we obtain a tight tradeoff between lightness and hop-diameter for doubling metrics in the entire regime of $k$."],"url":"http://arxiv.org/abs/2505.04536v1"}
{"created":"2025-05-07 16:13:21","title":"Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules","abstract":"Federated learning (FL) makes it possible to train models on data that would otherwise remain untapped and inaccessible. Simultaneously, pre-trained language models (LMs) have emerged as indispensable tools in modern workflows. These models exhibit extraordinary capabilities and are easily adapted to downstream tasks. This opens one of the most exciting frontiers in FL: fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid communication of parameters, a problem which is magnified by the sheer size of these modern models. Currently, the FedOpt family of algorithms is the prevailing approach in FL, though it relies on fixed, heuristic intervals for model synchronization. Recently, the FDA algorithm introduced a dynamic alternative by monitoring training progress, but it came with its own drawbacks; namely, a hard-to-tune threshold parameter and a rigid synchronization scheme. In this work, we introduce the FDA-Opt family of algorithms -- a unified generalization that extends the principles behind both FDA and FedOpt, while resolving their core limitations. We evaluate our approach on fine-tuning LMs across a range of downstream NLP tasks, and demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt operates under hyper-parameter settings originally optimized for its competitors. In other words, we show that FDA-Opt is a practical, drop-in replacement for FedOpt in modern FL libraries and systems: it requires no additional configuration and delivers superior performance out of the box.","sentences":["Federated learning (FL) makes it possible to train models on data that would otherwise remain untapped and inaccessible.","Simultaneously, pre-trained language models (LMs) have emerged as indispensable tools in modern workflows.","These models exhibit extraordinary capabilities and are easily adapted to downstream tasks.","This opens one of the most exciting frontiers in FL: fine-tuning LMs.","However, a persistent challenge in FL is the frequent, rigid communication of parameters, a problem which is magnified by the sheer size of these modern models.","Currently, the FedOpt family of algorithms is the prevailing approach in FL, though it relies on fixed, heuristic intervals for model synchronization.","Recently, the FDA algorithm introduced a dynamic alternative by monitoring training progress, but it came with its own drawbacks; namely, a hard-to-tune threshold parameter and a rigid synchronization scheme.","In this work, we introduce the FDA-Opt family of algorithms -- a unified generalization that extends the principles behind both FDA and FedOpt, while resolving their core limitations.","We evaluate our approach on fine-tuning LMs across a range of downstream NLP tasks, and demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt operates under hyper-parameter settings originally optimized for its competitors.","In other words, we show that FDA-Opt is a practical, drop-in replacement for FedOpt in modern FL libraries and systems: it requires no additional configuration and delivers superior performance out of the box."],"url":"http://arxiv.org/abs/2505.04535v1"}
{"created":"2025-05-07 16:04:45","title":"Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review","abstract":"Generative language modelling has surged in popularity with the emergence of services such as ChatGPT and Google Gemini. While these models have demonstrated transformative potential in productivity and communication, they overwhelmingly cater to high-resource languages like English. This has amplified concerns over linguistic inequality in natural language processing (NLP). This paper presents the first systematic review focused specifically on strategies to address data scarcity in generative language modelling for low-resource languages (LRL). Drawing from 54 studies, we identify, categorise and evaluate technical approaches, including monolingual data augmentation, back-translation, multilingual training, and prompt engineering, across generative tasks. We also analyse trends in architecture choices, language family representation, and evaluation methods. Our findings highlight a strong reliance on transformer-based models, a concentration on a small subset of LRLs, and a lack of consistent evaluation across studies. We conclude with recommendations for extending these methods to a wider range of LRLs and outline open challenges in building equitable generative language systems. Ultimately, this review aims to support researchers and developers in building inclusive AI tools for underrepresented languages, a necessary step toward empowering LRL speakers and the preservation of linguistic diversity in a world increasingly shaped by large-scale language technologies.","sentences":["Generative language modelling has surged in popularity with the emergence of services such as ChatGPT and Google Gemini.","While these models have demonstrated transformative potential in productivity and communication, they overwhelmingly cater to high-resource languages like English.","This has amplified concerns over linguistic inequality in natural language processing (NLP).","This paper presents the first systematic review focused specifically on strategies to address data scarcity in generative language modelling for low-resource languages (LRL).","Drawing from 54 studies, we identify, categorise and evaluate technical approaches, including monolingual data augmentation, back-translation, multilingual training, and prompt engineering, across generative tasks.","We also analyse trends in architecture choices, language family representation, and evaluation methods.","Our findings highlight a strong reliance on transformer-based models, a concentration on a small subset of LRLs, and a lack of consistent evaluation across studies.","We conclude with recommendations for extending these methods to a wider range of LRLs and outline open challenges in building equitable generative language systems.","Ultimately, this review aims to support researchers and developers in building inclusive AI tools for underrepresented languages, a necessary step toward empowering LRL speakers and the preservation of linguistic diversity in a world increasingly shaped by large-scale language technologies."],"url":"http://arxiv.org/abs/2505.04531v1"}
{"created":"2025-05-07 16:02:46","title":"RAFT: Robust Augmentation of FeaTures for Image Segmentation","abstract":"Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.   To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real \"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU.","sentences":["Image segmentation is a powerful computer vision technique for scene understanding.","However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets.","Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation.","However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.   ","To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning.","To validate RAFT, we perform experiments on the synthetic-to-real \"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks.","We managed to surpass the previous state of the art, HALO.","SYNTHIA->Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%/73.2%.","Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU."],"url":"http://arxiv.org/abs/2505.04529v1"}
{"created":"2025-05-07 15:59:45","title":"DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once","abstract":"Visible and infrared image fusion is one of the most crucial tasks in the field of image fusion, aiming to generate fused images with clear structural information and high-quality texture features for high-level vision tasks. However, when faced with severe illumination degradation in visible images, the fusion results of existing image fusion methods often exhibit blurry and dim visual effects, posing major challenges for autonomous driving. To this end, a Darkness-Free network is proposed to handle Visible and infrared image disentanglement and fusion all at Once (DFVO), which employs a cascaded multi-task approach to replace the traditional two-stage cascaded training (enhancement and fusion), addressing the issue of information entropy loss caused by hierarchical data transmission. Specifically, we construct a latent-common feature extractor (LCFE) to obtain latent features for the cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised to acquire high-frequency semantic information. Secondly, we design a hyper cross-attention module (HCAM) to extract low-frequency information and preserve texture features from source images. Finally, a relevant loss function is designed to guide the holistic network learning, thereby achieving better image fusion. Extensive experiments demonstrate that our proposed approach outperforms state-of-the-art alternatives in terms of qualitative and quantitative evaluations. Particularly, DFVO can generate clearer, more informative, and more evenly illuminated fusion results in the dark environments, achieving best performance on the LLVIP dataset with 63.258 dB PSNR and 0.724 CC, providing more effective information for high-level vision tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.","sentences":["Visible and infrared image fusion is one of the most crucial tasks in the field of image fusion, aiming to generate fused images with clear structural information and high-quality texture features for high-level vision tasks.","However, when faced with severe illumination degradation in visible images, the fusion results of existing image fusion methods often exhibit blurry and dim visual effects, posing major challenges for autonomous driving.","To this end, a Darkness-Free network is proposed to handle Visible and infrared image disentanglement and fusion all at Once (DFVO), which employs a cascaded multi-task approach to replace the traditional two-stage cascaded training (enhancement and fusion), addressing the issue of information entropy loss caused by hierarchical data transmission.","Specifically, we construct a latent-common feature extractor (LCFE) to obtain latent features for the cascaded tasks strategy.","Firstly, a details-extraction module (DEM) is devised to acquire high-frequency semantic information.","Secondly, we design a hyper cross-attention module (HCAM) to extract low-frequency information and preserve texture features from source images.","Finally, a relevant loss function is designed to guide the holistic network learning, thereby achieving better image fusion.","Extensive experiments demonstrate that our proposed approach outperforms state-of-the-art alternatives in terms of qualitative and quantitative evaluations.","Particularly, DFVO can generate clearer, more informative, and more evenly illuminated fusion results in the dark environments, achieving best performance on the LLVIP dataset with 63.258 dB PSNR and 0.724 CC, providing more effective information for high-level vision tasks.","Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO."],"url":"http://arxiv.org/abs/2505.04526v1"}
{"created":"2025-05-07 15:53:56","title":"Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model","abstract":"Generating 3D CT volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. In this paper, we introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual descriptions using the diffusion model. Unlike previous methods that rely on fixed-format text input, Text2CT employs a novel prompt formulation that enables generation from diverse, free-text descriptions. The proposed framework encodes medical text into latent representations and decodes them into high-resolution 3D CT scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3D framework. Our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. Extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation.","sentences":["Generating 3D CT volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research.","In this paper, we introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual descriptions using the diffusion model.","Unlike previous methods that rely on fixed-format text input, Text2CT employs a novel prompt formulation that enables generation from diverse, free-text descriptions.","The proposed framework encodes medical text into latent representations and decodes them into high-resolution 3D CT scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3D framework.","Our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text.","Extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation."],"url":"http://arxiv.org/abs/2505.04522v1"}
{"created":"2025-05-07 15:45:04","title":"User and Recommender Behavior Over Time: Contextualizing Activity, Effectiveness, Diversity, and Fairness in Book Recommendation","abstract":"Data is an essential resource for studying recommender systems. While there has been significant work on improving and evaluating state-of-the-art models and measuring various properties of recommender system outputs, less attention has been given to the data itself, particularly how data has changed over time. Such documentation and analysis provide guidance and context for designing and evaluating recommender systems, particularly for evaluation designs making use of time (e.g., temporal splitting). In this paper, we present a temporal explanatory analysis of the UCSD Book Graph dataset scraped from Goodreads, a social reading and recommendation platform active since 2006. We measure the book interaction data using a set of activity, diversity, and fairness metrics; we then train a set of collaborative filtering algorithms on rolling training windows to observe how the same measures evolve over time in the recommendations. Additionally, we explore whether the introduction of algorithmic recommendations in 2011 was followed by observable changes in user or recommender system behavior.","sentences":["Data is an essential resource for studying recommender systems.","While there has been significant work on improving and evaluating state-of-the-art models and measuring various properties of recommender system outputs, less attention has been given to the data itself, particularly how data has changed over time.","Such documentation and analysis provide guidance and context for designing and evaluating recommender systems, particularly for evaluation designs making use of time (e.g., temporal splitting).","In this paper, we present a temporal explanatory analysis of the UCSD Book Graph dataset scraped from Goodreads, a social reading and recommendation platform active since 2006.","We measure the book interaction data using a set of activity, diversity, and fairness metrics; we then train a set of collaborative filtering algorithms on rolling training windows to observe how the same measures evolve over time in the recommendations.","Additionally, we explore whether the introduction of algorithmic recommendations in 2011 was followed by observable changes in user or recommender system behavior."],"url":"http://arxiv.org/abs/2505.04518v1"}
{"created":"2025-05-07 15:27:59","title":"Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts","abstract":"The quality of natural language texts in fine-tuning datasets plays a critical role in the performance of generative models, particularly in computational creativity tasks such as poem or song lyric generation. Fluency defects in generated poems significantly reduce their value. However, training texts are often sourced from internet-based platforms without stringent quality control, posing a challenge for data engineers to manage defect levels effectively.   To address this issue, we propose the use of automated linguistic anomaly detection to identify and filter out low-quality texts from training datasets for creative models. In this paper, we present a comprehensive comparison of unsupervised and supervised text anomaly detection approaches, utilizing both synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a collection of Russian-language human-labeled poems designed for cross-sentence grammatical error detection, and provide the full evaluation code. Our work aims to empower the community with tools and insights to improve the quality of training datasets for generative models in creative domains.","sentences":["The quality of natural language texts in fine-tuning datasets plays a critical role in the performance of generative models, particularly in computational creativity tasks such as poem or song lyric generation.","Fluency defects in generated poems significantly reduce their value.","However, training texts are often sourced from internet-based platforms without stringent quality control, posing a challenge for data engineers to manage defect levels effectively.   ","To address this issue, we propose the use of automated linguistic anomaly detection to identify and filter out low-quality texts from training datasets for creative models.","In this paper, we present a comprehensive comparison of unsupervised and supervised text anomaly detection approaches, utilizing both synthetic and human-labeled datasets.","We also introduce the RUPOR dataset, a collection of Russian-language human-labeled poems designed for cross-sentence grammatical error detection, and provide the full evaluation code.","Our work aims to empower the community with tools and insights to improve the quality of training datasets for generative models in creative domains."],"url":"http://arxiv.org/abs/2505.04507v1"}
{"created":"2025-05-07 15:01:23","title":"A Design Space for the Critical Validation of LLM-Generated Tabular Data","abstract":"LLM-generated tabular data is creating new opportunities for data-driven applications in academia, business, and society. To leverage benefits like missing value imputation, labeling, and enrichment with context-aware attributes, LLM-generated data needs a critical validation process. The number of pioneering approaches is increasing fast, opening a promising validation space that, so far, remains unstructured. We present a design space for the critical validation of LLM-generated tabular data with two dimensions: First, the Analysis Granularity dimension: from within-attribute (single-item and multi-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second, the Data Source dimension: differentiating between LLM-generated values, ground truth values, explanations, and their combinations. We discuss analysis tasks for each dimension cross-cut, map 19 existing validation approaches, and discuss the characteristics of two approaches in detail, demonstrating descriptive power.","sentences":["LLM-generated tabular data is creating new opportunities for data-driven applications in academia, business, and society.","To leverage benefits like missing value imputation, labeling, and enrichment with context-aware attributes, LLM-generated data needs a critical validation process.","The number of pioneering approaches is increasing fast, opening a promising validation space that, so far, remains unstructured.","We present a design space for the critical validation of LLM-generated tabular data with two dimensions:","First, the Analysis Granularity dimension: from within-attribute (single-item and multi-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n).","Second, the Data Source dimension: differentiating between LLM-generated values, ground truth values, explanations, and their combinations.","We discuss analysis tasks for each dimension cross-cut, map 19 existing validation approaches, and discuss the characteristics of two approaches in detail, demonstrating descriptive power."],"url":"http://arxiv.org/abs/2505.04487v1"}
{"created":"2025-05-07 14:59:23","title":"Efficient Flow Matching using Latent Variables","abstract":"Flow matching models have shown great potential in image generation tasks among probabilistic generative models. Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data. Most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present \\texttt{Latent-CFM}, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \\texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\\sim 50\\%$ less in some cases) and computation than state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features.","sentences":["Flow matching models have shown great potential in image generation tasks among probabilistic generative models.","Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data.","Most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian.","This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold.","Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance.","To this end, we present \\texttt{Latent-CFM}, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models.","Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \\texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\\sim 50\\%$ less in some cases) and computation than state-of-the-art flow matching models.","Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches.","In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features."],"url":"http://arxiv.org/abs/2505.04486v1"}
{"created":"2025-05-07 14:58:04","title":"FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging","abstract":"We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural network architecture built on top of the well-known KPConv, a widely adopted backbone for 3D point cloud analysis. Even though invariance and/or equivariance to Euclidean transformations are required for many common tasks, KPConv-based networks can only approximately achieve such properties when training on large datasets or with significant data augmentations. Using Frame Averaging, we allow to flexibly customize point cloud neural networks built with KPConv layers, by making them exactly invariant and/or equivariant to translations, rotations and/or reflections of the input point clouds. By simply wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical prior knowledge into it while preserving the number of learnable parameters and not compromising any input information. We showcase the benefit of such an introduced bias for point cloud classification and point cloud registration, especially in challenging cases such as scarce training data or randomly rotated test data.","sentences":["We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural network architecture built on top of the well-known KPConv, a widely adopted backbone for 3D point cloud analysis.","Even though invariance and/or equivariance to Euclidean transformations are required for many common tasks, KPConv-based networks can only approximately achieve such properties when training on large datasets or with significant data augmentations.","Using Frame Averaging, we allow to flexibly customize point cloud neural networks built with KPConv layers, by making them exactly invariant and/or equivariant to translations, rotations and/or reflections of the input point clouds.","By simply wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical prior knowledge into it while preserving the number of learnable parameters and not compromising any input information.","We showcase the benefit of such an introduced bias for point cloud classification and point cloud registration, especially in challenging cases such as scarce training data or randomly rotated test data."],"url":"http://arxiv.org/abs/2505.04485v1"}
{"created":"2025-05-07 14:51:43","title":"TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution","abstract":"Trajectory prediction is a crucial task in modeling human behavior, especially in fields as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy, while recently proposed deep learning approaches suffer from computational cost, lack of explainability, and generalization issues that limit their practical adoption. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We introduce a Cross-Generation Elite Sampling to promote population diversity and a Statistics Feedback Loop allowing the LLM to analyze alternative predictions. Our evaluations show TrajEvo outperforms previous heuristic methods on the ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning methods when generalizing to the unseen SDD dataset. TrajEvo represents a first step toward automated design of fast, explainable, and generalizable trajectory prediction heuristics. We make our source code publicly available to foster future research at https://github.com/ai4co/trajevo.","sentences":["Trajectory prediction is a crucial task in modeling human behavior, especially in fields as social robotics and autonomous vehicle navigation.","Traditional heuristics based on handcrafted rules often lack accuracy, while recently proposed deep learning approaches suffer from computational cost, lack of explainability, and generalization issues that limit their practical adoption.","In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics.","TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data.","We introduce a Cross-Generation Elite Sampling to promote population diversity and a Statistics Feedback Loop allowing the LLM to analyze alternative predictions.","Our evaluations show TrajEvo outperforms previous heuristic methods on the ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning methods when generalizing to the unseen SDD dataset.","TrajEvo represents a first step toward automated design of fast, explainable, and generalizable trajectory prediction heuristics.","We make our source code publicly available to foster future research at https://github.com/ai4co/trajevo."],"url":"http://arxiv.org/abs/2505.04480v1"}
{"created":"2025-05-07 14:35:39","title":"Discriminative Ordering Through Ensemble Consensus","abstract":"Evaluating the performance of clustering models is a challenging task where the outcome depends on the definition of what constitutes a cluster. Due to this design, current existing metrics rarely handle multiple clustering models with diverse cluster definitions, nor do they comply with the integration of constraints when available. In this work, we take inspiration from consensus clustering and assume that a set of clustering models is able to uncover hidden structures in the data. We propose to construct a discriminative ordering through ensemble clustering based on the distance between the connectivity of a clustering model and the consensus matrix. We first validate the proposed method with synthetic scenarios, highlighting that the proposed score ranks the models that best match the consensus first. We then show that this simple ranking score significantly outperforms other scoring methods when comparing sets of different clustering algorithms that are not restricted to a fixed number of clusters and is compatible with clustering constraints.","sentences":["Evaluating the performance of clustering models is a challenging task where the outcome depends on the definition of what constitutes a cluster.","Due to this design, current existing metrics rarely handle multiple clustering models with diverse cluster definitions, nor do they comply with the integration of constraints when available.","In this work, we take inspiration from consensus clustering and assume that a set of clustering models is able to uncover hidden structures in the data.","We propose to construct a discriminative ordering through ensemble clustering based on the distance between the connectivity of a clustering model and the consensus matrix.","We first validate the proposed method with synthetic scenarios, highlighting that the proposed score ranks the models that best match the consensus first.","We then show that this simple ranking score significantly outperforms other scoring methods when comparing sets of different clustering algorithms that are not restricted to a fixed number of clusters and is compatible with clustering constraints."],"url":"http://arxiv.org/abs/2505.04464v1"}
{"created":"2025-05-07 14:31:10","title":"A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities","abstract":"Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.","sentences":["Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors.","As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years.","TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments.","In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies.","We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs.","To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations.","Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field."],"url":"http://arxiv.org/abs/2505.04461v1"}
{"created":"2025-05-07 14:27:46","title":"Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration","abstract":"Training data cleaning is a new application for generative model-based speech restoration (SR). This paper introduces Miipher-2, an SR model designed for million-hour scale data, for training data cleaning for large-scale generative models like large language models. Key challenges addressed include generalization to unseen languages, operation without explicit conditioning (e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a frozen, pre-trained Universal Speech Model (USM), supporting over 300 languages, as a robust, conditioning-free feature extractor. To optimize efficiency and minimize memory, Miipher-2 incorporates parallel adapters for predicting clean USM features from noisy inputs and employs the WaneFit neural vocoder for waveform synthesis. These components were trained on 3,000 hours of multi-lingual, studio-quality recordings with augmented degradations, while USM parameters remained fixed. Experimental results demonstrate Miipher-2's superior or comparable performance to conventional SR models in word-error-rate, speaker similarity, and both objective and subjective sound quality scores across all tested languages. Miipher-2 operates efficiently on consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling the processing of a million-hour speech dataset in approximately three days using only 100 such accelerators.","sentences":["Training data cleaning is a new application for generative model-based speech restoration (SR).","This paper introduces Miipher-2, an SR model designed for million-hour scale data, for training data cleaning for large-scale generative models like large language models.","Key challenges addressed include generalization to unseen languages, operation without explicit conditioning (e.g., text, speaker ID), and computational efficiency.","Miipher-2 utilizes a frozen, pre-trained Universal Speech Model (USM), supporting over 300 languages, as a robust, conditioning-free feature extractor.","To optimize efficiency and minimize memory, Miipher-2 incorporates parallel adapters for predicting clean USM features from noisy inputs and employs the WaneFit neural vocoder for waveform synthesis.","These components were trained on 3,000 hours of multi-lingual, studio-quality recordings with augmented degradations, while USM parameters remained fixed.","Experimental results demonstrate Miipher-2's superior or comparable performance to conventional SR models in word-error-rate, speaker similarity, and both objective and subjective sound quality scores across all tested languages.","Miipher-2 operates efficiently on consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling the processing of a million-hour speech dataset in approximately three days using only 100 such accelerators."],"url":"http://arxiv.org/abs/2505.04457v1"}
{"created":"2025-05-07 14:14:29","title":"M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation","abstract":"Sequential recommendation systems aim to predict users' next preferences based on their interaction histories, but existing approaches face critical limitations in efficiency and multi-scale pattern recognition. While Transformer-based methods struggle with quadratic computational complexity, recent Mamba-based models improve efficiency but fail to capture periodic user behaviors, leverage rich semantic information, or effectively fuse multimodal features. To address these challenges, we propose \\model, a novel sequential recommendation framework that integrates multi-scale Mamba with Fourier analysis, Large Language Models (LLMs), and adaptive gating. First, we enhance Mamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns in the frequency domain, separating meaningful trends from noise. Second, we incorporate LLM-based text embeddings to enrich sparse interaction data with semantic context from item descriptions. Finally, we introduce a learnable gate mechanism to dynamically balance temporal (Mamba), frequency (FFT), and semantic (LLM) features, ensuring harmonious multimodal fusion. Extensive experiments demonstrate that \\model\\ achieves state-of-the-art performance, improving Hit Rate@10 by 3.2\\% over existing Mamba-based models while maintaining 20\\% faster inference than Transformer baselines. Our results highlight the effectiveness of combining frequency analysis, semantic understanding, and adaptive fusion for sequential recommendation. Code and datasets are available at: https://anonymous.4open.science/r/M2Rec.","sentences":["Sequential recommendation systems aim to predict users' next preferences based on their interaction histories, but existing approaches face critical limitations in efficiency and multi-scale pattern recognition.","While Transformer-based methods struggle with quadratic computational complexity, recent Mamba-based models improve efficiency but fail to capture periodic user behaviors, leverage rich semantic information, or effectively fuse multimodal features.","To address these challenges, we propose \\model, a novel sequential recommendation framework that integrates multi-scale Mamba with Fourier analysis, Large Language Models (LLMs), and adaptive gating.","First, we enhance Mamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns in the frequency domain, separating meaningful trends from noise.","Second, we incorporate LLM-based text embeddings to enrich sparse interaction data with semantic context from item descriptions.","Finally, we introduce a learnable gate mechanism to dynamically balance temporal (Mamba), frequency (FFT), and semantic (LLM) features, ensuring harmonious multimodal fusion.","Extensive experiments demonstrate that \\model\\ achieves state-of-the-art performance, improving Hit Rate@10 by 3.2\\% over existing Mamba-based models while maintaining 20\\% faster inference than Transformer baselines.","Our results highlight the effectiveness of combining frequency analysis, semantic understanding, and adaptive fusion for sequential recommendation.","Code and datasets are available at: https://anonymous.4open.science/r/M2Rec."],"url":"http://arxiv.org/abs/2505.04445v1"}
{"created":"2025-05-07 14:07:01","title":"Do We Still Need to Work on Odometry for Autonomous Driving?","abstract":"Over the past decades, a tremendous amount of work has addressed the topic of ego-motion estimation of moving platforms based on various proprioceptive and exteroceptive sensors. At the cost of ever-increasing computational load and sensor complexity, odometry algorithms have reached impressive levels of accuracy with minimal drift in various conditions. In this paper, we question the need for more research on odometry for autonomous driving by assessing the accuracy of one of the simplest algorithms: the direct integration of wheel encoder data and yaw rate measurements from a gyroscope. We denote this algorithm as Odometer-Gyroscope (OG) odometry. This work shows that OG odometry can outperform current state-of-the-art radar-inertial SE(2) odometry for a fraction of the computational cost in most scenarios. For example, the OG odometry is on top of the Boreas leaderboard with a relative translation error of 0.20%, while the second-best method displays an error of 0.26%. Lidar-inertial approaches can provide more accurate estimates, but the computational load is three orders of magnitude higher than the OG odometry. To further the analysis, we have pushed the limits of the OG odometry by purposely violating its fundamental no-slip assumption using data collected during a heavy snowstorm with different driving behaviours. Our conclusion shows that a significant amount of slippage is required to result in non-satisfactory pose estimates from the OG odometry.","sentences":["Over the past decades, a tremendous amount of work has addressed the topic of ego-motion estimation of moving platforms based on various proprioceptive and exteroceptive sensors.","At the cost of ever-increasing computational load and sensor complexity, odometry algorithms have reached impressive levels of accuracy with minimal drift in various conditions.","In this paper, we question the need for more research on odometry for autonomous driving by assessing the accuracy of one of the simplest algorithms: the direct integration of wheel encoder data and yaw rate measurements from a gyroscope.","We denote this algorithm as Odometer-Gyroscope (OG) odometry.","This work shows that OG odometry can outperform current state-of-the-art radar-inertial SE(2) odometry for a fraction of the computational cost in most scenarios.","For example, the OG odometry is on top of the Boreas leaderboard with a relative translation error of 0.20%, while the second-best method displays an error of 0.26%.","Lidar-inertial approaches can provide more accurate estimates, but the computational load is three orders of magnitude higher than the OG odometry.","To further the analysis, we have pushed the limits of the OG odometry by purposely violating its fundamental no-slip assumption using data collected during a heavy snowstorm with different driving behaviours.","Our conclusion shows that a significant amount of slippage is required to result in non-satisfactory pose estimates from the OG odometry."],"url":"http://arxiv.org/abs/2505.04438v1"}
{"created":"2025-05-07 14:02:35","title":"FedBWO: Enhancing Communication Efficiency in Federated Learning","abstract":"Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a shared model is collaboratively trained by various clients using their local datasets while keeping the data private. Considering resource-constrained devices, FL clients often suffer from restricted transmission capacity. Aiming to enhance the system performance, the communication between clients and server needs to be diminished. Current FL strategies transmit a tremendous amount of data (model weights) within the FL process, which needs a high communication bandwidth. Considering resource constraints, increasing the number of clients and, consequently, the amount of data (model weights) can lead to a bottleneck. In this paper, we introduce the Federated Black Widow Optimization (FedBWO) technique to decrease the amount of transmitted data by transmitting only a performance score rather than the local model weights from clients. FedBWO employs the BWO algorithm to improve local model updates. The conducted experiments prove that FedBWO remarkably improves the performance of the global model and the communication efficiency of the overall system. According to the experimental outcomes, FedBWO enhances the global model accuracy by an average of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically decreases the communication cost compared to other methods.","sentences":["Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a shared model is collaboratively trained by various clients using their local datasets while keeping the data private.","Considering resource-constrained devices, FL clients often suffer from restricted transmission capacity.","Aiming to enhance the system performance, the communication between clients and server needs to be diminished.","Current FL strategies transmit a tremendous amount of data (model weights) within the FL process, which needs a high communication bandwidth.","Considering resource constraints, increasing the number of clients and, consequently, the amount of data (model weights) can lead to a bottleneck.","In this paper, we introduce the Federated Black Widow Optimization (FedBWO) technique to decrease the amount of transmitted data by transmitting only a performance score rather than the local model weights from clients.","FedBWO employs the BWO algorithm to improve local model updates.","The conducted experiments prove that FedBWO remarkably improves the performance of the global model and the communication efficiency of the overall system.","According to the experimental outcomes, FedBWO enhances the global model accuracy by an average of 21% over FedAvg, and 12% over FedGWO.","Furthermore, FedBWO dramatically decreases the communication cost compared to other methods."],"url":"http://arxiv.org/abs/2505.04435v1"}
{"created":"2025-05-07 13:51:42","title":"OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models","abstract":"Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.","sentences":["Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content.","To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility.","The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact.","Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality.","We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency.","Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios."],"url":"http://arxiv.org/abs/2505.04416v1"}
{"created":"2025-05-07 13:47:22","title":"Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization","abstract":"Manifold learning aims to discover and represent low-dimensional structures underlying high-dimensional data while preserving critical topological and geometric properties. Existing methods often fail to capture local details with global topological integrity from noisy data or construct a balanced dimensionality reduction, resulting in distorted or fractured embeddings. We present an AutoEncoder-based method that integrates a manifold reconstruction layer, which uncovers latent manifold structures from noisy point clouds, and further provides regularizations on topological and geometric properties during dimensionality reduction, whereas the two components promote each other during training. Experiments on point cloud datasets demonstrate that our method outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in discovering manifold structures from noisy data and preserving them through dimensionality reduction, as validated by visualization and quantitative metrics. This work demonstrates the significance of combining manifold reconstruction with manifold learning to achieve reliable representation of the latent manifold, particularly when dealing with noisy real-world data. Code repository: https://github.com/Thanatorika/mrtg.","sentences":["Manifold learning aims to discover and represent low-dimensional structures underlying high-dimensional data while preserving critical topological and geometric properties.","Existing methods often fail to capture local details with global topological integrity from noisy data or construct a balanced dimensionality reduction, resulting in distorted or fractured embeddings.","We present an AutoEncoder-based method that integrates a manifold reconstruction layer, which uncovers latent manifold structures from noisy point clouds, and further provides regularizations on topological and geometric properties during dimensionality reduction, whereas the two components promote each other during training.","Experiments on point cloud datasets demonstrate that our method outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in discovering manifold structures from noisy data and preserving them through dimensionality reduction, as validated by visualization and quantitative metrics.","This work demonstrates the significance of combining manifold reconstruction with manifold learning to achieve reliable representation of the latent manifold, particularly when dealing with noisy real-world data.","Code repository: https://github.com/Thanatorika/mrtg."],"url":"http://arxiv.org/abs/2505.04412v1"}
{"created":"2025-05-07 13:36:59","title":"In-Context Adaptation to Concept Drift for Learned Database Operations","abstract":"Machine learning has demonstrated transformative potential for database operations, such as query optimization and in-database data analytics. However, dynamic database environments, characterized by frequent updates and evolving data distributions, introduce concept drift, which leads to performance degradation for learned models and limits their practical applicability. Addressing this challenge requires efficient frameworks capable of adapting to shifting concepts while minimizing the overhead of retraining or fine-tuning.   In this paper, we propose FLAIR, an online adaptation framework that introduces a new paradigm called \\textit{in-context adaptation} for learned database operations. FLAIR leverages the inherent property of data systems, i.e., immediate availability of execution results for predictions, to enable dynamic context construction. By formalizing adaptation as $f:(\\mathbf{x} \\,| \\,\\mathcal{C}_t) \\to \\mathbf{y}$, with $\\mathcal{C}_t$ representing a dynamic context memory, FLAIR delivers predictions aligned with the current concept, eliminating the need for runtime parameter optimization. To achieve this, FLAIR integrates two key modules: a Task Featurization Module for encoding task-specific features into standardized representations, and a Dynamic Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly using contextual information at runtime. Extensive experiments across key database tasks demonstrate that FLAIR outperforms state-of-the-art baselines, achieving up to 5.2x faster adaptation and reducing error by 22.5% for cardinality estimation.","sentences":["Machine learning has demonstrated transformative potential for database operations, such as query optimization and in-database data analytics.","However, dynamic database environments, characterized by frequent updates and evolving data distributions, introduce concept drift, which leads to performance degradation for learned models and limits their practical applicability.","Addressing this challenge requires efficient frameworks capable of adapting to shifting concepts while minimizing the overhead of retraining or fine-tuning.   ","In this paper, we propose FLAIR, an online adaptation framework that introduces a new paradigm called \\textit{in-context adaptation} for learned database operations.","FLAIR leverages the inherent property of data systems, i.e., immediate availability of execution results for predictions, to enable dynamic context construction.","By formalizing adaptation as $f:(\\mathbf{x} \\,| \\,\\mathcal{C}_t) \\to \\mathbf{y}$, with $\\mathcal{C}_t$ representing a dynamic context memory, FLAIR delivers predictions aligned with the current concept, eliminating the need for runtime parameter optimization.","To achieve this, FLAIR integrates two key modules: a Task Featurization Module for encoding task-specific features into standardized representations, and a Dynamic Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly using contextual information at runtime.","Extensive experiments across key database tasks demonstrate that FLAIR outperforms state-of-the-art baselines, achieving up to 5.2x faster adaptation and reducing error by 22.5% for cardinality estimation."],"url":"http://arxiv.org/abs/2505.04404v1"}
{"created":"2025-05-07 13:36:43","title":"Blockchain Data Analytics: A Scoping Literature Review and Directions for Future Research","abstract":"Blockchain technology has rapidly expanded beyond its original use in cryptocurrencies to a broad range of applications, creating vast amounts of immutable, decentralized data. As blockchain adoption grows, so does the need for advanced data analytics techniques to extract insights for business intelligence, fraud detection, financial analysis and many more. While previous research has examined specific aspects of blockchain data analytics, such as transaction patterns, illegal activity detection, and data management, there remains a lack of comprehensive reviews that explore the full scope of blockchain data analytics. This study addresses this gap through a scoping literature review, systematically mapping the existing research landscape, identifying key topics, and highlighting emerging trends. Using established methodologies for literature reviews, we analyze 466 publications, clustering them into six major research themes: illegal activity detection, data management, financial analysis, user analysis, community detection, and mining analysis. Our findings reveal a strong focus on detecting illicit activities and financial applications, while holistic business intelligence use cases remain underexplored. This review provides a structured overview of blockchain data analytics, identifying research gaps and proposing future directions to enhance the fields impact.","sentences":["Blockchain technology has rapidly expanded beyond its original use in cryptocurrencies to a broad range of applications, creating vast amounts of immutable, decentralized data.","As blockchain adoption grows, so does the need for advanced data analytics techniques to extract insights for business intelligence, fraud detection, financial analysis and many more.","While previous research has examined specific aspects of blockchain data analytics, such as transaction patterns, illegal activity detection, and data management, there remains a lack of comprehensive reviews that explore the full scope of blockchain data analytics.","This study addresses this gap through a scoping literature review, systematically mapping the existing research landscape, identifying key topics, and highlighting emerging trends.","Using established methodologies for literature reviews, we analyze 466 publications, clustering them into six major research themes: illegal activity detection, data management, financial analysis, user analysis, community detection, and mining analysis.","Our findings reveal a strong focus on detecting illicit activities and financial applications, while holistic business intelligence use cases remain underexplored.","This review provides a structured overview of blockchain data analytics, identifying research gaps and proposing future directions to enhance the fields impact."],"url":"http://arxiv.org/abs/2505.04403v1"}
{"created":"2025-05-07 13:20:36","title":"Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast","abstract":"The planning and operation of renewable energy, especially wind power, depend crucially on accurate, timely, and high-resolution weather information. Coarse-grid global numerical weather forecasts are typically downscaled to meet these requirements, introducing challenges of scale inconsistency, process representation error, computation cost, and entanglement of distinct uncertainty sources from chaoticity, model bias, and large-scale forcing. We address these challenges by learning the climatological distribution of a target wind farm using its high-resolution numerical weather simulations. An optimal combination of this learned high-resolution climatological prior with coarse-grid large scale forecasts yields highly accurate, fine-grained, full-variable, large ensemble of weather pattern forecasts. Using observed meteorological records and wind turbine power outputs as references, the proposed methodology verifies advantageously compared to existing numerical/statistical forecasting-downscaling pipelines, regarding either deterministic/probabilistic skills or economic gains. Moreover, a 100-member, 10-day forecast with spatial resolution of 1 km and output frequency of 15 min takes < 1 hour on a moderate-end GPU, as contrast to $\\mathcal{O}(10^3)$ CPU hours for conventional numerical simulation. By drastically reducing computational costs while maintaining accuracy, our method paves the way for more efficient and reliable renewable energy planning and operation.","sentences":["The planning and operation of renewable energy, especially wind power, depend crucially on accurate, timely, and high-resolution weather information.","Coarse-grid global numerical weather forecasts are typically downscaled to meet these requirements, introducing challenges of scale inconsistency, process representation error, computation cost, and entanglement of distinct uncertainty sources from chaoticity, model bias, and large-scale forcing.","We address these challenges by learning the climatological distribution of a target wind farm using its high-resolution numerical weather simulations.","An optimal combination of this learned high-resolution climatological prior with coarse-grid large scale forecasts yields highly accurate, fine-grained, full-variable, large ensemble of weather pattern forecasts.","Using observed meteorological records and wind turbine power outputs as references, the proposed methodology verifies advantageously compared to existing numerical/statistical forecasting-downscaling pipelines, regarding either deterministic/probabilistic skills or economic gains.","Moreover, a 100-member, 10-day forecast with spatial resolution of 1 km and output frequency of 15 min takes < 1 hour on a moderate-end GPU, as contrast to $\\mathcal{O}(10^3)$ CPU hours for conventional numerical simulation.","By drastically reducing computational costs while maintaining accuracy, our method paves the way for more efficient and reliable renewable energy planning and operation."],"url":"http://arxiv.org/abs/2505.04396v1"}
{"created":"2025-05-07 13:18:43","title":"SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer","abstract":"This paper presents an efficient visual speech encoder for lip reading. While most recent lip reading studies have been based on the ResNet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information. Additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation). To overcome the limitations of Convolutional Neural Network (CNN)-based models, we apply the hierarchical structure and window self-attention of the Swin Transformer to lip reading. We configure a new lightweight scale of the Swin Transformer suitable for processing lip reading data and present the SwinLip visual speech encoder, which efficiently reduces computational load by integrating modified Convolution-augmented Transformer (Conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure. Through extensive experiments, we have validated that our SwinLip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load. In particular, our SwinLip demonstrated robust performance in both English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art performance on the Mandarin LRW-1000 dataset with less computation compared to the existing state-of-the-art model.","sentences":["This paper presents an efficient visual speech encoder for lip reading.","While most recent lip reading studies have been based on the ResNet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information.","Additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation).","To overcome the limitations of Convolutional Neural Network (CNN)-based models, we apply the hierarchical structure and window self-attention of the Swin Transformer to lip reading.","We configure a new lightweight scale of the Swin Transformer suitable for processing lip reading data and present the SwinLip visual speech encoder, which efficiently reduces computational load by integrating modified Convolution-augmented Transformer (Conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure.","Through extensive experiments, we have validated that our SwinLip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load.","In particular, our SwinLip demonstrated robust performance in both English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art performance on the Mandarin LRW-1000 dataset with less computation compared to the existing state-of-the-art model."],"url":"http://arxiv.org/abs/2505.04394v1"}
{"created":"2025-05-07 13:18:41","title":"Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters","abstract":"With the increasing prevalence of artificial intelligence, careful evaluation of inherent biases needs to be conducted to form the basis for alleviating the effects these predispositions can have on users. Large language models (LLMs) are predominantly used by many as a primary source of information for various topics. LLMs frequently make factual errors, fabricate data (hallucinations), or present biases, exposing users to misinformation and influencing opinions. Educating users on their risks is key to responsible use, as bias, unlike hallucinations, cannot be caught through data verification. We quantify the political bias of popular LLMs in the context of the recent vote of the German Bundestag using the score produced by the Wahl-O-Mat. This metric measures the alignment between an individual's political views and the positions of German political parties. We compare the models' alignment scores to identify factors influencing their political preferences. Doing so, we discover a bias toward left-leaning parties, most dominant in larger LLMs. Also, we find that the language we use to communicate with the models affects their political views. Additionally, we analyze the influence of a model's origin and release date and compare the results to the outcome of the recent vote of the Bundestag. Our results imply that LLMs are prone to exhibiting political bias. Large corporations with the necessary means to develop LLMs, thus, knowingly or unknowingly, have a responsibility to contain these biases, as they can influence each voter's decision-making process and inform public opinion in general and at scale.","sentences":["With the increasing prevalence of artificial intelligence, careful evaluation of inherent biases needs to be conducted to form the basis for alleviating the effects these predispositions can have on users.","Large language models (LLMs) are predominantly used by many as a primary source of information for various topics.","LLMs frequently make factual errors, fabricate data (hallucinations), or present biases, exposing users to misinformation and influencing opinions.","Educating users on their risks is key to responsible use, as bias, unlike hallucinations, cannot be caught through data verification.","We quantify the political bias of popular LLMs in the context of the recent vote of the German Bundestag using the score produced by the Wahl-O-Mat.","This metric measures the alignment between an individual's political views and the positions of German political parties.","We compare the models' alignment scores to identify factors influencing their political preferences.","Doing so, we discover a bias toward left-leaning parties, most dominant in larger LLMs.","Also, we find that the language we use to communicate with the models affects their political views.","Additionally, we analyze the influence of a model's origin and release date and compare the results to the outcome of the recent vote of the Bundestag.","Our results imply that LLMs are prone to exhibiting political bias.","Large corporations with the necessary means to develop LLMs, thus, knowingly or unknowingly, have a responsibility to contain these biases, as they can influence each voter's decision-making process and inform public opinion in general and at scale."],"url":"http://arxiv.org/abs/2505.04393v1"}
{"created":"2025-05-07 13:13:46","title":"Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets","abstract":"Clustering is a fundamental task in data mining and machine learning, particularly for analyzing large-scale data. In this paper, we introduce Clust-Splitter, an efficient algorithm based on nonsmooth optimization, designed to solve the minimum sum-of-squares clustering problem in very large datasets. The clustering task is approached through a sequence of three nonsmooth optimization problems: two auxiliary problems used to generate suitable starting points, followed by a main clustering formulation. To solve these problems effectively, the limited memory bundle method is combined with an incremental approach to develop the Clust-Splitter algorithm. We evaluate Clust-Splitter on real-world datasets characterized by both a large number of attributes and a large number of data points and compare its performance with several state-of-the-art large-scale clustering algorithms. Experimental results demonstrate the efficiency of the proposed method for clustering very large datasets, as well as the high quality of its solutions, which are on par with those of the best existing methods.","sentences":["Clustering is a fundamental task in data mining and machine learning, particularly for analyzing large-scale data.","In this paper, we introduce Clust-Splitter, an efficient algorithm based on nonsmooth optimization, designed to solve the minimum sum-of-squares clustering problem in very large datasets.","The clustering task is approached through a sequence of three nonsmooth optimization problems: two auxiliary problems used to generate suitable starting points, followed by a main clustering formulation.","To solve these problems effectively, the limited memory bundle method is combined with an incremental approach to develop the Clust-Splitter algorithm.","We evaluate Clust-Splitter on real-world datasets characterized by both a large number of attributes and a large number of data points and compare its performance with several state-of-the-art large-scale clustering algorithms.","Experimental results demonstrate the efficiency of the proposed method for clustering very large datasets, as well as the high quality of its solutions, which are on par with those of the best existing methods."],"url":"http://arxiv.org/abs/2505.04389v1"}
{"created":"2025-05-07 13:13:14","title":"The Aloe Family Recipe for Open and Specialized Healthcare LLMs","abstract":"Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.","sentences":["Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest.","This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG).","The evaluation methodology used, which includes four different types of tests, defines a new standard for the field.","The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   ","Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples.","The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks.","Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   ","Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family.","These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals.","On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks.","For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   ","Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements.","This work sets a new standard for developing and reporting aligned LLMs in healthcare."],"url":"http://arxiv.org/abs/2505.04388v1"}
{"created":"2025-05-07 13:05:32","title":"DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution","abstract":"Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of 'Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in 2.55% / 5.7% under different settings, compared with the existing methods.","sentences":["Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations.","However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features.","Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios.","To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task.","Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of 'Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information.","Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements.","Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules.","Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in 2.55% / 5.7% under different settings, compared with the existing methods."],"url":"http://arxiv.org/abs/2505.04384v1"}
{"created":"2025-05-07 12:59:59","title":"Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic","abstract":"Transportation systems have long been shaped by complexity and heterogeneity, driven by the interdependency of agent actions and traffic outcomes. The deployment of automated vehicles (AVs) in such systems introduces a new challenge: achieving consensus across safety, interaction quality, and traffic performance. In this work, we position consensus as a fundamental property of the traffic system and aim to quantify it. We use high-resolution trajectory data from the Third Generation Simulation (TGSIM) dataset to empirically analyze AV and human-driven vehicle (HDV) behavior at a signalized urban intersection and around vulnerable road users (VRUs). Key metrics, including Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns, headways, and string stability, are evaluated across the three performance dimensions. Results show that full consensus across safety, interaction, and performance is rare, with only 1.63% of AV-VRU interaction frames meeting all three conditions. These findings highlight the need for AV models that explicitly balance multi-dimensional performance in mixed-traffic environments. Full reproducibility is supported via our open-source codebase on https://github.com/wissamkontar/Consensus-AV-Analysis.","sentences":["Transportation systems have long been shaped by complexity and heterogeneity, driven by the interdependency of agent actions and traffic outcomes.","The deployment of automated vehicles (AVs) in such systems introduces a new challenge: achieving consensus across safety, interaction quality, and traffic performance.","In this work, we position consensus as a fundamental property of the traffic system and aim to quantify it.","We use high-resolution trajectory data from the Third Generation Simulation (TGSIM) dataset to empirically analyze AV and human-driven vehicle (HDV) behavior at a signalized urban intersection and around vulnerable road users (VRUs).","Key metrics, including Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns, headways, and string stability, are evaluated across the three performance dimensions.","Results show that full consensus across safety, interaction, and performance is rare, with only 1.63% of AV-VRU interaction frames meeting all three conditions.","These findings highlight the need for AV models that explicitly balance multi-dimensional performance in mixed-traffic environments.","Full reproducibility is supported via our open-source codebase on https://github.com/wissamkontar/Consensus-AV-Analysis."],"url":"http://arxiv.org/abs/2505.04379v1"}
{"created":"2025-05-07 12:57:40","title":"Label-efficient Single Photon Images Classification via Active Learning","abstract":"Single-photon LiDAR achieves high-precision 3D imaging in extreme environments through quantum-level photon detection technology. Current research primarily focuses on reconstructing 3D scenes from sparse photon events, whereas the semantic interpretation of single-photon images remains underexplored, due to high annotation costs and inefficient labeling strategies. This paper presents the first active learning framework for single-photon image classification. The core contribution is an imaging condition-aware sampling strategy that integrates synthetic augmentation to model variability across imaging conditions. By identifying samples where the model is both uncertain and sensitive to these conditions, the proposed method selectively annotates only the most informative examples. Experiments on both synthetic and real-world datasets show that our approach outperforms all baselines and achieves high classification accuracy with significantly fewer labeled samples. Specifically, our approach achieves 97% accuracy on synthetic single-photon data using only 1.5% labeled samples. On real-world data, we maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher than the best-performing baseline. This illustrates that active learning enables the same level of classification performance on single-photon images as on classical images, opening doors to large-scale integration of single-photon data in real-world applications.","sentences":["Single-photon LiDAR achieves high-precision 3D imaging in extreme environments through quantum-level photon detection technology.","Current research primarily focuses on reconstructing 3D scenes from sparse photon events, whereas the semantic interpretation of single-photon images remains underexplored, due to high annotation costs and inefficient labeling strategies.","This paper presents the first active learning framework for single-photon image classification.","The core contribution is an imaging condition-aware sampling strategy that integrates synthetic augmentation to model variability across imaging conditions.","By identifying samples where the model is both uncertain and sensitive to these conditions, the proposed method selectively annotates only the most informative examples.","Experiments on both synthetic and real-world datasets show that our approach outperforms all baselines and achieves high classification accuracy with significantly fewer labeled samples.","Specifically, our approach achieves 97% accuracy on synthetic single-photon data using only 1.5% labeled samples.","On real-world data, we maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher than the best-performing baseline.","This illustrates that active learning enables the same level of classification performance on single-photon images as on classical images, opening doors to large-scale integration of single-photon data in real-world applications."],"url":"http://arxiv.org/abs/2505.04376v1"}
{"created":"2025-05-07 12:32:05","title":"CDE-Mapper: Using Retrieval-Augmented Language Models for Linking Clinical Data Elements to Controlled Vocabularies","abstract":"The standardization of clinical data elements (CDEs) aims to ensure consistent and comprehensive patient information across various healthcare systems. Existing methods often falter when standardizing CDEs of varying representation and complex structure, impeding data integration and interoperability in clinical research. We introduce CDE-Mapper, an innovative framework that leverages Retrieval-Augmented Generation approach combined with Large Language Models to automate the linking of CDEs to controlled vocabularies. Our modular approach features query decomposition to manage varying levels of CDEs complexity, integrates expert-defined rules within prompt engineering, and employs in-context learning alongside multiple retriever components to resolve terminological ambiguities. In addition, we propose a knowledge reservoir validated by a human-in-loop approach, achieving accurate concept linking for future applications while minimizing computational costs. For four diverse datasets, CDE-Mapper achieved an average of 7.2\\% higher accuracy improvement compared to baseline methods. This work highlights the potential of advanced language models in improving data harmonization and significantly advancing capabilities in clinical decision support systems and research.","sentences":["The standardization of clinical data elements (CDEs) aims to ensure consistent and comprehensive patient information across various healthcare systems.","Existing methods often falter when standardizing CDEs of varying representation and complex structure, impeding data integration and interoperability in clinical research.","We introduce CDE-Mapper, an innovative framework that leverages Retrieval-Augmented Generation approach combined with Large Language Models to automate the linking of CDEs to controlled vocabularies.","Our modular approach features query decomposition to manage varying levels of CDEs complexity, integrates expert-defined rules within prompt engineering, and employs in-context learning alongside multiple retriever components to resolve terminological ambiguities.","In addition, we propose a knowledge reservoir validated by a human-in-loop approach, achieving accurate concept linking for future applications while minimizing computational costs.","For four diverse datasets, CDE-Mapper achieved an average of 7.2\\% higher accuracy improvement compared to baseline methods.","This work highlights the potential of advanced language models in improving data harmonization and significantly advancing capabilities in clinical decision support systems and research."],"url":"http://arxiv.org/abs/2505.04365v1"}
{"created":"2025-05-07 12:20:55","title":"RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery Scheme in Mobile Crowdsensing","abstract":"Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS). However, existing TD methods, including privacy-preserving TD approaches, estimate the truth by weighting only the data submitted in the current round, which often results in low data quality. Moreover, there is a lack of effective TD methods that preserve both reputation and data privacy. To address these issues, a Reputation and Data Privacy-Preserving based Truth Discovery (RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD) approach, which integrates the weight of current-round data with workers' reputation values to estimate the truth, thereby achieving more accurate results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which ensures privacy preservation for sensing data and reputation values. First, the RDPP approach, when seamlessly integrated with RTD, can effectively evaluate the reliability of workers and their sensing data in a privacy-preserving manner. Second, the RDPP scheme supports reputation-based worker recruitment and rewards, ensuring high-quality data collection while incentivizing workers to provide accurate information. Comprehensive theoretical analysis and extensive experiments based on real-world datasets demonstrate that the proposed RDPP-TD scheme provides strong privacy protection and improves data quality by up to 33.3%.","sentences":["Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS).","However, existing TD methods, including privacy-preserving TD approaches, estimate the truth by weighting only the data submitted in the current round, which often results in low data quality.","Moreover, there is a lack of effective TD methods that preserve both reputation and data privacy.","To address these issues, a Reputation and Data Privacy-Preserving based Truth Discovery (RDPP-TD) scheme is proposed to obtain high-quality data for MCS.","The RDPP-TD scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD) approach, which integrates the weight of current-round data with workers' reputation values to estimate the truth, thereby achieving more accurate results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which ensures privacy preservation for sensing data and reputation values.","First, the RDPP approach, when seamlessly integrated with RTD, can effectively evaluate the reliability of workers and their sensing data in a privacy-preserving manner.","Second, the RDPP scheme supports reputation-based worker recruitment and rewards, ensuring high-quality data collection while incentivizing workers to provide accurate information.","Comprehensive theoretical analysis and extensive experiments based on real-world datasets demonstrate that the proposed RDPP-TD scheme provides strong privacy protection and improves data quality by up to 33.3%."],"url":"http://arxiv.org/abs/2505.04361v1"}
{"created":"2025-05-07 11:46:02","title":"Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration","abstract":"Clustering aims to form groups of similar data points in an unsupervised regime. Yet, clustering complex datasets containing critically intertwined shapes poses significant challenges. The prevailing clustering algorithms widely depend on evaluating similarity measures based on Euclidean metrics. Exploring topological characteristics to perform clustering of complex datasets inevitably presents a better scope. The topological clustering algorithms predominantly perceive the point set through the lens of Simplicial complexes and Persistent homology. Despite these approaches, the existing topological clustering algorithms cannot somehow fully exploit topological structures and show inconsistent performances on some highly complicated datasets. This work aims to mitigate the limitations by identifying topologically similar neighbors through the Vietoris-Rips complex and Betti number filtration. In addition, we introduce the concept of the Betti sequences to capture flexibly essential features from the topological structures. Our proposed algorithm is adept at clustering complex, intertwined shapes contained in the datasets. We carried out experiments on several synthetic and real-world datasets. Our algorithm demonstrated commendable performances across the datasets compared to some of the well-known topology-based clustering algorithms.","sentences":["Clustering aims to form groups of similar data points in an unsupervised regime.","Yet, clustering complex datasets containing critically intertwined shapes poses significant challenges.","The prevailing clustering algorithms widely depend on evaluating similarity measures based on Euclidean metrics.","Exploring topological characteristics to perform clustering of complex datasets inevitably presents a better scope.","The topological clustering algorithms predominantly perceive the point set through the lens of Simplicial complexes and Persistent homology.","Despite these approaches, the existing topological clustering algorithms cannot somehow fully exploit topological structures and show inconsistent performances on some highly complicated datasets.","This work aims to mitigate the limitations by identifying topologically similar neighbors through the Vietoris-Rips complex and Betti number filtration.","In addition, we introduce the concept of the Betti sequences to capture flexibly essential features from the topological structures.","Our proposed algorithm is adept at clustering complex, intertwined shapes contained in the datasets.","We carried out experiments on several synthetic and real-world datasets.","Our algorithm demonstrated commendable performances across the datasets compared to some of the well-known topology-based clustering algorithms."],"url":"http://arxiv.org/abs/2505.04346v1"}
{"created":"2025-05-07 11:37:23","title":"Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning","abstract":"DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.","sentences":["DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data.","However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications.","In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN.","First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree.","Each partition is then assigned to an agent to find the best clustering parameters without manual assistance.","The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions.","Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed.","The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process.","Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters.","Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces.","Extensive experiments are conducted on nine artificial datasets and a real-world dataset.","The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters."],"url":"http://arxiv.org/abs/2505.04339v1"}
{"created":"2025-05-07 11:32:53","title":"Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces","abstract":"Clustering algorithms play a pivotal role in unsupervised learning by identifying and grouping similar objects based on shared characteristics. While traditional clustering techniques, such as hard and fuzzy center-based clustering, have been widely used, they struggle with complex, high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy $C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits notable limitations in non-Euclidean spaces. Euclidean spaces assume linear separability and uniform distance scaling, limiting their effectiveness in capturing complex, hierarchical, or non-Euclidean structures in fuzzy clustering. To overcome these challenges, we introduce Filtration-based Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for better representation of data relationships in non-Euclidean spaces. HypeFCM integrates the principles of fuzzy clustering with hyperbolic geometry and employs a weight-based filtering mechanism to improve performance. The algorithm initializes weights using a Dirichlet distribution and iteratively refines cluster centroids and membership assignments based on a hyperbolic metric in the Poincar\\'e Disc model. Extensive experimental evaluations demonstrate that HypeFCM significantly outperforms conventional fuzzy clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.","sentences":["Clustering algorithms play a pivotal role in unsupervised learning by identifying and grouping similar objects based on shared characteristics.","While traditional clustering techniques, such as hard and fuzzy center-based clustering, have been widely used, they struggle with complex, high-dimensional, and non-Euclidean datasets.","In particular, the Fuzzy $C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits notable limitations in non-Euclidean spaces.","Euclidean spaces assume linear separability and uniform distance scaling, limiting their effectiveness in capturing complex, hierarchical, or non-Euclidean structures in fuzzy clustering.","To overcome these challenges, we introduce Filtration-based Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for better representation of data relationships in non-Euclidean spaces.","HypeFCM integrates the principles of fuzzy clustering with hyperbolic geometry and employs a weight-based filtering mechanism to improve performance.","The algorithm initializes weights using a Dirichlet distribution and iteratively refines cluster centroids and membership assignments based on a hyperbolic metric in the Poincar\\'e Disc model.","Extensive experimental evaluations demonstrate that HypeFCM significantly outperforms conventional fuzzy clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness."],"url":"http://arxiv.org/abs/2505.04335v1"}
{"created":"2025-05-07 11:21:12","title":"Design and Evaluation of an NDN-Based Network for Distributed Digital Twins","abstract":"Digital twins (DT) have received significant attention due to their numerous benefits, such as real-time data analytics and cost reduction in production. DT serves as a fundamental component of many applications, encompassing smart manufacturing, intelligent vehicles, and smart cities. By using Machine Learning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently facilitate decision-making and productivity by simulating the status and changes of a physical entity. To handle the massive amount of data brought by DTs, it is challenging to achieve low response latency for data fetching over existing IP-based networks. IP-based networks use host addresses for end-to-end communication, making data distribution between DTs inefficient. Thus, we propose to use DTs in a distributed manner over Named Data Networking (NDN) networks. NDN is data-centric where data is routed based on content names, dynamically adjusting paths to optimize latency. Popular data is cached in network nodes, reducing data transmission and network congestion. Since data is fetched by content names, users and mobile devices can move freely without IP address reassignment. By using in-network caching and adaptive routing, we reckon NDN is an ideal fit for Future G Networks in the context of Digital Twins. We compared DTs in edge scenarios with cloud scenarios over NDN and IP-based networks to validate our insights. Extensive simulation results show that using DT in the edge reduces response latency by 10.2x. This position paper represents an initial investigation into the gap in distributed DTs over NDN, serving as an early-stage study.","sentences":["Digital twins (DT) have received significant attention due to their numerous benefits, such as real-time data analytics and cost reduction in production.","DT serves as a fundamental component of many applications, encompassing smart manufacturing, intelligent vehicles, and smart cities.","By using Machine Learning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently facilitate decision-making and productivity by simulating the status and changes of a physical entity.","To handle the massive amount of data brought by DTs, it is challenging to achieve low response latency for data fetching over existing IP-based networks.","IP-based networks use host addresses for end-to-end communication, making data distribution between DTs inefficient.","Thus, we propose to use DTs in a distributed manner over Named Data Networking (NDN) networks.","NDN is data-centric where data is routed based on content names, dynamically adjusting paths to optimize latency.","Popular data is cached in network nodes, reducing data transmission and network congestion.","Since data is fetched by content names, users and mobile devices can move freely without IP address reassignment.","By using in-network caching and adaptive routing, we reckon NDN is an ideal fit for Future G Networks in the context of Digital Twins.","We compared DTs in edge scenarios with cloud scenarios over NDN and IP-based networks to validate our insights.","Extensive simulation results show that using DT in the edge reduces response latency by 10.2x.","This position paper represents an initial investigation into the gap in distributed DTs over NDN, serving as an early-stage study."],"url":"http://arxiv.org/abs/2505.04326v1"}
{"created":"2025-05-07 11:13:15","title":"Towards Federated Digital Twin Platforms","abstract":"Digital Twin (DT) technology has become rather popular in recent years, promising to optimize production processes, manage the operation of cyber-physical systems, with an impact spanning across multiple application domains (e.g., manufacturing, robotics, space etc.). DTs can include different kinds of assets, e.g., models, data, which could potentially be reused across DT projects by multiple users, directly affecting development costs, as well as enabling collaboration and further development of these assets. To provide user support for these purposes, dedicated DT frameworks and platforms are required, that take into account user needs, providing the infrastructure and building blocks for DT development and management. In this demo paper, we show how the DT as a Service (DTaaS) platform has been extended to enable a federated approach to DT development and management, that allows multiple users across multiple instances of DTaaS to discover, reuse, reconfigure, and modify existing DT assets.","sentences":["Digital Twin (DT) technology has become rather popular in recent years, promising to optimize production processes, manage the operation of cyber-physical systems, with an impact spanning across multiple application domains (e.g., manufacturing, robotics, space etc.).","DTs can include different kinds of assets, e.g., models, data, which could potentially be reused across DT projects by multiple users, directly affecting development costs, as well as enabling collaboration and further development of these assets.","To provide user support for these purposes, dedicated DT frameworks and platforms are required, that take into account user needs, providing the infrastructure and building blocks for DT development and management.","In this demo paper, we show how the DT as a Service (DTaaS) platform has been extended to enable a federated approach to DT development and management, that allows multiple users across multiple instances of DTaaS to discover, reuse, reconfigure, and modify existing DT assets."],"url":"http://arxiv.org/abs/2505.04324v1"}
{"created":"2025-05-07 11:04:47","title":"Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing","abstract":"As the adoption of deep learning models has grown beyond human capacity for verification, meta-algorithms are needed to ensure reliable model inference. Concept drift detection is a field dedicated to identifying statistical shifts that is underutilized in monitoring neural networks that may encounter inference data with distributional characteristics diverging from their training data. Given the wide variety of model architectures, applications, and datasets, it is important that concept drift detection algorithms are adaptable to different inference scenarios. In this paper, we introduce an application of the $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection meta-algorithm applied to a multilayer perceptron, a convolutional neural network, and a transformer trained for machine vision as they are exposed to simulated drift during inference. To that end, we demonstrate how unexpected drops in accuracy due to concept drift can be detected without directly examining the inference outputs. Our approach enhances safety by ensuring models are continually evaluated for reliability across varying conditions.","sentences":["As the adoption of deep learning models has grown beyond human capacity for verification, meta-algorithms are needed to ensure reliable model inference.","Concept drift detection is a field dedicated to identifying statistical shifts that is underutilized in monitoring neural networks that may encounter inference data with distributional characteristics diverging from their training data.","Given the wide variety of model architectures, applications, and datasets, it is important that concept drift detection algorithms are adaptable to different inference scenarios.","In this paper, we introduce an application of the $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection meta-algorithm applied to a multilayer perceptron, a convolutional neural network, and a transformer trained for machine vision as they are exposed to simulated drift during inference.","To that end, we demonstrate how unexpected drops in accuracy due to concept drift can be detected without directly examining the inference outputs.","Our approach enhances safety by ensuring models are continually evaluated for reliability across varying conditions."],"url":"http://arxiv.org/abs/2505.04318v1"}
{"created":"2025-05-07 10:48:04","title":"Integrating Large Citation Datasets","abstract":"This paper explores methods for building a comprehensive citation graph using big data techniques to evaluate scientific impact more accurately. Traditional citation metrics have limitations, and this work investigates merging large citation datasets to create a more accurate picture. Challenges of big data, like inconsistent data formats and lack of unique identifiers, are addressed through deduplication efforts, resulting in a streamlined and reliable merged dataset with over 119 million records and 1.4 billion citations. We demonstrate that merging large citation datasets builds a more accurate citation graph facilitating a more robust evaluation of scientific impact.","sentences":["This paper explores methods for building a comprehensive citation graph using big data techniques to evaluate scientific impact more accurately.","Traditional citation metrics have limitations, and this work investigates merging large citation datasets to create a more accurate picture.","Challenges of big data, like inconsistent data formats and lack of unique identifiers, are addressed through deduplication efforts, resulting in a streamlined and reliable merged dataset with over 119 million records and 1.4 billion citations.","We demonstrate that merging large citation datasets builds a more accurate citation graph facilitating a more robust evaluation of scientific impact."],"url":"http://arxiv.org/abs/2505.04309v1"}
{"created":"2025-05-07 09:59:36","title":"From Incidents to Insights: Patterns of Responsibility following AI Harms","abstract":"The AI Incident Database was inspired by aviation safety databases, which enable collective learning from failures to prevent future incidents. The database documents hundreds of AI failures, collected from the news and media. However, criticism highlights that the AIID's reliance on media reporting limits its utility for learning about implementation failures. In this paper, we accept that the AIID falls short in its original mission, but argue that by looking beyond technically-focused learning, the dataset can provide new, highly valuable insights: specifically, opportunities to learn about patterns between developers, deployers, victims, wider society, and law-makers that emerge after AI failures. Through a three-tier mixed-methods analysis of 962 incidents and 4,743 related reports from the AIID, we examine patterns across incidents, focusing on cases with public responses tagged in the database. We identify 'typical' incidents found in the AIID, from Tesla crashes to deepfake scams.   Focusing on this interplay between relevant parties, we uncover patterns in accountability and social expectations of responsibility. We find that the presence of identifiable responsible parties does not necessarily lead to increased accountability. The likelihood of a response and what it amounts to depends highly on context, including who built the technology, who was harmed, and to what extent. Controversy-rich incidents provide valuable data about societal reactions, including insights into social expectations. Equally informative are cases where controversy is notably absent. This work shows that the AIID's value lies not just in preventing technical failures, but in documenting patterns of harms and of institutional response and social learning around AI incidents. These patterns offer crucial insights for understanding how society adapts to and governs emerging AI technologies.","sentences":["The AI Incident Database was inspired by aviation safety databases, which enable collective learning from failures to prevent future incidents.","The database documents hundreds of AI failures, collected from the news and media.","However, criticism highlights that the AIID's reliance on media reporting limits its utility for learning about implementation failures.","In this paper, we accept that the AIID falls short in its original mission, but argue that by looking beyond technically-focused learning, the dataset can provide new, highly valuable insights: specifically, opportunities to learn about patterns between developers, deployers, victims, wider society, and law-makers that emerge after AI failures.","Through a three-tier mixed-methods analysis of 962 incidents and 4,743 related reports from the AIID, we examine patterns across incidents, focusing on cases with public responses tagged in the database.","We identify 'typical' incidents found in the AIID, from Tesla crashes to deepfake scams.   ","Focusing on this interplay between relevant parties, we uncover patterns in accountability and social expectations of responsibility.","We find that the presence of identifiable responsible parties does not necessarily lead to increased accountability.","The likelihood of a response and what it amounts to depends highly on context, including who built the technology, who was harmed, and to what extent.","Controversy-rich incidents provide valuable data about societal reactions, including insights into social expectations.","Equally informative are cases where controversy is notably absent.","This work shows that the AIID's value lies not just in preventing technical failures, but in documenting patterns of harms and of institutional response and social learning around AI incidents.","These patterns offer crucial insights for understanding how society adapts to and governs emerging AI technologies."],"url":"http://arxiv.org/abs/2505.04291v1"}
{"created":"2025-05-07 09:40:18","title":"GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance","abstract":"In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making. While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer. This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment. To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset. This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug. Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model. Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks. Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations. This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care. The code and dataset used in this work are publicly available.","sentences":["In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making.","While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer.","This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment.","To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset.","This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug.","Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model.","Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks.","Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations.","This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care.","The code and dataset used in this work are publicly available."],"url":"http://arxiv.org/abs/2505.04284v1"}
{"created":"2025-05-07 09:35:05","title":"TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement","abstract":"This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount of real RAW data to adapt to the noise characteristics of specific cameras. A structural reparameterization technique further simplifies CFI$^T$ for efficient deployment. To address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. Additionally, a novel dataset, QID, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. Experimental results demonstrate that TS-Diff achieves state-of-the-art performance on multiple datasets, including QID, SID, and ELD, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. These findings highlight the robustness and versatility of TS-Diff, making it a practical solution for low-light imaging applications. Source codes and models are available at https://github.com/CircccleK/TS-Diff","sentences":["This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images.","In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space.","Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras.","During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount of real RAW data to adapt to the noise characteristics of specific cameras.","A structural reparameterization technique further simplifies CFI$^T$ for efficient deployment.","To address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions.","Additionally, a novel dataset, QID, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions.","Experimental results demonstrate that TS-Diff achieves state-of-the-art performance on multiple datasets, including QID, SID, and ELD, excelling in denoising, generalization, and color consistency across various cameras and illumination levels.","These findings highlight the robustness and versatility of TS-Diff, making it a practical solution for low-light imaging applications.","Source codes and models are available at https://github.com/CircccleK/TS-Diff"],"url":"http://arxiv.org/abs/2505.04281v1"}
{"created":"2025-05-07 09:29:39","title":"Non-stationary Diffusion For Probabilistic Time Series Forecasting","abstract":"Due to the dynamics of underlying physics and external influences, the uncertainty of time series often varies over time. However, existing Denoising Diffusion Probabilistic Models (DDPMs) often fail to capture this non-stationary nature, constrained by their constant variance assumption from the additive noise model (ANM). In this paper, we innovatively utilize the Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of ANM. A diffusion-based probabilistic forecasting framework, termed Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of modeling the changing pattern of uncertainty. Specifically, NsDiff combines a denoising diffusion-based conditional generative model with a pre-trained conditional mean and variance estimator, enabling adaptive endpoint distribution modeling. Furthermore, we propose an uncertainty-aware noise schedule, which dynamically adjusts the noise levels to accurately reflect the data uncertainty at each step and integrates the time-varying variances into the diffusion process. Extensive experiments conducted on nine real-world and synthetic datasets demonstrate the superior performance of NsDiff compared to existing approaches. Code is available at https://github.com/wwy155/NsDiff.","sentences":["Due to the dynamics of underlying physics and external influences, the uncertainty of time series often varies over time.","However, existing Denoising Diffusion Probabilistic Models (DDPMs) often fail to capture this non-stationary nature, constrained by their constant variance assumption from the additive noise model (ANM).","In this paper, we innovatively utilize the Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of ANM.","A diffusion-based probabilistic forecasting framework, termed Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of modeling the changing pattern of uncertainty.","Specifically, NsDiff combines a denoising diffusion-based conditional generative model with a pre-trained conditional mean and variance estimator, enabling adaptive endpoint distribution modeling.","Furthermore, we propose an uncertainty-aware noise schedule, which dynamically adjusts the noise levels to accurately reflect the data uncertainty at each step and integrates the time-varying variances into the diffusion process.","Extensive experiments conducted on nine real-world and synthetic datasets demonstrate the superior performance of NsDiff compared to existing approaches.","Code is available at https://github.com/wwy155/NsDiff."],"url":"http://arxiv.org/abs/2505.04278v1"}
{"created":"2025-05-07 09:21:54","title":"Joint Task Offloading and Channel Allocation in Spatial-Temporal Dynamic for MEC Networks","abstract":"Computation offloading and resource allocation are critical in mobile edge computing (MEC) systems to handle the massive and complex requirements of applications restricted by limited resources. In a multi-user multi-server MEC network, the mobility of terminals causes computing requests to be dynamically distributed in space. At the same time, the non-negligible dependencies among tasks in some specific applications impose temporal correlation constraints on the solution as well, leading the time-adjacent tasks to experience varying resource availability and competition from parallel counterparts. To address such dynamic spatial-temporal characteristics as a challenge in the allocation of communication and computation resources, we formulate a long-term delay-energy trade-off cost minimization problem in the view of jointly optimizing task offloading and resource allocation. We begin by designing a priority evaluation scheme to decouple task dependencies and then develop a grouped Knapsack problem for channel allocation considering the current data load and channel status. Afterward, in order to meet the rapid response needs of MEC systems, we exploit the double duel deep Q network (D3QN) to make offloading decisions and integrate channel allocation results into the reward as part of the dynamic environment feedback in D3QN, constituting the joint optimization of task offloading and channel allocation. Finally, comprehensive simulations demonstrate the performance of the proposed algorithm in the delay-energy trade-off cost and its adaptability for various applications.","sentences":["Computation offloading and resource allocation are critical in mobile edge computing (MEC) systems to handle the massive and complex requirements of applications restricted by limited resources.","In a multi-user multi-server MEC network, the mobility of terminals causes computing requests to be dynamically distributed in space.","At the same time, the non-negligible dependencies among tasks in some specific applications impose temporal correlation constraints on the solution as well, leading the time-adjacent tasks to experience varying resource availability and competition from parallel counterparts.","To address such dynamic spatial-temporal characteristics as a challenge in the allocation of communication and computation resources, we formulate a long-term delay-energy trade-off cost minimization problem in the view of jointly optimizing task offloading and resource allocation.","We begin by designing a priority evaluation scheme to decouple task dependencies and then develop a grouped Knapsack problem for channel allocation considering the current data load and channel status.","Afterward, in order to meet the rapid response needs of MEC systems, we exploit the double duel deep Q network (D3QN) to make offloading decisions and integrate channel allocation results into the reward as part of the dynamic environment feedback in D3QN, constituting the joint optimization of task offloading and channel allocation.","Finally, comprehensive simulations demonstrate the performance of the proposed algorithm in the delay-energy trade-off cost and its adaptability for various applications."],"url":"http://arxiv.org/abs/2505.04272v1"}
{"created":"2025-05-07 09:20:03","title":"Accelerating Triangle Counting with Real Processing-in-Memory Systems","abstract":"Triangle Counting (TC) is a procedure that involves enumerating the number of triangles within a graph. It has important applications in numerous fields, such as social or biological network analysis and network security. TC is a memory-bound workload that does not scale efficiently in conventional processor-centric systems due to several memory accesses across large memory regions and low data reuse. However, recent Processing-in-Memory (PIM) architectures present a promising solution to alleviate these bottlenecks. Our work presents the first TC algorithm that leverages the capabilities of the UPMEM system, the first commercially available PIM architecture, while at the same time addressing its limitations. We use a vertex coloring technique to avoid expensive communication between PIM cores and employ reservoir sampling to address the limited amount of memory available in the PIM cores' DRAM banks. In addition, our work makes use of the Misra-Gries summary to speed up counting triangles on graphs with high-degree nodes and uniform sampling of the graph edges for quicker approximate results. Our PIM implementation surpasses state-of-the-art CPU-based TC implementations when processing dynamic graphs in Coordinate List format, showcasing the effectiveness of the UPMEM architecture in addressing TC's memory-bound challenges.","sentences":["Triangle Counting (TC) is a procedure that involves enumerating the number of triangles within a graph.","It has important applications in numerous fields, such as social or biological network analysis and network security.","TC is a memory-bound workload that does not scale efficiently in conventional processor-centric systems due to several memory accesses across large memory regions and low data reuse.","However, recent Processing-in-Memory (PIM) architectures present a promising solution to alleviate these bottlenecks.","Our work presents the first TC algorithm that leverages the capabilities of the UPMEM system, the first commercially available PIM architecture, while at the same time addressing its limitations.","We use a vertex coloring technique to avoid expensive communication between PIM cores and employ reservoir sampling to address the limited amount of memory available in the PIM cores' DRAM banks.","In addition, our work makes use of the Misra-Gries summary to speed up counting triangles on graphs with high-degree nodes and uniform sampling of the graph edges for quicker approximate results.","Our PIM implementation surpasses state-of-the-art CPU-based TC implementations when processing dynamic graphs in Coordinate List format, showcasing the effectiveness of the UPMEM architecture in addressing TC's memory-bound challenges."],"url":"http://arxiv.org/abs/2505.04269v1"}
{"created":"2025-05-07 08:45:44","title":"Technology prediction of a 3D model using Neural Network","abstract":"Accurate estimation of production times is critical for effective manufacturing scheduling, yet traditional methods relying on expert analysis or historical data often fall short in dynamic or customized production environments. This paper introduces a data-driven approach that predicts manufacturing steps and their durations directly from a product's 3D model. By rendering the model into multiple 2D images and leveraging a neural network inspired by the Generative Query Network, the method learns to map geometric features into time estimates for predefined production steps enabling scalable, adaptive, and precise process planning across varied product types.","sentences":["Accurate estimation of production times is critical for effective manufacturing scheduling, yet traditional methods relying on expert analysis or historical data often fall short in dynamic or customized production environments.","This paper introduces a data-driven approach that predicts manufacturing steps and their durations directly from a product's 3D model.","By rendering the model into multiple 2D images and leveraging a neural network inspired by the Generative Query Network, the method learns to map geometric features into time estimates for predefined production steps enabling scalable, adaptive, and precise process planning across varied product types."],"url":"http://arxiv.org/abs/2505.04241v1"}
{"created":"2025-05-07 08:27:18","title":"A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation","abstract":"The scarcity and high cost of labeled high-resolution imagery have long challenged remote sensing applications, particularly in low-income regions where high-resolution data are scarce. In this study, we propose a weak supervision framework that estimates parking lot occupancy using 3m resolution satellite imagery. By leveraging coarse temporal labels -- based on the assumption that parking lots of major supermarkets and hardware stores in Germany are typically full on Saturdays and empty on Sundays -- we train a pairwise comparison model that achieves an AUC of 0.92 on large parking lots. The proposed approach minimizes the reliance on expensive high-resolution images and holds promise for scalable urban mobility analysis. Moreover, the method can be adapted to assess transit patterns and resource allocation in vulnerable communities, providing a data-driven basis to improve the well-being of those most in need.","sentences":["The scarcity and high cost of labeled high-resolution imagery have long challenged remote sensing applications, particularly in low-income regions where high-resolution data are scarce.","In this study, we propose a weak supervision framework that estimates parking lot occupancy using 3m resolution satellite imagery.","By leveraging coarse temporal labels -- based on the assumption that parking lots of major supermarkets and hardware stores in Germany are typically full on Saturdays and empty on Sundays -- we train a pairwise comparison model that achieves an AUC of 0.92 on large parking lots.","The proposed approach minimizes the reliance on expensive high-resolution images and holds promise for scalable urban mobility analysis.","Moreover, the method can be adapted to assess transit patterns and resource allocation in vulnerable communities, providing a data-driven basis to improve the well-being of those most in need."],"url":"http://arxiv.org/abs/2505.04229v1"}
{"created":"2025-05-07 08:20:23","title":"FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning","abstract":"Federated learning (FL) enables collaborative model training across distributed clients while preserving data locality. Although FedAvg pioneered synchronous rounds for global model averaging, slower devices can delay collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by continuously integrating client updates, yet naive implementations risk client drift due to non-IID data and stale contributions. Some Blockchain-based FL approaches (e.g., BRAIN) employ robust weighting or scoring of updates to resist malicious or misaligned proposals. However, performance drops can still persist under severe data heterogeneity or high staleness, and synchronization overhead has emerged as a new concern due to its aggregator-free architectures.   We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL method that mitigates these limitations by incorporating two key ideas. First, our FastSync strategy eliminates the need to replay past model versions, enabling newcomers and infrequent participants to efficiently approximate the global model. Second, we adopt spherical linear interpolation (SLERP) when merging parameters, preserving models' directions and alleviating destructive interference from divergent local training.   Experiments with a CNN image-classification model and a Transformer-based language model demonstrate that FRAIN achieves more stable and robust convergence than FedAvg, FedAsync, and BRAIN, especially under harsh environments: non-IID data distributions, networks that experience delays and require frequent re-synchronization, and the presence of malicious nodes.","sentences":["Federated learning (FL) enables collaborative model training across distributed clients while preserving data locality.","Although FedAvg pioneered synchronous rounds for global model averaging, slower devices can delay collective progress.","Asynchronous FL (e.g., FedAsync) addresses stragglers by continuously integrating client updates, yet naive implementations risk client drift due to non-IID data and stale contributions.","Some Blockchain-based FL approaches (e.g., BRAIN) employ robust weighting or scoring of updates to resist malicious or misaligned proposals.","However, performance drops can still persist under severe data heterogeneity or high staleness, and synchronization overhead has emerged as a new concern due to its aggregator-free architectures.   ","We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL method that mitigates these limitations by incorporating two key ideas.","First, our FastSync strategy eliminates the need to replay past model versions, enabling newcomers and infrequent participants to efficiently approximate the global model.","Second, we adopt spherical linear interpolation (SLERP) when merging parameters, preserving models' directions and alleviating destructive interference from divergent local training.   ","Experiments with a CNN image-classification model and a Transformer-based language model demonstrate that FRAIN achieves more stable and robust convergence than FedAvg, FedAsync, and BRAIN, especially under harsh environments: non-IID data distributions, networks that experience delays and require frequent re-synchronization, and the presence of malicious nodes."],"url":"http://arxiv.org/abs/2505.04223v1"}
{"created":"2025-05-07 08:09:16","title":"Random walks with resetting on hypergraph","abstract":"Hypergraph has been selected as a powerful candidate for characterizing higher-order networks and has received   increasing attention in recent years. In this article, we study random walks with resetting on hypergraph by utilizing   spectral theory. Specifically, we derive exact expressions for some fundamental yet key parameters, including occupation   probability, stationary distribution, and mean first passage time, all of which are expressed in terms of the eigenvalues   and eigenvectors of the transition matrix. Furthermore, we provide a general condition for determining the optimal   reset probability and a sufficient condition for its existence. In addition, we build up a close relationship between   random walks with resetting on hypergraph and simple random walks. Concretely, the eigenvalues and eigenvectors   of the former can be precisely represented by those of the latter. More importantly, when considering random walks,   we abandon the traditional approach of converting hypergraph into a graph and propose a research framework that   preserves the intrinsic structure of hypergraph itself, which is based on assigning proper weights to neighboring nodes.   Through extensive experiments, we show that the new framework produces distinct and more reliable results than   the traditional approach in node ranking. Finally, we explore the impact of the resetting mechanism on cover time,   providing a potential solution for optimizing search efficiency.","sentences":["Hypergraph has been selected as a powerful candidate for characterizing higher-order networks and has received   increasing attention in recent years.","In this article, we study random walks with resetting on hypergraph by utilizing   spectral theory.","Specifically, we derive exact expressions for some fundamental yet key parameters, including occupation   probability, stationary distribution, and mean first passage time, all of which are expressed in terms of the eigenvalues   and eigenvectors of the transition matrix.","Furthermore, we provide a general condition for determining the optimal   reset probability and a sufficient condition for its existence.","In addition, we build up a close relationship between   random walks with resetting on hypergraph and simple random walks.","Concretely, the eigenvalues and eigenvectors   of the former can be precisely represented by those of the latter.","More importantly, when considering random walks,   we abandon the traditional approach of converting hypergraph into a graph and propose a research framework that   preserves the intrinsic structure of hypergraph itself, which is based on assigning proper weights to neighboring nodes.   ","Through extensive experiments, we show that the new framework produces distinct and more reliable results than   the traditional approach in node ranking.","Finally, we explore the impact of the resetting mechanism on cover time,   providing a potential solution for optimizing search efficiency."],"url":"http://arxiv.org/abs/2505.04215v1"}
{"created":"2025-05-07 08:08:58","title":"CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models","abstract":"The automatic extraction of key-value information from handwritten documents is a key challenge in document analysis. A reliable extraction is a prerequisite for the mass digitization efforts of many archives. Large Vision Language Models (LVLM) are a promising technology to tackle this problem especially in scenarios where little annotated training data is available. In this work, we present a novel dataset specifically designed to evaluate the few-shot capabilities of LVLMs. The CM1 documents are a historic collection of forms with handwritten entries created in Europe to administer the Care and Maintenance program after World War Two. The dataset establishes three benchmarks on extracting name and birthdate information and, furthermore, considers different training set sizes. We provide baseline results for two different LVLMs and compare performances to an established full-page extraction model. While the traditional full-page model achieves highly competitive performances, our experiments show that when only a few training samples are available the considered LVLMs benefit from their size and heavy pretraining and outperform the classical approach.","sentences":["The automatic extraction of key-value information from handwritten documents is a key challenge in document analysis.","A reliable extraction is a prerequisite for the mass digitization efforts of many archives.","Large Vision Language Models (LVLM) are a promising technology to tackle this problem especially in scenarios where little annotated training data is available.","In this work, we present a novel dataset specifically designed to evaluate the few-shot capabilities of LVLMs.","The CM1 documents are a historic collection of forms with handwritten entries created in Europe to administer the Care and Maintenance program after World War Two.","The dataset establishes three benchmarks on extracting name and birthdate information and, furthermore, considers different training set sizes.","We provide baseline results for two different LVLMs and compare performances to an established full-page extraction model.","While the traditional full-page model achieves highly competitive performances, our experiments show that when only a few training samples are available the considered LVLMs benefit from their size and heavy pretraining and outperform the classical approach."],"url":"http://arxiv.org/abs/2505.04214v1"}
{"created":"2025-05-07 07:58:57","title":"An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement","abstract":"Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired.","sentences":["Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems.","Therefore, early and accurate detection of potholes is crucial.","Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes.","In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis.","The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images.","The data was labeled in YOLO format suitable for segmentation.","A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU).","The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy.","The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50.","With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively.","Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved.","The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity.","In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired."],"url":"http://arxiv.org/abs/2505.04207v1"}
{"created":"2025-05-07 07:57:33","title":"Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets","abstract":"Cybersecurity has become essential worldwide and at all levels, concerning individuals, institutions, and governments. A basic principle in cybersecurity is to be always alert. Therefore, automation is imperative in processes where the volume of daily operations is large. Several cybersecurity applications can be addressed as binary classification problems, including anomaly detection, fraud detection, intrusion detection, spam detection, or malware detection. We present three experiments. In the first experiment, we evaluate single classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting Decision Tree. In the second experiment, we test different sampling techniques including over-sampling, under-sampling, Synthetic Minority Over-sampling Technique, and Self-Paced Ensembling. In the last experiment, we evaluate Self-Paced Ensembling and its number of base classifiers. We found that imbalance learning techniques had positive and negative effects, as reported in related studies. Thus, these techniques should be applied with caution. Besides, we found different best performers for each dataset. Therefore, we recommend testing single classifiers and imbalance learning techniques for each new dataset and application involving imbalanced datasets as is the case in several cyber security applications.","sentences":["Cybersecurity has become essential worldwide and at all levels, concerning individuals, institutions, and governments.","A basic principle in cybersecurity is to be always alert.","Therefore, automation is imperative in processes where the volume of daily operations is large.","Several cybersecurity applications can be addressed as binary classification problems, including anomaly detection, fraud detection, intrusion detection, spam detection, or malware detection.","We present three experiments.","In the first experiment, we evaluate single classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting Decision Tree.","In the second experiment, we test different sampling techniques including over-sampling, under-sampling, Synthetic Minority Over-sampling Technique, and Self-Paced Ensembling.","In the last experiment, we evaluate Self-Paced Ensembling and its number of base classifiers.","We found that imbalance learning techniques had positive and negative effects, as reported in related studies.","Thus, these techniques should be applied with caution.","Besides, we found different best performers for each dataset.","Therefore, we recommend testing single classifiers and imbalance learning techniques for each new dataset and application involving imbalanced datasets as is the case in several cyber security applications."],"url":"http://arxiv.org/abs/2505.04204v1"}
{"created":"2025-05-07 07:55:35","title":"SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios","abstract":"This paper explores the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly in enabling commonsense reasoning about the open-ended physical world. We identify two key challenges: modality discrepancy, where existing large touch-language models often treat touch as a mere sub-modality of language, and open-ended tactile data scarcity, where current datasets lack the diversity, open-endness and complexity needed for reasoning. To overcome these challenges, we introduce SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of Experts (MoE) to dynamically process, unify, and manage tactile and language modalities, capturing their unique characteristics. Crucially, we also present a comprehensive tactile commonsense reasoning dataset and benchmark featuring free-form questions and responses, 8 physical properties, 4 interactive characteristics, and diverse commonsense knowledge. Experiments show SToLa exhibits competitive performance compared to existing models on the PhysiCLeAR benchmark and self-constructed datasets, proving the effectiveness of the Mixture of Experts architecture in multimodal management and the performance advantages for open-scenario tactile commonsense reasoning tasks.","sentences":["This paper explores the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly in enabling commonsense reasoning about the open-ended physical world.","We identify two key challenges: modality discrepancy, where existing large touch-language models often treat touch as a mere sub-modality of language, and open-ended tactile data scarcity, where current datasets lack the diversity, open-endness and complexity needed for reasoning.","To overcome these challenges, we introduce SToLa, a Self-Adaptive Touch-Language framework.","SToLa utilizes Mixture of Experts (MoE) to dynamically process, unify, and manage tactile and language modalities, capturing their unique characteristics.","Crucially, we also present a comprehensive tactile commonsense reasoning dataset and benchmark featuring free-form questions and responses, 8 physical properties, 4 interactive characteristics, and diverse commonsense knowledge.","Experiments show SToLa exhibits competitive performance compared to existing models on the PhysiCLeAR benchmark and self-constructed datasets, proving the effectiveness of the Mixture of Experts architecture in multimodal management and the performance advantages for open-scenario tactile commonsense reasoning tasks."],"url":"http://arxiv.org/abs/2505.04201v1"}
{"created":"2025-05-07 07:54:33","title":"Estimating Causal Effects in Networks with Cluster-Based Bandits","abstract":"The gold standard for estimating causal effects is randomized controlled trial (RCT) or A/B testing where a random group of individuals from a population of interest are given treatment and the outcome is compared to a random group of individuals from the same population. However, A/B testing is challenging in the presence of interference, commonly occurring in social networks, where individuals can impact each others outcome. Moreover, A/B testing can incur a high performance loss when one of the treatment arms has a poor performance and the test continues to treat individuals with it. Therefore, it is important to design a strategy that can adapt over time and efficiently learn the total treatment effect in the network. We introduce two cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the total treatment effect in a network while maximizing the expected reward by making a tradeoff between exploration and exploitation. We compare the performance of our MAB algorithms with a vanilla MAB algorithm that ignores clusters and the corresponding RCT methods on semi-synthetic data with simulated interference. The vanilla MAB algorithm shows higher reward-action ratio at the cost of higher treatment effect error due to undesired spillover. The cluster-based MAB algorithms show higher reward-action ratio compared to their corresponding RCT methods without sacrificing much accuracy in treatment effect estimation.","sentences":["The gold standard for estimating causal effects is randomized controlled trial (RCT) or A/B testing where a random group of individuals from a population of interest are given treatment and the outcome is compared to a random group of individuals from the same population.","However, A/B testing is challenging in the presence of interference, commonly occurring in social networks, where individuals can impact each others outcome.","Moreover, A/B testing can incur a high performance loss when one of the treatment arms has a poor performance and the test continues to treat individuals with it.","Therefore, it is important to design a strategy that can adapt over time and efficiently learn the total treatment effect in the network.","We introduce two cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the total treatment effect in a network while maximizing the expected reward by making a tradeoff between exploration and exploitation.","We compare the performance of our MAB algorithms with a vanilla MAB algorithm that ignores clusters and the corresponding RCT methods on semi-synthetic data with simulated interference.","The vanilla MAB algorithm shows higher reward-action ratio at the cost of higher treatment effect error due to undesired spillover.","The cluster-based MAB algorithms show higher reward-action ratio compared to their corresponding RCT methods without sacrificing much accuracy in treatment effect estimation."],"url":"http://arxiv.org/abs/2505.04200v1"}
{"created":"2025-05-07 07:41:29","title":"Trajectory Entropy Reinforcement Learning for Predictable and Robust Control","abstract":"Simplicity is a critical inductive bias for designing data-driven controllers, especially when robustness is important. Despite the impressive results of deep reinforcement learning in complex control tasks, it is prone to capturing intricate and spurious correlations between observations and actions, leading to failure under slight perturbations to the environment. To tackle this problem, in this work we introduce a novel inductive bias towards simple policies in reinforcement learning. The simplicity inductive bias is introduced by minimizing the entropy of entire action trajectories, corresponding to the number of bits required to describe information in action trajectories after the agent observes state trajectories. Our reinforcement learning agent, Trajectory Entropy Reinforcement Learning, is optimized to minimize the trajectory entropy while maximizing rewards. We show that the trajectory entropy can be effectively estimated by learning a variational parameterized action prediction model, and use the prediction model to construct an information-regularized reward function. Furthermore, we construct a practical algorithm that enables the joint optimization of models, including the policy and the prediction model. Experimental evaluations on several high-dimensional locomotion tasks show that our learned policies produce more cyclical and consistent action trajectories, and achieve superior performance, and robustness to noise and dynamic changes than the state-of-the-art.","sentences":["Simplicity is a critical inductive bias for designing data-driven controllers, especially when robustness is important.","Despite the impressive results of deep reinforcement learning in complex control tasks, it is prone to capturing intricate and spurious correlations between observations and actions, leading to failure under slight perturbations to the environment.","To tackle this problem, in this work we introduce a novel inductive bias towards simple policies in reinforcement learning.","The simplicity inductive bias is introduced by minimizing the entropy of entire action trajectories, corresponding to the number of bits required to describe information in action trajectories after the agent observes state trajectories.","Our reinforcement learning agent, Trajectory Entropy Reinforcement Learning, is optimized to minimize the trajectory entropy while maximizing rewards.","We show that the trajectory entropy can be effectively estimated by learning a variational parameterized action prediction model, and use the prediction model to construct an information-regularized reward function.","Furthermore, we construct a practical algorithm that enables the joint optimization of models, including the policy and the prediction model.","Experimental evaluations on several high-dimensional locomotion tasks show that our learned policies produce more cyclical and consistent action trajectories, and achieve superior performance, and robustness to noise and dynamic changes than the state-of-the-art."],"url":"http://arxiv.org/abs/2505.04193v1"}
{"created":"2025-05-07 07:41:19","title":"VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning","abstract":"We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.   Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube. Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning. Our code, data, and model are publicly available at https://github.com/trinhvg/VideoPath-LLaVA.","sentences":["We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists.","By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.   ","Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube.","Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume.","To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos.","VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning.","Our code, data, and model are publicly available at https://github.com/trinhvg/VideoPath-LLaVA."],"url":"http://arxiv.org/abs/2505.04192v1"}
{"created":"2025-05-07 07:34:37","title":"S3D: Sketch-Driven 3D Model Generation","abstract":"Generating high-quality 3D models from 2D sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data. In this paper, we present S3D, a novel framework that converts simple hand-drawn sketches into detailed 3D models. Our method utilizes a U-Net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3D representation that can be rendered from novel views. To ensure robust consistency between the sketch domain and the 3D output, we introduce a novel style-alignment loss that aligns the U-Net bottleneck features with the initial encoder outputs of the 3D generation module, significantly enhancing reconstruction fidelity. To further enhance the network's robustness, we apply augmentation techniques to the sketch dataset. This streamlined framework demonstrates the effectiveness of S3D in generating high-quality 3D models from sketch inputs. The source code for this project is publicly available at https://github.com/hailsong/S3D.","sentences":["Generating high-quality 3D models from 2D sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data.","In this paper, we present S3D, a novel framework that converts simple hand-drawn sketches into detailed 3D models.","Our method utilizes a U-Net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3D representation that can be rendered from novel views.","To ensure robust consistency between the sketch domain and the 3D output, we introduce a novel style-alignment loss that aligns the U-Net bottleneck features with the initial encoder outputs of the 3D generation module, significantly enhancing reconstruction fidelity.","To further enhance the network's robustness, we apply augmentation techniques to the sketch dataset.","This streamlined framework demonstrates the effectiveness of S3D in generating high-quality 3D models from sketch inputs.","The source code for this project is publicly available at https://github.com/hailsong/S3D."],"url":"http://arxiv.org/abs/2505.04185v1"}
{"created":"2025-05-07 07:28:03","title":"Privacy Challenges In Image Processing Applications","abstract":"As image processing systems proliferate, privacy concerns intensify given the sensitive personal information contained in images. This paper examines privacy challenges in image processing and surveys emerging privacy-preserving techniques including differential privacy, secure multiparty computation, homomorphic encryption, and anonymization. Key applications with heightened privacy risks include healthcare, where medical images contain patient health data, and surveillance systems that can enable unwarranted tracking. Differential privacy offers rigorous privacy guarantees by injecting controlled noise, while MPC facilitates collaborative analytics without exposing raw data inputs. Homomorphic encryption enables computations on encrypted data and anonymization directly removes identifying elements. However, balancing privacy protections and utility remains an open challenge. Promising future directions identified include quantum-resilient cryptography, federated learning, dedicated hardware, and conceptual innovations like privacy by design. Ultimately, a holistic effort combining technological innovations, ethical considerations, and policy frameworks is necessary to uphold the fundamental right to privacy as image processing capabilities continue advancing rapidly.","sentences":["As image processing systems proliferate, privacy concerns intensify given the sensitive personal information contained in images.","This paper examines privacy challenges in image processing and surveys emerging privacy-preserving techniques including differential privacy, secure multiparty computation, homomorphic encryption, and anonymization.","Key applications with heightened privacy risks include healthcare, where medical images contain patient health data, and surveillance systems that can enable unwarranted tracking.","Differential privacy offers rigorous privacy guarantees by injecting controlled noise, while MPC facilitates collaborative analytics without exposing raw data inputs.","Homomorphic encryption enables computations on encrypted data and anonymization directly removes identifying elements.","However, balancing privacy protections and utility remains an open challenge.","Promising future directions identified include quantum-resilient cryptography, federated learning, dedicated hardware, and conceptual innovations like privacy by design.","Ultimately, a holistic effort combining technological innovations, ethical considerations, and policy frameworks is necessary to uphold the fundamental right to privacy as image processing capabilities continue advancing rapidly."],"url":"http://arxiv.org/abs/2505.04181v1"}
{"created":"2025-05-07 06:41:33","title":"STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting","abstract":"Irregular multivariate time series (IMTS) are prevalent in real-world applications across many fields, where varying sensor frequencies and asynchronous measurements pose significant modeling challenges. Existing solutions often rely on a pre-alignment strategy to normalize data, which can distort intrinsic patterns and escalate computational and memory demands. Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational Graph Convolutional Network that avoids pre-alignment and directly captures the complex interdependencies in IMTS by representing them as a fully connected graph. Each observation is represented as a node, allowing the model to effectively handle misaligned timestamps by mapping all inter-node relationships, thus faithfully preserving the asynchronous nature of the data. Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that strategically aggregates nodes to optimize graph embeddings, reducing computational overhead while maintaining detailed local and global context. Extensive experiments on four public datasets demonstrate that STRGCN achieves state-of-the-art accuracy, competitive memory usage and training speed.","sentences":["Irregular multivariate time series (IMTS) are prevalent in real-world applications across many fields, where varying sensor frequencies and asynchronous measurements pose significant modeling challenges.","Existing solutions often rely on a pre-alignment strategy to normalize data, which can distort intrinsic patterns and escalate computational and memory demands.","Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational Graph Convolutional Network that avoids pre-alignment and directly captures the complex interdependencies in IMTS by representing them as a fully connected graph.","Each observation is represented as a node, allowing the model to effectively handle misaligned timestamps by mapping all inter-node relationships, thus faithfully preserving the asynchronous nature of the data.","Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that strategically aggregates nodes to optimize graph embeddings, reducing computational overhead while maintaining detailed local and global context.","Extensive experiments on four public datasets demonstrate that STRGCN achieves state-of-the-art accuracy, competitive memory usage and training speed."],"url":"http://arxiv.org/abs/2505.04167v1"}
{"created":"2025-05-07 06:26:11","title":"Retrieval Augmented Time Series Forecasting","abstract":"Time series forecasting uses historical data to predict future trends, leveraging the relationships between past observations and available features. In this paper, we propose RAFT, a retrieval-augmented time series forecasting method to provide sufficient inductive biases and complement the model's learning capacity. When forecasting the subsequent time frames, we directly retrieve historical data candidates from the training dataset with patterns most similar to the input, and utilize the future values of these candidates alongside the inputs to obtain predictions. This simple approach augments the model's capacity by externally providing information about past patterns via retrieval modules. Our empirical evaluations on ten benchmark datasets show that RAFT consistently outperforms contemporary baselines with an average win ratio of 86%.","sentences":["Time series forecasting uses historical data to predict future trends, leveraging the relationships between past observations and available features.","In this paper, we propose RAFT, a retrieval-augmented time series forecasting method to provide sufficient inductive biases and complement the model's learning capacity.","When forecasting the subsequent time frames, we directly retrieve historical data candidates from the training dataset with patterns most similar to the input, and utilize the future values of these candidates alongside the inputs to obtain predictions.","This simple approach augments the model's capacity by externally providing information about past patterns via retrieval modules.","Our empirical evaluations on ten benchmark datasets show that RAFT consistently outperforms contemporary baselines with an average win ratio of 86%."],"url":"http://arxiv.org/abs/2505.04163v1"}
{"created":"2025-05-07 06:23:26","title":"Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data","abstract":"Globally, the outbreaks of infectious diseases have exerted an extremely profound and severe influence on health security and the economy. During the critical phases of epidemics, devising effective intervention measures poses a significant challenge to both the academic and practical arenas. There is numerous research based on reinforcement learning to optimize intervention measures of infectious diseases. Nevertheless, most of these efforts have been confined within the differential equation based on infectious disease models. Although a limited number of studies have incorporated reinforcement learning methodologies into individual-based infectious disease models, the models employed therein have entailed simplifications and limitations, rendering it incapable of modeling the complexity and dynamics inherent in infectious disease transmission. We establish a decision-making framework based on an individual agent-based transmission model, utilizing reinforcement learning to continuously explore and develop a strategy function. The framework's validity is verified through both experimental and theoretical approaches. Covasim, a detailed and widely used agent-based disease transmission model, was modified to support reinforcement learning research. We conduct an exhaustive exploration of the application efficacy of multiple algorithms across diverse action spaces. Furthermore, we conduct an innovative preliminary theoretical analysis concerning the issue of \"time coverage\". The results of the experiment robustly validate the effectiveness and feasibility of the methodological framework of this study. The coping strategies gleaned therefrom prove highly efficacious in suppressing the expansion of the epidemic scale and safeguarding the stability of the economic system, thereby providing crucial reference perspectives for the formulation of global public health security strategies.","sentences":["Globally, the outbreaks of infectious diseases have exerted an extremely profound and severe influence on health security and the economy.","During the critical phases of epidemics, devising effective intervention measures poses a significant challenge to both the academic and practical arenas.","There is numerous research based on reinforcement learning to optimize intervention measures of infectious diseases.","Nevertheless, most of these efforts have been confined within the differential equation based on infectious disease models.","Although a limited number of studies have incorporated reinforcement learning methodologies into individual-based infectious disease models, the models employed therein have entailed simplifications and limitations, rendering it incapable of modeling the complexity and dynamics inherent in infectious disease transmission.","We establish a decision-making framework based on an individual agent-based transmission model, utilizing reinforcement learning to continuously explore and develop a strategy function.","The framework's validity is verified through both experimental and theoretical approaches.","Covasim, a detailed and widely used agent-based disease transmission model, was modified to support reinforcement learning research.","We conduct an exhaustive exploration of the application efficacy of multiple algorithms across diverse action spaces.","Furthermore, we conduct an innovative preliminary theoretical analysis concerning the issue of \"time coverage\".","The results of the experiment robustly validate the effectiveness and feasibility of the methodological framework of this study.","The coping strategies gleaned therefrom prove highly efficacious in suppressing the expansion of the epidemic scale and safeguarding the stability of the economic system, thereby providing crucial reference perspectives for the formulation of global public health security strategies."],"url":"http://arxiv.org/abs/2505.04161v1"}
{"created":"2025-05-07 06:06:46","title":"Global Hash Tables Strike Back! An Analysis of Parallel GROUP BY Aggregation","abstract":"Efficiently computing group aggregations (i.e., GROUP BY) on modern many-core architectures is critical for analytic database systems. Today's engines predominately use a partitioned approach to group aggregation, in which an incoming data stream is partitioned by key values so that every row for a particular key is sent to the same thread. In this paper, we revisit a simpler strategy: a fully concurrent group aggregation technique using a shared global hash table. While approaches using general-purpose concurrent hash tables have generally been found to perform worse than partitioning-based approaches, we argue that the key ingredient is customizing the concurrent hash table for the specific task of group aggregation. Through extensive experiments on synthetic workloads (varying key cardinality, skew, and thread counts), we demonstrate that a purpose-built concurrent hash table can match or surpass partitioning-based techniques. We also analyze the operational characteristics of both techniques, including resizing costs and memory pressure. In the process, we derive practical guidelines for database implementers. Overall, our analysis indicates that fully concurrent group aggregation is a viable alternative to partitioning.","sentences":["Efficiently computing group aggregations (i.e., GROUP BY) on modern many-core architectures is critical for analytic database systems.","Today's engines predominately use a partitioned approach to group aggregation, in which an incoming data stream is partitioned by key values so that every row for a particular key is sent to the same thread.","In this paper, we revisit a simpler strategy: a fully concurrent group aggregation technique using a shared global hash table.","While approaches using general-purpose concurrent hash tables have generally been found to perform worse than partitioning-based approaches, we argue that the key ingredient is customizing the concurrent hash table for the specific task of group aggregation.","Through extensive experiments on synthetic workloads (varying key cardinality, skew, and thread counts), we demonstrate that a purpose-built concurrent hash table can match or surpass partitioning-based techniques.","We also analyze the operational characteristics of both techniques, including resizing costs and memory pressure.","In the process, we derive practical guidelines for database implementers.","Overall, our analysis indicates that fully concurrent group aggregation is a viable alternative to partitioning."],"url":"http://arxiv.org/abs/2505.04153v1"}
{"created":"2025-05-07 06:02:27","title":"Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages","abstract":"Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions. However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations. OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class. Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.","sentences":["Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease.","The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers.","There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis.","Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions.","However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information.","To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations.","OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class.","Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages."],"url":"http://arxiv.org/abs/2505.04150v1"}
{"created":"2025-05-07 05:54:04","title":"Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety","abstract":"Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.   We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation. Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3. The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging. All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors.   Warning: This paper includes visual examples of adversarial inputs designed to test model safety. All outputs have been redacted to ensure responsible disclosure.","sentences":["Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks.","Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.   ","We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation.","Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3.","The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging.","All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers.","UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors.   ","Warning:","This paper includes visual examples of adversarial inputs designed to test model safety.","All outputs have been redacted to ensure responsible disclosure."],"url":"http://arxiv.org/abs/2505.04146v1"}
{"created":"2025-05-07 05:48:56","title":"Evaluating Performance Consistency in Competitive Programming: Educational Implications and Contest Design Insights","abstract":"Competitive programming (CP) contests are often treated as interchangeable proxies for algorithmic skill, yet the extent to which results at lower contest tiers anticipate performance at higher tiers, and how closely any tier resembles the ubiquitous online-contest circuit, remains unclear. We analyze ten years (2015--2024) of International Collegiate Programming Contest (ICPC) standings, comprising five long-running superregional championships (Africa \\& Arab, Asia East, Asia West, North America, and Northern Eurasia), associated local regionals of North America and Northern Eurasia, and the World Finals. For 366 World Finalist teams (2021--2024) we augment the dataset with pre-contest Codeforces ratings. Pairwise rank alignment is measured with Kendall's $\\tau$.   Overall, superregional ranks predict World Final ranks only moderately (weighted $\\tau=0.407$), but regional-to-superregional consistency varies widely: Northern Eurasia exhibits the strongest alignment ($\\tau=0.521$) while Asia West exhibits the weakest ($\\tau=0.188$). Internal consistency within a region can exceed its predictive value for Worlds -- e.g., Northern Eurasia and North America regionals vs. superregionals ($\\tau=0.666$ and $\\tau=0.577$, respectively). Codeforces ratings correlate more strongly with World Final results ($\\tau=0.596$) than any single ICPC tier, suggesting that high-frequency online contests capture decisive skill factors that many superregional sets miss.   We argue that contest organizers can improve both fairness and pedagogical value by aligning problem style and selection rules with the formats that demonstrably differentiate teams, in particular the Northern-Eurasian model and well-curated online rounds. All data, scripts, and additional analyses are publicly released to facilitate replication and further study.","sentences":["Competitive programming (CP) contests are often treated as interchangeable proxies for algorithmic skill, yet the extent to which results at lower contest tiers anticipate performance at higher tiers, and how closely any tier resembles the ubiquitous online-contest circuit, remains unclear.","We analyze ten years (2015--2024) of International Collegiate Programming Contest (ICPC) standings, comprising five long-running superregional championships (Africa \\& Arab, Asia East, Asia West, North America, and Northern Eurasia), associated local regionals of North America and Northern Eurasia, and the World Finals.","For 366 World Finalist teams (2021--2024) we augment the dataset with pre-contest Codeforces ratings.","Pairwise rank alignment is measured with Kendall's $\\tau$.   Overall, superregional ranks predict World Final ranks only moderately (weighted $\\tau=0.407$), but regional-to-superregional consistency varies widely: Northern Eurasia exhibits the strongest alignment ($\\tau=0.521$) while Asia West exhibits the weakest ($\\tau=0.188$).","Internal consistency within a region can exceed its predictive value for Worlds -- e.g., Northern Eurasia and North America regionals vs. superregionals ($\\tau=0.666$ and $\\tau=0.577$, respectively).","Codeforces ratings correlate more strongly with World Final results ($\\tau=0.596$) than any single ICPC tier, suggesting that high-frequency online contests capture decisive skill factors that many superregional sets miss.   ","We argue that contest organizers can improve both fairness and pedagogical value by aligning problem style and selection rules with the formats that demonstrably differentiate teams, in particular the Northern-Eurasian model and well-curated online rounds.","All data, scripts, and additional analyses are publicly released to facilitate replication and further study."],"url":"http://arxiv.org/abs/2505.04143v1"}
{"created":"2025-05-07 04:35:32","title":"A Framework to Prevent Biometric Data Leakage in the Immersive Technologies Domain","abstract":"Doubtlessly, the immersive technologies have potential to ease people's life and uplift economy, however the obvious data privacy risks cannot be ignored. For example, a participant wears a 3D headset device which detects participant's head motion to track the pose of participant's head to match the orientation of camera with participant's eyes positions in the real-world. In a preliminary study, researchers have proved that the voice command features on such headsets could lead to major privacy leakages. By analyzing the facial dynamics captured with the motion sensors, the headsets suffer security vulnerabilities revealing a user's sensitive speech without user's consent. The psychography data (such as voice command features, facial dynamics, etc.) is sensitive data and it should not be leaked out of the device without users consent else it is a privacy breach. To the best of our literature review, the work done in this particular research problem is very limited. Motivated from this, we develop a simple technical framework to mitigate sensitive data (or biometric data) privacy leaks in immersive technology domain. The performance evaluation is conducted in a robust way using six data sets, to show that the proposed solution is effective and feasible to prevent this issue.","sentences":["Doubtlessly, the immersive technologies have potential to ease people's life and uplift economy, however the obvious data privacy risks cannot be ignored.","For example, a participant wears a 3D headset device which detects participant's head motion to track the pose of participant's head to match the orientation of camera with participant's eyes positions in the real-world.","In a preliminary study, researchers have proved that the voice command features on such headsets could lead to major privacy leakages.","By analyzing the facial dynamics captured with the motion sensors, the headsets suffer security vulnerabilities revealing a user's sensitive speech without user's consent.","The psychography data (such as voice command features, facial dynamics, etc.) is sensitive data and it should not be leaked out of the device without users consent else it is a privacy breach.","To the best of our literature review, the work done in this particular research problem is very limited.","Motivated from this, we develop a simple technical framework to mitigate sensitive data (or biometric data) privacy leaks in immersive technology domain.","The performance evaluation is conducted in a robust way using six data sets, to show that the proposed solution is effective and feasible to prevent this issue."],"url":"http://arxiv.org/abs/2505.04123v1"}
{"created":"2025-05-07 04:29:09","title":"GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model","abstract":"Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.","sentences":["Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data.","However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive.","Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds.","To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models.","First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details.","Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level.","Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics.","Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters.","Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP."],"url":"http://arxiv.org/abs/2505.04119v1"}
{"created":"2025-05-07 04:04:31","title":"Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment","abstract":"Modern zero-shot text-to-speech (TTS) systems, despite using extensive pre-training, often struggle in challenging scenarios such as tongue twisters, repeated words, code-switching, and cross-lingual synthesis, leading to intelligibility issues. To address these limitations, this paper leverages preference alignment techniques, which enable targeted construction of out-of-pretraining-distribution data to enhance performance. We introduce a new dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend the Direct Preference Optimization (DPO) framework to accommodate diverse TTS architectures. After INTP alignment, in addition to intelligibility, we observe overall improvements including naturalness, similarity, and audio quality for multiple TTS models across diverse domains. Based on that, we also verify the weak-to-strong generalization ability of INTP for more intelligible models such as CosyVoice 2 and Ints. Moreover, we showcase the potential for further improvements through iterative alignment based on Ints. Audio samples are available at https://intalign.github.io/.","sentences":["Modern zero-shot text-to-speech (TTS) systems, despite using extensive pre-training, often struggle in challenging scenarios such as tongue twisters, repeated words, code-switching, and cross-lingual synthesis, leading to intelligibility issues.","To address these limitations, this paper leverages preference alignment techniques, which enable targeted construction of out-of-pretraining-distribution data to enhance performance.","We introduce a new dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend the Direct Preference Optimization (DPO) framework to accommodate diverse TTS architectures.","After INTP alignment, in addition to intelligibility, we observe overall improvements including naturalness, similarity, and audio quality for multiple TTS models across diverse domains.","Based on that, we also verify the weak-to-strong generalization ability of INTP for more intelligible models such as CosyVoice 2 and Ints.","Moreover, we showcase the potential for further improvements through iterative alignment based on Ints.","Audio samples are available at https://intalign.github.io/."],"url":"http://arxiv.org/abs/2505.04113v1"}
{"created":"2025-05-07 03:54:59","title":"One2Any: One-Reference 6D Pose Estimation for Any Object","abstract":"6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories. These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available. To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints. We treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object shape, orientation, and texture from a single reference view. Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation. This simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute.","sentences":["6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories.","These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available.","To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints.","We treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object shape, orientation, and texture from a single reference view.","Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation.","This simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability.","Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute."],"url":"http://arxiv.org/abs/2505.04109v1"}
{"created":"2025-05-07 03:51:02","title":"In-Situ Hardware Error Detection Using Specification-Derived Petri Net Models and Behavior-Derived State Sequences","abstract":"In hardware accelerators used in data centers and safety-critical applications, soft errors and resultant silent data corruption significantly compromise reliability, particularly when upsets occur in control-flow operations, leading to severe failures. To address this, we introduce two methods for monitoring control flows: using specification-derived Petri nets and using behavior-derived state transitions. We validated our method across four designs: convolutional layer operation, Gaussian blur, AES encryption, and a router in Network-on-Chip. Our fault injection campaign targeting the control registers and primary control inputs demonstrated high error detection rates in both datapath and control logic. Synthesis results show that a maximum detection rate is achieved with a few to around 10% area overhead in most cases. The proposed detectors quickly detect 48% to 100% of failures resulting from upsets in internal control registers and perturbations in primary control inputs. The two proposed methods were compared in terms of area overhead and error detection rate. By selectively applying these two methods, a wide range of area constraints can be accommodated, enabling practical implementation and effectively enhancing error detection capabilities.","sentences":["In hardware accelerators used in data centers and safety-critical applications, soft errors and resultant silent data corruption significantly compromise reliability, particularly when upsets occur in control-flow operations, leading to severe failures.","To address this, we introduce two methods for monitoring control flows: using specification-derived Petri nets and using behavior-derived state transitions.","We validated our method across four designs: convolutional layer operation, Gaussian blur, AES encryption, and a router in Network-on-Chip.","Our fault injection campaign targeting the control registers and primary control inputs demonstrated high error detection rates in both datapath and control logic.","Synthesis results show that a maximum detection rate is achieved with a few to around 10% area overhead in most cases.","The proposed detectors quickly detect 48% to 100% of failures resulting from upsets in internal control registers and perturbations in primary control inputs.","The two proposed methods were compared in terms of area overhead and error detection rate.","By selectively applying these two methods, a wide range of area constraints can be accommodated, enabling practical implementation and effectively enhancing error detection capabilities."],"url":"http://arxiv.org/abs/2505.04108v1"}
{"created":"2025-05-07 02:58:37","title":"SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation","abstract":"Test-Time adaptation (TTA) aims to enhance model robustness against distribution shifts through rapid model adaptation during inference. While existing TTA methods often rely on entropy-based unsupervised training and achieve promising results, the common practice of a single round of entropy training is typically unable to adequately utilize reliable samples, hindering adaptation efficiency. In this paper, we discover augmentation strategies can effectively unleash the potential of reliable samples, but the rapidly growing computational cost impedes their real-time application. To address this limitation, we propose a novel TTA approach named Single-step Ensemble of Vicinal Augmentations (SEVA), which can take advantage of data augmentations without increasing the computational burden. Specifically, instead of explicitly utilizing the augmentation strategy to generate new data, SEVA develops a theoretical framework to explore the impacts of multiple augmentations on model adaptation and proposes to optimize an upper bound of the entropy loss to integrate the effects of multiple rounds of augmentation training into a single step. Furthermore, we discover and verify that using the upper bound as the loss is more conducive to the selection mechanism, as it can effectively filter out harmful samples that confuse the model. Combining these two key advantages, the proposed efficient loss and a complementary selection strategy can simultaneously boost the potential of reliable samples and meet the stringent time requirements of TTA. The comprehensive experiments on various network architectures across challenging testing scenarios demonstrate impressive performances and the broad adaptability of SEVA. The code will be publicly available.","sentences":["Test-Time adaptation (TTA) aims to enhance model robustness against distribution shifts through rapid model adaptation during inference.","While existing TTA methods often rely on entropy-based unsupervised training and achieve promising results, the common practice of a single round of entropy training is typically unable to adequately utilize reliable samples, hindering adaptation efficiency.","In this paper, we discover augmentation strategies can effectively unleash the potential of reliable samples, but the rapidly growing computational cost impedes their real-time application.","To address this limitation, we propose a novel TTA approach named Single-step Ensemble of Vicinal Augmentations (SEVA), which can take advantage of data augmentations without increasing the computational burden.","Specifically, instead of explicitly utilizing the augmentation strategy to generate new data, SEVA develops a theoretical framework to explore the impacts of multiple augmentations on model adaptation and proposes to optimize an upper bound of the entropy loss to integrate the effects of multiple rounds of augmentation training into a single step.","Furthermore, we discover and verify that using the upper bound as the loss is more conducive to the selection mechanism, as it can effectively filter out harmful samples that confuse the model.","Combining these two key advantages, the proposed efficient loss and a complementary selection strategy can simultaneously boost the potential of reliable samples and meet the stringent time requirements of TTA.","The comprehensive experiments on various network architectures across challenging testing scenarios demonstrate impressive performances and the broad adaptability of SEVA.","The code will be publicly available."],"url":"http://arxiv.org/abs/2505.04087v1"}
{"created":"2025-05-07 02:49:52","title":"Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training","abstract":"Graph neural networks have emerged as a potent class of neural networks capable of leveraging the connectivity and structure of real-world graphs to learn intricate properties and relationships between nodes. Many real-world graphs exceed the memory capacity of a GPU due to their sheer size, and using GNNs on them requires techniques such as mini-batch sampling to scale. However, this can lead to reduced accuracy in some cases, and sampling and data transfer from the CPU to the GPU can also slow down training. On the other hand, distributed full-graph training suffers from high communication overhead and load imbalance due to the irregular structure of graphs. We propose Plexus, a three-dimensional (3D) parallel approach for full-graph training that tackles these issues and scales to billion-edge graphs. Additionally, we introduce optimizations such as a permutation scheme for load balancing, and a performance model to predict the optimal 3D configuration. We evaluate Plexus on several graph datasets and show scaling results for up to 2048 GPUs on Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus achieves unprecedented speedups of 2.3x-12.5x over existing methods and a reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on Frontier.","sentences":["Graph neural networks have emerged as a potent class of neural networks capable of leveraging the connectivity and structure of real-world graphs to learn intricate properties and relationships between nodes.","Many real-world graphs exceed the memory capacity of a GPU due to their sheer size, and using GNNs on them requires techniques such as mini-batch sampling to scale.","However, this can lead to reduced accuracy in some cases, and sampling and data transfer from the CPU to the GPU can also slow down training.","On the other hand, distributed full-graph training suffers from high communication overhead and load imbalance due to the irregular structure of graphs.","We propose Plexus, a three-dimensional (3D) parallel approach for full-graph training that tackles these issues and scales to billion-edge graphs.","Additionally, we introduce optimizations such as a permutation scheme for load balancing, and a performance model to predict the optimal 3D configuration.","We evaluate Plexus on several graph datasets and show scaling results for up to 2048 GPUs on Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier.","Plexus achieves unprecedented speedups of 2.3x-12.5x over existing methods and a reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on Frontier."],"url":"http://arxiv.org/abs/2505.04083v1"}
{"created":"2025-05-07 02:47:51","title":"QStore: Quantization-Aware Compressed Model Storage","abstract":"Modern applications commonly leverage large, multi-modal foundation models. These applications often feature complex workflows that demand the storage and usage of similar models in multiple precisions. A straightforward approach is to maintain a separate file for each model precision (e.g., INT8, BF16), which is indeed the approach taken by many model providers such as HuggingFace and Ollama. However, this approach incurs excessive storage costs since a higher precision model (e.g., BF16) is a strict superset of a lower precision model (e.g., INT8) in terms of information. Unfortunately, simply maintaining only the higher-precision model and requiring every user to dynamically convert the model precision is not desirable because every user of lower precision models must pay the cost for model download and precision conversion.   In this paper, we present QStore, a unified, lossless compression format for simultaneously storing a model in two (high and low) precisions efficiently. Instead of storing low-precision and high-precision models separately, QStore stores low-precision model and only the residual information needed to reconstruct high-precision models. The size of residual information is significantly smaller than the original high-precision models, thus achieving high savings in storage cost. Moreover, QStore does not compromise the speed of model loading. The low-precision models can be loaded quickly just like before. The high-precision models can also be reconstructed efficiently in memory by merging low-precision data and the residual with QStore's lightweight decoding logic. We evaluate QStore for compressing multiple precisions of popular foundation models, and show that QStore reduces overall storage footprint by up to 2.2x (45% of the original size) while enabling up to 1.7x and 1.8x faster model saving and loading versus existing approaches.","sentences":["Modern applications commonly leverage large, multi-modal foundation models.","These applications often feature complex workflows that demand the storage and usage of similar models in multiple precisions.","A straightforward approach is to maintain a separate file for each model precision (e.g., INT8, BF16), which is indeed the approach taken by many model providers such as HuggingFace and Ollama.","However, this approach incurs excessive storage costs since a higher precision model (e.g., BF16) is a strict superset of a lower precision model (e.g., INT8) in terms of information.","Unfortunately, simply maintaining only the higher-precision model and requiring every user to dynamically convert the model precision is not desirable because every user of lower precision models must pay the cost for model download and precision conversion.   ","In this paper, we present QStore, a unified, lossless compression format for simultaneously storing a model in two (high and low) precisions efficiently.","Instead of storing low-precision and high-precision models separately, QStore stores low-precision model and only the residual information needed to reconstruct high-precision models.","The size of residual information is significantly smaller than the original high-precision models, thus achieving high savings in storage cost.","Moreover, QStore does not compromise the speed of model loading.","The low-precision models can be loaded quickly just like before.","The high-precision models can also be reconstructed efficiently in memory by merging low-precision data and the residual with QStore's lightweight decoding logic.","We evaluate QStore for compressing multiple precisions of popular foundation models, and show that QStore reduces overall storage footprint by up to 2.2x (45% of the original size) while enabling up to 1.7x and 1.8x faster model saving and loading versus existing approaches."],"url":"http://arxiv.org/abs/2505.04081v1"}
{"created":"2025-05-07 02:42:05","title":"MojoFrame: Dataframe Library in Mojo Language","abstract":"Mojo is an emerging programming language built on MLIR (Multi-Level Intermediate Representation) and JIT compilation. It enables transparent optimizations with respect to the underlying hardware (e.g., CPUs, GPUs), while allowing users to express their logic using Python-like user-friendly syntax. Mojo has been shown to offer great performance in tensor operations; however, its performance has not been tested for relational operations (e.g., filtering, join, and group-by), which are common in data science workflows. To date, no dataframe implementation exists in the Mojo ecosystem.   In this paper, we introduce the first Mojo-native dataframe library, called MojoFrame, that supports core relational operations and user-defined functions (UDFs). MojoFrame is built on top of Mojo's tensor to achieve fast operations on numeric columns, while utilizing a cardinality-aware approach to effectively integrate non-numeric columns for flexible data representation. To achieve high efficiency, MojoFrame takes significantly different approaches than existing libraries. MojoFrame supports all operations for TPC-H queries, and achieves up to 2.97x speedup versus existing dataframe libraries in other programming languages. Nevertheless, there remain optimization opportunities for MojoFrame (and the Mojo language), particularly in data loading and dictionary operations.","sentences":["Mojo is an emerging programming language built on MLIR (Multi-Level Intermediate Representation) and JIT compilation.","It enables transparent optimizations with respect to the underlying hardware (e.g., CPUs, GPUs), while allowing users to express their logic using Python-like user-friendly syntax.","Mojo has been shown to offer great performance in tensor operations; however, its performance has not been tested for relational operations (e.g., filtering, join, and group-by), which are common in data science workflows.","To date, no dataframe implementation exists in the Mojo ecosystem.   ","In this paper, we introduce the first Mojo-native dataframe library, called MojoFrame, that supports core relational operations and user-defined functions (UDFs).","MojoFrame is built on top of Mojo's tensor to achieve fast operations on numeric columns, while utilizing a cardinality-aware approach to effectively integrate non-numeric columns for flexible data representation.","To achieve high efficiency, MojoFrame takes significantly different approaches than existing libraries.","MojoFrame supports all operations for TPC-H queries, and achieves up to 2.97x speedup versus existing dataframe libraries in other programming languages.","Nevertheless, there remain optimization opportunities for MojoFrame (and the Mojo language), particularly in data loading and dictionary operations."],"url":"http://arxiv.org/abs/2505.04080v1"}
{"created":"2025-05-07 02:25:29","title":"Natural Language Generation in Healthcare: A Review of Methods and Applications","abstract":"Natural language generation (NLG) is the key technology to achieve generative artificial intelligence (AI). With the breakthroughs in large language models (LLMs), NLG has been widely used in various medical applications, demonstrating the potential to enhance clinical workflows, support clinical decision-making, and improve clinical documentation. Heterogeneous and diverse medical data modalities, such as medical text, images, and knowledge bases, are utilized in NLG. Researchers have proposed many generative models and applied them in a number of healthcare applications. There is a need for a comprehensive review of NLG methods and applications in the medical domain. In this study, we systematically reviewed 113 scientific publications from a total of 3,988 NLG-related articles identified using a literature search, focusing on data modality, model architecture, clinical applications, and evaluation methods. Following PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) guidelines, we categorize key methods, identify clinical applications, and assess their capabilities, limitations, and emerging challenges. This timely review covers the key NLG technologies and medical applications and provides valuable insights for future studies to leverage NLG to transform medical discovery and healthcare.","sentences":["Natural language generation (NLG) is the key technology to achieve generative artificial intelligence (AI).","With the breakthroughs in large language models (LLMs), NLG has been widely used in various medical applications, demonstrating the potential to enhance clinical workflows, support clinical decision-making, and improve clinical documentation.","Heterogeneous and diverse medical data modalities, such as medical text, images, and knowledge bases, are utilized in NLG.","Researchers have proposed many generative models and applied them in a number of healthcare applications.","There is a need for a comprehensive review of NLG methods and applications in the medical domain.","In this study, we systematically reviewed 113 scientific publications from a total of 3,988 NLG-related articles identified using a literature search, focusing on data modality, model architecture, clinical applications, and evaluation methods.","Following PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) guidelines, we categorize key methods, identify clinical applications, and assess their capabilities, limitations, and emerging challenges.","This timely review covers the key NLG technologies and medical applications and provides valuable insights for future studies to leverage NLG to transform medical discovery and healthcare."],"url":"http://arxiv.org/abs/2505.04073v1"}
{"created":"2025-05-07 02:25:20","title":"Advancing and Benchmarking Personalized Tool Invocation for LLMs","abstract":"Tool invocation is a crucial mechanism for extending the capabilities of Large Language Models (LLMs) and has recently garnered significant attention. It enables LLMs to solve complex problems through tool calls while accessing up-to-date world knowledge. However, existing work primarily focuses on the fundamental ability of LLMs to invoke tools for problem-solving, without considering personalized constraints in tool invocation. In this work, we introduce the concept of Personalized Tool Invocation and define two key tasks: Tool Preference and Profile-dependent Query. Tool Preference addresses user preferences when selecting among functionally similar tools, while Profile-dependent Query considers cases where a user query lacks certain tool parameters, requiring the model to infer them from the user profile. To tackle these challenges, we propose PTool, a data synthesis framework designed for personalized tool invocation. Additionally, we construct \\textbf{PTBench}, the first benchmark for evaluating personalized tool invocation. We then fine-tune various open-source models, demonstrating the effectiveness of our framework and providing valuable insights. Our benchmark is public at https://github.com/hyfshadow/PTBench.","sentences":["Tool invocation is a crucial mechanism for extending the capabilities of Large Language Models (LLMs) and has recently garnered significant attention.","It enables LLMs to solve complex problems through tool calls while accessing up-to-date world knowledge.","However, existing work primarily focuses on the fundamental ability of LLMs to invoke tools for problem-solving, without considering personalized constraints in tool invocation.","In this work, we introduce the concept of Personalized Tool Invocation and define two key tasks: Tool Preference and Profile-dependent Query.","Tool Preference addresses user preferences when selecting among functionally similar tools, while Profile-dependent Query considers cases where a user query lacks certain tool parameters, requiring the model to infer them from the user profile.","To tackle these challenges, we propose PTool, a data synthesis framework designed for personalized tool invocation.","Additionally, we construct \\textbf{PTBench}, the first benchmark for evaluating personalized tool invocation.","We then fine-tune various open-source models, demonstrating the effectiveness of our framework and providing valuable insights.","Our benchmark is public at https://github.com/hyfshadow/PTBench."],"url":"http://arxiv.org/abs/2505.04072v1"}
{"created":"2025-05-07 01:47:15","title":"Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control","abstract":"Compositing human figures into scene images has broad applications in areas such as entertainment and advertising. However, existing methods often cannot handle occlusion of the inserted person by foreground objects and unnaturally place the person in the frontmost layer. Moreover, they offer limited control over the inserted person's pose. To address these challenges, we propose two methods. Both allow explicit pose control via a 3D body model and leverage latent diffusion models to synthesize the person at a contextually appropriate depth, naturally handling occlusions without requiring occlusion masks. The first is a two-stage approach: the model first learns a depth map of the scene with the person through supervised learning, and then synthesizes the person accordingly. The second method learns occlusion implicitly and synthesizes the person directly from input data without explicit depth supervision. Quantitative and qualitative evaluations show that both methods outperform existing approaches by better preserving scene consistency while accurately reflecting occlusions and user-specified poses.","sentences":["Compositing human figures into scene images has broad applications in areas such as entertainment and advertising.","However, existing methods often cannot handle occlusion of the inserted person by foreground objects and unnaturally place the person in the frontmost layer.","Moreover, they offer limited control over the inserted person's pose.","To address these challenges, we propose two methods.","Both allow explicit pose control via a 3D body model and leverage latent diffusion models to synthesize the person at a contextually appropriate depth, naturally handling occlusions without requiring occlusion masks.","The first is a two-stage approach: the model first learns a depth map of the scene with the person through supervised learning, and then synthesizes the person accordingly.","The second method learns occlusion implicitly and synthesizes the person directly from input data without explicit depth supervision.","Quantitative and qualitative evaluations show that both methods outperform existing approaches by better preserving scene consistency while accurately reflecting occlusions and user-specified poses."],"url":"http://arxiv.org/abs/2505.04052v1"}
{"created":"2025-05-07 01:17:45","title":"The Kinetic Hourglass Data Structure for Computing the Bottleneck Distance of Dynamic Data","abstract":"The kinetic data structure (KDS) framework is a powerful tool for maintaining various geometric configurations of continuously moving objects. In this work, we introduce the kinetic hourglass, a novel KDS implementation designed to compute the bottleneck distance for geometric matching problems. We detail the events and updates required for handling general graphs, accompanied by a complexity analysis. Furthermore, we demonstrate the utility of the kinetic hourglass by applying it to compute the bottleneck distance between two persistent homology transforms (PHTs) derived from shapes in $\\mathbb{R}^2$, which are topological summaries obtained by computing persistent homology from every direction in $\\mathbb{S}^1$.","sentences":["The kinetic data structure (KDS) framework is a powerful tool for maintaining various geometric configurations of continuously moving objects.","In this work, we introduce the kinetic hourglass, a novel KDS implementation designed to compute the bottleneck distance for geometric matching problems.","We detail the events and updates required for handling general graphs, accompanied by a complexity analysis.","Furthermore, we demonstrate the utility of the kinetic hourglass by applying it to compute the bottleneck distance between two persistent homology transforms (PHTs) derived from shapes in $\\mathbb{R}^2$, which are topological summaries obtained by computing persistent homology from every direction in $\\mathbb{S}^1$."],"url":"http://arxiv.org/abs/2505.04048v1"}
{"created":"2025-05-07 01:12:00","title":"Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks","abstract":"Recently, trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods implicitly assume that multi-view data is secure. In practice, however, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view learning models. This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning. To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor. Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them. Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed. Extensive experiments on multi-view classification tasks with adversarial attacks show that our RDML outperforms the state-of-the-art multi-view learning methods by a relatively large margin.","sentences":["Recently, trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions.","Existing trusted multi-view learning methods implicitly assume that multi-view data is secure.","In practice, however, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view learning models.","This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning.","To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML).","Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor.","Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them.","Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed.","Extensive experiments on multi-view classification tasks with adversarial attacks show that our RDML outperforms the state-of-the-art multi-view learning methods by a relatively large margin."],"url":"http://arxiv.org/abs/2505.04046v1"}
{"created":"2025-05-06 23:32:16","title":"Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest","abstract":"The need to explain predictive models is well-established in modern machine learning. However, beyond model interpretability, understanding pre-processing methods is equally essential. Understanding how data modifications impact model performance improvements and potential biases and promoting a reliable pipeline is mandatory for developing robust machine learning solutions. Isolation Forest (iForest) is a widely used technique for outlier detection that performs well. Its effectiveness increases with the number of tree-based learners. However, this also complicates the explanation of outlier selection and the decision boundaries for inliers. This research introduces a novel Explainable AI (XAI) method, tackling the problem of global explainability. In detail, it aims to offer a global explanation for outlier detection to address its opaque nature. Our approach is based on the Decision Predicate Graph (DPG), which clarifies the logic of ensemble methods and provides both insights and a graph-based metric to explain how samples are identified as outliers using the proposed Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's explainability and provides a comprehensive view of the decision-making process, detailing which features contribute to outlier identification and how the model utilizes them. This method advances the state-of-the-art by providing insights into decision boundaries and a comprehensive view of holistic feature usage in outlier identification. -- thus promoting a fully explainable machine learning pipeline.","sentences":["The need to explain predictive models is well-established in modern machine learning.","However, beyond model interpretability, understanding pre-processing methods is equally essential.","Understanding how data modifications impact model performance improvements and potential biases and promoting a reliable pipeline is mandatory for developing robust machine learning solutions.","Isolation Forest (iForest) is a widely used technique for outlier detection that performs well.","Its effectiveness increases with the number of tree-based learners.","However, this also complicates the explanation of outlier selection and the decision boundaries for inliers.","This research introduces a novel Explainable AI (XAI) method, tackling the problem of global explainability.","In detail, it aims to offer a global explanation for outlier detection to address its opaque nature.","Our approach is based on the Decision Predicate Graph (DPG), which clarifies the logic of ensemble methods and provides both insights and a graph-based metric to explain how samples are identified as outliers using the proposed Inlier-Outlier Propagation Score (IOP-Score).","Our proposal enhances iForest's explainability and provides a comprehensive view of the decision-making process, detailing which features contribute to outlier identification and how the model utilizes them.","This method advances the state-of-the-art by providing insights into decision boundaries and a comprehensive view of holistic feature usage in outlier identification.","-- thus promoting a fully explainable machine learning pipeline."],"url":"http://arxiv.org/abs/2505.04019v1"}
{"created":"2025-05-06 23:31:37","title":"Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers","abstract":"Modal identification is crucial for structural health monitoring and structural control, providing critical insights into structural dynamics and performance. This study presents a novel deep learning framework that integrates graph neural networks (GNNs), transformers, and a physics-informed loss function to achieve modal decomposition and identification across a population of structures. The transformer module decomposes multi-degrees-of-freedom (MDOF) structural dynamic measurements into single-degree-of-freedom (SDOF) modal responses, facilitating the identification of natural frequencies and damping ratios. Concurrently, the GNN captures the structural configurations and identifies mode shapes corresponding to the decomposed SDOF modal responses. The proposed model is trained in a purely physics-informed and unsupervised manner, leveraging modal decomposition theory and the independence of structural modes to guide learning without the need for labeled data. Validation through numerical simulations and laboratory experiments demonstrates its effectiveness in accurately decomposing dynamic responses and identifying modal properties from sparse structural dynamic measurements, regardless of variations in external loads or structural configurations. Comparative analyses against established modal identification techniques and model variations further underscore its superior performance, positioning it as a favorable approach for population-based structural health monitoring.","sentences":["Modal identification is crucial for structural health monitoring and structural control, providing critical insights into structural dynamics and performance.","This study presents a novel deep learning framework that integrates graph neural networks (GNNs), transformers, and a physics-informed loss function to achieve modal decomposition and identification across a population of structures.","The transformer module decomposes multi-degrees-of-freedom (MDOF) structural dynamic measurements into single-degree-of-freedom (SDOF) modal responses, facilitating the identification of natural frequencies and damping ratios.","Concurrently, the GNN captures the structural configurations and identifies mode shapes corresponding to the decomposed SDOF modal responses.","The proposed model is trained in a purely physics-informed and unsupervised manner, leveraging modal decomposition theory and the independence of structural modes to guide learning without the need for labeled data.","Validation through numerical simulations and laboratory experiments demonstrates its effectiveness in accurately decomposing dynamic responses and identifying modal properties from sparse structural dynamic measurements, regardless of variations in external loads or structural configurations.","Comparative analyses against established modal identification techniques and model variations further underscore its superior performance, positioning it as a favorable approach for population-based structural health monitoring."],"url":"http://arxiv.org/abs/2505.04018v1"}
{"created":"2025-05-06 23:29:43","title":"SLOT: Structuring the Output of Large Language Models","abstract":"Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.","sentences":["Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction.","Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development.","We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats.","While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications.","We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity.","Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively).","Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments."],"url":"http://arxiv.org/abs/2505.04016v1"}
{"created":"2025-05-06 22:29:07","title":"PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers","abstract":"Humans excel in navigating diverse, complex environments with agile motor skills, exemplified by parkour practitioners performing dynamic maneuvers, such as climbing up walls and jumping across gaps. Reproducing these agile movements with simulated characters remains challenging, in part due to the scarcity of motion capture data for agile terrain traversal behaviors and the high cost of acquiring such data. In this work, we introduce PARC (Physics-based Augmentation with Reinforcement Learning for Character Controllers), a framework that leverages machine learning and physics-based simulation to iteratively augment motion datasets and expand the capabilities of terrain traversal controllers. PARC begins by training a motion generator on a small dataset consisting of core terrain traversal skills. The motion generator is then used to produce synthetic data for traversing new terrains. However, these generated motions often exhibit artifacts, such as incorrect contacts or discontinuities. To correct these artifacts, we train a physics-based tracking controller to imitate the motions in simulation. The corrected motions are then added to the dataset, which is used to continue training the motion generator in the next iteration. PARC's iterative process jointly expands the capabilities of the motion generator and tracker, creating agile and versatile models for interacting with complex environments. PARC provides an effective approach to develop controllers for agile terrain traversal, which bridges the gap between the scarcity of motion data and the need for versatile character controllers.","sentences":["Humans excel in navigating diverse, complex environments with agile motor skills, exemplified by parkour practitioners performing dynamic maneuvers, such as climbing up walls and jumping across gaps.","Reproducing these agile movements with simulated characters remains challenging, in part due to the scarcity of motion capture data for agile terrain traversal behaviors and the high cost of acquiring such data.","In this work, we introduce PARC (Physics-based Augmentation with Reinforcement Learning for Character Controllers), a framework that leverages machine learning and physics-based simulation to iteratively augment motion datasets and expand the capabilities of terrain traversal controllers.","PARC begins by training a motion generator on a small dataset consisting of core terrain traversal skills.","The motion generator is then used to produce synthetic data for traversing new terrains.","However, these generated motions often exhibit artifacts, such as incorrect contacts or discontinuities.","To correct these artifacts, we train a physics-based tracking controller to imitate the motions in simulation.","The corrected motions are then added to the dataset, which is used to continue training the motion generator in the next iteration.","PARC's iterative process jointly expands the capabilities of the motion generator and tracker, creating agile and versatile models for interacting with complex environments.","PARC provides an effective approach to develop controllers for agile terrain traversal, which bridges the gap between the scarcity of motion data and the need for versatile character controllers."],"url":"http://arxiv.org/abs/2505.04002v1"}
{"created":"2025-05-06 22:02:53","title":"Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics","abstract":"Evaluating machine learning models is crucial not only for determining their technical accuracy but also for assessing their potential societal implications. While the potential for low-sample-size bias in algorithms is well known, we demonstrate the significance of sample-size bias induced by combinatorics in classification metrics. This revelation challenges the efficacy of these metrics in assessing bias with high resolution, especially when comparing groups of disparate sizes, which frequently arise in social applications. We provide analyses of the bias that appears in several commonly applied metrics and propose a model-agnostic assessment and correction technique. Additionally, we analyze counts of undefined cases in metric calculations, which can lead to misleading evaluations if improperly handled. This work illuminates the previously unrecognized challenge of combinatorics and probability in standard evaluation practices and thereby advances approaches for performing fair and trustworthy classification methods.","sentences":["Evaluating machine learning models is crucial not only for determining their technical accuracy but also for assessing their potential societal implications.","While the potential for low-sample-size bias in algorithms is well known, we demonstrate the significance of sample-size bias induced by combinatorics in classification metrics.","This revelation challenges the efficacy of these metrics in assessing bias with high resolution, especially when comparing groups of disparate sizes, which frequently arise in social applications.","We provide analyses of the bias that appears in several commonly applied metrics and propose a model-agnostic assessment and correction technique.","Additionally, we analyze counts of undefined cases in metric calculations, which can lead to misleading evaluations if improperly handled.","This work illuminates the previously unrecognized challenge of combinatorics and probability in standard evaluation practices and thereby advances approaches for performing fair and trustworthy classification methods."],"url":"http://arxiv.org/abs/2505.03992v1"}
{"created":"2025-05-06 21:41:20","title":"Can Large Language Models Predict Parallel Code Performance?","abstract":"Accurate determination of the performance of parallel GPU code typically requires execution-time profiling on target hardware -- an increasingly prohibitive step due to limited access to high-end GPUs. This paper explores whether Large Language Models (LLMs) can offer an alternative approach for GPU performance prediction without relying on hardware. We frame the problem as a roofline classification task: given the source code of a GPU kernel and the hardware specifications of a target GPU, can an LLM predict whether the GPU kernel is compute-bound or bandwidth-bound?   For this study, we build a balanced dataset of 340 GPU kernels, obtained from HeCBench benchmark and written in CUDA and OpenMP, along with their ground-truth labels obtained via empirical GPU profiling. We evaluate LLMs across four scenarios: (1) with access to profiling data of the kernel source, (2) zero-shot with source code only, (3) few-shot with code and label pairs, and (4) fine-tuned on a small custom dataset.   Our results show that state-of-the-art LLMs have a strong understanding of the Roofline model, achieving 100% classification accuracy when provided with explicit profiling data. We also find that reasoning-capable LLMs significantly outperform standard LLMs in zero- and few-shot settings, achieving up to 64% accuracy on GPU source codes, without profiling information. Lastly, we find that LLM fine-tuning will require much more data than what we currently have available.   This work is among the first to use LLMs for source-level roofline performance prediction via classification, and illustrates their potential to guide optimization efforts when runtime profiling is infeasible. Our findings suggest that with better datasets and prompt strategies, LLMs could become practical tools for HPC performance analysis and performance portability.","sentences":["Accurate determination of the performance of parallel GPU code typically requires execution-time profiling on target hardware -- an increasingly prohibitive step due to limited access to high-end GPUs.","This paper explores whether Large Language Models (LLMs) can offer an alternative approach for GPU performance prediction without relying on hardware.","We frame the problem as a roofline classification task: given the source code of a GPU kernel and the hardware specifications of a target GPU, can an LLM predict whether the GPU kernel is compute-bound or bandwidth-bound?   ","For this study, we build a balanced dataset of 340 GPU kernels, obtained from HeCBench benchmark and written in CUDA and OpenMP, along with their ground-truth labels obtained via empirical GPU profiling.","We evaluate LLMs across four scenarios: (1) with access to profiling data of the kernel source, (2) zero-shot with source code only, (3) few-shot with code and label pairs, and (4) fine-tuned on a small custom dataset.   ","Our results show that state-of-the-art LLMs have a strong understanding of the Roofline model, achieving 100% classification accuracy when provided with explicit profiling data.","We also find that reasoning-capable LLMs significantly outperform standard LLMs in zero- and few-shot settings, achieving up to 64% accuracy on GPU source codes, without profiling information.","Lastly, we find that LLM fine-tuning will require much more data than what we currently have available.   ","This work is among the first to use LLMs for source-level roofline performance prediction via classification, and illustrates their potential to guide optimization efforts when runtime profiling is infeasible.","Our findings suggest that with better datasets and prompt strategies, LLMs could become practical tools for HPC performance analysis and performance portability."],"url":"http://arxiv.org/abs/2505.03988v1"}
{"created":"2025-05-06 21:27:07","title":"LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration","abstract":"Emergency response services are critical to public safety, with 9-1-1 call-takers playing a key role in ensuring timely and effective emergency operations. To ensure call-taking performance consistency, quality assurance is implemented to evaluate and refine call-takers' skillsets. However, traditional human-led evaluations struggle with high call volumes, leading to low coverage and delayed assessments. We introduce LogiDebrief, an AI-driven framework that automates traditional 9-1-1 call debriefing by integrating Signal-Temporal Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous performance evaluation. LogiDebrief formalizes call-taking requirements as logical specifications, enabling systematic assessment of 9-1-1 calls against procedural guidelines. It employs a three-step verification process: (1) contextual understanding to identify responder types, incident classifications, and critical conditions; (2) STL-based runtime checking with LLM integration to ensure compliance; and (3) automated aggregation of results into quality assurance reports. Beyond its technical contributions, LogiDebrief has demonstrated real-world impact. Successfully deployed at Metro Nashville Department of Emergency Communications, it has assisted in debriefing 1,701 real-world calls, saving 311.85 hours of active engagement. Empirical evaluation with real-world data confirms its accuracy, while a case study and extensive user study highlight its effectiveness in enhancing call-taking performance.","sentences":["Emergency response services are critical to public safety, with 9-1-1 call-takers playing a key role in ensuring timely and effective emergency operations.","To ensure call-taking performance consistency, quality assurance is implemented to evaluate and refine call-takers' skillsets.","However, traditional human-led evaluations struggle with high call volumes, leading to low coverage and delayed assessments.","We introduce LogiDebrief, an AI-driven framework that automates traditional 9-1-1 call debriefing by integrating Signal-Temporal Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous performance evaluation.","LogiDebrief formalizes call-taking requirements as logical specifications, enabling systematic assessment of 9-1-1 calls against procedural guidelines.","It employs a three-step verification process: (1) contextual understanding to identify responder types, incident classifications, and critical conditions; (2) STL-based runtime checking with LLM integration to ensure compliance; and (3) automated aggregation of results into quality assurance reports.","Beyond its technical contributions, LogiDebrief has demonstrated real-world impact.","Successfully deployed at Metro Nashville Department of Emergency Communications, it has assisted in debriefing 1,701 real-world calls, saving 311.85 hours of active engagement.","Empirical evaluation with real-world data confirms its accuracy, while a case study and extensive user study highlight its effectiveness in enhancing call-taking performance."],"url":"http://arxiv.org/abs/2505.03985v1"}
{"created":"2025-05-06 21:08:27","title":"X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains","abstract":"Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.","sentences":["Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities.","Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks.","Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains.","This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains?","Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning.","Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards.","Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1).","Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data.","Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks."],"url":"http://arxiv.org/abs/2505.03981v1"}
{"created":"2025-05-06 20:52:58","title":"Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces","abstract":"Recently, there has been an impetus for the application of cutting-edge data collection platforms such as drones mounted with camera sensors for infrastructure asset management. However, the sensor characteristics, proximity to the structure, hard-to-reach access, and environmental conditions often limit the resolution of the datasets. A few studies used super-resolution techniques to address the problem of low-resolution images. Nevertheless, these techniques were observed to increase computational cost and false alarms of distress detection due to the consideration of all the infrastructure images i.e., positive and negative distress classes. In order to address the pre-processing of false alarm and achieve efficient super-resolution, this study developed a framework consisting of convolutional neural network (CNN) and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately classified both the classes. ESPCNN, which is the lightweight super-resolution technique, generated high-resolution infrastructure image of positive distress obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the evaluation metrics for super-resolution. Based on the performance metrics, the combination of CNN and ESPCNN was observed to be effective in preprocessing the infrastructure images with negative distress, reducing the computational cost and false alarms in the next step of super-resolution. The visual inspection showed that EPSCNN is able to capture crack propagation, complex geometry of even minor cracks. The proposed framework is expected to help the highway agencies in accurately performing distress detection and assist in efficient asset management practices.","sentences":["Recently, there has been an impetus for the application of cutting-edge data collection platforms such as drones mounted with camera sensors for infrastructure asset management.","However, the sensor characteristics, proximity to the structure, hard-to-reach access, and environmental conditions often limit the resolution of the datasets.","A few studies used super-resolution techniques to address the problem of low-resolution images.","Nevertheless, these techniques were observed to increase computational cost and false alarms of distress detection due to the consideration of all the infrastructure images i.e., positive and negative distress classes.","In order to address the pre-processing of false alarm and achieve efficient super-resolution, this study developed a framework consisting of convolutional neural network (CNN) and efficient sub-pixel convolutional neural network (ESPCNN).","CNN accurately classified both the classes.","ESPCNN, which is the lightweight super-resolution technique, generated high-resolution infrastructure image of positive distress obtained from CNN.","The ESPCNN outperformed bicubic interpolation in all the evaluation metrics for super-resolution.","Based on the performance metrics, the combination of CNN and ESPCNN was observed to be effective in preprocessing the infrastructure images with negative distress, reducing the computational cost and false alarms in the next step of super-resolution.","The visual inspection showed that EPSCNN is able to capture crack propagation, complex geometry of even minor cracks.","The proposed framework is expected to help the highway agencies in accurately performing distress detection and assist in efficient asset management practices."],"url":"http://arxiv.org/abs/2505.03974v1"}
{"created":"2025-05-06 20:44:03","title":"A Reasoning-Focused Legal Retrieval Benchmark","abstract":"As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs (\"RAG\" systems) to improve system performance and robustness. An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering. To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA. Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research. We describe the construction of these benchmarks and the performance of existing retriever pipelines. Our results suggest that legal RAG remains a challenging application, thus motivating future research.","sentences":["As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs (\"RAG\" systems) to improve system performance and robustness.","An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering.","To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.","Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research.","We describe the construction of these benchmarks and the performance of existing retriever pipelines.","Our results suggest that legal RAG remains a challenging application, thus motivating future research."],"url":"http://arxiv.org/abs/2505.03970v1"}
{"created":"2025-05-06 20:19:00","title":"Bicluster Editing with Overlaps: A Vertex Splitting Approach","abstract":"The BiCluster Editing problem aims at editing a given bipartite graph into a disjoint union of bicliques via a minimum number of edge deletion or addition operations. As a graph-based model for data clustering, the problem aims at a partition of the input dataset, which cannot always obtain meaningful clusters when some data elements are expected to belong to more than one cluster each. To address this limitation, we introduce the Bicluster Editing with Vertex Splitting problem (BCEVS) which consists of finding a minimum sequence of edge editions and vertex splittings such that the result graph is a disjoint union of bicliques. The vertex splitting operation consists of replacing a vertex $v$ with two vertices whose union of neighborhoods is the neighborhood of $v$. We also introduce the problem of Bicluster Editing with One-Sided Vertex Splitting (BCEOVS) where we restrict the splitting operations to the first set of the bipartition (often corresponding to data elements in the input raw data). We prove the two problems are NP-complete even when restricted to bipartite planar graphs of maximum degree three. Moreover, assuming the Exponential Time Hypothesis holds, there is no $2^{o(n)}n^{O(1)}$-time (resp. $2^{o(\\sqrt{n})}n^{O(1)}$-time) algorithm for BCEVS and BCEOVS on bipartite (resp. planar) graphs with maximum degree three where $n$ is the number of vertices of the graph. Furthermore we prove both problems are APX-hard and solvable in polynomial time on trees. On the other hand, we prove that BCEOVS is fixed parameter tractable with respect to solution size and admits a polynomial size kernel.","sentences":["The BiCluster Editing problem aims at editing a given bipartite graph into a disjoint union of bicliques via a minimum number of edge deletion or addition operations.","As a graph-based model for data clustering, the problem aims at a partition of the input dataset, which cannot always obtain meaningful clusters when some data elements are expected to belong to more than one cluster each.","To address this limitation, we introduce the Bicluster Editing with Vertex Splitting problem (BCEVS) which consists of finding a minimum sequence of edge editions and vertex splittings such that the result graph is a disjoint union of bicliques.","The vertex splitting operation consists of replacing a vertex $v$ with two vertices whose union of neighborhoods is the neighborhood of $v$. We also introduce the problem of Bicluster Editing with One-Sided Vertex Splitting (BCEOVS) where we restrict the splitting operations to the first set of the bipartition (often corresponding to data elements in the input raw data).","We prove the two problems are NP-complete even when restricted to bipartite planar graphs of maximum degree three.","Moreover, assuming the Exponential Time Hypothesis holds, there is no $2^{o(n)}n^{O(1)}$-time (resp.","$2^{o(\\sqrt{n})}n^{O(1)}$-time) algorithm for BCEVS and BCEOVS on bipartite (resp. planar) graphs with maximum degree three where $n$ is the number of vertices of the graph.","Furthermore we prove both problems are APX-hard and solvable in polynomial time on trees.","On the other hand, we prove that BCEOVS is fixed parameter tractable with respect to solution size and admits a polynomial size kernel."],"url":"http://arxiv.org/abs/2505.03959v1"}
{"created":"2025-05-06 20:10:17","title":"Sufficient Decision Proxies for Decision-Focused Learning","abstract":"When solving optimization problems under uncertainty with contextual data, utilizing machine learning to predict the uncertain parameters is a popular and effective approach. Decision-focused learning (DFL) aims at learning a predictive model such that decision quality, instead of prediction accuracy, is maximized. Common practice here is to predict a single value for each uncertain parameter, implicitly assuming that there exists a (single-scenario) deterministic problem approximation (proxy) that is sufficient to obtain an optimal decision. Other work assumes the opposite, where the underlying distribution needs to be estimated. However, little is known about when either choice is valid. This paper investigates for the first time problem properties that justify using either assumption. Using this, we present effective decision proxies for DFL, with very limited compromise on the complexity of the learning task. We show the effectiveness of presented approaches in experiments on problems with continuous and discrete variables, as well as uncertainty in the objective function and in the constraints.","sentences":["When solving optimization problems under uncertainty with contextual data, utilizing machine learning to predict the uncertain parameters is a popular and effective approach.","Decision-focused learning (DFL) aims at learning a predictive model such that decision quality, instead of prediction accuracy, is maximized.","Common practice here is to predict a single value for each uncertain parameter, implicitly assuming that there exists a (single-scenario) deterministic problem approximation (proxy) that is sufficient to obtain an optimal decision.","Other work assumes the opposite, where the underlying distribution needs to be estimated.","However, little is known about when either choice is valid.","This paper investigates for the first time problem properties that justify using either assumption.","Using this, we present effective decision proxies for DFL, with very limited compromise on the complexity of the learning task.","We show the effectiveness of presented approaches in experiments on problems with continuous and discrete variables, as well as uncertainty in the objective function and in the constraints."],"url":"http://arxiv.org/abs/2505.03953v1"}
{"created":"2025-05-06 19:45:13","title":"AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience","abstract":"Cloud security concerns have been greatly realized in recent years due to the increase of complicated threats in the computing world. Many traditional solutions do not work well in real-time to detect or prevent more complex threats. Artificial intelligence is today regarded as a revolution in determining a protection plan for cloud data architecture through machine learning, statistical visualization of computing infrastructure, and detection of security breaches followed by counteraction. These AI-enabled systems make work easier as more network activities are scrutinized, and any anomalous behavior that might be a precursor to a more serious breach is prevented. This paper examines ways AI can enhance cloud security by applying predictive analytics, behavior-based security threat detection, and AI-stirring encryption. It also outlines the problems of the previous security models and how AI overcomes them. For a similar reason, issues like data privacy, biases in the AI model, and regulatory compliance are also covered. So, AI improves the protection of cloud computing contexts; however, more efforts are needed in the subsequent phases to extend the technology's reliability, modularity, and ethical aspects. This means that AI can be blended with other new computing technologies, including blockchain, to improve security frameworks further. The paper discusses the current trends in securing cloud data architecture using AI and presents further research and application directions.","sentences":["Cloud security concerns have been greatly realized in recent years due to the increase of complicated threats in the computing world.","Many traditional solutions do not work well in real-time to detect or prevent more complex threats.","Artificial intelligence is today regarded as a revolution in determining a protection plan for cloud data architecture through machine learning, statistical visualization of computing infrastructure, and detection of security breaches followed by counteraction.","These AI-enabled systems make work easier as more network activities are scrutinized, and any anomalous behavior that might be a precursor to a more serious breach is prevented.","This paper examines ways AI can enhance cloud security by applying predictive analytics, behavior-based security threat detection, and AI-stirring encryption.","It also outlines the problems of the previous security models and how AI overcomes them.","For a similar reason, issues like data privacy, biases in the AI model, and regulatory compliance are also covered.","So, AI improves the protection of cloud computing contexts; however, more efforts are needed in the subsequent phases to extend the technology's reliability, modularity, and ethical aspects.","This means that AI can be blended with other new computing technologies, including blockchain, to improve security frameworks further.","The paper discusses the current trends in securing cloud data architecture using AI and presents further research and application directions."],"url":"http://arxiv.org/abs/2505.03945v1"}
{"created":"2025-05-06 19:38:07","title":"GRAML: Dynamic Goal Recognition As Metric Learning","abstract":"Goal Recognition (GR) is the problem of recognizing an agent's objectives based on observed actions. Recent data-driven approaches for GR alleviate the need for costly, manually crafted domain models. However, these approaches can only reason about a pre-defined set of goals, and time-consuming training is needed for new emerging goals. To keep this model-learning automated while enabling quick adaptation to new goals, this paper introduces GRAML: Goal Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a deep metric learning task, employing an RNN that learns a metric over an embedding space, where the embeddings for observation traces leading to different goals are distant, and embeddings of traces leading to the same goals are close. This metric is especially useful when adapting to new goals, even if given just one example observation trace per goal. Evaluated on a versatile set of environments, GRAML shows speed, flexibility, and runtime improvements over the state-of-the-art GR while maintaining accurate recognition.","sentences":["Goal Recognition (GR) is the problem of recognizing an agent's objectives based on observed actions.","Recent data-driven approaches for GR alleviate the need for costly, manually crafted domain models.","However, these approaches can only reason about a pre-defined set of goals, and time-consuming training is needed for new emerging goals.","To keep this model-learning automated while enabling quick adaptation to new goals, this paper introduces GRAML:","Goal Recognition As Metric Learning.","GRAML uses a Siamese network to treat GR as a deep metric learning task, employing an RNN that learns a metric over an embedding space, where the embeddings for observation traces leading to different goals are distant, and embeddings of traces leading to the same goals are close.","This metric is especially useful when adapting to new goals, even if given just one example observation trace per goal.","Evaluated on a versatile set of environments, GRAML shows speed, flexibility, and runtime improvements over the state-of-the-art GR while maintaining accurate recognition."],"url":"http://arxiv.org/abs/2505.03941v1"}
{"created":"2025-05-06 18:59:35","title":"SAND: One-Shot Feature Selection with Additive Noise Distortion","abstract":"Feature selection is a critical step in data-driven applications, reducing input dimensionality to enhance learning accuracy, computational efficiency, and interpretability. Existing state-of-the-art methods often require post-selection retraining and extensive hyperparameter tuning, complicating their adoption. We introduce a novel, non-intrusive feature selection layer that, given a target feature count $k$, automatically identifies and selects the $k$ most informative features during neural network training. Our method is uniquely simple, requiring no alterations to the loss function, network architecture, or post-selection retraining. The layer is mathematically elegant and can be fully described by: \\begin{align} \\nonumber \\tilde{x}_i = a_i x_i + (1-a_i)z_i \\end{align} where $x_i$ is the input feature, $\\tilde{x}_i$ the output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that $\\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect, driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the rest to $0$ (discarding redundant ones) via weighted noise distortion and gain normalization. Despite its extreme simplicity, our method delivers state-of-the-art performance on standard benchmark datasets and a novel real-world dataset, outperforming or matching existing approaches without requiring hyperparameter search for $k$ or retraining. Theoretical analysis in the context of linear regression further validates its efficacy. Our work demonstrates that simplicity and performance are not mutually exclusive, offering a powerful yet straightforward tool for feature selection in machine learning.","sentences":["Feature selection is a critical step in data-driven applications, reducing input dimensionality to enhance learning accuracy, computational efficiency, and interpretability.","Existing state-of-the-art methods often require post-selection retraining and extensive hyperparameter tuning, complicating their adoption.","We introduce a novel, non-intrusive feature selection layer that, given a target feature count $k$, automatically identifies and selects the $k$ most informative features during neural network training.","Our method is uniquely simple, requiring no alterations to the loss function, network architecture, or post-selection retraining.","The layer is mathematically elegant and can be fully described by: \\begin{align} \\nonumber \\tilde{x}_i = a_i x_i","+ (1-a_i)z_i \\end{align} where $x_i$ is the input feature, $\\tilde{x}_i$ the output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that $\\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect, driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the rest to $0$ (discarding redundant ones) via weighted noise distortion and gain normalization.","Despite its extreme simplicity, our method delivers state-of-the-art performance on standard benchmark datasets and a novel real-world dataset, outperforming or matching existing approaches without requiring hyperparameter search for $k$ or retraining.","Theoretical analysis in the context of linear regression further validates its efficacy.","Our work demonstrates that simplicity and performance are not mutually exclusive, offering a powerful yet straightforward tool for feature selection in machine learning."],"url":"http://arxiv.org/abs/2505.03923v1"}
{"created":"2025-05-06 18:50:19","title":"Omnidirectional vision sensors based on catadioptric systems with discrete infrared photoreceptors for swarm robotics","abstract":"In this work, we fabricated and studied two designs for omnidirectional vision sensors for swarm robotics, based on catadioptric systems consisting of a mirror with rotational symmetry, eight discrete infrared photodiodes and a single LED, in order to provide localization and navigation abilities for mobile robotic agents. We considered two arrangements for the photodiodes: one in which they point upward into the mirror, and one in which they point outward, perpendicular to the mirror. To determine which design offers a better field of view on the plane, as well as detection of distance and orientation between two agents, we developed a test rail with three degrees of freedom to experimentally and systematically measure the signal registered by the photodiodes of a given sensor (in a single readout) from the light emitted by another as functions of the distance and orientation. Afterwards, we processed and analyzed the experimental data to develop mathematical models for the mean response of a photodiode in each design. Finally, by numerically inverting the models, we compared the two designs in terms of their accuracy. Our results show that the design with the photodiodes pointing upward resolves better the distance, while the other resolves better the orientation of the emitting agent, both providing an omnidirectional field of view.","sentences":["In this work, we fabricated and studied two designs for omnidirectional vision sensors for swarm robotics, based on catadioptric systems consisting of a mirror with rotational symmetry, eight discrete infrared photodiodes and a single LED, in order to provide localization and navigation abilities for mobile robotic agents.","We considered two arrangements for the photodiodes: one in which they point upward into the mirror, and one in which they point outward, perpendicular to the mirror.","To determine which design offers a better field of view on the plane, as well as detection of distance and orientation between two agents, we developed a test rail with three degrees of freedom to experimentally and systematically measure the signal registered by the photodiodes of a given sensor (in a single readout) from the light emitted by another as functions of the distance and orientation.","Afterwards, we processed and analyzed the experimental data to develop mathematical models for the mean response of a photodiode in each design.","Finally, by numerically inverting the models, we compared the two designs in terms of their accuracy.","Our results show that the design with the photodiodes pointing upward resolves better the distance, while the other resolves better the orientation of the emitting agent, both providing an omnidirectional field of view."],"url":"http://arxiv.org/abs/2505.03920v1"}
{"created":"2025-05-06 18:45:50","title":"Improving Failure Prediction in Aircraft Fastener Assembly Using Synthetic Data in Imbalanced Datasets","abstract":"Automating aircraft manufacturing still relies heavily on human labor due to the complexity of the assembly processes and customization requirements. One key challenge is achieving precise positioning, especially for large aircraft structures, where errors can lead to substantial maintenance costs or part rejection. Existing solutions often require costly hardware or lack flexibility. Used in aircraft by the thousands, threaded fasteners, e.g., screws, bolts, and collars, are traditionally executed by fixed-base robots and usually have problems in being deployed in the mentioned manufacturing sites. This paper emphasizes the importance of error detection and classification for efficient and safe assembly of threaded fasteners, especially aeronautical collars. Safe assembly of threaded fasteners is paramount since acquiring sufficient data for training deep learning models poses challenges due to the rarity of failure cases and imbalanced datasets. The paper addresses this by proposing techniques like class weighting and data augmentation, specifically tailored for temporal series data, to improve classification performance. Furthermore, the paper introduces a novel problem-modeling approach, emphasizing metrics relevant to collar assembly rather than solely focusing on accuracy. This tailored approach enhances the models' capability to handle the challenges of threaded fastener assembly effectively.","sentences":["Automating aircraft manufacturing still relies heavily on human labor due to the complexity of the assembly processes and customization requirements.","One key challenge is achieving precise positioning, especially for large aircraft structures, where errors can lead to substantial maintenance costs or part rejection.","Existing solutions often require costly hardware or lack flexibility.","Used in aircraft by the thousands, threaded fasteners, e.g., screws, bolts, and collars, are traditionally executed by fixed-base robots and usually have problems in being deployed in the mentioned manufacturing sites.","This paper emphasizes the importance of error detection and classification for efficient and safe assembly of threaded fasteners, especially aeronautical collars.","Safe assembly of threaded fasteners is paramount since acquiring sufficient data for training deep learning models poses challenges due to the rarity of failure cases and imbalanced datasets.","The paper addresses this by proposing techniques like class weighting and data augmentation, specifically tailored for temporal series data, to improve classification performance.","Furthermore, the paper introduces a novel problem-modeling approach, emphasizing metrics relevant to collar assembly rather than solely focusing on accuracy.","This tailored approach enhances the models' capability to handle the challenges of threaded fastener assembly effectively."],"url":"http://arxiv.org/abs/2505.03917v1"}
{"created":"2025-05-06 18:35:05","title":"Explaining Anomalies with Tensor Networks","abstract":"Tensor networks, a class of variational quantum many-body wave functions have attracted considerable research interest across many disciplines, including classical machine learning. Recently, Aizpurua et al. demonstrated explainable anomaly detection with matrix product states on a discrete-valued cyber-security task, using quantum-inspired methods to gain insight into the learned model and detected anomalies. Here, we extend this framework to real-valued data domains. We furthermore introduce tree tensor networks for the task of explainable anomaly detection. We demonstrate these methods with three benchmark problems, show adequate predictive performance compared to several baseline models and both tensor network architectures' ability to explain anomalous samples. We thereby extend the application of tensor networks to a broader class of potential problems and open a pathway for future extensions to more complex tensor network architectures.","sentences":["Tensor networks, a class of variational quantum many-body wave functions have attracted considerable research interest across many disciplines, including classical machine learning.","Recently, Aizpurua et al. demonstrated explainable anomaly detection with matrix product states on a discrete-valued cyber-security task, using quantum-inspired methods to gain insight into the learned model and detected anomalies.","Here, we extend this framework to real-valued data domains.","We furthermore introduce tree tensor networks for the task of explainable anomaly detection.","We demonstrate these methods with three benchmark problems, show adequate predictive performance compared to several baseline models and both tensor network architectures' ability to explain anomalous samples.","We thereby extend the application of tensor networks to a broader class of potential problems and open a pathway for future extensions to more complex tensor network architectures."],"url":"http://arxiv.org/abs/2505.03911v1"}
{"created":"2025-05-06 18:29:57","title":"Minimum Congestion Routing of Unsplittable Flows in Data-Center Networks","abstract":"Millions of flows are routed concurrently through a modern data-center. These networks are often built as Clos topologies, and flow demands are constrained only by the link capacities at the ingress and egress points. The minimum congestion routing problem seeks to route a set of flows through a data center while minimizing the maximum flow demand on any link. This is easily achieved by splitting flow demands along all available paths. However, arbitrary flow splitting is unrealistic. Instead, network operators rely on heuristics for routing unsplittable flows, the best of which results in a worst-case congestion of $2$ (twice the uniform link capacities). But is $2$ the lowest possible congestion? If not, can an efficient routing algorithm attain congestion below $2$?   Guided by these questions, we investigate the minimum congestion routing problem in Clos networks with unsplittable flows. First, we show that for some sets of flows the minimum congestion is at least $\\nicefrac{3}{2}$, and that it is $NP$-hard to approximate a minimum congestion routing by a factor less than $\\nicefrac{3}{2}$. Second, addressing the motivating questions directly, we present a polynomial-time algorithm that guarantees a congestion of at most $\\nicefrac{9}{5}$ for any set of flows, while also providing a $\\nicefrac{9}{5}$ approximation of a minimum congestion routing. Last, shifting to the online setting, we demonstrate that no online algorithm (even randomized) can approximate a minimum congestion routing by a factor less than $2$, providing a strict separation between the online and the offline setting.","sentences":["Millions of flows are routed concurrently through a modern data-center.","These networks are often built as Clos topologies, and flow demands are constrained only by the link capacities at the ingress and egress points.","The minimum congestion routing problem seeks to route a set of flows through a data center while minimizing the maximum flow demand on any link.","This is easily achieved by splitting flow demands along all available paths.","However, arbitrary flow splitting is unrealistic.","Instead, network operators rely on heuristics for routing unsplittable flows, the best of which results in a worst-case congestion of $2$ (twice the uniform link capacities).","But is $2$ the lowest possible congestion?","If not, can an efficient routing algorithm attain congestion below $2$?   ","Guided by these questions, we investigate the minimum congestion routing problem in Clos networks with unsplittable flows.","First, we show that for some sets of flows the minimum congestion is at least $\\nicefrac{3}{2}$, and that it is $NP$-hard to approximate a minimum congestion routing by a factor less than $\\nicefrac{3}{2}$. Second, addressing the motivating questions directly, we present a polynomial-time algorithm that guarantees a congestion of at most $\\nicefrac{9}{5}$ for any set of flows, while also providing a $\\nicefrac{9}{5}$ approximation of a minimum congestion routing.","Last, shifting to the online setting, we demonstrate that no online algorithm (even randomized) can approximate a minimum congestion routing by a factor less than $2$, providing a strict separation between the online and the offline setting."],"url":"http://arxiv.org/abs/2505.03908v1"}
{"created":"2025-05-06 18:16:08","title":"Unveiling the Role of ChatGPT in Software Development: Insights from Developer-ChatGPT Interactions on GitHub","abstract":"The advent of Large Language Models (LLMs) has introduced a new paradigm in software engineering, with generative AI tools like ChatGPT gaining widespread adoption among developers. While ChatGPT's potential has been extensively discussed, there is limited empirical evidence exploring its real-world usage by developers. This study bridges this gap by conducting a large-scale empirical analysis of ChatGPT-assisted development activities, leveraging a curated dataset, DevChat, comprising 2,547 unique shared ChatGPT links collected from GitHub between May 2023 and June 2024. Our study examines the characteristics of ChatGPT's usage on GitHub (including the tendency, prompt turns distribution, and link descriptions) and identifies five categories of developers' purposes for sharing developer-ChatGPT conversations during software development. Additionally, we analyzed the development-related activities where developers shared ChatGPT links to facilitate their workflows. We then established a mapping framework among data sources, activities, and SE tasks associated with these shared ChatGPT links. Our study offers a comprehensive view of ChatGPT's application in real-world software development scenarios and provides a foundation for its future integration into software development workflows.","sentences":["The advent of Large Language Models (LLMs) has introduced a new paradigm in software engineering, with generative AI tools like ChatGPT gaining widespread adoption among developers.","While ChatGPT's potential has been extensively discussed, there is limited empirical evidence exploring its real-world usage by developers.","This study bridges this gap by conducting a large-scale empirical analysis of ChatGPT-assisted development activities, leveraging a curated dataset, DevChat, comprising 2,547 unique shared ChatGPT links collected from GitHub between May 2023 and June 2024.","Our study examines the characteristics of ChatGPT's usage on GitHub (including the tendency, prompt turns distribution, and link descriptions) and identifies five categories of developers' purposes for sharing developer-ChatGPT conversations during software development.","Additionally, we analyzed the development-related activities where developers shared ChatGPT links to facilitate their workflows.","We then established a mapping framework among data sources, activities, and SE tasks associated with these shared ChatGPT links.","Our study offers a comprehensive view of ChatGPT's application in real-world software development scenarios and provides a foundation for its future integration into software development workflows."],"url":"http://arxiv.org/abs/2505.03901v1"}
