{"created":"2024-01-31 18:59:57","title":"Binding Touch to Everything: Learning Unified Multimodal Tactile Representations","abstract":"The ability to associate touch with other modalities has huge implications for humans and computational systems. However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs. We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound. We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities. We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time. UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering. To the best of our knowledge, UniTouch is the first to demonstrate such capabilities. Project page: https://cfeng16.github.io/UniTouch/","sentences":["The ability to associate touch with other modalities has huge implications for humans and computational systems.","However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs.","We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound.","We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities.","We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time.","UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering.","To the best of our knowledge, UniTouch is the first to demonstrate such capabilities.","Project page: https://cfeng16.github.io/UniTouch/"],"url":"http://arxiv.org/abs/2401.18084v1"}
{"created":"2024-01-31 18:41:08","title":"Neural Locality Sensitive Hashing for Entity Blocking","abstract":"Locality-sensitive hashing (LSH) is a fundamental algorithmic technique widely employed in large-scale data processing applications, such as nearest-neighbor search, entity resolution, and clustering. However, its applicability in some real-world scenarios is limited due to the need for careful design of hashing functions that align with specific metrics. Existing LSH-based Entity Blocking solutions primarily rely on generic similarity metrics such as Jaccard similarity, whereas practical use cases often demand complex and customized similarity rules surpassing the capabilities of generic similarity metrics. Consequently, designing LSH functions for these customized similarity rules presents considerable challenges. In this research, we propose a neuralization approach to enhance locality-sensitive hashing by training deep neural networks to serve as hashing functions for complex metrics. We assess the effectiveness of this approach within the context of the entity resolution problem, which frequently involves the use of task-specific metrics in real-world applications. Specifically, we introduce NLSHBlock (Neural-LSH Block), a novel blocking methodology that leverages pre-trained language models, fine-tuned with a novel LSH-based loss function. Through extensive evaluations conducted on a diverse range of real-world datasets, we demonstrate the superiority of NLSHBlock over existing methods, exhibiting significant performance improvements. Furthermore, we showcase the efficacy of NLSHBlock in enhancing the performance of the entity matching phase, particularly within the semi-supervised setting.","sentences":["Locality-sensitive hashing (LSH) is a fundamental algorithmic technique widely employed in large-scale data processing applications, such as nearest-neighbor search, entity resolution, and clustering.","However, its applicability in some real-world scenarios is limited due to the need for careful design of hashing functions that align with specific metrics.","Existing LSH-based Entity Blocking solutions primarily rely on generic similarity metrics such as Jaccard similarity, whereas practical use cases often demand complex and customized similarity rules surpassing the capabilities of generic similarity metrics.","Consequently, designing LSH functions for these customized similarity rules presents considerable challenges.","In this research, we propose a neuralization approach to enhance locality-sensitive hashing by training deep neural networks to serve as hashing functions for complex metrics.","We assess the effectiveness of this approach within the context of the entity resolution problem, which frequently involves the use of task-specific metrics in real-world applications.","Specifically, we introduce NLSHBlock (Neural-LSH Block), a novel blocking methodology that leverages pre-trained language models, fine-tuned with a novel LSH-based loss function.","Through extensive evaluations conducted on a diverse range of real-world datasets, we demonstrate the superiority of NLSHBlock over existing methods, exhibiting significant performance improvements.","Furthermore, we showcase the efficacy of NLSHBlock in enhancing the performance of the entity matching phase, particularly within the semi-supervised setting."],"url":"http://arxiv.org/abs/2401.18064v1"}
{"created":"2024-01-31 18:29:39","title":"LongAlign: A Recipe for Long Context Alignment of Large Language Models","abstract":"Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\\%, while also maintaining their proficiency in handling short, generic tasks. The code, data, and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.","sentences":["Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length.","To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment.","First, we construct a long instruction-following dataset using Self-Instruct.","To ensure the data diversity, it covers a broad range of tasks from various long context sources.","Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions.","Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training.","Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length.","Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\\%, while also maintaining their proficiency in handling short, generic tasks.","The code, data, and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign."],"url":"http://arxiv.org/abs/2401.18058v1"}
{"created":"2024-01-31 18:29:10","title":"Rank Supervised Contrastive Learning for Time Series Classification","abstract":"Recently, various contrastive learning techniques have been developed to categorize time series data and exhibit promising performance. A general paradigm is to utilize appropriate augmentations and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart. Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is largely ignored, especially when labeled samples are limited. To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification. Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted way in the embedding space and adopts certain filtering rules to select more informative positive and negative pairs of samples. Moreover, a novel rank loss is developed to assign different weights for different levels of positive samples, enable the encoder to extract the fine-grained information of the same class, and produce a clear boundary among different classes. Thoroughly empirical studies on 128 UCR datasets and 30 UEA datasets demonstrate that the proposed RankSCL can achieve state-of-the-art performance compared to existing baseline methods.","sentences":["Recently, various contrastive learning techniques have been developed to categorize time series data and exhibit promising performance.","A general paradigm is to utilize appropriate augmentations and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart.","Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is largely ignored, especially when labeled samples are limited.","To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification.","Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted way in the embedding space and adopts certain filtering rules to select more informative positive and negative pairs of samples.","Moreover, a novel rank loss is developed to assign different weights for different levels of positive samples, enable the encoder to extract the fine-grained information of the same class, and produce a clear boundary among different classes.","Thoroughly empirical studies on 128 UCR datasets and 30 UEA datasets demonstrate that the proposed RankSCL can achieve state-of-the-art performance compared to existing baseline methods."],"url":"http://arxiv.org/abs/2401.18057v1"}
{"created":"2024-01-31 18:20:36","title":"How to Measure TLS, X.509 Certificates, and Web PKI: A Tutorial and Brief Survey","abstract":"Transport Layer Security (TLS) is the base for many Internet applications and services to achieve end-to-end security. In this paper, we provide guidance on how to measure TLS deployments, including X.509 certificates and Web PKI. We introduce common data sources and tools, and systematically describe necessary steps to conduct sound measurements and data analysis. By surveying prior TLS measurement studies we find that diverging results are rather rooted in different setups instead of different deployments. To improve the situation, we identify common pitfalls and introduce a framework to describe TLS and Web PKI measurements. Where necessary, our insights are bolstered by a data-driven approach, in which we complement arguments by additional measurements.","sentences":["Transport Layer Security (TLS) is the base for many Internet applications and services to achieve end-to-end security.","In this paper, we provide guidance on how to measure TLS deployments, including X.509 certificates and Web PKI.","We introduce common data sources and tools, and systematically describe necessary steps to conduct sound measurements and data analysis.","By surveying prior TLS measurement studies we find that diverging results are rather rooted in different setups instead of different deployments.","To improve the situation, we identify common pitfalls and introduce a framework to describe TLS and Web PKI measurements.","Where necessary, our insights are bolstered by a data-driven approach, in which we complement arguments by additional measurements."],"url":"http://arxiv.org/abs/2401.18053v1"}
{"created":"2024-01-31 18:14:13","title":"Hypermultiplexed Integrated Tensor Optical Processor","abstract":"Optical processors hold great potential to accelerate deep learning tasks with their high clock-rates and low-loss data transmission. However, existing integrated systems are hindered by low scalability due to the quadratic scaling of device counts, energy costs with high-speed analog-to-digital converters, and lack of inline nonlinearity. Here, we overcome these challenges with a wavelength-space-time multiplexed optical tensor processor. Hyperdimensional parallelism allows matrix-matrix multiplications ($N^{3}$ operations) using $O(N)$ devices. We incorporated wavelength-multiplexed III/V-based micron-scale lasers (spanning ~1 THz) for input activation with inline rectifier (ReLU) nonlinearities and thin-film Lithium-Niobate electro-optic modulators ($V_{\\pi}\\approx1.3 V$) for dynamic weighting. With each device encoding 10-billion activations per second, we demonstrated a machine-learning model with 405,000 parameters. High-clock-rate (10 GS/s), low-energy (500 fJ/OP) parallel computing with real-time programmability unlocks the full potential of light for next-generation scalable AI accelerators.","sentences":["Optical processors hold great potential to accelerate deep learning tasks with their high clock-rates and low-loss data transmission.","However, existing integrated systems are hindered by low scalability due to the quadratic scaling of device counts, energy costs with high-speed analog-to-digital converters, and lack of inline nonlinearity.","Here, we overcome these challenges with a wavelength-space-time multiplexed optical tensor processor.","Hyperdimensional parallelism allows matrix-matrix multiplications ($N^{3}$ operations) using $O(N)$ devices.","We incorporated wavelength-multiplexed III/V-based micron-scale lasers (spanning ~1 THz) for input activation with inline rectifier (ReLU) nonlinearities and thin-film Lithium-Niobate electro-optic modulators ($V_{\\pi}\\approx1.3 V$) for dynamic weighting.","With each device encoding 10-billion activations per second, we demonstrated a machine-learning model with 405,000 parameters.","High-clock-rate (10 GS/s), low-energy (500 fJ/OP) parallel computing with real-time programmability unlocks the full potential of light for next-generation scalable AI accelerators."],"url":"http://arxiv.org/abs/2401.18050v1"}
{"created":"2024-01-31 18:08:06","title":"Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm Optimization, and Deep Learning","abstract":"Epidemiological models are best suitable to model an epidemic if the spread pattern is stationary. To deal with non-stationary patterns and multiple waves of an epidemic, we develop a hybrid model encompassing epidemic modeling, particle swarm optimization, and deep learning. The model mainly caters to three objectives for better prediction: 1. Periodic estimation of the model parameters. 2. Incorporating impact of all the aspects using data fitting and parameter optimization 3. Deep learning based prediction of the model parameters. In our model, we use a system of ordinary differential equations (ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling, Particle Swarm Optimization (PSO) for model parameter optimization, and stacked-LSTM for forecasting the model parameters. Initial or one time estimation of model parameters is not able to model multiple waves of an epidemic. So, we estimate the model parameters periodically (weekly). We use PSO to identify the optimum values of the model parameters. We next train the stacked-LSTM on the optimized parameters, and perform forecasting of the model parameters for upcoming four weeks. Further, we fed the LSTM forecasted parameters into the SIRD model to forecast the number of COVID-19 cases. We evaluate the model for highly affected three countries namely; the USA, India, and the UK. The proposed hybrid model is able to deal with multiple waves, and has outperformed existing methods on all the three datasets.","sentences":["Epidemiological models are best suitable to model an epidemic if the spread pattern is stationary.","To deal with non-stationary patterns and multiple waves of an epidemic, we develop a hybrid model encompassing epidemic modeling, particle swarm optimization, and deep learning.","The model mainly caters to three objectives for better prediction:","1.","Periodic estimation of the model parameters.","2. Incorporating impact of all the aspects using data fitting and parameter optimization 3.","Deep learning based prediction of the model parameters.","In our model, we use a system of ordinary differential equations (ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling, Particle Swarm Optimization (PSO) for model parameter optimization, and stacked-LSTM for forecasting the model parameters.","Initial or one time estimation of model parameters is not able to model multiple waves of an epidemic.","So, we estimate the model parameters periodically (weekly).","We use PSO to identify the optimum values of the model parameters.","We next train the stacked-LSTM on the optimized parameters, and perform forecasting of the model parameters for upcoming four weeks.","Further, we fed the LSTM forecasted parameters into the SIRD model to forecast the number of COVID-19 cases.","We evaluate the model for highly affected three countries namely; the USA, India, and the UK.","The proposed hybrid model is able to deal with multiple waves, and has outperformed existing methods on all the three datasets."],"url":"http://arxiv.org/abs/2401.18047v1"}
{"created":"2024-01-31 18:07:12","title":"Multipath parsing in the brain","abstract":"Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.","sentences":["Humans understand sentences word-by-word, in the order that they hear them.","This incrementality entails resolving temporary ambiguities about syntactic relationships.","We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook.","In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one.","This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset.","In both English and Chinese data, we find evidence for multipath parsing.","Brain regions associated with this multipath effect include bilateral superior temporal gyrus."],"url":"http://arxiv.org/abs/2401.18046v1"}
{"created":"2024-01-31 17:45:05","title":"Context-Sensitive Abstract Interpretation of Dynamic Languages","abstract":"There is a vast gap in the quality of IDE tooling between static languages like Java and dynamic languages like Python or JavaScript. Modern frameworks and libraries in these languages heavily use their dynamic capabilities to achieve the best ergonomics and readability. This has a side effect of making the current generation of IDEs blind to control flow and data flow, which often breaks navigation, autocompletion and refactoring. In this thesis we propose an algorithm that can bridge this gap between tooling for dynamic and static languages by statically analyzing dynamic metaprogramming and runtime reflection in programs. We use a technique called abstract interpretation to partially execute programs and extract information that is usually only available at runtime. Our algorithm has been implemented in a prototype analyzer that can analyze programs written in a subset of JavaScript.","sentences":["There is a vast gap in the quality of IDE tooling between static languages like Java and dynamic languages like Python or JavaScript.","Modern frameworks and libraries in these languages heavily use their dynamic capabilities to achieve the best ergonomics and readability.","This has a side effect of making the current generation of IDEs blind to control flow and data flow, which often breaks navigation, autocompletion and refactoring.","In this thesis we propose an algorithm that can bridge this gap between tooling for dynamic and static languages by statically analyzing dynamic metaprogramming and runtime reflection in programs.","We use a technique called abstract interpretation to partially execute programs and extract information that is usually only available at runtime.","Our algorithm has been implemented in a prototype analyzer that can analyze programs written in a subset of JavaScript."],"url":"http://arxiv.org/abs/2401.18029v1"}
{"created":"2024-01-31 17:43:04","title":"Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI","abstract":"Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and Mistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3) using a sample from this baseline. We find that the generated impacts using Mistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively on par with impacts generated using a larger scale model such as GPT-4. Moreover, we find that these LLMs generate impacts that largely reflect the taxonomy of negative impacts identified in the news media, however the impacts produced by instruction-based models had gaps in the production of certain categories of impacts in comparison to fine-tuned models. This research highlights a potential bias in state-of-the-art LLMs when used for anticipating impacts and demonstrates the advantages of aligning smaller LLMs with a diverse range of impacts, such as those reflected in the news media, to better reflect such impacts during anticipatory exercises.","sentences":["Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development.","An understudied approach to such anticipation is the use of LLMs to enhance and guide this process.","Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks.","Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating.","In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against.","By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts.","We then evaluate both instruction-based (GPT-4 and Mistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3) using a sample from this baseline.","We find that the generated impacts using Mistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively on par with impacts generated using a larger scale model such as GPT-4.","Moreover, we find that these LLMs generate impacts that largely reflect the taxonomy of negative impacts identified in the news media, however the impacts produced by instruction-based models had gaps in the production of certain categories of impacts in comparison to fine-tuned models.","This research highlights a potential bias in state-of-the-art LLMs when used for anticipating impacts and demonstrates the advantages of aligning smaller LLMs with a diverse range of impacts, such as those reflected in the news media, to better reflect such impacts during anticipatory exercises."],"url":"http://arxiv.org/abs/2401.18028v1"}
{"created":"2024-01-31 17:38:34","title":"Benchmarking Private Population Data Release Mechanisms: Synthetic Data vs. TopDown","abstract":"Differential privacy (DP) is increasingly used to protect the release of hierarchical, tabular population data, such as census data. A common approach for implementing DP in this setting is to release noisy responses to a predefined set of queries. For example, this is the approach of the TopDown algorithm used by the US Census Bureau. Such methods have an important shortcoming: they cannot answer queries for which they were not optimized. An appealing alternative is to generate DP synthetic data, which is drawn from some generating distribution. Like the TopDown method, synthetic data can also be optimized to answer specific queries, while also allowing the data user to later submit arbitrary queries over the synthetic population data. To our knowledge, there has not been a head-to-head empirical comparison of these approaches. This study conducts such a comparison between the TopDown algorithm and private synthetic data generation to determine how accuracy is affected by query complexity, in-distribution vs. out-of-distribution queries, and privacy guarantees. Our results show that for in-distribution queries, the TopDown algorithm achieves significantly better privacy-fidelity tradeoffs than any of the synthetic data methods we evaluated; for instance, in our experiments, TopDown achieved at least $20\\times$ lower error on counting queries than the leading synthetic data method at the same privacy budget. Our findings suggest guidelines for practitioners and the synthetic data research community.","sentences":["Differential privacy (DP) is increasingly used to protect the release of hierarchical, tabular population data, such as census data.","A common approach for implementing DP in this setting is to release noisy responses to a predefined set of queries.","For example, this is the approach of the TopDown algorithm used by the US Census Bureau.","Such methods have an important shortcoming: they cannot answer queries for which they were not optimized.","An appealing alternative is to generate DP synthetic data, which is drawn from some generating distribution.","Like the TopDown method, synthetic data can also be optimized to answer specific queries, while also allowing the data user to later submit arbitrary queries over the synthetic population data.","To our knowledge, there has not been a head-to-head empirical comparison of these approaches.","This study conducts such a comparison between the TopDown algorithm and private synthetic data generation to determine how accuracy is affected by query complexity, in-distribution vs. out-of-distribution queries, and privacy guarantees.","Our results show that for in-distribution queries, the TopDown algorithm achieves significantly better privacy-fidelity tradeoffs than any of the synthetic data methods we evaluated; for instance, in our experiments, TopDown achieved at least $20\\times$ lower error on counting queries than the leading synthetic data method at the same privacy budget.","Our findings suggest guidelines for practitioners and the synthetic data research community."],"url":"http://arxiv.org/abs/2401.18024v1"}
{"created":"2024-01-31 17:29:16","title":"Joining Entities Across Relation and Graph with a Unified Model","abstract":"This paper introduces RG (Relational Genetic) model, a revised relational model to represent graph-structured data in RDBMS while preserving its topology, for efficiently and effectively extracting data in different formats from disparate sources. Along with: (a) SQL$_\\delta$, an SQL dialect augmented with graph pattern queries and tuple-vertex joins, such that one can extract graph properties via graph pattern matching, and \"semantically\" match entities across relations and graphs; (b) a logical representation of graphs in RDBMS, which introduces an exploration operator for efficient pattern querying, supports also browsing and updating graph-structured data; and (c) a strategy to uniformly evaluate SQL, pattern and hybrid queries that join tuples and vertices, all inside an RDBMS by leveraging its optimizer without performance degradation on switching different execution engines. A lightweight system, WhiteDB, is developed as an implementation to evaluate the benefits it can actually bring on real-life data. We empirically verified that the RG model enables the graph pattern queries to be answered as efficiently as in native graph engines; can consider the access on graph and relation in any order for optimal plan; and supports effective data enrichment.","sentences":["This paper introduces RG (Relational Genetic) model, a revised relational model to represent graph-structured data in RDBMS while preserving its topology, for efficiently and effectively extracting data in different formats from disparate sources.","Along with: (a) SQL$_\\delta$, an SQL dialect augmented with graph pattern queries and tuple-vertex joins, such that one can extract graph properties via graph pattern matching, and \"semantically\" match entities across relations and graphs; (b) a logical representation of graphs in RDBMS, which introduces an exploration operator for efficient pattern querying, supports also browsing and updating graph-structured data; and (c) a strategy to uniformly evaluate SQL, pattern and hybrid queries that join tuples and vertices, all inside an RDBMS by leveraging its optimizer without performance degradation on switching different execution engines.","A lightweight system, WhiteDB, is developed as an implementation to evaluate the benefits it can actually bring on real-life data.","We empirically verified that the RG model enables the graph pattern queries to be answered as efficiently as in native graph engines; can consider the access on graph and relation in any order for optimal plan; and supports effective data enrichment."],"url":"http://arxiv.org/abs/2401.18019v1"}
{"created":"2024-01-31 16:55:44","title":"Development and Adaptation of Robotic Vision in the Real-World: the Challenge of Door Detection","abstract":"Mobile service robots are increasingly prevalent in human-centric, real-world domains, operating autonomously in unconstrained indoor environments. In such a context, robotic vision plays a central role in enabling service robots to perceive high-level environmental features from visual observations. Despite the data-driven approaches based on deep learning push the boundaries of vision systems, applying these techniques to real-world robotic scenarios presents unique methodological challenges. Traditional models fail to represent the challenging perception constraints typical of service robots and must be adapted for the specific environment where robots finally operate. We propose a method leveraging photorealistic simulations that balances data quality and acquisition costs for synthesizing visual datasets from the robot perspective used to train deep architectures. Then, we show the benefits in qualifying a general detector for the target domain in which the robot is deployed, showing also the trade-off between the effort for obtaining new examples from such a setting and the performance gain. In our extensive experimental campaign, we focus on the door detection task (namely recognizing the presence and the traversability of doorways) that, in dynamic settings, is useful to infer the topology of the map. Our findings are validated in a real-world robot deployment, comparing prominent deep-learning models and demonstrating the effectiveness of our approach in practical settings.","sentences":["Mobile service robots are increasingly prevalent in human-centric, real-world domains, operating autonomously in unconstrained indoor environments.","In such a context, robotic vision plays a central role in enabling service robots to perceive high-level environmental features from visual observations.","Despite the data-driven approaches based on deep learning push the boundaries of vision systems, applying these techniques to real-world robotic scenarios presents unique methodological challenges.","Traditional models fail to represent the challenging perception constraints typical of service robots and must be adapted for the specific environment where robots finally operate.","We propose a method leveraging photorealistic simulations that balances data quality and acquisition costs for synthesizing visual datasets from the robot perspective used to train deep architectures.","Then, we show the benefits in qualifying a general detector for the target domain in which the robot is deployed, showing also the trade-off between the effort for obtaining new examples from such a setting and the performance gain.","In our extensive experimental campaign, we focus on the door detection task (namely recognizing the presence and the traversability of doorways) that, in dynamic settings, is useful to infer the topology of the map.","Our findings are validated in a real-world robot deployment, comparing prominent deep-learning models and demonstrating the effectiveness of our approach in practical settings."],"url":"http://arxiv.org/abs/2401.17996v1"}
{"created":"2024-01-31 16:44:20","title":"Shrub of a thousand faces: an individual segmentation from satellite images using deep learning","abstract":"Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems. Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision. Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes. However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain). In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectively develop and externally validate the model. We also propose a new shrub-tailored evaluation algorithm based on a new metric called Multiple Intersections over Ground Truth Area (MIoGTA) to assess and optimize the model shrub delineation performance. Finally, we deploy the developed model for the first time to generate a wall-to-wall map of Juniperus individuals.   The experimental results demonstrate the efficiency of our dual data construction approach in overcoming the limitations associated with traditional field survey methods. They also highlight the robustness of MIoGTA metric in evaluating instance segmentation models on species with complex growth patterns showing more resilience against data annotation uncertainty. Furthermore, they show the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in delineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%, respectively.","sentences":["Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems.","Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision.","Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes.","However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   ","This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain).","In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectively develop and externally validate the model.","We also propose a new shrub-tailored evaluation algorithm based on a new metric called Multiple Intersections over Ground Truth Area (MIoGTA) to assess and optimize the model shrub delineation performance.","Finally, we deploy the developed model for the first time to generate a wall-to-wall map of Juniperus individuals.   ","The experimental results demonstrate the efficiency of our dual data construction approach in overcoming the limitations associated with traditional field survey methods.","They also highlight the robustness of MIoGTA metric in evaluating instance segmentation models on species with complex growth patterns showing more resilience against data annotation uncertainty.","Furthermore, they show the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in delineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%, respectively."],"url":"http://arxiv.org/abs/2401.17985v1"}
{"created":"2024-01-31 16:08:01","title":"University Students Motives and Challenges in Utilising Institutional Repository Resources","abstract":"One of the core functions of an academic institution is to generate knowledge, disseminate it to the intended audiences, and preserve it for future use. Academic institutions are now establishing Institutional Repositories (IRs) to collect produced resources to facilitate accessibility, dissemination, utilization, and management of intellectual materials produced within an institution. This study aimed to assess postgraduate students motives for utilizing IR resources and the challenges they encounter when utilizing IR resources at the University of Dar es Salaam. This study was conducted using a descriptive study design whereby it used both qualitative and quantitative research approaches. The population of this study comprised postgraduate students, librarians, and ICT personnel from the University of Dar es Salaam. A sample of 102 respondents was drawn conveniently and purposively for this study. Data were collected through questionnaires, interviews, as well as a review of documentary sources. Quantitative data were analyzed through a Version 16 Statistics Package for Social Science and qualitative data were analyzed using content analysis. The findings indicate that access to fulltext documents, the relevance of IR resources, and easy searching of the materials in the repository system motivate the utilization of IR resources. However, several challenges impede the utilization of these resources including unreliable internet access, inaccessibility of full-text and lack of guiding policy have been revealed as the major challenges toward utilization of IR resources. The study recommends training postgraduate students on the general use of IRs. Also, the University management should develop an IR policy that will guide the utilization of IR resources","sentences":["One of the core functions of an academic institution is to generate knowledge, disseminate it to the intended audiences, and preserve it for future use.","Academic institutions are now establishing Institutional Repositories (IRs) to collect produced resources to facilitate accessibility, dissemination, utilization, and management of intellectual materials produced within an institution.","This study aimed to assess postgraduate students motives for utilizing IR resources and the challenges they encounter when utilizing IR resources at the University of Dar es Salaam.","This study was conducted using a descriptive study design whereby it used both qualitative and quantitative research approaches.","The population of this study comprised postgraduate students, librarians, and ICT personnel from the University of Dar es Salaam.","A sample of 102 respondents was drawn conveniently and purposively for this study.","Data were collected through questionnaires, interviews, as well as a review of documentary sources.","Quantitative data were analyzed through a Version 16 Statistics Package for Social Science and qualitative data were analyzed using content analysis.","The findings indicate that access to fulltext documents, the relevance of IR resources, and easy searching of the materials in the repository system motivate the utilization of IR resources.","However, several challenges impede the utilization of these resources including unreliable internet access, inaccessibility of full-text and lack of guiding policy have been revealed as the major challenges toward utilization of IR resources.","The study recommends training postgraduate students on the general use of IRs.","Also, the University management should develop an IR policy that will guide the utilization of IR resources"],"url":"http://arxiv.org/abs/2401.17959v1"}
{"created":"2024-01-31 15:59:16","title":"Error-Tolerant E-Discovery Protocols","abstract":"We consider the multi-party classification problem introduced by Dong, Hartline, and Vijayaraghavan (2022) in the context of electronic discovery (e-discovery). Based on a request for production from the requesting party, the responding party is required to provide documents that are responsive to the request except for those that are legally privileged. Our goal is to find a protocol that verifies that the responding party sends almost all responsive documents while minimizing the disclosure of non-responsive documents. We provide protocols in the challenging non-realizable setting, where the instance may not be perfectly separated by a linear classifier. We demonstrate empirically that our protocol successfully manages to find almost all relevant documents, while incurring only a small disclosure of non-responsive documents. We complement this with a theoretical analysis of our protocol in the single-dimensional setting, and other experiments on simulated data which suggest that the non-responsive disclosure incurred by our protocol may be unavoidable.","sentences":["We consider the multi-party classification problem introduced by Dong, Hartline, and Vijayaraghavan (2022) in the context of electronic discovery (e-discovery).","Based on a request for production from the requesting party, the responding party is required to provide documents that are responsive to the request except for those that are legally privileged.","Our goal is to find a protocol that verifies that the responding party sends almost all responsive documents while minimizing the disclosure of non-responsive documents.","We provide protocols in the challenging non-realizable setting, where the instance may not be perfectly separated by a linear classifier.","We demonstrate empirically that our protocol successfully manages to find almost all relevant documents, while incurring only a small disclosure of non-responsive documents.","We complement this with a theoretical analysis of our protocol in the single-dimensional setting, and other experiments on simulated data which suggest that the non-responsive disclosure incurred by our protocol may be unavoidable."],"url":"http://arxiv.org/abs/2401.17952v1"}
{"created":"2024-01-31 15:33:29","title":"GuardFS: a File System for Integrated Detection and Mitigation of Linux-based Ransomware","abstract":"Although ransomware has received broad attention in media and research, this evolving threat vector still poses a systematic threat. Related literature has explored their detection using various approaches leveraging Machine and Deep Learning. While these approaches are effective in detecting malware, they do not answer how to use this intelligence to protect against threats, raising concerns about their applicability in a hostile environment. Solutions that focus on mitigation rarely explore how to prevent and not just alert or halt its execution, especially when considering Linux-based samples. This paper presents GuardFS, a file system-based approach to investigate the integration of detection and mitigation of ransomware. Using a bespoke overlay file system, data is extracted before files are accessed. Models trained on this data are used by three novel defense configurations that obfuscate, delay, or track access to the file system. The experiments on GuardFS test the configurations in a reactive setting. The results demonstrate that although data loss cannot be completely prevented, it can be significantly reduced. Usability and performance analysis demonstrate that the defense effectiveness of the configurations relates to their impact on resource consumption and usability.","sentences":["Although ransomware has received broad attention in media and research, this evolving threat vector still poses a systematic threat.","Related literature has explored their detection using various approaches leveraging Machine and Deep Learning.","While these approaches are effective in detecting malware, they do not answer how to use this intelligence to protect against threats, raising concerns about their applicability in a hostile environment.","Solutions that focus on mitigation rarely explore how to prevent and not just alert or halt its execution, especially when considering Linux-based samples.","This paper presents GuardFS, a file system-based approach to investigate the integration of detection and mitigation of ransomware.","Using a bespoke overlay file system, data is extracted before files are accessed.","Models trained on this data are used by three novel defense configurations that obfuscate, delay, or track access to the file system.","The experiments on GuardFS test the configurations in a reactive setting.","The results demonstrate that although data loss cannot be completely prevented, it can be significantly reduced.","Usability and performance analysis demonstrate that the defense effectiveness of the configurations relates to their impact on resource consumption and usability."],"url":"http://arxiv.org/abs/2401.17917v1"}
{"created":"2024-01-31 15:32:44","title":"Source-free Domain Adaptive Object Detection in Remote Sensing Images","abstract":"Recent studies have used unsupervised domain adaptive object detection (UDAOD) methods to bridge the domain gap in remote sensing (RS) images. However, UDAOD methods typically assume that the source domain data can be accessed during the domain adaptation process. This setting is often impractical in the real world due to RS data privacy and transmission difficulty. To address this challenge, we propose a practical source-free object detection (SFOD) setting for RS images, which aims to perform target domain adaptation using only the source pre-trained model. We propose a new SFOD method for RS images consisting of two parts: perturbed domain generation and alignment. The proposed multilevel perturbation constructs the perturbed domain in a simple yet efficient form by perturbing the domain-variant features at the image level and feature level according to the color and style bias. The proposed multilevel alignment calculates feature and label consistency between the perturbed domain and the target domain across the teacher-student network, and introduces the distillation of feature prototype to mitigate the noise of pseudo-labels. By requiring the detector to be consistent in the perturbed domain and the target domain, the detector is forced to focus on domaininvariant features. Extensive results of three synthetic-to-real experiments and three cross-sensor experiments have validated the effectiveness of our method which does not require access to source domain RS images. Furthermore, experiments on computer vision datasets show that our method can be extended to other fields as well. Our code will be available at: https://weixliu.github.io/ .","sentences":["Recent studies have used unsupervised domain adaptive object detection (UDAOD) methods to bridge the domain gap in remote sensing (RS) images.","However, UDAOD methods typically assume that the source domain data can be accessed during the domain adaptation process.","This setting is often impractical in the real world due to RS data privacy and transmission difficulty.","To address this challenge, we propose a practical source-free object detection (SFOD) setting for RS images, which aims to perform target domain adaptation using only the source pre-trained model.","We propose a new SFOD method for RS images consisting of two parts: perturbed domain generation and alignment.","The proposed multilevel perturbation constructs the perturbed domain in a simple yet efficient form by perturbing the domain-variant features at the image level and feature level according to the color and style bias.","The proposed multilevel alignment calculates feature and label consistency between the perturbed domain and the target domain across the teacher-student network, and introduces the distillation of feature prototype to mitigate the noise of pseudo-labels.","By requiring the detector to be consistent in the perturbed domain and the target domain, the detector is forced to focus on domaininvariant features.","Extensive results of three synthetic-to-real experiments and three cross-sensor experiments have validated the effectiveness of our method which does not require access to source domain RS images.","Furthermore, experiments on computer vision datasets show that our method can be extended to other fields as well.","Our code will be available at: https://weixliu.github.io/ ."],"url":"http://arxiv.org/abs/2401.17916v1"}
{"created":"2024-01-31 15:15:41","title":"Controllable Dense Captioner with Multimodal Embedding Bridging","abstract":"In this paper, we propose a controllable dense captioner (ControlCap), which accommodates user's intention to dense captioning by introducing linguistic guidance. ControlCap is defined as a multimodal embedding bridging architecture, which comprises multimodal embedding generation (MEG) module and bi-directional embedding bridging (BEB) module. While MEG module represents objects/regions by combining embeddings of detailed information with context-aware ones, it also endows ControlCap the adaptability to specialized controls by utilizing them as linguistic guidance. BEB module aligns the linguistic guidance with visual embeddings through borrowing/returning features from/to the visual domain and gathering such features to predict text descriptions. Experiments on Visual Genome and VG-COCO datasets show that ControlCap respectively outperforms the state-of-the-art methods by 1.5% and 3.7% (mAP). Last but not least, with the capability of converting region-category pairs to region-text pairs, ControlCap is able to act as a powerful data engine for dense captioning. Code is available at https://github.com/callsys/ControlCap.","sentences":["In this paper, we propose a controllable dense captioner (ControlCap), which accommodates user's intention to dense captioning by introducing linguistic guidance.","ControlCap is defined as a multimodal embedding bridging architecture, which comprises multimodal embedding generation (MEG) module and bi-directional embedding bridging (BEB) module.","While MEG module represents objects/regions by combining embeddings of detailed information with context-aware ones, it also endows ControlCap the adaptability to specialized controls by utilizing them as linguistic guidance.","BEB module aligns the linguistic guidance with visual embeddings through borrowing/returning features from/to the visual domain and gathering such features to predict text descriptions.","Experiments on Visual Genome and VG-COCO datasets show that ControlCap respectively outperforms the state-of-the-art methods by 1.5% and 3.7% (mAP).","Last but not least, with the capability of converting region-category pairs to region-text pairs, ControlCap is able to act as a powerful data engine for dense captioning.","Code is available at https://github.com/callsys/ControlCap."],"url":"http://arxiv.org/abs/2401.17910v1"}
{"created":"2024-01-31 14:53:33","title":"The inherent randomness of news virality on social media","abstract":"Initially conceived for entertainment, social media platforms have profoundly transformed the dissemination of information and consequently reshaped the dynamics of agenda-setting. In this scenario, understanding the factors that capture audience attention and drive viral content is crucial. Employing Gibrat's Law, which posits that an entity's growth rate is unrelated to its size, we examine the engagement growth dynamics of news outlets on social media. Our analysis encloses the Facebook historical data of over a thousand news outlets, encompassing approximately 57 million posts in four European languages from 2008 to the end of 2022. We discover universal growth dynamics according to which news virality is independent of the traditional size or engagement with the outlet. Moreover, our analysis reveals a significant long-term impact of news source reliability on engagement growth, with engagement induced by unreliable sources decreasing over time. We conclude the paper by presenting a statistical model replicating the observed growth dynamics.","sentences":["Initially conceived for entertainment, social media platforms have profoundly transformed the dissemination of information and consequently reshaped the dynamics of agenda-setting.","In this scenario, understanding the factors that capture audience attention and drive viral content is crucial.","Employing Gibrat's Law, which posits that an entity's growth rate is unrelated to its size, we examine the engagement growth dynamics of news outlets on social media.","Our analysis encloses the Facebook historical data of over a thousand news outlets, encompassing approximately 57 million posts in four European languages from 2008 to the end of 2022.","We discover universal growth dynamics according to which news virality is independent of the traditional size or engagement with the outlet.","Moreover, our analysis reveals a significant long-term impact of news source reliability on engagement growth, with engagement induced by unreliable sources decreasing over time.","We conclude the paper by presenting a statistical model replicating the observed growth dynamics."],"url":"http://arxiv.org/abs/2401.17890v1"}
{"created":"2024-01-31 14:37:06","title":"Graph Attention-based Reinforcement Learning for Trajectory Design and Resource Assignment in Multi-UAV Assisted Communication","abstract":"In the multiple unmanned aerial vehicle (UAV)- assisted downlink communication, it is challenging for UAV base stations (UAV BSs) to realize trajectory design and resource assignment in unknown environments. The cooperation and competition between UAV BSs in the communication network leads to a Markov game problem. Multi-agent reinforcement learning is a significant solution for the above decision-making. However, there are still many common issues, such as the instability of the system and low utilization of historical data, that limit its application. In this paper, a novel graph-attention multi-agent trust region (GA-MATR) reinforcement learning framework is proposed to solve the multi-UAV assisted communication problem. Graph recurrent network is introduced to process and analyze complex topology of the communication network, so as to extract useful information and patterns from observational information. The attention mechanism provides additional weighting for conveyed information, so that the critic network can accurately evaluate the value of behavior for UAV BSs. This provides more reliable feedback signals and helps the actor network update the strategy more effectively. Ablation simulations indicate that the proposed approach attains improved convergence over the baselines. UAV BSs learn the optimal communication strategies to achieve their maximum cumulative rewards. Additionally, multi-agent trust region method with monotonic convergence provides an estimated Nash equilibrium for the multi-UAV assisted communication Markov game.","sentences":["In the multiple unmanned aerial vehicle (UAV)- assisted downlink communication, it is challenging for UAV base stations (UAV BSs) to realize trajectory design and resource assignment in unknown environments.","The cooperation and competition between UAV BSs in the communication network leads to a Markov game problem.","Multi-agent reinforcement learning is a significant solution for the above decision-making.","However, there are still many common issues, such as the instability of the system and low utilization of historical data, that limit its application.","In this paper, a novel graph-attention multi-agent trust region (GA-MATR) reinforcement learning framework is proposed to solve the multi-UAV assisted communication problem.","Graph recurrent network is introduced to process and analyze complex topology of the communication network, so as to extract useful information and patterns from observational information.","The attention mechanism provides additional weighting for conveyed information, so that the critic network can accurately evaluate the value of behavior for UAV BSs.","This provides more reliable feedback signals and helps the actor network update the strategy more effectively.","Ablation simulations indicate that the proposed approach attains improved convergence over the baselines.","UAV BSs learn the optimal communication strategies to achieve their maximum cumulative rewards.","Additionally, multi-agent trust region method with monotonic convergence provides an estimated Nash equilibrium for the multi-UAV assisted communication Markov game."],"url":"http://arxiv.org/abs/2401.17880v1"}
{"created":"2024-01-31 14:36:44","title":"A Survey on Data-Centric Recommender Systems","abstract":"Recommender systems (RS) have become essential tools for mitigating information overload in a range of real-world scenarios. Recent trends in RS have seen a paradigm shift, moving the spotlight from model-centric innovations to the importance of data quality and quantity. This evolution has given rise to the concept of data-centric recommender systems (Data-Centric RS), marking a significant development in the field. This survey provides the first systematic overview of Data-Centric RS, covering 1) the foundational concepts of recommendation data and Data-Centric RS; 2) three primary issues in recommendation data; 3) recent research developed to address these issues; and 4) several potential future directions in Data-Centric RS.","sentences":["Recommender systems (RS) have become essential tools for mitigating information overload in a range of real-world scenarios.","Recent trends in RS have seen a paradigm shift, moving the spotlight from model-centric innovations to the importance of data quality and quantity.","This evolution has given rise to the concept of data-centric recommender systems (Data-Centric RS), marking a significant development in the field.","This survey provides the first systematic overview of Data-Centric RS, covering 1) the foundational concepts of recommendation data and Data-Centric RS; 2) three primary issues in recommendation data; 3) recent research developed to address these issues; and 4) several potential future directions in Data-Centric RS."],"url":"http://arxiv.org/abs/2401.17878v1"}
{"created":"2024-01-31 14:32:56","title":"VR-based generation of photorealistic synthetic data for training hand-object tracking models","abstract":"Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images. To address these issues, we present \"blender-hoisynth\", an interactive synthetic data generator based on the Blender software. Blender-hoisynth can scalably generate and automatically annotate visual HOI training data. Other competing approaches usually generate synthetic HOI data compeletely without human input. While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent. With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware. The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it. We show that there is no significant degradation in the model performance despite the data replacement.","sentences":["Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training.","Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images.","To address these issues, we present \"blender-hoisynth\", an interactive synthetic data generator based on the Blender software.","Blender-hoisynth can scalably generate and automatically annotate visual HOI training data.","Other competing approaches usually generate synthetic HOI data compeletely without human input.","While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent.","With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware.","The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it.","We show that there is no significant degradation in the model performance despite the data replacement."],"url":"http://arxiv.org/abs/2401.17874v1"}
{"created":"2024-01-31 14:23:51","title":"Manipulating Predictions over Discrete Inputs in Machine Teaching","abstract":"Machine teaching often involves the creation of an optimal (typically minimal) dataset to help a model (referred to as the `student') achieve specific goals given by a teacher. While abundant in the continuous domain, the studies on the effectiveness of machine teaching in the discrete domain are relatively limited. This paper focuses on machine teaching in the discrete domain, specifically on manipulating student models' predictions based on the goals of teachers via changing the training data efficiently. We formulate this task as a combinatorial optimization problem and solve it by proposing an iterative searching algorithm. Our algorithm demonstrates significant numerical merit in the scenarios where a teacher attempts at correcting erroneous predictions to improve the student's models, or maliciously manipulating the model to misclassify some specific samples to the target class aligned with his personal profits. Experimental results show that our proposed algorithm can have superior performance in effectively and efficiently manipulating the predictions of the model, surpassing conventional baselines.","sentences":["Machine teaching often involves the creation of an optimal (typically minimal) dataset to help a model (referred to as the `student') achieve specific goals given by a teacher.","While abundant in the continuous domain, the studies on the effectiveness of machine teaching in the discrete domain are relatively limited.","This paper focuses on machine teaching in the discrete domain, specifically on manipulating student models' predictions based on the goals of teachers via changing the training data efficiently.","We formulate this task as a combinatorial optimization problem and solve it by proposing an iterative searching algorithm.","Our algorithm demonstrates significant numerical merit in the scenarios where a teacher attempts at correcting erroneous predictions to improve the student's models, or maliciously manipulating the model to misclassify some specific samples to the target class aligned with his personal profits.","Experimental results show that our proposed algorithm can have superior performance in effectively and efficiently manipulating the predictions of the model, surpassing conventional baselines."],"url":"http://arxiv.org/abs/2401.17865v1"}
{"created":"2024-01-31 14:17:52","title":"Beyond Numbers: Creating Analogies to Enhance Data Comprehension and Communication with Generative AI","abstract":"Unfamiliar measurements usually hinder readers from grasping the scale of the numerical data, understanding the content, and feeling engaged with the context. To enhance data comprehension and communication, we leverage analogies to bridge the gap between abstract data and familiar measurements. In this work, we first conduct semi-structured interviews with design experts to identify design problems and summarize design considerations. Then, we collect an analogy dataset of 138 cases from various online sources. Based on the collected dataset, we characterize a design space for creating data analogies. Next, we build a prototype system, AnalogyMate, that automatically suggests data analogies, their corresponding design solutions, and generated visual representations powered by generative AI. The study results show the usefulness of AnalogyMate in aiding the creation process of data analogies and the effectiveness of data analogy in enhancing data comprehension and communication.","sentences":["Unfamiliar measurements usually hinder readers from grasping the scale of the numerical data, understanding the content, and feeling engaged with the context.","To enhance data comprehension and communication, we leverage analogies to bridge the gap between abstract data and familiar measurements.","In this work, we first conduct semi-structured interviews with design experts to identify design problems and summarize design considerations.","Then, we collect an analogy dataset of 138 cases from various online sources.","Based on the collected dataset, we characterize a design space for creating data analogies.","Next, we build a prototype system, AnalogyMate, that automatically suggests data analogies, their corresponding design solutions, and generated visual representations powered by generative AI.","The study results show the usefulness of AnalogyMate in aiding the creation process of data analogies and the effectiveness of data analogy in enhancing data comprehension and communication."],"url":"http://arxiv.org/abs/2401.17856v1"}
{"created":"2024-01-31 13:28:07","title":"Privacy-preserving data release leveraging optimal transport and particle gradient descent","abstract":"We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.","sentences":["We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government.","Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals.","In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent.","Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints."],"url":"http://arxiv.org/abs/2401.17823v1"}
{"created":"2024-01-31 12:53:11","title":"SimAda: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes","abstract":"Segment anything model (SAM) has demonstrated excellent generalization capabilities in common vision scenarios, yet lacking an understanding of specialized data. Although numerous works have focused on optimizing SAM for downstream tasks, these task-specific approaches usually limit the generalizability to other downstream tasks. In this paper, we aim to investigate the impact of the general vision modules on finetuning SAM and enable them to generalize across all downstream tasks. We propose a simple unified framework called SimAda for adapting SAM in underperformed scenes. Specifically, our framework abstracts the general modules of different methods into basic design elements, and we design four variants based on a shared theoretical framework. SimAda is simple yet effective, which removes all dataset-specific designs and focuses solely on general optimization, ensuring that SimAda can be applied to all SAM-based and even Transformer-based models. We conduct extensive experiments on nine datasets of six downstream tasks. The results demonstrate that SimAda significantly improves the performance of SAM on multiple downstream tasks and achieves state-of-the-art performance on most of them, without requiring task-specific designs. Code is available at: https://github.com/zongzi13545329/SimAda","sentences":["Segment anything model (SAM) has demonstrated excellent generalization capabilities in common vision scenarios, yet lacking an understanding of specialized data.","Although numerous works have focused on optimizing SAM for downstream tasks, these task-specific approaches usually limit the generalizability to other downstream tasks.","In this paper, we aim to investigate the impact of the general vision modules on finetuning SAM and enable them to generalize across all downstream tasks.","We propose a simple unified framework called SimAda for adapting SAM in underperformed scenes.","Specifically, our framework abstracts the general modules of different methods into basic design elements, and we design four variants based on a shared theoretical framework.","SimAda is simple yet effective, which removes all dataset-specific designs and focuses solely on general optimization, ensuring that SimAda can be applied to all SAM-based and even Transformer-based models.","We conduct extensive experiments on nine datasets of six downstream tasks.","The results demonstrate that SimAda significantly improves the performance of SAM on multiple downstream tasks and achieves state-of-the-art performance on most of them, without requiring task-specific designs.","Code is available at: https://github.com/zongzi13545329/SimAda"],"url":"http://arxiv.org/abs/2401.17803v1"}
{"created":"2024-01-31 12:52:10","title":"Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning","abstract":"Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two tasks. By developing model loss from multiple tasks, we can learn effective representations for downstream forecasting task. Extensive experiments, in comparison with state-of-the-arts, well demonstrate the effectiveness of DE-TSMCL, where the maximum improvement can reach to 27.3%.","sentences":["Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal.","However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data.","To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting.","Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences.","Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series.","Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process.","Finally, we jointly optimize the above two tasks.","By developing model loss from multiple tasks, we can learn effective representations for downstream forecasting task.","Extensive experiments, in comparison with state-of-the-arts, well demonstrate the effectiveness of DE-TSMCL, where the maximum improvement can reach to 27.3%."],"url":"http://arxiv.org/abs/2401.17802v1"}
{"created":"2024-01-31 12:51:26","title":"Dance-to-Music Generation with Encoder-based Textual Inversion of Diffusion Models","abstract":"The harmonious integration of music with dance movements is pivotal in vividly conveying the artistic essence of dance. This alignment also significantly elevates the immersive quality of gaming experiences and animation productions. While there has been remarkable advancement in creating high-fidelity music from textual descriptions, current methodologies mainly concentrate on modulating overarching characteristics such as genre and emotional tone. They often overlook the nuanced management of temporal rhythm, which is indispensable in crafting music for dance, since it intricately aligns the musical beats with the dancers' movements. Recognizing this gap, we propose an encoder-based textual inversion technique for augmenting text-to-music models with visual control, facilitating personalized music generation. Specifically, we develop dual-path rhythm-genre inversion to effectively integrate the rhythm and genre of a dance motion sequence into the textual space of a text-to-music model. Contrary to the classical textual inversion method, which directly updates text embeddings to reconstruct a single target object, our approach utilizes separate rhythm and genre encoders to obtain text embeddings for two pseudo-words, adapting to the varying rhythms and genres. To achieve a more accurate evaluation, we propose improved evaluation metrics for rhythm alignment. We demonstrate that our approach outperforms state-of-the-art methods across multiple evaluation metrics. Furthermore, our method seamlessly adapts to in-the-wild data and effectively integrates with the inherent text-guided generation capability of the pre-trained model. Samples are available at \\url{https://youtu.be/D7XDwtH1YwE}.","sentences":["The harmonious integration of music with dance movements is pivotal in vividly conveying the artistic essence of dance.","This alignment also significantly elevates the immersive quality of gaming experiences and animation productions.","While there has been remarkable advancement in creating high-fidelity music from textual descriptions, current methodologies mainly concentrate on modulating overarching characteristics such as genre and emotional tone.","They often overlook the nuanced management of temporal rhythm, which is indispensable in crafting music for dance, since it intricately aligns the musical beats with the dancers' movements.","Recognizing this gap, we propose an encoder-based textual inversion technique for augmenting text-to-music models with visual control, facilitating personalized music generation.","Specifically, we develop dual-path rhythm-genre inversion to effectively integrate the rhythm and genre of a dance motion sequence into the textual space of a text-to-music model.","Contrary to the classical textual inversion method, which directly updates text embeddings to reconstruct a single target object, our approach utilizes separate rhythm and genre encoders to obtain text embeddings for two pseudo-words, adapting to the varying rhythms and genres.","To achieve a more accurate evaluation, we propose improved evaluation metrics for rhythm alignment.","We demonstrate that our approach outperforms state-of-the-art methods across multiple evaluation metrics.","Furthermore, our method seamlessly adapts to in-the-wild data and effectively integrates with the inherent text-guided generation capability of the pre-trained model.","Samples are available at \\url{https://youtu.be/D7XDwtH1YwE}."],"url":"http://arxiv.org/abs/2401.17800v1"}
{"created":"2024-01-31 12:48:28","title":"AI-enabled Cyber-Physical In-Orbit Factory -- AI approaches based on digital twin technology for robotic small satellite production","abstract":"With the ever increasing number of active satellites in space, the rising demand for larger formations of small satellites and the commercialization of the space industry (so-called New Space), the realization of manufacturing processes in orbit comes closer to reality. Reducing launch costs and risks, allowing for faster on-demand deployment of individually configured satellites as well as the prospect for possible on-orbit servicing for satellites makes the idea of realizing an in-orbit factory promising. In this paper, we present a novel approach to an in-orbit factory of small satellites covering a digital process twin, AI-based fault detection, and teleoperated robot-control, which are being researched as part of the \"AI-enabled Cyber-Physical In-Orbit Factory\" project. In addition to the integration of modern automation and Industry 4.0 production approaches, the question of how artificial intelligence (AI) and learning approaches can be used to make the production process more robust, fault-tolerant and autonomous is addressed. This lays the foundation for a later realisation of satellite production in space in the form of an in-orbit factory. Central aspect is the development of a robotic AIT (Assembly, Integration and Testing) system where a small satellite could be assembled by a manipulator robot from modular subsystems. Approaches developed to improving this production process with AI include employing neural networks for optical and electrical fault detection of components. Force sensitive measuring and motion training helps to deal with uncertainties and tolerances during assembly. An AI-guided teleoperated control of the robot arm allows for human intervention while a Digital Process Twin represents process data and provides supervision during the whole production process. Approaches and results towards automated satellite production are presented in detail.","sentences":["With the ever increasing number of active satellites in space, the rising demand for larger formations of small satellites and the commercialization of the space industry (so-called New Space), the realization of manufacturing processes in orbit comes closer to reality.","Reducing launch costs and risks, allowing for faster on-demand deployment of individually configured satellites as well as the prospect for possible on-orbit servicing for satellites makes the idea of realizing an in-orbit factory promising.","In this paper, we present a novel approach to an in-orbit factory of small satellites covering a digital process twin, AI-based fault detection, and teleoperated robot-control, which are being researched as part of the \"AI-enabled Cyber-Physical In-Orbit Factory\" project.","In addition to the integration of modern automation and Industry 4.0 production approaches, the question of how artificial intelligence (AI) and learning approaches can be used to make the production process more robust, fault-tolerant and autonomous is addressed.","This lays the foundation for a later realisation of satellite production in space in the form of an in-orbit factory.","Central aspect is the development of a robotic AIT (Assembly, Integration and Testing) system where a small satellite could be assembled by a manipulator robot from modular subsystems.","Approaches developed to improving this production process with AI include employing neural networks for optical and electrical fault detection of components.","Force sensitive measuring and motion training helps to deal with uncertainties and tolerances during assembly.","An AI-guided teleoperated control of the robot arm allows for human intervention while a Digital Process Twin represents process data and provides supervision during the whole production process.","Approaches and results towards automated satellite production are presented in detail."],"url":"http://arxiv.org/abs/2401.17799v1"}
{"created":"2024-01-31 12:45:44","title":"M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval","abstract":"We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP. Upon popular image-text models like CLIP, most current adaptation-based video-text pre-training methods are confronted by three major issues, i.e., noisy data corpus, time-consuming pre-training, and limited performance gain. Towards this end, we conduct a comprehensive study including four critical steps in video-text pre-training. Specifically, we investigate 1) data filtering and refinement, 2) video input type selection, 3) temporal modeling, and 4) video feature enhancement. We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the replacement of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features. We conduct extensive experiments by adapting three image-text foundation models on two refined video-text datasets from different languages, validating the robustness and reproducibility of M2-RAAP for adaptation-based pre-training. Results demonstrate that M2-RAAP yields superior performance with significantly reduced data (-90%) and time consumption (-95%), establishing a new SOTA on four English zero-shot retrieval datasets and two Chinese ones. We are preparing our refined bilingual data annotations and codebase, which will be available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.","sentences":["We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP.","Upon popular image-text models like CLIP, most current adaptation-based video-text pre-training methods are confronted by three major issues, i.e., noisy data corpus, time-consuming pre-training, and limited performance gain.","Towards this end, we conduct a comprehensive study including four critical steps in video-text pre-training.","Specifically, we investigate 1) data filtering and refinement, 2) video input type selection, 3) temporal modeling, and 4) video feature enhancement.","We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the replacement of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features.","We conduct extensive experiments by adapting three image-text foundation models on two refined video-text datasets from different languages, validating the robustness and reproducibility of M2-RAAP for adaptation-based pre-training.","Results demonstrate that M2-RAAP yields superior performance with significantly reduced data (-90%) and time consumption (-95%), establishing a new SOTA on four English zero-shot retrieval datasets and two Chinese ones.","We are preparing our refined bilingual data annotations and codebase, which will be available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP."],"url":"http://arxiv.org/abs/2401.17797v1"}
{"created":"2024-01-31 12:26:59","title":"SDRDPy: An application to graphically visualize the knowledge obtained with supervised descriptive rule algorithms","abstract":"SDRDPy is a desktop application that allows experts an intuitive graphic and tabular representation of the knowledge extracted by any supervised descriptive rule discovery algorithm. The application is able to provide an analysis of the data showing the relevant information of the data set and the relationship between the rules, data and the quality measures associated for each rule regardless of the tool where algorithm has been executed. All of the information is presented in a user-friendly application in order to facilitate expert analysis and also the exportation of reports in different formats.","sentences":["SDRDPy is a desktop application that allows experts an intuitive graphic and tabular representation of the knowledge extracted by any supervised descriptive rule discovery algorithm.","The application is able to provide an analysis of the data showing the relevant information of the data set and the relationship between the rules, data and the quality measures associated for each rule regardless of the tool where algorithm has been executed.","All of the information is presented in a user-friendly application in order to facilitate expert analysis and also the exportation of reports in different formats."],"url":"http://arxiv.org/abs/2401.17783v1"}
{"created":"2024-01-31 12:12:56","title":"SNP-S3: Shared Network Pre-training and Significant Semantic Strengthening for Various Video-Text Tasks","abstract":"We present a framework for learning cross-modal video representations by directly pre-training on raw data to facilitate various downstream video-text tasks. Our main contributions lie in the pre-training framework and proxy tasks. First, based on the shortcomings of two mainstream pixel-level pre-training architectures (limited applications or less efficient), we propose Shared Network Pre-training (SNP). By employing one shared BERT-type network to refine textual and cross-modal features simultaneously, SNP is lightweight and could support various downstream applications. Second, based on the intuition that people always pay attention to several \"significant words\" when understanding a sentence, we propose the Significant Semantic Strengthening (S3) strategy, which includes a novel masking and matching proxy task to promote the pre-training performance. Experiments conducted on three downstream video-text tasks and six datasets demonstrate that, we establish a new state-of-the-art in pixel-level video-text pre-training; we also achieve a satisfactory balance between the pre-training efficiency and the fine-tuning performance. The codebase are available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/snps3_vtp.","sentences":["We present a framework for learning cross-modal video representations by directly pre-training on raw data to facilitate various downstream video-text tasks.","Our main contributions lie in the pre-training framework and proxy tasks.","First, based on the shortcomings of two mainstream pixel-level pre-training architectures (limited applications or less efficient), we propose Shared Network Pre-training (SNP).","By employing one shared BERT-type network to refine textual and cross-modal features simultaneously, SNP is lightweight and could support various downstream applications.","Second, based on the intuition that people always pay attention to several \"significant words\" when understanding a sentence, we propose the Significant Semantic Strengthening (S3) strategy, which includes a novel masking and matching proxy task to promote the pre-training performance.","Experiments conducted on three downstream video-text tasks and six datasets demonstrate that, we establish a new state-of-the-art in pixel-level video-text pre-training; we also achieve a satisfactory balance between the pre-training efficiency and the fine-tuning performance.","The codebase are available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/snps3_vtp."],"url":"http://arxiv.org/abs/2401.17773v1"}
{"created":"2024-01-31 11:09:07","title":"Model-driven development of data intensive applications over cloud resources","abstract":"The proliferation of sensors over the last years has generated large amounts of raw data, forming data streams that need to be processed. In many cases, cloud resources are used for such processing, exploiting their flexibility, but these sensor streaming applications often need to support operational and control actions that have real-time and low-latency requirements that go beyond the cost effective and flexible solutions supported by existing cloud frameworks, such as Apache Kafka, Apache Spark Streaming, or Map-Reduce Streams. In this paper, we describe a model-driven and stepwise refinement methodological approach for streaming applications executed over clouds. The central role is assigned to a set of Petri Net models for specifying functional and non-functional requirements. They support model reuse, and a way to combine formal analysis, simulation, and approximate computation of minimal and maximal boundaries of non-functional requirements when the problem is either mathematically or computationally intractable. We show how our proposal can assist developers in their design and implementation decisions from a performance perspective. Our methodology allows to conduct performance analysis: The methodology is intended for all the engineering process stages, and we can (i) analyse how it can be mapped onto cloud resources, and (ii) obtain key performance indicators, including throughput or economic cost, so that developers are assisted in their development tasks and in their decision taking. In order to illustrate our approach, we make use of the pipelined wavefront array.","sentences":["The proliferation of sensors over the last years has generated large amounts of raw data, forming data streams that need to be processed.","In many cases, cloud resources are used for such processing, exploiting their flexibility, but these sensor streaming applications often need to support operational and control actions that have real-time and low-latency requirements that go beyond the cost effective and flexible solutions supported by existing cloud frameworks, such as Apache Kafka, Apache Spark Streaming, or Map-Reduce Streams.","In this paper, we describe a model-driven and stepwise refinement methodological approach for streaming applications executed over clouds.","The central role is assigned to a set of Petri Net models for specifying functional and non-functional requirements.","They support model reuse, and a way to combine formal analysis, simulation, and approximate computation of minimal and maximal boundaries of non-functional requirements when the problem is either mathematically or computationally intractable.","We show how our proposal can assist developers in their design and implementation decisions from a performance perspective.","Our methodology allows to conduct performance analysis: The methodology is intended for all the engineering process stages, and we can (i) analyse how it can be mapped onto cloud resources, and (ii) obtain key performance indicators, including throughput or economic cost, so that developers are assisted in their development tasks and in their decision taking.","In order to illustrate our approach, we make use of the pipelined wavefront array."],"url":"http://arxiv.org/abs/2401.17747v1"}
{"created":"2024-01-31 11:07:49","title":"Logit Poisoning Attack in Distillation-based Federated Learning and its Countermeasures","abstract":"Distillation-based federated learning has emerged as a promising collaborative learning approach, where clients share the output logit vectors of a public dataset rather than their private model parameters. This practice reduces the risk of privacy invasion attacks and facilitates heterogeneous learning. The landscape of poisoning attacks within distillation-based federated learning is complex, with existing research employing traditional data poisoning strategies targeting the models' parameters. However, these attack schemes primarily have shortcomings rooted in their original designs, which target the model parameters rather than the logit vectors. Furthermore, they do not adequately consider the role of logit vectors in carrying information during the knowledge transfer process. This misalignment results in less efficiency in the context of distillation-based federated learning. Due to the limitations of existing methodologies, our research delves into the intrinsic properties of the logit vector, striving for a more nuanced understanding. We introduce a two-stage scheme for logit poisoning attacks, addressing previous shortcomings. Initially, we collect the local logits, generate the representative vectors, categorize the logit elements within the vector, and design a shuffling table to maximize information entropy. Then, we intentionally scale the shuffled logit vectors to enhance the magnitude of the target vectors. Concurrently, we propose an efficient defense algorithm to counter this new poisoning scheme by calculating the distance between estimated benign vectors and vectors uploaded by users. Through extensive experiments, our study illustrates the significant threat posed by the proposed logit poisoning attack and highlights the effectiveness of our defense algorithm.","sentences":["Distillation-based federated learning has emerged as a promising collaborative learning approach, where clients share the output logit vectors of a public dataset rather than their private model parameters.","This practice reduces the risk of privacy invasion attacks and facilitates heterogeneous learning.","The landscape of poisoning attacks within distillation-based federated learning is complex, with existing research employing traditional data poisoning strategies targeting the models' parameters.","However, these attack schemes primarily have shortcomings rooted in their original designs, which target the model parameters rather than the logit vectors.","Furthermore, they do not adequately consider the role of logit vectors in carrying information during the knowledge transfer process.","This misalignment results in less efficiency in the context of distillation-based federated learning.","Due to the limitations of existing methodologies, our research delves into the intrinsic properties of the logit vector, striving for a more nuanced understanding.","We introduce a two-stage scheme for logit poisoning attacks, addressing previous shortcomings.","Initially, we collect the local logits, generate the representative vectors, categorize the logit elements within the vector, and design a shuffling table to maximize information entropy.","Then, we intentionally scale the shuffled logit vectors to enhance the magnitude of the target vectors.","Concurrently, we propose an efficient defense algorithm to counter this new poisoning scheme by calculating the distance between estimated benign vectors and vectors uploaded by users.","Through extensive experiments, our study illustrates the significant threat posed by the proposed logit poisoning attack and highlights the effectiveness of our defense algorithm."],"url":"http://arxiv.org/abs/2401.17746v1"}
{"created":"2024-01-31 11:00:26","title":"Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance","abstract":"This paper presents Haris, an advanced autonomous mobile robot system for tracking the location of vehicles in crowded car parks using license plate recognition. The system employs simultaneous localization and mapping (SLAM) for autonomous navigation and precise mapping of the parking area, eliminating the need for GPS dependency. In addition, the system utilizes a sophisticated framework using computer vision techniques for object detection and automatic license plate recognition (ALPR) for reading and associating license plate numbers with location data. This information is subsequently synchronized with a back-end service and made accessible to users via a user-friendly mobile app, offering effortless vehicle location and alleviating congestion within the parking facility. The proposed system has the potential to improve the management of short-term large outdoor parking areas in crowded places such as sports stadiums. The demo of the robot can be found on https://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx.","sentences":["This paper presents Haris, an advanced autonomous mobile robot system for tracking the location of vehicles in crowded car parks using license plate recognition.","The system employs simultaneous localization and mapping (SLAM) for autonomous navigation and precise mapping of the parking area, eliminating the need for GPS dependency.","In addition, the system utilizes a sophisticated framework using computer vision techniques for object detection and automatic license plate recognition (ALPR) for reading and associating license plate numbers with location data.","This information is subsequently synchronized with a back-end service and made accessible to users via a user-friendly mobile app, offering effortless vehicle location and alleviating congestion within the parking facility.","The proposed system has the potential to improve the management of short-term large outdoor parking areas in crowded places such as sports stadiums.","The demo of the robot can be found on https://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx."],"url":"http://arxiv.org/abs/2401.17741v1"}
{"created":"2024-01-31 10:58:59","title":"Harnessing Smartwatch Microphone Sensors for Cough Detection and Classification","abstract":"This study investigates the potential of using smartwatches with built-in microphone sensors for monitoring coughs and detecting various cough types. We conducted a study involving 32 participants and collected 9 hours of audio data in a controlled manner. Afterward, we processed this data using a structured approach, resulting in 223 positive cough samples. We further improved the dataset through augmentation techniques and employed a specialized 1D CNN model. This model achieved an impressive accuracy rate of 98.49% while non-walking and 98.2% while walking, showing smartwatches can detect cough. Moreover, our research successfully identified four distinct types of coughs using clustering techniques.","sentences":["This study investigates the potential of using smartwatches with built-in microphone sensors for monitoring coughs and detecting various cough types.","We conducted a study involving 32 participants and collected 9 hours of audio data in a controlled manner.","Afterward, we processed this data using a structured approach, resulting in 223 positive cough samples.","We further improved the dataset through augmentation techniques and employed a specialized 1D CNN model.","This model achieved an impressive accuracy rate of 98.49% while non-walking and 98.2% while walking, showing smartwatches can detect cough.","Moreover, our research successfully identified four distinct types of coughs using clustering techniques."],"url":"http://arxiv.org/abs/2401.17738v1"}
{"created":"2024-01-31 10:57:07","title":"Leveraging Human-Machine Interactions for Computer Vision Dataset Quality Enhancement","abstract":"Large-scale datasets for single-label multi-class classification, such as \\emph{ImageNet-1k}, have been instrumental in advancing deep learning and computer vision. However, a critical and often understudied aspect is the comprehensive quality assessment of these datasets, especially regarding potential multi-label annotation errors. In this paper, we introduce a lightweight, user-friendly, and scalable framework that synergizes human and machine intelligence for efficient dataset validation and quality enhancement. We term this novel framework \\emph{Multilabelfy}. Central to Multilabelfy is an adaptable web-based platform that systematically guides annotators through the re-evaluation process, effectively leveraging human-machine interactions to enhance dataset quality. By using Multilabelfy on the ImageNetV2 dataset, we found that approximately $47.88\\%$ of the images contained at least two labels, underscoring the need for more rigorous assessments of such influential datasets. Furthermore, our analysis showed a negative correlation between the number of potential labels per image and model top-1 accuracy, illuminating a crucial factor in model evaluation and selection. Our open-source framework, Multilabelfy, offers a convenient, lightweight solution for dataset enhancement, emphasizing multi-label proportions. This study tackles major challenges in dataset integrity and provides key insights into model performance evaluation. Moreover, it underscores the advantages of integrating human expertise with machine capabilities to produce more robust models and trustworthy data development. The source code for Multilabelfy will be available at https://github.com/esla/Multilabelfy.   \\keywords{Computer Vision \\and Dataset Quality Enhancement \\and Dataset Validation \\and Human-Computer Interaction \\and Multi-label Annotation.}","sentences":["Large-scale datasets for single-label multi-class classification, such as \\emph{ImageNet-1k}, have been instrumental in advancing deep learning and computer vision.","However, a critical and often understudied aspect is the comprehensive quality assessment of these datasets, especially regarding potential multi-label annotation errors.","In this paper, we introduce a lightweight, user-friendly, and scalable framework that synergizes human and machine intelligence for efficient dataset validation and quality enhancement.","We term this novel framework \\emph{Multilabelfy}.","Central to Multilabelfy is an adaptable web-based platform that systematically guides annotators through the re-evaluation process, effectively leveraging human-machine interactions to enhance dataset quality.","By using Multilabelfy on the ImageNetV2 dataset, we found that approximately $47.88\\%$ of the images contained at least two labels, underscoring the need for more rigorous assessments of such influential datasets.","Furthermore, our analysis showed a negative correlation between the number of potential labels per image and model top-1 accuracy, illuminating a crucial factor in model evaluation and selection.","Our open-source framework, Multilabelfy, offers a convenient, lightweight solution for dataset enhancement, emphasizing multi-label proportions.","This study tackles major challenges in dataset integrity and provides key insights into model performance evaluation.","Moreover, it underscores the advantages of integrating human expertise with machine capabilities to produce more robust models and trustworthy data development.","The source code for Multilabelfy will be available at https://github.com/esla/Multilabelfy.   ","\\keywords{Computer Vision \\and Dataset Quality Enhancement \\and Dataset Validation \\and Human-Computer Interaction \\and Multi-label Annotation.}"],"url":"http://arxiv.org/abs/2401.17736v1"}
{"created":"2024-01-31 10:56:00","title":"Efficient Shape Formation by 3D Hybrid Programmable Matter: An Algorithm for Low Diameter Intermediate Structures","abstract":"This paper considers the shape formation problem within the 3D hybrid model, where a single agent with a strictly limited viewing range and the computational capacity of a deterministic finite automaton manipulates passive tiles through pick-up, movement, and placement actions. The goal is to reconfigure a set of tiles into a specific shape termed an icicle. The icicle, identified as a dense, hole-free structure, is strategically chosen to function as an intermediate shape for more intricate shape formation tasks. It is designed for easy exploration by a finite state agent, enabling the identification of tiles that can be lifted without breaking connectivity. Compared to the line shape, the icicle presents distinct advantages, including a reduced diameter and the presence of multiple removable tiles. We propose an algorithm that transforms an arbitrary initially connected tile structure into an icicle in $\\mathcal{O}(n^3)$ steps, matching the runtime of the line formation algorithm from prior work. Our theoretical contribution is accompanied by an extensive experimental analysis, indicating that our algorithm decreases the diameter of tile structures on average.","sentences":["This paper considers the shape formation problem within the 3D hybrid model, where a single agent with a strictly limited viewing range and the computational capacity of a deterministic finite automaton manipulates passive tiles through pick-up, movement, and placement actions.","The goal is to reconfigure a set of tiles into a specific shape termed an icicle.","The icicle, identified as a dense, hole-free structure, is strategically chosen to function as an intermediate shape for more intricate shape formation tasks.","It is designed for easy exploration by a finite state agent, enabling the identification of tiles that can be lifted without breaking connectivity.","Compared to the line shape, the icicle presents distinct advantages, including a reduced diameter and the presence of multiple removable tiles.","We propose an algorithm that transforms an arbitrary initially connected tile structure into an icicle in $\\mathcal{O}(n^3)$ steps, matching the runtime of the line formation algorithm from prior work.","Our theoretical contribution is accompanied by an extensive experimental analysis, indicating that our algorithm decreases the diameter of tile structures on average."],"url":"http://arxiv.org/abs/2401.17734v1"}
{"created":"2024-01-31 10:54:22","title":"High-performance Racing on Unmapped Tracks using Local Maps","abstract":"Map-based methods for autonomous racing estimate the vehicle's location, which is used to follow a high-level plan. While map-based optimisation methods demonstrate high-performance results, they are limited by requiring a map of the environment. In contrast, mapless methods can operate in unmapped contexts since they directly process raw sensor data (often LiDAR) to calculate commands. However, a major limitation in mapless methods is poor performance due to a lack of optimisation. In response, we propose the local map framework that uses easily extractable, low-level features to build local maps of the visible region that form the input to optimisation-based controllers. Our local map generation extracts the visible racetrack boundaries and calculates a centreline and track widths used for planning. We evaluate our method for simulated F1Tenth autonomous racing using a two-stage trajectory optimisation and tracking strategy and a model predictive controller. Our method achieves lap times that are 8.8% faster than the Follow-The-Gap method and 3.22% faster than end-to-end neural networks due to the optimisation resulting in a faster speed profile. The local map planner is 3.28% slower than global methods that have access to an entire map of the track that can be used for planning. Critically, our approach enables high-speed autonomous racing on unmapped tracks, achieving performance similar to global methods without requiring a track map.","sentences":["Map-based methods for autonomous racing estimate the vehicle's location, which is used to follow a high-level plan.","While map-based optimisation methods demonstrate high-performance results, they are limited by requiring a map of the environment.","In contrast, mapless methods can operate in unmapped contexts since they directly process raw sensor data (often LiDAR) to calculate commands.","However, a major limitation in mapless methods is poor performance due to a lack of optimisation.","In response, we propose the local map framework that uses easily extractable, low-level features to build local maps of the visible region that form the input to optimisation-based controllers.","Our local map generation extracts the visible racetrack boundaries and calculates a centreline and track widths used for planning.","We evaluate our method for simulated F1Tenth autonomous racing using a two-stage trajectory optimisation and tracking strategy and a model predictive controller.","Our method achieves lap times that are 8.8% faster than the Follow-The-Gap method and 3.22% faster than end-to-end neural networks due to the optimisation resulting in a faster speed profile.","The local map planner is 3.28% slower than global methods that have access to an entire map of the track that can be used for planning.","Critically, our approach enables high-speed autonomous racing on unmapped tracks, achieving performance similar to global methods without requiring a track map."],"url":"http://arxiv.org/abs/2401.17732v1"}
{"created":"2024-01-31 10:47:25","title":"COMET: Contrastive Mean Teacher for Online Source-Free Universal Domain Adaptation","abstract":"In real-world applications, there is often a domain shift from training to test data. This observation resulted in the development of test-time adaptation (TTA). It aims to adapt a pre-trained source model to the test data without requiring access to the source data. Thereby, most existing works are limited to the closed-set assumption, i.e. there is no category shift between source and target domain. We argue that in a realistic open-world setting a category shift can appear in addition to a domain shift. This means, individual source classes may not appear in the target domain anymore, samples of new classes may be part of the target domain or even both at the same time. Moreover, in many real-world scenarios the test data is not accessible all at once but arrives sequentially as a stream of batches demanding an immediate prediction. Hence, TTA must be applied in an online manner. To the best of our knowledge, the combination of these aspects, i.e. online source-free universal domain adaptation (online SF-UniDA), has not been studied yet. In this paper, we introduce a Contrastive Mean Teacher (COMET) tailored to this novel scenario. It applies a contrastive loss to rebuild a feature space where the samples of known classes build distinct clusters and the samples of new classes separate well from them. It is complemented by an entropy loss which ensures that the classifier output has a small entropy for samples of known classes and a large entropy for samples of new classes to be easily detected and rejected as unknown. To provide the losses with reliable pseudo labels, they are embedded into a mean teacher (MT) framework. We evaluate our method across two datasets and all category shifts to set an initial benchmark for online SF-UniDA. Thereby, COMET yields state-of-the-art performance and proves to be consistent and robust across a variety of different scenarios.","sentences":["In real-world applications, there is often a domain shift from training to test data.","This observation resulted in the development of test-time adaptation (TTA).","It aims to adapt a pre-trained source model to the test data without requiring access to the source data.","Thereby, most existing works are limited to the closed-set assumption, i.e. there is no category shift between source and target domain.","We argue that in a realistic open-world setting a category shift can appear in addition to a domain shift.","This means, individual source classes may not appear in the target domain anymore, samples of new classes may be part of the target domain or even both at the same time.","Moreover, in many real-world scenarios the test data is not accessible all at once but arrives sequentially as a stream of batches demanding an immediate prediction.","Hence, TTA must be applied in an online manner.","To the best of our knowledge, the combination of these aspects, i.e. online source-free universal domain adaptation (online SF-UniDA), has not been studied yet.","In this paper, we introduce a Contrastive Mean Teacher (COMET) tailored to this novel scenario.","It applies a contrastive loss to rebuild a feature space where the samples of known classes build distinct clusters and the samples of new classes separate well from them.","It is complemented by an entropy loss which ensures that the classifier output has a small entropy for samples of known classes and a large entropy for samples of new classes to be easily detected and rejected as unknown.","To provide the losses with reliable pseudo labels, they are embedded into a mean teacher (MT) framework.","We evaluate our method across two datasets and all category shifts to set an initial benchmark for online SF-UniDA.","Thereby, COMET yields state-of-the-art performance and proves to be consistent and robust across a variety of different scenarios."],"url":"http://arxiv.org/abs/2401.17728v1"}
{"created":"2024-01-31 10:37:20","title":"Challenges in Understanding the Relationship between Teamwork Quality and Project Success in Large-Scale Agile Projects","abstract":"A number of methods for large-scale agile development have recently been suggested. Much of the advice in agile methods focuses on teamwork. Prior research has established that teamwork quality influences project success both for traditional software development teams and agile teams. Further, prior studies have also suggested that teamwork quality may play out differently in large projects compared to small. We investigated the relationship between teamwork quality and project success with a survey of 196 project participants across 34 teams in four projects, replicating a previous study on single teams. The new data do not fit the previously established theoretical model, which raises several concerns. The observed effect of teamwork quality on project success operates differently across projects. We discuss possible reasons, which include disagreements on what characterises success in large-scale agile development, \"concept drift\" of teamwork quality factors, the possibility that interteam factors might have more influence on project success than intrateam factors, and finally, that our study design does not capture all relevant levels and functions. We conclude with a call for more studies on the quality and frequency of interaction between teams in addition to internal team factors to further advance theory and practice within large-scale agile software development.","sentences":["A number of methods for large-scale agile development have recently been suggested.","Much of the advice in agile methods focuses on teamwork.","Prior research has established that teamwork quality influences project success both for traditional software development teams and agile teams.","Further, prior studies have also suggested that teamwork quality may play out differently in large projects compared to small.","We investigated the relationship between teamwork quality and project success with a survey of 196 project participants across 34 teams in four projects, replicating a previous study on single teams.","The new data do not fit the previously established theoretical model, which raises several concerns.","The observed effect of teamwork quality on project success operates differently across projects.","We discuss possible reasons, which include disagreements on what characterises success in large-scale agile development, \"concept drift\" of teamwork quality factors, the possibility that interteam factors might have more influence on project success than intrateam factors, and finally, that our study design does not capture all relevant levels and functions.","We conclude with a call for more studies on the quality and frequency of interaction between teams in addition to internal team factors to further advance theory and practice within large-scale agile software development."],"url":"http://arxiv.org/abs/2401.17725v1"}
{"created":"2024-01-31 10:36:53","title":"High-Performance Data Mapping for BNNs on PCM-based Integrated Photonics","abstract":"State-of-the-Art (SotA) hardware implementations of Deep Neural Networks (DNNs) incur high latencies and costs. Binary Neural Networks (BNNs) are potential alternative solutions to realize faster implementations without losing accuracy. In this paper, we first present a new data mapping, called TacitMap, suited for BNNs implemented based on a Computation-In-Memory (CIM) architecture. TacitMap maximizes the use of available parallelism, while CIM architecture eliminates the data movement overhead. We then propose a hardware accelerator based on optical phase change memory (oPCM) called EinsteinBarrier. Ein-steinBarrier incorporates TacitMap and adds an extra dimension for parallelism through wavelength division multiplexing, leading to extra latency reduction. The simulation results show that, compared to the SotA CIM baseline, TacitMap and EinsteinBarrier significantly improve execution time by up to ~154x and ~3113x, respectively, while also maintaining the energy consumption within 60% of that in the CIM baseline.","sentences":["State-of-the-Art (SotA) hardware implementations of Deep Neural Networks (DNNs) incur high latencies and costs.","Binary Neural Networks (BNNs) are potential alternative solutions to realize faster implementations without losing accuracy.","In this paper, we first present a new data mapping, called TacitMap, suited for BNNs implemented based on a Computation-In-Memory (CIM) architecture.","TacitMap maximizes the use of available parallelism, while CIM architecture eliminates the data movement overhead.","We then propose a hardware accelerator based on optical phase change memory (oPCM) called EinsteinBarrier.","Ein-steinBarrier incorporates TacitMap and adds an extra dimension for parallelism through wavelength division multiplexing, leading to extra latency reduction.","The simulation results show that, compared to the SotA CIM baseline, TacitMap and EinsteinBarrier significantly improve execution time by up to ~154x and ~3113x, respectively, while also maintaining the energy consumption within 60% of that in the CIM baseline."],"url":"http://arxiv.org/abs/2401.17724v1"}
{"created":"2024-01-31 10:35:53","title":"LoRec: Large Language Model for Robust Sequential Recommendation against Poisoning Attacks","abstract":"Sequential recommender systems stand out for their ability to capture users' dynamic interests and the patterns of item-to-item transitions. However, the inherent openness of sequential recommender systems renders them vulnerable to poisoning attacks, where fraudulent users are injected into the training data to manipulate learned patterns. Traditional defense strategies predominantly depend on predefined assumptions or rules extracted from specific known attacks, limiting their generalizability to unknown attack types. To solve the above problems, considering the rich open-world knowledge encapsulated in Large Language Models (LLMs), our research initially focuses on the capabilities of LLMs in the detection of unknown fraudulent activities within recommender systems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the substantial capability of LLMs in identifying unknown fraudsters, leveraging their expansive, open-world knowledge.   Building upon this, we propose the integration of LLMs into defense strategies to extend their effectiveness beyond the confines of known attacks. We propose LoRec, an advanced framework that employs LLM-Enhanced Calibration to strengthen the robustness of sequential recommender systems against poisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that refines the training process of sequential recommender systems with knowledge derived from LLMs, applying a user-wise reweighting to diminish the impact of fraudsters injected by attacks. By incorporating LLMs' open-world knowledge, the LCT effectively converts the limited, specific priors or rules into a more general pattern of fraudsters, offering improved defenses against poisoning attacks. Our comprehensive experiments validate that LoRec, as a general framework, significantly strengthens the robustness of sequential recommender systems.","sentences":["Sequential recommender systems stand out for their ability to capture users' dynamic interests and the patterns of item-to-item transitions.","However, the inherent openness of sequential recommender systems renders them vulnerable to poisoning attacks, where fraudulent users are injected into the training data to manipulate learned patterns.","Traditional defense strategies predominantly depend on predefined assumptions or rules extracted from specific known attacks, limiting their generalizability to unknown attack types.","To solve the above problems, considering the rich open-world knowledge encapsulated in Large Language Models (LLMs), our research initially focuses on the capabilities of LLMs in the detection of unknown fraudulent activities within recommender systems, a strategy we denote as LLM4Dec.","Empirical evaluations demonstrate the substantial capability of LLMs in identifying unknown fraudsters, leveraging their expansive, open-world knowledge.   ","Building upon this, we propose the integration of LLMs into defense strategies to extend their effectiveness beyond the confines of known attacks.","We propose LoRec, an advanced framework that employs LLM-Enhanced Calibration to strengthen the robustness of sequential recommender systems against poisoning attacks.","LoRec integrates an LLM-enhanced CalibraTor (LCT) that refines the training process of sequential recommender systems with knowledge derived from LLMs, applying a user-wise reweighting to diminish the impact of fraudsters injected by attacks.","By incorporating LLMs' open-world knowledge, the LCT effectively converts the limited, specific priors or rules into a more general pattern of fraudsters, offering improved defenses against poisoning attacks.","Our comprehensive experiments validate that LoRec, as a general framework, significantly strengthens the robustness of sequential recommender systems."],"url":"http://arxiv.org/abs/2401.17723v1"}
{"created":"2024-01-31 10:33:36","title":"Time Synchronization for 5G and TSN Integrated Networking","abstract":"Emerging industrial applications involving robotic collaborative operations and mobile robots require a more reliable and precise wireless network for deterministic data transmission. To meet this demand, the 3rd Generation Partnership Project (3GPP) is promoting the integration of 5th Generation Mobile Communication Technology (5G) and Time-Sensitive Networking (TSN). Time synchronization is essential for deterministic data transmission. Based on the 3GPP's vision of the 5G and TSN integrated networking with interoperability, we improve the time synchronization of TSN to conquer the multi-gNB competition, re-transmission, and mobility problems for the integrated 5G time synchronization. We implemented the improvement mechanisms and systematically validated the performance of 5G+TSN time synchronization. Based on the simulation in 500m x 500m industrial environments, the improved time synchronization achieved a precision of 1 microsecond with interoperability between 5G nodes and TSN nodes.","sentences":["Emerging industrial applications involving robotic collaborative operations and mobile robots require a more reliable and precise wireless network for deterministic data transmission.","To meet this demand, the 3rd Generation Partnership Project (3GPP) is promoting the integration of 5th Generation Mobile Communication Technology (5G) and Time-Sensitive Networking (TSN).","Time synchronization is essential for deterministic data transmission.","Based on the 3GPP's vision of the 5G and TSN integrated networking with interoperability, we improve the time synchronization of TSN to conquer the multi-gNB competition, re-transmission, and mobility problems for the integrated 5G time synchronization.","We implemented the improvement mechanisms and systematically validated the performance of 5G+TSN time synchronization.","Based on the simulation in 500m x 500m industrial environments, the improved time synchronization achieved a precision of 1 microsecond with interoperability between 5G nodes and TSN nodes."],"url":"http://arxiv.org/abs/2401.17721v1"}
{"created":"2024-01-31 10:09:26","title":"3D-Plotting Algorithm for Insects using YOLOv5","abstract":"In ecological research, accurately collecting spatiotemporal position data is a fundamental task for understanding the behavior and ecology of insects and other organisms. In recent years, advancements in computer vision techniques have reached a stage of maturity where they can support, and in some cases, replace manual observation. In this study, a simple and inexpensive method for monitoring insects in three dimensions (3D) was developed so that their behavior could be observed automatically in experimental environments. The main achievements of this study have been to create a 3D monitoring algorithm using inexpensive cameras and other equipment to design an adjusting algorithm for depth error, and to validate how our plotting algorithm is quantitatively precise, all of which had not been realized in conventional studies. By offering detailed 3D visualizations of insects, the plotting algorithm aids researchers in more effectively comprehending how insects interact within their environments.","sentences":["In ecological research, accurately collecting spatiotemporal position data is a fundamental task for understanding the behavior and ecology of insects and other organisms.","In recent years, advancements in computer vision techniques have reached a stage of maturity where they can support, and in some cases, replace manual observation.","In this study, a simple and inexpensive method for monitoring insects in three dimensions (3D) was developed so that their behavior could be observed automatically in experimental environments.","The main achievements of this study have been to create a 3D monitoring algorithm using inexpensive cameras and other equipment to design an adjusting algorithm for depth error, and to validate how our plotting algorithm is quantitatively precise, all of which had not been realized in conventional studies.","By offering detailed 3D visualizations of insects, the plotting algorithm aids researchers in more effectively comprehending how insects interact within their environments."],"url":"http://arxiv.org/abs/2401.17714v1"}
{"created":"2024-01-31 10:03:27","title":"Prediction of multitasking performance post-longitudinal tDCS via EEG-based functional connectivity and machine learning methods","abstract":"Predicting and understanding the changes in cognitive performance, especially after a longitudinal intervention, is a fundamental goal in neuroscience. Longitudinal brain stimulation-based interventions like transcranial direct current stimulation (tDCS) induce short-term changes in the resting membrane potential and influence cognitive processes. However, very little research has been conducted on predicting these changes in cognitive performance post-intervention. In this research, we intend to address this gap in the literature by employing different EEG-based functional connectivity analyses and machine learning algorithms to predict changes in cognitive performance in a complex multitasking task. Forty subjects were divided into experimental and active-control conditions. On Day 1, all subjects executed a multitasking task with simultaneous 32-channel EEG being acquired. From Day 2 to Day 7, subjects in the experimental condition undertook 15 minutes of 2mA anodal tDCS stimulation during task training. Subjects in the active-control condition undertook 15 minutes of sham stimulation during task training. On Day 10, all subjects again executed the multitasking task with EEG acquisition. Source-level functional connectivity metrics, namely phase lag index and directed transfer function, were extracted from the EEG data on Day 1 and Day 10. Various machine learning models were employed to predict changes in cognitive performance. Results revealed that the multi-layer perceptron and directed transfer function recorded a cross-validation training RMSE of 5.11% and a test RMSE of 4.97%. We discuss the implications of our results in developing real-time cognitive state assessors for accurately predicting cognitive performance in dynamic and complex tasks post-tDCS intervention","sentences":["Predicting and understanding the changes in cognitive performance, especially after a longitudinal intervention, is a fundamental goal in neuroscience.","Longitudinal brain stimulation-based interventions like transcranial direct current stimulation (tDCS) induce short-term changes in the resting membrane potential and influence cognitive processes.","However, very little research has been conducted on predicting these changes in cognitive performance post-intervention.","In this research, we intend to address this gap in the literature by employing different EEG-based functional connectivity analyses and machine learning algorithms to predict changes in cognitive performance in a complex multitasking task.","Forty subjects were divided into experimental and active-control conditions.","On Day 1, all subjects executed a multitasking task with simultaneous 32-channel EEG being acquired.","From Day 2 to Day 7, subjects in the experimental condition undertook 15 minutes of 2mA anodal tDCS stimulation during task training.","Subjects in the active-control condition undertook 15 minutes of sham stimulation during task training.","On Day 10, all subjects again executed the multitasking task with EEG acquisition.","Source-level functional connectivity metrics, namely phase lag index and directed transfer function, were extracted from the EEG data on Day 1 and Day 10.","Various machine learning models were employed to predict changes in cognitive performance.","Results revealed that the multi-layer perceptron and directed transfer function recorded a cross-validation training RMSE of 5.11% and a test RMSE of 4.97%.","We discuss the implications of our results in developing real-time cognitive state assessors for accurately predicting cognitive performance in dynamic and complex tasks post-tDCS intervention"],"url":"http://arxiv.org/abs/2401.17711v1"}
{"created":"2024-01-31 09:49:46","title":"Predicting suicidal behavior among Indian adults using childhood trauma, mental health questionnaires and machine learning cascade ensembles","abstract":"Among young adults, suicide is India's leading cause of death, accounting for an alarming national suicide rate of around 16%. In recent years, machine learning algorithms have emerged to predict suicidal behavior using various behavioral traits. But to date, the efficacy of machine learning algorithms in predicting suicidal behavior in the Indian context has not been explored in literature. In this study, different machine learning algorithms and ensembles were developed to predict suicide behavior based on childhood trauma, different mental health parameters, and other behavioral factors. The dataset was acquired from 391 individuals from a wellness center in India. Information regarding their childhood trauma, psychological wellness, and other mental health issues was acquired through standardized questionnaires. Results revealed that cascade ensemble learning methods using a support vector machine, decision trees, and random forest were able to classify suicidal behavior with an accuracy of 95.04% using data from childhood trauma and mental health questionnaires. The study highlights the potential of using these machine learning ensembles to identify individuals with suicidal tendencies so that targeted interinterventions could be provided efficiently.","sentences":["Among young adults, suicide is India's leading cause of death, accounting for an alarming national suicide rate of around 16%.","In recent years, machine learning algorithms have emerged to predict suicidal behavior using various behavioral traits.","But to date, the efficacy of machine learning algorithms in predicting suicidal behavior in the Indian context has not been explored in literature.","In this study, different machine learning algorithms and ensembles were developed to predict suicide behavior based on childhood trauma, different mental health parameters, and other behavioral factors.","The dataset was acquired from 391 individuals from a wellness center in India.","Information regarding their childhood trauma, psychological wellness, and other mental health issues was acquired through standardized questionnaires.","Results revealed that cascade ensemble learning methods using a support vector machine, decision trees, and random forest were able to classify suicidal behavior with an accuracy of 95.04% using data from childhood trauma and mental health questionnaires.","The study highlights the potential of using these machine learning ensembles to identify individuals with suicidal tendencies so that targeted interinterventions could be provided efficiently."],"url":"http://arxiv.org/abs/2401.17705v1"}
{"created":"2024-01-31 09:45:03","title":"Classification of executive functioning performance post-longitudinal tDCS using functional connectivity and machine learning methods","abstract":"Executive functioning is a cognitive process that enables humans to plan, organize, and regulate their behavior in a goal-directed manner. Understanding and classifying the changes in executive functioning after longitudinal interventions (like transcranial direct current stimulation (tDCS)) has not been explored in the literature. This study employs functional connectivity and machine learning algorithms to classify executive functioning performance post-tDCS. Fifty subjects were divided into experimental and placebo control groups. EEG data was collected while subjects performed an executive functioning task on Day 1. The experimental group received tDCS during task training from Day 2 to Day 8, while the control group received sham tDCS. On Day 10, subjects repeated the tasks specified on Day 1. Different functional connectivity metrics were extracted from EEG data and eventually used for classifying executive functioning performance using different machine learning algorithms. Results revealed that a novel combination of partial directed coherence and multi-layer perceptron (along with recursive feature elimination) resulted in a high classification accuracy of 95.44%. We discuss the implications of our results in developing real-time neurofeedback systems for assessing and enhancing executive functioning performance post-tDCS administration.","sentences":["Executive functioning is a cognitive process that enables humans to plan, organize, and regulate their behavior in a goal-directed manner.","Understanding and classifying the changes in executive functioning after longitudinal interventions (like transcranial direct current stimulation (tDCS)) has not been explored in the literature.","This study employs functional connectivity and machine learning algorithms to classify executive functioning performance post-tDCS.","Fifty subjects were divided into experimental and placebo control groups.","EEG data was collected while subjects performed an executive functioning task on Day 1.","The experimental group received tDCS during task training from Day 2 to Day 8, while the control group received sham tDCS.","On Day 10, subjects repeated the tasks specified on Day 1.","Different functional connectivity metrics were extracted from EEG data and eventually used for classifying executive functioning performance using different machine learning algorithms.","Results revealed that a novel combination of partial directed coherence and multi-layer perceptron (along with recursive feature elimination) resulted in a high classification accuracy of 95.44%.","We discuss the implications of our results in developing real-time neurofeedback systems for assessing and enhancing executive functioning performance post-tDCS administration."],"url":"http://arxiv.org/abs/2401.17700v1"}
{"created":"2024-01-31 09:38:09","title":"Bi-ACT: Bilateral Control-Based Imitation Learning via Action Chunking with Transformer","abstract":"Autonomous manipulation in robot arms is a complex and evolving field of study in robotics. This paper proposes work stands at the intersection of two innovative approaches in the field of robotics and machine learning. Inspired by the Action Chunking with Transformer (ACT) model, which employs joint location and image data to predict future movements, our work integrates principles of Bilateral Control-Based Imitation Learning to enhance robotic control. Our objective is to synergize these techniques, thereby creating a more robust and efficient control mechanism. In our approach, the data collected from the environment are images from the gripper and overhead cameras, along with the joint angles, angular velocities, and forces of the follower robot using bilateral control. The model is designed to predict the subsequent steps for the joint angles, angular velocities, and forces of the leader robot. This predictive capability is crucial for implementing effective bilateral control in the follower robot, allowing for more nuanced and responsive maneuvering.","sentences":["Autonomous manipulation in robot arms is a complex and evolving field of study in robotics.","This paper proposes work stands at the intersection of two innovative approaches in the field of robotics and machine learning.","Inspired by the Action Chunking with Transformer (ACT) model, which employs joint location and image data to predict future movements, our work integrates principles of Bilateral Control-Based Imitation Learning to enhance robotic control.","Our objective is to synergize these techniques, thereby creating a more robust and efficient control mechanism.","In our approach, the data collected from the environment are images from the gripper and overhead cameras, along with the joint angles, angular velocities, and forces of the follower robot using bilateral control.","The model is designed to predict the subsequent steps for the joint angles, angular velocities, and forces of the leader robot.","This predictive capability is crucial for implementing effective bilateral control in the follower robot, allowing for more nuanced and responsive maneuvering."],"url":"http://arxiv.org/abs/2401.17698v1"}
{"created":"2024-01-31 09:31:28","title":"Datacube segmentation via Deep Spectral Clustering","abstract":"Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping spectra into lower dimensional metric spaces, while the clustering process is performed by a (learnable) iterative K-Means clustering algorithm.   We apply this technique to two different use cases, of different physical origins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on pictorial artworks, and a dataset of simulated astrophysical observations.","sentences":["Extended Vision techniques are ubiquitous in physics.","However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   ","Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   ","To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels.","A statistical dimensional reduction is performed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping spectra into lower dimensional metric spaces, while the clustering process is performed by a (learnable) iterative K-Means clustering algorithm.   ","We apply this technique to two different use cases, of different physical origins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on pictorial artworks, and a dataset of simulated astrophysical observations."],"url":"http://arxiv.org/abs/2401.17695v1"}
{"created":"2024-01-31 09:28:06","title":"Mitigating the Problem of Strong Priors in LMs with Context Extrapolation","abstract":"Language models (LMs) have become important tools in a variety of applications, from data processing to the creation of instruction-following assistants. But despite their advantages, LMs have certain idiosyncratic limitations such as the problem of `strong priors', where a model learns to output typical continuations in response to certain, usually local, portions of the input regardless of any earlier instructions. For example, prompt injection attacks can induce models to ignore explicit directives. In some cases, larger models have been shown to be more susceptible to these problems than similar smaller models, an example of the phenomenon of `inverse scaling'. We develop a new technique for mitigating the problem of strong priors: we take the original set of instructions, produce a weakened version of the original prompt that is even more susceptible to the strong priors problem, and then extrapolate the continuation away from the weakened prompt. This lets us infer how the model would continue a hypothetical strengthened set of instructions. Our technique conceptualises LMs as mixture models which combine a family of data generation processes, reinforcing the desired elements of the mixture. Our approach works at inference time, removing any need for retraining. We apply it to eleven models including GPT-2, GPT-3, Llama 2, and Mistral on four tasks, and find improvements in 41/44. Across all 44 combinations the median increase in proportion of tasks completed is 40%.","sentences":["Language models (LMs) have become important tools in a variety of applications, from data processing to the creation of instruction-following assistants.","But despite their advantages, LMs have certain idiosyncratic limitations such as the problem of `strong priors', where a model learns to output typical continuations in response to certain, usually local, portions of the input regardless of any earlier instructions.","For example, prompt injection attacks can induce models to ignore explicit directives.","In some cases, larger models have been shown to be more susceptible to these problems than similar smaller models, an example of the phenomenon of `inverse scaling'.","We develop a new technique for mitigating the problem of strong priors: we take the original set of instructions, produce a weakened version of the original prompt that is even more susceptible to the strong priors problem, and then extrapolate the continuation away from the weakened prompt.","This lets us infer how the model would continue a hypothetical strengthened set of instructions.","Our technique conceptualises LMs as mixture models which combine a family of data generation processes, reinforcing the desired elements of the mixture.","Our approach works at inference time, removing any need for retraining.","We apply it to eleven models including GPT-2, GPT-3, Llama 2, and Mistral on four tasks, and find improvements in 41/44.","Across all 44 combinations the median increase in proportion of tasks completed is 40%."],"url":"http://arxiv.org/abs/2401.17692v1"}
{"created":"2024-01-31 09:16:35","title":"Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning","abstract":"Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves DBS's capability of detecting diverse and subtle reasoning errors and robustness on different model scales.","sentences":["Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning.","However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.","In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs.","Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation.","Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities.","Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic.","Moreover, our analysis proves DBS's capability of detecting diverse and subtle reasoning errors and robustness on different model scales."],"url":"http://arxiv.org/abs/2401.17686v1"}
{"created":"2024-01-31 08:35:40","title":"Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation","abstract":"The multifaceted nature of human perception and comprehension indicates that, when we think, our body can naturally take any combination of senses, a.k.a., modalities and form a beautiful picture in our brain. For example, when we see a cattery and simultaneously perceive the cat's purring sound, our brain can construct a picture of a cat in the cattery. Intuitively, generative AI models should hold the versatility of humans and be capable of generating images from any combination of modalities efficiently and collaboratively. This paper presents ImgAny, a novel end-to-end multi-modal generative model that can mimic human reasoning and generate high-quality images. Our method serves as the first attempt in its capacity of efficiently and flexibly taking any combination of seven modalities, ranging from language, audio to vision modalities, including image, point cloud, thermal, depth, and event data. Our key idea is inspired by human-level cognitive processes and involves the integration and harmonization of multiple input modalities at both the entity and attribute levels without specific tuning across modalities. Accordingly, our method brings two novel training-free technical branches: 1) Entity Fusion Branch ensures the coherence between inputs and outputs. It extracts entity features from the multi-modal representations powered by our specially constructed entity knowledge graph; 2) Attribute Fusion Branch adeptly preserves and processes the attributes. It efficiently amalgamates distinct attributes from diverse input modalities via our proposed attribute knowledge graph. Lastly, the entity and attribute features are adaptively fused as the conditional inputs to the pre-trained Stable Diffusion model for image generation. Extensive experiments under diverse modality combinations demonstrate its exceptional capability for visual content creation.","sentences":["The multifaceted nature of human perception and comprehension indicates that, when we think, our body can naturally take any combination of senses, a.k.a., modalities and form a beautiful picture in our brain.","For example, when we see a cattery and simultaneously perceive the cat's purring sound, our brain can construct a picture of a cat in the cattery.","Intuitively, generative AI models should hold the versatility of humans and be capable of generating images from any combination of modalities efficiently and collaboratively.","This paper presents ImgAny, a novel end-to-end multi-modal generative model that can mimic human reasoning and generate high-quality images.","Our method serves as the first attempt in its capacity of efficiently and flexibly taking any combination of seven modalities, ranging from language, audio to vision modalities, including image, point cloud, thermal, depth, and event data.","Our key idea is inspired by human-level cognitive processes and involves the integration and harmonization of multiple input modalities at both the entity and attribute levels without specific tuning across modalities.","Accordingly, our method brings two novel training-free technical branches: 1) Entity Fusion Branch ensures the coherence between inputs and outputs.","It extracts entity features from the multi-modal representations powered by our specially constructed entity knowledge graph; 2) Attribute Fusion Branch adeptly preserves and processes the attributes.","It efficiently amalgamates distinct attributes from diverse input modalities via our proposed attribute knowledge graph.","Lastly, the entity and attribute features are adaptively fused as the conditional inputs to the pre-trained Stable Diffusion model for image generation.","Extensive experiments under diverse modality combinations demonstrate its exceptional capability for visual content creation."],"url":"http://arxiv.org/abs/2401.17664v1"}
{"created":"2024-01-31 08:28:06","title":"Document Structure in Long Document Transformers","abstract":"Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs. Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque. Do long-document Transformer models acquire an internal representation of document structure during pre-training? How can structural information be communicated to a model after pre-training, and how does it influence downstream performance? To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks. Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task performance. To foster research on the role of document structure in NLP modeling, we make our data and code publicly available.","sentences":["Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs.","Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque.","Do long-document Transformer models acquire an internal representation of document structure during pre-training?","How can structural information be communicated to a model after pre-training, and how does it influence downstream performance?","To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks.","Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task performance.","To foster research on the role of document structure in NLP modeling, we make our data and code publicly available."],"url":"http://arxiv.org/abs/2401.17658v1"}
{"created":"2024-01-31 08:16:32","title":"All Beings Are Equal in Open Set Recognition","abstract":"In open-set recognition (OSR), a promising strategy is exploiting pseudo-unknown data outside given $K$ known classes as an additional $K$+$1$-th class to explicitly model potential open space. However, treating unknown classes without distinction is unequal for them relative to known classes due to the category-agnostic and scale-agnostic of the unknowns. This inevitably not only disrupts the inherent distributions of unknown classes but also incurs both class-wise and instance-wise imbalances between known and unknown classes. Ideally, the OSR problem should model the whole class space as $K$+$\\infty$, but enumerating all unknowns is impractical. Since the core of OSR is to effectively model the boundaries of known classes, this means just focusing on the unknowns nearing the boundaries of targeted known classes seems sufficient. Thus, as a compromise, we convert the open classes from infinite to $K$, with a novel concept Target-Aware Universum (TAU) and propose a simple yet effective framework Dual Contrastive Learning with Target-Aware Universum (DCTAU). In details, guided by the targeted known classes, TAU automatically expands the unknown classes from the previous $1$ to $K$, effectively alleviating the distribution disruption and the imbalance issues mentioned above. Then, a novel Dual Contrastive (DC) loss is designed, where all instances irrespective of known or TAU are considered as positives to contrast with their respective negatives. Experimental results indicate DCTAU sets a new state-of-the-art.","sentences":["In open-set recognition (OSR), a promising strategy is exploiting pseudo-unknown data outside given $K$ known classes as an additional $K$+$1$-th class to explicitly model potential open space.","However, treating unknown classes without distinction is unequal for them relative to known classes due to the category-agnostic and scale-agnostic of the unknowns.","This inevitably not only disrupts the inherent distributions of unknown classes but also incurs both class-wise and instance-wise imbalances between known and unknown classes.","Ideally, the OSR problem should model the whole class space as $K$+$\\infty$, but enumerating all unknowns is impractical.","Since the core of OSR is to effectively model the boundaries of known classes, this means just focusing on the unknowns nearing the boundaries of targeted known classes seems sufficient.","Thus, as a compromise, we convert the open classes from infinite to $K$, with a novel concept Target-Aware Universum (TAU) and propose a simple yet effective framework Dual Contrastive Learning with Target-Aware Universum (DCTAU).","In details, guided by the targeted known classes, TAU automatically expands the unknown classes from the previous $1$ to $K$, effectively alleviating the distribution disruption and the imbalance issues mentioned above.","Then, a novel Dual Contrastive (DC) loss is designed, where all instances irrespective of known or TAU are considered as positives to contrast with their respective negatives.","Experimental results indicate DCTAU sets a new state-of-the-art."],"url":"http://arxiv.org/abs/2401.17654v1"}
{"created":"2024-01-31 08:13:35","title":"A primer on synthetic health data","abstract":"Recent advances in deep generative models have greatly expanded the potential to create realistic synthetic health datasets. These synthetic datasets aim to preserve the characteristics, patterns, and overall scientific conclusions derived from sensitive health datasets without disclosing patient identity or sensitive information. Thus, synthetic data can facilitate safe data sharing that supports a range of initiatives including the development of new predictive models, advanced health IT platforms, and general project ideation and hypothesis development. However, many questions and challenges remain, including how to consistently evaluate a synthetic dataset's similarity and predictive utility in comparison to the original real dataset and risk to privacy when shared. Additional regulatory and governance issues have not been widely addressed. In this primer, we map the state of synthetic health data, including generation and evaluation methods and tools, existing examples of deployment, the regulatory and ethical landscape, access and governance options, and opportunities for further development.","sentences":["Recent advances in deep generative models have greatly expanded the potential to create realistic synthetic health datasets.","These synthetic datasets aim to preserve the characteristics, patterns, and overall scientific conclusions derived from sensitive health datasets without disclosing patient identity or sensitive information.","Thus, synthetic data can facilitate safe data sharing that supports a range of initiatives including the development of new predictive models, advanced health IT platforms, and general project ideation and hypothesis development.","However, many questions and challenges remain, including how to consistently evaluate a synthetic dataset's similarity and predictive utility in comparison to the original real dataset and risk to privacy when shared.","Additional regulatory and governance issues have not been widely addressed.","In this primer, we map the state of synthetic health data, including generation and evaluation methods and tools, existing examples of deployment, the regulatory and ethical landscape, access and governance options, and opportunities for further development."],"url":"http://arxiv.org/abs/2401.17653v1"}
{"created":"2024-01-31 08:04:58","title":"Covering All Bases: The Next Inning in DNA Sequencing Efficiency","abstract":"DNA emerges as a promising medium for the exponential growth of digital data due to its density and durability. This study extends recent research by addressing the \\emph{coverage depth problem} in practical scenarios, exploring optimal error-correcting code pairings with DNA storage systems to minimize coverage depth. Conducted within random access settings, the study provides theoretical analyses and experimental simulations to examine the expectation and probability distribution of samples needed for files recovery. Structured into sections covering definitions, analyses, lower bounds, and comparative evaluations of coding schemes, the paper unveils insights into effective coding schemes for optimizing DNA storage systems.","sentences":["DNA emerges as a promising medium for the exponential growth of digital data due to its density and durability.","This study extends recent research by addressing the \\emph{coverage depth problem} in practical scenarios, exploring optimal error-correcting code pairings with DNA storage systems to minimize coverage depth.","Conducted within random access settings, the study provides theoretical analyses and experimental simulations to examine the expectation and probability distribution of samples needed for files recovery.","Structured into sections covering definitions, analyses, lower bounds, and comparative evaluations of coding schemes, the paper unveils insights into effective coding schemes for optimizing DNA storage systems."],"url":"http://arxiv.org/abs/2401.17649v1"}
{"created":"2024-01-31 07:52:48","title":"Towards Efficient and Reliable LLM Serving: A Real-World Workload Study","abstract":"Large language models (LLMs), especially Generative Pretrained Transformer (GPT) models, have significantly advanced in the industry in recent years. However, these models' broader development faces considerable challenges due to high operational and deployment costs. This has led to active research in improving the hardware efficiency of LLMs. Yet, the characteristics of real-world LLM workloads are often overlooked in current optimizations of LLM serving systems. In this work, we find that the absence of reliable workload data for evaluating LLM serving systems impacts the quality of service (QoS) and reliability in industrial deployments. This paper introduces the first real-world trace dataset of LLM serving workloads, detailing user, system, and LLM behaviors. We analyze this trace, highlighting burstiness, request and response distributions, and focusing on the reliability of GPT services. Based on this, we have developed a benchmark suite that reflects our dataset's workload patterns, enabling performance evaluation of serving systems. This suite captures the core patterns of workload distributions, allowing for precise scaling of the workload dataset to match system sizes. Our evaluation uncovers a previously unrecognized vulnerability of LLM serving systems to short-term burstiness, particularly in common workload scenarios. We observe that GPU memory limitations, caused by the fluctuating nature of burstiness, lead to significant performance degradation in existing LLM serving systems. Beyond benchmarking, understanding these patterns is valuable for optimizing LLM workload management, enabling elastic hardware resource adjustments to varying workloads. We will make the dataset and benchmark suite publicly available to encourage further research.","sentences":["Large language models (LLMs), especially Generative Pretrained Transformer (GPT) models, have significantly advanced in the industry in recent years.","However, these models' broader development faces considerable challenges due to high operational and deployment costs.","This has led to active research in improving the hardware efficiency of LLMs.","Yet, the characteristics of real-world LLM workloads are often overlooked in current optimizations of LLM serving systems.","In this work, we find that the absence of reliable workload data for evaluating LLM serving systems impacts the quality of service (QoS) and reliability in industrial deployments.","This paper introduces the first real-world trace dataset of LLM serving workloads, detailing user, system, and LLM behaviors.","We analyze this trace, highlighting burstiness, request and response distributions, and focusing on the reliability of GPT services.","Based on this, we have developed a benchmark suite that reflects our dataset's workload patterns, enabling performance evaluation of serving systems.","This suite captures the core patterns of workload distributions, allowing for precise scaling of the workload dataset to match system sizes.","Our evaluation uncovers a previously unrecognized vulnerability of LLM serving systems to short-term burstiness, particularly in common workload scenarios.","We observe that GPU memory limitations, caused by the fluctuating nature of burstiness, lead to significant performance degradation in existing LLM serving systems.","Beyond benchmarking, understanding these patterns is valuable for optimizing LLM workload management, enabling elastic hardware resource adjustments to varying workloads.","We will make the dataset and benchmark suite publicly available to encourage further research."],"url":"http://arxiv.org/abs/2401.17644v1"}
{"created":"2024-01-31 07:20:56","title":"Towards Personalized Privacy: User-Governed Data Contribution for Federated Recommendation","abstract":"Federated recommender systems (FedRecs) have gained significant attention for their potential to protect user's privacy by keeping user privacy data locally and only communicating model parameters/gradients to the server. Nevertheless, the currently existing architecture of FedRecs assumes that all users have the same 0-privacy budget, i.e., they do not upload any data to the server, thus overlooking those users who are less concerned about privacy and are willing to upload data to get a better recommendation service. To bridge this gap, this paper explores a user-governed data contribution federated recommendation architecture where users are free to take control of whether they share data and the proportion of data they share to the server. To this end, this paper presents a cloud-device collaborative graph neural network federated recommendation model, named CDCGNNFed. It trains user-centric ego graphs locally, and high-order graphs based on user-shared data in the server in a collaborative manner via contrastive learning. Furthermore, a graph mending strategy is utilized to predict missing links in the graph on the server, thus leveraging the capabilities of graph neural networks over high-order graphs. Extensive experiments were conducted on two public datasets, and the results demonstrate the effectiveness of the proposed method.","sentences":["Federated recommender systems (FedRecs) have gained significant attention for their potential to protect user's privacy by keeping user privacy data locally and only communicating model parameters/gradients to the server.","Nevertheless, the currently existing architecture of FedRecs assumes that all users have the same 0-privacy budget, i.e., they do not upload any data to the server, thus overlooking those users who are less concerned about privacy and are willing to upload data to get a better recommendation service.","To bridge this gap, this paper explores a user-governed data contribution federated recommendation architecture where users are free to take control of whether they share data and the proportion of data they share to the server.","To this end, this paper presents a cloud-device collaborative graph neural network federated recommendation model, named CDCGNNFed.","It trains user-centric ego graphs locally, and high-order graphs based on user-shared data in the server in a collaborative manner via contrastive learning.","Furthermore, a graph mending strategy is utilized to predict missing links in the graph on the server, thus leveraging the capabilities of graph neural networks over high-order graphs.","Extensive experiments were conducted on two public datasets, and the results demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2401.17630v1"}
{"created":"2024-01-31 07:11:01","title":"Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models","abstract":"Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.","sentences":["Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods.","Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity.","In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise.","Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality.","We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution.","Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics."],"url":"http://arxiv.org/abs/2401.17629v1"}
{"created":"2024-01-31 07:08:14","title":"Elephants Do Not Forget: Differential Privacy with State Continuity for Privacy Budget","abstract":"Current implementations of differentially-private (DP) systems either lack support to track the global privacy budget consumed on a dataset, or fail to faithfully maintain the state continuity of this budget. We show that failure to maintain a privacy budget enables an adversary to mount replay, rollback and fork attacks - obtaining answers to many more queries than what a secure system would allow. As a result the attacker can reconstruct secret data that DP aims to protect - even if DP code runs in a Trusted Execution Environment (TEE). We propose ElephantDP, a system that aims to provide the same guarantees as a trusted curator in the global DP model would, albeit set in an untrusted environment. Our system relies on a state continuity module to provide protection for the privacy budget and a TEE to faithfully execute DP code and update the budget. To provide security, our protocol makes several design choices including the content of the persistent state and the order between budget updates and query answers. We prove that ElephantDP provides liveness (i.e., the protocol can restart from a correct state and respond to queries as long as the budget is not exceeded) and DP confidentiality (i.e., an attacker learns about a dataset as much as it would from interacting with a trusted curator). Our implementation and evaluation of the protocol use Intel SGX as a TEE to run the DP code and a network of TEEs to maintain state continuity. Compared to an insecure baseline, we observe only 1.1-2$\\times$ overheads and lower relative overheads for larger datasets and complex DP queries.","sentences":["Current implementations of differentially-private (DP) systems either lack support to track the global privacy budget consumed on a dataset, or fail to faithfully maintain the state continuity of this budget.","We show that failure to maintain a privacy budget enables an adversary to mount replay, rollback and fork attacks - obtaining answers to many more queries than what a secure system would allow.","As a result the attacker can reconstruct secret data that DP aims to protect - even if DP code runs in a Trusted Execution Environment (TEE).","We propose ElephantDP, a system that aims to provide the same guarantees as a trusted curator in the global DP model would, albeit set in an untrusted environment.","Our system relies on a state continuity module to provide protection for the privacy budget and a TEE to faithfully execute DP code and update the budget.","To provide security, our protocol makes several design choices including the content of the persistent state and the order between budget updates and query answers.","We prove that ElephantDP provides liveness (i.e., the protocol can restart from a correct state and respond to queries as long as the budget is not exceeded) and DP confidentiality (i.e., an attacker learns about a dataset as much as it would from interacting with a trusted curator).","Our implementation and evaluation of the protocol use Intel SGX as a TEE to run the DP code and a network of TEEs to maintain state continuity.","Compared to an insecure baseline, we observe only 1.1-2$\\times$ overheads and lower relative overheads for larger datasets and complex DP queries."],"url":"http://arxiv.org/abs/2401.17628v1"}
{"created":"2024-01-31 06:58:26","title":"Generative AI to Generate Test Data Generators","abstract":"Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability.","sentences":["Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries.","Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains.","In this paper, we assess the ability of generative AI for generating test data in different domains.","We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries.","We evaluate our approach by prompting LLMs to generate test data for 11 domains.","The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability."],"url":"http://arxiv.org/abs/2401.17626v1"}
{"created":"2024-01-31 06:47:12","title":"Commit Messages in the Age of Large Language Models","abstract":"Commit messages are explanations of changes made to a codebase that are stored in version control systems. They help developers understand the codebase as it evolves. However, writing commit messages can be tedious and inconsistent among developers. To address this issue, researchers have tried using different methods to automatically generate commit messages, including rule-based, retrieval-based, and learning-based approaches. Advances in large language models offer new possibilities for generating commit messages. In this study, we evaluate the performance of OpenAI's ChatGPT for generating commit messages based on code changes. We compare the results obtained with ChatGPT to previous automatic commit message generation methods that have been trained specifically on commit data. Our goal is to assess the extent to which large pre-trained language models can generate commit messages that are both quantitatively and qualitatively acceptable. We found that ChatGPT was able to outperform previous Automatic Commit Message Generation (ACMG) methods by orders of magnitude, and that, generally, the messages it generates are both accurate and of high-quality. We also provide insights, and a categorization, for the cases where it fails.","sentences":["Commit messages are explanations of changes made to a codebase that are stored in version control systems.","They help developers understand the codebase as it evolves.","However, writing commit messages can be tedious and inconsistent among developers.","To address this issue, researchers have tried using different methods to automatically generate commit messages, including rule-based, retrieval-based, and learning-based approaches.","Advances in large language models offer new possibilities for generating commit messages.","In this study, we evaluate the performance of OpenAI's ChatGPT for generating commit messages based on code changes.","We compare the results obtained with ChatGPT to previous automatic commit message generation methods that have been trained specifically on commit data.","Our goal is to assess the extent to which large pre-trained language models can generate commit messages that are both quantitatively and qualitatively acceptable.","We found that ChatGPT was able to outperform previous Automatic Commit Message Generation (ACMG) methods by orders of magnitude, and that, generally, the messages it generates are both accurate and of high-quality.","We also provide insights, and a categorization, for the cases where it fails."],"url":"http://arxiv.org/abs/2401.17622v1"}
{"created":"2024-01-31 06:17:51","title":"Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and KiSing-v2","abstract":"In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability, a constraint less common in text-to-speech (TTS). This study proposes a new approach to address this data scarcity. We utilize an existing singing voice synthesizer for data augmentation and apply precise manual tuning to reduce unnatural voice synthesis. Our development of two extensive singing voice corpora, ACE-Opencpop and KiSing-v2, facilitates large-scale, multi-singer voice synthesis. Utilizing pre-trained models derived from these corpora, we achieve notable improvements in voice quality, evident in both in-domain and out-of-domain scenarios. The corpora, pre-trained models, and their related training recipes are publicly available at Muskits-ESPnet (https://github.com/espnet/espnet).","sentences":["In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability, a constraint less common in text-to-speech (TTS).","This study proposes a new approach to address this data scarcity.","We utilize an existing singing voice synthesizer for data augmentation and apply precise manual tuning to reduce unnatural voice synthesis.","Our development of two extensive singing voice corpora, ACE-Opencpop and KiSing-v2, facilitates large-scale, multi-singer voice synthesis.","Utilizing pre-trained models derived from these corpora, we achieve notable improvements in voice quality, evident in both in-domain and out-of-domain scenarios.","The corpora, pre-trained models, and their related training recipes are publicly available at Muskits-ESPnet (https://github.com/espnet/espnet)."],"url":"http://arxiv.org/abs/2401.17619v1"}
{"created":"2024-01-31 06:16:00","title":"Beyond Control: Exploring Novel File System Objects for Data-Only Attacks on Linux Systems","abstract":"The widespread deployment of control-flow integrity has propelled non-control data attacks into the mainstream. In the domain of OS kernel exploits, by corrupting critical non-control data, local attackers can directly gain root access or privilege escalation without hijacking the control flow. As a result, OS kernels have been restricting the availability of such non-control data. This forces attackers to continue to search for more exploitable non-control data in OS kernels. However, discovering unknown non-control data can be daunting because they are often tied heavily to semantics and lack universal patterns.   We make two contributions in this paper: (1) discover critical non-control objects in the file subsystem and (2) analyze their exploitability. This work represents the first study, with minimal domain knowledge, to semi-automatically discover and evaluate exploitable non-control data within the file subsystem of the Linux kernel. Our solution utilizes a custom analysis and testing framework that statically and dynamically identifies promising candidate objects. Furthermore, we categorize these discovered objects into types that are suitable for various exploit strategies, including a novel strategy necessary to overcome the defense that isolates many of these objects. These objects have the advantage of being exploitable without requiring KASLR, thus making the exploits simpler and more reliable. We use 18 real-world CVEs to evaluate the exploitability of the file system objects using various exploit strategies. We develop 10 end-to-end exploits using a subset of CVEs against the kernel with all state-of-the-art mitigations enabled.","sentences":["The widespread deployment of control-flow integrity has propelled non-control data attacks into the mainstream.","In the domain of OS kernel exploits, by corrupting critical non-control data, local attackers can directly gain root access or privilege escalation without hijacking the control flow.","As a result, OS kernels have been restricting the availability of such non-control data.","This forces attackers to continue to search for more exploitable non-control data in OS kernels.","However, discovering unknown non-control data can be daunting because they are often tied heavily to semantics and lack universal patterns.   ","We make two contributions in this paper: (1) discover critical non-control objects in the file subsystem and (2) analyze their exploitability.","This work represents the first study, with minimal domain knowledge, to semi-automatically discover and evaluate exploitable non-control data within the file subsystem of the Linux kernel.","Our solution utilizes a custom analysis and testing framework that statically and dynamically identifies promising candidate objects.","Furthermore, we categorize these discovered objects into types that are suitable for various exploit strategies, including a novel strategy necessary to overcome the defense that isolates many of these objects.","These objects have the advantage of being exploitable without requiring KASLR, thus making the exploits simpler and more reliable.","We use 18 real-world CVEs to evaluate the exploitability of the file system objects using various exploit strategies.","We develop 10 end-to-end exploits using a subset of CVEs against the kernel with all state-of-the-art mitigations enabled."],"url":"http://arxiv.org/abs/2401.17618v1"}
{"created":"2024-01-31 05:52:11","title":"IGCN: Integrative Graph Convolutional Networks for Multi-modal Data","abstract":"Recent advances in Graph Neural Networks (GNN) have led to a considerable growth in graph data modeling for multi-modal data which contains various types of nodes and edges. Although some integrative prediction solutions have been developed recently for network-structured data, these methods have some restrictions. For a node classification task involving multi-modal data, certain data modalities may perform better when predicting one class, while others might excel in predicting a different class. Thus, to obtain a better learning representation, advanced computational methodologies are required for the integrative analysis of multi-modal data. Moreover, existing integrative tools lack a comprehensive and cohesive understanding of the rationale behind their specific predictions, making them unsuitable for enhancing model interpretability. Addressing these restrictions, we introduce a novel integrative neural network approach for multi-modal data networks, named Integrative Graph Convolutional Networks (IGCN). IGCN learns node embeddings from multiple topologies and fuses the multiple node embeddings into a weighted form by assigning attention coefficients to the node embeddings. Our proposed attention mechanism helps identify which types of data receive more emphasis for each sample to predict a certain class. Therefore, IGCN has the potential to unravel previously unknown characteristics within different node classification tasks. We benchmarked IGCN on several datasets from different domains, including a multi-omics dataset to predict cancer subtypes and a multi-modal clinical dataset to predict the progression of Alzheimer's disease. Experimental results show that IGCN outperforms or is on par with the state-of-the-art and baseline methods.","sentences":["Recent advances in Graph Neural Networks (GNN) have led to a considerable growth in graph data modeling for multi-modal data which contains various types of nodes and edges.","Although some integrative prediction solutions have been developed recently for network-structured data, these methods have some restrictions.","For a node classification task involving multi-modal data, certain data modalities may perform better when predicting one class, while others might excel in predicting a different class.","Thus, to obtain a better learning representation, advanced computational methodologies are required for the integrative analysis of multi-modal data.","Moreover, existing integrative tools lack a comprehensive and cohesive understanding of the rationale behind their specific predictions, making them unsuitable for enhancing model interpretability.","Addressing these restrictions, we introduce a novel integrative neural network approach for multi-modal data networks, named Integrative Graph Convolutional Networks (IGCN).","IGCN learns node embeddings from multiple topologies and fuses the multiple node embeddings into a weighted form by assigning attention coefficients to the node embeddings.","Our proposed attention mechanism helps identify which types of data receive more emphasis for each sample to predict a certain class.","Therefore, IGCN has the potential to unravel previously unknown characteristics within different node classification tasks.","We benchmarked IGCN on several datasets from different domains, including a multi-omics dataset to predict cancer subtypes and a multi-modal clinical dataset to predict the progression of Alzheimer's disease.","Experimental results show that IGCN outperforms or is on par with the state-of-the-art and baseline methods."],"url":"http://arxiv.org/abs/2401.17612v1"}
{"created":"2024-01-31 05:46:44","title":"Estimating Diffusion Degree on Graph Streams","abstract":"The challenges of graph stream algorithms are twofold. First, each edge needs to be processed only once, and second, it needs to work on highly constrained memory. Diffusion degree is a measure of node centrality that can be calculated (for all nodes) trivially for static graphs using a single Breadth-First Search (BFS). However, keeping track of the Diffusion Degree in a graph stream is nontrivial. The memory requirement for exact calculation is equivalent to keeping the whole graph in memory. The present paper proposes an estimator (or sketch) of diffusion degree for graph streams. We prove the correctness of the proposed sketch and the upper bound of the estimated error. Given $\\epsilon, \\delta \\in (0,1)$, we achieve error below $\\epsilon(b_u-a_u)d_u\\lambda$ in node $u$ with probability $1-\\delta$ by utilizing $O(n\\frac1{\\epsilon^2}\\log{\\frac1{\\delta}})$ space, where $b_u$ and $a_u$ are the maximum and minimum degrees of neighbors of $u$, $\\lambda$ is diffusion probability, and $d_u$ is the degree of node $u$. With the help of this sketch, we propose an algorithm to extract the top-$k$ influencing nodes in the graph stream. Comparative experiments show that the spread of top-$k$ nodes by the proposed graph stream algorithm is equivalent to or better than the spread of top-$k$ nodes extracted by the exact algorithm.","sentences":["The challenges of graph stream algorithms are twofold.","First, each edge needs to be processed only once, and second, it needs to work on highly constrained memory.","Diffusion degree is a measure of node centrality that can be calculated (for all nodes) trivially for static graphs using a single Breadth-First Search (BFS).","However, keeping track of the Diffusion Degree in a graph stream is nontrivial.","The memory requirement for exact calculation is equivalent to keeping the whole graph in memory.","The present paper proposes an estimator (or sketch) of diffusion degree for graph streams.","We prove the correctness of the proposed sketch and the upper bound of the estimated error.","Given $\\epsilon, \\delta \\in (0,1)$, we achieve error below $\\epsilon(b_u-a_u)d_u\\lambda$ in node $u$ with probability $1-\\delta$ by utilizing $O(n\\frac1{\\epsilon^2}\\log{\\frac1{\\delta}})$ space, where $b_u$ and $a_u$ are the maximum and minimum degrees of neighbors of $u$, $\\lambda$ is diffusion probability, and $d_u$ is the degree of node $u$. With the help of this sketch, we propose an algorithm to extract the top-$k$ influencing nodes in the graph stream.","Comparative experiments show that the spread of top-$k$ nodes by the proposed graph stream algorithm is equivalent to or better than the spread of top-$k$ nodes extracted by the exact algorithm."],"url":"http://arxiv.org/abs/2401.17611v1"}
{"created":"2024-01-31 05:44:01","title":"LaneGraph2Seq: Lane Topology Extraction with Language Model via Vertex-Edge Encoding and Connectivity Enhancement","abstract":"Understanding road structures is crucial for autonomous driving. Intricate road structures are often depicted using lane graphs, which include centerline curves and connections forming a Directed Acyclic Graph (DAG). Accurate extraction of lane graphs relies on precisely estimating vertex and edge information within the DAG. Recent research highlights Transformer-based language models' impressive sequence prediction abilities, making them effective for learning graph representations when graph data are encoded as sequences. However, existing studies focus mainly on modeling vertices explicitly, leaving edge information simply embedded in the network. Consequently, these approaches fall short in the task of lane graph extraction. To address this, we introduce LaneGraph2Seq, a novel approach for lane graph extraction. It leverages a language model with vertex-edge encoding and connectivity enhancement. Our serialization strategy includes a vertex-centric depth-first traversal and a concise edge-based partition sequence. Additionally, we use classifier-free guidance combined with nucleus sampling to improve lane connectivity. We validate our method on prominent datasets, nuScenes and Argoverse 2, showcasing consistent and compelling results. Our LaneGraph2Seq approach demonstrates superior performance compared to state-of-the-art techniques in lane graph extraction.","sentences":["Understanding road structures is crucial for autonomous driving.","Intricate road structures are often depicted using lane graphs, which include centerline curves and connections forming a Directed Acyclic Graph (DAG).","Accurate extraction of lane graphs relies on precisely estimating vertex and edge information within the DAG.","Recent research highlights Transformer-based language models' impressive sequence prediction abilities, making them effective for learning graph representations when graph data are encoded as sequences.","However, existing studies focus mainly on modeling vertices explicitly, leaving edge information simply embedded in the network.","Consequently, these approaches fall short in the task of lane graph extraction.","To address this, we introduce LaneGraph2Seq, a novel approach for lane graph extraction.","It leverages a language model with vertex-edge encoding and connectivity enhancement.","Our serialization strategy includes a vertex-centric depth-first traversal and a concise edge-based partition sequence.","Additionally, we use classifier-free guidance combined with nucleus sampling to improve lane connectivity.","We validate our method on prominent datasets, nuScenes and Argoverse 2, showcasing consistent and compelling results.","Our LaneGraph2Seq approach demonstrates superior performance compared to state-of-the-art techniques in lane graph extraction."],"url":"http://arxiv.org/abs/2401.17609v1"}
{"created":"2024-01-31 05:24:23","title":"Ambush from All Sides: Understanding Security Threats in Open-Source Software CI/CD Pipelines","abstract":"The continuous integration and continuous deployment (CI/CD) pipelines are widely adopted on Internet hosting platforms, such as GitHub. With the popularity, the CI/CD pipeline faces various security threats. However, current CI/CD pipelines suffer from malicious code and severe vulnerabilities. Even worse, people have not been fully aware of its attack surfaces and the corresponding impacts.   Therefore, in this paper, we conduct a large-scale measurement and a systematic analysis to reveal the attack surfaces of the CI/CD pipeline and quantify their security impacts. Specifically, for the measurement, we collect a data set of 320,000+ CI/CD pipeline-configured GitHub repositories and build an analysis tool to parse the CI/CD pipelines and extract security-critical usages. Besides, current CI/CD ecosystem heavily relies on several core scripts, which may lead to a single point of failure. While the CI/CD pipelines contain sensitive information/operations, making them the attacker's favorite targets.   Inspired by the measurement findings, we abstract the threat model and the attack approach toward CI/CD pipelines, followed by a systematic analysis of attack surfaces, attack strategies, and the corresponding impacts. We further launch case studies on five attacks in real-world CI/CD environments to validate the revealed attack surfaces. Finally, we give suggestions on mitigating attacks on CI/CD scripts, including securing CI/CD configurations, securing CI/CD scripts, and improving CI/CD infrastructure.","sentences":["The continuous integration and continuous deployment (CI/CD) pipelines are widely adopted on Internet hosting platforms, such as GitHub.","With the popularity, the CI/CD pipeline faces various security threats.","However, current CI/CD pipelines suffer from malicious code and severe vulnerabilities.","Even worse, people have not been fully aware of its attack surfaces and the corresponding impacts.   ","Therefore, in this paper, we conduct a large-scale measurement and a systematic analysis to reveal the attack surfaces of the CI/CD pipeline and quantify their security impacts.","Specifically, for the measurement, we collect a data set of 320,000+ CI/CD pipeline-configured GitHub repositories and build an analysis tool to parse the CI/CD pipelines and extract security-critical usages.","Besides, current CI/CD ecosystem heavily relies on several core scripts, which may lead to a single point of failure.","While the CI/CD pipelines contain sensitive information/operations, making them the attacker's favorite targets.   ","Inspired by the measurement findings, we abstract the threat model and the attack approach toward CI/CD pipelines, followed by a systematic analysis of attack surfaces, attack strategies, and the corresponding impacts.","We further launch case studies on five attacks in real-world CI/CD environments to validate the revealed attack surfaces.","Finally, we give suggestions on mitigating attacks on CI/CD scripts, including securing CI/CD configurations, securing CI/CD scripts, and improving CI/CD infrastructure."],"url":"http://arxiv.org/abs/2401.17606v1"}
{"created":"2024-01-31 05:23:08","title":"Exploring Uni-manual Around Ear Off-Device Gestures for Earables","abstract":"Small form factor limits physical input space in earable (i.e., ear-mounted wearable) devices. Off-device earable inputs in alternate mid-air and on-skin around-ear interaction spaces using uni-manual gestures can address this input space limitation. Segmenting these alternate interaction spaces to create multiple gesture regions for reusing off-device gestures can expand earable input vocabulary by a large margin. Although prior earable interaction research has explored off-device gesture preferences and recognition techniques in such interaction spaces, supporting gesture reuse over multiple gesture regions needs further exploration. We collected and analyzed 7560 uni-manual gesture motion data from 18 participants to explore earable gesture reuse by segmentation of on-skin and mid-air spaces around the ear. Our results show that gesture performance degrades significantly beyond 3 mid-air and 5 on-skin around-ear gesture regions for different uni-manual gesture classes (e.g., swipe, pinch, tap). We also present qualitative findings on most and least preferred regions (and associated boundaries) by end-users for different uni-manual gesture shapes across both interaction spaces for earable devices. Our results complement earlier elicitation studies and interaction technologies for earables to help expand the gestural input vocabulary and potentially drive future commercialization of such devices.","sentences":["Small form factor limits physical input space in earable (i.e., ear-mounted wearable) devices.","Off-device earable inputs in alternate mid-air and on-skin around-ear interaction spaces using uni-manual gestures can address this input space limitation.","Segmenting these alternate interaction spaces to create multiple gesture regions for reusing off-device gestures can expand earable input vocabulary by a large margin.","Although prior earable interaction research has explored off-device gesture preferences and recognition techniques in such interaction spaces, supporting gesture reuse over multiple gesture regions needs further exploration.","We collected and analyzed 7560 uni-manual gesture motion data from 18 participants to explore earable gesture reuse by segmentation of on-skin and mid-air spaces around the ear.","Our results show that gesture performance degrades significantly beyond 3 mid-air and 5 on-skin around-ear gesture regions for different uni-manual gesture classes (e.g., swipe, pinch, tap).","We also present qualitative findings on most and least preferred regions (and associated boundaries) by end-users for different uni-manual gesture shapes across both interaction spaces for earable devices.","Our results complement earlier elicitation studies and interaction technologies for earables to help expand the gestural input vocabulary and potentially drive future commercialization of such devices."],"url":"http://arxiv.org/abs/2401.17605v1"}
{"created":"2024-01-31 05:20:29","title":"Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition","abstract":"Cued Speech (CS) is a pure visual coding method used by hearing-impaired people that combines lip reading with several specific hand shapes to make the spoken language visible. Automatic CS recognition (ACSR) seeks to transcribe visual cues of speech into text, which can help hearing-impaired people to communicate effectively. The visual information of CS contains lip reading and hand cueing, thus the fusion of them plays an important role in ACSR. However, most previous fusion methods struggle to capture the global dependency present in long sequence inputs of multi-modal CS data. As a result, these methods generally fail to learn the effective cross-modal relationships that contribute to the fusion. Recently, attention-based transformers have been a prevalent idea for capturing the global dependency over the long sequence in multi-modal fusion, but existing multi-modal fusion transformers suffer from both poor recognition accuracy and inefficient computation for the ACSR task. To address these problems, we develop a novel computation and parameter efficient multi-modal fusion transformer by proposing a novel Token-Importance-Aware Attention mechanism (TIAA), where a token utilization rate (TUR) is formulated to select the important tokens from the multi-modal streams. More precisely, TIAA firstly models the modality-specific fine-grained temporal dependencies over all tokens of each modality, and then learns the efficient cross-modal interaction for the modality-shared coarse-grained temporal dependencies over the important tokens of different modalities. Besides, a light-weight gated hidden projection is designed to control the feature flows of TIAA. The resulting model, named Economical Cued Speech Fusion Transformer (EcoCued), achieves state-of-the-art performance on all existing CS datasets, compared with existing transformer-based fusion methods and ACSR fusion methods.","sentences":["Cued Speech (CS) is a pure visual coding method used by hearing-impaired people that combines lip reading with several specific hand shapes to make the spoken language visible.","Automatic CS recognition (ACSR) seeks to transcribe visual cues of speech into text, which can help hearing-impaired people to communicate effectively.","The visual information of CS contains lip reading and hand cueing, thus the fusion of them plays an important role in ACSR.","However, most previous fusion methods struggle to capture the global dependency present in long sequence inputs of multi-modal CS data.","As a result, these methods generally fail to learn the effective cross-modal relationships that contribute to the fusion.","Recently, attention-based transformers have been a prevalent idea for capturing the global dependency over the long sequence in multi-modal fusion, but existing multi-modal fusion transformers suffer from both poor recognition accuracy and inefficient computation for the ACSR task.","To address these problems, we develop a novel computation and parameter efficient multi-modal fusion transformer by proposing a novel Token-Importance-Aware Attention mechanism (TIAA), where a token utilization rate (TUR) is formulated to select the important tokens from the multi-modal streams.","More precisely, TIAA firstly models the modality-specific fine-grained temporal dependencies over all tokens of each modality, and then learns the efficient cross-modal interaction for the modality-shared coarse-grained temporal dependencies over the important tokens of different modalities.","Besides, a light-weight gated hidden projection is designed to control the feature flows of TIAA.","The resulting model, named Economical Cued Speech Fusion Transformer (EcoCued), achieves state-of-the-art performance on all existing CS datasets, compared with existing transformer-based fusion methods and ACSR fusion methods."],"url":"http://arxiv.org/abs/2401.17604v1"}
{"created":"2024-01-31 05:11:00","title":"Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning","abstract":"In this study, we aim to address the task of assertion detection when extracting medical concepts from clinical notes, a key process in clinical natural language processing (NLP). Assertion detection in clinical NLP usually involves identifying assertion types for medical concepts in the clinical text, namely certainty (whether the medical concept is positive, negated, possible, or hypothetical), temporality (whether the medical concept is for present or the past history), and experiencer (whether the medical concept is described for the patient or a family member). These assertion types are essential for healthcare professionals to quickly and clearly understand the context of medical conditions from unstructured clinical texts, directly influencing the quality and outcomes of patient care. Although widely used, traditional methods, particularly rule-based NLP systems and machine learning or deep learning models, demand intensive manual efforts to create patterns and tend to overlook less common assertion types, leading to an incomplete understanding of the context. To address this challenge, our research introduces a novel methodology that utilizes Large Language Models (LLMs) pre-trained on a vast array of medical data for assertion detection. We enhanced the current method with advanced reasoning techniques, including Tree of Thought (ToT), Chain of Thought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank Adaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010 assertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11 improvements over the previous works. To further assess the generalizability of our approach, we extended our evaluation to a local dataset that focused on sleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31 higher than the previous method.","sentences":["In this study, we aim to address the task of assertion detection when extracting medical concepts from clinical notes, a key process in clinical natural language processing (NLP).","Assertion detection in clinical NLP usually involves identifying assertion types for medical concepts in the clinical text, namely certainty (whether the medical concept is positive, negated, possible, or hypothetical), temporality (whether the medical concept is for present or the past history), and experiencer (whether the medical concept is described for the patient or a family member).","These assertion types are essential for healthcare professionals to quickly and clearly understand the context of medical conditions from unstructured clinical texts, directly influencing the quality and outcomes of patient care.","Although widely used, traditional methods, particularly rule-based NLP systems and machine learning or deep learning models, demand intensive manual efforts to create patterns and tend to overlook less common assertion types, leading to an incomplete understanding of the context.","To address this challenge, our research introduces a novel methodology that utilizes Large Language Models (LLMs) pre-trained on a vast array of medical data for assertion detection.","We enhanced the current method with advanced reasoning techniques, including Tree of Thought (ToT), Chain of Thought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank Adaptation (LoRA) fine-tuning.","We first evaluated the model on the i2b2 2010 assertion dataset.","Our method achieved a micro-averaged F-1 of 0.89, with 0.11 improvements over the previous works.","To further assess the generalizability of our approach, we extended our evaluation to a local dataset that focused on sleep concept extraction.","Our approach achieved an F-1 of 0.74, which is 0.31 higher than the previous method."],"url":"http://arxiv.org/abs/2401.17602v1"}
{"created":"2024-01-31 04:57:12","title":"Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data","abstract":"Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting tasks. Our benchmark will be made publicly available at https://vleo.danielz.ch/ and on Hugging Face at https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70 for easy model evaluation.","sentences":["Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions.","However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data.","In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks.","Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation.","We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting tasks.","Our benchmark will be made publicly available at https://vleo.danielz.ch/ and on Hugging Face at https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70 for easy model evaluation."],"url":"http://arxiv.org/abs/2401.17600v1"}
{"created":"2024-01-31 04:54:17","title":"A Graphics Function Standard Specification Validator","abstract":"A validation methodology is proposed and implemented for natural language software specifications of standard graphics functions. Checks are made for consistency, completeness, and lack of ambiguity in data element and function descriptions. Functions and data elements are maintained in a relational database representation. The appropriate checks are performed by sequences of database operations. The relational database manager INGRES was used to support a prototype implementation of the proposed technique. The methodology supports the development of a scenario-based prototype from the information available in the specification. This permits various function sequences to be checked without implementation of the environment specified. The application of a prototype implementation of the proposed methodology to the specification of the Graphics Kernel System (GKS) software package demonstrates the practicability of the method. Several inconsistencies in GKS related to the definition of data elements have been identified.","sentences":["A validation methodology is proposed and implemented for natural language software specifications of standard graphics functions.","Checks are made for consistency, completeness, and lack of ambiguity in data element and function descriptions.","Functions and data elements are maintained in a relational database representation.","The appropriate checks are performed by sequences of database operations.","The relational database manager INGRES was used to support a prototype implementation of the proposed technique.","The methodology supports the development of a scenario-based prototype from the information available in the specification.","This permits various function sequences to be checked without implementation of the environment specified.","The application of a prototype implementation of the proposed methodology to the specification of the Graphics Kernel System (GKS) software package demonstrates the practicability of the method.","Several inconsistencies in GKS related to the definition of data elements have been identified."],"url":"http://arxiv.org/abs/2401.17599v1"}
{"created":"2024-01-31 04:49:04","title":"An Interactive Empirical Approach to the Validation of Software Package Specifications","abstract":"The objective of this research is the development of a practical system to manipulate and validate software package specifications. The validation process developed is based on consistency checks. Furthermore, by means of scenarios, the customer will be able to interactively experience the specified system prior to its implementation. Functions, data, and data types constitute the framework of our validation system. The specification of the Graphical Kernel System (GKS) is a typical example of the target software package specifications to be manipulated.","sentences":["The objective of this research is the development of a practical system to manipulate and validate software package specifications.","The validation process developed is based on consistency checks.","Furthermore, by means of scenarios, the customer will be able to interactively experience the specified system prior to its implementation.","Functions, data, and data types constitute the framework of our validation system.","The specification of the Graphical Kernel System (GKS) is a typical example of the target software package specifications to be manipulated."],"url":"http://arxiv.org/abs/2401.17596v1"}
{"created":"2024-01-31 02:19:35","title":"Three-Stage Adjusted Regression Forecasting (TSARF) for Software Defect Prediction","abstract":"Software reliability growth models (SRGM) enable failure data collected during testing. Specifically, nonhomogeneous Poisson process (NHPP) SRGM are the most commonly employed models. While software reliability growth models are important, efficient modeling of complex software systems increases the complexity of models. Increased model complexity presents a challenge in identifying robust and computationally efficient algorithms to identify model parameters and reduces the generalizability of the models. Existing studies on traditional software reliability growth models suggest that NHPP models characterize defect data as a smooth continuous curve and fail to capture changes in the defect discovery process. Therefore, the model fits well under ideal conditions, but it is not adaptable and will only fit appropriately shaped data. Neural networks and other machine learning methods have been applied to greater effect [5], however limited due to lack of large samples of defect data especially at earlier stages of testing.","sentences":["Software reliability growth models (SRGM) enable failure data collected during testing.","Specifically, nonhomogeneous Poisson process (NHPP) SRGM are the most commonly employed models.","While software reliability growth models are important, efficient modeling of complex software systems increases the complexity of models.","Increased model complexity presents a challenge in identifying robust and computationally efficient algorithms to identify model parameters and reduces the generalizability of the models.","Existing studies on traditional software reliability growth models suggest that NHPP models characterize defect data as a smooth continuous curve and fail to capture changes in the defect discovery process.","Therefore, the model fits well under ideal conditions, but it is not adaptable and will only fit appropriately shaped data.","Neural networks and other machine learning methods have been applied to greater effect [5], however limited due to lack of large samples of defect data especially at earlier stages of testing."],"url":"http://arxiv.org/abs/2401.17545v1"}
{"created":"2024-01-31 02:14:31","title":"Fr\u00e9chet Distance for Offline Evaluation of Information Retrieval Systems with Sparse Labels","abstract":"The rapid advancement of natural language processing, information retrieval (IR), computer vision, and other technologies has presented significant challenges in evaluating the performance of these systems. One of the main challenges is the scarcity of human-labeled data, which hinders the fair and accurate assessment of these systems. In this work, we specifically focus on evaluating IR systems with sparse labels, borrowing from recent research on evaluating computer vision tasks. taking inspiration from the success of using Fr\\'echet Inception Distance (FID) in assessing text-to-image generation systems. We propose leveraging the Fr\\'echet Distance to measure the distance between the distributions of relevant judged items and retrieved results. Our experimental results on MS MARCO V1 dataset and TREC Deep Learning Tracks query sets demonstrate the effectiveness of the Fr\\'echet Distance as a metric for evaluating IR systems, particularly in settings where a few labels are available. This approach contributes to the advancement of evaluation methodologies in real-world scenarios such as the assessment of generative IR systems.","sentences":["The rapid advancement of natural language processing, information retrieval (IR), computer vision, and other technologies has presented significant challenges in evaluating the performance of these systems.","One of the main challenges is the scarcity of human-labeled data, which hinders the fair and accurate assessment of these systems.","In this work, we specifically focus on evaluating IR systems with sparse labels, borrowing from recent research on evaluating computer vision tasks.","taking inspiration from the success of using Fr\\'echet Inception Distance (FID) in assessing text-to-image generation systems.","We propose leveraging the Fr\\'echet Distance to measure the distance between the distributions of relevant judged items and retrieved results.","Our experimental results on MS MARCO V1 dataset and TREC Deep Learning Tracks query sets demonstrate the effectiveness of the Fr\\'echet Distance as a metric for evaluating IR systems, particularly in settings where a few labels are available.","This approach contributes to the advancement of evaluation methodologies in real-world scenarios such as the assessment of generative IR systems."],"url":"http://arxiv.org/abs/2401.17543v1"}
{"created":"2024-01-31 02:09:21","title":"Data-Effective Learning: A Comprehensive Medical Benchmark","abstract":"Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive experimental results show the baseline MedDEL can achieve performance comparable to the original large dataset with only 5% of the data. Establishing such an open data-effective learning benchmark is crucial for the medical AI research community because it facilitates efficient data use, promotes collaborative breakthroughs, and fosters the development of cost-effective, scalable, and impactful healthcare solutions. The project can be accessed at https://github.com/shadow2469/Data-Effective-Learning-A-Comprehensive-Medical-Benchmark.git.","sentences":["Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value.","Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations.","However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied.","To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field.","This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance.","Our extensive experimental results show the baseline MedDEL can achieve performance comparable to the original large dataset with only 5% of the data.","Establishing such an open data-effective learning benchmark is crucial for the medical AI research community because it facilitates efficient data use, promotes collaborative breakthroughs, and fosters the development of cost-effective, scalable, and impactful healthcare solutions.","The project can be accessed at https://github.com/shadow2469/Data-Effective-Learning-A-Comprehensive-Medical-Benchmark.git."],"url":"http://arxiv.org/abs/2401.17542v1"}
{"created":"2024-01-31 02:08:43","title":"Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration","abstract":"Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses representational information, achieves a balance in improving ECE while preserving accuracy relatively. This finding is pivotal, as it demonstrates a feasible path to maintaining robustness without compromising accuracy. Nonetheless, our experiments also caution against over-regularization, which can diminish accuracy. This underscores the necessity for a systematic approach in evaluating out-of-distribution generalization metrics, one that beyond mere accuracy to address the nuanced interplay between accuracy and calibration.","sentences":["Machine learning models traditionally assume that training and test data are independently and identically distributed.","However, in real-world applications, the test distribution often differs from training.","This problem, known as out-of-distribution generalization, challenges conventional models.","Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness.","However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods.","Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric.","ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features.","Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses representational information, achieves a balance in improving ECE while preserving accuracy relatively.","This finding is pivotal, as it demonstrates a feasible path to maintaining robustness without compromising accuracy.","Nonetheless, our experiments also caution against over-regularization, which can diminish accuracy.","This underscores the necessity for a systematic approach in evaluating out-of-distribution generalization metrics, one that beyond mere accuracy to address the nuanced interplay between accuracy and calibration."],"url":"http://arxiv.org/abs/2401.17541v1"}
{"created":"2024-01-31 01:37:33","title":"PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs","abstract":"It is well acknowledged that incorporating explicit knowledge graphs (KGs) can benefit question answering. Existing approaches typically follow a grounding-reasoning pipeline in which entity nodes are first grounded for the query (question and candidate answers), and then a reasoning module reasons over the matched multi-hop subgraph for answer prediction. Although the pipeline largely alleviates the issue of extracting essential information from giant KGs, efficiency is still an open challenge when scaling up hops in grounding the subgraphs. In this paper, we target at finding semantically related entity nodes in the subgraph to improve the efficiency of graph reasoning with KG. We propose a grounding-pruning-reasoning pipeline to prune noisy nodes, remarkably reducing the computation cost and memory usage while also obtaining decent subgraph representation. In detail, the pruning module first scores concept nodes based on the dependency distance between matched spans and then prunes the nodes according to score ranks. To facilitate the evaluation of pruned subgraphs, we also propose a graph attention network (GAT) based module to reason with the subgraph data. Experimental results on CommonsenseQA and OpenBookQA demonstrate the effectiveness of our method.","sentences":["It is well acknowledged that incorporating explicit knowledge graphs (KGs) can benefit question answering.","Existing approaches typically follow a grounding-reasoning pipeline in which entity nodes are first grounded for the query (question and candidate answers), and then a reasoning module reasons over the matched multi-hop subgraph for answer prediction.","Although the pipeline largely alleviates the issue of extracting essential information from giant KGs, efficiency is still an open challenge when scaling up hops in grounding the subgraphs.","In this paper, we target at finding semantically related entity nodes in the subgraph to improve the efficiency of graph reasoning with KG.","We propose a grounding-pruning-reasoning pipeline to prune noisy nodes, remarkably reducing the computation cost and memory usage while also obtaining decent subgraph representation.","In detail, the pruning module first scores concept nodes based on the dependency distance between matched spans and then prunes the nodes according to score ranks.","To facilitate the evaluation of pruned subgraphs, we also propose a graph attention network (GAT) based module to reason with the subgraph data.","Experimental results on CommonsenseQA","and OpenBookQA demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2401.17536v1"}
{"created":"2024-01-31 01:09:40","title":"Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming","abstract":"Cutting planes (cuts) play an important role in solving mixed-integer linear programs (MILPs), as they significantly tighten the dual bounds and improve the solving performance. A key problem for cuts is when to stop cuts generation, which is important for the efficiency of solving MILPs. However, many modern MILP solvers employ hard-coded heuristics to tackle this problem, which tends to neglect underlying patterns among MILPs from certain applications. To address this challenge, we formulate the cuts generation stopping problem as a reinforcement learning problem and propose a novel hybrid graph representation model (HYGRO) to learn effective stopping strategies. An appealing feature of HYGRO is that it can effectively capture both the dynamic and static features of MILPs, enabling dynamic decision-making for the stopping strategies. To the best of our knowledge, HYGRO is the first data-driven method to tackle the cuts generation stopping problem. By integrating our approach with modern solvers, experiments demonstrate that HYGRO significantly improves the efficiency of solving MILPs compared to competitive baselines, achieving up to 31% improvement.","sentences":["Cutting planes (cuts) play an important role in solving mixed-integer linear programs (MILPs), as they significantly tighten the dual bounds and improve the solving performance.","A key problem for cuts is when to stop cuts generation, which is important for the efficiency of solving MILPs.","However, many modern MILP solvers employ hard-coded heuristics to tackle this problem, which tends to neglect underlying patterns among MILPs from certain applications.","To address this challenge, we formulate the cuts generation stopping problem as a reinforcement learning problem and propose a novel hybrid graph representation model (HYGRO) to learn effective stopping strategies.","An appealing feature of HYGRO is that it can effectively capture both the dynamic and static features of MILPs, enabling dynamic decision-making for the stopping strategies.","To the best of our knowledge, HYGRO is the first data-driven method to tackle the cuts generation stopping problem.","By integrating our approach with modern solvers, experiments demonstrate that HYGRO significantly improves the efficiency of solving MILPs compared to competitive baselines, achieving up to 31% improvement."],"url":"http://arxiv.org/abs/2401.17527v1"}
{"created":"2024-01-31 00:43:30","title":"Game-Theoretic Unlearnable Example Generator","abstract":"Unlearnable example attacks are data poisoning attacks aiming to degrade the clean test accuracy of deep learning by adding imperceptible perturbations to the training samples, which can be formulated as a bi-level optimization problem. However, directly solving this optimization problem is intractable for deep neural networks. In this paper, we investigate unlearnable example attacks from a game-theoretic perspective, by formulating the attack as a nonzero sum Stackelberg game. First, the existence of game equilibria is proved under the normal setting and the adversarial training setting. It is shown that the game equilibrium gives the most powerful poison attack in that the victim has the lowest test accuracy among all networks within the same hypothesis space, when certain loss functions are used. Second, we propose a novel attack method, called the Game Unlearnable Example (GUE), which has three main gradients. (1) The poisons are obtained by directly solving the equilibrium of the Stackelberg game with a first-order algorithm. (2) We employ an autoencoder-like generative network model as the poison attacker. (3) A novel payoff function is introduced to evaluate the performance of the poison. Comprehensive experiments demonstrate that GUE can effectively poison the model in various scenarios. Furthermore, the GUE still works by using a relatively small percentage of the training data to train the generator, and the poison generator can generalize to unseen data well. Our implementation code can be found at https://github.com/hong-xian/gue.","sentences":["Unlearnable example attacks are data poisoning attacks aiming to degrade the clean test accuracy of deep learning by adding imperceptible perturbations to the training samples, which can be formulated as a bi-level optimization problem.","However, directly solving this optimization problem is intractable for deep neural networks.","In this paper, we investigate unlearnable example attacks from a game-theoretic perspective, by formulating the attack as a nonzero sum Stackelberg game.","First, the existence of game equilibria is proved under the normal setting and the adversarial training setting.","It is shown that the game equilibrium gives the most powerful poison attack in that the victim has the lowest test accuracy among all networks within the same hypothesis space, when certain loss functions are used.","Second, we propose a novel attack method, called the Game Unlearnable Example (GUE), which has three main gradients.","(1) The poisons are obtained by directly solving the equilibrium of the Stackelberg game with a first-order algorithm.","(2) We employ an autoencoder-like generative network model as the poison attacker.","(3) A novel payoff function is introduced to evaluate the performance of the poison.","Comprehensive experiments demonstrate that GUE can effectively poison the model in various scenarios.","Furthermore, the GUE still works by using a relatively small percentage of the training data to train the generator, and the poison generator can generalize to unseen data well.","Our implementation code can be found at https://github.com/hong-xian/gue."],"url":"http://arxiv.org/abs/2401.17523v1"}
{"created":"2024-01-31 00:15:34","title":"FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation","abstract":"A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for classification. We conduct extensive experiments on 24 real-world domain pairs to show the effectiveness of our method over strong domain-invariant learning methods. Our analysis sheds light on why masked language modeling improves target-domain classification performance in prompt-based UDA. We discover that MLM helps the model learn both semantic and background knowledge of a domain, which are both beneficial for downstream classification.","sentences":["A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation.","However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training.","The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string.","To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks.","Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for classification.","We conduct extensive experiments on 24 real-world domain pairs to show the effectiveness of our method over strong domain-invariant learning methods.","Our analysis sheds light on why masked language modeling improves target-domain classification performance in prompt-based UDA.","We discover that MLM helps the model learn both semantic and background knowledge of a domain, which are both beneficial for downstream classification."],"url":"http://arxiv.org/abs/2401.17514v1"}
{"created":"2024-01-30 23:54:43","title":"Anything in Any Scene: Photorealistic Video Object Insertion","abstract":"Realistic video simulation has shown significant potential across diverse applications, from virtual reality to film production. This is particularly true for scenarios where capturing videos in real-world settings is either impractical or expensive. Existing approaches in video simulation often fail to accurately model the lighting environment, represent the object geometry, or achieve high levels of photorealism. In this paper, we propose Anything in Any Scene, a novel and generic framework for realistic video simulation that seamlessly inserts any object into an existing dynamic video with a strong emphasis on physical realism. Our proposed general framework encompasses three key processes: 1) integrating a realistic object into a given scene video with proper placement to ensure geometric realism; 2) estimating the sky and environmental lighting distribution and simulating realistic shadows to enhance the light realism; 3) employing a style transfer network that refines the final video output to maximize photorealism. We experimentally demonstrate that Anything in Any Scene framework produces simulated videos of great geometric realism, lighting realism, and photorealism. By significantly mitigating the challenges associated with video data generation, our framework offers an efficient and cost-effective solution for acquiring high-quality videos. Furthermore, its applications extend well beyond video data augmentation, showing promising potential in virtual reality, video editing, and various other video-centric applications. Please check our project website https://anythinginanyscene.github.io for access to our project code and more high-resolution video results.","sentences":["Realistic video simulation has shown significant potential across diverse applications, from virtual reality to film production.","This is particularly true for scenarios where capturing videos in real-world settings is either impractical or expensive.","Existing approaches in video simulation often fail to accurately model the lighting environment, represent the object geometry, or achieve high levels of photorealism.","In this paper, we propose Anything in Any Scene, a novel and generic framework for realistic video simulation that seamlessly inserts any object into an existing dynamic video with a strong emphasis on physical realism.","Our proposed general framework encompasses three key processes: 1) integrating a realistic object into a given scene video with proper placement to ensure geometric realism; 2) estimating the sky and environmental lighting distribution and simulating realistic shadows to enhance the light realism; 3) employing a style transfer network that refines the final video output to maximize photorealism.","We experimentally demonstrate that Anything in Any Scene framework produces simulated videos of great geometric realism, lighting realism, and photorealism.","By significantly mitigating the challenges associated with video data generation, our framework offers an efficient and cost-effective solution for acquiring high-quality videos.","Furthermore, its applications extend well beyond video data augmentation, showing promising potential in virtual reality, video editing, and various other video-centric applications.","Please check our project website https://anythinginanyscene.github.io for access to our project code and more high-resolution video results."],"url":"http://arxiv.org/abs/2401.17509v1"}
{"created":"2024-01-30 23:39:40","title":"CaMU: Disentangling Causal Effects in Deep Model Unlearning","abstract":"Machine unlearning requires removing the information of forgetting data while keeping the necessary information of remaining data. Despite recent advancements in this area, existing methodologies mainly focus on the effect of removing forgetting data without considering the negative impact this can have on the information of the remaining data, resulting in significant performance degradation after data removal. Although some methods try to repair the performance of remaining data after removal, the forgotten information can also return after repair. Such an issue is due to the intricate intertwining of the forgetting and remaining data. Without adequately differentiating the influence of these two kinds of data on the model, existing algorithms take the risk of either inadequate removal of the forgetting data or unnecessary loss of valuable information from the remaining data. To address this shortcoming, the present study undertakes a causal analysis of the unlearning and introduces a novel framework termed Causal Machine Unlearning (CaMU). This framework adds intervention on the information of remaining data to disentangle the causal effects between forgetting data and remaining data. Then CaMU eliminates the causal impact associated with forgetting data while concurrently preserving the causal relevance of the remaining data. Comprehensive empirical results on various datasets and models suggest that CaMU enhances performance on the remaining data and effectively minimizes the influences of forgetting data. Notably, this work is the first to interpret deep model unlearning tasks from a new perspective of causality and provide a solution based on causal analysis, which opens up new possibilities for future research in deep model unlearning.","sentences":["Machine unlearning requires removing the information of forgetting data while keeping the necessary information of remaining data.","Despite recent advancements in this area, existing methodologies mainly focus on the effect of removing forgetting data without considering the negative impact this can have on the information of the remaining data, resulting in significant performance degradation after data removal.","Although some methods try to repair the performance of remaining data after removal, the forgotten information can also return after repair.","Such an issue is due to the intricate intertwining of the forgetting and remaining data.","Without adequately differentiating the influence of these two kinds of data on the model, existing algorithms take the risk of either inadequate removal of the forgetting data or unnecessary loss of valuable information from the remaining data.","To address this shortcoming, the present study undertakes a causal analysis of the unlearning and introduces a novel framework termed Causal Machine Unlearning (CaMU).","This framework adds intervention on the information of remaining data to disentangle the causal effects between forgetting data and remaining data.","Then CaMU eliminates the causal impact associated with forgetting data while concurrently preserving the causal relevance of the remaining data.","Comprehensive empirical results on various datasets and models suggest that CaMU enhances performance on the remaining data and effectively minimizes the influences of forgetting data.","Notably, this work is the first to interpret deep model unlearning tasks from a new perspective of causality and provide a solution based on causal analysis, which opens up new possibilities for future research in deep model unlearning."],"url":"http://arxiv.org/abs/2401.17504v1"}
{"created":"2024-01-30 23:13:41","title":"AdvGPS: Adversarial GPS for Multi-Agent Perception Attack","abstract":"The multi-agent perception system collects visual data from sensors located on various agents and leverages their relative poses determined by GPS signals to effectively fuse information, mitigating the limitations of single-agent sensing, such as occlusion. However, the precision of GPS signals can be influenced by a range of factors, including wireless transmission and obstructions like buildings. Given the pivotal role of GPS signals in perception fusion and the potential for various interference, it becomes imperative to investigate whether specific GPS signals can easily mislead the multi-agent perception system. To address this concern, we frame the task as an adversarial attack challenge and introduce \\textsc{AdvGPS}, a method capable of generating adversarial GPS signals which are also stealthy for individual agents within the system, significantly reducing object detection accuracy. To enhance the success rates of these attacks in a black-box scenario, we introduce three types of statistically sensitive natural discrepancies: appearance-based discrepancy, distribution-based discrepancy, and task-aware discrepancy. Our extensive experiments on the OPV2V dataset demonstrate that these attacks substantially undermine the performance of state-of-the-art methods, showcasing remarkable transferability across different point cloud based 3D detection systems. This alarming revelation underscores the pressing need to address security implications within multi-agent perception systems, thereby underscoring a critical area of research.","sentences":["The multi-agent perception system collects visual data from sensors located on various agents and leverages their relative poses determined by GPS signals to effectively fuse information, mitigating the limitations of single-agent sensing, such as occlusion.","However, the precision of GPS signals can be influenced by a range of factors, including wireless transmission and obstructions like buildings.","Given the pivotal role of GPS signals in perception fusion and the potential for various interference, it becomes imperative to investigate whether specific GPS signals can easily mislead the multi-agent perception system.","To address this concern, we frame the task as an adversarial attack challenge and introduce \\textsc{AdvGPS}, a method capable of generating adversarial GPS signals which are also stealthy for individual agents within the system, significantly reducing object detection accuracy.","To enhance the success rates of these attacks in a black-box scenario, we introduce three types of statistically sensitive natural discrepancies: appearance-based discrepancy, distribution-based discrepancy, and task-aware discrepancy.","Our extensive experiments on the OPV2V dataset demonstrate that these attacks substantially undermine the performance of state-of-the-art methods, showcasing remarkable transferability across different point cloud based 3D detection systems.","This alarming revelation underscores the pressing need to address security implications within multi-agent perception systems, thereby underscoring a critical area of research."],"url":"http://arxiv.org/abs/2401.17499v1"}
{"created":"2024-01-30 23:08:26","title":"Improving QA Model Performance with Cartographic Inoculation","abstract":"QA models are faced with complex and open-ended contextual reasoning problems, but can often learn well-performing solution heuristics by exploiting dataset-specific patterns in their training data. These patterns, or \"dataset artifacts\", reduce the model's ability to generalize to real-world QA problems. Utilizing an ElectraSmallDiscriminator model trained for QA, we analyze the impacts and incidence of dataset artifacts using an adversarial challenge set designed to confuse models reliant on artifacts for prediction. Extending existing work on methods for mitigating artifact impacts, we propose cartographic inoculation, a novel method that fine-tunes models on an optimized subset of the challenge data to reduce model reliance on dataset artifacts. We show that by selectively fine-tuning a model on ambiguous adversarial examples from a challenge set, significant performance improvements can be made on the full challenge dataset with minimal loss of model generalizability to other challenging environments and QA datasets.","sentences":["QA models are faced with complex and open-ended contextual reasoning problems, but can often learn well-performing solution heuristics by exploiting dataset-specific patterns in their training data.","These patterns, or \"dataset artifacts\", reduce the model's ability to generalize to real-world QA problems.","Utilizing an ElectraSmallDiscriminator model trained for QA, we analyze the impacts and incidence of dataset artifacts using an adversarial challenge set designed to confuse models reliant on artifacts for prediction.","Extending existing work on methods for mitigating artifact impacts, we propose cartographic inoculation, a novel method that fine-tunes models on an optimized subset of the challenge data to reduce model reliance on dataset artifacts.","We show that by selectively fine-tuning a model on ambiguous adversarial examples from a challenge set, significant performance improvements can be made on the full challenge dataset with minimal loss of model generalizability to other challenging environments and QA datasets."],"url":"http://arxiv.org/abs/2401.17498v1"}
