{"created":"2023-12-06 18:59:19","title":"OneLLM: One Framework to Align All Modalities with Language","abstract":"Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we first train an image projection module to connect a vision encoder with LLM. Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing. Finally, we progressively align more modalities to LLM with the UPM. To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance. Code, data, model and online demo are available at https://github.com/csuhan/OneLLM","sentences":["Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability.","However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities.","In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework.","We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline.","In detail, we first train an image projection module to connect a vision encoder with LLM.","Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing.","Finally, we progressively align more modalities to LLM with the UPM.","To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity.","OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance.","Code, data, model and online demo are available at https://github.com/csuhan/OneLLM"],"url":"http://arxiv.org/abs/2312.03700v1"}
{"created":"2023-12-06 18:47:06","title":"Testing Connectedness of Images","abstract":"We investigate algorithms for testing whether an image is connected. Given a proximity parameter $\\epsilon\\in(0,1)$ and query access to a black-and-white image represented by an $n\\times n$ matrix of Boolean pixel values, a (1-sided error) connectedness tester accepts if the image is connected and rejects with probability at least 2/3 if the image is $\\epsilon$-far from connected. We show that connectedness can be tested nonadaptively with $O(\\frac 1{\\epsilon^2})$ queries and adaptively with $O(\\frac{1}{\\epsilon^{3/2}} \\sqrt{\\log\\frac{1}{\\epsilon}})$ queries. The best connectedness tester to date, by Berman, Raskhodnikova, and Yaroslavtsev (STOC 2014) had query complexity $O(\\frac 1{\\epsilon^2}\\log \\frac 1{\\epsilon})$ and was adaptive. We also prove that every nonadaptive, 1-sided error tester for connectedness must make $\\Omega(\\frac 1\\epsilon\\log \\frac 1\\epsilon)$ queries.","sentences":["We investigate algorithms for testing whether an image is connected.","Given a proximity parameter $\\epsilon\\in(0,1)$ and query access to a black-and-white image represented by an $n\\times n$ matrix of Boolean pixel values, a (1-sided error) connectedness tester accepts if the image is connected and rejects with probability at least 2/3 if the image is $\\epsilon$-far from connected.","We show that connectedness can be tested nonadaptively with $O(\\frac 1{\\epsilon^2})$ queries and adaptively with $O(\\frac{1}{\\epsilon^{3/2}} \\sqrt{\\log\\frac{1}{\\epsilon}})$ queries.","The best connectedness tester to date, by Berman, Raskhodnikova, and Yaroslavtsev (STOC 2014) had query complexity $O(\\frac 1{\\epsilon^2}\\log \\frac 1{\\epsilon})$ and was adaptive.","We also prove that every nonadaptive, 1-sided error tester for connectedness must make $\\Omega(\\frac 1\\epsilon\\log","\\frac 1\\epsilon)$ queries."],"url":"http://arxiv.org/abs/2312.03681v1"}
{"created":"2023-12-06 18:39:29","title":"GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models","abstract":"This paper introduces GeoShapley, a game theory approach to measuring spatial effects in machine learning models. GeoShapley extends the Nobel Prize-winning Shapley value framework in game theory by conceptualizing location as a player in a model prediction game, which enables the quantification of the importance of location and the synergies between location and other features in a model. GeoShapley is a model-agnostic approach and can be applied to statistical or black-box machine learning models in various structures. The interpretation of GeoShapley is directly linked with spatially varying coefficient models for explaining spatial effects and additive models for explaining non-spatial effects. Using simulated data, GeoShapley values are validated against known data-generating processes and are used for cross-comparison of seven statistical and machine learning models. An empirical example of house price modeling is used to illustrate GeoShapley's utility and interpretation with real world data. The method is available as an open-source Python package named geoshapley.","sentences":["This paper introduces GeoShapley, a game theory approach to measuring spatial effects in machine learning models.","GeoShapley extends the Nobel Prize-winning Shapley value framework in game theory by conceptualizing location as a player in a model prediction game, which enables the quantification of the importance of location and the synergies between location and other features in a model.","GeoShapley is a model-agnostic approach and can be applied to statistical or black-box machine learning models in various structures.","The interpretation of GeoShapley is directly linked with spatially varying coefficient models for explaining spatial effects and additive models for explaining non-spatial effects.","Using simulated data, GeoShapley values are validated against known data-generating processes and are used for cross-comparison of seven statistical and machine learning models.","An empirical example of house price modeling is used to illustrate GeoShapley's utility and interpretation with real world data.","The method is available as an open-source Python package named geoshapley."],"url":"http://arxiv.org/abs/2312.03675v1"}
{"created":"2023-12-06 18:34:01","title":"Towards small and accurate convolutional neural networks for acoustic biodiversity monitoring","abstract":"Automated classification of animal sounds is a prerequisite for large-scale monitoring of biodiversity. Convolutional Neural Networks (CNNs) are among the most promising algorithms but they are slow, often achieve poor classification in the field and typically require large training data sets. Our objective was to design CNNs that are fast at inference time and achieve good classification performance while learning from moderate-sized data. Recordings from a rainforest ecosystem were used. Start and end-point of sounds from 20 bird species were manually annotated. Spectrograms from 10 second segments were used as CNN input. We designed simple CNNs with a frequency unwrapping layer (SIMP-FU models) such that any output unit was connected to all spectrogram frequencies but only to a sub-region of time, the Receptive Field (RF). Our models allowed experimentation with different RF durations. Models either used the time-indexed labels that encode start and end-point of sounds or simpler segment-level labels. Models learning from time-indexed labels performed considerably better than their segment-level counterparts. Best classification performances was achieved for models with intermediate RF duration of 1.5 seconds. The best SIMP-FU models achieved AUCs over 0.95 in 18 of 20 classes on the test set. On compact low-cost hardware the best SIMP-FU models evaluated up to seven times faster than real-time data acquisition. RF duration was a major driver of classification performance. The optimum of 1.5 s was in the same range as the duration of the sounds. Our models achieved good classification performance while learning from moderate-sized training data. This is explained by the usage of time-indexed labels during training and adequately sized RF. Results confirm the feasibility of deploying small CNNs with good classification performance on compact low-cost devices.","sentences":["Automated classification of animal sounds is a prerequisite for large-scale monitoring of biodiversity.","Convolutional Neural Networks (CNNs) are among the most promising algorithms but they are slow, often achieve poor classification in the field and typically require large training data sets.","Our objective was to design CNNs that are fast at inference time and achieve good classification performance while learning from moderate-sized data.","Recordings from a rainforest ecosystem were used.","Start and end-point of sounds from 20 bird species were manually annotated.","Spectrograms from 10 second segments were used as CNN input.","We designed simple CNNs with a frequency unwrapping layer (SIMP-FU models) such that any output unit was connected to all spectrogram frequencies but only to a sub-region of time, the Receptive Field (RF).","Our models allowed experimentation with different RF durations.","Models either used the time-indexed labels that encode start and end-point of sounds or simpler segment-level labels.","Models learning from time-indexed labels performed considerably better than their segment-level counterparts.","Best classification performances was achieved for models with intermediate RF duration of 1.5 seconds.","The best SIMP-FU models achieved AUCs over 0.95 in 18 of 20 classes on the test set.","On compact low-cost hardware the best SIMP-FU models evaluated up to seven times faster than real-time data acquisition.","RF duration was a major driver of classification performance.","The optimum of 1.5 s was in the same range as the duration of the sounds.","Our models achieved good classification performance while learning from moderate-sized training data.","This is explained by the usage of time-indexed labels during training and adequately sized RF.","Results confirm the feasibility of deploying small CNNs with good classification performance on compact low-cost devices."],"url":"http://arxiv.org/abs/2312.03666v1"}
{"created":"2023-12-06 18:33:50","title":"Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia","abstract":"Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act \"reasonably\", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.","sentences":["Agent-based modeling has been around for decades, and applied widely across the social and natural sciences.","The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s.","Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another.","Rather, GABMs are constructed using an LLM to apply common sense to situations, act \"reasonably\", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside.","Here we present Concordia, a library to facilitate constructing and working with GABMs.","Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments.","Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval.","A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact.","Agents take actions by describing what they want to do in natural language.","The GM then translates their actions into appropriate implementations.","In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects.","In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.).","Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data."],"url":"http://arxiv.org/abs/2312.03664v1"}
{"created":"2023-12-06 18:20:46","title":"Efficient Inverse Design Optimization through Multi-fidelity Simulations, Machine Learning, and Search Space Reduction Strategies","abstract":"This paper introduces a methodology designed to augment the inverse design optimization process in scenarios constrained by limited compute, through the strategic synergy of multi-fidelity evaluations, machine learning models, and optimization algorithms. The proposed methodology is analyzed on two distinct engineering inverse design problems: airfoil inverse design and the scalar field reconstruction problem. It leverages a machine learning model trained with low-fidelity simulation data, in each optimization cycle, thereby proficiently predicting a target variable and discerning whether a high-fidelity simulation is necessitated, which notably conserves computational resources. Additionally, the machine learning model is strategically deployed prior to optimization to reduce the search space, thereby further accelerating convergence toward the optimal solution. The methodology has been employed to enhance two optimization algorithms, namely Differential Evolution and Particle Swarm Optimization. Comparative analyses illustrate performance improvements across both algorithms. Notably, this method is adeptly adaptable across any inverse design application, facilitating a harmonious synergy between a representative low-fidelity machine learning model, and high-fidelity simulation, and can be seamlessly applied across any variety of population-based optimization algorithms.","sentences":["This paper introduces a methodology designed to augment the inverse design optimization process in scenarios constrained by limited compute, through the strategic synergy of multi-fidelity evaluations, machine learning models, and optimization algorithms.","The proposed methodology is analyzed on two distinct engineering inverse design problems: airfoil inverse design and the scalar field reconstruction problem.","It leverages a machine learning model trained with low-fidelity simulation data, in each optimization cycle, thereby proficiently predicting a target variable and discerning whether a high-fidelity simulation is necessitated, which notably conserves computational resources.","Additionally, the machine learning model is strategically deployed prior to optimization to reduce the search space, thereby further accelerating convergence toward the optimal solution.","The methodology has been employed to enhance two optimization algorithms, namely Differential Evolution and Particle Swarm Optimization.","Comparative analyses illustrate performance improvements across both algorithms.","Notably, this method is adeptly adaptable across any inverse design application, facilitating a harmonious synergy between a representative low-fidelity machine learning model, and high-fidelity simulation, and can be seamlessly applied across any variety of population-based optimization algorithms."],"url":"http://arxiv.org/abs/2312.03654v1"}
{"created":"2023-12-06 18:13:21","title":"MICRACLE: Inverse Reinforcement and Curriculum Learning Model for Human-inspired Mobile Robot Navigation","abstract":"In emergency scenarios, mobile robots must navigate like humans, interpreting stimuli to locate potential victims rapidly without interfering with first responders. Existing socially-aware navigation algorithms face computational and adaptability challenges. To overcome these, we propose a solution, MIRACLE -- an inverse reinforcement and curriculum learning model, that employs gamified learning to gather stimuli-driven human navigational data. This data is then used to train a Deep Inverse Maximum Entropy Reinforcement Learning model, reducing reliance on demonstrator abilities. Testing reveals a low loss of 2.7717 within a 400-sized environment, signifying human-like response replication. Current databases lack comprehensive stimuli-driven data, necessitating our approach. By doing so, we enable robots to navigate emergency situations with human-like perception, enhancing their life-saving capabilities.","sentences":["In emergency scenarios, mobile robots must navigate like humans, interpreting stimuli to locate potential victims rapidly without interfering with first responders.","Existing socially-aware navigation algorithms face computational and adaptability challenges.","To overcome these, we propose a solution, MIRACLE -- an inverse reinforcement and curriculum learning model, that employs gamified learning to gather stimuli-driven human navigational data.","This data is then used to train a Deep Inverse Maximum Entropy Reinforcement Learning model, reducing reliance on demonstrator abilities.","Testing reveals a low loss of 2.7717 within a 400-sized environment, signifying human-like response replication.","Current databases lack comprehensive stimuli-driven data, necessitating our approach.","By doing so, we enable robots to navigate emergency situations with human-like perception, enhancing their life-saving capabilities."],"url":"http://arxiv.org/abs/2312.03651v1"}
{"created":"2023-12-06 18:03:04","title":"An Irredundant Decomposition of Data Flow with Affine Dependences","abstract":"Optimization pipelines targeting polyhedral programs try to maximize the compute throughput. Traditional approaches favor reuse and temporal locality; while the communicated volume can be low, failure to optimize spatial locality may cause a low I/O performance.   Memory allocation schemes using data partitioning such as data tiling can improve the spatial locality, but they are domain-specific and rarely applied by compilers when an existing allocation is supplied.   In this paper, we propose to derive a partitioned memory allocation for tiled polyhedral programs using their data flow information. We extend the existing MARS partitioning to handle affine dependences, and determine which dependences can lead to a regular, simple control flow for communications.   While this paper consists in a theoretical study, previous work on data partitioning in inter-node scenarios has shown performance improvements due to better bandwidth utilization.","sentences":["Optimization pipelines targeting polyhedral programs try to maximize the compute throughput.","Traditional approaches favor reuse and temporal locality; while the communicated volume can be low, failure to optimize spatial locality may cause a low I/O performance.   ","Memory allocation schemes using data partitioning such as data tiling can improve the spatial locality, but they are domain-specific and rarely applied by compilers when an existing allocation is supplied.   ","In this paper, we propose to derive a partitioned memory allocation for tiled polyhedral programs using their data flow information.","We extend the existing MARS partitioning to handle affine dependences, and determine which dependences can lead to a regular, simple control flow for communications.   ","While this paper consists in a theoretical study, previous work on data partitioning in inter-node scenarios has shown performance improvements due to better bandwidth utilization."],"url":"http://arxiv.org/abs/2312.03646v1"}
{"created":"2023-12-06 17:59:34","title":"MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment","abstract":"Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios where online interaction is impractical or risky. While independent learning in MARL offers flexibility and scalability, accurately assigning credit to individual agents in offline settings poses challenges due to partial observability and emergent behavior. Directly transferring the online credit assignment method to offline settings results in suboptimal outcomes due to the absence of real-time feedback and intricate agent interactions. Our approach, MACCA, characterizing the generative process as a Dynamic Bayesian Network, captures relationships between environmental variables, states, actions, and rewards. Estimating this model on offline data, MACCA can learn each agent's contribution by analyzing the causal relationship of their individual rewards, ensuring accurate and interpretable credit assignment. Additionally, the modularity of our approach allows it to seamlessly integrate with various offline MARL methods. Theoretically, we proved that under the setting of the offline dataset, the underlying causal structure and the function for generating the individual rewards of agents are identifiable, which laid the foundation for the correctness of our modeling. Experimentally, we tested MACCA in two environments, including discrete and continuous action settings. The results show that MACCA outperforms SOTA methods and improves performance upon their backbones.","sentences":["Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios where online interaction is impractical or risky.","While independent learning in MARL offers flexibility and scalability, accurately assigning credit to individual agents in offline settings poses challenges due to partial observability and emergent behavior.","Directly transferring the online credit assignment method to offline settings results in suboptimal outcomes due to the absence of real-time feedback and intricate agent interactions.","Our approach, MACCA, characterizing the generative process as a Dynamic Bayesian Network, captures relationships between environmental variables, states, actions, and rewards.","Estimating this model on offline data, MACCA can learn each agent's contribution by analyzing the causal relationship of their individual rewards, ensuring accurate and interpretable credit assignment.","Additionally, the modularity of our approach allows it to seamlessly integrate with various offline MARL methods.","Theoretically, we proved that under the setting of the offline dataset, the underlying causal structure and the function for generating the individual rewards of agents are identifiable, which laid the foundation for the correctness of our modeling.","Experimentally, we tested MACCA in two environments, including discrete and continuous action settings.","The results show that MACCA outperforms SOTA methods and improves performance upon their backbones."],"url":"http://arxiv.org/abs/2312.03644v1"}
{"created":"2023-12-06 17:53:06","title":"Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap with Extremely Limited Data","abstract":"Recent advances in machine learning, specifically transformer architecture, have led to significant advancements in commercial domains. These powerful models have demonstrated superior capability to learn complex relationships and often generalize better to new data and problems. This paper presents a novel transformer-powered approach for enhancing prediction accuracy in multi-modal output scenarios, where sparse experimental data is supplemented with simulation data. The proposed approach integrates transformer-based architecture with a novel graph-based hyper-parameter optimization technique. The resulting system not only effectively reduces simulation bias, but also achieves superior prediction accuracy compared to the prior method. We demonstrate the efficacy of our approach on inertial confinement fusion experiments, where only 10 shots of real-world data are available, as well as synthetic versions of these experiments.","sentences":["Recent advances in machine learning, specifically transformer architecture, have led to significant advancements in commercial domains.","These powerful models have demonstrated superior capability to learn complex relationships and often generalize better to new data and problems.","This paper presents a novel transformer-powered approach for enhancing prediction accuracy in multi-modal output scenarios, where sparse experimental data is supplemented with simulation data.","The proposed approach integrates transformer-based architecture with a novel graph-based hyper-parameter optimization technique.","The resulting system not only effectively reduces simulation bias, but also achieves superior prediction accuracy compared to the prior method.","We demonstrate the efficacy of our approach on inertial confinement fusion experiments, where only 10 shots of real-world data are available, as well as synthetic versions of these experiments."],"url":"http://arxiv.org/abs/2312.03642v1"}
{"created":"2023-12-06 17:49:57","title":"MotionCtrl: A Unified and Flexible Motion Controller for Video Generation","abstract":"Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement. Accurate control of both camera and object motion is essential for video generation. However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity. Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion. The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data. Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion. 2) Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos. 3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods.","sentences":["Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement.","Accurate control of both camera and object motion is essential for video generation.","However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity.","Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion.","The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data.","Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion.","2)","Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos.","3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained.","Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods."],"url":"http://arxiv.org/abs/2312.03641v1"}
{"created":"2023-12-06 17:31:16","title":"Fed-urlBERT: Client-side Lightweight Federated Transformers for URL Threat Analysis","abstract":"In evolving cyber landscapes, the detection of malicious URLs calls for cooperation and knowledge sharing across domains. However, collaboration is often hindered by concerns over privacy and business sensitivities. Federated learning addresses these issues by enabling multi-clients collaboration without direct data exchange. Unfortunately, if highly expressive Transformer models are used, clients may face intolerable computational burdens, and the exchange of weights could quickly deplete network bandwidth. In this paper, we propose Fed-urlBERT, a federated URL pre-trained model designed to address both privacy concerns and the need for cross-domain collaboration in cybersecurity. Fed-urlBERT leverages split learning to divide the pre-training model into client and server part, so that the client part takes up less extensive computation resources and bandwidth. Our appraoch achieves performance comparable to centralized model under both independently and identically distributed (IID) and two non-IID data scenarios. Significantly, our federated model shows about an 7% decrease in the FPR compared to the centralized model. Additionally, we implement an adaptive local aggregation strategy that mitigates heterogeneity among clients, demonstrating promising performance improvements. Overall, our study validates the applicability of the proposed Transformer federated learning for URL threat analysis, establishing a foundation for real-world collaborative cybersecurity efforts. The source code is accessible at https://github.com/Davidup1/FedURLBERT.","sentences":["In evolving cyber landscapes, the detection of malicious URLs calls for cooperation and knowledge sharing across domains.","However, collaboration is often hindered by concerns over privacy and business sensitivities.","Federated learning addresses these issues by enabling multi-clients collaboration without direct data exchange.","Unfortunately, if highly expressive Transformer models are used, clients may face intolerable computational burdens, and the exchange of weights could quickly deplete network bandwidth.","In this paper, we propose Fed-urlBERT, a federated URL pre-trained model designed to address both privacy concerns and the need for cross-domain collaboration in cybersecurity.","Fed-urlBERT leverages split learning to divide the pre-training model into client and server part, so that the client part takes up less extensive computation resources and bandwidth.","Our appraoch achieves performance comparable to centralized model under both independently and identically distributed (IID) and two non-IID data scenarios.","Significantly, our federated model shows about an 7% decrease in the FPR compared to the centralized model.","Additionally, we implement an adaptive local aggregation strategy that mitigates heterogeneity among clients, demonstrating promising performance improvements.","Overall, our study validates the applicability of the proposed Transformer federated learning for URL threat analysis, establishing a foundation for real-world collaborative cybersecurity efforts.","The source code is accessible at https://github.com/Davidup1/FedURLBERT."],"url":"http://arxiv.org/abs/2312.03636v1"}
{"created":"2023-12-06 17:29:03","title":"Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models","abstract":"Interactions with virtual assistants typically start with a trigger phrase followed by a command. In this work, we explore the possibility of making these interactions more natural by eliminating the need for a trigger phrase. Our goal is to determine whether a user addressed the virtual assistant based on signals obtained from the streaming audio recorded by the device microphone. We address this task by combining 1-best hypotheses and decoder signals from an automatic speech recognition system with acoustic representations from an audio encoder as input features to a large language model (LLM). In particular, we are interested in data and resource efficient systems that require only a small amount of training data and can operate in scenarios with only a single frozen LLM available on a device. For this reason, our model is trained on 80k or less examples of multimodal data using a combination of low-rank adaptation and prefix tuning. We compare the proposed system to unimodal baselines and show that the multimodal approach achieves lower equal-error-rates (EERs), while using only a fraction of the training data. We also show that low-dimensional specialized audio representations lead to lower EERs than high-dimensional general audio representations.","sentences":["Interactions with virtual assistants typically start with a trigger phrase followed by a command.","In this work, we explore the possibility of making these interactions more natural by eliminating the need for a trigger phrase.","Our goal is to determine whether a user addressed the virtual assistant based on signals obtained from the streaming audio recorded by the device microphone.","We address this task by combining 1-best hypotheses and decoder signals from an automatic speech recognition system with acoustic representations from an audio encoder as input features to a large language model (LLM).","In particular, we are interested in data and resource efficient systems that require only a small amount of training data and can operate in scenarios with only a single frozen LLM available on a device.","For this reason, our model is trained on 80k or less examples of multimodal data using a combination of low-rank adaptation and prefix tuning.","We compare the proposed system to unimodal baselines and show that the multimodal approach achieves lower equal-error-rates (EERs), while using only a fraction of the training data.","We also show that low-dimensional specialized audio representations lead to lower EERs than high-dimensional general audio representations."],"url":"http://arxiv.org/abs/2312.03632v1"}
{"created":"2023-12-06 16:56:28","title":"Physical Symbolic Optimization","abstract":"We present a framework for constraining the automatic sequential generation of equations to obey the rules of dimensional analysis by construction. Combining this approach with reinforcement learning, we built $\\Phi$-SO, a Physical Symbolic Optimization method for recovering analytical functions from physical data leveraging units constraints. Our symbolic regression algorithm achieves state-of-the-art results in contexts in which variables and constants have known physical units, outperforming all other methods on SRBench's Feynman benchmark in the presence of noise (exceeding 0.1%) and showing resilience even in the presence of significant (10%) levels of noise.","sentences":["We present a framework for constraining the automatic sequential generation of equations to obey the rules of dimensional analysis by construction.","Combining this approach with reinforcement learning, we built $\\Phi$-SO, a Physical Symbolic Optimization method for recovering analytical functions from physical data leveraging units constraints.","Our symbolic regression algorithm achieves state-of-the-art results in contexts in which variables and constants have known physical units, outperforming all other methods on SRBench's Feynman benchmark in the presence of noise (exceeding 0.1%) and showing resilience even in the presence of significant (10%) levels of noise."],"url":"http://arxiv.org/abs/2312.03612v1"}
{"created":"2023-12-06 16:54:24","title":"Automated Multimodal Data Annotation via Calibration With Indoor Positioning System","abstract":"Learned object detection methods based on fusion of LiDAR and camera data require labeled training samples, but niche applications, such as warehouse robotics or automated infrastructure, require semantic classes not available in large existing datasets. Therefore, to facilitate the rapid creation of multimodal object detection datasets and alleviate the burden of human labeling, we propose a novel automated annotation pipeline. Our method uses an indoor positioning system (IPS) to produce accurate detection labels for both point clouds and images and eliminates manual annotation entirely. In an experiment, the system annotates objects of interest 261.8 times faster than a human baseline and speeds up end-to-end dataset creation by 61.5%.","sentences":["Learned object detection methods based on fusion of LiDAR and camera data require labeled training samples, but niche applications, such as warehouse robotics or automated infrastructure, require semantic classes not available in large existing datasets.","Therefore, to facilitate the rapid creation of multimodal object detection datasets and alleviate the burden of human labeling, we propose a novel automated annotation pipeline.","Our method uses an indoor positioning system (IPS) to produce accurate detection labels for both point clouds and images and eliminates manual annotation entirely.","In an experiment, the system annotates objects of interest 261.8 times faster than a human baseline and speeds up end-to-end dataset creation by 61.5%."],"url":"http://arxiv.org/abs/2312.03608v1"}
{"created":"2023-12-06 16:53:17","title":"DiffusionSat: A Generative Foundation Model for Satellite Imagery","abstract":"Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets. As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, superresolution given multi-spectral inputs and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale $\\textit{generative}$ foundation model for satellite imagery.","sentences":["Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video.","However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction.","Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them.","Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images.","In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets.","As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information.","Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, superresolution given multi-spectral inputs and in-painting.","Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale $\\textit{generative}$ foundation model for satellite imagery."],"url":"http://arxiv.org/abs/2312.03606v1"}
{"created":"2023-12-06 16:32:46","title":"Streaming Algorithms for the $k$-Submodular Cover Problem","abstract":"Given a natural number $k\\ge 2$, we consider the $k$-submodular cover problem ($k$-SC). The objective is to find a minimum cost subset of a ground set $\\mathcal{X}$ subject to the value of a $k$-submodular utility function being at least a certain predetermined value $\\tau$. For this problem, we design a bicriteria algorithm with a cost at most $O(1/\\epsilon)$ times the optimal value, while the utility is at least $(1-\\epsilon)\\tau/r$, where $r$ depends on the monotonicity of $g$.","sentences":["Given a natural number $k\\ge 2$, we consider the $k$-submodular cover problem ($k$-SC).","The objective is to find a minimum cost subset of a ground set $\\mathcal{X}$ subject to the value of a $k$-submodular utility function being at least a certain predetermined value $\\tau$. For this problem, we design a bicriteria algorithm with a cost at most $O(1/\\epsilon)$ times the optimal value, while the utility is at least $(1-\\epsilon)\\tau/r$, where $r$ depends on the monotonicity of $g$."],"url":"http://arxiv.org/abs/2312.03593v1"}
{"created":"2023-12-06 16:15:00","title":"Improving Bias Mitigation through Bias Experts in Natural Language Understanding","abstract":"Biases in the dataset often enable the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data. To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples identified by an auxiliary model, which is trained with explicit bias labels. However, finding a type of bias in datasets is a costly process. Therefore, recent studies have attempted to make the auxiliary model biased without the guidance (or annotation) of bias labels, by constraining the model's training environment or the capability of the model itself. Despite the promising debiasing results of recent works, the multi-class learning objective, which has been naively used to train the auxiliary model, may harm the bias mitigation effect due to its regularization effect and competitive nature across classes. As an alternative, we propose a new debiasing framework that introduces binary classifiers between the auxiliary model and the main model, coined bias experts. Specifically, each bias expert is trained on a binary classification task derived from the multi-class classification task via the One-vs-Rest approach. Experimental results demonstrate that our proposed strategy improves the bias identification ability of the auxiliary model. Consequently, our debiased model consistently outperforms the state-of-the-art on various challenge datasets.","sentences":["Biases in the dataset often enable the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data.","To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples identified by an auxiliary model, which is trained with explicit bias labels.","However, finding a type of bias in datasets is a costly process.","Therefore, recent studies have attempted to make the auxiliary model biased without the guidance (or annotation) of bias labels, by constraining the model's training environment or the capability of the model itself.","Despite the promising debiasing results of recent works, the multi-class learning objective, which has been naively used to train the auxiliary model, may harm the bias mitigation effect due to its regularization effect and competitive nature across classes.","As an alternative, we propose a new debiasing framework that introduces binary classifiers between the auxiliary model and the main model, coined bias experts.","Specifically, each bias expert is trained on a binary classification task derived from the multi-class classification task via the One-vs-Rest approach.","Experimental results demonstrate that our proposed strategy improves the bias identification ability of the auxiliary model.","Consequently, our debiased model consistently outperforms the state-of-the-art on various challenge datasets."],"url":"http://arxiv.org/abs/2312.03577v1"}
{"created":"2023-12-06 15:59:06","title":"XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering","abstract":"Extractive question answering (QA) systems can enable physicians and researchers to query medical records, a foundational capability for designing clinical studies and understanding patient medical history. However, building these systems typically requires expert-annotated QA pairs. Large language models (LLMs), which can perform extractive QA, depend on high quality data in their prompts, specialized for the application domain. We introduce a novel approach, XAIQA, for generating synthetic QA pairs at scale from data naturally available in electronic health records. Our method uses the idea of a classification model explainer to generate questions and answers about medical concepts corresponding to medical codes. In an expert evaluation with two physicians, our method identifies $2.2\\times$ more semantic matches and $3.8\\times$ more clinical abbreviations than two popular approaches that use sentence transformers to create QA pairs. In an ML evaluation, adding our QA pairs improves performance of GPT-4 as an extractive QA model, including on difficult questions. In both the expert and ML evaluations, we examine trade-offs between our method and sentence transformers for QA pair generation depending on question difficulty.","sentences":["Extractive question answering (QA) systems can enable physicians and researchers to query medical records, a foundational capability for designing clinical studies and understanding patient medical history.","However, building these systems typically requires expert-annotated QA pairs.","Large language models (LLMs), which can perform extractive QA, depend on high quality data in their prompts, specialized for the application domain.","We introduce a novel approach, XAIQA, for generating synthetic QA pairs at scale from data naturally available in electronic health records.","Our method uses the idea of a classification model explainer to generate questions and answers about medical concepts corresponding to medical codes.","In an expert evaluation with two physicians, our method identifies $2.2\\times$ more semantic matches and $3.8\\times$ more clinical abbreviations than two popular approaches that use sentence transformers to create QA pairs.","In an ML evaluation, adding our QA pairs improves performance of GPT-4 as an extractive QA model, including on difficult questions.","In both the expert and ML evaluations, we examine trade-offs between our method and sentence transformers for QA pair generation depending on question difficulty."],"url":"http://arxiv.org/abs/2312.03567v1"}
{"created":"2023-12-06 15:43:26","title":"MCAIMem: a Mixed SRAM and eDRAM Cell for Area and Energy-efficient on-chip AI Memory","abstract":"AI chips commonly employ SRAM memory as buffers for their reliability and speed, which contribute to high performance. However, SRAM is expensive and demands significant area and energy consumption. Previous studies have explored replacing SRAM with emerging technologies like non-volatile memory, which offers fast-read memory access and a small cell area. Despite these advantages, non-volatile memory's slow write memory access and high write energy consumption prevent it from surpassing SRAM performance in AI applications with extensive memory access requirements. Some research has also investigated eDRAM as an area-efficient on-chip memory with similar access times as SRAM. Still, refresh power remains a concern, leaving the trade-off between performance, area, and power consumption unresolved. To address this issue, our paper presents a novel mixed CMOS cell memory design that balances performance, area, and energy efficiency for AI memory by combining SRAM and eDRAM cells. We consider the proportion ratio of one SRAM and seven eDRAM cells in the memory to achieve area reduction using mixed CMOS cell memory. Additionally, we capitalize on the characteristics of DNN data representation and integrate asymmetric eDRAM cells to lower energy consumption. To validate our proposed MCAIMem solution, we conduct extensive simulations and benchmarking against traditional SRAM. Our results demonstrate that MCAIMem significantly outperforms these alternatives in terms of area and energy efficiency. Specifically, our MCAIMem can reduce the area by 48\\% and energy consumption by 3.4$\\times$ compared to SRAM designs, without incurring any accuracy loss.","sentences":["AI chips commonly employ SRAM memory as buffers for their reliability and speed, which contribute to high performance.","However, SRAM is expensive and demands significant area and energy consumption.","Previous studies have explored replacing SRAM with emerging technologies like non-volatile memory, which offers fast-read memory access and a small cell area.","Despite these advantages, non-volatile memory's slow write memory access and high write energy consumption prevent it from surpassing SRAM performance in AI applications with extensive memory access requirements.","Some research has also investigated eDRAM as an area-efficient on-chip memory with similar access times as SRAM.","Still, refresh power remains a concern, leaving the trade-off between performance, area, and power consumption unresolved.","To address this issue, our paper presents a novel mixed CMOS cell memory design that balances performance, area, and energy efficiency for AI memory by combining SRAM and eDRAM cells.","We consider the proportion ratio of one SRAM and seven eDRAM cells in the memory to achieve area reduction using mixed CMOS cell memory.","Additionally, we capitalize on the characteristics of DNN data representation and integrate asymmetric eDRAM cells to lower energy consumption.","To validate our proposed MCAIMem solution, we conduct extensive simulations and benchmarking against traditional SRAM.","Our results demonstrate that MCAIMem significantly outperforms these alternatives in terms of area and energy efficiency.","Specifically, our MCAIMem can reduce the area by 48\\% and energy consumption by 3.4$\\times$ compared to SRAM designs, without incurring any accuracy loss."],"url":"http://arxiv.org/abs/2312.03559v1"}
{"created":"2023-12-06 15:27:26","title":"Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment","abstract":"Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated remarkable accuracy in a wide range of tasks. However, training these models can incur significant expenses, often requiring tens of thousands of GPUs for months of continuous operation. Typically, this training is carried out in specialized GPU clusters equipped with homogeneous high-speed Remote Direct Memory Access (RDMA) network interface cards (NICs). The acquisition and maintenance of such dedicated clusters is challenging. Current LLM training frameworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on optimizing training within homogeneous cluster settings. In this paper, we introduce Holmes, a training framework for LLMs that employs thoughtfully crafted data and model parallelism strategies over the heterogeneous NIC environment. Our primary technical contribution lies in a novel scheduling method that intelligently allocates distinct computational tasklets in LLM training to specific groups of GPU devices based on the characteristics of their connected NICs. Furthermore, our proposed framework, utilizing pipeline parallel techniques, demonstrates scalability to multiple GPU clusters, even in scenarios without high-speed interconnects between nodes in distinct clusters. We conducted comprehensive experiments that involved various scenarios in the heterogeneous NIC environment. In most cases, our framework achieves performance levels close to those achievable with homogeneous RDMA-capable networks (InfiniBand or RoCE), significantly exceeding training efficiency within the pure Ethernet environment. Additionally, we verified that our framework outperforms other mainstream LLM frameworks under heterogeneous NIC environment in terms of training efficiency and can be seamlessly integrated with them.","sentences":["Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated remarkable accuracy in a wide range of tasks.","However, training these models can incur significant expenses, often requiring tens of thousands of GPUs for months of continuous operation.","Typically, this training is carried out in specialized GPU clusters equipped with homogeneous high-speed Remote Direct Memory Access (RDMA) network interface cards (NICs).","The acquisition and maintenance of such dedicated clusters is challenging.","Current LLM training frameworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on optimizing training within homogeneous cluster settings.","In this paper, we introduce Holmes, a training framework for LLMs that employs thoughtfully crafted data and model parallelism strategies over the heterogeneous NIC environment.","Our primary technical contribution lies in a novel scheduling method that intelligently allocates distinct computational tasklets in LLM training to specific groups of GPU devices based on the characteristics of their connected NICs.","Furthermore, our proposed framework, utilizing pipeline parallel techniques, demonstrates scalability to multiple GPU clusters, even in scenarios without high-speed interconnects between nodes in distinct clusters.","We conducted comprehensive experiments that involved various scenarios in the heterogeneous NIC environment.","In most cases, our framework achieves performance levels close to those achievable with homogeneous RDMA-capable networks (InfiniBand or RoCE), significantly exceeding training efficiency within the pure Ethernet environment.","Additionally, we verified that our framework outperforms other mainstream LLM frameworks under heterogeneous NIC environment in terms of training efficiency and can be seamlessly integrated with them."],"url":"http://arxiv.org/abs/2312.03549v1"}
{"created":"2023-12-06 15:14:30","title":"GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models","abstract":"In the field of autonomous vehicles (AVs), accurately discerning commander intent and executing linguistic commands within a visual context presents a significant challenge. This paper introduces a sophisticated encoder-decoder framework, developed to address visual grounding in AVs.Our Context-Aware Visual Grounding (CAVG) model is an advanced system that integrates five core encoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This integration enables the CAVG model to adeptly capture contextual semantics and to learn human emotional features, augmented by state-of-the-art Large Language Models (LLMs) including GPT-4. The architecture of CAVG is reinforced by the implementation of multi-head cross-modal attention mechanisms and a Region-Specific Dynamic (RSD) layer for attention modulation. This architectural design enables the model to efficiently process and interpret a range of cross-modal inputs, yielding a comprehensive understanding of the correlation between verbal commands and corresponding visual scenes. Empirical evaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that CAVG establishes new standards in prediction accuracy and operational efficiency. Notably, the model exhibits exceptional performance even with limited training data, ranging from 50% to 75% of the full dataset. This feature highlights its effectiveness and potential for deployment in practical AV applications. Moreover, CAVG has shown remarkable robustness and adaptability in challenging scenarios, including long-text command interpretation, low-light conditions, ambiguous command contexts, inclement weather conditions, and densely populated urban environments. The code for the proposed model is available at our Github.","sentences":["In the field of autonomous vehicles (AVs), accurately discerning commander intent and executing linguistic commands within a visual context presents a significant challenge.","This paper introduces a sophisticated encoder-decoder framework, developed to address visual grounding in AVs.","Our Context-Aware Visual Grounding (CAVG) model is an advanced system that integrates five core encoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder.","This integration enables the CAVG model to adeptly capture contextual semantics and to learn human emotional features, augmented by state-of-the-art Large Language Models (LLMs) including GPT-4.","The architecture of CAVG is reinforced by the implementation of multi-head cross-modal attention mechanisms and a Region-Specific Dynamic (RSD) layer for attention modulation.","This architectural design enables the model to efficiently process and interpret a range of cross-modal inputs, yielding a comprehensive understanding of the correlation between verbal commands and corresponding visual scenes.","Empirical evaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that CAVG establishes new standards in prediction accuracy and operational efficiency.","Notably, the model exhibits exceptional performance even with limited training data, ranging from 50% to 75% of the full dataset.","This feature highlights its effectiveness and potential for deployment in practical AV applications.","Moreover, CAVG has shown remarkable robustness and adaptability in challenging scenarios, including long-text command interpretation, low-light conditions, ambiguous command contexts, inclement weather conditions, and densely populated urban environments.","The code for the proposed model is available at our Github."],"url":"http://arxiv.org/abs/2312.03543v1"}
{"created":"2023-12-06 15:07:12","title":"FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation","abstract":"Current state-of-the-art image generation models such as Latent Diffusion Models (LDMs) have demonstrated the capacity to produce visually striking food-related images. However, these generated images often exhibit an artistic or surreal quality that diverges from the authenticity of real-world food representations. This inadequacy renders them impractical for applications requiring realistic food imagery, such as training models for image-based dietary assessment. To address these limitations, we introduce FoodFusion, a Latent Diffusion model engineered specifically for the faithful synthesis of realistic food images from textual descriptions. The development of the FoodFusion model involves harnessing an extensive array of open-source food datasets, resulting in over 300,000 curated image-caption pairs. Additionally, we propose and employ two distinct data cleaning methodologies to ensure that the resulting image-text pairs maintain both realism and accuracy. The FoodFusion model, thus trained, demonstrates a remarkable ability to generate food images that exhibit a significant improvement in terms of both realism and diversity over the publicly available image generation models. We openly share the dataset and fine-tuned models to support advancements in this critical field of food image synthesis at https://bit.ly/genai4good.","sentences":["Current state-of-the-art image generation models such as Latent Diffusion Models (LDMs) have demonstrated the capacity to produce visually striking food-related images.","However, these generated images often exhibit an artistic or surreal quality that diverges from the authenticity of real-world food representations.","This inadequacy renders them impractical for applications requiring realistic food imagery, such as training models for image-based dietary assessment.","To address these limitations, we introduce FoodFusion, a Latent Diffusion model engineered specifically for the faithful synthesis of realistic food images from textual descriptions.","The development of the FoodFusion model involves harnessing an extensive array of open-source food datasets, resulting in over 300,000 curated image-caption pairs.","Additionally, we propose and employ two distinct data cleaning methodologies to ensure that the resulting image-text pairs maintain both realism and accuracy.","The FoodFusion model, thus trained, demonstrates a remarkable ability to generate food images that exhibit a significant improvement in terms of both realism and diversity over the publicly available image generation models.","We openly share the dataset and fine-tuned models to support advancements in this critical field of food image synthesis at https://bit.ly/genai4good."],"url":"http://arxiv.org/abs/2312.03540v1"}
{"created":"2023-12-06 14:54:10","title":"Low-shot Object Learning with Mutual Exclusivity Bias","abstract":"This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias (LSME), the first computational framing of mutual exclusivity bias, a phenomenon commonly observed in infants during word learning. We provide a novel dataset, comprehensive baselines, and a state-of-the-art method to enable the ML community to tackle this challenging learning task. The goal of LSME is to analyze an RGB image of a scene containing multiple objects and correctly associate a previously-unknown object instance with a provided category label. This association is then used to perform low-shot learning to test category generalization. We provide a data generation pipeline for the LSME problem and conduct a thorough analysis of the factors that contribute to its difficulty. Additionally, we evaluate the performance of multiple baselines, including state-of-the-art foundation models. Finally, we present a baseline approach that outperforms state-of-the-art models in terms of low-shot accuracy.","sentences":["This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias (LSME), the first computational framing of mutual exclusivity bias, a phenomenon commonly observed in infants during word learning.","We provide a novel dataset, comprehensive baselines, and a state-of-the-art method to enable the ML community to tackle this challenging learning task.","The goal of LSME is to analyze an RGB image of a scene containing multiple objects and correctly associate a previously-unknown object instance with a provided category label.","This association is then used to perform low-shot learning to test category generalization.","We provide a data generation pipeline for the LSME problem and conduct a thorough analysis of the factors that contribute to its difficulty.","Additionally, we evaluate the performance of multiple baselines, including state-of-the-art foundation models.","Finally, we present a baseline approach that outperforms state-of-the-art models in terms of low-shot accuracy."],"url":"http://arxiv.org/abs/2312.03533v1"}
{"created":"2023-12-06 14:40:05","title":"On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm","abstract":"Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes, achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU (while the SOTA only achieves 21% but requires 6 hours).","sentences":["Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands.","Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training.","However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility.","To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency.","As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data.","Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes, achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU (while the SOTA only achieves 21% but requires 6 hours)."],"url":"http://arxiv.org/abs/2312.03526v1"}
{"created":"2023-12-06 14:34:30","title":"Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling","abstract":"We present an open-source, pip installable toolkit, Sig-Networks, the first of its kind for longitudinal language modelling. A central focus is the incorporation of Signature-based Neural Network models, which have recently shown success in temporal tasks. We apply and extend published research providing a full suite of signature-based models. Their components can be used as PyTorch building blocks in future architectures. Sig-Networks enables task-agnostic dataset plug-in, seamless pre-processing for sequential data, parameter flexibility, automated tuning across a range of models. We examine signature networks under three different NLP tasks of varying temporal granularity: counselling conversations, rumour stance switch and mood changes in social media threads, showing SOTA performance in all three, and provide guidance for future tasks. We release the Toolkit as a PyTorch package with an introductory video, Git repositories for preprocessing and modelling including sample notebooks on the modeled NLP tasks.","sentences":["We present an open-source, pip installable toolkit, Sig-Networks, the first of its kind for longitudinal language modelling.","A central focus is the incorporation of Signature-based Neural Network models, which have recently shown success in temporal tasks.","We apply and extend published research providing a full suite of signature-based models.","Their components can be used as PyTorch building blocks in future architectures.","Sig-Networks enables task-agnostic dataset plug-in, seamless pre-processing for sequential data, parameter flexibility, automated tuning across a range of models.","We examine signature networks under three different NLP tasks of varying temporal granularity: counselling conversations, rumour stance switch and mood changes in social media threads, showing SOTA performance in all three, and provide guidance for future tasks.","We release the Toolkit as a PyTorch package with an introductory video, Git repositories for preprocessing and modelling including sample notebooks on the modeled NLP tasks."],"url":"http://arxiv.org/abs/2312.03523v1"}
{"created":"2023-12-06 14:30:15","title":"Optimal Wildfire Escape Route Planning for Drones under Dynamic Fire and Smoke","abstract":"In recent years, the increasing prevalence and intensity of wildfires have posed significant challenges to emergency response teams. The utilization of unmanned aerial vehicles (UAVs), commonly known as drones, has shown promise in aiding wildfire management efforts. This work focuses on the development of an optimal wildfire escape route planning system specifically designed for drones, considering dynamic fire and smoke models. First, the location of the source of the wildfire can be well located by information fusion between UAV and satellite, and the road conditions in the vicinity of the fire can be assessed and analyzed using multi-channel remote sensing data. Second, the road network can be extracted and segmented in real time using UAV vision technology, and each road in the road network map can be given priority based on the results of road condition classification. Third, the spread model of dynamic fires calculates the new location of the fire source based on the fire intensity, wind speed and direction, and the radius increases as the wildfire spreads. Smoke is generated around the fire source to create a visual representation of a burning fire. Finally, based on the improved A* algorithm, which considers all the above factors, the UAV can quickly plan an escape route based on the starting and destination locations that avoid the location of the fire source and the area where it is spreading. By considering dynamic fire and smoke models, the proposed system enhances the safety and efficiency of drone operations in wildfire environments.","sentences":["In recent years, the increasing prevalence and intensity of wildfires have posed significant challenges to emergency response teams.","The utilization of unmanned aerial vehicles (UAVs), commonly known as drones, has shown promise in aiding wildfire management efforts.","This work focuses on the development of an optimal wildfire escape route planning system specifically designed for drones, considering dynamic fire and smoke models.","First, the location of the source of the wildfire can be well located by information fusion between UAV and satellite, and the road conditions in the vicinity of the fire can be assessed and analyzed using multi-channel remote sensing data.","Second, the road network can be extracted and segmented in real time using UAV vision technology, and each road in the road network map can be given priority based on the results of road condition classification.","Third, the spread model of dynamic fires calculates the new location of the fire source based on the fire intensity, wind speed and direction, and the radius increases as the wildfire spreads.","Smoke is generated around the fire source to create a visual representation of a burning fire.","Finally, based on the improved A* algorithm, which considers all the above factors, the UAV can quickly plan an escape route based on the starting and destination locations that avoid the location of the fire source and the area where it is spreading.","By considering dynamic fire and smoke models, the proposed system enhances the safety and efficiency of drone operations in wildfire environments."],"url":"http://arxiv.org/abs/2312.03521v1"}
{"created":"2023-12-06 14:29:16","title":"Defense Against Adversarial Attacks using Convolutional Auto-Encoders","abstract":"Deep learning models, while achieving state-of-the-art performance on many tasks, are susceptible to adversarial attacks that exploit inherent vulnerabilities in their architectures. Adversarial attacks manipulate the input data with imperceptible perturbations, causing the model to misclassify the data or produce erroneous outputs. This work is based on enhancing the robustness of targeted classifier models against adversarial attacks. To achieve this, an convolutional autoencoder-based approach is employed that effectively counters adversarial perturbations introduced to the input images. By generating images closely resembling the input images, the proposed methodology aims to restore the model's accuracy.","sentences":["Deep learning models, while achieving state-of-the-art performance on many tasks, are susceptible to adversarial attacks that exploit inherent vulnerabilities in their architectures.","Adversarial attacks manipulate the input data with imperceptible perturbations, causing the model to misclassify the data or produce erroneous outputs.","This work is based on enhancing the robustness of targeted classifier models against adversarial attacks.","To achieve this, an convolutional autoencoder-based approach is employed that effectively counters adversarial perturbations introduced to the input images.","By generating images closely resembling the input images, the proposed methodology aims to restore the model's accuracy."],"url":"http://arxiv.org/abs/2312.03520v1"}
{"created":"2023-12-06 14:13:38","title":"Kandinsky 3.0 Technical Report","abstract":"We present Kandinsky 3.0, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0 leverages a two times larger U-Net backbone, a ten times larger text encoder and removes diffusion mapping. We describe the architecture of the model, the data collection procedure, the training technique, and the production system of user interaction. We focus on the key components that, as we have identified as a result of a large number of experiments, had the most significant impact on improving the quality of our model compared to the others. By our side-by-side comparisons, Kandinsky becomes better in text understanding and works better on specific domains. Project page: https://ai-forever.github.io/Kandinsky-3","sentences":["We present Kandinsky 3.0, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation.","Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0 leverages a two times larger U-Net backbone, a ten times larger text encoder and removes diffusion mapping.","We describe the architecture of the model, the data collection procedure, the training technique, and the production system of user interaction.","We focus on the key components that, as we have identified as a result of a large number of experiments, had the most significant impact on improving the quality of our model compared to the others.","By our side-by-side comparisons, Kandinsky becomes better in text understanding and works better on specific domains.","Project page: https://ai-forever.github.io/Kandinsky-3"],"url":"http://arxiv.org/abs/2312.03511v1"}
{"created":"2023-12-06 14:08:05","title":"Gravitational cell detection and tracking in fluorescence microscopy data","abstract":"Automatic detection and tracking of cells in microscopy images are major applications of computer vision technologies in both biomedical research and clinical practice. Though machine learning methods are increasingly common in these fields, classical algorithms still offer significant advantages for both tasks, including better explainability, faster computation, lower hardware requirements and more consistent performance. In this paper, we present a novel approach based on gravitational force fields that can compete with, and potentially outperform modern machine learning models when applied to fluorescence microscopy images. This method includes detection, segmentation, and tracking elements, with the results demonstrated on a Cell Tracking Challenge dataset.","sentences":["Automatic detection and tracking of cells in microscopy images are major applications of computer vision technologies in both biomedical research and clinical practice.","Though machine learning methods are increasingly common in these fields, classical algorithms still offer significant advantages for both tasks, including better explainability, faster computation, lower hardware requirements and more consistent performance.","In this paper, we present a novel approach based on gravitational force fields that can compete with, and potentially outperform modern machine learning models when applied to fluorescence microscopy images.","This method includes detection, segmentation, and tracking elements, with the results demonstrated on a Cell Tracking Challenge dataset."],"url":"http://arxiv.org/abs/2312.03509v1"}
{"created":"2023-12-06 14:06:40","title":"Task-Parameterized Imitation Learning with Time-Sensitive Constraints","abstract":"Programming a robot manipulator should be as intuitive as possible. To achieve that, the paradigm of teaching motion skills by providing few demonstrations has become widely popular in recent years. Probabilistic versions thereof take into account the uncertainty given by the distribution of the training data. However, precise execution of start-, via-, and end-poses at given times can not always be guaranteed. This limits the technology transfer to industrial application. To address this problem, we propose a novel constrained formulation of the Expectation Maximization algorithm for learning Gaussian Mixture Models (GMM) on Riemannian Manifolds. Our approach applies to probabilistic imitation learning and extends also to the well-established TP-GMM framework with Task-Parameterization. It allows to prescribe end-effector poses at defined execution times, for instance for precise pick & place scenarios. The probabilistic approach is compared with state-of-the-art learning-from-demonstration methods using the KUKA LBR iiwa robot. The reader is encouraged to watch the accompanying video available at https://youtu.be/JMI1YxtN9C0","sentences":["Programming a robot manipulator should be as intuitive as possible.","To achieve that, the paradigm of teaching motion skills by providing few demonstrations has become widely popular in recent years.","Probabilistic versions thereof take into account the uncertainty given by the distribution of the training data.","However, precise execution of start-, via-, and end-poses at given times can not always be guaranteed.","This limits the technology transfer to industrial application.","To address this problem, we propose a novel constrained formulation of the Expectation Maximization algorithm for learning Gaussian Mixture Models (GMM) on Riemannian Manifolds.","Our approach applies to probabilistic imitation learning and extends also to the well-established TP-GMM framework with Task-Parameterization.","It allows to prescribe end-effector poses at defined execution times, for instance for precise pick & place scenarios.","The probabilistic approach is compared with state-of-the-art learning-from-demonstration methods using the KUKA LBR iiwa robot.","The reader is encouraged to watch the accompanying video available at https://youtu.be/JMI1YxtN9C0"],"url":"http://arxiv.org/abs/2312.03506v1"}
{"created":"2023-12-06 13:37:59","title":"A Subexponential Time Algorithm for Makespan Scheduling of Unit Jobs with Precedence Constraints","abstract":"In a classical scheduling problem, we are given a set of $n$ jobs of unit length along with precedence constraints, and the goal is to find a schedule of these jobs on $m$ identical machines that minimizes the makespan. Using the standard 3-field notation, it is known as $Pm|\\text{prec}, p_j=1|C_{\\max}$. Settling the complexity of $Pm|\\text{prec}, p_j=1|C_{\\max}$ even for $m=3$ machines is the last open problem from the book of Garey and Johnson [GJ79] for which both upper and lower bounds on the worst-case running times of exact algorithms solving them remain essentially unchanged since the publication of [GJ79]. We present an algorithm for this problem that runs in $(1+\\frac{n}{m})^{\\mathcal{O}(\\sqrt{nm})}$ time. This algorithm is subexponential when $m = o(n)$. In the regime of $m=\\Theta(n)$ we show an algorithm that runs in$\\mathcal{O}(1.997^n)$ time. Before our work, even for $m=3$ machines there were no algorithms known that run in $\\mathcal{O}((2-\\varepsilon)^n)$ time for some $\\varepsilon > 0$.","sentences":["In a classical scheduling problem, we are given a set of $n$ jobs of unit length along with precedence constraints, and the goal is to find a schedule of these jobs on $m$ identical machines that minimizes the makespan.","Using the standard 3-field notation, it is known as $Pm|\\text{prec}, p_j=1|C_{\\max}$. Settling the complexity of $Pm|\\text{prec}, p_j=1|C_{\\max}$ even for $m=3$ machines is the last open problem from the book of Garey and Johnson","[GJ79] for which both upper and lower bounds on the worst-case running times of exact algorithms solving them remain essentially unchanged since the publication of [GJ79].","We present an algorithm for this problem that runs in $(1+\\frac{n}{m})^{\\mathcal{O}(\\sqrt{nm})}$ time.","This algorithm is subexponential when $m =","o(n)$.","In the regime of $m=\\Theta(n)$ we show an algorithm that runs in$\\mathcal{O}(1.997^n)$ time.","Before our work, even for $m=3$ machines there were no algorithms known that run in $\\mathcal{O}((2-\\varepsilon)^n)$ time for some $\\varepsilon > 0$."],"url":"http://arxiv.org/abs/2312.03495v1"}
{"created":"2023-12-06 13:35:57","title":"Radio Source Localization using Sparse Signal Measurements from Uncrewed Ground Vehicles","abstract":"Radio source localization can benefit many fields, including wireless communications, radar, radio astronomy, wireless sensor networks, positioning systems, and surveillance systems. However, accurately estimating the position of a radio transmitter using a remote sensor is not an easy task, as many factors contribute to the highly dynamic behavior of radio signals. In this study, we investigate techniques to use a mobile robot to explore an outdoor area and localize the radio source using sparse Received Signal Strength Indicator (RSSI) measurements. We propose a novel radio source localization method with fast turnaround times and reduced complexity compared to the state-of-the-art. Our technique uses RSSI measurements collected while the robot completed a sparse trajectory using a coverage path planning map. The mean RSSI within each grid cell was used to find the most likely cell containing the source. Three techniques were analyzed with the data from eight field tests using a mobile robot. The proposed method can localize a gas source in a basketball field with a 1.2 m accuracy and within three minutes of convergence time, whereas the state-of-the-art active sensing technique took more than 30 minutes to reach a source estimation accuracy below 1 m.","sentences":["Radio source localization can benefit many fields, including wireless communications, radar, radio astronomy, wireless sensor networks, positioning systems, and surveillance systems.","However, accurately estimating the position of a radio transmitter using a remote sensor is not an easy task, as many factors contribute to the highly dynamic behavior of radio signals.","In this study, we investigate techniques to use a mobile robot to explore an outdoor area and localize the radio source using sparse Received Signal Strength Indicator (RSSI) measurements.","We propose a novel radio source localization method with fast turnaround times and reduced complexity compared to the state-of-the-art.","Our technique uses RSSI measurements collected while the robot completed a sparse trajectory using a coverage path planning map.","The mean RSSI within each grid cell was used to find the most likely cell containing the source.","Three techniques were analyzed with the data from eight field tests using a mobile robot.","The proposed method can localize a gas source in a basketball field with a 1.2 m accuracy and within three minutes of convergence time, whereas the state-of-the-art active sensing technique took more than 30 minutes to reach a source estimation accuracy below 1 m."],"url":"http://arxiv.org/abs/2312.03493v1"}
{"created":"2023-12-06 13:31:55","title":"Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis","abstract":"In text-to-speech (TTS) synthesis, diffusion models have achieved promising generation quality. However, because of the pre-defined data-to-noise diffusion process, their prior distribution is restricted to a noisy representation, which provides little information of the generation target. In this work, we present a novel TTS system, Bridge-TTS, making the first attempt to substitute the noisy Gaussian prior in established diffusion-based TTS methods with a clean and deterministic one, which provides strong structural information of the target. Specifically, we leverage the latent representation obtained from text input as our prior, and build a fully tractable Schrodinger bridge between it and the ground-truth mel-spectrogram, leading to a data-to-data process. Moreover, the tractability and flexibility of our formulation allow us to empirically study the design spaces such as noise schedules, as well as to develop stochastic and deterministic samplers. Experimental results on the LJ-Speech dataset illustrate the effectiveness of our method in terms of both synthesis quality and sampling efficiency, significantly outperforming our diffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast TTS models in few-step scenarios. Project page: https://bridge-tts.github.io/","sentences":["In text-to-speech (TTS) synthesis, diffusion models have achieved promising generation quality.","However, because of the pre-defined data-to-noise diffusion process, their prior distribution is restricted to a noisy representation, which provides little information of the generation target.","In this work, we present a novel TTS system, Bridge-TTS, making the first attempt to substitute the noisy Gaussian prior in established diffusion-based TTS methods with a clean and deterministic one, which provides strong structural information of the target.","Specifically, we leverage the latent representation obtained from text input as our prior, and build a fully tractable Schrodinger bridge between it and the ground-truth mel-spectrogram, leading to a data-to-data process.","Moreover, the tractability and flexibility of our formulation allow us to empirically study the design spaces such as noise schedules, as well as to develop stochastic and deterministic samplers.","Experimental results on the LJ-Speech dataset illustrate the effectiveness of our method in terms of both synthesis quality and sampling efficiency, significantly outperforming our diffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast TTS models in few-step scenarios.","Project page: https://bridge-tts.github.io/"],"url":"http://arxiv.org/abs/2312.03491v1"}
{"created":"2023-12-06 13:10:02","title":"From Detection to Action Recognition: An Edge-Based Pipeline for Robot Human Perception","abstract":"Mobile service robots are proving to be increasingly effective in a range of applications, such as healthcare, monitoring Activities of Daily Living (ADL), and facilitating Ambient Assisted Living (AAL). These robots heavily rely on Human Action Recognition (HAR) to interpret human actions and intentions. However, for HAR to function effectively on service robots, it requires prior knowledge of human presence (human detection) and identification of individuals to monitor (human tracking). In this work, we propose an end-to-end pipeline that encompasses the entire process, starting from human detection and tracking, leading to action recognition. The pipeline is designed to operate in near real-time while ensuring all stages of processing are performed on the edge, reducing the need for centralised computation. To identify the most suitable models for our mobile robot, we conducted a series of experiments comparing state-of-the-art solutions based on both their detection performance and efficiency. To evaluate the effectiveness of our proposed pipeline, we proposed a dataset comprising daily household activities. By presenting our findings and analysing the results, we demonstrate the efficacy of our approach in enabling mobile robots to understand and respond to human behaviour in real-world scenarios relying mainly on the data from their RGB cameras.","sentences":["Mobile service robots are proving to be increasingly effective in a range of applications, such as healthcare, monitoring Activities of Daily Living (ADL), and facilitating Ambient Assisted Living (AAL).","These robots heavily rely on Human Action Recognition (HAR) to interpret human actions and intentions.","However, for HAR to function effectively on service robots, it requires prior knowledge of human presence (human detection) and identification of individuals to monitor (human tracking).","In this work, we propose an end-to-end pipeline that encompasses the entire process, starting from human detection and tracking, leading to action recognition.","The pipeline is designed to operate in near real-time while ensuring all stages of processing are performed on the edge, reducing the need for centralised computation.","To identify the most suitable models for our mobile robot, we conducted a series of experiments comparing state-of-the-art solutions based on both their detection performance and efficiency.","To evaluate the effectiveness of our proposed pipeline, we proposed a dataset comprising daily household activities.","By presenting our findings and analysing the results, we demonstrate the efficacy of our approach in enabling mobile robots to understand and respond to human behaviour in real-world scenarios relying mainly on the data from their RGB cameras."],"url":"http://arxiv.org/abs/2312.03477v1"}
{"created":"2023-12-06 12:41:53","title":"Search Strategies for Self-driving Laboratories with Pending Experiments","abstract":"Self-driving laboratories (SDLs) consist of multiple stations that perform material synthesis and characterisation tasks. To minimize station downtime and maximize experimental throughput, it is practical to run experiments in asynchronous parallel, in which multiple experiments are being performed at once in different stages. Asynchronous parallelization of experiments, however, introduces delayed feedback (i.e. \"pending experiments\"), which is known to reduce Bayesian optimiser performance. Here, we build a simulator for a multi-stage SDL and compare optimisation strategies for dealing with delayed feedback and asynchronous parallelized operation. Using data from a real SDL, we build a ground truth Bayesian optimisation simulator from 177 previously run experiments for maximizing the conductivity of functional coatings. We then compare search strategies such as expected improvement, noisy expected improvement, 4-mode exploration and random sampling. We evaluate their performance in terms of amount of delay and problem dimensionality. Our simulation results showcase the trade-off between the asynchronous parallel operation and delayed feedback.","sentences":["Self-driving laboratories (SDLs) consist of multiple stations that perform material synthesis and characterisation tasks.","To minimize station downtime and maximize experimental throughput, it is practical to run experiments in asynchronous parallel, in which multiple experiments are being performed at once in different stages.","Asynchronous parallelization of experiments, however, introduces delayed feedback (i.e. \"pending experiments\"), which is known to reduce Bayesian optimiser performance.","Here, we build a simulator for a multi-stage SDL and compare optimisation strategies for dealing with delayed feedback and asynchronous parallelized operation.","Using data from a real SDL, we build a ground truth Bayesian optimisation simulator from 177 previously run experiments for maximizing the conductivity of functional coatings.","We then compare search strategies such as expected improvement, noisy expected improvement, 4-mode exploration and random sampling.","We evaluate their performance in terms of amount of delay and problem dimensionality.","Our simulation results showcase the trade-off between the asynchronous parallel operation and delayed feedback."],"url":"http://arxiv.org/abs/2312.03466v1"}
{"created":"2023-12-06 12:37:28","title":"DBCopilot: Scaling Natural Language Querying to Massive Databases","abstract":"Text-to-SQL simplifies database interactions by enabling non-experts to convert their natural language (NL) questions into Structured Query Language (SQL) queries. While recent advances in large language models (LLMs) have improved the zero-shot text-to-SQL paradigm, existing methods face scalability challenges when dealing with massive, dynamically changing databases. This paper introduces DBCopilot, a framework that addresses these challenges by employing a compact and flexible copilot model for routing across massive databases. Specifically, DBCopilot decouples the text-to-SQL process into schema routing and SQL generation, leveraging a lightweight sequence-to-sequence neural network-based router to formulate database connections and navigate natural language questions through databases and tables. The routed schemas and questions are then fed into LLMs for efficient SQL generation. Furthermore, DBCopilot also introduced a reverse schema-to-question generation paradigm, which can learn and adapt the router over massive databases automatically without requiring manual intervention. Experimental results demonstrate that DBCopilot is a scalable and effective solution for real-world text-to-SQL tasks, providing a significant advancement in handling large-scale schemas.","sentences":["Text-to-SQL simplifies database interactions by enabling non-experts to convert their natural language (NL) questions into Structured Query Language (SQL) queries.","While recent advances in large language models (LLMs) have improved the zero-shot text-to-SQL paradigm, existing methods face scalability challenges when dealing with massive, dynamically changing databases.","This paper introduces DBCopilot, a framework that addresses these challenges by employing a compact and flexible copilot model for routing across massive databases.","Specifically, DBCopilot decouples the text-to-SQL process into schema routing and SQL generation, leveraging a lightweight sequence-to-sequence neural network-based router to formulate database connections and navigate natural language questions through databases and tables.","The routed schemas and questions are then fed into LLMs for efficient SQL generation.","Furthermore, DBCopilot also introduced a reverse schema-to-question generation paradigm, which can learn and adapt the router over massive databases automatically without requiring manual intervention.","Experimental results demonstrate that DBCopilot is a scalable and effective solution for real-world text-to-SQL tasks, providing a significant advancement in handling large-scale schemas."],"url":"http://arxiv.org/abs/2312.03463v1"}
{"created":"2023-12-06 12:34:46","title":"Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification","abstract":"The proliferation of Large Language Models (LLMs) has spurred extensive research into LLM-related Prompt investigations, such as Instruction Learning (IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches aim to improve LLMs' responses by enabling them to provide concise statements or examples for deeper contemplation when addressing questions. However, independent thinking by LLMs can introduce variability in their thought processes, leading to potential inaccuracies. In response, our study seeks to bridge the gap between LLM and human-like thinking processes, recognizing that text comprehension begins with understanding individual words. To tackle this challenge, we have expanded the CoT method to cater to a specific domain. Our approach, known as \"Think from Words\" (TFW), initiates the comprehension process at the word level and then extends it to encompass the entire text. We also propose \"TFW with Extra word-level information\" (TFW Extra), augmenting comprehension with additional word-level data. To assess our methods, we employ text classification on six Japanese datasets comprising text-level and word-level elements. Our findings not only validate the effectiveness of TFW but also shed light on the impact of various word-level information types on LLMs' text comprehension, offering insights into their potential to cause misinterpretations and errors in the overall comprehension of the final text.","sentences":["The proliferation of Large Language Models (LLMs) has spurred extensive research into LLM-related Prompt investigations, such as Instruction Learning (IL), In-context Learning (ICL), and Chain-of-Thought (CoT).","These approaches aim to improve LLMs' responses by enabling them to provide concise statements or examples for deeper contemplation when addressing questions.","However, independent thinking by LLMs can introduce variability in their thought processes, leading to potential inaccuracies.","In response, our study seeks to bridge the gap between LLM and human-like thinking processes, recognizing that text comprehension begins with understanding individual words.","To tackle this challenge, we have expanded the CoT method to cater to a specific domain.","Our approach, known as \"Think from Words\" (TFW), initiates the comprehension process at the word level and then extends it to encompass the entire text.","We also propose \"TFW with Extra word-level information\" (TFW Extra), augmenting comprehension with additional word-level data.","To assess our methods, we employ text classification on six Japanese datasets comprising text-level and word-level elements.","Our findings not only validate the effectiveness of TFW but also shed light on the impact of various word-level information types on LLMs' text comprehension, offering insights into their potential to cause misinterpretations and errors in the overall comprehension of the final text."],"url":"http://arxiv.org/abs/2312.03458v1"}
{"created":"2023-12-06 12:27:25","title":"Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data","abstract":"Perceptual metrics are traditionally used to evaluate the quality of natural signals, such as images and audio. They are designed to mimic the perceptual behaviour of human observers and usually reflect structures found in natural signals. This motivates their use as loss functions for training generative models such that models will learn to capture the structure held in the metric. We take this idea to the extreme in the audio domain by training a compressive autoencoder to reconstruct uniform noise, in lieu of natural data. We show that training with perceptual losses improves the reconstruction of spectrograms and re-synthesized audio at test time over models trained with a standard Euclidean loss. This demonstrates better generalisation to unseen natural signals when using perceptual metrics.","sentences":["Perceptual metrics are traditionally used to evaluate the quality of natural signals, such as images and audio.","They are designed to mimic the perceptual behaviour of human observers and usually reflect structures found in natural signals.","This motivates their use as loss functions for training generative models such that models will learn to capture the structure held in the metric.","We take this idea to the extreme in the audio domain by training a compressive autoencoder to reconstruct uniform noise, in lieu of natural data.","We show that training with perceptual losses improves the reconstruction of spectrograms and re-synthesized audio at test time over models trained with a standard Euclidean loss.","This demonstrates better generalisation to unseen natural signals when using perceptual metrics."],"url":"http://arxiv.org/abs/2312.03455v1"}
{"created":"2023-12-06 11:54:50","title":"Data-driven Crop Growth Simulation on Time-varying Generated Images using Multi-conditional Generative Adversarial Networks","abstract":"Image-based crop growth modeling can substantially contribute to precision agriculture by revealing spatial crop development over time, which allows an early and location-specific estimation of relevant future plant traits, such as leaf area or biomass. A prerequisite for realistic and sharp crop image generation is the integration of multiple growth-influencing conditions in a model, such as an image of an initial growth stage, the associated growth time, and further information about the field treatment. We present a two-stage framework consisting first of an image prediction model and second of a growth estimation model, which both are independently trained. The image prediction model is a conditional Wasserstein generative adversarial network (CWGAN). In the generator of this model, conditional batch normalization (CBN) is used to integrate different conditions along with the input image. This allows the model to generate time-varying artificial images dependent on multiple influencing factors of different kinds. These images are used by the second part of the framework for plant phenotyping by deriving plant-specific traits and comparing them with those of non-artificial (real) reference images. For various crop datasets, the framework allows realistic, sharp image predictions with a slight loss of quality from short-term to long-term predictions. Simulations of varying growth-influencing conditions performed with the trained framework provide valuable insights into how such factors relate to crop appearances, which is particularly useful in complex, less explored crop mixture systems. Further results show that adding process-based simulated biomass as a condition increases the accuracy of the derived phenotypic traits from the predicted images. This demonstrates the potential of our framework to serve as an interface between an image- and process-based crop growth model.","sentences":["Image-based crop growth modeling can substantially contribute to precision agriculture by revealing spatial crop development over time, which allows an early and location-specific estimation of relevant future plant traits, such as leaf area or biomass.","A prerequisite for realistic and sharp crop image generation is the integration of multiple growth-influencing conditions in a model, such as an image of an initial growth stage, the associated growth time, and further information about the field treatment.","We present a two-stage framework consisting first of an image prediction model and second of a growth estimation model, which both are independently trained.","The image prediction model is a conditional Wasserstein generative adversarial network (CWGAN).","In the generator of this model, conditional batch normalization (CBN) is used to integrate different conditions along with the input image.","This allows the model to generate time-varying artificial images dependent on multiple influencing factors of different kinds.","These images are used by the second part of the framework for plant phenotyping by deriving plant-specific traits and comparing them with those of non-artificial (real) reference images.","For various crop datasets, the framework allows realistic, sharp image predictions with a slight loss of quality from short-term to long-term predictions.","Simulations of varying growth-influencing conditions performed with the trained framework provide valuable insights into how such factors relate to crop appearances, which is particularly useful in complex, less explored crop mixture systems.","Further results show that adding process-based simulated biomass as a condition increases the accuracy of the derived phenotypic traits from the predicted images.","This demonstrates the potential of our framework to serve as an interface between an image- and process-based crop growth model."],"url":"http://arxiv.org/abs/2312.03443v1"}
{"created":"2023-12-06 11:38:26","title":"Data-Centric Digital Agriculture: A Perspective","abstract":"In response to the increasing global demand for food, feed, fiber, and fuel, digital agriculture is rapidly evolving to meet these demands while reducing environmental impact. This evolution involves incorporating data science, machine learning, sensor technologies, robotics, and new management strategies to establish a more sustainable agricultural framework. So far, machine learning research in digital agriculture has predominantly focused on model-centric approaches, focusing on model design and evaluation. These efforts aim to optimize model accuracy and efficiency, often treating data as a static benchmark. Despite the availability of agricultural data and methodological advancements, a saturation point has been reached, with many established machine learning methods achieving comparable levels of accuracy and facing similar limitations. To fully realize the potential of digital agriculture, it is crucial to have a comprehensive understanding of the role of data in the field and to adopt data-centric machine learning. This involves developing strategies to acquire and curate valuable data and implementing effective learning and evaluation strategies that utilize the intrinsic value of data. This approach has the potential to create accurate, generalizable, and adaptable machine learning methods that effectively and sustainably address agricultural tasks such as yield prediction, weed detection, and early disease identification","sentences":["In response to the increasing global demand for food, feed, fiber, and fuel, digital agriculture is rapidly evolving to meet these demands while reducing environmental impact.","This evolution involves incorporating data science, machine learning, sensor technologies, robotics, and new management strategies to establish a more sustainable agricultural framework.","So far, machine learning research in digital agriculture has predominantly focused on model-centric approaches, focusing on model design and evaluation.","These efforts aim to optimize model accuracy and efficiency, often treating data as a static benchmark.","Despite the availability of agricultural data and methodological advancements, a saturation point has been reached, with many established machine learning methods achieving comparable levels of accuracy and facing similar limitations.","To fully realize the potential of digital agriculture, it is crucial to have a comprehensive understanding of the role of data in the field and to adopt data-centric machine learning.","This involves developing strategies to acquire and curate valuable data and implementing effective learning and evaluation strategies that utilize the intrinsic value of data.","This approach has the potential to create accurate, generalizable, and adaptable machine learning methods that effectively and sustainably address agricultural tasks such as yield prediction, weed detection, and early disease identification"],"url":"http://arxiv.org/abs/2312.03437v1"}
{"created":"2023-12-06 11:36:15","title":"Counting Butterflies in Fully Dynamic Bipartite Graph Streams","abstract":"A bipartite graph extensively models relationships between real-world entities of two different types, such as user-product data in e-commerce. Such graph data are inherently becoming more and more streaming, entailing continuous insertions and deletions of edges. A butterfly (i.e., 2x2 bi-clique) is the smallest non-trivial cohesive structure that plays a crucial role. Counting such butterfly patterns in streaming bipartite graphs is a core problem in applications such as dense subgraph discovery and anomaly detection. Yet, existing approximate solutions consider insert-only streams and, thus, achieve very low accuracy in fully dynamic bipartite graph streams that involve both insertions and deletions of edges. Adapting them to consider deletions is not trivial either, because different sampling schemes and new accuracy analyses are required. In this paper, we propose Abacus, a novel approximate algorithm that counts butterflies in the presence of both insertions and deletions by utilizing sampling. We prove that Abacus always delivers unbiased estimates of low variance. Furthermore, we extend Abacus and devise a parallel mini-batch variant, namely, Parabacus, which counts butterflies in parallel. Parabacus counts butterflies in a load-balanced manner using versioned samples, which results in significant speedup and is thus ideal for critical applications in the streaming environment. We evaluate Abacus/Parabacus using a diverse set of real bipartite graphs and assess its performance in terms of accuracy, throughput, and speedup. The results indicate that our proposal is the first capable of efficiently providing accurate butterfly counts in the most generic setting, i.e., a fully dynamic graph streaming environment that entails both insertions and deletions. It does so without sacrificing throughput and even improving it with the parallel version.","sentences":["A bipartite graph extensively models relationships between real-world entities of two different types, such as user-product data in e-commerce.","Such graph data are inherently becoming more and more streaming, entailing continuous insertions and deletions of edges.","A butterfly (i.e., 2x2 bi-clique) is the smallest non-trivial cohesive structure that plays a crucial role.","Counting such butterfly patterns in streaming bipartite graphs is a core problem in applications such as dense subgraph discovery and anomaly detection.","Yet, existing approximate solutions consider insert-only streams and, thus, achieve very low accuracy in fully dynamic bipartite graph streams that involve both insertions and deletions of edges.","Adapting them to consider deletions is not trivial either, because different sampling schemes and new accuracy analyses are required.","In this paper, we propose Abacus, a novel approximate algorithm that counts butterflies in the presence of both insertions and deletions by utilizing sampling.","We prove that Abacus always delivers unbiased estimates of low variance.","Furthermore, we extend Abacus and devise a parallel mini-batch variant, namely, Parabacus, which counts butterflies in parallel.","Parabacus counts butterflies in a load-balanced manner using versioned samples, which results in significant speedup and is thus ideal for critical applications in the streaming environment.","We evaluate Abacus/Parabacus using a diverse set of real bipartite graphs and assess its performance in terms of accuracy, throughput, and speedup.","The results indicate that our proposal is the first capable of efficiently providing accurate butterfly counts in the most generic setting, i.e., a fully dynamic graph streaming environment that entails both insertions and deletions.","It does so without sacrificing throughput and even improving it with the parallel version."],"url":"http://arxiv.org/abs/2312.03435v1"}
{"created":"2023-12-06 11:25:40","title":"ShareCMP: Polarization-Aware RGB-P Semantic Segmentation","abstract":"Multimodal semantic segmentation is developing rapidly, but the modality of RGB-Polarization remains underexplored. To delve into this problem, we construct a UPLight RGB-P segmentation benchmark with 12 typical underwater semantic classes which provides data support for Autonomous Underwater Vehicles (AUVs) to perform special perception tasks. In this work, we design the ShareCMP, an RGB-P semantic segmentation framework with a shared dual-branch architecture, which reduces the number of parameters by about 26-33% compared to previous dual-branch models. It encompasses a Polarization Generate Attention (PGA) module designed to generate polarization modal images with richer polarization properties for the encoder. In addition, we introduce the Class Polarization-Aware Loss (CPALoss) to improve the learning and understanding of the encoder for polarization modal information and to optimize the PGA module. With extensive experiments on a total of three RGB-P benchmarks, our ShareCMP achieves state-of-the-art performance in mIoU with fewer parameters on the UPLight (92.45%), ZJU (92.7%), and MCubeS (50.99%) datasets. The code is available at https://github.com/LEFTeyex/ShareCMP.","sentences":["Multimodal semantic segmentation is developing rapidly, but the modality of RGB-Polarization remains underexplored.","To delve into this problem, we construct a UPLight RGB-P segmentation benchmark with 12 typical underwater semantic classes which provides data support for Autonomous Underwater Vehicles (AUVs) to perform special perception tasks.","In this work, we design the ShareCMP, an RGB-P semantic segmentation framework with a shared dual-branch architecture, which reduces the number of parameters by about 26-33% compared to previous dual-branch models.","It encompasses a Polarization Generate Attention (PGA) module designed to generate polarization modal images with richer polarization properties for the encoder.","In addition, we introduce the Class Polarization-Aware Loss (CPALoss) to improve the learning and understanding of the encoder for polarization modal information and to optimize the PGA module.","With extensive experiments on a total of three RGB-P benchmarks, our ShareCMP achieves state-of-the-art performance in mIoU with fewer parameters on the UPLight (92.45%), ZJU (92.7%), and MCubeS (50.99%) datasets.","The code is available at https://github.com/LEFTeyex/ShareCMP."],"url":"http://arxiv.org/abs/2312.03430v1"}
{"created":"2023-12-06 11:17:14","title":"Internal and External Calculi: Ordering the Jungle without Being Lost in Translations","abstract":"This paper gives a broad account of the various sequent-based proof formalisms in the proof-theoretic literature. We consider formalisms for various modal and tense logics, intuitionistic logic, conditional logics, and bunched logics. After providing an overview of the logics and proof formalisms under consideration, we show how these sequent-based formalisms can be placed in a hierarchy in terms of the underlying data structure of the sequents. We then discuss how this hierarchy can be traversed using translations. Translating proofs up this hierarchy is found to be relatively easy while translating proofs down the hierarchy is substantially more difficult. Finally, we inspect the prevalent distinction in structural proof theory between 'internal calculi' and 'external calculi'. It is observed that these classes resist a rigorous separation, and we critically assess the properties that (calculi from) these classes are purported to possess.","sentences":["This paper gives a broad account of the various sequent-based proof formalisms in the proof-theoretic literature.","We consider formalisms for various modal and tense logics, intuitionistic logic, conditional logics, and bunched logics.","After providing an overview of the logics and proof formalisms under consideration, we show how these sequent-based formalisms can be placed in a hierarchy in terms of the underlying data structure of the sequents.","We then discuss how this hierarchy can be traversed using translations.","Translating proofs up this hierarchy is found to be relatively easy while translating proofs down the hierarchy is substantially more difficult.","Finally, we inspect the prevalent distinction in structural proof theory between 'internal calculi' and 'external calculi'.","It is observed that these classes resist a rigorous separation, and we critically assess the properties that (calculi from) these classes are purported to possess."],"url":"http://arxiv.org/abs/2312.03426v1"}
{"created":"2023-12-06 11:05:11","title":"Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models","abstract":"Backdoor attacks, representing an emerging threat to the integrity of deep neural networks, have garnered significant attention due to their ability to compromise deep learning systems clandestinely. While numerous backdoor attacks occur within the digital realm, their practical implementation in real-world prediction systems remains limited and vulnerable to disturbances in the physical world. Consequently, this limitation has given rise to the development of physical backdoor attacks, where trigger objects manifest as physical entities within the real world. However, creating the requisite dataset to train or evaluate a physical backdoor model is a daunting task, limiting the backdoor researchers and practitioners from studying such physical attack scenarios. This paper unleashes a recipe that empowers backdoor researchers to effortlessly create a malicious, physical backdoor dataset based on advances in generative modeling. Particularly, this recipe involves 3 automatic modules: suggesting the suitable physical triggers, generating the poisoned candidate samples (either by synthesizing new samples or editing existing clean samples), and finally refining for the most plausible ones. As such, it effectively mitigates the perceived complexity associated with creating a physical backdoor dataset, transforming it from a daunting task into an attainable objective. Extensive experiment results show that datasets created by our \"recipe\" enable adversaries to achieve an impressive attack success rate on real physical world data and exhibit similar properties compared to previous physical backdoor attack studies. This paper offers researchers a valuable toolkit for studies of physical backdoors, all within the confines of their laboratories.","sentences":["Backdoor attacks, representing an emerging threat to the integrity of deep neural networks, have garnered significant attention due to their ability to compromise deep learning systems clandestinely.","While numerous backdoor attacks occur within the digital realm, their practical implementation in real-world prediction systems remains limited and vulnerable to disturbances in the physical world.","Consequently, this limitation has given rise to the development of physical backdoor attacks, where trigger objects manifest as physical entities within the real world.","However, creating the requisite dataset to train or evaluate a physical backdoor model is a daunting task, limiting the backdoor researchers and practitioners from studying such physical attack scenarios.","This paper unleashes a recipe that empowers backdoor researchers to effortlessly create a malicious, physical backdoor dataset based on advances in generative modeling.","Particularly, this recipe involves 3 automatic modules: suggesting the suitable physical triggers, generating the poisoned candidate samples (either by synthesizing new samples or editing existing clean samples), and finally refining for the most plausible ones.","As such, it effectively mitigates the perceived complexity associated with creating a physical backdoor dataset, transforming it from a daunting task into an attainable objective.","Extensive experiment results show that datasets created by our \"recipe\" enable adversaries to achieve an impressive attack success rate on real physical world data and exhibit similar properties compared to previous physical backdoor attack studies.","This paper offers researchers a valuable toolkit for studies of physical backdoors, all within the confines of their laboratories."],"url":"http://arxiv.org/abs/2312.03419v1"}
{"created":"2023-12-06 10:46:53","title":"Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future","abstract":"With the continuous maturation and application of autonomous driving technology, a systematic examination of open-source autonomous driving datasets becomes instrumental in fostering the robust evolution of the industry ecosystem. Current autonomous driving datasets can broadly be categorized into two generations. The first-generation autonomous driving datasets are characterized by relatively simpler sensor modalities, smaller data scale, and is limited to perception-level tasks. KITTI, introduced in 2012, serves as a prominent representative of this initial wave. In contrast, the second-generation datasets exhibit heightened complexity in sensor modalities, greater data scale and diversity, and an expansion of tasks from perception to encompass prediction and control. Leading examples of the second generation include nuScenes and Waymo, introduced around 2019. This comprehensive review, conducted in collaboration with esteemed colleagues from both academia and industry, systematically assesses over seventy open-source autonomous driving datasets from domestic and international sources. It offers insights into various aspects, such as the principles underlying the creation of high-quality datasets, the pivotal role of data engine systems, and the utilization of generative foundation models to facilitate scalable data generation. Furthermore, this review undertakes an exhaustive analysis and discourse regarding the characteristics and data scales that future third-generation autonomous driving datasets should possess. It also delves into the scientific and technical challenges that warrant resolution. These endeavors are pivotal in advancing autonomous innovation and fostering technological enhancement in critical domains. For further details, please refer to https://github.com/OpenDriveLab/DriveAGI.","sentences":["With the continuous maturation and application of autonomous driving technology, a systematic examination of open-source autonomous driving datasets becomes instrumental in fostering the robust evolution of the industry ecosystem.","Current autonomous driving datasets can broadly be categorized into two generations.","The first-generation autonomous driving datasets are characterized by relatively simpler sensor modalities, smaller data scale, and is limited to perception-level tasks.","KITTI, introduced in 2012, serves as a prominent representative of this initial wave.","In contrast, the second-generation datasets exhibit heightened complexity in sensor modalities, greater data scale and diversity, and an expansion of tasks from perception to encompass prediction and control.","Leading examples of the second generation include nuScenes and Waymo, introduced around 2019.","This comprehensive review, conducted in collaboration with esteemed colleagues from both academia and industry, systematically assesses over seventy open-source autonomous driving datasets from domestic and international sources.","It offers insights into various aspects, such as the principles underlying the creation of high-quality datasets, the pivotal role of data engine systems, and the utilization of generative foundation models to facilitate scalable data generation.","Furthermore, this review undertakes an exhaustive analysis and discourse regarding the characteristics and data scales that future third-generation autonomous driving datasets should possess.","It also delves into the scientific and technical challenges that warrant resolution.","These endeavors are pivotal in advancing autonomous innovation and fostering technological enhancement in critical domains.","For further details, please refer to https://github.com/OpenDriveLab/DriveAGI."],"url":"http://arxiv.org/abs/2312.03408v1"}
{"created":"2023-12-06 10:43:08","title":"Extremal Fitting CQs do not Generalize","abstract":"A fitting algorithm for conjunctive queries (CQs) produces, given a set of positively and negatively labeled data examples, a CQ that fits these examples. In general, there may be many non-equivalent fitting CQs and thus the algorithm has some freedom in producing its output. Additional desirable properties of the produced CQ are that it generalizes well to unseen examples in the sense of PAC learning and that it is most general or most specific in the set of all fitting CQs. In this research note, we show that these desiderata are incompatible when we require PAC-style generalization from a polynomial sample: we prove that any fitting algorithm that produces a most-specific fitting CQ cannot be a sample-efficient PAC learning algorithm, and the same is true for fitting algorithms that produce a most-general fitting CQ (when it exists). Our proofs rely on a polynomial construction of relativized homomorphism dualities for path-shaped structures.","sentences":["A fitting algorithm for conjunctive queries (CQs) produces, given a set of positively and negatively labeled data examples, a CQ that fits these examples.","In general, there may be many non-equivalent fitting CQs and thus the algorithm has some freedom in producing its output.","Additional desirable properties of the produced CQ are that it generalizes well to unseen examples in the sense of PAC learning and that it is most general or most specific in the set of all fitting CQs.","In this research note, we show that these desiderata are incompatible when we require PAC-style generalization from a polynomial sample: we prove that any fitting algorithm that produces a most-specific fitting CQ cannot be a sample-efficient PAC learning algorithm, and the same is true for fitting algorithms that produce a most-general fitting CQ (when it exists).","Our proofs rely on a polynomial construction of relativized homomorphism dualities for path-shaped structures."],"url":"http://arxiv.org/abs/2312.03407v1"}
{"created":"2023-12-06 10:10:21","title":"Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning","abstract":"We present Generalized Contrastive Divergence (GCD), a novel objective function for training an energy-based model (EBM) and a sampler simultaneously. GCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm for training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution with a trainable sampler, such as a diffusion model. In GCD, the joint training of EBM and a diffusion model is formulated as a minimax problem, which reaches an equilibrium when both models converge to the data distribution. The minimax learning with GCD bears interesting equivalence to inverse reinforcement learning, where the energy corresponds to a negative reward, the diffusion model is a policy, and the real data is expert demonstrations. We present preliminary yet promising results showing that joint training is beneficial for both EBM and a diffusion model. GCD enables EBM training without MCMC while improving the sample quality of a diffusion model.","sentences":["We present Generalized Contrastive Divergence (GCD), a novel objective function for training an energy-based model (EBM) and a sampler simultaneously.","GCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm for training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution with a trainable sampler, such as a diffusion model.","In GCD, the joint training of EBM and a diffusion model is formulated as a minimax problem, which reaches an equilibrium when both models converge to the data distribution.","The minimax learning with GCD bears interesting equivalence to inverse reinforcement learning, where the energy corresponds to a negative reward, the diffusion model is a policy, and the real data is expert demonstrations.","We present preliminary yet promising results showing that joint training is beneficial for both EBM and a diffusion model.","GCD enables EBM training without MCMC while improving the sample quality of a diffusion model."],"url":"http://arxiv.org/abs/2312.03397v1"}
{"created":"2023-12-06 09:33:33","title":"Riemannian Complex Matrix Convolution Network for PolSAR Image Classification","abstract":"Recently, deep learning methods have achieved superior performance for Polarimetric Synthetic Aperture Radar(PolSAR) image classification. Existing deep learning methods learn PolSAR data by converting the covariance matrix into a feature vector or complex-valued vector as the input. However, all these methods cannot learn the structure of complex matrix directly and destroy the channel correlation. To learn geometric structure of complex matrix, we propose a Riemannian complex matrix convolution network for PolSAR image classification in Riemannian space for the first time, which directly utilizes the complex matrix as the network input and defines the Riemannian operations to learn complex matrix's features. The proposed Riemannian complex matrix convolution network considers PolSAR complex matrix endowed in Riemannian manifold, and defines a series of new Riemannian convolution, ReLu and LogEig operations in Riemannian space, which breaks through the Euclidean constraint of conventional networks. Then, a CNN module is appended to enhance contextual Riemannian features. Besides, a fast kernel learning method is developed for the proposed method to learn class-specific features and reduce the computation time effectively. Experiments are conducted on three sets of real PolSAR data with different bands and sensors. Experiments results demonstrates the proposed method can obtain superior performance than the state-of-the-art methods.","sentences":["Recently, deep learning methods have achieved superior performance for Polarimetric Synthetic Aperture Radar(PolSAR) image classification.","Existing deep learning methods learn PolSAR data by converting the covariance matrix into a feature vector or complex-valued vector as the input.","However, all these methods cannot learn the structure of complex matrix directly and destroy the channel correlation.","To learn geometric structure of complex matrix, we propose a Riemannian complex matrix convolution network for PolSAR image classification in Riemannian space for the first time, which directly utilizes the complex matrix as the network input and defines the Riemannian operations to learn complex matrix's features.","The proposed Riemannian complex matrix convolution network considers PolSAR complex matrix endowed in Riemannian manifold, and defines a series of new Riemannian convolution, ReLu and LogEig operations in Riemannian space, which breaks through the Euclidean constraint of conventional networks.","Then, a CNN module is appended to enhance contextual Riemannian features.","Besides, a fast kernel learning method is developed for the proposed method to learn class-specific features and reduce the computation time effectively.","Experiments are conducted on three sets of real PolSAR data with different bands and sensors.","Experiments results demonstrates the proposed method can obtain superior performance than the state-of-the-art methods."],"url":"http://arxiv.org/abs/2312.03378v1"}
{"created":"2023-12-06 09:15:52","title":"EnvGuard : Guaranteeing Environment-Centric Properties in Web of Things","abstract":"With the accelerated advancement of IoT, diverse devices are ubiquitously deployed in environments. Building on this, Web of Things (WoT) further integrates fragmented device services and provides unified interfaces using standardized Web technologies, promoting the development and deployment of WoT applications to sense and regulate the environment. However, disparate WoT applications independently control devices in the WoT environment, causing interference among devices and with the environment. This results in device behaviors that deviate from user expectations, causing violations of the user's desired environment properties. The intricate interplay of applications, user activities, and environment changes makes identifying and resolving potential violations a complex task. In this paper, we introduce EnvGuard, an environment-centric approach for property description, violation identification, and resolution in WoT environment. EnvGuard proposes a conceptual schema to model the relationship between device services and environment context, and automatically extends the conceptual schema into a specific environment representation based on device and space information. Furthermore, EnvGuard employs a template-based approach, enabling users to describe spatial and temporal properties based on the abstract device effects on the environment, and translating properties description into formal expressions. EnvGuard adopts a hybrid model checking method to respectively identify the spatial and temporal violations, and a resolution strategy that align with user intention is proposed to resolve violations. We evaluate EnvGuard through user studies and our proposed dataset, which is constructed by collecting real-world data from a laboratory WoT environment and manually labeling ten types of violations. The results confirm the usability, feasibility and efficiency of EnvGuard.","sentences":["With the accelerated advancement of IoT, diverse devices are ubiquitously deployed in environments.","Building on this, Web of Things (WoT) further integrates fragmented device services and provides unified interfaces using standardized Web technologies, promoting the development and deployment of WoT applications to sense and regulate the environment.","However, disparate WoT applications independently control devices in the WoT environment, causing interference among devices and with the environment.","This results in device behaviors that deviate from user expectations, causing violations of the user's desired environment properties.","The intricate interplay of applications, user activities, and environment changes makes identifying and resolving potential violations a complex task.","In this paper, we introduce EnvGuard, an environment-centric approach for property description, violation identification, and resolution in WoT environment.","EnvGuard proposes a conceptual schema to model the relationship between device services and environment context, and automatically extends the conceptual schema into a specific environment representation based on device and space information.","Furthermore, EnvGuard employs a template-based approach, enabling users to describe spatial and temporal properties based on the abstract device effects on the environment, and translating properties description into formal expressions.","EnvGuard adopts a hybrid model checking method to respectively identify the spatial and temporal violations, and a resolution strategy that align with user intention is proposed to resolve violations.","We evaluate EnvGuard through user studies and our proposed dataset, which is constructed by collecting real-world data from a laboratory WoT environment and manually labeling ten types of violations.","The results confirm the usability, feasibility and efficiency of EnvGuard."],"url":"http://arxiv.org/abs/2312.03373v1"}
{"created":"2023-12-06 09:01:21","title":"KhabarChin: Automatic Detection of Important News in the Persian Language","abstract":"Being aware of important news is crucial for staying informed and making well-informed decisions efficiently. Natural Language Processing (NLP) approaches can significantly automate this process. This paper introduces the detection of important news, in a previously unexplored area, and presents a new benchmarking dataset (Khabarchin) for detecting important news in the Persian language. We define important news articles as those deemed significant for a considerable portion of society, capable of influencing their mindset or decision-making. The news articles are obtained from seven different prominent Persian news agencies, resulting in the annotation of 7,869 samples and the creation of the dataset. Two challenges of high disagreement and imbalance between classes were faced, and solutions were provided for them. We also propose several learning-based models, ranging from conventional machine learning to state-of-the-art transformer models, to tackle this task. Furthermore, we introduce the second task of important sentence detection in news articles, as they often come with a significant contextual length that makes it challenging for readers to identify important information. We identify these sentences in a weakly supervised manner.","sentences":["Being aware of important news is crucial for staying informed and making well-informed decisions efficiently.","Natural Language Processing (NLP) approaches can significantly automate this process.","This paper introduces the detection of important news, in a previously unexplored area, and presents a new benchmarking dataset (Khabarchin) for detecting important news in the Persian language.","We define important news articles as those deemed significant for a considerable portion of society, capable of influencing their mindset or decision-making.","The news articles are obtained from seven different prominent Persian news agencies, resulting in the annotation of 7,869 samples and the creation of the dataset.","Two challenges of high disagreement and imbalance between classes were faced, and solutions were provided for them.","We also propose several learning-based models, ranging from conventional machine learning to state-of-the-art transformer models, to tackle this task.","Furthermore, we introduce the second task of important sentence detection in news articles, as they often come with a significant contextual length that makes it challenging for readers to identify important information.","We identify these sentences in a weakly supervised manner."],"url":"http://arxiv.org/abs/2312.03361v1"}
{"created":"2023-12-06 08:49:55","title":"PointMoment:Mixed-Moment-based Self-Supervised Representation Learning for 3D Point Clouds","abstract":"Large and rich data is a prerequisite for effective training of deep neural networks. However, the irregularity of point cloud data makes manual annotation time-consuming and laborious. Self-supervised representation learning, which leverages the intrinsic structure of large-scale unlabelled data to learn meaningful feature representations, has attracted increasing attention in the field of point cloud research. However, self-supervised representation learning often suffers from model collapse, resulting in reduced information and diversity of the learned representation, and consequently degrading the performance of downstream tasks. To address this problem, we propose PointMoment, a novel framework for point cloud self-supervised representation learning that utilizes a high-order mixed moment loss function rather than the conventional contrastive loss function. Moreover, our framework does not require any special techniques such as asymmetric network architectures, gradient stopping, etc. Specifically, we calculate the high-order mixed moment of the feature variables and force them to decompose into products of their individual moment, thereby making multiple variables more independent and minimizing the feature redundancy. We also incorporate a contrastive learning approach to maximize the feature invariance under different data augmentations of the same point cloud. Experimental results show that our approach outperforms previous unsupervised learning methods on the downstream task of 3D point cloud classification and segmentation.","sentences":["Large and rich data is a prerequisite for effective training of deep neural networks.","However, the irregularity of point cloud data makes manual annotation time-consuming and laborious.","Self-supervised representation learning, which leverages the intrinsic structure of large-scale unlabelled data to learn meaningful feature representations, has attracted increasing attention in the field of point cloud research.","However, self-supervised representation learning often suffers from model collapse, resulting in reduced information and diversity of the learned representation, and consequently degrading the performance of downstream tasks.","To address this problem, we propose PointMoment, a novel framework for point cloud self-supervised representation learning that utilizes a high-order mixed moment loss function rather than the conventional contrastive loss function.","Moreover, our framework does not require any special techniques such as asymmetric network architectures, gradient stopping, etc.","Specifically, we calculate the high-order mixed moment of the feature variables and force them to decompose into products of their individual moment, thereby making multiple variables more independent and minimizing the feature redundancy.","We also incorporate a contrastive learning approach to maximize the feature invariance under different data augmentations of the same point cloud.","Experimental results show that our approach outperforms previous unsupervised learning methods on the downstream task of 3D point cloud classification and segmentation."],"url":"http://arxiv.org/abs/2312.03350v1"}
{"created":"2023-12-06 08:44:48","title":"Identifying hubs in directed networks","abstract":"Nodes in networks that exhibit high connectivity, also called \"hubs\", play a critical role in determining the structural and functional properties of networked systems. However, there is no clear definition of what constitutes a hub node in a network, and the classification of network hubs in existing work has either been purely qualitative or relies on ad hoc criteria for thresholding continuous data that do not generalize well to networks with certain degree sequences. Here we develop a set of efficient nonparametric methods that classify hub nodes in directed networks using the Minimum Description Length principle, effectively providing a clear and principled definition for network hubs. We adapt our methods to both unweighted and weighted networks and demonstrate them in a range of example applications using real and synthetic network data.","sentences":["Nodes in networks that exhibit high connectivity, also called \"hubs\", play a critical role in determining the structural and functional properties of networked systems.","However, there is no clear definition of what constitutes a hub node in a network, and the classification of network hubs in existing work has either been purely qualitative or relies on ad hoc criteria for thresholding continuous data that do not generalize well to networks with certain degree sequences.","Here we develop a set of efficient nonparametric methods that classify hub nodes in directed networks using the Minimum Description Length principle, effectively providing a clear and principled definition for network hubs.","We adapt our methods to both unweighted and weighted networks and demonstrate them in a range of example applications using real and synthetic network data."],"url":"http://arxiv.org/abs/2312.03347v1"}
{"created":"2023-12-06 08:36:23","title":"Interpretable Mechanistic Representations for Meal-level Glycemic Control in the Wild","abstract":"Diabetes encompasses a complex landscape of glycemic control that varies widely among individuals. However, current methods do not faithfully capture this variability at the meal level. On the one hand, expert-crafted features lack the flexibility of data-driven methods; on the other hand, learned representations tend to be uninterpretable which hampers clinical adoption. In this paper, we propose a hybrid variational autoencoder to learn interpretable representations of CGM and meal data. Our method grounds the latent space to the inputs of a mechanistic differential equation, producing embeddings that reflect physiological quantities, such as insulin sensitivity, glucose effectiveness, and basal glucose levels. Moreover, we introduce a novel method to infer the glucose appearance rate, making the mechanistic model robust to unreliable meal logs. On a dataset of CGM and self-reported meals from individuals with type-2 diabetes and pre-diabetes, our unsupervised representation discovers a separation between individuals proportional to their disease severity. Our embeddings produce clusters that are up to 4x better than naive, expert, black-box, and pure mechanistic features. Our method provides a nuanced, yet interpretable, embedding space to compare glycemic control within and across individuals, directly learnable from in-the-wild data.","sentences":["Diabetes encompasses a complex landscape of glycemic control that varies widely among individuals.","However, current methods do not faithfully capture this variability at the meal level.","On the one hand, expert-crafted features lack the flexibility of data-driven methods; on the other hand, learned representations tend to be uninterpretable which hampers clinical adoption.","In this paper, we propose a hybrid variational autoencoder to learn interpretable representations of CGM and meal data.","Our method grounds the latent space to the inputs of a mechanistic differential equation, producing embeddings that reflect physiological quantities, such as insulin sensitivity, glucose effectiveness, and basal glucose levels.","Moreover, we introduce a novel method to infer the glucose appearance rate, making the mechanistic model robust to unreliable meal logs.","On a dataset of CGM and self-reported meals from individuals with type-2 diabetes and pre-diabetes, our unsupervised representation discovers a separation between individuals proportional to their disease severity.","Our embeddings produce clusters that are up to 4x better than naive, expert, black-box, and pure mechanistic features.","Our method provides a nuanced, yet interpretable, embedding space to compare glycemic control within and across individuals, directly learnable from in-the-wild data."],"url":"http://arxiv.org/abs/2312.03344v1"}
{"created":"2023-12-06 08:21:42","title":"PointJEM: Self-supervised Point Cloud Understanding for Reducing Feature Redundancy via Joint Entropy Maximization","abstract":"Most deep learning-based point cloud processing methods are supervised and require large scale of labeled data. However, manual labeling of point cloud data is laborious and time-consuming. Self-supervised representation learning can address the aforementioned issue by learning robust and generalized representations from unlabeled datasets. Nevertheless, the embedded features obtained by representation learning usually contain redundant information, and most current methods reduce feature redundancy by linear correlation constraints. In this paper, we propose PointJEM, a self-supervised representation learning method applied to the point cloud field. PointJEM comprises an embedding scheme and a loss function based on joint entropy. The embedding scheme divides the embedding vector into different parts, each part can learn a distinctive feature. To reduce redundant information in the features, PointJEM maximizes the joint entropy between the different parts, thereby rendering the learned feature variables pairwise independent. To validate the effectiveness of our method, we conducted experiments on multiple datasets. The results demonstrate that our method can significantly reduce feature redundancy beyond linear correlation. Furthermore, PointJEM achieves competitive performance in downstream tasks such as classification and segmentation.","sentences":["Most deep learning-based point cloud processing methods are supervised and require large scale of labeled data.","However, manual labeling of point cloud data is laborious and time-consuming.","Self-supervised representation learning can address the aforementioned issue by learning robust and generalized representations from unlabeled datasets.","Nevertheless, the embedded features obtained by representation learning usually contain redundant information, and most current methods reduce feature redundancy by linear correlation constraints.","In this paper, we propose PointJEM, a self-supervised representation learning method applied to the point cloud field.","PointJEM comprises an embedding scheme and a loss function based on joint entropy.","The embedding scheme divides the embedding vector into different parts, each part can learn a distinctive feature.","To reduce redundant information in the features, PointJEM maximizes the joint entropy between the different parts, thereby rendering the learned feature variables pairwise independent.","To validate the effectiveness of our method, we conducted experiments on multiple datasets.","The results demonstrate that our method can significantly reduce feature redundancy beyond linear correlation.","Furthermore, PointJEM achieves competitive performance in downstream tasks such as classification and segmentation."],"url":"http://arxiv.org/abs/2312.03339v1"}
{"created":"2023-12-06 07:38:46","title":"Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities","abstract":"Generic `toxicity' classifiers continue to be used for evaluating the potential for harm in natural language generation, despite mounting evidence of their shortcomings. We consider the challenge of measuring misogyny in natural language generation, and argue that generic `toxicity' classifiers are inadequate for this task. We use data from two well-characterised `Incel' communities on Reddit that differ primarily in their degrees of misogyny to construct a pair of training corpora which we use to fine-tune two language models. We show that an open source `toxicity' classifier is unable to distinguish meaningfully between generations from these models. We contrast this with a misogyny-specific lexicon recently proposed by feminist subject-matter experts, demonstrating that, despite the limitations of simple lexicon-based approaches, this shows promise as a benchmark to evaluate language models for misogyny, and that it is sensitive enough to reveal the known differences in these Reddit communities. Our preliminary findings highlight the limitations of a generic approach to evaluating harms, and further emphasise the need for careful benchmark design and selection in natural language evaluation.","sentences":["Generic `toxicity' classifiers continue to be used for evaluating the potential for harm in natural language generation, despite mounting evidence of their shortcomings.","We consider the challenge of measuring misogyny in natural language generation, and argue that generic `toxicity' classifiers are inadequate for this task.","We use data from two well-characterised `Incel' communities on Reddit that differ primarily in their degrees of misogyny to construct a pair of training corpora which we use to fine-tune two language models.","We show that an open source `toxicity' classifier is unable to distinguish meaningfully between generations from these models.","We contrast this with a misogyny-specific lexicon recently proposed by feminist subject-matter experts, demonstrating that, despite the limitations of simple lexicon-based approaches, this shows promise as a benchmark to evaluate language models for misogyny, and that it is sensitive enough to reveal the known differences in these Reddit communities.","Our preliminary findings highlight the limitations of a generic approach to evaluating harms, and further emphasise the need for careful benchmark design and selection in natural language evaluation."],"url":"http://arxiv.org/abs/2312.03330v1"}
{"created":"2023-12-06 07:26:02","title":"GCFA:Geodesic Curve Feature Augmentation via Shape Space Theory","abstract":"Deep learning has yielded remarkable outcomes in various domains. However, the challenge of requiring large-scale labeled samples still persists in deep learning. Thus, data augmentation has been introduced as a critical strategy to train deep learning models. However, data augmentation suffers from information loss and poor performance in small sample environments. To overcome these drawbacks, we propose a feature augmentation method based on shape space theory, i.e., Geodesic curve feature augmentation, called GCFA in brevity. First, we extract features from the image with the neural network model. Then, the multiple image features are projected into a pre-shape space as features. In the pre-shape space, a Geodesic curve is built to fit the features. Finally, the many generated features on the Geodesic curve are used to train the various machine learning models. The GCFA module can be seamlessly integrated with most machine learning methods. And the proposed method is simple, effective and insensitive for the small sample datasets. Several examples demonstrate that the GCFA method can greatly improve the performance of the data preprocessing model in a small sample environment.","sentences":["Deep learning has yielded remarkable outcomes in various domains.","However, the challenge of requiring large-scale labeled samples still persists in deep learning.","Thus, data augmentation has been introduced as a critical strategy to train deep learning models.","However, data augmentation suffers from information loss and poor performance in small sample environments.","To overcome these drawbacks, we propose a feature augmentation method based on shape space theory, i.e., Geodesic curve feature augmentation, called GCFA in brevity.","First, we extract features from the image with the neural network model.","Then, the multiple image features are projected into a pre-shape space as features.","In the pre-shape space, a Geodesic curve is built to fit the features.","Finally, the many generated features on the Geodesic curve are used to train the various machine learning models.","The GCFA module can be seamlessly integrated with most machine learning methods.","And the proposed method is simple, effective and insensitive for the small sample datasets.","Several examples demonstrate that the GCFA method can greatly improve the performance of the data preprocessing model in a small sample environment."],"url":"http://arxiv.org/abs/2312.03325v1"}
{"created":"2023-12-06 07:02:22","title":"Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift","abstract":"Self-training and contrastive learning have emerged as leading techniques for incorporating unlabeled data, both under distribution shift (unsupervised domain adaptation) and when it is absent (semi-supervised learning). However, despite the popularity and compatibility of these techniques, their efficacy in combination remains unexplored. In this paper, we undertake a systematic empirical investigation of this combination, finding that (i) in domain adaptation settings, self-training and contrastive learning offer significant complementary gains; and (ii) in semi-supervised learning settings, surprisingly, the benefits are not synergistic. Across eight distribution shift datasets (e.g., BREEDs, WILDS), we demonstrate that the combined method obtains 3--8% higher accuracy than either approach independently. We then theoretically analyze these techniques in a simplified model of distribution shift, demonstrating scenarios under which the features produced by contrastive learning can yield a good initialization for self-training to further amplify gains and achieve optimal performance, even when either method alone would fail.","sentences":["Self-training and contrastive learning have emerged as leading techniques for incorporating unlabeled data, both under distribution shift (unsupervised domain adaptation) and when it is absent (semi-supervised learning).","However, despite the popularity and compatibility of these techniques, their efficacy in combination remains unexplored.","In this paper, we undertake a systematic empirical investigation of this combination, finding that (i) in domain adaptation settings, self-training and contrastive learning offer significant complementary gains; and (ii) in semi-supervised learning settings, surprisingly, the benefits are not synergistic.","Across eight distribution shift datasets (e.g., BREEDs, WILDS), we demonstrate that the combined method obtains 3--8% higher accuracy than either approach independently.","We then theoretically analyze these techniques in a simplified model of distribution shift, demonstrating scenarios under which the features produced by contrastive learning can yield a good initialization for self-training to further amplify gains and achieve optimal performance, even when either method alone would fail."],"url":"http://arxiv.org/abs/2312.03318v1"}
{"created":"2023-12-06 05:39:00","title":"DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction","abstract":"Point cloud streaming is increasingly getting popular, evolving into the norm for interactive service delivery and the future Metaverse. However, the substantial volume of data associated with point clouds presents numerous challenges, particularly in terms of high bandwidth consumption and large storage capacity. Despite various solutions proposed thus far, with a focus on point cloud compression, upsampling, and completion, these reconstruction-related methods continue to fall short in delivering high fidelity point cloud output. As a solution, in DiffPMAE, we propose an effective point cloud reconstruction architecture. Inspired by self-supervised learning concepts, we combine Masked Auto-Encoding and Diffusion Model mechanism to remotely reconstruct point cloud data. By the nature of this reconstruction process, DiffPMAE can be extended to many related downstream tasks including point cloud compression, upsampling and completion. Leveraging ShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the performance of DiffPMAE exceeding many state-of-the-art methods in-terms of auto-encoding and downstream tasks considered.","sentences":["Point cloud streaming is increasingly getting popular, evolving into the norm for interactive service delivery and the future Metaverse.","However, the substantial volume of data associated with point clouds presents numerous challenges, particularly in terms of high bandwidth consumption and large storage capacity.","Despite various solutions proposed thus far, with a focus on point cloud compression, upsampling, and completion, these reconstruction-related methods continue to fall short in delivering high fidelity point cloud output.","As a solution, in DiffPMAE, we propose an effective point cloud reconstruction architecture.","Inspired by self-supervised learning concepts, we combine Masked Auto-Encoding and Diffusion Model mechanism to remotely reconstruct point cloud data.","By the nature of this reconstruction process, DiffPMAE can be extended to many related downstream tasks including point cloud compression, upsampling and completion.","Leveraging ShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the performance of DiffPMAE exceeding many state-of-the-art methods in-terms of auto-encoding and downstream tasks considered."],"url":"http://arxiv.org/abs/2312.03298v1"}
{"created":"2023-12-06 05:04:37","title":"Securing Data Platforms: Strategic Masking Techniques for Privacy and Security for B2B Enterprise Data","abstract":"In today's digital age, the imperative to protect data privacy and security is a paramount concern, especially for business-to-business (B2B) enterprises that handle sensitive information. These enterprises are increasingly constructing data platforms, which are integrated suites of technology solutions architected for the efficient management, processing, storage, and data analysis. It has become critical to design these data platforms with mechanisms that inherently support data privacy and security, particularly as they encounter the added complexity of safeguarding unstructured data types such as log files and text documents. Within this context, data masking stands out as a vital feature of data platform architecture. It proactively conceals sensitive elements, ensuring data privacy while preserving the information's value for business operations and analytics. This protective measure entails a strategic two-fold process: firstly, accurately pinpointing the sensitive data that necessitates concealment, and secondly, applying sophisticated methods to disguise that data effectively within the data platform infrastructure. This research delves into the nuances of embedding advanced data masking techniques within the very fabric of data platforms and an in-depth exploration of how enterprises can adopt a comprehensive approach toward effective data masking implementation by exploring different identification and anonymization techniques.","sentences":["In today's digital age, the imperative to protect data privacy and security is a paramount concern, especially for business-to-business (B2B) enterprises that handle sensitive information.","These enterprises are increasingly constructing data platforms, which are integrated suites of technology solutions architected for the efficient management, processing, storage, and data analysis.","It has become critical to design these data platforms with mechanisms that inherently support data privacy and security, particularly as they encounter the added complexity of safeguarding unstructured data types such as log files and text documents.","Within this context, data masking stands out as a vital feature of data platform architecture.","It proactively conceals sensitive elements, ensuring data privacy while preserving the information's value for business operations and analytics.","This protective measure entails a strategic two-fold process: firstly, accurately pinpointing the sensitive data that necessitates concealment, and secondly, applying sophisticated methods to disguise that data effectively within the data platform infrastructure.","This research delves into the nuances of embedding advanced data masking techniques within the very fabric of data platforms and an in-depth exploration of how enterprises can adopt a comprehensive approach toward effective data masking implementation by exploring different identification and anonymization techniques."],"url":"http://arxiv.org/abs/2312.03293v1"}
{"created":"2023-12-06 05:02:10","title":"Enhancing Molecular Property Prediction via Mixture of Collaborative Experts","abstract":"Molecular Property Prediction (MPP) task involves predicting biochemical properties based on molecular features, such as molecular graph structures, contributing to the discovery of lead compounds in drug development. To address data scarcity and imbalance in MPP, some studies have adopted Graph Neural Networks (GNN) as an encoder to extract commonalities from molecular graphs. However, these approaches often use a separate predictor for each task, neglecting the shared characteristics among predictors corresponding to different tasks. In response to this limitation, we introduce the GNN-MoCE architecture. It employs the Mixture of Collaborative Experts (MoCE) as predictors, exploiting task commonalities while confronting the homogeneity issue in the expert pool and the decision dominance dilemma within the expert group. To enhance expert diversity for collaboration among all experts, the Expert-Specific Projection method is proposed to assign a unique projection perspective to each expert. To balance decision-making influence for collaboration within the expert group, the Expert-Specific Loss is presented to integrate individual expert loss into the weighted decision loss of the group for more equitable training. Benefiting from the enhancements of MoCE in expert creation, dynamic expert group formation, and experts' collaboration, our model demonstrates superior performance over traditional methods on 24 MPP datasets, especially in tasks with limited data or high imbalance.","sentences":["Molecular Property Prediction (MPP) task involves predicting biochemical properties based on molecular features, such as molecular graph structures, contributing to the discovery of lead compounds in drug development.","To address data scarcity and imbalance in MPP, some studies have adopted Graph Neural Networks (GNN) as an encoder to extract commonalities from molecular graphs.","However, these approaches often use a separate predictor for each task, neglecting the shared characteristics among predictors corresponding to different tasks.","In response to this limitation, we introduce the GNN-MoCE architecture.","It employs the Mixture of Collaborative Experts (MoCE) as predictors, exploiting task commonalities while confronting the homogeneity issue in the expert pool and the decision dominance dilemma within the expert group.","To enhance expert diversity for collaboration among all experts, the Expert-Specific Projection method is proposed to assign a unique projection perspective to each expert.","To balance decision-making influence for collaboration within the expert group, the Expert-Specific Loss is presented to integrate individual expert loss into the weighted decision loss of the group for more equitable training.","Benefiting from the enhancements of MoCE in expert creation, dynamic expert group formation, and experts' collaboration, our model demonstrates superior performance over traditional methods on 24 MPP datasets, especially in tasks with limited data or high imbalance."],"url":"http://arxiv.org/abs/2312.03292v1"}
{"created":"2023-12-06 04:53:12","title":"OMNIINPUT: A Model-centric Evaluation Framework through Output Distribution","abstract":"We propose a novel model-centric evaluation framework, OmniInput, to evaluate the quality of an AI/ML model's predictions on all possible inputs (including human-unrecognizable ones), which is crucial for AI safety and reliability. Unlike traditional data-centric evaluation based on pre-defined test sets, the test set in OmniInput is self-constructed by the model itself and the model quality is evaluated by investigating its output distribution. We employ an efficient sampler to obtain representative inputs and the output distribution of the trained model, which, after selective annotation, can be used to estimate the model's precision and recall at different output values and a comprehensive precision-recall curve. Our experiments demonstrate that OmniInput enables a more fine-grained comparison between models, especially when their performance is almost the same on pre-defined datasets, leading to new findings and insights for how to train more robust, generalizable models.","sentences":["We propose a novel model-centric evaluation framework, OmniInput, to evaluate the quality of an AI/ML model's predictions on all possible inputs (including human-unrecognizable ones), which is crucial for AI safety and reliability.","Unlike traditional data-centric evaluation based on pre-defined test sets, the test set in OmniInput is self-constructed by the model itself and the model quality is evaluated by investigating its output distribution.","We employ an efficient sampler to obtain representative inputs and the output distribution of the trained model, which, after selective annotation, can be used to estimate the model's precision and recall at different output values and a comprehensive precision-recall curve.","Our experiments demonstrate that OmniInput enables a more fine-grained comparison between models, especially when their performance is almost the same on pre-defined datasets, leading to new findings and insights for how to train more robust, generalizable models."],"url":"http://arxiv.org/abs/2312.03291v1"}
{"created":"2023-12-06 04:38:02","title":"Class Incremental Learning for Adversarial Robustness","abstract":"Adversarial training integrates adversarial examples during model training to enhance robustness. However, its application in fixed dataset settings differs from real-world dynamics, where data accumulates incrementally. In this study, we investigate Adversarially Robust Class Incremental Learning (ARCIL), a method that combines adversarial robustness with incremental learning. We observe that combining incremental learning with naive adversarial training easily leads to a loss of robustness. We discover that this is attributed to the disappearance of the flatness of the loss function, a characteristic of adversarial training. To address this issue, we propose the Flatness Preserving Distillation (FPD) loss that leverages the output difference between adversarial and clean examples. Additionally, we introduce the Logit Adjustment Distillation (LAD) loss, which adapts the model's knowledge to perform well on new tasks. Experimental results demonstrate the superiority of our method over approaches that apply adversarial training to existing incremental learning methods, which provides a strong baseline for incremental learning on adversarial robustness in the future. Our method achieves AutoAttack accuracy that is 5.99\\%p, 5.27\\%p, and 3.90\\%p higher on average than the baseline on split CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively. The code will be made available.","sentences":["Adversarial training integrates adversarial examples during model training to enhance robustness.","However, its application in fixed dataset settings differs from real-world dynamics, where data accumulates incrementally.","In this study, we investigate Adversarially Robust Class Incremental Learning (ARCIL), a method that combines adversarial robustness with incremental learning.","We observe that combining incremental learning with naive adversarial training easily leads to a loss of robustness.","We discover that this is attributed to the disappearance of the flatness of the loss function, a characteristic of adversarial training.","To address this issue, we propose the Flatness Preserving Distillation (FPD) loss that leverages the output difference between adversarial and clean examples.","Additionally, we introduce the Logit Adjustment Distillation (LAD) loss, which adapts the model's knowledge to perform well on new tasks.","Experimental results demonstrate the superiority of our method over approaches that apply adversarial training to existing incremental learning methods, which provides a strong baseline for incremental learning on adversarial robustness in the future.","Our method achieves AutoAttack accuracy that is 5.99\\%p, 5.27\\%p, and 3.90\\%p higher on average than the baseline on split CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively.","The code will be made available."],"url":"http://arxiv.org/abs/2312.03289v1"}
{"created":"2023-12-06 04:32:38","title":"Indirect Gradient Matching for Adversarial Robust Distillation","abstract":"Adversarial training significantly improves adversarial robustness, but superior performance is primarily attained with large models. This substantial performance gap for smaller models has spurred active research into adversarial distillation (AD) to mitigate the difference. Existing AD methods leverage the teacher's logits as a guide. In contrast to these approaches, we aim to transfer another piece of knowledge from the teacher, the input gradient. In this paper, we propose a distillation module termed Indirect Gradient Distillation Module (IGDM) that indirectly matches the student's input gradient with that of the teacher. We hypothesize that students can better acquire the teacher's knowledge by matching the input gradient. Leveraging the observation that adversarial training renders the model locally linear on the input space, we employ Taylor approximation to effectively align gradients without directly calculating them. Experimental results show that IGDM seamlessly integrates with existing AD methods, significantly enhancing the performance of all AD methods. Particularly, utilizing IGDM on the CIFAR-100 dataset improves the AutoAttack accuracy from 28.06% to 30.32% with the ResNet-18 model and from 26.18% to 29.52% with the MobileNetV2 model when integrated into the SOTA method without additional data augmentation. The code will be made available.","sentences":["Adversarial training significantly improves adversarial robustness, but superior performance is primarily attained with large models.","This substantial performance gap for smaller models has spurred active research into adversarial distillation (AD) to mitigate the difference.","Existing AD methods leverage the teacher's logits as a guide.","In contrast to these approaches, we aim to transfer another piece of knowledge from the teacher, the input gradient.","In this paper, we propose a distillation module termed Indirect Gradient Distillation Module (IGDM) that indirectly matches the student's input gradient with that of the teacher.","We hypothesize that students can better acquire the teacher's knowledge by matching the input gradient.","Leveraging the observation that adversarial training renders the model locally linear on the input space, we employ Taylor approximation to effectively align gradients without directly calculating them.","Experimental results show that IGDM seamlessly integrates with existing AD methods, significantly enhancing the performance of all AD methods.","Particularly, utilizing IGDM on the CIFAR-100 dataset improves the AutoAttack accuracy from 28.06% to 30.32% with the ResNet-18 model and from 26.18% to 29.52% with the MobileNetV2 model when integrated into the SOTA method without additional data augmentation.","The code will be made available."],"url":"http://arxiv.org/abs/2312.03286v1"}
{"created":"2023-12-06 04:08:21","title":"Almanac: An API for Recommending Text Annotations For Time-Series Charts Using News Headlines","abstract":"Authors often add text annotations to charts to provide additional context for visually prominent features such as peaks, valleys, and trends. However, writing annotations that provide contextual information, such as descriptions of temporal events, often requires considerable manual effort. To address this problem, we introduce Almanac, a JavaScript API that recommends annotations sourced from the New York Times Archive of news headlines. Almanac consists of two independent parts: (1) a prominence feature detector and (2) a contextual annotation recommender. We demonstrate the utility of the API using D3.js and Vega-Lite to annotate a variety of time-series charts covering many different data domains. Preliminary user feedback shows that Almanac is useful to support the authoring of charts with more descriptive annotations.","sentences":["Authors often add text annotations to charts to provide additional context for visually prominent features such as peaks, valleys, and trends.","However, writing annotations that provide contextual information, such as descriptions of temporal events, often requires considerable manual effort.","To address this problem, we introduce Almanac, a JavaScript API that recommends annotations sourced from the New York Times Archive of news headlines.","Almanac consists of two independent parts: (1) a prominence feature detector and (2) a contextual annotation recommender.","We demonstrate the utility of the API using D3.js and Vega-Lite to annotate a variety of time-series charts covering many different data domains.","Preliminary user feedback shows that Almanac is useful to support the authoring of charts with more descriptive annotations."],"url":"http://arxiv.org/abs/2312.03278v1"}
{"created":"2023-12-06 03:49:13","title":"Geometric Deep Learning Towards the Iterative Classification of Graph-Based Aircraft Thermal Management Systems","abstract":"In this paper, we use graph-based techniques to investigate the use of geometric deep learning (GDL) in the classification and down-selection of aircraft thermal management systems (TMS). Previous work developed an enumerative graph generation procedure using a component catalog with network structure constraints to represent novel aircraft TMSs as graphs. However, as with many enumerative approaches, combinatorial explosion limits its efficacy in many real-world problems, particularly when simulations and optimization must be performed on the many (automatically-generated) physics models. Therefore, we present an approach that takes the directed graphs representing aircraft TMSs and use GDL to predict the critical characteristics of the remaining graphs. This paper's findings demonstrate that incorporating additional graph-based features enhances performance, achieving an accuracy of 97% for determining a graph's compilability and simulatability while using only 5% of the data for training. By applying iterative classification methods, we also successfully segmented the total set of graphs into more specific groups with an average inclusion of 84.7 of the top 100 highest-performing graphs, achieved by training on 45% of the data.","sentences":["In this paper, we use graph-based techniques to investigate the use of geometric deep learning (GDL) in the classification and down-selection of aircraft thermal management systems (TMS).","Previous work developed an enumerative graph generation procedure using a component catalog with network structure constraints to represent novel aircraft TMSs as graphs.","However, as with many enumerative approaches, combinatorial explosion limits its efficacy in many real-world problems, particularly when simulations and optimization must be performed on the many (automatically-generated) physics models.","Therefore, we present an approach that takes the directed graphs representing aircraft TMSs and use GDL to predict the critical characteristics of the remaining graphs.","This paper's findings demonstrate that incorporating additional graph-based features enhances performance, achieving an accuracy of 97% for determining a graph's compilability and simulatability while using only 5% of the data for training.","By applying iterative classification methods, we also successfully segmented the total set of graphs into more specific groups with an average inclusion of 84.7 of the top 100 highest-performing graphs, achieved by training on 45% of the data."],"url":"http://arxiv.org/abs/2312.03270v1"}
{"created":"2023-12-06 03:31:13","title":"SO-NeRF: Active View Planning for NeRF using Surrogate Objectives","abstract":"Despite the great success of Neural Radiance Fields (NeRF), its data-gathering process remains vague with only a general rule of thumb of sampling as densely as possible. The lack of understanding of what actually constitutes good views for NeRF makes it difficult to actively plan a sequence of views that yield the maximal reconstruction quality. We propose Surrogate Objectives for Active Radiance Fields (SOAR), which is a set of interpretable functions that evaluates the goodness of views using geometric and photometric visual cues - surface coverage, geometric complexity, textural complexity, and ray diversity. Moreover, by learning to infer the SOAR scores from a deep network, SOARNet, we are able to effectively select views in mere seconds instead of hours, without the need for prior visits to all the candidate views or training any radiance field during such planning. Our experiments show SOARNet outperforms the baselines with $\\sim$80x speed-up while achieving better or comparable reconstruction qualities. We finally show that SOAR is model-agnostic, thus it generalizes across fully neural-implicit to fully explicit approaches.","sentences":["Despite the great success of Neural Radiance Fields (NeRF), its data-gathering process remains vague with only a general rule of thumb of sampling as densely as possible.","The lack of understanding of what actually constitutes good views for NeRF makes it difficult to actively plan a sequence of views that yield the maximal reconstruction quality.","We propose Surrogate Objectives for Active Radiance Fields (SOAR), which is a set of interpretable functions that evaluates the goodness of views using geometric and photometric visual cues - surface coverage, geometric complexity, textural complexity, and ray diversity.","Moreover, by learning to infer the SOAR scores from a deep network, SOARNet, we are able to effectively select views in mere seconds instead of hours, without the need for prior visits to all the candidate views or training any radiance field during such planning.","Our experiments show SOARNet outperforms the baselines with $\\sim$80x speed-up while achieving better or comparable reconstruction qualities.","We finally show that SOAR is model-agnostic, thus it generalizes across fully neural-implicit to fully explicit approaches."],"url":"http://arxiv.org/abs/2312.03266v1"}
{"created":"2023-12-06 03:14:16","title":"f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization","abstract":"Training and deploying machine learning models that meet fairness criteria for protected groups are fundamental in modern artificial intelligence. While numerous constraints and regularization terms have been proposed in the literature to promote fairness in machine learning tasks, most of these methods are not amenable to stochastic optimization due to the complex and nonlinear structure of constraints and regularizers. Here, the term \"stochastic\" refers to the ability of the algorithm to work with small mini-batches of data. Motivated by the limitation of existing literature, this paper presents a unified stochastic optimization framework for fair empirical risk minimization based on f-divergence measures (f-FERM). The proposed stochastic algorithm enjoys theoretical convergence guarantees. In addition, our experiments demonstrate the superiority of fairness-accuracy tradeoffs offered by f-FERM for almost all batch sizes (ranging from full-batch to batch size of one). Moreover, we show that our framework can be extended to the case where there is a distribution shift from training to the test data. Our extension is based on a distributionally robust optimization reformulation of f-FERM objective under $L_p$ norms as uncertainty sets. Again, in this distributionally robust setting, f-FERM not only enjoys theoretical convergence guarantees but also outperforms other baselines in the literature in the tasks involving distribution shifts. An efficient stochastic implementation of $f$-FERM is publicly available.","sentences":["Training and deploying machine learning models that meet fairness criteria for protected groups are fundamental in modern artificial intelligence.","While numerous constraints and regularization terms have been proposed in the literature to promote fairness in machine learning tasks, most of these methods are not amenable to stochastic optimization due to the complex and nonlinear structure of constraints and regularizers.","Here, the term \"stochastic\" refers to the ability of the algorithm to work with small mini-batches of data.","Motivated by the limitation of existing literature, this paper presents a unified stochastic optimization framework for fair empirical risk minimization based on f-divergence measures (f-FERM).","The proposed stochastic algorithm enjoys theoretical convergence guarantees.","In addition, our experiments demonstrate the superiority of fairness-accuracy tradeoffs offered by f-FERM for almost all batch sizes (ranging from full-batch to batch size of one).","Moreover, we show that our framework can be extended to the case where there is a distribution shift from training to the test data.","Our extension is based on a distributionally robust optimization reformulation of f-FERM objective under $L_p$ norms as uncertainty sets.","Again, in this distributionally robust setting, f-FERM not only enjoys theoretical convergence guarantees but also outperforms other baselines in the literature in the tasks involving distribution shifts.","An efficient stochastic implementation of $f$-FERM is publicly available."],"url":"http://arxiv.org/abs/2312.03259v1"}
{"created":"2023-12-06 03:09:19","title":"CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models","abstract":"Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive, and Fast Embedding compression framework that addresses the above requirements. The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones. In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time. For each reported hot feature, we assign it a unique embedding. For the non-hot features, we allow multiple features to share one embedding by using hash embedding technique. Guided by our design philosophy, we further propose a multi-level hash embedding framework to optimize the embedding tables of non-hot features. We theoretically analyze the accuracy of HotSketch, and analyze the model convergence against deviation. Extensive experiments show that CAFE significantly outperforms existing embedding compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The source codes of CAFE are available at GitHub.","sentences":["Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment.","Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution.","This paper presents CAFE, a Compact, Adaptive, and Fast Embedding compression framework that addresses the above requirements.","The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones.","In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time.","For each reported hot feature, we assign it a unique embedding.","For the non-hot features, we allow multiple features to share one embedding by using hash embedding technique.","Guided by our design philosophy, we further propose a multi-level hash embedding framework to optimize the embedding tables of non-hot features.","We theoretically analyze the accuracy of HotSketch, and analyze the model convergence against deviation.","Extensive experiments show that CAFE significantly outperforms existing embedding compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x.","The source codes of CAFE are available at GitHub."],"url":"http://arxiv.org/abs/2312.03256v1"}
{"created":"2023-12-06 02:58:49","title":"Seller-side Outcome Fairness in Online Marketplaces","abstract":"This paper aims to investigate and achieve seller-side fairness within online marketplaces, where many sellers and their items are not sufficiently exposed to customers in an e-commerce platform. This phenomenon raises concerns regarding the potential loss of revenue associated with less exposed items as well as less marketplace diversity. We introduce the notion of seller-side outcome fairness and build an optimization model to balance collected recommendation rewards and the fairness metric. We then propose a gradient-based data-driven algorithm based on the duality and bandit theory. Our numerical experiments on real e-commerce data sets show that our algorithm can lift seller fairness measures while not hurting metrics like collected Gross Merchandise Value (GMV) and total purchases.","sentences":["This paper aims to investigate and achieve seller-side fairness within online marketplaces, where many sellers and their items are not sufficiently exposed to customers in an e-commerce platform.","This phenomenon raises concerns regarding the potential loss of revenue associated with less exposed items as well as less marketplace diversity.","We introduce the notion of seller-side outcome fairness and build an optimization model to balance collected recommendation rewards and the fairness metric.","We then propose a gradient-based data-driven algorithm based on the duality and bandit theory.","Our numerical experiments on real e-commerce data sets show that our algorithm can lift seller fairness measures while not hurting metrics like collected Gross Merchandise Value (GMV) and total purchases."],"url":"http://arxiv.org/abs/2312.03253v1"}
{"created":"2023-12-06 02:57:56","title":"Privacy-Preserving Task-Oriented Semantic Communications Against Model Inversion Attacks","abstract":"Semantic communication has been identified as a core technology for the sixth generation (6G) of wireless networks. Recently, task-oriented semantic communications have been proposed for low-latency inference with limited bandwidth. Although transmitting only task-related information does protect a certain level of user privacy, adversaries could apply model inversion techniques to reconstruct the raw data or extract useful information, thereby infringing on users' privacy. To mitigate privacy infringement, this paper proposes an information bottleneck and adversarial learning (IBAL) approach to protect users' privacy against model inversion attacks. Specifically, we extract task-relevant features from the input based on the information bottleneck (IB) theory. To overcome the difficulty in calculating the mutual information in high-dimensional space, we derive a variational upper bound to estimate the true mutual information. To prevent data reconstruction from task-related features by adversaries, we leverage adversarial learning to train encoder to fool adversaries by maximizing reconstruction distortion. Furthermore, considering the impact of channel variations on privacy-utility trade-off and the difficulty in manually tuning the weights of each loss, we propose an adaptive weight adjustment method. Numerical results demonstrate that the proposed approaches can effectively protect privacy without significantly affecting task performance and achieve better privacy-utility trade-offs than baseline methods.","sentences":["Semantic communication has been identified as a core technology for the sixth generation (6G) of wireless networks.","Recently, task-oriented semantic communications have been proposed for low-latency inference with limited bandwidth.","Although transmitting only task-related information does protect a certain level of user privacy, adversaries could apply model inversion techniques to reconstruct the raw data or extract useful information, thereby infringing on users' privacy.","To mitigate privacy infringement, this paper proposes an information bottleneck and adversarial learning (IBAL) approach to protect users' privacy against model inversion attacks.","Specifically, we extract task-relevant features from the input based on the information bottleneck (IB) theory.","To overcome the difficulty in calculating the mutual information in high-dimensional space, we derive a variational upper bound to estimate the true mutual information.","To prevent data reconstruction from task-related features by adversaries, we leverage adversarial learning to train encoder to fool adversaries by maximizing reconstruction distortion.","Furthermore, considering the impact of channel variations on privacy-utility trade-off and the difficulty in manually tuning the weights of each loss, we propose an adaptive weight adjustment method.","Numerical results demonstrate that the proposed approaches can effectively protect privacy without significantly affecting task performance and achieve better privacy-utility trade-offs than baseline methods."],"url":"http://arxiv.org/abs/2312.03252v1"}
{"created":"2023-12-06 01:51:03","title":"Rethinking Object Saliency Ranking: A Novel Whole-flow Processing Paradigm","abstract":"Existing salient object detection methods are capable of predicting binary maps that highlight visually salient regions. However, these methods are limited in their ability to differentiate the relative importance of multiple objects and the relationships among them, which can lead to errors and reduced accuracy in downstream tasks that depend on the relative importance of multiple objects. To conquer, this paper proposes a new paradigm for saliency ranking, which aims to completely focus on ranking salient objects by their \"importance order\". While previous works have shown promising performance, they still face ill-posed problems. First, the saliency ranking ground truth (GT) orders generation methods are unreasonable since determining the correct ranking order is not well-defined, resulting in false alarms. Second, training a ranking model remains challenging because most saliency ranking methods follow the multi-task paradigm, leading to conflicts and trade-offs among different tasks. Third, existing regression-based saliency ranking methods are complex for saliency ranking models due to their reliance on instance mask-based saliency ranking orders. These methods require a significant amount of data to perform accurately and can be challenging to implement effectively. To solve these problems, this paper conducts an in-depth analysis of the causes and proposes a whole-flow processing paradigm of saliency ranking task from the perspective of \"GT data generation\", \"network structure design\" and \"training protocol\". The proposed approach outperforms existing state-of-the-art methods on the widely-used SALICON set, as demonstrated by extensive experiments with fair and reasonable comparisons. The saliency ranking task is still in its infancy, and our proposed unified framework can serve as a fundamental strategy to guide future work.","sentences":["Existing salient object detection methods are capable of predicting binary maps that highlight visually salient regions.","However, these methods are limited in their ability to differentiate the relative importance of multiple objects and the relationships among them, which can lead to errors and reduced accuracy in downstream tasks that depend on the relative importance of multiple objects.","To conquer, this paper proposes a new paradigm for saliency ranking, which aims to completely focus on ranking salient objects by their \"importance order\".","While previous works have shown promising performance, they still face ill-posed problems.","First, the saliency ranking ground truth (GT) orders generation methods are unreasonable since determining the correct ranking order is not well-defined, resulting in false alarms.","Second, training a ranking model remains challenging because most saliency ranking methods follow the multi-task paradigm, leading to conflicts and trade-offs among different tasks.","Third, existing regression-based saliency ranking methods are complex for saliency ranking models due to their reliance on instance mask-based saliency ranking orders.","These methods require a significant amount of data to perform accurately and can be challenging to implement effectively.","To solve these problems, this paper conducts an in-depth analysis of the causes and proposes a whole-flow processing paradigm of saliency ranking task from the perspective of \"GT data generation\", \"network structure design\" and \"training protocol\".","The proposed approach outperforms existing state-of-the-art methods on the widely-used SALICON set, as demonstrated by extensive experiments with fair and reasonable comparisons.","The saliency ranking task is still in its infancy, and our proposed unified framework can serve as a fundamental strategy to guide future work."],"url":"http://arxiv.org/abs/2312.03226v1"}
{"created":"2023-12-06 01:41:49","title":"Predicting Scores of Various Aesthetic Attribute Sets by Learning from Overall Score Labels","abstract":"Now many mobile phones embed deep-learning models for evaluation or guidance on photography. These models cannot provide detailed results like human pose scores or scene color scores because of the rare of corresponding aesthetic attribute data. However, the annotation of image aesthetic attribute scores requires experienced artists and professional photographers, which hinders the collection of large-scale fully-annotated datasets. In this paper, we propose to replace image attribute labels with feature extractors. First, a novel aesthetic attribute evaluation framework based on attribute features is proposed to predict attribute scores and overall scores. We call it the F2S (attribute features to attribute scores) model. We use networks from different tasks to provide attribute features to our F2S models. Then, we define an aesthetic attribute contribution to describe the role of aesthetic attributes throughout an image and use it with the attribute scores and the overall scores to train our F2S model. Sufficient experiments on publicly available datasets demonstrate that our F2S model achieves comparable performance with those trained on the datasets with fully-annotated aesthetic attribute score labels. Our method makes it feasible to learn meaningful attribute scores for various aesthetic attribute sets in different types of images with only overall aesthetic scores.","sentences":["Now many mobile phones embed deep-learning models for evaluation or guidance on photography.","These models cannot provide detailed results like human pose scores or scene color scores because of the rare of corresponding aesthetic attribute data.","However, the annotation of image aesthetic attribute scores requires experienced artists and professional photographers, which hinders the collection of large-scale fully-annotated datasets.","In this paper, we propose to replace image attribute labels with feature extractors.","First, a novel aesthetic attribute evaluation framework based on attribute features is proposed to predict attribute scores and overall scores.","We call it the F2S (attribute features to attribute scores) model.","We use networks from different tasks to provide attribute features to our F2S models.","Then, we define an aesthetic attribute contribution to describe the role of aesthetic attributes throughout an image and use it with the attribute scores and the overall scores to train our F2S model.","Sufficient experiments on publicly available datasets demonstrate that our F2S model achieves comparable performance with those trained on the datasets with fully-annotated aesthetic attribute score labels.","Our method makes it feasible to learn meaningful attribute scores for various aesthetic attribute sets in different types of images with only overall aesthetic scores."],"url":"http://arxiv.org/abs/2312.03222v1"}
{"created":"2023-12-06 01:15:40","title":"Rethinking E-Commerce Search","abstract":"E-commerce search and recommendation usually operate on structured data such as product catalogs and taxonomies. However, creating better search and recommendation systems often requires a large variety of unstructured data including customer reviews and articles on the web. Traditionally, the solution has always been converting unstructured data into structured data through information extraction, and conducting search over the structured data. However, this is a costly approach that often has low quality. In this paper, we envision a solution that does entirely the opposite. Instead of converting unstructured data (web pages, customer reviews, etc) to structured data, we instead convert structured data (product inventory, catalogs, taxonomies, etc) into textual data, which can be easily integrated into the text corpus that trains LLMs. Then, search and recommendation can be performed through a Q/A mechanism through an LLM instead of using traditional information retrieval methods over structured data.","sentences":["E-commerce search and recommendation usually operate on structured data such as product catalogs and taxonomies.","However, creating better search and recommendation systems often requires a large variety of unstructured data including customer reviews and articles on the web.","Traditionally, the solution has always been converting unstructured data into structured data through information extraction, and conducting search over the structured data.","However, this is a costly approach that often has low quality.","In this paper, we envision a solution that does entirely the opposite.","Instead of converting unstructured data (web pages, customer reviews, etc) to structured data, we instead convert structured data (product inventory, catalogs, taxonomies, etc) into textual data, which can be easily integrated into the text corpus that trains LLMs.","Then, search and recommendation can be performed through a Q/A mechanism through an LLM instead of using traditional information retrieval methods over structured data."],"url":"http://arxiv.org/abs/2312.03217v1"}
{"created":"2023-12-06 01:00:07","title":"Constrained Bayesian Optimization Under Partial Observations: Balanced Improvements and Provable Convergence","abstract":"The partially observable constrained optimization problems (POCOPs) impede data-driven optimization techniques since an infeasible solution of POCOPs can provide little information about the objective as well as the constraints. We endeavor to design an efficient and provable method for expensive POCOPs under the framework of constrained Bayesian optimization. Our method consists of two key components. Firstly, we present an improved design of the acquisition functions that introduces balanced exploration during optimization. We rigorously study the convergence properties of this design to demonstrate its effectiveness. Secondly, we propose a Gaussian process embedding different likelihoods as the surrogate model for a partially observable constraint. This model leads to a more accurate representation of the feasible regions compared to traditional classification-based models. Our proposed method is empirically studied on both synthetic and real-world problems. The results demonstrate the competitiveness of our method for solving POCOPs.","sentences":["The partially observable constrained optimization problems (POCOPs) impede data-driven optimization techniques since an infeasible solution of POCOPs can provide little information about the objective as well as the constraints.","We endeavor to design an efficient and provable method for expensive POCOPs under the framework of constrained Bayesian optimization.","Our method consists of two key components.","Firstly, we present an improved design of the acquisition functions that introduces balanced exploration during optimization.","We rigorously study the convergence properties of this design to demonstrate its effectiveness.","Secondly, we propose a Gaussian process embedding different likelihoods as the surrogate model for a partially observable constraint.","This model leads to a more accurate representation of the feasible regions compared to traditional classification-based models.","Our proposed method is empirically studied on both synthetic and real-world problems.","The results demonstrate the competitiveness of our method for solving POCOPs."],"url":"http://arxiv.org/abs/2312.03212v1"}
{"created":"2023-12-06 00:48:50","title":"Satellite Imagery and AI: A New Era in Ocean Conservation, from Research to Deployment and Impact","abstract":"Illegal, unreported, and unregulated (IUU) fishing poses a global threat to ocean habitats. Publicly available satellite data offered by NASA and the European Space Agency (ESA) provide an opportunity to actively monitor this activity. Effectively leveraging satellite data for maritime conservation requires highly reliable machine learning models operating globally with minimal latency. This paper introduces three specialized computer vision models designed for synthetic aperture radar (Sentinel-1), optical imagery (Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents best practices for developing and delivering real-time computer vision services for conservation. These models have been deployed in Skylight, a real time maritime monitoring platform, which is provided at no cost to users worldwide.","sentences":["Illegal, unreported, and unregulated (IUU) fishing poses a global threat to ocean habitats.","Publicly available satellite data offered by NASA and the European Space Agency (ESA) provide an opportunity to actively monitor this activity.","Effectively leveraging satellite data for maritime conservation requires highly reliable machine learning models operating globally with minimal latency.","This paper introduces three specialized computer vision models designed for synthetic aperture radar (Sentinel-1), optical imagery (Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20).","It also presents best practices for developing and delivering real-time computer vision services for conservation.","These models have been deployed in Skylight, a real time maritime monitoring platform, which is provided at no cost to users worldwide."],"url":"http://arxiv.org/abs/2312.03207v1"}
{"created":"2023-12-06 00:47:55","title":"Who Leaked the Model? Tracking IP Infringers in Accountable Federated Learning","abstract":"Federated learning (FL) emerges as an effective collaborative learning framework to coordinate data and computation resources from massive and distributed clients in training. Such collaboration results in non-trivial intellectual property (IP) represented by the model parameters that should be protected and shared by the whole party rather than an individual user. Meanwhile, the distributed nature of FL endorses a malicious client the convenience to compromise IP through illegal model leakage to unauthorized third parties. To block such IP leakage, it is essential to make the IP identifiable in the shared model and locate the anonymous infringer who first leaks it. The collective challenges call for \\emph{accountable federated learning}, which requires verifiable ownership of the model and is capable of revealing the infringer's identity upon leakage. In this paper, we propose Decodable Unique Watermarking (DUW) for complying with the requirements of accountable FL. Specifically, before a global model is sent to a client in an FL round, DUW encodes a client-unique key into the model by leveraging a backdoor-based watermark injection. To identify the infringer of a leaked model, DUW examines the model and checks if the triggers can be decoded as the corresponding keys. Extensive empirical results show that DUW is highly effective and robust, achieving over $99\\%$ watermark success rate for Digits, CIFAR-10, and CIFAR-100 datasets under heterogeneous FL settings, and identifying the IP infringer with $100\\%$ accuracy even after common watermark removal attempts.","sentences":["Federated learning (FL) emerges as an effective collaborative learning framework to coordinate data and computation resources from massive and distributed clients in training.","Such collaboration results in non-trivial intellectual property (IP) represented by the model parameters that should be protected and shared by the whole party rather than an individual user.","Meanwhile, the distributed nature of FL endorses a malicious client the convenience to compromise IP through illegal model leakage to unauthorized third parties.","To block such IP leakage, it is essential to make the IP identifiable in the shared model and locate the anonymous infringer who first leaks it.","The collective challenges call for \\emph{accountable federated learning}, which requires verifiable ownership of the model and is capable of revealing the infringer's identity upon leakage.","In this paper, we propose Decodable Unique Watermarking (DUW) for complying with the requirements of accountable FL.","Specifically, before a global model is sent to a client in an FL round, DUW encodes a client-unique key into the model by leveraging a backdoor-based watermark injection.","To identify the infringer of a leaked model, DUW examines the model and checks if the triggers can be decoded as the corresponding keys.","Extensive empirical results show that DUW is highly effective and robust, achieving over $99\\%$ watermark success rate for Digits, CIFAR-10, and CIFAR-100 datasets under heterogeneous FL settings, and identifying the IP infringer with $100\\%$ accuracy even after common watermark removal attempts."],"url":"http://arxiv.org/abs/2312.03205v1"}
{"created":"2023-12-06 00:28:08","title":"Domain Invariant Representation Learning and Sleep Dynamics Modeling for Automatic Sleep Staging","abstract":"Sleep staging has become a critical task in diagnosing and treating sleep disorders to prevent sleep related diseases. With rapidly growing large scale public sleep databases and advances in machine learning, significant progress has been made toward automatic sleep staging. However, previous studies face some critical problems in sleep studies; the heterogeneity of subjects' physiological signals, the inability to extract meaningful information from unlabeled sleep signal data to improve predictive performances, the difficulty in modeling correlations between sleep stages, and the lack of an effective mechanism to quantify predictive uncertainty. In this study, we propose a neural network based automatic sleep staging model, named DREAM, to learn domain generalized representations from physiological signals and models sleep dynamics. DREAM learns sleep related and subject invariant representations from diverse subjects' sleep signal segments and models sleep dynamics by capturing interactions between sequential signal segments and between sleep stages. In the experiments, we demonstrate that DREAM outperforms the existing sleep staging methods on three datasets. The case study demonstrates that our model can learn the generalized decision function resulting in good prediction performances for the new subjects, especially in case there are differences between testing and training subjects. The usage of unlabeled data shows the benefit of leveraging unlabeled EEG data. Further, uncertainty quantification demonstrates that DREAM provides prediction uncertainty, making the model reliable and helping sleep experts in real world applications.","sentences":["Sleep staging has become a critical task in diagnosing and treating sleep disorders to prevent sleep related diseases.","With rapidly growing large scale public sleep databases and advances in machine learning, significant progress has been made toward automatic sleep staging.","However, previous studies face some critical problems in sleep studies; the heterogeneity of subjects' physiological signals, the inability to extract meaningful information from unlabeled sleep signal data to improve predictive performances, the difficulty in modeling correlations between sleep stages, and the lack of an effective mechanism to quantify predictive uncertainty.","In this study, we propose a neural network based automatic sleep staging model, named DREAM, to learn domain generalized representations from physiological signals and models sleep dynamics.","DREAM learns sleep related and subject invariant representations from diverse subjects' sleep signal segments and models sleep dynamics by capturing interactions between sequential signal segments and between sleep stages.","In the experiments, we demonstrate that DREAM outperforms the existing sleep staging methods on three datasets.","The case study demonstrates that our model can learn the generalized decision function resulting in good prediction performances for the new subjects, especially in case there are differences between testing and training subjects.","The usage of unlabeled data shows the benefit of leveraging unlabeled EEG data.","Further, uncertainty quantification demonstrates that DREAM provides prediction uncertainty, making the model reliable and helping sleep experts in real world applications."],"url":"http://arxiv.org/abs/2312.03196v1"}
{"created":"2023-12-06 00:05:25","title":"Corporate Bankruptcy Prediction with Domain-Adapted BERT","abstract":"This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on bankruptcy prediction mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform sentiment analysis on MD&A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in logistic regression, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply self-learning with confidence-based filtering to corporate disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy.","sentences":["This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies.","Prior literature on bankruptcy prediction mainly focuses on developing more sophisticated prediction methodologies with financial variables.","However, in our study, we focus on improving the quality of input dataset.","Specifically, we employ BERT model to perform sentiment analysis on MD&A disclosures.","We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in logistic regression, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM).","Further, instead of pre-training the BERT model from scratch, we apply self-learning with confidence-based filtering to corporate disclosure data (10-K).","We achieve the accuracy rate of 91.56% and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy."],"url":"http://arxiv.org/abs/2312.03194v1"}
{"created":"2023-12-05 23:33:49","title":"FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction","abstract":"Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is most consistently reflective of negative evaluations of the generated image. This can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in AU4 responses to them with an accuracy significantly outperforming state-of-the-art scoring models. Secondly, directly integrating the AU4 responses with the scoring models improves their consistency with human preferences. Additionally, the AU4 response best reflects the user's evaluation of the image fidelity, making it complementary to the state-of-the-art scoring models, which are generally better at reflecting image-text alignment. Finally, this method of automatic annotation with facial expression analysis can be potentially generalized to other generation tasks. The code is available at https://github.com/ShuangquanFeng/FERGI, and the dataset is also available at the same link for research purposes.","sentences":["Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models.","However, the scalability of human feedback collection has been limited by its reliance on manual annotation.","Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images.","We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images.","Specifically, AU4 (brow lowerer) is most consistently reflective of negative evaluations of the generated image.","This can be useful in two ways.","Firstly, we can automatically annotate user preferences between image pairs with substantial difference in AU4 responses to them with an accuracy significantly outperforming state-of-the-art scoring models.","Secondly, directly integrating the AU4 responses with the scoring models improves their consistency with human preferences.","Additionally, the AU4 response best reflects the user's evaluation of the image fidelity, making it complementary to the state-of-the-art scoring models, which are generally better at reflecting image-text alignment.","Finally, this method of automatic annotation with facial expression analysis can be potentially generalized to other generation tasks.","The code is available at https://github.com/ShuangquanFeng/FERGI, and the dataset is also available at the same link for research purposes."],"url":"http://arxiv.org/abs/2312.03187v1"}
{"created":"2023-12-05 23:32:48","title":"Data-Driven Traffic Reconstruction and Kernel Methods for Identifying Stop-and-Go Congestion","abstract":"Identifying stop-and-go events (SAGs) in traffic flow presents an important avenue for advancing data-driven research for climate change mitigation and sustainability, owing to their substantial impact on carbon emissions, travel time, fuel consumption, and roadway safety. In fact, SAGs are estimated to account for 33-50% of highway driving externalities. However, insufficient attention has been paid to precisely quantifying where, when, and how much these SAGs take place -necessary for downstream decision making, such as intervention design and policy analysis. A key challenge is that the data available to researchers and governments are typically sparse and aggregated to a granularity that obscures SAGs. To overcome such data limitations, this study thus explores the use of traffic reconstruction techniques for SAG identification. In particular, we introduce a kernel-based method for identifying spatio-temporal features in traffic and leverage bootstrapping to quantify the uncertainty of the reconstruction process. Experimental results on California highway data demonstrate the promise of the method for capturing SAGs. This work contributes to a foundation for data-driven decision making to advance sustainability of traffic systems.","sentences":["Identifying stop-and-go events (SAGs) in traffic flow presents an important avenue for advancing data-driven research for climate change mitigation and sustainability, owing to their substantial impact on carbon emissions, travel time, fuel consumption, and roadway safety.","In fact, SAGs are estimated to account for 33-50% of highway driving externalities.","However, insufficient attention has been paid to precisely quantifying where, when, and how much these SAGs take place -necessary for downstream decision making, such as intervention design and policy analysis.","A key challenge is that the data available to researchers and governments are typically sparse and aggregated to a granularity that obscures SAGs.","To overcome such data limitations, this study thus explores the use of traffic reconstruction techniques for SAG identification.","In particular, we introduce a kernel-based method for identifying spatio-temporal features in traffic and leverage bootstrapping to quantify the uncertainty of the reconstruction process.","Experimental results on California highway data demonstrate the promise of the method for capturing SAGs.","This work contributes to a foundation for data-driven decision making to advance sustainability of traffic systems."],"url":"http://arxiv.org/abs/2312.03186v1"}
{"created":"2023-12-05 22:44:05","title":"Active Learning for Abrupt Shifts Change-point Detection via Derivative-Aware Gaussian Processes","abstract":"Change-point detection (CPD) is crucial for identifying abrupt shifts in data, which influence decision-making and efficient resource allocation across various domains. To address the challenges posed by the costly and time-intensive data acquisition in CPD, we introduce the Derivative-Aware Change Detection (DACD) method. It leverages the derivative process of a Gaussian process (GP) for Active Learning (AL), aiming to pinpoint change-point locations effectively. DACD balances the exploitation and exploration of derivative processes through multiple data acquisition functions (AFs). By utilizing GP derivative mean and variance as criteria, DACD sequentially selects the next sampling data point, thus enhancing algorithmic efficiency and ensuring reliable and accurate results. We investigate the effectiveness of DACD method in diverse scenarios and show it outperforms other active learning change-point detection approaches.","sentences":["Change-point detection (CPD) is crucial for identifying abrupt shifts in data, which influence decision-making and efficient resource allocation across various domains.","To address the challenges posed by the costly and time-intensive data acquisition in CPD, we introduce the Derivative-Aware Change Detection (DACD) method.","It leverages the derivative process of a Gaussian process (GP) for Active Learning (AL), aiming to pinpoint change-point locations effectively.","DACD balances the exploitation and exploration of derivative processes through multiple data acquisition functions (AFs).","By utilizing GP derivative mean and variance as criteria, DACD sequentially selects the next sampling data point, thus enhancing algorithmic efficiency and ensuring reliable and accurate results.","We investigate the effectiveness of DACD method in diverse scenarios and show it outperforms other active learning change-point detection approaches."],"url":"http://arxiv.org/abs/2312.03176v1"}
{"created":"2023-12-05 22:28:42","title":"Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review","abstract":"Technology Assisted Review (TAR) stopping rules aim to reduce the cost of manually assessing documents for relevance by minimising the number of documents that need to be examined to ensure a desired level of recall. This paper extends an effective stopping rule using information derived from a text classifier that can be trained without the need for any additional annotation. Experiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal and RCV1) showed that the proposed approach consistently improves performance and outperforms several alternative methods.","sentences":["Technology Assisted Review (TAR) stopping rules aim to reduce the cost of manually assessing documents for relevance by minimising the number of documents that need to be examined to ensure a desired level of recall.","This paper extends an effective stopping rule using information derived from a text classifier that can be trained without the need for any additional annotation.","Experiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal and RCV1) showed that the proposed approach consistently improves performance and outperforms several alternative methods."],"url":"http://arxiv.org/abs/2312.03171v1"}
{"created":"2023-12-05 22:22:25","title":"Adaptive spectral graph wavelets for collaborative filtering","abstract":"Collaborative filtering is a popular approach in recommender systems, whose objective is to provide personalized item suggestions to potential users based on their purchase or browsing history. However, personalized recommendations require considerable amount of behavioral data on users, which is usually unavailable for new users, giving rise to the cold-start problem. To help alleviate this challenging problem, we introduce a spectral graph wavelet collaborative filtering framework for implicit feedback data, where users, items and their interactions are represented as a bipartite graph. Specifically, we first propose an adaptive transfer function by leveraging a power transform with the goal of stabilizing the variance of graph frequencies in the spectral domain. Then, we design a deep recommendation model for efficient learning of low-dimensional embeddings of users and items using spectral graph wavelets in an end-to-end fashion. In addition to capturing the graph's local and global structures, our approach yields localization of graph signals in both spatial and spectral domains, and hence not only learns discriminative representations of users and items, but also promotes the recommendation quality. The effectiveness of our proposed model is demonstrated through extensive experiments on real-world benchmark datasets, achieving better recommendation performance compared with strong baseline methods.","sentences":["Collaborative filtering is a popular approach in recommender systems, whose objective is to provide personalized item suggestions to potential users based on their purchase or browsing history.","However, personalized recommendations require considerable amount of behavioral data on users, which is usually unavailable for new users, giving rise to the cold-start problem.","To help alleviate this challenging problem, we introduce a spectral graph wavelet collaborative filtering framework for implicit feedback data, where users, items and their interactions are represented as a bipartite graph.","Specifically, we first propose an adaptive transfer function by leveraging a power transform with the goal of stabilizing the variance of graph frequencies in the spectral domain.","Then, we design a deep recommendation model for efficient learning of low-dimensional embeddings of users and items using spectral graph wavelets in an end-to-end fashion.","In addition to capturing the graph's local and global structures, our approach yields localization of graph signals in both spatial and spectral domains, and hence not only learns discriminative representations of users and items, but also promotes the recommendation quality.","The effectiveness of our proposed model is demonstrated through extensive experiments on real-world benchmark datasets, achieving better recommendation performance compared with strong baseline methods."],"url":"http://arxiv.org/abs/2312.03167v1"}
{"created":"2023-12-05 22:16:54","title":"Deep Learning for Fast Inference of Mechanistic Models' Parameters","abstract":"Inferring parameters of macro-kinetic growth models, typically represented by Ordinary Differential Equations (ODE), from the experimental data is a crucial step in bioprocess engineering. Conventionally, estimates of the parameters are obtained by fitting the mechanistic model to observations. Fitting, however, requires a significant computational power. Specifically, during the development of new bioprocesses that use previously unknown organisms or strains, efficient, robust, and computationally cheap methods for parameter estimation are of great value. In this work, we propose using Deep Neural Networks (NN) for directly predicting parameters of mechanistic models given observations. The approach requires spending computational resources for training a NN, nonetheless, once trained, such a network can provide parameter estimates orders of magnitude faster than conventional methods. We consider a training procedure that combines Neural Networks and mechanistic models. We demonstrate the performance of the proposed algorithms on data sampled from several mechanistic models used in bioengineering describing a typical industrial batch process and compare the proposed method, a typical gradient-based fitting procedure, and the combination of the two. We find that, while Neural Network estimates are slightly improved by further fitting, these estimates are measurably better than the fitting procedure alone.","sentences":["Inferring parameters of macro-kinetic growth models, typically represented by Ordinary Differential Equations (ODE), from the experimental data is a crucial step in bioprocess engineering.","Conventionally, estimates of the parameters are obtained by fitting the mechanistic model to observations.","Fitting, however, requires a significant computational power.","Specifically, during the development of new bioprocesses that use previously unknown organisms or strains, efficient, robust, and computationally cheap methods for parameter estimation are of great value.","In this work, we propose using Deep Neural Networks (NN) for directly predicting parameters of mechanistic models given observations.","The approach requires spending computational resources for training a NN, nonetheless, once trained, such a network can provide parameter estimates orders of magnitude faster than conventional methods.","We consider a training procedure that combines Neural Networks and mechanistic models.","We demonstrate the performance of the proposed algorithms on data sampled from several mechanistic models used in bioengineering describing a typical industrial batch process and compare the proposed method, a typical gradient-based fitting procedure, and the combination of the two.","We find that, while Neural Network estimates are slightly improved by further fitting, these estimates are measurably better than the fitting procedure alone."],"url":"http://arxiv.org/abs/2312.03166v1"}
{"created":"2023-12-05 21:38:24","title":"Multitask Learning Can Improve Worst-Group Outcomes","abstract":"In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the common setting of fine-tuning a pre-trained model, where, following recent work (Gururangan et al., 2020; Dery et al., 2023), we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not always, achieves better worst-group accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language and find that our regularized MTL approach consistently outperforms JTT on both worst and average group outcomes. Our official code can be found here: https://github.com/atharvajk98/MTL-group-robustness.","sentences":["In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups.","However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error.","Multitask learning (MTL) is one such widely used technique.","In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness.","We primarily consider the common setting of fine-tuning a pre-trained model, where, following recent work (Gururangan et al., 2020; Dery et al., 2023), we multitask the end task with the pre-training objective constructed from the end task data itself.","In settings with few or no group annotations, we find that multitasking often, but not always, achieves better worst-group accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative distributionally robust optimization (DRO) method.","Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space.","We run a large number of fine-tuning experiments across computer vision and natural language and find that our regularized MTL approach consistently outperforms JTT on both worst and average group outcomes.","Our official code can be found here: https://github.com/atharvajk98/MTL-group-robustness."],"url":"http://arxiv.org/abs/2312.03151v1"}
{"created":"2023-12-05 21:34:59","title":"Neural parameter calibration and uncertainty quantification for epidemic forecasting","abstract":"The recent COVID-19 pandemic has thrown the importance of accurately forecasting contagion dynamics and learning infection parameters into sharp focus. At the same time, effective policy-making requires knowledge of the uncertainty on such predictions, in order, for instance, to be able to ready hospitals and intensive care units for a worst-case scenario without needlessly wasting resources. In this work, we apply a novel and powerful computational method to the problem of learning probability densities on contagion parameters and providing uncertainty quantification for pandemic projections. Using a neural network, we calibrate an ODE model to data of the spread of COVID-19 in Berlin in 2020, achieving both a significantly more accurate calibration and prediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes. The uncertainties on our predictions provide meaningful confidence intervals e.g. on infection figures and hospitalisation rates, while training and running the neural scheme takes minutes where MCMC takes hours. We show convergence of our method to the true posterior on a simplified SIR model of epidemics, and also demonstrate our method's learning capabilities on a reduced dataset, where a complex model is learned from a small number of compartments for which data is available.","sentences":["The recent COVID-19 pandemic has thrown the importance of accurately forecasting contagion dynamics and learning infection parameters into sharp focus.","At the same time, effective policy-making requires knowledge of the uncertainty on such predictions, in order, for instance, to be able to ready hospitals and intensive care units for a worst-case scenario without needlessly wasting resources.","In this work, we apply a novel and powerful computational method to the problem of learning probability densities on contagion parameters and providing uncertainty quantification for pandemic projections.","Using a neural network, we calibrate an ODE model to data of the spread of COVID-19 in Berlin in 2020, achieving both a significantly more accurate calibration and prediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes.","The uncertainties on our predictions provide meaningful confidence intervals e.g. on infection figures and hospitalisation rates, while training and running the neural scheme takes minutes where MCMC takes hours.","We show convergence of our method to the true posterior on a simplified SIR model of epidemics, and also demonstrate our method's learning capabilities on a reduced dataset, where a complex model is learned from a small number of compartments for which data is available."],"url":"http://arxiv.org/abs/2312.03147v1"}
{"created":"2023-12-05 21:21:01","title":"In-Storage Acceleration of Graph-Traversal-Based Approximate Nearest Neighbor Search","abstract":"Approximate nearest neighbor search (ANNS) is a key retrieval technique for vector database and many data center applications, such as person re-identification and recommendation systems. Among all the ANNS algorithms, graph-traversal-based ANNS achieves the highest recall rate. However, as the size of dataset increases, the graph may require hundreds of gigabytes of memory, exceeding the main memory capacity of a single workstation node. Although we can do partitioning and use solid-state drive (SSD) as the backing storage, the limited SSD I/O bandwidth severely degrades the performance of the system. To address this challenge, we present NDSearch, a near-data processing (NDP) solution for ANNS processing. NDSearch consists of a novel in-storage computing architecture, namely, SEARSSD, that supports the ANNS kernels and leverages logic unit (LUN)-level parallelism inside the NAND flash chips. NDSearch also includes a processing model that is customized for NDP and cooperates with SEARSSD. The processing model enables us to apply a two-level scheduling to improve the data locality and exploit the internal bandwidth in NDSEARCH, and a speculative searching mechanism to further accelerate the ANNS workload. Our results show that NDSearch improves the throughput by up to 31.7x, 14.6x, 7.4x, 2.9x over CPU, GPU, a state-of-the-art SmartSSD-only design, and DeepStore, respectively. NDSEARCH also achieves two orders-of-magnitude higher energy efficiency than CPU and GPU.","sentences":["Approximate nearest neighbor search (ANNS) is a key retrieval technique for vector database and many data center applications, such as person re-identification and recommendation systems.","Among all the ANNS algorithms, graph-traversal-based ANNS achieves the highest recall rate.","However, as the size of dataset increases, the graph may require hundreds of gigabytes of memory, exceeding the main memory capacity of a single workstation node.","Although we can do partitioning and use solid-state drive (SSD) as the backing storage, the limited SSD I/O bandwidth severely degrades the performance of the system.","To address this challenge, we present NDSearch, a near-data processing (NDP) solution for ANNS processing.","NDSearch consists of a novel in-storage computing architecture, namely, SEARSSD, that supports the ANNS kernels and leverages logic unit (LUN)-level parallelism inside the NAND flash chips.","NDSearch also includes a processing model that is customized for NDP and cooperates with SEARSSD.","The processing model enables us to apply a two-level scheduling to improve the data locality and exploit the internal bandwidth in NDSEARCH, and a speculative searching mechanism to further accelerate the ANNS workload.","Our results show that NDSearch improves the throughput by up to 31.7x, 14.6x, 7.4x, 2.9x over CPU, GPU, a state-of-the-art SmartSSD-only design, and DeepStore, respectively.","NDSEARCH also achieves two orders-of-magnitude higher energy efficiency than CPU and GPU."],"url":"http://arxiv.org/abs/2312.03141v1"}
{"created":"2023-12-05 20:40:37","title":"Evaluating Agents using Social Choice Theory","abstract":"We argue that many general evaluation problems can be viewed through the lens of voting theory. Each task is interpreted as a separate voter, which requires only ordinal rankings or pairwise comparisons of agents to produce an overall evaluation. By viewing the aggregator as a social welfare function, we are able to leverage centuries of research in social choice theory to derive principled evaluation frameworks with axiomatic foundations. These evaluations are interpretable and flexible, while avoiding many of the problems currently facing cross-task evaluation. We apply this Voting-as-Evaluation (VasE) framework across multiple settings, including reinforcement learning, large language models, and humans. In practice, we observe that VasE can be more robust than popular evaluation frameworks (Elo and Nash averaging), discovers properties in the evaluation data not evident from scores alone, and can predict outcomes better than Elo in a complex seven-player game. We identify one particular approach, maximal lotteries, that satisfies important consistency properties relevant to evaluation, is computationally efficient (polynomial in the size of the evaluation data), and identifies game-theoretic cycles","sentences":["We argue that many general evaluation problems can be viewed through the lens of voting theory.","Each task is interpreted as a separate voter, which requires only ordinal rankings or pairwise comparisons of agents to produce an overall evaluation.","By viewing the aggregator as a social welfare function, we are able to leverage centuries of research in social choice theory to derive principled evaluation frameworks with axiomatic foundations.","These evaluations are interpretable and flexible, while avoiding many of the problems currently facing cross-task evaluation.","We apply this Voting-as-Evaluation (VasE) framework across multiple settings, including reinforcement learning, large language models, and humans.","In practice, we observe that VasE can be more robust than popular evaluation frameworks (Elo and Nash averaging), discovers properties in the evaluation data not evident from scores alone, and can predict outcomes better than Elo in a complex seven-player game.","We identify one particular approach, maximal lotteries, that satisfies important consistency properties relevant to evaluation, is computationally efficient (polynomial in the size of the evaluation data), and identifies game-theoretic cycles"],"url":"http://arxiv.org/abs/2312.03121v1"}
{"created":"2023-12-05 20:40:05","title":"The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning","abstract":"With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.","sentences":["With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products.","In this study, we present a review of modern machine and deep learning.","We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks.","Our discussion encompasses parallel distributed learning, deep learning as well as federated learning.","As a result, our work serves as an introductory text to the vast field of modern machine learning."],"url":"http://arxiv.org/abs/2312.03120v1"}
{"created":"2023-12-05 19:53:25","title":"Improving Automated Algorithm Selection by Advancing Fitness Landscape Analysis","abstract":"Optimization is ubiquitous in our daily lives. In the past, (sub-)optimal solutions to any problem have been derived by trial and error, sheer luck, or the expertise of knowledgeable individuals. In our contemporary age, there thankfully exists a plethora of different algorithms that can find solutions more reliably than ever before. Yet, choosing an appropriate algorithm for any given problem is challenging in itself. The field of automated algorithm selection provides various approaches to tackle this latest problem. This is done by delegating the selection of a suitable algorithm for a given problem to a complex computer model. This computer model is generated through the use of Artificial Intelligence. Many of these computer models rely on some sort of information about the problem to make a reasonable selection. Various methods exist to provide this informative input to the computer model in the form of numerical data.   In this cumulative dissertation, I propose several improvements to the different variants of informative inputs. This in turn enhances and refines the current state-of-the-art of automated algorithm selection. Specifically, I identify and address current issues with the existing body of work to strengthen the foundation that future work builds upon. Furthermore, the rise of deep learning offers ample opportunities for automated algorithm selection. In several joint works, my colleagues and I developed and evaluated several different methods that replace the existing methods to extract an informative input. Lastly, automated algorithm selection approaches have been restricted to certain types of problems. I propose a method to extend the generation of informative inputs to other problem types and provide an outlook on further promising research directions.","sentences":["Optimization is ubiquitous in our daily lives.","In the past, (sub-)optimal solutions to any problem have been derived by trial and error, sheer luck, or the expertise of knowledgeable individuals.","In our contemporary age, there thankfully exists a plethora of different algorithms that can find solutions more reliably than ever before.","Yet, choosing an appropriate algorithm for any given problem is challenging in itself.","The field of automated algorithm selection provides various approaches to tackle this latest problem.","This is done by delegating the selection of a suitable algorithm for a given problem to a complex computer model.","This computer model is generated through the use of Artificial Intelligence.","Many of these computer models rely on some sort of information about the problem to make a reasonable selection.","Various methods exist to provide this informative input to the computer model in the form of numerical data.   ","In this cumulative dissertation, I propose several improvements to the different variants of informative inputs.","This in turn enhances and refines the current state-of-the-art of automated algorithm selection.","Specifically, I identify and address current issues with the existing body of work to strengthen the foundation that future work builds upon.","Furthermore, the rise of deep learning offers ample opportunities for automated algorithm selection.","In several joint works, my colleagues and I developed and evaluated several different methods that replace the existing methods to extract an informative input.","Lastly, automated algorithm selection approaches have been restricted to certain types of problems.","I propose a method to extend the generation of informative inputs to other problem types and provide an outlook on further promising research directions."],"url":"http://arxiv.org/abs/2312.03105v1"}
{"created":"2023-12-05 19:29:54","title":"Incidental Polysemanticity","abstract":"Polysemantic neurons (neurons that activate for a set of unrelated features) have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more \"features\" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand the network's internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, using a combination of theory and experiments. This second type of polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap. Due to its origin, we term this \\textit{incidental polysemanticity}.","sentences":["Polysemantic neurons (neurons that activate for a set of unrelated features) have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety.","The classic origin story of polysemanticity is that the data contains more \"features\" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand the network's internal processing.","In this work, we present a second and non-mutually exclusive origin story of polysemanticity.","We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, using a combination of theory and experiments.","This second type of polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap.","Due to its origin, we term this \\textit{incidental polysemanticity}."],"url":"http://arxiv.org/abs/2312.03096v1"}
{"created":"2023-12-05 19:26:28","title":"Understanding Environmental Posts: Sentiment and Emotion Analysis of Social Media Data","abstract":"Social media is now the predominant source of information due to the availability of immediate public response. As a result, social media data has become a valuable resource for comprehending public sentiments. Studies have shown that it can amplify ideas and influence public sentiments. This study analyzes the public perception of climate change and the environment over a decade from 2014 to 2023. Using the Pointwise Mutual Information (PMI) algorithm, we identify sentiment and explore prevailing emotions expressed within environmental tweets across various social media platforms, namely Twitter, Reddit, and YouTube. Accuracy on a human-annotated dataset was 0.65, higher than Vader score but lower than that of an expert rater (0.90). Our findings suggest that negative environmental tweets are far more common than positive or neutral ones. Climate change, air quality, emissions, plastic, and recycling are the most discussed topics on all social media platforms, highlighting its huge global concern. The most common emotions in environmental tweets are fear, trust, and anticipation, demonstrating public reactions wide and complex nature. By identifying patterns and trends in opinions related to the environment, we hope to provide insights that can help raise awareness regarding environmental issues, inform the development of interventions, and adapt further actions to meet environmental challenges.","sentences":["Social media is now the predominant source of information due to the availability of immediate public response.","As a result, social media data has become a valuable resource for comprehending public sentiments.","Studies have shown that it can amplify ideas and influence public sentiments.","This study analyzes the public perception of climate change and the environment over a decade from 2014 to 2023.","Using the Pointwise Mutual Information (PMI) algorithm, we identify sentiment and explore prevailing emotions expressed within environmental tweets across various social media platforms, namely Twitter, Reddit, and YouTube.","Accuracy on a human-annotated dataset was 0.65, higher than Vader score but lower than that of an expert rater (0.90).","Our findings suggest that negative environmental tweets are far more common than positive or neutral ones.","Climate change, air quality, emissions, plastic, and recycling are the most discussed topics on all social media platforms, highlighting its huge global concern.","The most common emotions in environmental tweets are fear, trust, and anticipation, demonstrating public reactions wide and complex nature.","By identifying patterns and trends in opinions related to the environment, we hope to provide insights that can help raise awareness regarding environmental issues, inform the development of interventions, and adapt further actions to meet environmental challenges."],"url":"http://arxiv.org/abs/2312.03095v1"}
{"created":"2023-12-05 19:03:33","title":"ScAR: Scaling Adversarial Robustness for LiDAR Object Detection","abstract":"The adversarial robustness of a model is its ability to resist adversarial attacks in the form of small perturbations to input data. Universal adversarial attack methods such as Fast Sign Gradient Method (FSGM) and Projected Gradient Descend (PGD) are popular for LiDAR object detection, but they are often deficient compared to task-specific adversarial attacks. Additionally, these universal methods typically require unrestricted access to the model's information, which is difficult to obtain in real-world applications. To address these limitations, we present a black-box Scaling Adversarial Robustness (ScAR) method for LiDAR object detection. By analyzing the statistical characteristics of 3D object detection datasets such as KITTI, Waymo, and nuScenes, we have found that the model's prediction is sensitive to scaling of 3D instances. We propose three black-box scaling adversarial attack methods based on the available information: model-aware attack, distribution-aware attack, and blind attack. We also introduce a strategy for generating scaling adversarial examples to improve the model's robustness against these three scaling adversarial attacks. Comparison with other methods on public datasets under different 3D object detection architectures demonstrates the effectiveness of our proposed method.","sentences":["The adversarial robustness of a model is its ability to resist adversarial attacks in the form of small perturbations to input data.","Universal adversarial attack methods such as Fast Sign Gradient Method (FSGM) and Projected Gradient Descend (PGD) are popular for LiDAR object detection, but they are often deficient compared to task-specific adversarial attacks.","Additionally, these universal methods typically require unrestricted access to the model's information, which is difficult to obtain in real-world applications.","To address these limitations, we present a black-box Scaling Adversarial Robustness (ScAR) method for LiDAR object detection.","By analyzing the statistical characteristics of 3D object detection datasets such as KITTI, Waymo, and nuScenes, we have found that the model's prediction is sensitive to scaling of 3D instances.","We propose three black-box scaling adversarial attack methods based on the available information: model-aware attack, distribution-aware attack, and blind attack.","We also introduce a strategy for generating scaling adversarial examples to improve the model's robustness against these three scaling adversarial attacks.","Comparison with other methods on public datasets under different 3D object detection architectures demonstrates the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2312.03085v1"}
{"created":"2023-12-05 19:00:18","title":"Clinical Notes Reveal Physician Fatigue","abstract":"Physicians write notes about patients. In doing so, they reveal much about themselves. Using data from 129,228 emergency room visits, we train a model to identify notes written by fatigued physicians -- those who worked 5 or more of the prior 7 days. In a hold-out set, the model accurately identifies notes written by these high-workload physicians, and also flags notes written in other high-fatigue settings: on overnight shifts, and after high patient volumes. Model predictions also correlate with worse decision-making on at least one important metric: yield of testing for heart attack is 18% lower with each standard deviation increase in model-predicted fatigue. Finally, the model indicates that notes written about Black and Hispanic patients have 12% and 21% higher predicted fatigue than Whites -- larger than overnight vs. daytime differences. These results have an important implication for large language models (LLMs). Our model indicates that fatigued doctors write more predictable notes. Perhaps unsurprisingly, because word prediction is the core of how LLMs work, we find that LLM-written notes have 17% higher predicted fatigue than real physicians' notes. This indicates that LLMs may introduce distortions in generated text that are not yet fully understood.","sentences":["Physicians write notes about patients.","In doing so, they reveal much about themselves.","Using data from 129,228 emergency room visits, we train a model to identify notes written by fatigued physicians -- those who worked 5 or more of the prior 7 days.","In a hold-out set, the model accurately identifies notes written by these high-workload physicians, and also flags notes written in other high-fatigue settings: on overnight shifts, and after high patient volumes.","Model predictions also correlate with worse decision-making on at least one important metric: yield of testing for heart attack is 18% lower with each standard deviation increase in model-predicted fatigue.","Finally, the model indicates that notes written about Black and Hispanic patients have 12% and 21% higher predicted fatigue than Whites -- larger than overnight vs. daytime differences.","These results have an important implication for large language models (LLMs).","Our model indicates that fatigued doctors write more predictable notes.","Perhaps unsurprisingly, because word prediction is the core of how LLMs work, we find that LLM-written notes have 17% higher predicted fatigue than real physicians' notes.","This indicates that LLMs may introduce distortions in generated text that are not yet fully understood."],"url":"http://arxiv.org/abs/2312.03077v1"}
{"created":"2023-12-05 18:58:37","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models","abstract":"Solving complex visual tasks such as \"Who invented the musical instrument on the right?\" involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incorrect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Extensive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data.","sentences":["Solving complex visual tasks such as \"Who invented the musical instrument on the right?\" involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge.","Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models.","However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incorrect outputs.","Moreover, they require loading multiple models, incurring high latency and computation costs.","We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass.","VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one.","It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM.","Extensive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally.","Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes.","An evaluation with human annotators also confirms that VPD improves model response factuality and consistency.","Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data."],"url":"http://arxiv.org/abs/2312.03052v1"}
{"created":"2023-12-05 18:42:25","title":"Architectural Approaches to Overcome Challenges in the Development of Data-Intensive Systems","abstract":"Orientation of modern software systems towards data-intensive processing raises new difficulties in software engineering on how to build and maintain such systems. Some of the important challenges concern the design of software architecture. In this article, we survey the fundamental challenges when designing data-intensive computing systems and present some of the most popular software architectural styles together with their potential to tackle these challenges.","sentences":["Orientation of modern software systems towards data-intensive processing raises new difficulties in software engineering on how to build and maintain such systems.","Some of the important challenges concern the design of software architecture.","In this article, we survey the fundamental challenges when designing data-intensive computing systems and present some of the most popular software architectural styles together with their potential to tackle these challenges."],"url":"http://arxiv.org/abs/2312.03049v1"}
{"created":"2023-12-05 18:34:12","title":"DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control","abstract":"Large, pretrained latent diffusion models (LDMs) have demonstrated an extraordinary ability to generate creative content, specialize to user data through few-shot fine-tuning, and condition their output on other modalities, such as semantic maps. However, are they usable as large-scale data generators, e.g., to improve tasks in the perception stack, like semantic segmentation? We investigate this question in the context of autonomous driving, and answer it with a resounding \"yes\". We propose an efficient data generation pipeline termed DGInStyle. First, we examine the problem of specializing a pretrained LDM to semantically-controlled generation within a narrow domain. Second, we design a Multi-resolution Latent Fusion technique to overcome the bias of LDMs towards dominant objects. Third, we propose a Style Swap technique to endow the rich generative prior with the learned semantic control. Using DGInStyle, we generate a diverse dataset of street scenes, train a domain-agnostic semantic segmentation model on it, and evaluate the model on multiple popular autonomous driving datasets. Our approach consistently increases the performance of several domain generalization methods, in some cases by +2.5 mIoU compared to the previous state-of-the-art method without our generative augmentation scheme. Source code and dataset are available at https://dginstyle.github.io .","sentences":["Large, pretrained latent diffusion models (LDMs) have demonstrated an extraordinary ability to generate creative content, specialize to user data through few-shot fine-tuning, and condition their output on other modalities, such as semantic maps.","However, are they usable as large-scale data generators, e.g., to improve tasks in the perception stack, like semantic segmentation?","We investigate this question in the context of autonomous driving, and answer it with a resounding \"yes\".","We propose an efficient data generation pipeline termed DGInStyle.","First, we examine the problem of specializing a pretrained LDM to semantically-controlled generation within a narrow domain.","Second, we design a Multi-resolution Latent Fusion technique to overcome the bias of LDMs towards dominant objects.","Third, we propose a Style Swap technique to endow the rich generative prior with the learned semantic control.","Using DGInStyle, we generate a diverse dataset of street scenes, train a domain-agnostic semantic segmentation model on it, and evaluate the model on multiple popular autonomous driving datasets.","Our approach consistently increases the performance of several domain generalization methods, in some cases by +2.5 mIoU compared to the previous state-of-the-art method without our generative augmentation scheme.","Source code and dataset are available at https://dginstyle.github.io ."],"url":"http://arxiv.org/abs/2312.03048v1"}
