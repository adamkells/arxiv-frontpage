{"created":"2025-02-04 18:59:55","title":"Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling","abstract":"3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate Anymesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate Anymesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system. Our Github website is https://articulate-anymesh.github.io.","sentences":["3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints.","Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context.","To address these limitations, we propose Articulate Anymesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner.","Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints.","Our experiments show that Articulate Anymesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets.","Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system.","Our Github website is https://articulate-anymesh.github.io."],"url":"http://arxiv.org/abs/2502.02590v1"}
{"created":"2025-02-04 18:59:23","title":"Calibrated Multi-Preference Optimization for Aligning Diffusion Models","abstract":"Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets, but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however, current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise preference distribution. Furthermore, they lack generalization to multi-preference scenarios and struggle to handle inconsistencies between rewards. To address this, we present Calibrated Preference Optimization (CaPO), a novel method to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data. The core of our approach involves a reward calibration method to approximate the general preference by computing the expected win-rate against the samples generated by the pretrained models. Additionally, we propose a frontier-based pair selection method that effectively manages the multi-preference distribution by selecting pairs from Pareto frontiers. Finally, we use regression loss to fine-tune diffusion models to match the difference between calibrated rewards of a selected pair. Experimental results show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I benchmarks, including GenEval and T2I-Compbench.","sentences":["Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets, but the heavy cost of manual data collection limits scalability.","Using reward models offers an alternative, however, current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise preference distribution.","Furthermore, they lack generalization to multi-preference scenarios and struggle to handle inconsistencies between rewards.","To address this, we present Calibrated Preference Optimization (CaPO), a novel method to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data.","The core of our approach involves a reward calibration method to approximate the general preference by computing the expected win-rate against the samples generated by the pretrained models.","Additionally, we propose a frontier-based pair selection method that effectively manages the multi-preference distribution by selecting pairs from Pareto frontiers.","Finally, we use regression loss to fine-tune diffusion models to match the difference between calibrated rewards of a selected pair.","Experimental results show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I benchmarks, including GenEval and T2I-Compbench."],"url":"http://arxiv.org/abs/2502.02588v1"}
{"created":"2025-02-04 18:58:31","title":"QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search","abstract":"Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.","sentences":["Language agents have become a promising solution to complex interactive tasks.","One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference.","However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories.","This may lead to sub-optimal policies and hinder the overall performance.","To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents.","By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step.","With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks.","Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision.","We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis.","We will release our code and data."],"url":"http://arxiv.org/abs/2502.02584v1"}
{"created":"2025-02-04 18:56:00","title":"Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism","abstract":"Mixture-of-Experts (MoE) has emerged as a promising sparse paradigm for scaling up pre-trained models (PTMs) with remarkable cost-effectiveness. However, the dynamic nature of MoE leads to rapid fluctuations and imbalances in expert loads during training, resulting in significant straggler effects that hinder training performance when using expert parallelism (EP). Existing MoE training systems attempt to mitigate these effects through expert rearrangement strategies, but they face challenges in terms of memory efficiency and timeliness of rearrangement. This paper proposes Fully Sharded Sparse Data Parallelism (FSSDP), an innovative approach that tackles the parallelization of MoE layers and potential straggler effects caused by imbalanced expert loads from a new perspective. FSSDP fully shards the parameters and optimizer states of MoE layers across devices and sparsely materializes MoE parameters from scratch in each iteration with two sparse collectives SparseAllGather and SparseReduceScatter. We build Hecate, a high-performance MoE training system that incorporates FSSDP to fully unlock its potential. Hecate introduces heterogeneous sharding, sparse materialization, and re-materialization techniques to construct flexible and efficient expert placements with low memory and communication overhead. Our evaluation reveals that Hecate achieves up to 3.54x speedup compared over state-of-the-art MoE training systems and consistently demonstrates improvements across model architectures and hardware environments.","sentences":["Mixture-of-Experts (MoE) has emerged as a promising sparse paradigm for scaling up pre-trained models (PTMs) with remarkable cost-effectiveness.","However, the dynamic nature of MoE leads to rapid fluctuations and imbalances in expert loads during training, resulting in significant straggler effects that hinder training performance when using expert parallelism (EP).","Existing MoE training systems attempt to mitigate these effects through expert rearrangement strategies, but they face challenges in terms of memory efficiency and timeliness of rearrangement.","This paper proposes Fully Sharded Sparse Data Parallelism (FSSDP), an innovative approach that tackles the parallelization of MoE layers and potential straggler effects caused by imbalanced expert loads from a new perspective.","FSSDP fully shards the parameters and optimizer states of MoE layers across devices and sparsely materializes MoE parameters from scratch in each iteration with two sparse collectives SparseAllGather and SparseReduceScatter.","We build Hecate, a high-performance MoE training system that incorporates FSSDP to fully unlock its potential.","Hecate introduces heterogeneous sharding, sparse materialization, and re-materialization techniques to construct flexible and efficient expert placements with low memory and communication overhead.","Our evaluation reveals that Hecate achieves up to 3.54x speedup compared over state-of-the-art MoE training systems and consistently demonstrates improvements across model architectures and hardware environments."],"url":"http://arxiv.org/abs/2502.02581v1"}
{"created":"2025-02-04 18:53:42","title":"A comparison of translation performance between DeepL and Supertext","abstract":"As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context. This study compares two commercial MT systems -- DeepL and Supertext -- by assessing their performance on unsegmented texts. We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts. We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability. We release all evaluation data and scripts for further analysis and reproduction at https://github.com/supertext/evaluation_deepl_supertext.","sentences":["As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context.","This study compares two commercial MT systems -- DeepL and Supertext -- by assessing their performance on unsegmented texts.","We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context.","While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts.","We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability.","We release all evaluation data and scripts for further analysis and reproduction at https://github.com/supertext/evaluation_deepl_supertext."],"url":"http://arxiv.org/abs/2502.02577v1"}
{"created":"2025-02-04 18:47:19","title":"Algorithms and Hardness Results for the $(k,\\ell)$-Cover Problem","abstract":"A connected graph has a $(k,\\ell)$-cover if each of its edges is contained in at least $\\ell$ cliques of order $k$. Motivated by recent advances in extremal combinatorics and the literature on edge modification problems, we study the algorithmic version of the $(k,\\ell)$-cover problem. Given a connected graph $G$, the $(k, \\ell)$-cover problem is to identify the smallest subset of non-edges of $G$ such that their addition to $G$ results in a graph with a $(k, \\ell)$-cover. For every constant $k\\geq3$, we show that the $(k,1)$-cover problem is $\\mathbb{NP}$-complete for general graphs. Moreover, we show that for every constant $k\\geq 3$, the $(k,1)$-cover problem admits no polynomial-time constant-factor approximation algorithm unless $\\mathbb{P}=\\mathbb{NP}$. However, we show that the $(3,1)$-cover problem can be solved in polynomial time when the input graph is chordal. For the class of trees and general values of $k$, we show that the $(k,1)$-cover problem is $\\mathbb{NP}$-hard even for spiders. However, we show that for every $k\\geq4$, the $(3,k-2)$-cover and the $(k,1)$-cover problems are constant-factor approximable when the input graph is a tree.","sentences":["A connected graph has a $(k,\\ell)$-cover if each of its edges is contained in at least $\\ell$ cliques of order $k$. Motivated by recent advances in extremal combinatorics and the literature on edge modification problems, we study the algorithmic version of the $(k,\\ell)$-cover problem.","Given a connected graph $G$, the $(k, \\ell)$-cover problem is to identify the smallest subset of non-edges of $G$ such that their addition to $G$ results in a graph with a $(k, \\ell)$-cover.","For every constant $k\\geq3$, we show that the $(k,1)$-cover problem is $\\mathbb{NP}$-complete for general graphs.","Moreover, we show that for every constant $k\\geq 3$, the $(k,1)$-cover problem admits no polynomial-time constant-factor approximation algorithm unless $\\mathbb{P}=\\mathbb{NP}$. However, we show that the $(3,1)$-cover problem can be solved in polynomial time when the input graph is chordal.","For the class of trees and general values of $k$, we show that the $(k,1)$-cover problem is $\\mathbb{NP}$-hard even for spiders.","However, we show that for every $k\\geq4$, the $(3,k-2)$-cover and the $(k,1)$-cover problems are constant-factor approximable when the input graph is a tree."],"url":"http://arxiv.org/abs/2502.02572v1"}
{"created":"2025-02-04 18:40:38","title":"Fairness in Survival Analysis: A Novel Conditional Mutual Information Augmentation Approach","abstract":"Survival analysis, a vital tool for predicting the time to event, has been used in many domains such as healthcare, criminal justice, and finance. Like classification tasks, survival analysis can exhibit bias against disadvantaged groups, often due to biases inherent in data or algorithms. Several studies in both the IS and CS communities have attempted to address fairness in survival analysis. However, existing methods often overlook the importance of prediction fairness at pre-defined evaluation time points, which is crucial in real-world applications where decision making often hinges on specific time frames. To address this critical research gap, we introduce a new fairness concept: equalized odds (EO) in survival analysis, which emphasizes prediction fairness at pre-defined time points. To achieve the EO fairness in survival analysis, we propose a Conditional Mutual Information Augmentation (CMIA) approach, which features a novel fairness regularization term based on conditional mutual information and an innovative censored data augmentation technique. Our CMIA approach can effectively balance prediction accuracy and fairness, and it is applicable to various survival models. We evaluate the CMIA approach against several state-of-the-art methods within three different application domains, and the results demonstrate that CMIA consistently reduces prediction disparity while maintaining good accuracy and significantly outperforms the other competing methods across multiple datasets and survival models (e.g., linear COX, deep AFT).","sentences":["Survival analysis, a vital tool for predicting the time to event, has been used in many domains such as healthcare, criminal justice, and finance.","Like classification tasks, survival analysis can exhibit bias against disadvantaged groups, often due to biases inherent in data or algorithms.","Several studies in both the IS and CS communities have attempted to address fairness in survival analysis.","However, existing methods often overlook the importance of prediction fairness at pre-defined evaluation time points, which is crucial in real-world applications where decision making often hinges on specific time frames.","To address this critical research gap, we introduce a new fairness concept: equalized odds (EO) in survival analysis, which emphasizes prediction fairness at pre-defined time points.","To achieve the EO fairness in survival analysis, we propose a Conditional Mutual Information Augmentation (CMIA) approach, which features a novel fairness regularization term based on conditional mutual information and an innovative censored data augmentation technique.","Our CMIA approach can effectively balance prediction accuracy and fairness, and it is applicable to various survival models.","We evaluate the CMIA approach against several state-of-the-art methods within three different application domains, and the results demonstrate that CMIA consistently reduces prediction disparity while maintaining good accuracy and significantly outperforms the other competing methods across multiple datasets and survival models (e.g., linear COX, deep AFT)."],"url":"http://arxiv.org/abs/2502.02567v1"}
{"created":"2025-02-04 18:39:10","title":"CReIS: Computation Reuse through Image Similarity in ICN-Based Edge Computing","abstract":"At the edge, there is a high level of similarity in computing. One approach that has been proposed to enhance the efficiency of edge computing is computation reuse, which eliminates redundant computations. Edge computing is integrated with the ICN architecture, capitalizing on its inherent intelligence to facilitate computation reuse and reduce redundancies in computing operations. In many past works, ICN's ability to enable computation reuse through caching has been limited. In this context, a new approach is proposed that considers computation requests with similar input data, which yield identical results, as equivalent. This method facilitates computation reuse through caching in ICN. The use of approximate results to reduce redundant computations without requiring high accuracy in input matching is provided. This concept is termed the Similarity Index, which effectively considers images to be similar despite minor changes in the angle of photography. The Similarity Index is determined through an algorithm known as HNSW and utilizes the SIFT descriptor to identify similar data. This approach helps reduce user latency times by providing quick access to results. The evaluation, simulated using the ndnSIM tool, showed an 86% improvement in completion time compared to scenarios without computation reuse, whereas previous works reported only a 70% improvement. To strengthen this method, an analytical model for computing request transfer considering computation reuse in ICN-based edge computing is provided. To assess the accuracy of the model, several evaluations have been conducted in the simulator by varying the parameters, resulting in a maximum error percentage of approximately 16%.","sentences":["At the edge, there is a high level of similarity in computing.","One approach that has been proposed to enhance the efficiency of edge computing is computation reuse, which eliminates redundant computations.","Edge computing is integrated with the ICN architecture, capitalizing on its inherent intelligence to facilitate computation reuse and reduce redundancies in computing operations.","In many past works, ICN's ability to enable computation reuse through caching has been limited.","In this context, a new approach is proposed that considers computation requests with similar input data, which yield identical results, as equivalent.","This method facilitates computation reuse through caching in ICN.","The use of approximate results to reduce redundant computations without requiring high accuracy in input matching is provided.","This concept is termed the Similarity Index, which effectively considers images to be similar despite minor changes in the angle of photography.","The Similarity Index is determined through an algorithm known as HNSW and utilizes the SIFT descriptor to identify similar data.","This approach helps reduce user latency times by providing quick access to results.","The evaluation, simulated using the ndnSIM tool, showed an 86% improvement in completion time compared to scenarios without computation reuse, whereas previous works reported only a 70% improvement.","To strengthen this method, an analytical model for computing request transfer considering computation reuse in ICN-based edge computing is provided.","To assess the accuracy of the model, several evaluations have been conducted in the simulator by varying the parameters, resulting in a maximum error percentage of approximately 16%."],"url":"http://arxiv.org/abs/2502.02564v1"}
{"created":"2025-02-04 18:37:10","title":"Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents","abstract":"A fundamental question in data-driven decision making is how to quantify the uncertainty of predictions in ways that can usefully inform downstream action. This interface between prediction uncertainty and decision-making is especially important in risk-sensitive domains, such as medicine. In this paper, we develop decision-theoretic foundations that connect uncertainty quantification using prediction sets with risk-averse decision-making. Specifically, we answer three fundamental questions: (1) What is the correct notion of uncertainty quantification for risk-averse decision makers? We prove that prediction sets are optimal for decision makers who wish to optimize their value at risk. (2) What is the optimal policy that a risk averse decision maker should use to map prediction sets to actions? We show that a simple max-min decision policy is optimal for risk-averse decision makers. Finally, (3) How can we derive prediction sets that are optimal for such decision makers? We provide an exact characterization in the population regime and a distribution free finite-sample construction. Answering these questions naturally leads to an algorithm, Risk-Averse Calibration (RAC), which follows a provably optimal design for deriving action policies from predictions. RAC is designed to be both practical-capable of leveraging the quality of predictions in a black-box manner to enhance downstream utility-and safe-adhering to a user-defined risk threshold and optimizing the corresponding risk quantile of the user's downstream utility. Finally, we experimentally demonstrate the significant advantages of RAC in applications such as medical diagnosis and recommendation systems. Specifically, we show that RAC achieves a substantially improved trade-off between safety and utility, offering higher utility compared to existing methods while maintaining the safety guarantee.","sentences":["A fundamental question in data-driven decision making is how to quantify the uncertainty of predictions in ways that can usefully inform downstream action.","This interface between prediction uncertainty and decision-making is especially important in risk-sensitive domains, such as medicine.","In this paper, we develop decision-theoretic foundations that connect uncertainty quantification using prediction sets with risk-averse decision-making.","Specifically, we answer three fundamental questions: (1) What is the correct notion of uncertainty quantification for risk-averse decision makers?","We prove that prediction sets are optimal for decision makers who wish to optimize their value at risk.","(2) What is the optimal policy that a risk averse decision maker should use to map prediction sets to actions?","We show that a simple max-min decision policy is optimal for risk-averse decision makers.","Finally, (3) How can we derive prediction sets that are optimal for such decision makers?","We provide an exact characterization in the population regime and a distribution free finite-sample construction.","Answering these questions naturally leads to an algorithm, Risk-Averse Calibration (RAC), which follows a provably optimal design for deriving action policies from predictions.","RAC is designed to be both practical-capable of leveraging the quality of predictions in a black-box manner to enhance downstream utility-and safe-adhering to a user-defined risk threshold and optimizing the corresponding risk quantile of the user's downstream utility.","Finally, we experimentally demonstrate the significant advantages of RAC in applications such as medical diagnosis and recommendation systems.","Specifically, we show that RAC achieves a substantially improved trade-off between safety and utility, offering higher utility compared to existing methods while maintaining the safety guarantee."],"url":"http://arxiv.org/abs/2502.02561v1"}
{"created":"2025-02-04 18:23:22","title":"Hierarchical Sparse Bayesian Multitask Model with Scalable Inference for Microbiome Analysis","abstract":"This paper proposes a hierarchical Bayesian multitask learning model that is applicable to the general multi-task binary classification learning problem where the model assumes a shared sparsity structure across different tasks. We derive a computationally efficient inference algorithm based on variational inference to approximate the posterior distribution. We demonstrate the potential of the new approach on various synthetic datasets and for predicting human health status based on microbiome profile. Our analysis incorporates data pooled from multiple microbiome studies, along with a comprehensive comparison with other benchmark methods. Results in synthetic datasets show that the proposed approach has superior support recovery property when the underlying regression coefficients share a common sparsity structure across different tasks. Our experiments on microbiome classification demonstrate the utility of the method in extracting informative taxa while providing well-calibrated predictions with uncertainty quantification and achieving competitive performance in terms of prediction metrics. Notably, despite the heterogeneity of the pooled datasets (e.g., different experimental objectives, laboratory setups, sequencing equipment, patient demographics), our method delivers robust results.","sentences":["This paper proposes a hierarchical Bayesian multitask learning model that is applicable to the general multi-task binary classification learning problem where the model assumes a shared sparsity structure across different tasks.","We derive a computationally efficient inference algorithm based on variational inference to approximate the posterior distribution.","We demonstrate the potential of the new approach on various synthetic datasets and for predicting human health status based on microbiome profile.","Our analysis incorporates data pooled from multiple microbiome studies, along with a comprehensive comparison with other benchmark methods.","Results in synthetic datasets show that the proposed approach has superior support recovery property when the underlying regression coefficients share a common sparsity structure across different tasks.","Our experiments on microbiome classification demonstrate the utility of the method in extracting informative taxa while providing well-calibrated predictions with uncertainty quantification and achieving competitive performance in terms of prediction metrics.","Notably, despite the heterogeneity of the pooled datasets (e.g., different experimental objectives, laboratory setups, sequencing equipment, patient demographics), our method delivers robust results."],"url":"http://arxiv.org/abs/2502.02552v1"}
{"created":"2025-02-04 18:18:50","title":"Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation","abstract":"We tackle open-vocabulary 3D scene understanding by introducing a novel data generation pipeline and training framework. Our method addresses three critical requirements for effective training: precise 3D region segmentation, comprehensive textual descriptions, and sufficient dataset scale. By leveraging state-of-the-art open-vocabulary image segmentation models and region-aware Vision-Language Models, we develop an automatic pipeline that generates high-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scene datasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with 5.6M mask-text pairs, significantly larger than existing datasets. Building upon this data, we propose Mosaic3D, a foundation model combining a 3D encoder trained with contrastive learning and a lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation. Our approach achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of our large-scale training data.","sentences":["We tackle open-vocabulary 3D scene understanding by introducing a novel data generation pipeline and training framework.","Our method addresses three critical requirements for effective training: precise 3D region segmentation, comprehensive textual descriptions, and sufficient dataset scale.","By leveraging state-of-the-art open-vocabulary image segmentation models and region-aware Vision-Language Models, we develop an automatic pipeline that generates high-quality 3D mask-text pairs.","Applying this pipeline to multiple 3D scene datasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with 5.6M mask-text pairs, significantly larger than existing datasets.","Building upon this data, we propose Mosaic3D, a foundation model combining a 3D encoder trained with contrastive learning and a lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation.","Our approach achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of our large-scale training data."],"url":"http://arxiv.org/abs/2502.02548v1"}
{"created":"2025-02-04 18:14:27","title":"Addressing Label Shift in Distributed Learning via Entropy Regularization","abstract":"We address the challenge of minimizing true risk in multi-node distributed learning. These systems are frequently exposed to both inter-node and intra-node label shifts, which present a critical obstacle to effectively optimizing model performance while ensuring that data remains confined to each node. To tackle this, we propose the Versatile Robust Label Shift (VRLS) method, which enhances the maximum likelihood estimation of the test-to-train label density ratio. VRLS incorporates Shannon entropy-based regularization and adjusts the density ratio during training to better handle label shifts at the test time. In multi-node learning environments, VRLS further extends its capabilities by learning and adapting density ratios across nodes, effectively mitigating label shifts and improving overall model performance. Experiments conducted on MNIST, Fashion MNIST, and CIFAR-10 demonstrate the effectiveness of VRLS, outperforming baselines by up to 20% in imbalanced settings. These results highlight the significant improvements VRLS offers in addressing label shifts. Our theoretical analysis further supports this by establishing high-probability bounds on estimation errors.","sentences":["We address the challenge of minimizing true risk in multi-node distributed learning.","These systems are frequently exposed to both inter-node and intra-node label shifts, which present a critical obstacle to effectively optimizing model performance while ensuring that data remains confined to each node.","To tackle this, we propose the Versatile Robust Label Shift (VRLS) method, which enhances the maximum likelihood estimation of the test-to-train label density ratio.","VRLS incorporates Shannon entropy-based regularization and adjusts the density ratio during training to better handle label shifts at the test time.","In multi-node learning environments, VRLS further extends its capabilities by learning and adapting density ratios across nodes, effectively mitigating label shifts and improving overall model performance.","Experiments conducted on MNIST, Fashion MNIST, and CIFAR-10 demonstrate the effectiveness of VRLS, outperforming baselines by up to 20% in imbalanced settings.","These results highlight the significant improvements VRLS offers in addressing label shifts.","Our theoretical analysis further supports this by establishing high-probability bounds on estimation errors."],"url":"http://arxiv.org/abs/2502.02544v1"}
{"created":"2025-02-04 18:13:50","title":"Posted Price Mechanisms for Online Allocation with Diseconomies of Scale","abstract":"This paper addresses the online $k$-selection problem with diseconomies of scale (OSDoS), where a seller seeks to maximize social welfare by optimally pricing items for sequentially arriving buyers, accounting for increasing marginal production costs. Previous studies have investigated deterministic dynamic pricing mechanisms for such settings. However, significant challenges remain, particularly in achieving optimality with small or finite inventories and developing effective randomized posted price mechanisms. To bridge this gap, we propose a novel randomized dynamic pricing mechanism for OSDoS, providing a tighter lower bound on the competitive ratio compared to prior work. Our approach ensures optimal performance in small inventory settings (i.e., when $k$ is small) and surpasses existing online mechanisms in large inventory settings (i.e., when $k$ is large), leading to the best-known posted price mechanism for optimizing online selection and allocation with diseconomies of scale across varying inventory sizes.","sentences":["This paper addresses the online $k$-selection problem with diseconomies of scale (OSDoS), where a seller seeks to maximize social welfare by optimally pricing items for sequentially arriving buyers, accounting for increasing marginal production costs.","Previous studies have investigated deterministic dynamic pricing mechanisms for such settings.","However, significant challenges remain, particularly in achieving optimality with small or finite inventories and developing effective randomized posted price mechanisms.","To bridge this gap, we propose a novel randomized dynamic pricing mechanism for OSDoS, providing a tighter lower bound on the competitive ratio compared to prior work.","Our approach ensures optimal performance in small inventory settings (i.e., when $k$ is small) and surpasses existing online mechanisms in large inventory settings (i.e., when $k$ is large), leading to the best-known posted price mechanism for optimizing online selection and allocation with diseconomies of scale across varying inventory sizes."],"url":"http://arxiv.org/abs/2502.02543v1"}
{"created":"2025-02-04 18:04:05","title":"Flow Q-Learning","abstract":"We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data. Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process. We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values. This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity. We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL. Project page: https://seohong.me/projects/fql/","sentences":["We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data.","Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process.","We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values.","This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity.","We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL.","Project page: https://seohong.me/projects/fql/"],"url":"http://arxiv.org/abs/2502.02538v1"}
{"created":"2025-02-04 18:03:32","title":"Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks","abstract":"Collaborative Object Detection (COD) and collaborative perception can integrate data or features from various entities, and improve object detection accuracy compared with individual perception. However, adversarial attacks pose a potential threat to the deep learning COD models, and introduce high output uncertainty. With unknown attack models, it becomes even more challenging to improve COD resiliency and quantify the output uncertainty for highly dynamic perception scenes such as autonomous vehicles. In this study, we propose the Trusted Uncertainty Quantification in Collaborative Perception framework (TUQCP). TUQCP leverages both adversarial training and uncertainty quantification techniques to enhance the adversarial robustness of existing COD models. More specifically, TUQCP first adds perturbations to the shared information of randomly selected agents during object detection collaboration by adversarial training. TUQCP then alleviates the impacts of adversarial attacks by providing output uncertainty estimation through learning-based module and uncertainty calibration through conformal prediction. Our framework works for early and intermediate collaboration COD models and single-agent object detection models. We evaluate TUQCP on V2X-Sim, a comprehensive collaborative perception dataset for autonomous driving, and demonstrate a 80.41% improvement in object detection accuracy compared to the baselines under the same adversarial attacks. TUQCP demonstrates the importance of uncertainty quantification to COD under adversarial attacks.","sentences":["Collaborative Object Detection (COD) and collaborative perception can integrate data or features from various entities, and improve object detection accuracy compared with individual perception.","However, adversarial attacks pose a potential threat to the deep learning COD models, and introduce high output uncertainty.","With unknown attack models, it becomes even more challenging to improve COD resiliency and quantify the output uncertainty for highly dynamic perception scenes such as autonomous vehicles.","In this study, we propose the Trusted Uncertainty Quantification in Collaborative Perception framework (TUQCP).","TUQCP leverages both adversarial training and uncertainty quantification techniques to enhance the adversarial robustness of existing COD models.","More specifically, TUQCP first adds perturbations to the shared information of randomly selected agents during object detection collaboration by adversarial training.","TUQCP then alleviates the impacts of adversarial attacks by providing output uncertainty estimation through learning-based module and uncertainty calibration through conformal prediction.","Our framework works for early and intermediate collaboration COD models and single-agent object detection models.","We evaluate TUQCP on V2X-Sim, a comprehensive collaborative perception dataset for autonomous driving, and demonstrate a 80.41% improvement in object detection accuracy compared to the baselines under the same adversarial attacks.","TUQCP demonstrates the importance of uncertainty quantification to COD under adversarial attacks."],"url":"http://arxiv.org/abs/2502.02537v1"}
{"created":"2025-02-04 17:57:17","title":"Adaptive Self-improvement LLM Agentic System for ML Library Development","abstract":"ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\\times$ over a baseline single LLM.","sentences":["ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems.","However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL.","Large language models (LLMs), on the other hand, have shown general coding capabilities.","However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs.","Therefore, LLMs need complex reasoning with limited data in order to complete this task.","To address these challenges, we introduce an adaptive self-improvement agentic system.","In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark.","Our results show improvements of up to $3.9\\times$ over a baseline single LLM."],"url":"http://arxiv.org/abs/2502.02534v1"}
{"created":"2025-02-04 17:50:55","title":"Max-Min Diversification with Asymmetric Distances","abstract":"One of the most well-known and simplest models for diversity maximization is the Max-Min Diversification (MMD) model, which has been extensively studied in the data mining and database literature. In this paper, we initiate the study of the Asymmetric Max-Min Diversification (AMMD) problem. The input is a positive integer $k$ and a complete digraph over $n$ vertices, together with a nonnegative distance function over the edges obeying the directed triangle inequality. The objective is to select a set of $k$ vertices, which maximizes the smallest pairwise distance between them. AMMD reduces to the well-studied MMD problem in case the distances are symmetric, and has natural applications to query result diversification, web search, and facility location problems. Although the MMD problem admits a simple $\\frac{1}{2}$-approximation by greedily selecting the next-furthest point, this strategy fails for AMMD and it remained unclear how to design good approximation algorithms for AMMD. We propose a combinatorial $\\frac{1}{6k}$-approximation algorithm for AMMD by leveraging connections with the Maximum Antichain problem. We discuss several ways of speeding up the algorithm and compare its performance against heuristic baselines on real-life and synthetic datasets.","sentences":["One of the most well-known and simplest models for diversity maximization is the Max-Min Diversification (MMD) model, which has been extensively studied in the data mining and database literature.","In this paper, we initiate the study of the Asymmetric Max-Min Diversification (AMMD) problem.","The input is a positive integer $k$ and a complete digraph over $n$ vertices, together with a nonnegative distance function over the edges obeying the directed triangle inequality.","The objective is to select a set of $k$ vertices, which maximizes the smallest pairwise distance between them.","AMMD reduces to the well-studied MMD problem in case the distances are symmetric, and has natural applications to query result diversification, web search, and facility location problems.","Although the MMD problem admits a simple $\\frac{1}{2}$-approximation by greedily selecting the next-furthest point, this strategy fails for AMMD and it remained unclear how to design good approximation algorithms for AMMD.","We propose a combinatorial $\\frac{1}{6k}$-approximation algorithm for AMMD by leveraging connections with the Maximum Antichain problem.","We discuss several ways of speeding up the algorithm and compare its performance against heuristic baselines on real-life and synthetic datasets."],"url":"http://arxiv.org/abs/2502.02530v1"}
{"created":"2025-02-04 17:50:55","title":"Deep Linear Network Training Dynamics from Random Initialization: Data, Width, Depth, and Hyperparameter Transfer","abstract":"We theoretically characterize gradient descent dynamics in deep linear networks trained at large width from random initialization and on large quantities of random data. Our theory captures the ``wider is better\" effect of mean-field/maximum-update parameterized networks as well as hyperparameter transfer effects, which can be contrasted with the neural-tangent parameterization where optimal learning rates shift with model width. We provide asymptotic descriptions of both non-residual and residual neural networks, the latter of which enables an infinite depth limit when branches are scaled as $1/\\sqrt{\\text{depth}}$. We also compare training with one-pass stochastic gradient descent to the dynamics when training data are repeated at each iteration. Lastly, we show that this model recovers the accelerated power law training dynamics for power law structured data in the rich regime observed in recent works.","sentences":["We theoretically characterize gradient descent dynamics in deep linear networks trained at large width from random initialization and on large quantities of random data.","Our theory captures the ``wider is better\" effect of mean-field/maximum-update parameterized networks as well as hyperparameter transfer effects, which can be contrasted with the neural-tangent parameterization where optimal learning rates shift with model width.","We provide asymptotic descriptions of both non-residual and residual neural networks, the latter of which enables an infinite depth limit when branches are scaled as $1/\\sqrt{\\text{depth}}$. We also compare training with one-pass stochastic gradient descent to the dynamics when training data are repeated at each iteration.","Lastly, we show that this model recovers the accelerated power law training dynamics for power law structured data in the rich regime observed in recent works."],"url":"http://arxiv.org/abs/2502.02531v1"}
{"created":"2025-02-04 17:49:44","title":"TabPFN Unleashed: A Scalable and Effective Solution to Tabular Classification Problems","abstract":"TabPFN has emerged as a promising in-context learning model for tabular data, capable of directly predicting the labels of test samples given labeled training examples. It has demonstrated competitive performance, particularly on small-scale classification tasks. However, despite its effectiveness, TabPFN still requires further refinement in several areas, including handling high-dimensional features, aligning with downstream datasets, and scaling to larger datasets. In this paper, we revisit existing variants of TabPFN and observe that most approaches focus either on reducing bias or variance, often neglecting the need to address the other side, while also increasing inference overhead. To fill this gap, we propose Beta (Bagging and Encoder-based Fine-tuning for TabPFN Adaptation), a novel and effective method designed to minimize both bias and variance. To reduce bias, we introduce a lightweight encoder to better align downstream tasks with the pre-trained TabPFN. By increasing the number of encoders in a lightweight manner, Beta mitigate variance, thereby further improving the model's performance. Additionally, bootstrapped sampling is employed to further reduce the impact of data perturbations on the model, all while maintaining computational efficiency during inference. Our approach enhances TabPFN's ability to handle high-dimensional data and scale to larger datasets. Experimental results on over 200 benchmark classification datasets demonstrate that Beta either outperforms or matches state-of-the-art methods.","sentences":["TabPFN has emerged as a promising in-context learning model for tabular data, capable of directly predicting the labels of test samples given labeled training examples.","It has demonstrated competitive performance, particularly on small-scale classification tasks.","However, despite its effectiveness, TabPFN still requires further refinement in several areas, including handling high-dimensional features, aligning with downstream datasets, and scaling to larger datasets.","In this paper, we revisit existing variants of TabPFN and observe that most approaches focus either on reducing bias or variance, often neglecting the need to address the other side, while also increasing inference overhead.","To fill this gap, we propose Beta (Bagging and Encoder-based Fine-tuning for TabPFN Adaptation), a novel and effective method designed to minimize both bias and variance.","To reduce bias, we introduce a lightweight encoder to better align downstream tasks with the pre-trained TabPFN.","By increasing the number of encoders in a lightweight manner, Beta mitigate variance, thereby further improving the model's performance.","Additionally, bootstrapped sampling is employed to further reduce the impact of data perturbations on the model, all while maintaining computational efficiency during inference.","Our approach enhances TabPFN's ability to handle high-dimensional data and scale to larger datasets.","Experimental results on over 200 benchmark classification datasets demonstrate that Beta either outperforms or matches state-of-the-art methods."],"url":"http://arxiv.org/abs/2502.02527v1"}
{"created":"2025-02-04 17:46:34","title":"Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation","abstract":"Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucial for enabling augmented reality and robotic manipulation. Category-level methods have received extensive research attention due to their potential for generalization to intra-class unknown objects. However, these methods require manual collection and labeling of large-scale real-world training data. To address this problem, we introduce a diffusion-based paradigm for domain-generalized category-level 9-DoF object pose estimation. Our motivation is to leverage the latent generalization ability of the diffusion model to address the domain generalization challenge in object pose estimation. This entails training the model exclusively on rendered synthetic data to achieve generalization to real-world scenes. We propose an effective diffusion model to redefine 9-DoF object pose estimation from a generative perspective. Our model does not require any 3D shape priors during training or inference. By employing the Denoising Diffusion Implicit Model, we demonstrate that the reverse diffusion process can be executed in as few as 3 steps, achieving near real-time performance. Finally, we design a robotic grasping system comprising both hardware and software components. Through comprehensive experiments on two benchmark datasets and the real-world robotic system, we show that our method achieves state-of-the-art domain generalization performance. Our code will be made public at https://github.com/CNJianLiu/Diff9D.","sentences":["Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucial for enabling augmented reality and robotic manipulation.","Category-level methods have received extensive research attention due to their potential for generalization to intra-class unknown objects.","However, these methods require manual collection and labeling of large-scale real-world training data.","To address this problem, we introduce a diffusion-based paradigm for domain-generalized category-level 9-DoF object pose estimation.","Our motivation is to leverage the latent generalization ability of the diffusion model to address the domain generalization challenge in object pose estimation.","This entails training the model exclusively on rendered synthetic data to achieve generalization to real-world scenes.","We propose an effective diffusion model to redefine 9-DoF object pose estimation from a generative perspective.","Our model does not require any 3D shape priors during training or inference.","By employing the Denoising Diffusion Implicit Model, we demonstrate that the reverse diffusion process can be executed in as few as 3 steps, achieving near real-time performance.","Finally, we design a robotic grasping system comprising both hardware and software components.","Through comprehensive experiments on two benchmark datasets and the real-world robotic system, we show that our method achieves state-of-the-art domain generalization performance.","Our code will be made public at https://github.com/CNJianLiu/Diff9D."],"url":"http://arxiv.org/abs/2502.02525v1"}
{"created":"2025-02-04 17:32:17","title":"Generative Modeling on Lie Groups via Euclidean Generalized Score Matching","abstract":"We extend Euclidean score-based diffusion processes to generative modeling on Lie groups. Through the formalism of Generalized Score Matching, our approach yields a Langevin dynamics which decomposes as a direct sum of Lie algebra representations, enabling generative processes on Lie groups while operating in Euclidean space. Unlike equivariant models, which restrict the space of learnable functions by quotienting out group orbits, our method can model any target distribution on any (non-Abelian) Lie group. Standard score matching emerges as a special case of our framework when the Lie group is the translation group. We prove that our generalized generative processes arise as solutions to a new class of paired stochastic differential equations (SDEs), introduced here for the first time. We validate our approach through experiments on diverse data types, demonstrating its effectiveness in real-world applications such as SO(3)-guided molecular conformer generation and modeling ligand-specific global SE(3) transformations for molecular docking, showing improvement in comparison to Riemannian diffusion on the group itself. We show that an appropriate choice of Lie group enhances learning efficiency by reducing the effective dimensionality of the trajectory space and enables the modeling of transitions between complex data distributions. Additionally, we demonstrate the universality of our approach by deriving how it extends to flow matching.","sentences":["We extend Euclidean score-based diffusion processes to generative modeling on Lie groups.","Through the formalism of Generalized Score Matching, our approach yields a Langevin dynamics which decomposes as a direct sum of Lie algebra representations, enabling generative processes on Lie groups while operating in Euclidean space.","Unlike equivariant models, which restrict the space of learnable functions by quotienting out group orbits, our method can model any target distribution on any (non-Abelian) Lie group.","Standard score matching emerges as a special case of our framework when the Lie group is the translation group.","We prove that our generalized generative processes arise as solutions to a new class of paired stochastic differential equations (SDEs), introduced here for the first time.","We validate our approach through experiments on diverse data types, demonstrating its effectiveness in real-world applications such as SO(3)-guided molecular conformer generation and modeling ligand-specific global SE(3) transformations for molecular docking, showing improvement in comparison to Riemannian diffusion on the group itself.","We show that an appropriate choice of Lie group enhances learning efficiency by reducing the effective dimensionality of the trajectory space and enables the modeling of transitions between complex data distributions.","Additionally, we demonstrate the universality of our approach by deriving how it extends to flow matching."],"url":"http://arxiv.org/abs/2502.02513v1"}
{"created":"2025-02-04 17:26:58","title":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","abstract":"Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.","sentences":["Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains.","Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities.","This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system.","Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks.","Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM?","This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies).","To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning.","Our approach results in Satori, a 7B LLM trained on open-source models and data.","Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks.","Code, data, and models will be fully open-sourced."],"url":"http://arxiv.org/abs/2502.02508v1"}
{"created":"2025-02-04 17:18:54","title":"Unified Spatial-Temporal Edge-Enhanced Graph Networks for Pedestrian Trajectory Prediction","abstract":"Pedestrian trajectory prediction aims to forecast future movements based on historical paths. Spatial-temporal (ST) methods often separately model spatial interactions among pedestrians and temporal dependencies of individuals. They overlook the direct impacts of interactions among different pedestrians across various time steps (i.e., high-order cross-time interactions). This limits their ability to capture ST inter-dependencies and hinders prediction performance. To address these limitations, we propose UniEdge with three major designs. Firstly, we introduce a unified ST graph data structure that simplifies high-order cross-time interactions into first-order relationships, enabling the learning of ST inter-dependencies in a single step. This avoids the information loss caused by multi-step aggregation. Secondly, traditional GNNs focus on aggregating pedestrian node features, neglecting the propagation of implicit interaction patterns encoded in edge features. We propose the Edge-to-Edge-Node-to-Node Graph Convolution (E2E-N2N-GCN), a novel dual-graph network that jointly models explicit N2N social interactions among pedestrians and implicit E2E influence propagation across these interaction patterns. Finally, to overcome the limited receptive fields and challenges in capturing long-range dependencies of auto-regressive architectures, we introduce a transformer encoder-based predictor that enables global modeling of temporal correlation. UniEdge outperforms state-of-the-arts on multiple datasets, including ETH, UCY, and SDD.","sentences":["Pedestrian trajectory prediction aims to forecast future movements based on historical paths.","Spatial-temporal (ST) methods often separately model spatial interactions among pedestrians and temporal dependencies of individuals.","They overlook the direct impacts of interactions among different pedestrians across various time steps (i.e., high-order cross-time interactions).","This limits their ability to capture ST inter-dependencies and hinders prediction performance.","To address these limitations, we propose UniEdge with three major designs.","Firstly, we introduce a unified ST graph data structure that simplifies high-order cross-time interactions into first-order relationships, enabling the learning of ST inter-dependencies in a single step.","This avoids the information loss caused by multi-step aggregation.","Secondly, traditional GNNs focus on aggregating pedestrian node features, neglecting the propagation of implicit interaction patterns encoded in edge features.","We propose the Edge-to-Edge-Node-to-Node Graph Convolution (E2E-N2N-GCN), a novel dual-graph network that jointly models explicit N2N social interactions among pedestrians and implicit E2E influence propagation across these interaction patterns.","Finally, to overcome the limited receptive fields and challenges in capturing long-range dependencies of auto-regressive architectures, we introduce a transformer encoder-based predictor that enables global modeling of temporal correlation.","UniEdge outperforms state-of-the-arts on multiple datasets, including ETH, UCY, and SDD."],"url":"http://arxiv.org/abs/2502.02504v1"}
{"created":"2025-02-04 17:12:23","title":"The Causal-Effect Score in Data Management","abstract":"The Causal Effect (CE) is a numerical measure of causal influence of variables on observed results. Despite being widely used in many areas, only preliminary attempts have been made to use CE as an attribution score in data management, to measure the causal strength of tuples for query answering in databases. In this work, we introduce, generalize and investigate the so-called Causal-Effect Score in the context of classical and probabilistic databases.","sentences":["The Causal Effect (CE) is a numerical measure of causal influence of variables on observed results.","Despite being widely used in many areas, only preliminary attempts have been made to use CE as an attribution score in data management, to measure the causal strength of tuples for query answering in databases.","In this work, we introduce, generalize and investigate the so-called Causal-Effect Score in the context of classical and probabilistic databases."],"url":"http://arxiv.org/abs/2502.02495v1"}
{"created":"2025-02-04 17:09:44","title":"Analyzing Similarity Metrics for Data Selection for Language Model Pretraining","abstract":"Similarity between training examples is used to curate pretraining datasets for language models by many methods -- for diversification and to select examples similar to high-quality data. However, similarity is typically measured with off-the-shelf embedding models that are generic or trained for tasks such as retrieval. This paper introduces a framework to analyze the suitability of embedding models specifically for data curation in the language model pretraining setting. We quantify the correlation between similarity in the embedding space to similarity in pretraining loss between different training examples, and how diversifying in the embedding space affects pretraining quality. We analyze a variety of embedding models in our framework, with experiments using the Pile dataset for pretraining a 1.7B parameter decoder-only language model. We find that the embedding models we consider are all useful for pretraining data curation. Moreover, a simple approach of averaging per-token embeddings proves to be surprisingly competitive with more sophisticated embedding models -- likely because the latter are not designed specifically for pretraining data curation. Indeed, we believe our analysis and evaluation framework can serve as a foundation for the design of embedding models that specifically reason about similarity in pretraining datasets.","sentences":["Similarity between training examples is used to curate pretraining datasets for language models by many methods -- for diversification and to select examples similar to high-quality data.","However, similarity is typically measured with off-the-shelf embedding models that are generic or trained for tasks such as retrieval.","This paper introduces a framework to analyze the suitability of embedding models specifically for data curation in the language model pretraining setting.","We quantify the correlation between similarity in the embedding space to similarity in pretraining loss between different training examples, and how diversifying in the embedding space affects pretraining quality.","We analyze a variety of embedding models in our framework, with experiments using the Pile dataset for pretraining a 1.7B parameter decoder-only language model.","We find that the embedding models we consider are all useful for pretraining data curation.","Moreover, a simple approach of averaging per-token embeddings proves to be surprisingly competitive with more sophisticated embedding models -- likely because the latter are not designed specifically for pretraining data curation.","Indeed, we believe our analysis and evaluation framework can serve as a foundation for the design of embedding models that specifically reason about similarity in pretraining datasets."],"url":"http://arxiv.org/abs/2502.02494v1"}
{"created":"2025-02-04 17:07:10","title":"VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models","abstract":"Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/","sentences":["Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics.","We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence.","To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation.","VideoJAM is composed of two complementary units.","During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation.","During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal.","Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model.","VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations.","These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation.","Project website: https://hila-chefer.github.io/videojam-paper.github.io/"],"url":"http://arxiv.org/abs/2502.02492v1"}
{"created":"2025-02-04 17:06:41","title":"A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation","abstract":"Ultrasound (US) imaging is clinically invaluable due to its noninvasive and safe nature. However, interpreting US images is challenging, requires significant expertise, and time, and is often prone to errors. Deep learning offers assistive solutions such as segmentation. Supervised methods rely on large, high-quality, and consistently labeled datasets, which are challenging to curate. Moreover, these methods tend to underperform on out-of-distribution data, limiting their clinical utility. Self-supervised learning (SSL) has emerged as a promising alternative, leveraging unlabeled data to enhance model performance and generalisability. We introduce a contrastive SSL approach tailored for B-mode US images, incorporating a novel Relation Contrastive Loss (RCL). RCL encourages learning of distinct features by differentiating positive and negative sample pairs through a learnable metric. Additionally, we propose spatial and frequency-based augmentation strategies for the representation learning on US images. Our approach significantly outperforms traditional supervised segmentation methods across three public breast US datasets, particularly in data-limited scenarios. Notable improvements on the Dice similarity metric include a 4% increase on 20% and 50% of the BUSI dataset, nearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4% and 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively. Furthermore, we demonstrate superior generalisability on the out-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6% compared to the supervised baseline using 20% and 50% of the BUSI and BrEaST training data, respectively. Our research highlights that domain-inspired SSL can improve US segmentation, especially under data-limited conditions.","sentences":["Ultrasound (US) imaging is clinically invaluable due to its noninvasive and safe nature.","However, interpreting US images is challenging, requires significant expertise, and time, and is often prone to errors.","Deep learning offers assistive solutions such as segmentation.","Supervised methods rely on large, high-quality, and consistently labeled datasets, which are challenging to curate.","Moreover, these methods tend to underperform on out-of-distribution data, limiting their clinical utility.","Self-supervised learning (SSL) has emerged as a promising alternative, leveraging unlabeled data to enhance model performance and generalisability.","We introduce a contrastive SSL approach tailored for B-mode US images, incorporating a novel Relation Contrastive Loss (RCL).","RCL encourages learning of distinct features by differentiating positive and negative sample pairs through a learnable metric.","Additionally, we propose spatial and frequency-based augmentation strategies for the representation learning on US images.","Our approach significantly outperforms traditional supervised segmentation methods across three public breast US datasets, particularly in data-limited scenarios.","Notable improvements on the Dice similarity metric include a 4% increase on 20% and 50% of the BUSI dataset, nearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4% and 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively.","Furthermore, we demonstrate superior generalisability on the out-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6% compared to the supervised baseline using 20% and 50% of the BUSI and BrEaST training data, respectively.","Our research highlights that domain-inspired SSL can improve US segmentation, especially under data-limited conditions."],"url":"http://arxiv.org/abs/2502.02489v1"}
{"created":"2025-02-04 17:04:16","title":"Do Graph Diffusion Models Accurately Capture and Generate Substructure Distributions?","abstract":"Diffusion models have gained popularity in graph generation tasks; however, the extent of their expressivity concerning the graph distributions they can learn is not fully understood. Unlike models in other domains, popular backbones for graph diffusion models, such as Graph Transformers, do not possess universal expressivity to accurately model the distribution scores of complex graph data. Our work addresses this limitation by focusing on the frequency of specific substructures as a key characteristic of target graph distributions. When evaluating existing models using this metric, we find that they fail to maintain the distribution of substructure counts observed in the training set when generating new graphs. To address this issue, we establish a theoretical connection between the expressivity of Graph Neural Networks (GNNs) and the overall performance of graph diffusion models, demonstrating that more expressive GNN backbones can better capture complex distribution patterns. By integrating advanced GNNs into the backbone architecture, we achieve significant improvements in substructure generation.","sentences":["Diffusion models have gained popularity in graph generation tasks; however, the extent of their expressivity concerning the graph distributions they can learn is not fully understood.","Unlike models in other domains, popular backbones for graph diffusion models, such as Graph Transformers, do not possess universal expressivity to accurately model the distribution scores of complex graph data.","Our work addresses this limitation by focusing on the frequency of specific substructures as a key characteristic of target graph distributions.","When evaluating existing models using this metric, we find that they fail to maintain the distribution of substructure counts observed in the training set when generating new graphs.","To address this issue, we establish a theoretical connection between the expressivity of Graph Neural Networks (GNNs) and the overall performance of graph diffusion models, demonstrating that more expressive GNN backbones can better capture complex distribution patterns.","By integrating advanced GNNs into the backbone architecture, we achieve significant improvements in substructure generation."],"url":"http://arxiv.org/abs/2502.02488v1"}
{"created":"2025-02-04 16:59:03","title":"Distributional Diffusion Models with Scoring Rules","abstract":"Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to data until fully corrupted. The corresponding reverse process progressively \"denoises\" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to accomplish sample generation by learning the posterior {\\em distribution} of clean data samples given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is accomplished by replacing the standard regression loss used to estimate conditional means with a scoring rule. We validate our method on image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps.","sentences":["Diffusion models generate high-quality synthetic data.","They operate by defining a continuous-time forward process which gradually adds Gaussian noise to data until fully corrupted.","The corresponding reverse process progressively \"denoises\" a Gaussian sample into a sample from the data distribution.","However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process.","This is expensive and has motivated the development of many acceleration methods.","We propose to accomplish sample generation by learning the posterior {\\em distribution} of clean data samples given their noisy versions, instead of only the mean of this distribution.","This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output.","This is accomplished by replacing the standard regression loss used to estimate conditional means with a scoring rule.","We validate our method on image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps."],"url":"http://arxiv.org/abs/2502.02483v1"}
{"created":"2025-02-04 16:57:03","title":"Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study","abstract":"Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo.","sentences":["Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement.","In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks.","We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities.","We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages.","Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo."],"url":"http://arxiv.org/abs/2502.02481v1"}
{"created":"2025-02-04 16:57:02","title":"Stable Port-Hamiltonian Neural Networks","abstract":"In recent years, nonlinear dynamic system identification using artificial neural networks has garnered attention due to its manifold potential applications in virtually all branches of science and engineering. However, purely data-driven approaches often struggle with extrapolation and may yield physically implausible forecasts. Furthermore, the learned dynamics can exhibit instabilities, making it difficult to apply such models safely and robustly. This article proposes stable port-Hamiltonian neural networks, a machine learning architecture that incorporates the physical biases of energy conservation or dissipation while guaranteeing global Lyapunov stability of the learned dynamics. Evaluations with illustrative examples and real-world measurement data demonstrate the model's ability to generalize from sparse data, outperforming purely data-driven approaches and avoiding instability issues. In addition, the model's potential for data-driven surrogate modeling is highlighted in application to multi-physics simulation data.","sentences":["In recent years, nonlinear dynamic system identification using artificial neural networks has garnered attention due to its manifold potential applications in virtually all branches of science and engineering.","However, purely data-driven approaches often struggle with extrapolation and may yield physically implausible forecasts.","Furthermore, the learned dynamics can exhibit instabilities, making it difficult to apply such models safely and robustly.","This article proposes stable port-Hamiltonian neural networks, a machine learning architecture that incorporates the physical biases of energy conservation or dissipation while guaranteeing global Lyapunov stability of the learned dynamics.","Evaluations with illustrative examples and real-world measurement data demonstrate the model's ability to generalize from sparse data, outperforming purely data-driven approaches and avoiding instability issues.","In addition, the model's potential for data-driven surrogate modeling is highlighted in application to multi-physics simulation data."],"url":"http://arxiv.org/abs/2502.02480v1"}
{"created":"2025-02-04 16:53:10","title":"A Clique Partitioning-Based Algorithm for Graph Compression","abstract":"Reducing the running time of graph algorithms is vital for tackling real-world problems such as shortest paths and matching in large-scale graphs, where path information plays a crucial role. This paper addresses this critical challenge of reducing the running time of graph algorithms by proposing a new graph compression algorithm that partitions the graph into bipartite cliques and uses the partition to obtain a compressed graph having a smaller number of edges while preserving the path information. This compressed graph can then be used as input to other graph algorithms for which path information is essential, leading to a significant reduction of their running time, especially for large, dense graphs. The running time of the proposed algorithm is~$O(mn^\\delta)$, where $0 \\leq \\delta \\leq 1$, which is better than $O(mn^\\delta \\log^2 n)$, the running time of the best existing clique partitioning-based graph compression algorithm (the Feder-Motwani (\\textsf{FM}) algorithm). Our extensive experimental analysis show that our algorithm achieves a compression ratio of up to~$26\\%$ greater and executes up to~105.18 times faster than the \\textsf{FM} algorithm. In addition, on large graphs with up to 1.05 billion edges, it achieves a compression ratio of up to~3.9, reducing the number of edges up to~$74.36\\%$. Finally, our tests with a matching algorithm on sufficiently large, dense graphs, demonstrate a reduction in the running time of up to 72.83\\% when the input is the compressed graph obtained by our algorithm, compared to the case where the input is the original uncompressed graph.","sentences":["Reducing the running time of graph algorithms is vital for tackling real-world problems such as shortest paths and matching in large-scale graphs, where path information plays a crucial role.","This paper addresses this critical challenge of reducing the running time of graph algorithms by proposing a new graph compression algorithm that partitions the graph into bipartite cliques and uses the partition to obtain a compressed graph having a smaller number of edges while preserving the path information.","This compressed graph can then be used as input to other graph algorithms for which path information is essential, leading to a significant reduction of their running time, especially for large, dense graphs.","The running time of the proposed algorithm is~$O(mn^\\delta)$, where $0 \\leq \\delta \\leq 1$, which is better than $O(mn^\\delta \\log^2 n)$, the running time of the best existing clique partitioning-based graph compression algorithm (the Feder-Motwani (\\textsf{FM}) algorithm).","Our extensive experimental analysis show that our algorithm achieves a compression ratio of up to~$26\\%$ greater and executes up to~105.18 times faster than the \\textsf{FM} algorithm.","In addition, on large graphs with up to 1.05 billion edges, it achieves a compression ratio of up to~3.9, reducing the number of edges up to~$74.36\\%$.","Finally, our tests with a matching algorithm on sufficiently large, dense graphs, demonstrate a reduction in the running time of up to 72.83\\% when the input is the compressed graph obtained by our algorithm, compared to the case where the input is the original uncompressed graph."],"url":"http://arxiv.org/abs/2502.02477v1"}
{"created":"2025-02-04 16:29:42","title":"Computing with Smart Rings: A Systematic Literature Review","abstract":"A smart ring is a wearable electronic device in the form of a ring that incorporates diverse sensors and computing technologies to perform a variety of functions. Designed for use with fingers, smart rings are capable of sensing more subtle and abundant hand movements, thus making them a good platform for interaction. Meanwhile, fingers are abundant with blood vessels and nerve endings and accustomed to wearing rings, providing an ideal site for continuous health monitoring through smart rings, which combine comfort with the ability to capture vital biometric data, making them suitable for all-day wear. We collected in total of 206 smart ring-related publications and conducted a systematic literature review. We provide a taxonomy regarding the sensing and feedback modalities, applications, and phenomena. We review and categorize these literatures into four main areas: (1) interaction - input, (2) interaction - output, (3) passive sensing - in body feature, (4) passive sensing - out body activity. This comprehensive review highlights the current advancements within the field of smart ring and identifies potential areas for future research.","sentences":["A smart ring is a wearable electronic device in the form of a ring that incorporates diverse sensors and computing technologies to perform a variety of functions.","Designed for use with fingers, smart rings are capable of sensing more subtle and abundant hand movements, thus making them a good platform for interaction.","Meanwhile, fingers are abundant with blood vessels and nerve endings and accustomed to wearing rings, providing an ideal site for continuous health monitoring through smart rings, which combine comfort with the ability to capture vital biometric data, making them suitable for all-day wear.","We collected in total of 206 smart ring-related publications and conducted a systematic literature review.","We provide a taxonomy regarding the sensing and feedback modalities, applications, and phenomena.","We review and categorize these literatures into four main areas: (1) interaction - input, (2) interaction - output, (3) passive sensing - in body feature, (4) passive sensing - out body activity.","This comprehensive review highlights the current advancements within the field of smart ring and identifies potential areas for future research."],"url":"http://arxiv.org/abs/2502.02459v1"}
{"created":"2025-02-04 16:25:15","title":"Orientation-aware interaction-based deep material network in polycrystalline materials modeling","abstract":"Multiscale simulations are indispensable for connecting microstructural features to the macroscopic behavior of polycrystalline materials, but their high computational demands limit their practicality. Deep material networks (DMNs) have been proposed as efficient surrogate models, yet they fall short of capturing texture evolution. To address this limitation, we propose the orientation-aware interaction-based deep material network (ODMN), which incorporates an orientation-aware mechanism and an interaction mechanism grounded in the Hill-Mandel principle. The orientation-aware mechanism learns the crystallographic textures, while the interaction mechanism captures stress-equilibrium directions among representative volume element (RVE) subregions, offering insight into internal microstructural mechanics. Notably, ODMN requires only linear elastic data for training yet generalizes effectively to complex nonlinear and anisotropic responses. Our results show that ODMN accurately predicts both mechanical responses and texture evolution under complex plastic deformation, thus expanding the applicability of DMNs to polycrystalline materials. By balancing computational efficiency with predictive fidelity, ODMN provides a robust framework for multiscale simulations of polycrystalline materials.","sentences":["Multiscale simulations are indispensable for connecting microstructural features to the macroscopic behavior of polycrystalline materials, but their high computational demands limit their practicality.","Deep material networks (DMNs) have been proposed as efficient surrogate models, yet they fall short of capturing texture evolution.","To address this limitation, we propose the orientation-aware interaction-based deep material network (ODMN), which incorporates an orientation-aware mechanism and an interaction mechanism grounded in the Hill-Mandel principle.","The orientation-aware mechanism learns the crystallographic textures, while the interaction mechanism captures stress-equilibrium directions among representative volume element (RVE) subregions, offering insight into internal microstructural mechanics.","Notably, ODMN requires only linear elastic data for training yet generalizes effectively to complex nonlinear and anisotropic responses.","Our results show that ODMN accurately predicts both mechanical responses and texture evolution under complex plastic deformation, thus expanding the applicability of DMNs to polycrystalline materials.","By balancing computational efficiency with predictive fidelity, ODMN provides a robust framework for multiscale simulations of polycrystalline materials."],"url":"http://arxiv.org/abs/2502.02457v1"}
{"created":"2025-02-04 16:24:42","title":"Model Human Learners: Computational Models to Guide Instructional Design","abstract":"Instructional designers face an overwhelming array of design choices, making it challenging to identify the most effective interventions. To address this issue, I propose the concept of a Model Human Learner, a unified computational model of learning that can aid designers in evaluating candidate interventions. This paper presents the first successful demonstration of this concept, showing that a computational model can accurately predict the outcomes of two human A/B experiments -- one testing a problem sequencing intervention and the other testing an item design intervention. It also demonstrates that such a model can generate learning curves without requiring human data and provide theoretical insights into why an instructional intervention is effective. These findings lay the groundwork for future Model Human Learners that integrate cognitive and learning theories to support instructional design across diverse tasks and interventions.","sentences":["Instructional designers face an overwhelming array of design choices, making it challenging to identify the most effective interventions.","To address this issue, I propose the concept of a Model Human Learner, a unified computational model of learning that can aid designers in evaluating candidate interventions.","This paper presents the first successful demonstration of this concept, showing that a computational model can accurately predict the outcomes of two human A/B experiments -- one testing a problem sequencing intervention and the other testing an item design intervention.","It also demonstrates that such a model can generate learning curves without requiring human data and provide theoretical insights into why an instructional intervention is effective.","These findings lay the groundwork for future Model Human Learners that integrate cognitive and learning theories to support instructional design across diverse tasks and interventions."],"url":"http://arxiv.org/abs/2502.02456v1"}
{"created":"2025-02-04 16:21:02","title":"A note on Ordered Ruzsa-Szemer\u00e9di graphs","abstract":"A recent breakthrough of Behnezhad and Ghafari [FOCS 2024] and subsequent work of Assadi, Khanna, and Kiss [SODA 2025] gave algorithms for the fully dynamic $(1-\\varepsilon)$-approximate maximum matching problem whose runtimes are determined by a purely combinatorial quantity: the maximum density of Ordered Ruzsa-Szemer\\'edi (ORS) graphs. We say a graph $G$ is an $(r,t)$-ORS graph if its edges can be partitioned into $t$ matchings $M_1,M_2, \\ldots, M_t$ each of size $r$, such that for every $i$, $M_i$ is an induced matching in the subgraph $M_{i} \\cup M_{i+1} \\cup \\cdots \\cup M_t$. This is a relaxation of the extensively-studied notion of a Ruzsa-Szemer\\'edi (RS) graph, the difference being that in an RS graph each $M_i$ must be an induced matching in $G$.   In this note, we show that these two notions are roughly equivalent. Specifically, let $\\mathrm{ORS}(n)$ be the largest $t$ such that there exists an $n$-vertex ORS-$(\\Omega(n), t)$ graph, and define $\\mathrm{RS}(n)$ analogously. We show that if $\\mathrm{ORS}(n) \\ge \\Omega(n^c)$, then for any fixed $\\delta > 0$, $\\mathrm{RS}(n) \\ge \\Omega(n^{c(1-\\delta)})$. This resolves a question of Behnezhad and Ghafari.","sentences":["A recent breakthrough of Behnezhad and Ghafari [FOCS 2024] and subsequent work of Assadi, Khanna, and Kiss","[SODA 2025] gave algorithms for the fully dynamic $(1-\\varepsilon)$-approximate maximum matching problem whose runtimes are determined by a purely combinatorial quantity: the maximum density of Ordered Ruzsa-Szemer\\'edi (ORS) graphs.","We say a graph $G$ is an $(r,t)$-ORS graph if its edges can be partitioned into $t$ matchings $M_1,M_2, \\ldots, M_t$ each of size $r$, such that for every $i$, $M_i$ is an induced matching in the subgraph $M_{i} \\cup M_{i+1} \\cup \\cdots \\cup M_t$. This is a relaxation of the extensively-studied notion of a Ruzsa-Szemer\\'edi (RS) graph, the difference being that in an RS graph each $M_i$ must be an induced matching in $G$.   In this note, we show that these two notions are roughly equivalent.","Specifically, let $\\mathrm{ORS}(n)$ be the largest $t$ such that there exists an $n$-vertex ORS-$(\\Omega(n), t)$ graph, and define $\\mathrm{RS}(n)$ analogously.","We show that if $\\mathrm{ORS}(n) \\ge \\Omega(n^c)$, then for any fixed $\\delta > 0$, $\\mathrm{RS}(n) \\ge \\Omega(n^{c(1-\\delta)})$.","This resolves a question of Behnezhad and Ghafari."],"url":"http://arxiv.org/abs/2502.02455v1"}
{"created":"2025-02-04 16:20:41","title":"IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning","abstract":"Using extensive training data from SA-1B, the Segment Anything Model (SAM) has demonstrated exceptional generalization and zero-shot capabilities, attracting widespread attention in areas such as medical image segmentation and remote sensing image segmentation. However, its performance in the field of image manipulation detection remains largely unexplored and unconfirmed. There are two main challenges in applying SAM to image manipulation detection: a) reliance on manual prompts, and b) the difficulty of single-view information in supporting cross-dataset generalization. To address these challenges, we develops a cross-view prompt learning paradigm called IMDPrompter based on SAM. Benefiting from the design of automated prompts, IMDPrompter no longer relies on manual guidance, enabling automated detection and localization. Additionally, we propose components such as Cross-view Feature Perception, Optimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate cross-view perceptual learning and guide SAM to generate accurate masks. Extensive experimental results from five datasets (CASIA, Columbia, Coverage, IMD2020, and NIST16) validate the effectiveness of our proposed method.","sentences":["Using extensive training data from SA-1B, the Segment Anything Model (SAM) has demonstrated exceptional generalization and zero-shot capabilities, attracting widespread attention in areas such as medical image segmentation and remote sensing image segmentation.","However, its performance in the field of image manipulation detection remains largely unexplored and unconfirmed.","There are two main challenges in applying SAM to image manipulation detection: a) reliance on manual prompts, and b) the difficulty of single-view information in supporting cross-dataset generalization.","To address these challenges, we develops a cross-view prompt learning paradigm called IMDPrompter based on SAM.","Benefiting from the design of automated prompts, IMDPrompter no longer relies on manual guidance, enabling automated detection and localization.","Additionally, we propose components such as Cross-view Feature Perception, Optimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate cross-view perceptual learning and guide SAM to generate accurate masks.","Extensive experimental results from five datasets (CASIA, Columbia, Coverage, IMD2020, and NIST16) validate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2502.02454v1"}
{"created":"2025-02-04 16:17:01","title":"Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study","abstract":"This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.","sentences":["This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora.","Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited.","Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts.","The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information.","In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency.","Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements.","The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks."],"url":"http://arxiv.org/abs/2502.02451v1"}
{"created":"2025-02-04 16:14:28","title":"Sparse Data Generation Using Diffusion Models","abstract":"Sparse data is ubiquitous, appearing in numerous domains, from economics and recommender systems to astronomy and biomedical sciences. However, efficiently and realistically generating sparse data remains a significant challenge. We introduce Sparse Data Diffusion (SDD), a novel method for generating sparse data. SDD extends continuous state-space diffusion models by explicitly modeling sparsity through the introduction of Sparsity Bits. Empirical validation on image data from various domains-including two scientific applications, physics and biology-demonstrates that SDD achieves high fidelity in representing data sparsity while preserving the quality of the generated data.","sentences":["Sparse data is ubiquitous, appearing in numerous domains, from economics and recommender systems to astronomy and biomedical sciences.","However, efficiently and realistically generating sparse data remains a significant challenge.","We introduce Sparse Data Diffusion (SDD), a novel method for generating sparse data.","SDD extends continuous state-space diffusion models by explicitly modeling sparsity through the introduction of Sparsity Bits.","Empirical validation on image data from various domains-including two scientific applications, physics and biology-demonstrates that SDD achieves high fidelity in representing data sparsity while preserving the quality of the generated data."],"url":"http://arxiv.org/abs/2502.02448v1"}
{"created":"2025-02-04 16:08:48","title":"LLMER: Crafting Interactive Extended Reality Worlds with JSON Data Generated by Large Language Models","abstract":"The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs. However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene/object parameters from an overwhelming volume of XR artifacts. It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors. Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience. To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs. Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency. It employs a multi-stage strategy to supply only the essential contextual information adapted to the user's request and features multiple modules designed for various XR tasks. Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches. The analysis of users' feedback also illuminates a series of directions for further optimization.","sentences":["The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs.","However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene/object parameters from an overwhelming volume of XR artifacts.","It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors.","Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience.","To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs.","Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency.","It employs a multi-stage strategy to supply only the essential contextual information adapted to the user's request and features multiple modules designed for various XR tasks.","Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches.","The analysis of users' feedback also illuminates a series of directions for further optimization."],"url":"http://arxiv.org/abs/2502.02441v1"}
{"created":"2025-02-04 16:04:48","title":"Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment","abstract":"Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-STEAL), the first stealing attack against medical MLLMs. ADA-STEAL relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.","sentences":["Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis.","Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists.","As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property.","However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access.","So far, model stealing for the medical domain has focused on classification; however, existing attacks are not effective against MLLMs.","In this paper, we introduce Adversarial Domain Alignment (ADA-STEAL), the first stealing attack against medical MLLMs.","ADA-STEAL relies on natural images, which are public and widely available, as opposed to their medical counterparts.","We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM.","Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data."],"url":"http://arxiv.org/abs/2502.02438v1"}
{"created":"2025-02-04 15:53:30","title":"TransformDAS: Mapping \u03a6-OTDR Signals to Riemannian Manifold for Robust Classification","abstract":"Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) is a widely used distributed fiber optic sensing system in engineering. Machine learning algorithms for {\\Phi}-OTDR event classification require high volumes and quality of datasets; however, high-quality datasets are currently extremely scarce in the field, leading to a lack of robustness in models, which is manifested by higher false alarm rates in real-world scenarios. One promising approach to address this issue is to augment existing data using generative models combined with a small amount of real-world data. We explored mapping both {\\Phi}-OTDR features in a GAN-based generative pipeline and signal features in a Transformer classifier to hyperbolic space to seek more effective model generalization. The results indicate that state-of-the-art models exhibit stronger generalization performance and lower false alarm rates in real-world scenarios when trained on augmented datasets. TransformDAS, in particular, demonstrates the best classification performance, highlighting the benefits of Riemannian manifold mapping in {\\Phi}-OTDR data generation and model classification.","sentences":["Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) is a widely used distributed fiber optic sensing system in engineering.","Machine learning algorithms for {\\Phi}-OTDR event classification require high volumes and quality of datasets; however, high-quality datasets are currently extremely scarce in the field, leading to a lack of robustness in models, which is manifested by higher false alarm rates in real-world scenarios.","One promising approach to address this issue is to augment existing data using generative models combined with a small amount of real-world data.","We explored mapping both {\\Phi}-OTDR features in a GAN-based generative pipeline and signal features in a Transformer classifier to hyperbolic space to seek more effective model generalization.","The results indicate that state-of-the-art models exhibit stronger generalization performance and lower false alarm rates in real-world scenarios when trained on augmented datasets.","TransformDAS, in particular, demonstrates the best classification performance, highlighting the benefits of Riemannian manifold mapping in {\\Phi}-OTDR data generation and model classification."],"url":"http://arxiv.org/abs/2502.02428v1"}
{"created":"2025-02-04 15:35:25","title":"Towards Fast Graph Generation via Autoregressive Noisy Filtration Modeling","abstract":"Graph generative models often face a critical trade-off between learning complex distributions and achieving fast generation speed. We introduce Autoregressive Noisy Filtration Modeling (ANFM), a novel approach that addresses both challenges. ANFM leverages filtration, a concept from topological data analysis, to transform graphs into short sequences of monotonically increasing subgraphs. This formulation extends the sequence families used in previous autoregressive models. To learn from these sequences, we propose a novel autoregressive graph mixer model. Our experiments suggest that exposure bias might represent a substantial hurdle in autoregressive graph generation and we introduce two mitigation strategies to address it: noise augmentation and a reinforcement learning approach. Incorporating these techniques leads to substantial performance gains, making ANFM competitive with state-of-the-art diffusion models across diverse synthetic and real-world datasets. Notably, ANFM produces remarkably short sequences, achieving a 100-fold speedup in generation time compared to diffusion models. This work marks a significant step toward high-throughput graph generation.","sentences":["Graph generative models often face a critical trade-off between learning complex distributions and achieving fast generation speed.","We introduce Autoregressive Noisy Filtration Modeling (ANFM), a novel approach that addresses both challenges.","ANFM leverages filtration, a concept from topological data analysis, to transform graphs into short sequences of monotonically increasing subgraphs.","This formulation extends the sequence families used in previous autoregressive models.","To learn from these sequences, we propose a novel autoregressive graph mixer model.","Our experiments suggest that exposure bias might represent a substantial hurdle in autoregressive graph generation and we introduce two mitigation strategies to address it: noise augmentation and a reinforcement learning approach.","Incorporating these techniques leads to substantial performance gains, making ANFM competitive with state-of-the-art diffusion models across diverse synthetic and real-world datasets.","Notably, ANFM produces remarkably short sequences, achieving a 100-fold speedup in generation time compared to diffusion models.","This work marks a significant step toward high-throughput graph generation."],"url":"http://arxiv.org/abs/2502.02415v1"}
{"created":"2025-02-04 15:33:50","title":"Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries","abstract":"Although deep models have been widely explored in solving partial differential equations (PDEs), previous works are primarily limited to data only with up to tens of thousands of mesh points, far from the million-point scale required by industrial simulations that involve complex geometries. In the spirit of advancing neural PDE solvers to real industrial applications, we present Transolver++, a highly parallel and efficient neural solver that can accurately solve PDEs on million-scale geometries. Building upon previous advancements in solving PDEs by learning physical states via Transolver, Transolver++ is further equipped with an extremely optimized parallelism framework and a local adaptive mechanism to efficiently capture eidetic physical states from massive mesh points, successfully tackling the thorny challenges in computation and physics learning when scaling up input mesh size. Transolver++ increases the single-GPU input capacity to million-scale points for the first time and is capable of continuously scaling input size in linear complexity by increasing GPUs. Experimentally, Transolver++ yields 13% relative promotion across six standard PDE benchmarks and achieves over 20% performance gain in million-scale high-fidelity industrial simulations, whose sizes are 100$\\times$ larger than previous benchmarks, covering car and 3D aircraft designs.","sentences":["Although deep models have been widely explored in solving partial differential equations (PDEs), previous works are primarily limited to data only with up to tens of thousands of mesh points, far from the million-point scale required by industrial simulations that involve complex geometries.","In the spirit of advancing neural PDE solvers to real industrial applications, we present Transolver++, a highly parallel and efficient neural solver that can accurately solve PDEs on million-scale geometries.","Building upon previous advancements in solving PDEs by learning physical states via Transolver, Transolver++ is further equipped with an extremely optimized parallelism framework and a local adaptive mechanism to efficiently capture eidetic physical states from massive mesh points, successfully tackling the thorny challenges in computation and physics learning when scaling up input mesh size.","Transolver++ increases the single-GPU input capacity to million-scale points for the first time and is capable of continuously scaling input size in linear complexity by increasing GPUs.","Experimentally, Transolver++ yields 13% relative promotion across six standard PDE benchmarks and achieves over 20% performance gain in million-scale high-fidelity industrial simulations, whose sizes are 100$\\times$ larger than previous benchmarks, covering car and 3D aircraft designs."],"url":"http://arxiv.org/abs/2502.02414v1"}
{"created":"2025-02-04 15:29:00","title":"Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting","abstract":"Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with timeseries-specific tasks like forecasting, since they rely on the privacy amplification attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this structured subsampling to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.","sentences":["Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential.","The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD).","However, we observe in this work that the formal guarantees of DP-SGD are incompatible with timeseries-specific tasks like forecasting, since they rely on the privacy amplification attained by training on small, unstructured batches sampled from an unstructured dataset.","In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows.","We theoretically analyze the privacy amplification attained by this structured subsampling to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees.","Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models.","Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees."],"url":"http://arxiv.org/abs/2502.02410v1"}
{"created":"2025-02-04 15:23:43","title":"FPGA Innovation Research in the Netherlands: Present Landscape and Future Outlook","abstract":"FPGAs have transformed digital design by enabling versatile and customizable solutions that balance performance and power efficiency, yielding them essential for today's diverse computing challenges. Research in the Netherlands, both in academia and industry, plays a major role in developing new innovative FPGA solutions. This survey presents the current landscape of FPGA innovation research in the Netherlands by delving into ongoing projects, advancements, and breakthroughs in the field. Focusing on recent research outcome (within the past 5 years), we have identified five key research areas: a) FPGA architecture, b) FPGA robustness, c) data center infrastructure and high-performance computing, d) programming models and tools, and e) applications. This survey provides in-depth insights beyond a mere snapshot of the current innovation research landscape by highlighting future research directions within each key area; these insights can serve as a foundational resource to inform potential national-level investments in FPGA technology.","sentences":["FPGAs have transformed digital design by enabling versatile and customizable solutions that balance performance and power efficiency, yielding them essential for today's diverse computing challenges.","Research in the Netherlands, both in academia and industry, plays a major role in developing new innovative FPGA solutions.","This survey presents the current landscape of FPGA innovation research in the Netherlands by delving into ongoing projects, advancements, and breakthroughs in the field.","Focusing on recent research outcome (within the past 5 years), we have identified five key research areas: a) FPGA architecture, b) FPGA robustness, c) data center infrastructure and high-performance computing, d) programming models and tools, and e) applications.","This survey provides in-depth insights beyond a mere snapshot of the current innovation research landscape by highlighting future research directions within each key area; these insights can serve as a foundational resource to inform potential national-level investments in FPGA technology."],"url":"http://arxiv.org/abs/2502.02404v1"}
{"created":"2025-02-04 15:04:01","title":"Hypergraph Link Prediction via Hyperedge Copying","abstract":"We propose a generative model of temporally-evolving hypergraphs in which hyperedges form via noisy copying of previous hyperedges. Our proposed model reproduces several stylized facts from many empirical hypergraphs, is learnable from data, and defines a likelihood over a complete hypergraph rather than ego-based or other sub-hypergraphs. Analyzing our model, we derive descriptions of node degree, edge size, and edge intersection size distributions in terms of the model parameters. We also show several features of empirical hypergraphs which are and are not successfully captured by our model. We provide a scalable stochastic expectation maximization algorithm with which we can fit our model to hypergraph data sets with millions of nodes and edges. Finally, we assess our model on a hypergraph link prediction task, finding that an instantiation of our model with just 11 parameters can achieve competitive predictive performance with large neural networks.","sentences":["We propose a generative model of temporally-evolving hypergraphs in which hyperedges form via noisy copying of previous hyperedges.","Our proposed model reproduces several stylized facts from many empirical hypergraphs, is learnable from data, and defines a likelihood over a complete hypergraph rather than ego-based or other sub-hypergraphs.","Analyzing our model, we derive descriptions of node degree, edge size, and edge intersection size distributions in terms of the model parameters.","We also show several features of empirical hypergraphs which are and are not successfully captured by our model.","We provide a scalable stochastic expectation maximization algorithm with which we can fit our model to hypergraph data sets with millions of nodes and edges.","Finally, we assess our model on a hypergraph link prediction task, finding that an instantiation of our model with just 11 parameters can achieve competitive predictive performance with large neural networks."],"url":"http://arxiv.org/abs/2502.02386v1"}
{"created":"2025-02-04 15:02:55","title":"STAIR: Improving Safety Alignment with Introspective Reasoning","abstract":"Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose STAIR, a novel framework that integrates SafeTy Alignment with Itrospective Reasoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. Relevant resources in this work are available at https://github.com/thu-ml/STAIR.","sentences":["Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications.","However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries.","In this paper, we propose STAIR, a novel framework that integrates SafeTy","Alignment with Itrospective Reasoning.","We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness.","STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS).","We further train a process reward model on this data to guide test-time searches for improved responses.","Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies.","With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks.","Relevant resources in this work are available at https://github.com/thu-ml/STAIR."],"url":"http://arxiv.org/abs/2502.02384v1"}
{"created":"2025-02-04 14:59:03","title":"No Metric to Rule Them All: Toward Principled Evaluations of Graph-Learning Datasets","abstract":"Benchmark datasets have proved pivotal to the success of graph learning, and good benchmark datasets are crucial to guide the development of the field. Recent research has highlighted problems with graph-learning datasets and benchmarking practices -- revealing, for example, that methods which ignore the graph structure can outperform graph-based approaches on popular benchmark datasets. Such findings raise two questions: (1) What makes a good graph-learning dataset, and (2) how can we evaluate dataset quality in graph learning? Our work addresses these questions. As the classic evaluation setup uses datasets to evaluate models, it does not apply to dataset evaluation. Hence, we start from first principles. Observing that graph-learning datasets uniquely combine two modes -- the graph structure and the node features -- , we introduce RINGS, a flexible and extensible mode-perturbation framework to assess the quality of graph-learning datasets based on dataset ablations -- i.e., by quantifying differences between the original dataset and its perturbed representations. Within this framework, we propose two measures -- performance separability and mode complementarity -- as evaluation tools, each assessing, from a distinct angle, the capacity of a graph dataset to benchmark the power and efficacy of graph-learning methods. We demonstrate the utility of our framework for graph-learning dataset evaluation in an extensive set of experiments and derive actionable recommendations for improving the evaluation of graph-learning methods. Our work opens new research directions in data-centric graph learning, and it constitutes a first step toward the systematic evaluation of evaluations.","sentences":["Benchmark datasets have proved pivotal to the success of graph learning, and good benchmark datasets are crucial to guide the development of the field.","Recent research has highlighted problems with graph-learning datasets and benchmarking practices -- revealing, for example, that methods which ignore the graph structure can outperform graph-based approaches on popular benchmark datasets.","Such findings raise two questions: (1) What makes a good graph-learning dataset, and (2) how can we evaluate dataset quality in graph learning?","Our work addresses these questions.","As the classic evaluation setup uses datasets to evaluate models, it does not apply to dataset evaluation.","Hence, we start from first principles.","Observing that graph-learning datasets uniquely combine two modes -- the graph structure and the node features -- , we introduce RINGS, a flexible and extensible mode-perturbation framework to assess the quality of graph-learning datasets based on dataset ablations -- i.e., by quantifying differences between the original dataset and its perturbed representations.","Within this framework, we propose two measures -- performance separability and mode complementarity -- as evaluation tools, each assessing, from a distinct angle, the capacity of a graph dataset to benchmark the power and efficacy of graph-learning methods.","We demonstrate the utility of our framework for graph-learning dataset evaluation in an extensive set of experiments and derive actionable recommendations for improving the evaluation of graph-learning methods.","Our work opens new research directions in data-centric graph learning, and it constitutes a first step toward the systematic evaluation of evaluations."],"url":"http://arxiv.org/abs/2502.02379v1"}
{"created":"2025-02-04 14:52:34","title":"MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning","abstract":"The generation of a virtual digital avatar is a crucial research topic in the field of computer vision. Many existing works utilize Neural Radiance Fields (NeRF) to address this issue and have achieved impressive results. However, previous works assume the images of the training person are available and fixed while the appearances and poses of a subject could constantly change and increase in real-world scenarios. How to update the human avatar but also maintain the ability to render the old appearance of the person is a practical challenge. One trivial solution is to combine the existing virtual avatar models based on NeRF with continual learning methods. However, there are some critical issues in this approach: learning new appearances and poses can cause the model to forget past information, which in turn leads to a degradation in the rendering quality of past appearances, especially color bleeding issues, and incorrect human body poses. In this work, we propose a maintainable avatar (MaintaAvatar) based on neural radiance fields by continual learning, which resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose Distillation Module. Overall, our model requires only limited data collection to quickly fine-tune the model while avoiding catastrophic forgetting, thus achieving a maintainable virtual avatar. The experimental results validate the effectiveness of our MaintaAvatar model.","sentences":["The generation of a virtual digital avatar is a crucial research topic in the field of computer vision.","Many existing works utilize Neural Radiance Fields (NeRF) to address this issue and have achieved impressive results.","However, previous works assume the images of the training person are available and fixed while the appearances and poses of a subject could constantly change and increase in real-world scenarios.","How to update the human avatar but also maintain the ability to render the old appearance of the person is a practical challenge.","One trivial solution is to combine the existing virtual avatar models based on NeRF with continual learning methods.","However, there are some critical issues in this approach: learning new appearances and poses can cause the model to forget past information, which in turn leads to a degradation in the rendering quality of past appearances, especially color bleeding issues, and incorrect human body poses.","In this work, we propose a maintainable avatar (MaintaAvatar) based on neural radiance fields by continual learning, which resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose Distillation Module.","Overall, our model requires only limited data collection to quickly fine-tune the model while avoiding catastrophic forgetting, thus achieving a maintainable virtual avatar.","The experimental results validate the effectiveness of our MaintaAvatar model."],"url":"http://arxiv.org/abs/2502.02372v1"}
{"created":"2025-02-04 14:50:16","title":"Field Matching: an Electrostatic Paradigm to Generate and Transfer Data","abstract":"We propose Electrostatic Field Matching (EFM), a novel method that is suitable for both generative modeling and distribution transfer tasks. Our approach is inspired by the physics of an electrical capacitor. We place source and target distributions on the capacitor plates and assign them positive and negative charges, respectively. We then learn the electrostatic field of the capacitor using a neural network approximator. To map the distributions to each other, we start at one plate of the capacitor and move the samples along the learned electrostatic field lines until they reach the other plate. We theoretically justify that this approach provably yields the distribution transfer. In practice, we demonstrate the performance of our EFM in toy and image data experiments.","sentences":["We propose Electrostatic Field Matching (EFM), a novel method that is suitable for both generative modeling and distribution transfer tasks.","Our approach is inspired by the physics of an electrical capacitor.","We place source and target distributions on the capacitor plates and assign them positive and negative charges, respectively.","We then learn the electrostatic field of the capacitor using a neural network approximator.","To map the distributions to each other, we start at one plate of the capacitor and move the samples along the learned electrostatic field lines until they reach the other plate.","We theoretically justify that this approach provably yields the distribution transfer.","In practice, we demonstrate the performance of our EFM in toy and image data experiments."],"url":"http://arxiv.org/abs/2502.02367v1"}
{"created":"2025-02-04 14:35:29","title":"Exploring the Feasibility of AI-Assisted Spine MRI Protocol Optimization Using DICOM Image Metadata","abstract":"Artificial intelligence (AI) is increasingly being utilized to optimize magnetic resonance imaging (MRI) protocols. Given that image details are critical for diagnostic accuracy, optimizing MRI acquisition protocols is essential for enhancing image quality. While medical physicists are responsible for this optimization, the variability in equipment usage and the wide range of MRI protocols in clinical settings pose significant challenges. This study aims to validate the application of AI in optimizing MRI protocols using dynamic data from clinical practice, specifically DICOM metadata. To achieve this, four MRI spine exam databases were created, with the target attribute being the binary classification of image quality (good or bad). Five AI models were trained to identify trends in acquisition parameters that influence image quality, grounded in MRI theory. These trends were analyzed using SHAP graphs. The models achieved F1 performance ranging from 77% to 93% for datasets containing 292 or more instances, with the observed trends aligning with MRI theory. The models effectively reflected the practical realities of clinical MRI settings, offering a valuable tool for medical physicists in quality control tasks. In conclusion, AI has demonstrated its potential to optimize MRI protocols, supporting medical physicists in improving image quality and enhancing the efficiency of quality control in clinical practice.","sentences":["Artificial intelligence (AI) is increasingly being utilized to optimize magnetic resonance imaging (MRI) protocols.","Given that image details are critical for diagnostic accuracy, optimizing MRI acquisition protocols is essential for enhancing image quality.","While medical physicists are responsible for this optimization, the variability in equipment usage and the wide range of MRI protocols in clinical settings pose significant challenges.","This study aims to validate the application of AI in optimizing MRI protocols using dynamic data from clinical practice, specifically DICOM metadata.","To achieve this, four MRI spine exam databases were created, with the target attribute being the binary classification of image quality (good or bad).","Five AI models were trained to identify trends in acquisition parameters that influence image quality, grounded in MRI theory.","These trends were analyzed using SHAP graphs.","The models achieved F1 performance ranging from 77% to 93% for datasets containing 292 or more instances, with the observed trends aligning with MRI theory.","The models effectively reflected the practical realities of clinical MRI settings, offering a valuable tool for medical physicists in quality control tasks.","In conclusion, AI has demonstrated its potential to optimize MRI protocols, supporting medical physicists in improving image quality and enhancing the efficiency of quality control in clinical practice."],"url":"http://arxiv.org/abs/2502.02351v1"}
{"created":"2025-02-04 14:33:44","title":"Random Adaptive Cache Placement Policy","abstract":"This paper presents a new hybrid cache replacement algorithm that combines random allocation with a modified V-Way cache implementation. Our RAC adapts to complex cache access patterns and optimizes cache usage by improving the utilization of cache sets, unlike traditional cache policies. The algorithm utilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic allocation and flexible tag management. RAC extends the V-Way cache design and its variants by optimizing tag and data storage for enhanced efficiency.   We evaluated the algorithm using the ChampSim simulator with four diverse benchmark traces and observed significant improvements in cache hit rates up to 80.82% hit rate. Although the improvements in the instructions per cycle (IPC) were moderate, our findings emphasize the algorithm's potential to enhance cache utilization and reduce memory access times.","sentences":["This paper presents a new hybrid cache replacement algorithm that combines random allocation with a modified V-Way cache implementation.","Our RAC adapts to complex cache access patterns and optimizes cache usage by improving the utilization of cache sets, unlike traditional cache policies.","The algorithm utilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic allocation and flexible tag management.","RAC extends the V-Way cache design and its variants by optimizing tag and data storage for enhanced efficiency.   ","We evaluated the algorithm using the ChampSim simulator with four diverse benchmark traces and observed significant improvements in cache hit rates up to 80.82% hit rate.","Although the improvements in the instructions per cycle (IPC) were moderate, our findings emphasize the algorithm's potential to enhance cache utilization and reduce memory access times."],"url":"http://arxiv.org/abs/2502.02349v1"}
{"created":"2025-02-04 14:20:51","title":"SHIELD: APT Detection and Intelligent Explanation Using LLM","abstract":"Advanced persistent threats (APTs) are sophisticated cyber attacks that can remain undetected for extended periods, making their mitigation particularly challenging. Given their persistence, significant effort is required to detect them and respond effectively. Existing provenance-based attack detection methods often lack interpretability and suffer from high false positive rates, while investigation approaches are either supervised or limited to known attacks. To address these challenges, we introduce SHIELD, a novel approach that combines statistical anomaly detection and graph-based analysis with the contextual analysis capabilities of large language models (LLMs). SHIELD leverages the implicit knowledge of LLMs to uncover hidden attack patterns in provenance data, while reducing false positives and providing clear, interpretable attack descriptions. This reduces analysts' alert fatigue and makes it easier for them to understand the threat landscape. Our extensive evaluation demonstrates SHIELD's effectiveness and computational efficiency in real-world scenarios. SHIELD was shown to outperform state-of-the-art methods, achieving higher precision and recall. SHIELD's integration of anomaly detection, LLM-driven contextual analysis, and advanced graph-based correlation establishes a new benchmark for APT detection.","sentences":["Advanced persistent threats (APTs) are sophisticated cyber attacks that can remain undetected for extended periods, making their mitigation particularly challenging.","Given their persistence, significant effort is required to detect them and respond effectively.","Existing provenance-based attack detection methods often lack interpretability and suffer from high false positive rates, while investigation approaches are either supervised or limited to known attacks.","To address these challenges, we introduce SHIELD, a novel approach that combines statistical anomaly detection and graph-based analysis with the contextual analysis capabilities of large language models (LLMs).","SHIELD leverages the implicit knowledge of LLMs to uncover hidden attack patterns in provenance data, while reducing false positives and providing clear, interpretable attack descriptions.","This reduces analysts' alert fatigue and makes it easier for them to understand the threat landscape.","Our extensive evaluation demonstrates SHIELD's effectiveness and computational efficiency in real-world scenarios.","SHIELD was shown to outperform state-of-the-art methods, achieving higher precision and recall.","SHIELD's integration of anomaly detection, LLM-driven contextual analysis, and advanced graph-based correlation establishes a new benchmark for APT detection."],"url":"http://arxiv.org/abs/2502.02342v1"}
{"created":"2025-02-04 14:18:29","title":"Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking","abstract":"Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining substantial data and computational efficiency.","sentences":["Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning.","While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency.","A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization.","To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS).","AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures.","Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations.","This novel paradigm strikes a compelling balance between performance and efficiency.","Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining substantial data and computational efficiency."],"url":"http://arxiv.org/abs/2502.02339v1"}
{"created":"2025-02-04 14:16:02","title":"Rule-ATT&CK Mapper (RAM): Mapping SIEM Rules to TTPs Using LLMs","abstract":"The growing frequency of cyberattacks has heightened the demand for accurate and efficient threat detection systems. SIEM platforms are important for analyzing log data and detecting adversarial activities through rule-based queries, also known as SIEM rules. The efficiency of the threat analysis process relies heavily on mapping these SIEM rules to the relevant attack techniques in the MITRE ATT&CK framework. Inaccurate annotation of SIEM rules can result in the misinterpretation of attacks, increasing the likelihood that threats will be overlooked. Existing solutions for annotating SIEM rules with MITRE ATT&CK technique labels have notable limitations: manual annotation of SIEM rules is both time-consuming and prone to errors, and ML-based approaches mainly focus on annotating unstructured free text sources rather than structured data like SIEM rules. Structured data often contains limited information, further complicating the annotation process and making it a challenging task. To address these challenges, we propose Rule-ATT&CK Mapper (RAM), a novel framework that leverages LLMs to automate the mapping of structured SIEM rules to MITRE ATT&CK techniques. RAM's multi-stage pipeline, which was inspired by the prompt chaining technique, enhances mapping accuracy without requiring LLM pre-training or fine-tuning. Using the Splunk Security Content dataset, we evaluate RAM's performance using several LLMs, including GPT-4-Turbo, Qwen, IBM Granite, and Mistral. Our evaluation highlights GPT-4-Turbo's superior performance, which derives from its enriched knowledge base, and an ablation study emphasizes the importance of external contextual knowledge in overcoming the limitations of LLMs' implicit knowledge for domain-specific tasks. These findings demonstrate RAM's potential in automating cybersecurity workflows and provide valuable insights for future advancements in this field.","sentences":["The growing frequency of cyberattacks has heightened the demand for accurate and efficient threat detection systems.","SIEM platforms are important for analyzing log data and detecting adversarial activities through rule-based queries, also known as SIEM rules.","The efficiency of the threat analysis process relies heavily on mapping these SIEM rules to the relevant attack techniques in the MITRE ATT&CK framework.","Inaccurate annotation of SIEM rules can result in the misinterpretation of attacks, increasing the likelihood that threats will be overlooked.","Existing solutions for annotating SIEM rules with MITRE ATT&CK technique labels have notable limitations: manual annotation of SIEM rules is both time-consuming and prone to errors, and ML-based approaches mainly focus on annotating unstructured free text sources rather than structured data like SIEM rules.","Structured data often contains limited information, further complicating the annotation process and making it a challenging task.","To address these challenges, we propose Rule-ATT&CK Mapper (RAM), a novel framework that leverages LLMs to automate the mapping of structured SIEM rules to MITRE ATT&CK techniques.","RAM's multi-stage pipeline, which was inspired by the prompt chaining technique, enhances mapping accuracy without requiring LLM pre-training or fine-tuning.","Using the Splunk Security Content dataset, we evaluate RAM's performance using several LLMs, including GPT-4-Turbo, Qwen, IBM Granite, and Mistral.","Our evaluation highlights GPT-4-Turbo's superior performance, which derives from its enriched knowledge base, and an ablation study emphasizes the importance of external contextual knowledge in overcoming the limitations of LLMs' implicit knowledge for domain-specific tasks.","These findings demonstrate RAM's potential in automating cybersecurity workflows and provide valuable insights for future advancements in this field."],"url":"http://arxiv.org/abs/2502.02337v1"}
{"created":"2025-02-04 14:12:24","title":"Event-aided Semantic Scene Completion","abstract":"Autonomous driving systems rely on robust 3D scene understanding. Recent advances in Semantic Scene Completion (SSC) for autonomous driving underscore the limitations of RGB-based approaches, which struggle under motion blur, poor lighting, and adverse weather. Event cameras, offering high dynamic range and low latency, address these challenges by providing asynchronous data that complements RGB inputs. We present DSEC-SSC, the first real-world benchmark specifically designed for event-aided SSC, which includes a novel 4D labeling pipeline for generating dense, visibility-aware labels that adapt dynamically to object motion. Our proposed RGB-Event fusion framework, EvSSC, introduces an Event-aided Lifting Module (ELM) that effectively bridges 2D RGB-Event features to 3D space, enhancing view transformation and the robustness of 3D volume construction across SSC models. Extensive experiments on DSEC-SSC and simulated SemanticKITTI-E demonstrate that EvSSC is adaptable to both transformer-based and LSS-based SSC architectures. Notably, evaluations on SemanticKITTI-C demonstrate that EvSSC achieves consistently improved prediction accuracy across five degradation modes and both In-domain and Out-of-domain settings, achieving up to a 52.5% relative improvement in mIoU when the image sensor partially fails. Additionally, we quantitatively and qualitatively validate the superiority of EvSSC under motion blur and extreme weather conditions, where autonomous driving is challenged. The established datasets and our codebase will be made publicly at https://github.com/Pandapan01/EvSSC.","sentences":["Autonomous driving systems rely on robust 3D scene understanding.","Recent advances in Semantic Scene Completion (SSC) for autonomous driving underscore the limitations of RGB-based approaches, which struggle under motion blur, poor lighting, and adverse weather.","Event cameras, offering high dynamic range and low latency, address these challenges by providing asynchronous data that complements RGB inputs.","We present DSEC-SSC, the first real-world benchmark specifically designed for event-aided SSC, which includes a novel 4D labeling pipeline for generating dense, visibility-aware labels that adapt dynamically to object motion.","Our proposed RGB-Event fusion framework, EvSSC, introduces an Event-aided Lifting Module (ELM) that effectively bridges 2D RGB-Event features to 3D space, enhancing view transformation and the robustness of 3D volume construction across SSC models.","Extensive experiments on DSEC-SSC and simulated SemanticKITTI-E demonstrate that EvSSC is adaptable to both transformer-based and LSS-based SSC architectures.","Notably, evaluations on SemanticKITTI-C demonstrate that EvSSC achieves consistently improved prediction accuracy across five degradation modes and both In-domain and Out-of-domain settings, achieving up to a 52.5% relative improvement in mIoU when the image sensor partially fails.","Additionally, we quantitatively and qualitatively validate the superiority of EvSSC under motion blur and extreme weather conditions, where autonomous driving is challenged.","The established datasets and our codebase will be made publicly at https://github.com/Pandapan01/EvSSC."],"url":"http://arxiv.org/abs/2502.02334v1"}
{"created":"2025-02-04 14:00:32","title":"ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs","abstract":"Creating data reports is time-consuming, as it requires iterative exploration and understanding of data, followed by summarizing the insights. While large language models (LLMs) are powerful tools for data processing and text generation, they often struggle to produce complete data reports that fully meet user expectations. One significant challenge is effectively communicating the entire analysis logic to LLMs. Moreover, determining a comprehensive analysis logic can be mentally taxing for users. To address these challenges, we propose ReSpark, an LLM-based method that leverages existing data reports as references for creating new ones. Given a data table, ReSpark searches for similar-topic reports, parses them into interdependent segments corresponding to analytical objectives, and executes them with new data. It identifies inconsistencies and customizes the objectives, data transformations, and textual descriptions. ReSpark allows users to review real-time outputs, insert new objectives, and modify report content. Its effectiveness was evaluated through comparative and user studies.","sentences":["Creating data reports is time-consuming, as it requires iterative exploration and understanding of data, followed by summarizing the insights.","While large language models (LLMs) are powerful tools for data processing and text generation, they often struggle to produce complete data reports that fully meet user expectations.","One significant challenge is effectively communicating the entire analysis logic to LLMs.","Moreover, determining a comprehensive analysis logic can be mentally taxing for users.","To address these challenges, we propose ReSpark, an LLM-based method that leverages existing data reports as references for creating new ones.","Given a data table, ReSpark searches for similar-topic reports, parses them into interdependent segments corresponding to analytical objectives, and executes them with new data.","It identifies inconsistencies and customizes the objectives, data transformations, and textual descriptions.","ReSpark allows users to review real-time outputs, insert new objectives, and modify report content.","Its effectiveness was evaluated through comparative and user studies."],"url":"http://arxiv.org/abs/2502.02329v1"}
{"created":"2025-02-04 13:58:20","title":"Policy-Guided Causal State Representation for Offline Reinforcement Learning Recommendation","abstract":"In offline reinforcement learning-based recommender systems (RLRS), learning effective state representations is crucial for capturing user preferences that directly impact long-term rewards. However, raw state representations often contain high-dimensional, noisy information and components that are not causally relevant to the reward. Additionally, missing transitions in offline data make it challenging to accurately identify features that are most relevant to user satisfaction. To address these challenges, we propose Policy-Guided Causal Representation (PGCR), a novel two-stage framework for causal feature selection and state representation learning in offline RLRS. In the first stage, we learn a causal feature selection policy that generates modified states by isolating and retaining only the causally relevant components (CRCs) while altering irrelevant components. This policy is guided by a reward function based on the Wasserstein distance, which measures the causal effect of state components on the reward and encourages the preservation of CRCs that directly influence user interests. In the second stage, we train an encoder to learn compact state representations by minimizing the mean squared error (MSE) loss between the latent representations of the original and modified states, ensuring that the representations focus on CRCs. We provide a theoretical analysis proving the identifiability of causal effects from interventions, validating the ability of PGCR to isolate critical state components for decision-making. Extensive experiments demonstrate that PGCR significantly improves recommendation performance, confirming its effectiveness for offline RL-based recommender systems.","sentences":["In offline reinforcement learning-based recommender systems (RLRS), learning effective state representations is crucial for capturing user preferences that directly impact long-term rewards.","However, raw state representations often contain high-dimensional, noisy information and components that are not causally relevant to the reward.","Additionally, missing transitions in offline data make it challenging to accurately identify features that are most relevant to user satisfaction.","To address these challenges, we propose Policy-Guided Causal Representation (PGCR), a novel two-stage framework for causal feature selection and state representation learning in offline RLRS.","In the first stage, we learn a causal feature selection policy that generates modified states by isolating and retaining only the causally relevant components (CRCs) while altering irrelevant components.","This policy is guided by a reward function based on the Wasserstein distance, which measures the causal effect of state components on the reward and encourages the preservation of CRCs that directly influence user interests.","In the second stage, we train an encoder to learn compact state representations by minimizing the mean squared error (MSE) loss between the latent representations of the original and modified states, ensuring that the representations focus on CRCs.","We provide a theoretical analysis proving the identifiability of causal effects from interventions, validating the ability of PGCR to isolate critical state components for decision-making.","Extensive experiments demonstrate that PGCR significantly improves recommendation performance, confirming its effectiveness for offline RL-based recommender systems."],"url":"http://arxiv.org/abs/2502.02327v1"}
{"created":"2025-02-04 13:56:17","title":"NoteFlow: Recommending Charts as Sight Glasses for Tracing Data Flow in Computational Notebooks","abstract":"Exploratory Data Analysis (EDA) is a routine task for data analysts, often conducted using flexible computational notebooks. During EDA, data workers process, visualize, and interpret data tables, making decisions about subsequent analysis. However, the cell-by-cell programming approach, while flexible, can lead to disorganized code, making it difficult to trace the state of data tables across cells and increasing the cognitive load on data workers. This paper introduces NoteFlow, a notebook library that recommends charts as ``sight glasses'' for data tables, allowing users to monitor their dynamic updates throughout the EDA process. To ensure visual consistency and effectiveness, NoteFlow adapts chart encodings in response to data transformations, maintaining a coherent and insightful representation of the data. The proposed method was evaluated through user studies, demonstrating its ability to provide an overview of the EDA process and convey critical insights in the data tables.","sentences":["Exploratory Data Analysis (EDA) is a routine task for data analysts, often conducted using flexible computational notebooks.","During EDA, data workers process, visualize, and interpret data tables, making decisions about subsequent analysis.","However, the cell-by-cell programming approach, while flexible, can lead to disorganized code, making it difficult to trace the state of data tables across cells and increasing the cognitive load on data workers.","This paper introduces NoteFlow, a notebook library that recommends charts as ``sight glasses'' for data tables, allowing users to monitor their dynamic updates throughout the EDA process.","To ensure visual consistency and effectiveness, NoteFlow adapts chart encodings in response to data transformations, maintaining a coherent and insightful representation of the data.","The proposed method was evaluated through user studies, demonstrating its ability to provide an overview of the EDA process and convey critical insights in the data tables."],"url":"http://arxiv.org/abs/2502.02326v1"}
{"created":"2025-02-04 13:47:02","title":"Improving Generalization Ability for 3D Object Detection by Learning Sparsity-invariant Features","abstract":"In autonomous driving, 3D object detection is essential for accurately identifying and tracking objects. Despite the continuous development of various technologies for this task, a significant drawback is observed in most of them-they experience substantial performance degradation when detecting objects in unseen domains. In this paper, we propose a method to improve the generalization ability for 3D object detection on a single domain. We primarily focus on generalizing from a single source domain to target domains with distinct sensor configurations and scene distributions. To learn sparsity-invariant features from a single source domain, we selectively subsample the source data to a specific beam, using confidence scores determined by the current detector to identify the density that holds utmost importance for the detector. Subsequently, we employ the teacher-student framework to align the Bird's Eye View (BEV) features for different point clouds densities. We also utilize feature content alignment (FCA) and graph-based embedding relationship alignment (GERA) to instruct the detector to be domain-agnostic. Extensive experiments demonstrate that our method exhibits superior generalization capabilities compared to other baselines. Furthermore, our approach even outperforms certain domain adaptation methods that can access to the target domain data.","sentences":["In autonomous driving, 3D object detection is essential for accurately identifying and tracking objects.","Despite the continuous development of various technologies for this task, a significant drawback is observed in most of them-they experience substantial performance degradation when detecting objects in unseen domains.","In this paper, we propose a method to improve the generalization ability for 3D object detection on a single domain.","We primarily focus on generalizing from a single source domain to target domains with distinct sensor configurations and scene distributions.","To learn sparsity-invariant features from a single source domain, we selectively subsample the source data to a specific beam, using confidence scores determined by the current detector to identify the density that holds utmost importance for the detector.","Subsequently, we employ the teacher-student framework to align the Bird's Eye View (BEV) features for different point clouds densities.","We also utilize feature content alignment (FCA) and graph-based embedding relationship alignment (GERA) to instruct the detector to be domain-agnostic.","Extensive experiments demonstrate that our method exhibits superior generalization capabilities compared to other baselines.","Furthermore, our approach even outperforms certain domain adaptation methods that can access to the target domain data."],"url":"http://arxiv.org/abs/2502.02322v1"}
{"created":"2025-02-04 13:46:20","title":"Fostering Data Communities -- perspective from a Data Archive Service Provider","abstract":"This paper aims to bridge between the current scientific discourse about the dynamics of data communities in research infrastructures and practical experiences at a data archive which provides services for such data communities. We describe and analyse policies and practices within DANS-KNAW, the Dutch national centre of expertise and repository for research data concerning the interaction with communities in general. We take the case of the emerging DANS Data Station Life Sciences to study how a data archive navigates between observation of data research needs and anticipation of research data archival solutions. This paper offers a unique view of the complex dynamics between data communities (including lay experts) and data service providers. It adds nuances to understanding the emergence of a data community and the role of data service providers, both supporting and shaping, in this process.","sentences":["This paper aims to bridge between the current scientific discourse about the dynamics of data communities in research infrastructures and practical experiences at a data archive which provides services for such data communities.","We describe and analyse policies and practices within DANS-KNAW, the Dutch national centre of expertise and repository for research data concerning the interaction with communities in general.","We take the case of the emerging DANS Data Station Life Sciences to study how a data archive navigates between observation of data research needs and anticipation of research data archival solutions.","This paper offers a unique view of the complex dynamics between data communities (including lay experts) and data service providers.","It adds nuances to understanding the emergence of a data community and the role of data service providers, both supporting and shaping, in this process."],"url":"http://arxiv.org/abs/2502.02321v1"}
{"created":"2025-02-04 13:37:14","title":"DIME:Diffusion-Based Maximum Entropy Reinforcement Learning","abstract":"Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges--primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). DIME leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.","sentences":["Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties.","Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity.","Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges--primarily due to the intractability of computing their marginal entropy.","To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME).","DIME leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective.","Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy.","Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks.","It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity."],"url":"http://arxiv.org/abs/2502.02316v1"}
{"created":"2025-02-04 13:36:54","title":"VaiBot: Shuttle Between the Instructions and Parameters","abstract":"How to interact with LLMs through \\emph{instructions} has been widely studied by researchers. However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two. This paper proposes a neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both deduction and induction tasks under LLMs. Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities. We finally synergistically integrate the deductive and inductive processes of VaiBot. Through T-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks. The code and data for this paper can be found at https://anonymous.4open.science/r/VaiBot-021F.","sentences":["How to interact with LLMs through \\emph{instructions} has been widely studied by researchers.","However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two.","This paper proposes a neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both deduction and induction tasks under LLMs.","Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities.","We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities.","We finally synergistically integrate the deductive and inductive processes of VaiBot.","Through T-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks.","The code and data for this paper can be found at https://anonymous.4open.science/r/VaiBot-021F."],"url":"http://arxiv.org/abs/2502.02315v1"}
{"created":"2025-02-04 13:24:23","title":"UniGaze: Towards Universal Gaze Estimation via Large-scale Pre-Training","abstract":"Despite decades of research on data collection and model architectures, current gaze estimation models face significant challenges in generalizing across diverse data domains. While recent advances in self-supervised pre-training have shown remarkable potential for improving model generalization in various vision tasks, their effectiveness in gaze estimation remains unexplored due to the geometric nature of the gaze regression task. We propose UniGaze, which leverages large-scale, in-the-wild facial datasets through self-supervised pre-training for gaze estimation. We carefully curate multiple facial datasets that capture diverse variations in identity, lighting, background, and head poses. By directly applying Masked Autoencoder (MAE) pre-training on normalized face images with a Vision Transformer (ViT) backbone, our UniGaze learns appropriate feature representations within the specific input space required by downstream gaze estimation models. Through comprehensive experiments using challenging cross-dataset evaluation and novel protocols, including leave-one-dataset-out and joint-dataset settings, we demonstrate that UniGaze significantly improves generalization across multiple data domains while minimizing reliance on costly labeled data. The source code and pre-trained models will be released upon acceptance.","sentences":["Despite decades of research on data collection and model architectures, current gaze estimation models face significant challenges in generalizing across diverse data domains.","While recent advances in self-supervised pre-training have shown remarkable potential for improving model generalization in various vision tasks, their effectiveness in gaze estimation remains unexplored due to the geometric nature of the gaze regression task.","We propose UniGaze, which leverages large-scale, in-the-wild facial datasets through self-supervised pre-training for gaze estimation.","We carefully curate multiple facial datasets that capture diverse variations in identity, lighting, background, and head poses.","By directly applying Masked Autoencoder (MAE) pre-training on normalized face images with a Vision Transformer (ViT) backbone, our UniGaze learns appropriate feature representations within the specific input space required by downstream gaze estimation models.","Through comprehensive experiments using challenging cross-dataset evaluation and novel protocols, including leave-one-dataset-out and joint-dataset settings, we demonstrate that UniGaze significantly improves generalization across multiple data domains while minimizing reliance on costly labeled data.","The source code and pre-trained models will be released upon acceptance."],"url":"http://arxiv.org/abs/2502.02307v1"}
{"created":"2025-02-04 13:16:54","title":"EdgeGFL: Rethinking Edge Information in Graph Feature Preference Learning","abstract":"Graph Neural Networks (GNNs) have significant advantages in handling non-Euclidean data and have been widely applied across various areas, thus receiving increasing attention in recent years. The framework of GNN models mainly includes the information propagation phase and the aggregation phase, treating nodes and edges as information entities and propagation channels, respectively. However, most existing GNN models face the challenge of disconnection between node and edge feature information, as these models typically treat the learning of edge and node features as independent tasks. To address this limitation, we aim to develop an edge-empowered graph feature preference learning framework that can capture edge embeddings to assist node embeddings. By leveraging the learned multidimensional edge feature matrix, we construct multi-channel filters to more effectively capture accurate node features, thereby obtaining the non-local structural characteristics and fine-grained high-order node features. Specifically, the inclusion of multidimensional edge information enhances the functionality and flexibility of the GNN model, enabling it to handle complex and diverse graph data more effectively. Additionally, integrating relational representation learning into the message passing framework allows graph nodes to receive more useful information, thereby facilitating node representation learning. Finally, experiments on four real-world heterogeneous graphs demonstrate the effectiveness of theproposed model.","sentences":["Graph Neural Networks (GNNs) have significant advantages in handling non-Euclidean data and have been widely applied across various areas, thus receiving increasing attention in recent years.","The framework of GNN models mainly includes the information propagation phase and the aggregation phase, treating nodes and edges as information entities and propagation channels, respectively.","However, most existing GNN models face the challenge of disconnection between node and edge feature information, as these models typically treat the learning of edge and node features as independent tasks.","To address this limitation, we aim to develop an edge-empowered graph feature preference learning framework that can capture edge embeddings to assist node embeddings.","By leveraging the learned multidimensional edge feature matrix, we construct multi-channel filters to more effectively capture accurate node features, thereby obtaining the non-local structural characteristics and fine-grained high-order node features.","Specifically, the inclusion of multidimensional edge information enhances the functionality and flexibility of the GNN model, enabling it to handle complex and diverse graph data more effectively.","Additionally, integrating relational representation learning into the message passing framework allows graph nodes to receive more useful information, thereby facilitating node representation learning.","Finally, experiments on four real-world heterogeneous graphs demonstrate the effectiveness of theproposed model."],"url":"http://arxiv.org/abs/2502.02302v1"}
{"created":"2025-02-04 13:10:28","title":"Flow Graph-Based Classification of Defects4J Faults","abstract":"Software fault datasets such as Defects4J provide for each individual fault its location and repair, but do not characterize the faults. Current classifications use the repairs as proxies, which does not capture the intrinsic nature of the fault. In this paper, we propose a new, direct fault classification scheme based on the control- and data-flow graph representations of the program. Our scheme comprises six control-flow and two data-flow fault classes. We apply this to 488 faults from seven projects in the Defects4J dataset. We find that one of the data-flow fault classes (definition fault) is the most common individual class but that the majority of faults are classified with at least one control-flow fault class. The majority of the faults are assigned between one and three classes. Our proposed classification can be applied to other fault datasets and can be used to improve fault localization and automated program repair techniques for specific fault classes.","sentences":["Software fault datasets such as Defects4J provide for each individual fault its location and repair, but do not characterize the faults.","Current classifications use the repairs as proxies, which does not capture the intrinsic nature of the fault.","In this paper, we propose a new, direct fault classification scheme based on the control- and data-flow graph representations of the program.","Our scheme comprises six control-flow and two data-flow fault classes.","We apply this to 488 faults from seven projects in the Defects4J dataset.","We find that one of the data-flow fault classes (definition fault) is the most common individual class but that the majority of faults are classified with at least one control-flow fault class.","The majority of the faults are assigned between one and three classes.","Our proposed classification can be applied to other fault datasets and can be used to improve fault localization and automated program repair techniques for specific fault classes."],"url":"http://arxiv.org/abs/2502.02299v1"}
{"created":"2025-02-04 12:59:35","title":"FRAUD-RLA: A new reinforcement learning adversarial attack against credit card fraud detection","abstract":"Adversarial attacks pose a significant threat to data-driven systems, and researchers have spent considerable resources studying them. Despite its economic relevance, this trend largely overlooked the issue of credit card fraud detection. To address this gap, we propose a new threat model that demonstrates the limitations of existing attacks and highlights the necessity to investigate new approaches. We then design a new adversarial attack for credit card fraud detection, employing reinforcement learning to bypass classifiers. This attack, called FRAUD-RLA, is designed to maximize the attacker's reward by optimizing the exploration-exploitation tradeoff and working with significantly less required knowledge than competitors. Our experiments, conducted on three different heterogeneous datasets and against two fraud detection systems, indicate that FRAUD-RLA is effective, even considering the severe limitations imposed by our threat model.","sentences":["Adversarial attacks pose a significant threat to data-driven systems, and researchers have spent considerable resources studying them.","Despite its economic relevance, this trend largely overlooked the issue of credit card fraud detection.","To address this gap, we propose a new threat model that demonstrates the limitations of existing attacks and highlights the necessity to investigate new approaches.","We then design a new adversarial attack for credit card fraud detection, employing reinforcement learning to bypass classifiers.","This attack, called FRAUD-RLA, is designed to maximize the attacker's reward by optimizing the exploration-exploitation tradeoff and working with significantly less required knowledge than competitors.","Our experiments, conducted on three different heterogeneous datasets and against two fraud detection systems, indicate that FRAUD-RLA is effective, even considering the severe limitations imposed by our threat model."],"url":"http://arxiv.org/abs/2502.02290v1"}
{"created":"2025-02-04 12:40:07","title":"Error Distribution Smoothing:Advancing Low-Dimensional Imbalanced Regression","abstract":"In real-world regression tasks, datasets frequently exhibit imbalanced distributions, characterized by a scarcity of data in high-complexity regions and an abundance in low-complexity areas. This imbalance presents significant challenges for existing classification methods with clear class boundaries, while highlighting a scarcity of approaches specifically designed for imbalanced regression problems. To better address these issues, we introduce a novel concept of Imbalanced Regression, which takes into account both the complexity of the problem and the density of data points, extending beyond traditional definitions that focus only on data density. Furthermore, we propose Error Distribution Smoothing (EDS) as a solution to tackle imbalanced regression, effectively selecting a representative subset from the dataset to reduce redundancy while maintaining balance and representativeness. Through several experiments, EDS has shown its effectiveness, and the related code and dataset can be accessed at https://anonymous.4open.science/r/Error-Distribution-Smoothing-762F.","sentences":["In real-world regression tasks, datasets frequently exhibit imbalanced distributions, characterized by a scarcity of data in high-complexity regions and an abundance in low-complexity areas.","This imbalance presents significant challenges for existing classification methods with clear class boundaries, while highlighting a scarcity of approaches specifically designed for imbalanced regression problems.","To better address these issues, we introduce a novel concept of Imbalanced Regression, which takes into account both the complexity of the problem and the density of data points, extending beyond traditional definitions that focus only on data density.","Furthermore, we propose Error Distribution Smoothing (EDS) as a solution to tackle imbalanced regression, effectively selecting a representative subset from the dataset to reduce redundancy while maintaining balance and representativeness.","Through several experiments, EDS has shown its effectiveness, and the related code and dataset can be accessed at https://anonymous.4open.science/r/Error-Distribution-Smoothing-762F."],"url":"http://arxiv.org/abs/2502.02277v1"}
{"created":"2025-02-04 12:35:40","title":"A User Guide to Sampling Strategies for Sliced Optimal Transport","abstract":"This paper serves as a user guide to sampling strategies for sliced optimal transport. We provide reminders and additional regularity results on the Sliced Wasserstein distance. We detail the construction methods, generation time complexity, theoretical guarantees, and conditions for each strategy. Additionally, we provide insights into their suitability for sliced optimal transport in theory. Extensive experiments on both simulated and real-world data offer a representative comparison of the strategies, culminating in practical recommendations for their best usage.","sentences":["This paper serves as a user guide to sampling strategies for sliced optimal transport.","We provide reminders and additional regularity results on the Sliced Wasserstein distance.","We detail the construction methods, generation time complexity, theoretical guarantees, and conditions for each strategy.","Additionally, we provide insights into their suitability for sliced optimal transport in theory.","Extensive experiments on both simulated and real-world data offer a representative comparison of the strategies, culminating in practical recommendations for their best usage."],"url":"http://arxiv.org/abs/2502.02275v1"}
{"created":"2025-02-04 12:08:20","title":"UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic Segmentation","abstract":"Pre-training techniques significantly enhance the performance of semantic segmentation tasks with limited training data. However, the efficacy under a large domain gap between pre-training (e.g. RGB) and fine-tuning (e.g. infrared) remains underexplored. In this study, we first benchmark the infrared semantic segmentation performance of various pre-training methods and reveal several phenomena distinct from the RGB domain. Next, our layerwise analysis of pre-trained attention maps uncovers that: (1) There are three typical attention patterns (local, hybrid, and global); (2) Pre-training tasks notably influence the pattern distribution across layers; (3) The hybrid pattern is crucial for semantic segmentation as it attends to both nearby and foreground elements; (4) The texture bias impedes model generalization in infrared tasks. Building on these insights, we propose UNIP, a UNified Infrared Pre-training framework, to enhance the pre-trained model performance. This framework uses the hybrid-attention distillation NMI-HAD as the pre-training target, a large-scale mixed dataset InfMix for pre-training, and a last-layer feature pyramid network LL-FPN for fine-tuning. Experimental results show that UNIP outperforms various pre-training methods by up to 13.5\\% in average mIoU on three infrared segmentation tasks, evaluated using fine-tuning and linear probing metrics. UNIP-S achieves performance on par with MAE-L while requiring only 1/10 of the computational cost. Furthermore, UNIP significantly surpasses state-of-the-art (SOTA) infrared or RGB segmentation methods and demonstrates broad potential for application in other modalities, such as RGB and depth. Our code is available at https://github.com/casiatao/UNIP.","sentences":["Pre-training techniques significantly enhance the performance of semantic segmentation tasks with limited training data.","However, the efficacy under a large domain gap between pre-training (e.g. RGB) and fine-tuning (e.g. infrared) remains underexplored.","In this study, we first benchmark the infrared semantic segmentation performance of various pre-training methods and reveal several phenomena distinct from the RGB domain.","Next, our layerwise analysis of pre-trained attention maps uncovers that: (1) There are three typical attention patterns (local, hybrid, and global); (2) Pre-training tasks notably influence the pattern distribution across layers; (3) The hybrid pattern is crucial for semantic segmentation as it attends to both nearby and foreground elements; (4) The texture bias impedes model generalization in infrared tasks.","Building on these insights, we propose UNIP, a UNified Infrared Pre-training framework, to enhance the pre-trained model performance.","This framework uses the hybrid-attention distillation NMI-HAD as the pre-training target, a large-scale mixed dataset InfMix for pre-training, and a last-layer feature pyramid network LL-FPN for fine-tuning.","Experimental results show that UNIP outperforms various pre-training methods by up to 13.5\\% in average mIoU on three infrared segmentation tasks, evaluated using fine-tuning and linear probing metrics.","UNIP-S achieves performance on par with MAE-L while requiring only 1/10 of the computational cost.","Furthermore, UNIP significantly surpasses state-of-the-art (SOTA) infrared or RGB segmentation methods and demonstrates broad potential for application in other modalities, such as RGB and depth.","Our code is available at https://github.com/casiatao/UNIP."],"url":"http://arxiv.org/abs/2502.02257v1"}
{"created":"2025-02-04 11:59:58","title":"Network Digital Twin for 5G-Enabled Mobile Robots","abstract":"The maturity and commercial roll-out of 5G networks and its deployment for private networks makes 5G a key enabler for various vertical industries and applications, including robotics. Providing ultra-low latency, high data rates, and ubiquitous coverage and wireless connectivity, 5G fully unlocks the potential of robot autonomy and boosts emerging robotic applications, particularly in the domain of autonomous mobile robots. Ensuring seamless, efficient, and reliable navigation and operation of robots within a 5G network requires a clear understanding of the expected network quality in the deployment environment. However, obtaining real-time insights into network conditions, particularly in highly dynamic environments, presents a significant and practical challenge. In this paper, we present a novel framework for building a Network Digital Twin (NDT) using real-time data collected by robots. This framework provides a comprehensive solution for monitoring, controlling, and optimizing robotic operations in dynamic network environments. We develop a pipeline integrating robotic data into the NDT, demonstrating its evolution with real-world robotic traces. We evaluate its performances in radio-aware navigation use case, highlighting its potential to enhance energy efficiency and reliability for 5Genabled robotic operations.","sentences":["The maturity and commercial roll-out of 5G networks and its deployment for private networks makes 5G a key enabler for various vertical industries and applications, including robotics.","Providing ultra-low latency, high data rates, and ubiquitous coverage and wireless connectivity, 5G fully unlocks the potential of robot autonomy and boosts emerging robotic applications, particularly in the domain of autonomous mobile robots.","Ensuring seamless, efficient, and reliable navigation and operation of robots within a 5G network requires a clear understanding of the expected network quality in the deployment environment.","However, obtaining real-time insights into network conditions, particularly in highly dynamic environments, presents a significant and practical challenge.","In this paper, we present a novel framework for building a Network Digital Twin (NDT) using real-time data collected by robots.","This framework provides a comprehensive solution for monitoring, controlling, and optimizing robotic operations in dynamic network environments.","We develop a pipeline integrating robotic data into the NDT, demonstrating its evolution with real-world robotic traces.","We evaluate its performances in radio-aware navigation use case, highlighting its potential to enhance energy efficiency and reliability for 5Genabled robotic operations."],"url":"http://arxiv.org/abs/2502.02253v1"}
{"created":"2025-02-04 11:50:40","title":"Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation","abstract":"Large language models (LLMs) have shown impressive capabilities in natural language processing tasks, including dialogue generation. This research aims to conduct a novel comparative analysis of two prominent techniques, fine-tuning with LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG) framework, in the context of doctor-patient chat conversations with multiple datasets of mixed medical domains. The analysis involves three state-of-the-art models: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient dialogues, we comprehensively evaluate the performance of models, assessing key metrics such as language quality (perplexity, BLEU score), factual accuracy (fact-checking against medical knowledge bases), adherence to medical guidelines, and overall human judgments (coherence, empathy, safety). The findings provide insights into the strengths and limitations of each approach, shedding light on their suitability for healthcare applications. Furthermore, the research investigates the robustness of the models in handling diverse patient queries, ranging from general health inquiries to specific medical conditions. The impact of domain-specific knowledge integration is also explored, highlighting the potential for enhancing LLM performance through targeted data augmentation and retrieval strategies.","sentences":["Large language models (LLMs) have shown impressive capabilities in natural language processing tasks, including dialogue generation.","This research aims to conduct a novel comparative analysis of two prominent techniques, fine-tuning with LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG) framework, in the context of doctor-patient chat conversations with multiple datasets of mixed medical domains.","The analysis involves three state-of-the-art models: Llama-2, GPT, and the LSTM model.","Employing real-world doctor-patient dialogues, we comprehensively evaluate the performance of models, assessing key metrics such as language quality (perplexity, BLEU score), factual accuracy (fact-checking against medical knowledge bases), adherence to medical guidelines, and overall human judgments (coherence, empathy, safety).","The findings provide insights into the strengths and limitations of each approach, shedding light on their suitability for healthcare applications.","Furthermore, the research investigates the robustness of the models in handling diverse patient queries, ranging from general health inquiries to specific medical conditions.","The impact of domain-specific knowledge integration is also explored, highlighting the potential for enhancing LLM performance through targeted data augmentation and retrieval strategies."],"url":"http://arxiv.org/abs/2502.02249v1"}
{"created":"2025-02-04 11:23:48","title":"Mask-informed Deep Contrastive Incomplete Multi-view Clustering","abstract":"Multi-view clustering (MvC) utilizes information from multiple views to uncover the underlying structures of data. Despite significant advancements in MvC, mitigating the impact of missing samples in specific views on the integration of knowledge from different views remains a critical challenge. This paper proposes a novel Mask-informed Deep Contrastive Incomplete Multi-view Clustering (Mask-IMvC) method, which elegantly identifies a view-common representation for clustering. Specifically, we introduce a mask-informed fusion network that aggregates incomplete multi-view information while considering the observation status of samples across various views as a mask, thereby reducing the adverse effects of missing values. Additionally, we design a prior knowledge-assisted contrastive learning loss that boosts the representation capability of the aggregated view-common representation by injecting neighborhood information of samples from different views. Finally, extensive experiments are conducted to demonstrate the superiority of the proposed Mask-IMvC method over state-of-the-art approaches across multiple MvC datasets, both in complete and incomplete scenarios.","sentences":["Multi-view clustering (MvC) utilizes information from multiple views to uncover the underlying structures of data.","Despite significant advancements in MvC, mitigating the impact of missing samples in specific views on the integration of knowledge from different views remains a critical challenge.","This paper proposes a novel Mask-informed Deep Contrastive Incomplete Multi-view Clustering (Mask-IMvC) method, which elegantly identifies a view-common representation for clustering.","Specifically, we introduce a mask-informed fusion network that aggregates incomplete multi-view information while considering the observation status of samples across various views as a mask, thereby reducing the adverse effects of missing values.","Additionally, we design a prior knowledge-assisted contrastive learning loss that boosts the representation capability of the aggregated view-common representation by injecting neighborhood information of samples from different views.","Finally, extensive experiments are conducted to demonstrate the superiority of the proposed Mask-IMvC method over state-of-the-art approaches across multiple MvC datasets, both in complete and incomplete scenarios."],"url":"http://arxiv.org/abs/2502.02234v1"}
{"created":"2025-02-04 11:10:34","title":"A Robust Remote Photoplethysmography Method","abstract":"Remote photoplethysmography (rPPG) is a method for measuring a subjects heart rate remotely using a camera. Factors such as subject movement, ambient light level, makeup etc. complicate such measurements by distorting the observed pulse. Recent works on this topic have proposed a variety of approaches for accurately measuring heart rate in humans, however these methods were tested in ideal conditions, where the subject does not make significant movements and all measurements are taken at the same level of illumination. In more realistic conditions these methods suffer from decreased accuracy. The study proposes a more robust method that is less susceptible to distortions and has minimal hardware requirements. The proposed method uses a combination of mathematical transforms to calculate the subjects heart rate. It performs best when used with a camera that has been modified by removing its infrared filter, although using an unmodified camera is also possible. The method was tested on 26 videos taken from 19 volunteers of varying gender and age. The obtained results were compared to reference data and the average mean absolute error was found to be at 1.95 beats per minute, which is noticeably better than the results from previous works. The remote photoplethysmography method proposed in the present article is more resistant to distortions than methods from previous publications and thus allows one to remotely and accurately measure the subjects heart rate without imposing any significant limitations on the subjects behavior.","sentences":["Remote photoplethysmography (rPPG) is a method for measuring a subjects heart rate remotely using a camera.","Factors such as subject movement, ambient light level, makeup etc. complicate such measurements by distorting the observed pulse.","Recent works on this topic have proposed a variety of approaches for accurately measuring heart rate in humans, however these methods were tested in ideal conditions, where the subject does not make significant movements and all measurements are taken at the same level of illumination.","In more realistic conditions these methods suffer from decreased accuracy.","The study proposes a more robust method that is less susceptible to distortions and has minimal hardware requirements.","The proposed method uses a combination of mathematical transforms to calculate the subjects heart rate.","It performs best when used with a camera that has been modified by removing its infrared filter, although using an unmodified camera is also possible.","The method was tested on 26 videos taken from 19 volunteers of varying gender and age.","The obtained results were compared to reference data and the average mean absolute error was found to be at 1.95 beats per minute, which is noticeably better than the results from previous works.","The remote photoplethysmography method proposed in the present article is more resistant to distortions than methods from previous publications and thus allows one to remotely and accurately measure the subjects heart rate without imposing any significant limitations on the subjects behavior."],"url":"http://arxiv.org/abs/2502.02229v1"}
{"created":"2025-02-04 11:04:36","title":"Exploring the latent space of diffusion models directly through singular value decomposition","abstract":"Despite the groundbreaking success of diffusion models in generating high-fidelity images, their latent space remains relatively under-explored, even though it holds significant promise for enabling versatile and interpretable image editing capabilities. The complicated denoising trajectory and high dimensionality of the latent space make it extremely challenging to interpret. Existing methods mainly explore the feature space of U-Net in Diffusion Models (DMs) instead of the latent space itself. In contrast, we directly investigate the latent space via Singular Value Decomposition (SVD) and discover three useful properties that can be used to control generation results without the requirements of data collection and maintain identity fidelity generated images. Based on these properties, we propose a novel image editing framework that is capable of learning arbitrary attributes from one pair of latent codes destined by text prompts in Stable Diffusion Models. To validate our approach, extensive experiments are conducted to demonstrate its effectiveness and flexibility in image editing. We will release our codes soon to foster further research and applications in this area.","sentences":["Despite the groundbreaking success of diffusion models in generating high-fidelity images, their latent space remains relatively under-explored, even though it holds significant promise for enabling versatile and interpretable image editing capabilities.","The complicated denoising trajectory and high dimensionality of the latent space make it extremely challenging to interpret.","Existing methods mainly explore the feature space of U-Net in Diffusion Models (DMs) instead of the latent space itself.","In contrast, we directly investigate the latent space via Singular Value Decomposition (SVD) and discover three useful properties that can be used to control generation results without the requirements of data collection and maintain identity fidelity generated images.","Based on these properties, we propose a novel image editing framework that is capable of learning arbitrary attributes from one pair of latent codes destined by text prompts in Stable Diffusion Models.","To validate our approach, extensive experiments are conducted to demonstrate its effectiveness and flexibility in image editing.","We will release our codes soon to foster further research and applications in this area."],"url":"http://arxiv.org/abs/2502.02225v1"}
{"created":"2025-02-04 11:01:03","title":"Bias Detection via Maximum Subgroup Discrepancy","abstract":"Bias evaluation is fundamental to trustworthy AI, both in terms of checking data quality and in terms of checking the outputs of AI systems. In testing data quality, for example, one may study a distance of a given dataset, viewed as a distribution, to a given ground-truth reference dataset. However, classical metrics, such as the Total Variation and the Wasserstein distances, are known to have high sample complexities and, therefore, may fail to provide meaningful distinction in many practical scenarios.   In this paper, we propose a new notion of distance, the Maximum Subgroup Discrepancy (MSD). In this metric, two distributions are close if, roughly, discrepancies are low for all feature subgroups. While the number of subgroups may be exponential, we show that the sample complexity is linear in the number of features, thus making it feasible for practical applications. Moreover, we provide a practical algorithm for the evaluation of the distance, based on Mixed-integer optimization (MIO). We also note that the proposed distance is easily interpretable, thus providing clearer paths to fixing the biases once they have been identified. It also provides guarantees for all subgroups. Finally, we empirically evaluate, compare with other metrics, and demonstrate the above properties of MSD on real-world datasets.","sentences":["Bias evaluation is fundamental to trustworthy AI, both in terms of checking data quality and in terms of checking the outputs of AI systems.","In testing data quality, for example, one may study a distance of a given dataset, viewed as a distribution, to a given ground-truth reference dataset.","However, classical metrics, such as the Total Variation and the Wasserstein distances, are known to have high sample complexities and, therefore, may fail to provide meaningful distinction in many practical scenarios.   ","In this paper, we propose a new notion of distance, the Maximum Subgroup Discrepancy (MSD).","In this metric, two distributions are close if, roughly, discrepancies are low for all feature subgroups.","While the number of subgroups may be exponential, we show that the sample complexity is linear in the number of features, thus making it feasible for practical applications.","Moreover, we provide a practical algorithm for the evaluation of the distance, based on Mixed-integer optimization (MIO).","We also note that the proposed distance is easily interpretable, thus providing clearer paths to fixing the biases once they have been identified.","It also provides guarantees for all subgroups.","Finally, we empirically evaluate, compare with other metrics, and demonstrate the above properties of MSD on real-world datasets."],"url":"http://arxiv.org/abs/2502.02221v1"}
{"created":"2025-02-04 10:51:20","title":"InterLCM: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration","abstract":"Diffusion priors have been used for blind face restoration (BFR) by fine-tuning diffusion models (DMs) on restoration datasets to recover low-quality images. However, the naive application of DMs presents several key limitations. (i) The diffusion prior has inferior semantic consistency (e.g., ID, structure and color.), increasing the difficulty of optimizing the BFR model; (ii) reliance on hundreds of denoising iterations, preventing the effective cooperation with perceptual losses, which is crucial for faithful restoration. Observing that the latent consistency model (LCM) learns consistency noise-to-data mappings on the ODE-trajectory and therefore shows more semantic consistency in the subject identity, structural information and color preservation, we propose InterLCM to leverage the LCM for its superior semantic consistency and efficiency to counter the above issues. Treating low-quality images as the intermediate state of LCM, InterLCM achieves a balance between fidelity and quality by starting from earlier LCM steps. LCM also allows the integration of perceptual loss during training, leading to improved restoration quality, particularly in real-world scenarios. To mitigate structural and semantic uncertainties, InterLCM incorporates a Visual Module to extract visual features and a Spatial Encoder to capture spatial details, enhancing the fidelity of restored images. Extensive experiments demonstrate that InterLCM outperforms existing approaches in both synthetic and real-world datasets while also achieving faster inference speed.","sentences":["Diffusion priors have been used for blind face restoration (BFR) by fine-tuning diffusion models (DMs) on restoration datasets to recover low-quality images.","However, the naive application of DMs presents several key limitations.","(i) The diffusion prior has inferior semantic consistency (e.g., ID, structure and color.), increasing the difficulty of optimizing the BFR model; (ii) reliance on hundreds of denoising iterations, preventing the effective cooperation with perceptual losses, which is crucial for faithful restoration.","Observing that the latent consistency model (LCM) learns consistency noise-to-data mappings on the ODE-trajectory and therefore shows more semantic consistency in the subject identity, structural information and color preservation, we propose InterLCM to leverage the LCM for its superior semantic consistency and efficiency to counter the above issues.","Treating low-quality images as the intermediate state of LCM, InterLCM achieves a balance between fidelity and quality by starting from earlier LCM steps.","LCM also allows the integration of perceptual loss during training, leading to improved restoration quality, particularly in real-world scenarios.","To mitigate structural and semantic uncertainties, InterLCM incorporates a Visual Module to extract visual features and a Spatial Encoder to capture spatial details, enhancing the fidelity of restored images.","Extensive experiments demonstrate that InterLCM outperforms existing approaches in both synthetic and real-world datasets while also achieving faster inference speed."],"url":"http://arxiv.org/abs/2502.02215v1"}
{"created":"2025-02-04 10:27:54","title":"Multi-level Supervised Contrastive Learning","abstract":"Contrastive learning is a well-established paradigm in representation learning. The standard framework of contrastive learning minimizes the distance between \"similar\" instances and maximizes the distance between dissimilar ones in the projection space, disregarding the various aspects of similarity that can exist between two samples. Current methods rely on a single projection head, which fails to capture the full complexity of different aspects of a sample, leading to suboptimal performance, especially in scenarios with limited training data. In this paper, we present a novel supervised contrastive learning method in a unified framework called multilevel contrastive learning (MLCL), that can be applied to both multi-label and hierarchical classification tasks. The key strength of the proposed method is the ability to capture similarities between samples across different labels and/or hierarchies using multiple projection heads. Extensive experiments on text and image datasets demonstrate that the proposed approach outperforms state-of-the-art contrastive learning methods","sentences":["Contrastive learning is a well-established paradigm in representation learning.","The standard framework of contrastive learning minimizes the distance between \"similar\" instances and maximizes the distance between dissimilar ones in the projection space, disregarding the various aspects of similarity that can exist between two samples.","Current methods rely on a single projection head, which fails to capture the full complexity of different aspects of a sample, leading to suboptimal performance, especially in scenarios with limited training data.","In this paper, we present a novel supervised contrastive learning method in a unified framework called multilevel contrastive learning (MLCL), that can be applied to both multi-label and hierarchical classification tasks.","The key strength of the proposed method is the ability to capture similarities between samples across different labels and/or hierarchies using multiple projection heads.","Extensive experiments on text and image datasets demonstrate that the proposed approach outperforms state-of-the-art contrastive learning methods"],"url":"http://arxiv.org/abs/2502.02202v1"}
{"created":"2025-02-04 10:23:11","title":"When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks","abstract":"Large language models (LLMs) have shown remarkable success in language modelling due to scaling laws found in model size and the hidden dimension of the model's text representation. Yet, we demonstrate that compressed representations of text can yield better performance in LLM-based regression tasks. In this paper, we compare the relative performance of embedding compression in three different signal-to-noise contexts: financial return prediction, writing quality assessment and review scoring. Our results show that compressing embeddings, in a minimally supervised manner using an autoencoder's hidden representation, can mitigate overfitting and improve performance on noisy tasks, such as financial return prediction; but that compression reduces performance on tasks that have high causal dependencies between the input and target data. Our results suggest that the success of interpretable compressed representations such as sentiment may be due to a regularising effect.","sentences":["Large language models (LLMs) have shown remarkable success in language modelling due to scaling laws found in model size and the hidden dimension of the model's text representation.","Yet, we demonstrate that compressed representations of text can yield better performance in LLM-based regression tasks.","In this paper, we compare the relative performance of embedding compression in three different signal-to-noise contexts: financial return prediction, writing quality assessment and review scoring.","Our results show that compressing embeddings, in a minimally supervised manner using an autoencoder's hidden representation, can mitigate overfitting and improve performance on noisy tasks, such as financial return prediction; but that compression reduces performance on tasks that have high causal dependencies between the input and target data.","Our results suggest that the success of interpretable compressed representations such as sentiment may be due to a regularising effect."],"url":"http://arxiv.org/abs/2502.02199v1"}
{"created":"2025-02-04 10:19:11","title":"Extending the Applicability of Bloom Filters by Relaxing their Parameter Constraints","abstract":"These days, Key-Value Stores are widely used for scalable data storage. In this environment, Bloom filters serve as an efficient probabilistic data structure for the representation of sets of keys as they allow for set membership queries with controllable false positive rates and no false negatives. For optimal error rates, the right choice of the main parameters, namely the length of the Bloom filter array, the number of hash functions used to map an element to the array's indices, and the number of elements to be inserted in one filter, is crucial. However, these parameters are constrained: The number of hash functions is bounded to integer values, and the length of a Bloom filter is usually chosen to be a power-of-two to allow for efficient modulo operations using binary arithmetics. These modulo calculations are necessary to map from the output universe of the applied universal hash functions, like Murmur, to the set of indices of the Bloom filter. In this paper, we relax these constraints by proposing the Rational Bloom filter, which allows for non-integer numbers of hash functions. This results in optimized fraction-of-zero values for a known number of elements to be inserted. Based on this, we construct the Variably-Sized Block Bloom filters to allow for a flexible filter length, especially for large filters, while keeping computation efficient.","sentences":["These days, Key-Value Stores are widely used for scalable data storage.","In this environment, Bloom filters serve as an efficient probabilistic data structure for the representation of sets of keys as they allow for set membership queries with controllable false positive rates and no false negatives.","For optimal error rates, the right choice of the main parameters, namely the length of the Bloom filter array, the number of hash functions used to map an element to the array's indices, and the number of elements to be inserted in one filter, is crucial.","However, these parameters are constrained: The number of hash functions is bounded to integer values, and the length of a Bloom filter is usually chosen to be a power-of-two to allow for efficient modulo operations using binary arithmetics.","These modulo calculations are necessary to map from the output universe of the applied universal hash functions, like Murmur, to the set of indices of the Bloom filter.","In this paper, we relax these constraints by proposing the Rational Bloom filter, which allows for non-integer numbers of hash functions.","This results in optimized fraction-of-zero values for a known number of elements to be inserted.","Based on this, we construct the Variably-Sized Block Bloom filters to allow for a flexible filter length, especially for large filters, while keeping computation efficient."],"url":"http://arxiv.org/abs/2502.02193v1"}
{"created":"2025-02-04 10:13:14","title":"Large language models in climate and sustainability policy: limits and opportunities","abstract":"As multiple crises threaten the sustainability of our societies and pose at risk the planetary boundaries, complex challenges require timely, updated, and usable information. Natural-language processing (NLP) tools enhance and expand data collection and processing and knowledge utilization capabilities to support the definition of an inclusive, sustainable future. In this work, we apply different NLP techniques, tools and approaches to climate and sustainability documents to derive policy-relevant and actionable measures. We focus on general and domain-specific large language models (LLMs) using a combination of static and prompt-based methods. We find that the use of LLMs is successful at processing, classifying and summarizing heterogeneous text-based data. However, we also encounter challenges related to human intervention across different workflow stages and knowledge utilization for policy processes. Our work presents a critical but empirically grounded application of LLMs to complex policy problems and suggests avenues to further expand Artificial Intelligence-powered computational social sciences.","sentences":["As multiple crises threaten the sustainability of our societies and pose at risk the planetary boundaries, complex challenges require timely, updated, and usable information.","Natural-language processing (NLP) tools enhance and expand data collection and processing and knowledge utilization capabilities to support the definition of an inclusive, sustainable future.","In this work, we apply different NLP techniques, tools and approaches to climate and sustainability documents to derive policy-relevant and actionable measures.","We focus on general and domain-specific large language models (LLMs) using a combination of static and prompt-based methods.","We find that the use of LLMs is successful at processing, classifying and summarizing heterogeneous text-based data.","However, we also encounter challenges related to human intervention across different workflow stages and knowledge utilization for policy processes.","Our work presents a critical but empirically grounded application of LLMs to complex policy problems and suggests avenues to further expand Artificial Intelligence-powered computational social sciences."],"url":"http://arxiv.org/abs/2502.02191v1"}
{"created":"2025-02-04 10:09:47","title":"deCIFer: Crystal Structure Prediction from Powder Diffraction Data using Autoregressive Language Models","abstract":"Novel materials drive progress across applications from energy storage to electronics. Automated characterization of material structures with machine learning methods offers a promising strategy for accelerating this key step in material design. In this work, we introduce an autoregressive language model that performs crystal structure prediction (CSP) from powder diffraction data. The presented model, deCIFer, generates crystal structures in the widely used Crystallographic Information File (CIF) format and can be conditioned on powder X-ray diffraction (PXRD) data. Unlike earlier works that primarily rely on high-level descriptors like composition, deCIFer performs CSP from diffraction data. We train deCIFer on nearly 2.3M unique crystal structures and validate on diverse sets of PXRD patterns for characterizing challenging inorganic crystal systems. Qualitative and quantitative assessments using the residual weighted profile and Wasserstein distance show that deCIFer produces structures that more accurately match the target diffraction data when conditioned, compared to the unconditioned case. Notably, deCIFer can achieve a 94% match rate on unseen data. deCIFer bridges experimental diffraction data with computational CSP, lending itself as a powerful tool for crystal structure characterization and accelerating materials discovery.","sentences":["Novel materials drive progress across applications from energy storage to electronics.","Automated characterization of material structures with machine learning methods offers a promising strategy for accelerating this key step in material design.","In this work, we introduce an autoregressive language model that performs crystal structure prediction (CSP) from powder diffraction data.","The presented model, deCIFer, generates crystal structures in the widely used Crystallographic Information File (CIF) format and can be conditioned on powder X-ray diffraction (PXRD) data.","Unlike earlier works that primarily rely on high-level descriptors like composition, deCIFer performs CSP from diffraction data.","We train deCIFer on nearly 2.3M unique crystal structures and validate on diverse sets of PXRD patterns for characterizing challenging inorganic crystal systems.","Qualitative and quantitative assessments using the residual weighted profile and Wasserstein distance show that deCIFer produces structures that more accurately match the target diffraction data when conditioned, compared to the unconditioned case.","Notably, deCIFer can achieve a 94% match rate on unseen data.","deCIFer bridges experimental diffraction data with computational CSP, lending itself as a powerful tool for crystal structure characterization and accelerating materials discovery."],"url":"http://arxiv.org/abs/2502.02189v1"}
{"created":"2025-02-04 09:47:55","title":"Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge","abstract":"Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.","sentences":["Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks.","This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process.","Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications.","MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability.","Our code and data are at https://github.com/dtamayo-nlp/MEMAT."],"url":"http://arxiv.org/abs/2502.02173v1"}
{"created":"2025-02-04 09:45:49","title":"DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With Aerial Imaging","abstract":"Access to below-canopy volumetric vegetation data is crucial for understanding ecosystem dynamics. We address the long-standing limitation of remote sensing to penetrate deep into dense canopy layers. LiDAR and radar are currently considered the primary options for measuring 3D vegetation structures, while cameras can only extract the reflectance and depth of top layers. Using conventional, high-resolution aerial images, our approach allows sensing deep into self-occluding vegetation volumes, such as forests. It is similar in spirit to the imaging process of wide-field microscopy, but can handle much larger scales and strong occlusion. We scan focal stacks by synthetic-aperture imaging with drones and reduce out-of-focus signal contributions using pre-trained 3D convolutional neural networks. The resulting volumetric reflectance stacks contain low-frequency representations of the vegetation volume. Combining multiple reflectance stacks from various spectral channels provides insights into plant health, growth, and environmental conditions throughout the entire vegetation volume.","sentences":["Access to below-canopy volumetric vegetation data is crucial for understanding ecosystem dynamics.","We address the long-standing limitation of remote sensing to penetrate deep into dense canopy layers.","LiDAR and radar are currently considered the primary options for measuring 3D vegetation structures, while cameras can only extract the reflectance and depth of top layers.","Using conventional, high-resolution aerial images, our approach allows sensing deep into self-occluding vegetation volumes, such as forests.","It is similar in spirit to the imaging process of wide-field microscopy, but can handle much larger scales and strong occlusion.","We scan focal stacks by synthetic-aperture imaging with drones and reduce out-of-focus signal contributions using pre-trained 3D convolutional neural networks.","The resulting volumetric reflectance stacks contain low-frequency representations of the vegetation volume.","Combining multiple reflectance stacks from various spectral channels provides insights into plant health, growth, and environmental conditions throughout the entire vegetation volume."],"url":"http://arxiv.org/abs/2502.02171v1"}
{"created":"2025-02-04 09:43:40","title":"Multilingual Attribute Extraction from News Web Pages","abstract":"This paper addresses the challenge of automatically extracting attributes from news article web pages across multiple languages. Recent neural network models have shown high efficacy in extracting information from semi-structured web pages. However, these models are predominantly applied to domains like e-commerce and are pre-trained using English data, complicating their application to web pages in other languages. We prepared a multilingual dataset comprising 3,172 marked-up news web pages across six languages (English, German, Russian, Chinese, Korean, and Arabic) from 161 websites. The dataset is publicly available on GitHub. We fine-tuned the pre-trained state-of-the-art model, MarkupLM, to extract news attributes from these pages and evaluated the impact of translating pages into English on extraction quality. Additionally, we pre-trained another state-of-the-art model, DOM-LM, on multilingual data and fine-tuned it on our dataset. We compared both fine-tuned models to existing open-source news data extraction tools, achieving superior extraction metrics.","sentences":["This paper addresses the challenge of automatically extracting attributes from news article web pages across multiple languages.","Recent neural network models have shown high efficacy in extracting information from semi-structured web pages.","However, these models are predominantly applied to domains like e-commerce and are pre-trained using English data, complicating their application to web pages in other languages.","We prepared a multilingual dataset comprising 3,172 marked-up news web pages across six languages (English, German, Russian, Chinese, Korean, and Arabic) from 161 websites.","The dataset is publicly available on GitHub.","We fine-tuned the pre-trained state-of-the-art model, MarkupLM, to extract news attributes from these pages and evaluated the impact of translating pages into English on extraction quality.","Additionally, we pre-trained another state-of-the-art model, DOM-LM, on multilingual data and fine-tuned it on our dataset.","We compared both fine-tuned models to existing open-source news data extraction tools, achieving superior extraction metrics."],"url":"http://arxiv.org/abs/2502.02167v1"}
{"created":"2025-02-04 09:15:50","title":"Towards Efficient LUT-based PIM: A Scalable and Low-Power Approach for Modern Workloads","abstract":"Data movement in memory-intensive workloads, such as deep learning, incurs energy costs that are over three orders of magnitude higher than the cost of computation. Since these workloads involve frequent data transfers between memory and processing units, addressing data movement overheads is crucial for improving performance. Processing-using-memory (PuM) offers an effective solution by enabling in-memory computation, thereby minimizing data transfers. In this paper we propose Lama, a LUT-based PuM architecture designed to efficiently execute SIMD operations by supporting independent column accesses within each mat of a DRAM subarray. Lama exploits DRAM's mat-level parallelism and open-page policy to significantly reduce the number of energy-intensive memory activation (ACT) commands, which are the primary source of overhead in most PuM architectures. Unlike prior PuM solutions, Lama supports up to 8-bit operand precision without decomposing computations, while incurring only a 2.47% area overhead. Our evaluation shows Lama achieves an average performance improvement of 8.5x over state-of-the-art PuM architectures and a 3.8x improvement over CPU, along with energy efficiency gains of 6.9x/8x, respectively, for bulk 8-bit multiplication.   We also introduce LamaAccel, an HBM-based PuM accelerator that utilizes Lama to accelerate the inference of attention-based models. LamaAccel employs exponential quantization to optimize product/accumulation in dot-product operations, transforming them into simpler tasks like addition and counting. LamaAccel delivers up to 9.3x/19.2x reduction in energy and 4.8x/9.8x speedup over TPU/GPU, along with up to 5.8x energy reduction and 2.1x speedup over a state-of-the-art PuM baseline.","sentences":["Data movement in memory-intensive workloads, such as deep learning, incurs energy costs that are over three orders of magnitude higher than the cost of computation.","Since these workloads involve frequent data transfers between memory and processing units, addressing data movement overheads is crucial for improving performance.","Processing-using-memory (PuM) offers an effective solution by enabling in-memory computation, thereby minimizing data transfers.","In this paper we propose Lama, a LUT-based PuM architecture designed to efficiently execute SIMD operations by supporting independent column accesses within each mat of a DRAM subarray.","Lama exploits DRAM's mat-level parallelism and open-page policy to significantly reduce the number of energy-intensive memory activation (ACT) commands, which are the primary source of overhead in most PuM architectures.","Unlike prior PuM solutions, Lama supports up to 8-bit operand precision without decomposing computations, while incurring only a 2.47% area overhead.","Our evaluation shows Lama achieves an average performance improvement of 8.5x over state-of-the-art PuM architectures and a 3.8x improvement over CPU, along with energy efficiency gains of 6.9x/8x, respectively, for bulk 8-bit multiplication.   ","We also introduce LamaAccel, an HBM-based PuM accelerator that utilizes Lama to accelerate the inference of attention-based models.","LamaAccel employs exponential quantization to optimize product/accumulation in dot-product operations, transforming them into simpler tasks like addition and counting.","LamaAccel delivers up to 9.3x/19.2x reduction in energy and 4.8x/9.8x speedup over TPU/GPU, along with up to 5.8x energy reduction and 2.1x speedup over a state-of-the-art PuM baseline."],"url":"http://arxiv.org/abs/2502.02142v1"}
{"created":"2025-02-04 09:07:45","title":"Standard Neural Computation Alone Is Insufficient for Logical Intelligence","abstract":"Neural networks, as currently designed, fall short of achieving true logical intelligence. Modern AI models rely on standard neural computation-inner-product-based transformations and nonlinear activations-to approximate patterns from data. While effective for inductive learning, this architecture lacks the structural guarantees necessary for deductive inference and logical consistency. As a result, deep networks struggle with rule-based reasoning, structured generalization, and interpretability without extensive post-hoc modifications. This position paper argues that standard neural layers must be fundamentally rethought to integrate logical reasoning. We advocate for Logical Neural Units (LNUs)-modular components that embed differentiable approximations of logical operations (e.g., AND, OR, NOT) directly within neural architectures. We critique existing neurosymbolic approaches, highlight the limitations of standard neural computation for logical inference, and present LNUs as a necessary paradigm shift in AI. Finally, we outline a roadmap for implementation, discussing theoretical foundations, architectural integration, and key challenges for future research.","sentences":["Neural networks, as currently designed, fall short of achieving true logical intelligence.","Modern AI models rely on standard neural computation-inner-product-based transformations and nonlinear activations-to approximate patterns from data.","While effective for inductive learning, this architecture lacks the structural guarantees necessary for deductive inference and logical consistency.","As a result, deep networks struggle with rule-based reasoning, structured generalization, and interpretability without extensive post-hoc modifications.","This position paper argues that standard neural layers must be fundamentally rethought to integrate logical reasoning.","We advocate for Logical Neural Units (LNUs)-modular components that embed differentiable approximations of logical operations (e.g., AND, OR, NOT) directly within neural architectures.","We critique existing neurosymbolic approaches, highlight the limitations of standard neural computation for logical inference, and present LNUs as a necessary paradigm shift in AI.","Finally, we outline a roadmap for implementation, discussing theoretical foundations, architectural integration, and key challenges for future research."],"url":"http://arxiv.org/abs/2502.02135v1"}
{"created":"2025-02-04 09:03:21","title":"Deep Neural Cellular Potts Models","abstract":"The cellular Potts model (CPM) is a powerful computational method for simulating collective spatiotemporal dynamics of biological cells. To drive the dynamics, CPMs rely on physics-inspired Hamiltonians. However, as first principles remain elusive in biology, these Hamiltonians only approximate the full complexity of real multicellular systems. To address this limitation, we propose NeuralCPM, a more expressive cellular Potts model that can be trained directly on observational data. At the core of NeuralCPM lies the Neural Hamiltonian, a neural network architecture that respects universal symmetries in collective cellular dynamics. Moreover, this approach enables seamless integration of domain knowledge by combining known biological mechanisms and the expressive Neural Hamiltonian into a hybrid model. Our evaluation with synthetic and real-world multicellular systems demonstrates that NeuralCPM is able to model cellular dynamics that cannot be accounted for by traditional analytical Hamiltonians.","sentences":["The cellular Potts model (CPM) is a powerful computational method for simulating collective spatiotemporal dynamics of biological cells.","To drive the dynamics, CPMs rely on physics-inspired Hamiltonians.","However, as first principles remain elusive in biology, these Hamiltonians only approximate the full complexity of real multicellular systems.","To address this limitation, we propose NeuralCPM, a more expressive cellular Potts model that can be trained directly on observational data.","At the core of NeuralCPM lies the Neural Hamiltonian, a neural network architecture that respects universal symmetries in collective cellular dynamics.","Moreover, this approach enables seamless integration of domain knowledge by combining known biological mechanisms and the expressive Neural Hamiltonian into a hybrid model.","Our evaluation with synthetic and real-world multicellular systems demonstrates that NeuralCPM is able to model cellular dynamics that cannot be accounted for by traditional analytical Hamiltonians."],"url":"http://arxiv.org/abs/2502.02129v1"}
{"created":"2025-02-04 08:54:06","title":"BRIDLE: Generalized Self-supervised Learning with Quantization","abstract":"Self-supervised learning has been a powerful approach for learning meaningful representations from unlabeled data across various domains, reducing the reliance on large labeled datasets. Inspired by BERT's success in capturing deep bidirectional contexts in natural language processing, similar frameworks have been adapted to other modalities such as audio, with models like BEATs extending the bidirectional training paradigm to audio signals using vector quantization (VQ). However, these frameworks face challenges, notably their dependence on a single codebook for quantization, which may not capture the complex, multifaceted nature of signals. In addition, inefficiencies in codebook utilization lead to underutilized code vectors. To address these limitations, we introduce BRIDLE (Bidirectional Residual Quantization Interleaved Discrete Learning Encoder), a self-supervised encoder pretraining framework that incorporates residual quantization (RQ) into the bidirectional training process, and is generalized for pretraining with audio, image, and video. Using multiple hierarchical codebooks, RQ enables fine-grained discretization in the latent space, enhancing representation quality. BRIDLE involves an interleaved training procedure between the encoder and tokenizer. We evaluate BRIDLE on audio understanding tasks using classification benchmarks, achieving state-of-the-art results, and demonstrate competitive performance on image classification and video classification tasks, showing consistent improvements over traditional VQ methods in downstream performance.","sentences":["Self-supervised learning has been a powerful approach for learning meaningful representations from unlabeled data across various domains, reducing the reliance on large labeled datasets.","Inspired by BERT's success in capturing deep bidirectional contexts in natural language processing, similar frameworks have been adapted to other modalities such as audio, with models like BEATs extending the bidirectional training paradigm to audio signals using vector quantization (VQ).","However, these frameworks face challenges, notably their dependence on a single codebook for quantization, which may not capture the complex, multifaceted nature of signals.","In addition, inefficiencies in codebook utilization lead to underutilized code vectors.","To address these limitations, we introduce BRIDLE (Bidirectional Residual Quantization Interleaved Discrete Learning Encoder), a self-supervised encoder pretraining framework that incorporates residual quantization (RQ) into the bidirectional training process, and is generalized for pretraining with audio, image, and video.","Using multiple hierarchical codebooks, RQ enables fine-grained discretization in the latent space, enhancing representation quality.","BRIDLE involves an interleaved training procedure between the encoder and tokenizer.","We evaluate BRIDLE on audio understanding tasks using classification benchmarks, achieving state-of-the-art results, and demonstrate competitive performance on image classification and video classification tasks, showing consistent improvements over traditional VQ methods in downstream performance."],"url":"http://arxiv.org/abs/2502.02118v1"}
{"created":"2025-02-04 08:52:13","title":"Efficient and Practical Approximation Algorithms for Advertising in Content Feeds","abstract":"Content feeds provided by platforms such as X (formerly Twitter) and TikTok are consumed by users on a daily basis. In this paper, we revisit the native advertising problem in content feeds, initiated by Ieong et al. Given a sequence of organic items (e.g., videos or posts) relevant to a user's interests or to an information search, the goal is to place ads within the organic content so as to maximize a reward function (e.g., number of clicks), while accounting for two considerations: (1) an ad can only be inserted after a relevant content item; (2) the users' attention decays after consuming content or ads. These considerations provide a natural model for capturing both the advertisement effectiveness and the user experience. In this paper, we design fast and practical 2-approximation greedy algorithms for the associated optimization problem, improving over the best-known practical algorithm that only achieves an approximation factor of~4. Our algorithms exploit a counter-intuitive observation, namely, while top items are seemingly more important due to the decaying attention of the user, taking good care of the bottom items is key for obtaining improved approximation guarantees. We then provide the first comprehensive empirical evaluation on the problem, showing the strong empirical performance of our~methods.","sentences":["Content feeds provided by platforms such as X (formerly Twitter) and TikTok are consumed by users on a daily basis.","In this paper, we revisit the native advertising problem in content feeds, initiated by Ieong et al.","Given a sequence of organic items (e.g., videos or posts) relevant to a user's interests or to an information search, the goal is to place ads within the organic content so as to maximize a reward function (e.g., number of clicks), while accounting for two considerations: (1) an ad can only be inserted after a relevant content item; (2) the users' attention decays after consuming content or ads.","These considerations provide a natural model for capturing both the advertisement effectiveness and the user experience.","In this paper, we design fast and practical 2-approximation greedy algorithms for the associated optimization problem, improving over the best-known practical algorithm that only achieves an approximation factor of~4.","Our algorithms exploit a counter-intuitive observation, namely, while top items are seemingly more important due to the decaying attention of the user, taking good care of the bottom items is key for obtaining improved approximation guarantees.","We then provide the first comprehensive empirical evaluation on the problem, showing the strong empirical performance of our~methods."],"url":"http://arxiv.org/abs/2502.02115v1"}
{"created":"2025-02-04 08:05:34","title":"A New Rejection Sampling Approach to $k$-$\\mathtt{means}$++ With Improved Trade-Offs","abstract":"The $k$-$\\mathtt{means}$++ seeding algorithm (Arthur & Vassilvitskii, 2007) is widely used in practice for the $k$-means clustering problem where the goal is to cluster a dataset $\\mathcal{X} \\subset \\mathbb{R} ^d$ into $k$ clusters.   The popularity of this algorithm is due to its simplicity and provable guarantee of being $O(\\log k)$ competitive with the optimal solution in expectation. However, its running time is $O(|\\mathcal{X}|kd)$, making it expensive for large datasets.   In this work, we present a simple and effective rejection sampling based approach for speeding up $k$-$\\mathtt{means}$++.   Our first method runs in time $\\tilde{O}(\\mathtt{nnz} (\\mathcal{X}) + \\beta k^2d)$ while still being $O(\\log k )$ competitive in expectation. Here, $\\beta$ is a parameter which is the ratio of the variance of the dataset to the optimal $k$-$\\mathtt{means}$ cost in expectation and $\\tilde{O}$ hides logarithmic factors in $k$ and $|\\mathcal{X}|$.   Our second method presents a new trade-off between computational cost and solution quality. It incurs an additional scale-invariant factor of $ k^{-\\Omega( m/\\beta)} \\operatorname{Var} (\\mathcal{X})$ in addition to the $O(\\log k)$ guarantee of $k$-$\\mathtt{means}$++ improving upon a result of (Bachem et al, 2016a) who get an additional factor of $m^{-1}\\operatorname{Var}(\\mathcal{X})$ while still running in time $\\tilde{O}(\\mathtt{nnz}(\\mathcal{X}) + mk^2d)$. We perform extensive empirical evaluations to validate our theoretical results and to show the effectiveness of our approach on real datasets.","sentences":["The $k$-$\\mathtt{means}$++ seeding algorithm (Arthur & Vassilvitskii, 2007) is widely used in practice for the $k$-means clustering problem where the goal is to cluster a dataset $\\mathcal{X} \\subset \\mathbb{R} ^d$ into $k$ clusters.   ","The popularity of this algorithm is due to its simplicity and provable guarantee of being $O(\\log k)$ competitive with the optimal solution in expectation.","However, its running time is $O(|\\mathcal{X}|kd)$, making it expensive for large datasets.   ","In this work, we present a simple and effective rejection sampling based approach for speeding up $k$-$\\mathtt{means}$++.   ","Our first method runs in time $\\tilde{O}(\\mathtt{nnz} (\\mathcal{X})","+","\\beta k^2d)$ while still being $O(\\log k )$ competitive in expectation.","Here, $\\beta$ is a parameter which is the ratio of the variance of the dataset to the optimal $k$-$\\mathtt{means}$ cost in expectation and $\\tilde{O}$ hides logarithmic factors in $k$ and $|\\mathcal{X}|$.   Our second method presents a new trade-off between computational cost and solution quality.","It incurs an additional scale-invariant factor of $ k^{-\\Omega( m/\\beta)} \\operatorname{Var} (\\mathcal{X})$ in addition to the $O(\\log k)$ guarantee of $k$-$\\mathtt{means}$++ improving upon a result of (Bachem et al, 2016a) who get an additional factor of $m^{-1}\\operatorname{Var}(\\mathcal{X})$ while still running in time $\\tilde{O}(\\mathtt{nnz}(\\mathcal{X})","+ mk^2d)$. We perform extensive empirical evaluations to validate our theoretical results and to show the effectiveness of our approach on real datasets."],"url":"http://arxiv.org/abs/2502.02085v1"}
{"created":"2025-02-04 08:05:15","title":"Improving Power Plant CO2 Emission Estimation with Deep Learning and Satellite/Simulated Data","abstract":"CO2 emissions from power plants, as significant super emitters, contribute substantially to global warming. Accurate quantification of these emissions is crucial for effective climate mitigation strategies. While satellite-based plume inversion offers a promising approach, challenges arise from data limitations and the complexity of atmospheric conditions. This study addresses these challenges by (a) expanding the available dataset through the integration of NO2 data from Sentinel-5P, generating continuous XCO2 maps, and incorporating real satellite observations from OCO-2/3 for over 71 power plants in data-scarce regions; and (b) employing a customized U-Net model capable of handling diverse spatio-temporal resolutions for emission rate estimation. Our results demonstrate significant improvements in emission rate accuracy compared to previous methods. By leveraging this enhanced approach, we can enable near real-time, precise quantification of major CO2 emission sources, supporting environmental protection initiatives and informing regulatory frameworks.","sentences":["CO2 emissions from power plants, as significant super emitters, contribute substantially to global warming.","Accurate quantification of these emissions is crucial for effective climate mitigation strategies.","While satellite-based plume inversion offers a promising approach, challenges arise from data limitations and the complexity of atmospheric conditions.","This study addresses these challenges by (a) expanding the available dataset through the integration of NO2 data from Sentinel-5P, generating continuous XCO2 maps, and incorporating real satellite observations from OCO-2/3 for over 71 power plants in data-scarce regions; and (b) employing a customized U-Net model capable of handling diverse spatio-temporal resolutions for emission rate estimation.","Our results demonstrate significant improvements in emission rate accuracy compared to previous methods.","By leveraging this enhanced approach, we can enable near real-time, precise quantification of major CO2 emission sources, supporting environmental protection initiatives and informing regulatory frameworks."],"url":"http://arxiv.org/abs/2502.02083v1"}
{"created":"2025-02-04 07:55:41","title":"Online Clustering of Dueling Bandits","abstract":"The contextual multi-armed bandit (MAB) is a widely used framework for problems requiring sequential decision-making under uncertainty, such as recommendation systems. In applications involving a large number of users, the performance of contextual MAB can be significantly improved by facilitating collaboration among multiple users. This has been achieved by the clustering of bandits (CB) methods, which adaptively group the users into different clusters and achieve collaboration by allowing the users in the same cluster to share data. However, classical CB algorithms typically rely on numerical reward feedback, which may not be practical in certain real-world applications. For instance, in recommendation systems, it is more realistic and reliable to solicit preference feedback between pairs of recommended items rather than absolute rewards. To address this limitation, we introduce the first \"clustering of dueling bandit algorithms\" to enable collaborative decision-making based on preference feedback. We propose two novel algorithms: (1) Clustering of Linear Dueling Bandits (COLDB) which models the user reward functions as linear functions of the context vectors, and (2) Clustering of Neural Dueling Bandits (CONDB) which uses a neural network to model complex, non-linear user reward functions. Both algorithms are supported by rigorous theoretical analyses, demonstrating that user collaboration leads to improved regret bounds. Extensive empirical evaluations on synthetic and real-world datasets further validate the effectiveness of our methods, establishing their potential in real-world applications involving multiple users with preference-based feedback.","sentences":["The contextual multi-armed bandit (MAB) is a widely used framework for problems requiring sequential decision-making under uncertainty, such as recommendation systems.","In applications involving a large number of users, the performance of contextual MAB can be significantly improved by facilitating collaboration among multiple users.","This has been achieved by the clustering of bandits (CB) methods, which adaptively group the users into different clusters and achieve collaboration by allowing the users in the same cluster to share data.","However, classical CB algorithms typically rely on numerical reward feedback, which may not be practical in certain real-world applications.","For instance, in recommendation systems, it is more realistic and reliable to solicit preference feedback between pairs of recommended items rather than absolute rewards.","To address this limitation, we introduce the first \"clustering of dueling bandit algorithms\" to enable collaborative decision-making based on preference feedback.","We propose two novel algorithms: (1) Clustering of Linear Dueling Bandits (COLDB) which models the user reward functions as linear functions of the context vectors, and (2) Clustering of Neural Dueling Bandits (CONDB) which uses a neural network to model complex, non-linear user reward functions.","Both algorithms are supported by rigorous theoretical analyses, demonstrating that user collaboration leads to improved regret bounds.","Extensive empirical evaluations on synthetic and real-world datasets further validate the effectiveness of our methods, establishing their potential in real-world applications involving multiple users with preference-based feedback."],"url":"http://arxiv.org/abs/2502.02079v1"}
{"created":"2025-02-04 07:53:23","title":"Position Paper: Building Trust in Synthetic Data for Clinical AI","abstract":"Deep generative models and synthetic medical data have shown significant promise in addressing key challenges in healthcare, such as privacy concerns, data bias, and the scarcity of realistic datasets. While research in this area has grown rapidly and demonstrated substantial theoretical potential, its practical adoption in clinical settings remains limited. Despite the benefits synthetic data offers, questions surrounding its reliability and credibility persist, leading to a lack of trust among clinicians. This position paper argues that fostering trust in synthetic medical data is crucial for its clinical adoption. It aims to spark a discussion on the viability of synthetic medical data in clinical practice, particularly in the context of current advancements in AI. We present empirical evidence from brain tumor segmentation to demonstrate that the quality, diversity, and proportion of synthetic data directly impact trust in clinical AI models. Our findings provide insights to improve the deployment and acceptance of synthetic data-driven AI systems in real-world clinical workflows.","sentences":["Deep generative models and synthetic medical data have shown significant promise in addressing key challenges in healthcare, such as privacy concerns, data bias, and the scarcity of realistic datasets.","While research in this area has grown rapidly and demonstrated substantial theoretical potential, its practical adoption in clinical settings remains limited.","Despite the benefits synthetic data offers, questions surrounding its reliability and credibility persist, leading to a lack of trust among clinicians.","This position paper argues that fostering trust in synthetic medical data is crucial for its clinical adoption.","It aims to spark a discussion on the viability of synthetic medical data in clinical practice, particularly in the context of current advancements in AI.","We present empirical evidence from brain tumor segmentation to demonstrate that the quality, diversity, and proportion of synthetic data directly impact trust in clinical AI models.","Our findings provide insights to improve the deployment and acceptance of synthetic data-driven AI systems in real-world clinical workflows."],"url":"http://arxiv.org/abs/2502.02076v1"}
{"created":"2025-02-04 07:52:20","title":"Rethinking stance detection: A theoretically-informed research agenda for user-level inference using language models","abstract":"Stance detection has emerged as a popular task in natural language processing research, enabled largely by the abundance of target-specific social media data. While there has been considerable research on the development of stance detection models, datasets, and application, we highlight important gaps pertaining to (i) a lack of theoretical conceptualization of stance, and (ii) the treatment of stance at an individual- or user-level, as opposed to message-level. In this paper, we first review the interdisciplinary origins of stance as an individual-level construct to highlight relevant attributes (e.g., psychological features) that might be useful to incorporate in stance detection models. Further, we argue that recent pre-trained and large language models (LLMs) might offer a way to flexibly infer such user-level attributes and/or incorporate them in modelling stance. To better illustrate this, we briefly review and synthesize the emerging corpus of studies on using LLMs for inferring stance, and specifically on incorporating user attributes in such tasks. We conclude by proposing a four-point agenda for pursuing stance detection research that is theoretically informed, inclusive, and practically impactful.","sentences":["Stance detection has emerged as a popular task in natural language processing research, enabled largely by the abundance of target-specific social media data.","While there has been considerable research on the development of stance detection models, datasets, and application, we highlight important gaps pertaining to (i) a lack of theoretical conceptualization of stance, and (ii) the treatment of stance at an individual- or user-level, as opposed to message-level.","In this paper, we first review the interdisciplinary origins of stance as an individual-level construct to highlight relevant attributes (e.g., psychological features) that might be useful to incorporate in stance detection models.","Further, we argue that recent pre-trained and large language models (LLMs) might offer a way to flexibly infer such user-level attributes and/or incorporate them in modelling stance.","To better illustrate this, we briefly review and synthesize the emerging corpus of studies on using LLMs for inferring stance, and specifically on incorporating user attributes in such tasks.","We conclude by proposing a four-point agenda for pursuing stance detection research that is theoretically informed, inclusive, and practically impactful."],"url":"http://arxiv.org/abs/2502.02074v1"}
{"created":"2025-02-04 07:31:55","title":"Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments","abstract":"Assistive agents performing household tasks such as making the bed or cooking breakfast often compute and execute actions that accomplish one task at a time. However, efficiency can be improved by anticipating upcoming tasks and computing an action sequence that jointly achieves these tasks. State-of-the-art methods for task anticipation use data-driven deep networks and Large Language Models (LLMs), but they do so at the level of high-level tasks and/or require many training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework's abilities in realistic scenarios in the VirtualHome environment and demonstrate a 31% reduction in execution time compared with a system that does not consider upcoming tasks.","sentences":["Assistive agents performing household tasks such as making the bed or cooking breakfast often compute and execute actions that accomplish one task at a time.","However, efficiency can be improved by anticipating upcoming tasks and computing an action sequence that jointly achieves these tasks.","State-of-the-art methods for task anticipation use data-driven deep networks and Large Language Models (LLMs), but they do so at the level of high-level tasks and/or require many training examples.","Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals.","We ground and evaluate our framework's abilities in realistic scenarios in the VirtualHome environment and demonstrate a 31% reduction in execution time compared with a system that does not consider upcoming tasks."],"url":"http://arxiv.org/abs/2502.02066v1"}
{"created":"2025-02-04 06:58:16","title":"Optimal Routing in the Presence of Hooks: Three Case Studies","abstract":"We consider the problem of optimally executing a user trade over networks of constant function market makers (CFMMs) in the presence of hooks. Hooks, introduced in an upcoming version of Uniswap, are auxiliary smart contracts that allow for extra information to be added to liquidity pools. This allows liquidity providers to enable constraints on trades, allowing CFMMs to read external data, such as volatility information, and implement additional features, such as onchain limit orders. We consider three important case studies for how to optimally route trades in the presence of hooks: 1) routing through limit orders, 2) optimal liquidations and time-weighted average market makers (TWAMMs), and 3) noncomposable hooks, which provide additional output in exchange for fill risk. Leveraging tools from convex optimization and dynamic programming, we propose simple methods for formulating and solving these problems that can be useful for practitioners.","sentences":["We consider the problem of optimally executing a user trade over networks of constant function market makers (CFMMs) in the presence of hooks.","Hooks, introduced in an upcoming version of Uniswap, are auxiliary smart contracts that allow for extra information to be added to liquidity pools.","This allows liquidity providers to enable constraints on trades, allowing CFMMs to read external data, such as volatility information, and implement additional features, such as onchain limit orders.","We consider three important case studies for how to optimally route trades in the presence of hooks: 1) routing through limit orders, 2) optimal liquidations and time-weighted average market makers (TWAMMs), and 3) noncomposable hooks, which provide additional output in exchange for fill risk.","Leveraging tools from convex optimization and dynamic programming, we propose simple methods for formulating and solving these problems that can be useful for practitioners."],"url":"http://arxiv.org/abs/2502.02059v1"}
{"created":"2025-02-04 06:42:08","title":"RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation","abstract":"This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces while preserving the robustness of RL policies. A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments, ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.","sentences":["This paper introduces a learning-based visual planner for agile drone flight in cluttered environments.","The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules.","Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations.","BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency.","To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation.","By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces while preserving the robustness of RL policies.","A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments, ensuring comprehensive scenario coverage.","By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states.","While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning.","The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures.","The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments.","To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones."],"url":"http://arxiv.org/abs/2502.02054v1"}
{"created":"2025-02-04 06:12:43","title":"SMTFL: Secure Model Training to Untrusted Participants in Federated Learning","abstract":"Federated learning is an essential distributed model training technique. However, threats such as gradient inversion attacks and poisoning attacks pose significant risks to the privacy of training data and the model correctness. We propose a novel approach called SMTFL to achieve secure model training in federated learning without relying on trusted participants. To safeguard gradients privacy against gradient inversion attacks, clients are dynamically grouped, allowing one client's gradient to be divided to obfuscate the gradients of other clients within the group. This method incorporates checks and balances to reduce the collusion for inferring specific client data. To detect poisoning attacks from malicious clients, we assess the impact of aggregated gradients on the global model's performance, enabling effective identification and exclusion of malicious clients. Each client's gradients are encrypted and stored, with decryption collectively managed by all clients. The detected poisoning gradients are invalidated from the global model through a unlearning method. To our best knowledge, we present the first practical secure aggregation scheme, which does not require trusted participants, avoids the performance degradation associated with traditional noise-injection, and aviods complex cryptographic operations during gradient aggregation. Evaluation results are encouraging based on four datasets and two models: SMTFL is effective against poisoning attacks and gradient inversion attacks, achieving an accuracy rate of over 95% in locating malicious clients, while keeping the false positive rate for honest clients within 5%. The model accuracy is also nearly restored to its pre-attack state when SMTFL is deployed.","sentences":["Federated learning is an essential distributed model training technique.","However, threats such as gradient inversion attacks and poisoning attacks pose significant risks to the privacy of training data and the model correctness.","We propose a novel approach called SMTFL to achieve secure model training in federated learning without relying on trusted participants.","To safeguard gradients privacy against gradient inversion attacks, clients are dynamically grouped, allowing one client's gradient to be divided to obfuscate the gradients of other clients within the group.","This method incorporates checks and balances to reduce the collusion for inferring specific client data.","To detect poisoning attacks from malicious clients, we assess the impact of aggregated gradients on the global model's performance, enabling effective identification and exclusion of malicious clients.","Each client's gradients are encrypted and stored, with decryption collectively managed by all clients.","The detected poisoning gradients are invalidated from the global model through a unlearning method.","To our best knowledge, we present the first practical secure aggregation scheme, which does not require trusted participants, avoids the performance degradation associated with traditional noise-injection, and aviods complex cryptographic operations during gradient aggregation.","Evaluation results are encouraging based on four datasets and two models: SMTFL is effective against poisoning attacks and gradient inversion attacks, achieving an accuracy rate of over 95% in locating malicious clients, while keeping the false positive rate for honest clients within 5%.","The model accuracy is also nearly restored to its pre-attack state when SMTFL is deployed."],"url":"http://arxiv.org/abs/2502.02038v1"}
{"created":"2025-02-04 05:39:13","title":"MORPH-LER: Log-Euclidean Regularization for Population-Aware Image Registration","abstract":"Spatial transformations that capture population-level morphological statistics are critical for medical image analysis. Commonly used smoothness regularizers for image registration fail to integrate population statistics, leading to anatomically inconsistent transformations. Inverse consistency regularizers promote geometric consistency but lack population morphometrics integration. Regularizers that constrain deformation to low-dimensional manifold methods address this. However, they prioritize reconstruction over interpretability and neglect diffeomorphic properties, such as group composition and inverse consistency. We introduce MORPH-LER, a Log-Euclidean regularization framework for population-aware unsupervised image registration. MORPH-LER learns population morphometrics from spatial transformations to guide and regularize registration networks, ensuring anatomically plausible deformations. It features a bottleneck autoencoder that computes the principal logarithm of deformation fields via iterative square-root predictions. It creates a linearized latent space that respects diffeomorphic properties and enforces inverse consistency. By integrating a registration network with a diffeomorphic autoencoder, MORPH-LER produces smooth, meaningful deformation fields. The framework offers two main contributions: (1) a data-driven regularization strategy that incorporates population-level anatomical statistics to enhance transformation validity and (2) a linearized latent space that enables compact and interpretable deformation fields for efficient population morphometrics analysis. We validate MORPH-LER across two families of deep learning-based registration networks, demonstrating its ability to produce anatomically accurate, computationally efficient, and statistically meaningful transformations on the OASIS-1 brain imaging dataset.","sentences":["Spatial transformations that capture population-level morphological statistics are critical for medical image analysis.","Commonly used smoothness regularizers for image registration fail to integrate population statistics, leading to anatomically inconsistent transformations.","Inverse consistency regularizers promote geometric consistency but lack population morphometrics integration.","Regularizers that constrain deformation to low-dimensional manifold methods address this.","However, they prioritize reconstruction over interpretability and neglect diffeomorphic properties, such as group composition and inverse consistency.","We introduce MORPH-LER, a Log-Euclidean regularization framework for population-aware unsupervised image registration.","MORPH-LER learns population morphometrics from spatial transformations to guide and regularize registration networks, ensuring anatomically plausible deformations.","It features a bottleneck autoencoder that computes the principal logarithm of deformation fields via iterative square-root predictions.","It creates a linearized latent space that respects diffeomorphic properties and enforces inverse consistency.","By integrating a registration network with a diffeomorphic autoencoder, MORPH-LER produces smooth, meaningful deformation fields.","The framework offers two main contributions: (1) a data-driven regularization strategy that incorporates population-level anatomical statistics to enhance transformation validity and (2) a linearized latent space that enables compact and interpretable deformation fields for efficient population morphometrics analysis.","We validate MORPH-LER across two families of deep learning-based registration networks, demonstrating its ability to produce anatomically accurate, computationally efficient, and statistically meaningful transformations on the OASIS-1 brain imaging dataset."],"url":"http://arxiv.org/abs/2502.02029v1"}
{"created":"2025-02-04 05:21:29","title":"From Accidents to Insights: Leveraging Multimodal Data for Scenario-Driven ADS Testing","abstract":"The rapid advancements in Autonomous Driving Systems (ADS) have necessitated robust software testing to ensure safety and reliability. However, automating the generation of scalable and concrete test scenarios remains a significant challenge. Current scenario-based test case generation methods often face limitations, such as unrealistic scenes and inaccurate vehicle trajectories. These challenges largely result from the loss of map information during data extraction and the lack of an effective verification mechanism to mitigate hallucinations in large language models (LLMs). This paper introduces TRACE, a scenario-based ADS Test case Generation framework for Critical Scenarios. By leveraging multimodal data to extract challenging scenarios from real-world car crash reports, TRACE constructs numerous critical test cases with less data, significantly enhancing ADS bug detection efficiency. Using in-context learning, chain-of-thought prompting, and self-validation approaches, we use LLMs to extract environmental and road network information from crash reports. For vehicle trajectory planning, data containing map information and vehicle coordinates serves as a knowledge base to build a ChatGPT-based LLM with path-planning capabilities, which we named TrackMate. Based on 50 existing crash reports, our approach successfully tested three ADS models across two simulation platforms, MetaDrive and BeamNG. Of the 290 constructed test scenarios, 127 are identified as critical, as they resulted in vehicle collisions. Additionally, user feedback reveals that TRACE demonstrates superior scenario reconstruction accuracy, with 77.5% of the scenarios being rated as 'mostly or 'totally' consistent, compared to only 27% for the most related SOTA, LCTGen.","sentences":["The rapid advancements in Autonomous Driving Systems (ADS) have necessitated robust software testing to ensure safety and reliability.","However, automating the generation of scalable and concrete test scenarios remains a significant challenge.","Current scenario-based test case generation methods often face limitations, such as unrealistic scenes and inaccurate vehicle trajectories.","These challenges largely result from the loss of map information during data extraction and the lack of an effective verification mechanism to mitigate hallucinations in large language models (LLMs).","This paper introduces TRACE, a scenario-based ADS Test case Generation framework for Critical Scenarios.","By leveraging multimodal data to extract challenging scenarios from real-world car crash reports, TRACE constructs numerous critical test cases with less data, significantly enhancing ADS bug detection efficiency.","Using in-context learning, chain-of-thought prompting, and self-validation approaches, we use LLMs to extract environmental and road network information from crash reports.","For vehicle trajectory planning, data containing map information and vehicle coordinates serves as a knowledge base to build a ChatGPT-based LLM with path-planning capabilities, which we named TrackMate.","Based on 50 existing crash reports, our approach successfully tested three ADS models across two simulation platforms, MetaDrive and BeamNG.","Of the 290 constructed test scenarios, 127 are identified as critical, as they resulted in vehicle collisions.","Additionally, user feedback reveals that TRACE demonstrates superior scenario reconstruction accuracy, with 77.5% of the scenarios being rated as 'mostly or 'totally' consistent, compared to only 27% for the most related SOTA, LCTGen."],"url":"http://arxiv.org/abs/2502.02025v1"}
{"created":"2025-02-04 05:18:35","title":"Causal bandits with backdoor adjustment on unknown Gaussian DAGs","abstract":"The causal bandit problem aims to sequentially learn the intervention that maximizes the expectation of a reward variable within a system governed by a causal graph. Most existing approaches assume prior knowledge of the graph structure, or impose unrealistically restrictive conditions on the graph. In this paper, we assume a Gaussian linear directed acyclic graph (DAG) over arms and the reward variable, and study the causal bandit problem when the graph structure is unknown. We identify backdoor adjustment sets for each arm using sequentially generated experimental and observational data during the decision process, which allows us to estimate causal effects and construct upper confidence bounds. By integrating estimates from both data sources, we develop a novel bandit algorithm, based on modified upper confidence bounds, to sequentially determine the optimal intervention. We establish both case-dependent and case-independent upper bounds on the cumulative regret for our algorithm, which improve upon the bounds of the standard multi-armed bandit algorithms. Our empirical study demonstrates its advantage over existing methods with respect to cumulative regret and computation time.","sentences":["The causal bandit problem aims to sequentially learn the intervention that maximizes the expectation of a reward variable within a system governed by a causal graph.","Most existing approaches assume prior knowledge of the graph structure, or impose unrealistically restrictive conditions on the graph.","In this paper, we assume a Gaussian linear directed acyclic graph (DAG) over arms and the reward variable, and study the causal bandit problem when the graph structure is unknown.","We identify backdoor adjustment sets for each arm using sequentially generated experimental and observational data during the decision process, which allows us to estimate causal effects and construct upper confidence bounds.","By integrating estimates from both data sources, we develop a novel bandit algorithm, based on modified upper confidence bounds, to sequentially determine the optimal intervention.","We establish both case-dependent and case-independent upper bounds on the cumulative regret for our algorithm, which improve upon the bounds of the standard multi-armed bandit algorithms.","Our empirical study demonstrates its advantage over existing methods with respect to cumulative regret and computation time."],"url":"http://arxiv.org/abs/2502.02020v1"}
{"created":"2025-02-04 05:07:13","title":"A Periodic Bayesian Flow for Material Generation","abstract":"Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song et al., 2023). Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues. We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics. To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning. Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks. Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., ~100x speedup 10 v.s. 2000 steps network forwards) compared with previous diffusion-based methods on MP-20 dataset. Code is available at https://github.com/wu-han-lin/CrysBFN.","sentences":["Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals.","Diffusion-based methods have shown early promise in modeling crystal distribution.","More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song et al., 2023).","Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues.","We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics.","To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning.","Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks.","Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., ~100x speedup 10 v.s. 2000 steps network forwards) compared with previous diffusion-based methods on MP-20 dataset.","Code is available at https://github.com/wu-han-lin/CrysBFN."],"url":"http://arxiv.org/abs/2502.02016v1"}
{"created":"2025-02-04 05:05:49","title":"The Wisdom of Intellectually Humble Networks","abstract":"People's collectively held beliefs can have significant social implications, including on democratic processes and policies. Unfortunately, as people interact with peers to form and update their beliefs, various cognitive and social biases can hinder their collective wisdom. In this paper, we probe whether and how the psychological construct of intellectual humility can modulate collective wisdom in a networked interaction setting. Through agent-based modeling and data-calibrated simulations, we provide a proof of concept demonstrating that intellectual humility can foster more accurate estimations while mitigating polarization in social networks. We investigate the mechanisms behind the performance improvements and confirm robustness across task settings and network structures. Our work can guide intervention designs to capitalize on the promises of intellectual humility in boosting collective wisdom in social networks.","sentences":["People's collectively held beliefs can have significant social implications, including on democratic processes and policies.","Unfortunately, as people interact with peers to form and update their beliefs, various cognitive and social biases can hinder their collective wisdom.","In this paper, we probe whether and how the psychological construct of intellectual humility can modulate collective wisdom in a networked interaction setting.","Through agent-based modeling and data-calibrated simulations, we provide a proof of concept demonstrating that intellectual humility can foster more accurate estimations while mitigating polarization in social networks.","We investigate the mechanisms behind the performance improvements and confirm robustness across task settings and network structures.","Our work can guide intervention designs to capitalize on the promises of intellectual humility in boosting collective wisdom in social networks."],"url":"http://arxiv.org/abs/2502.02015v1"}
{"created":"2025-02-04 04:20:15","title":"Data Guard: A Fine-grained Purpose-based Access Control System for Large Data Warehouses","abstract":"The last few years have witnessed a spate of data protection regulations in conjunction with an ever-growing appetite for data usage in large businesses, thus presenting significant challenges for businesses to maintain compliance. To address this conflict, we present Data Guard - a fine-grained, purpose-based access control system for large data warehouses. Data Guard enables authoring policies based on semantic descriptions of data and purpose of data access. Data Guard then translates these policies into SQL views that mask data from the underlying warehouse tables. At access time, Data Guard ensures compliance by transparently routing each table access to the appropriate data-masking view based on the purpose of the access, thus minimizing the effort of adopting Data Guard in existing applications. Our enforcement solution allows masking data at much finer granularities than what traditional solutions allow. In addition to row and column level data masking, Data Guard can mask data at the sub-cell level for columns with non-atomic data types such as structs, arrays, and maps. This fine-grained masking allows Data Guard to preserve data utility for consumers while ensuring compliance. We implemented a number of performance optimizations to minimize the overhead of data masking operations. We perform numerous experiments to identify the key factors that influence the data masking overhead and demonstrate the efficiency of our implementation.","sentences":["The last few years have witnessed a spate of data protection regulations in conjunction with an ever-growing appetite for data usage in large businesses, thus presenting significant challenges for businesses to maintain compliance.","To address this conflict, we present Data Guard - a fine-grained, purpose-based access control system for large data warehouses.","Data Guard enables authoring policies based on semantic descriptions of data and purpose of data access.","Data Guard then translates these policies into SQL views that mask data from the underlying warehouse tables.","At access time, Data Guard ensures compliance by transparently routing each table access to the appropriate data-masking view based on the purpose of the access, thus minimizing the effort of adopting Data Guard in existing applications.","Our enforcement solution allows masking data at much finer granularities than what traditional solutions allow.","In addition to row and column level data masking, Data Guard can mask data at the sub-cell level for columns with non-atomic data types such as structs, arrays, and maps.","This fine-grained masking allows Data Guard to preserve data utility for consumers while ensuring compliance.","We implemented a number of performance optimizations to minimize the overhead of data masking operations.","We perform numerous experiments to identify the key factors that influence the data masking overhead and demonstrate the efficiency of our implementation."],"url":"http://arxiv.org/abs/2502.01998v1"}
{"created":"2025-02-04 04:17:20","title":"Licensing Open Government Data","abstract":"This article focuses on the legal issues associated with open government data licenses. This study compares current open data licenses and argues that licensing terms reflect policy considerations, which are quite different from those contemplated in business transactions or shared in typical commons communities. This article investigates the ambiguous legal status of data together with the new wave of open government data, which concerns some fundamental intellectual property (IP) questions not covered by, or analyzed in depth in, the current literature. Moreover, this study suggests that government should choose or adapt open data licenses according to their own IP regimes. In the end, this article argues that the design or choice of open government data license forms an important element of information policy; government, therefore, should make this decision in accordance with their policy goals and in compliance with their own jurisdictions' IP laws.","sentences":["This article focuses on the legal issues associated with open government data licenses.","This study compares current open data licenses and argues that licensing terms reflect policy considerations, which are quite different from those contemplated in business transactions or shared in typical commons communities.","This article investigates the ambiguous legal status of data together with the new wave of open government data, which concerns some fundamental intellectual property (IP) questions not covered by, or analyzed in depth in, the current literature.","Moreover, this study suggests that government should choose or adapt open data licenses according to their own IP regimes.","In the end, this article argues that the design or choice of open government data license forms an important element of information policy; government, therefore, should make this decision in accordance with their policy goals and in compliance with their own jurisdictions' IP laws."],"url":"http://arxiv.org/abs/2502.01996v1"}
{"created":"2025-02-04 04:10:23","title":"Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study on Vaccination Debate on Social Media","abstract":"Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks such as identifying morality frames makes relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a \"think-aloud\" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks.","sentences":["Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions.","In NLP, data scarcity and complexity of psycholinguistic tasks such as identifying morality frames makes relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load.","To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles.","Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media.","We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a \"think-aloud\" tool.","Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks."],"url":"http://arxiv.org/abs/2502.01991v1"}
{"created":"2025-02-04 04:02:58","title":"Online Adaptive Traversability Estimation through Interaction for Unstructured, Densely Vegetated Environments","abstract":"Navigating densely vegetated environments poses significant challenges for autonomous ground vehicles. Learning-based systems typically use prior and in-situ data to predict terrain traversability but often degrade in performance when encountering out-of-distribution elements caused by rapid environmental changes or novel conditions. This paper presents a novel, lidar-only, online adaptive traversability estimation (TE) method that trains a model directly on the robot using self-supervised data collected through robot-environment interaction. The proposed approach utilises a probabilistic 3D voxel representation to integrate lidar measurements and robot experience, creating a salient environmental model. To ensure computational efficiency, a sparse graph-based representation is employed to update temporarily evolving voxel distributions. Extensive experiments with an unmanned ground vehicle in natural terrain demonstrate that the system adapts to complex environments with as little as 8 minutes of operational data, achieving a Matthews Correlation Coefficient (MCC) score of 0.63 and enabling safe navigation in densely vegetated environments. This work examines different training strategies for voxel-based TE methods and offers recommendations for training strategies to improve adaptability. The proposed method is validated on a robotic platform with limited computational resources (25W GPU), achieving accuracy comparable to offline-trained models while maintaining reliable performance across varied environments.","sentences":["Navigating densely vegetated environments poses significant challenges for autonomous ground vehicles.","Learning-based systems typically use prior and in-situ data to predict terrain traversability but often degrade in performance when encountering out-of-distribution elements caused by rapid environmental changes or novel conditions.","This paper presents a novel, lidar-only, online adaptive traversability estimation (TE) method that trains a model directly on the robot using self-supervised data collected through robot-environment interaction.","The proposed approach utilises a probabilistic 3D voxel representation to integrate lidar measurements and robot experience, creating a salient environmental model.","To ensure computational efficiency, a sparse graph-based representation is employed to update temporarily evolving voxel distributions.","Extensive experiments with an unmanned ground vehicle in natural terrain demonstrate that the system adapts to complex environments with as little as 8 minutes of operational data, achieving a Matthews Correlation Coefficient (MCC) score of 0.63 and enabling safe navigation in densely vegetated environments.","This work examines different training strategies for voxel-based TE methods and offers recommendations for training strategies to improve adaptability.","The proposed method is validated on a robotic platform with limited computational resources (25W GPU), achieving accuracy comparable to offline-trained models while maintaining reliable performance across varied environments."],"url":"http://arxiv.org/abs/2502.01987v1"}
{"created":"2025-02-04 03:59:17","title":"Ilargi: a GPU Compatible Factorized ML Model Training Framework","abstract":"The machine learning (ML) training over disparate data sources traditionally involves materialization, which can impose substantial time and space overhead due to data movement and replication. Factorized learning, which leverages direct computation on disparate sources through linear algebra (LA) rewriting, has emerged as a viable alternative to improve computational efficiency. However, the adaptation of factorized learning to leverage the full capabilities of modern LA-friendly hardware like GPUs has been limited, often requiring manual intervention for algorithm compatibility. This paper introduces Ilargi, a novel factorized learning framework that utilizes matrix-represented data integration (DI) metadata to facilitate automatic factorization across CPU and GPU environments without the need for costly relational joins. Ilargi incorporates an ML-based cost estimator to intelligently selects between factorization and materialization based on data properties, algorithm complexity, hardware environments, and their interactions. This strategy ensures up to 8.9x speedups on GPUs and achieves over 20% acceleration in batch ML training workloads, thereby enhancing the practicability of ML training across diverse data integration scenarios and hardware platforms. To our knowledge, this work is the very first effort in GPU-compatible factorized learning.","sentences":["The machine learning (ML) training over disparate data sources traditionally involves materialization, which can impose substantial time and space overhead due to data movement and replication.","Factorized learning, which leverages direct computation on disparate sources through linear algebra (LA) rewriting, has emerged as a viable alternative to improve computational efficiency.","However, the adaptation of factorized learning to leverage the full capabilities of modern LA-friendly hardware like GPUs has been limited, often requiring manual intervention for algorithm compatibility.","This paper introduces Ilargi, a novel factorized learning framework that utilizes matrix-represented data integration (DI) metadata to facilitate automatic factorization across CPU and GPU environments without the need for costly relational joins.","Ilargi incorporates an ML-based cost estimator to intelligently selects between factorization and materialization based on data properties, algorithm complexity, hardware environments, and their interactions.","This strategy ensures up to 8.9x speedups on GPUs and achieves over 20% acceleration in batch ML training workloads, thereby enhancing the practicability of ML training across diverse data integration scenarios and hardware platforms.","To our knowledge, this work is the very first effort in GPU-compatible factorized learning."],"url":"http://arxiv.org/abs/2502.01985v1"}
{"created":"2025-02-04 03:52:45","title":"Evaluating Fault Tolerance and Scalability in Distributed File Systems: A Case Study of GFS, HDFS, and MinIO","abstract":"Distributed File Systems (DFS) are essential for managing vast datasets across multiple servers, offering benefits in scalability, fault tolerance, and data accessibility. This paper presents a comprehensive evaluation of three prominent DFSs - Google File System (GFS), Hadoop Distributed File System (HDFS), and MinIO - focusing on their fault tolerance mechanisms and scalability under varying data loads and client demands. Through detailed analysis, how these systems handle data redundancy, server failures, and client access protocols, ensuring reliability in dynamic, large-scale environments is assessed. In addition, the impact of system design on performance, particularly in distributed cloud and computing architectures is assessed. By comparing the strengths and limitations of each DFS, the paper provides practical insights for selecting the most appropriate system for different enterprise needs, from high availability storage to big data analytics.","sentences":["Distributed File Systems (DFS) are essential for managing vast datasets across multiple servers, offering benefits in scalability, fault tolerance, and data accessibility.","This paper presents a comprehensive evaluation of three prominent DFSs - Google File System (GFS), Hadoop Distributed File System (HDFS), and MinIO - focusing on their fault tolerance mechanisms and scalability under varying data loads and client demands.","Through detailed analysis, how these systems handle data redundancy, server failures, and client access protocols, ensuring reliability in dynamic, large-scale environments is assessed.","In addition, the impact of system design on performance, particularly in distributed cloud and computing architectures is assessed.","By comparing the strengths and limitations of each DFS, the paper provides practical insights for selecting the most appropriate system for different enterprise needs, from high availability storage to big data analytics."],"url":"http://arxiv.org/abs/2502.01981v1"}
{"created":"2025-02-04 03:51:00","title":"Generative Data Mining with Longtail-Guided Diffusion","abstract":"It is difficult to anticipate the myriad challenges that a predictive model will encounter once deployed. Common practice entails a reactive, cyclical approach: model deployment, data mining, and retraining. We instead develop a proactive longtail discovery process by imagining additional data during training. In particular, we develop general model-based longtail signals, including a differentiable, single forward pass formulation of epistemic uncertainty that does not impact model parameters or predictive performance but can flag rare or hard inputs. We leverage these signals as guidance to generate additional training data from a latent diffusion model in a process we call Longtail Guidance (LTG). Crucially, we can perform LTG without retraining the diffusion model or the predictive model, and we do not need to expose the predictive model to intermediate diffusion states. Data generated by LTG exhibit semantically meaningful variation, yield significant generalization improvements on image classification benchmarks, and can be analyzed to proactively discover, explain, and address conceptual gaps in a predictive model.","sentences":["It is difficult to anticipate the myriad challenges that a predictive model will encounter once deployed.","Common practice entails a reactive, cyclical approach: model deployment, data mining, and retraining.","We instead develop a proactive longtail discovery process by imagining additional data during training.","In particular, we develop general model-based longtail signals, including a differentiable, single forward pass formulation of epistemic uncertainty that does not impact model parameters or predictive performance but can flag rare or hard inputs.","We leverage these signals as guidance to generate additional training data from a latent diffusion model in a process we call Longtail Guidance (LTG).","Crucially, we can perform LTG without retraining the diffusion model or the predictive model, and we do not need to expose the predictive model to intermediate diffusion states.","Data generated by LTG exhibit semantically meaningful variation, yield significant generalization improvements on image classification benchmarks, and can be analyzed to proactively discover, explain, and address conceptual gaps in a predictive model."],"url":"http://arxiv.org/abs/2502.01980v1"}
{"created":"2025-02-04 03:39:59","title":"AutoGUI: Scaling GUI Grounding with Automatic Functionality Annotations from LLMs","abstract":"User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation. However, existing UI datasets either only provide large-scale context-free element annotations or contextualized functional descriptions for elements at a much smaller scale. In this work, we propose the \\methodname{} pipeline for automatically annotating UI elements with detailed functionality descriptions at scale. Specifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI content changes before and after simulated interactions with specific UI elements. To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid and incorrect annotations without human labor. We construct an \\methodname{}-704k dataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations that have never been provided by previous datasets. Human evaluation shows that the AutoGUI pipeline achieves annotation correctness comparable to trained human annotators. Extensive experimental results show that our \\methodname{}-704k dataset remarkably enhances VLM's UI grounding capabilities, exhibits significant scaling effects, and outperforms existing web pre-training data types. We envision AutoGUI as a scalable pipeline for generating massive data to build GUI-oriented VLMs. AutoGUI dataset can be viewed at this anonymous URL: https://autogui-project.github.io/.","sentences":["User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation.","However, existing UI datasets either only provide large-scale context-free element annotations or contextualized functional descriptions for elements at a much smaller scale.","In this work, we propose the \\methodname{} pipeline for automatically annotating UI elements with detailed functionality descriptions at scale.","Specifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI content changes before and after simulated interactions with specific UI elements.","To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid and incorrect annotations without human labor.","We construct an \\methodname{}-704k dataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations that have never been provided by previous datasets.","Human evaluation shows that the AutoGUI pipeline achieves annotation correctness comparable to trained human annotators.","Extensive experimental results show that our \\methodname{}-704k dataset remarkably enhances VLM's UI grounding capabilities, exhibits significant scaling effects, and outperforms existing web pre-training data types.","We envision AutoGUI as a scalable pipeline for generating massive data to build GUI-oriented VLMs.","AutoGUI dataset can be viewed at this anonymous URL: https://autogui-project.github.io/."],"url":"http://arxiv.org/abs/2502.01977v1"}
{"created":"2025-02-04 03:26:58","title":"Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning","abstract":"Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance. In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks. Our method filters out uninformative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves performance across multiple downstream tasks.","sentences":["Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity.","While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly.","After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative.","Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance.","In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks.","Our method filters out uninformative tokens while preserving those carrying key task-specific information.","Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation.","The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models.","The benefits and limitations of both methods are analyzed theoretically by error upper bounds.","Extensive experiments show that our framework consistently improves performance across multiple downstream tasks."],"url":"http://arxiv.org/abs/2502.01968v1"}
{"created":"2025-02-04 03:25:01","title":"Optimizing Spot Instance Reliability and Security Using Cloud-Native Data and Tools","abstract":"This paper represents \"Cloudlab\", a comprehensive, cloud - native laboratory designed to support network security research and training. Built on Google Cloud and adhering to GitOps methodologies, Cloudlab facilitates the the creation, testing, and deployment of secure, containerized workloads using Kubernetes and serverless architectures. The lab integrates tools like Palo Alto Networks firewalls, Bridgecrew for \"Security as Code,\" and automated GitHub workflows to establish a robust Continuous Integration/Continuous Machine Learning pipeline. By providing an adaptive and scalable environment, Cloudlab supports advanced security concepts such as role-based access control, Policy as Code, and container security. This initiative enables data scientists and engineers to explore cutting-edge practices in a dynamic cloud-native ecosystem, fostering innovation and improving operational resilience in modern IT infrastructures.","sentences":["This paper represents \"Cloudlab\", a comprehensive, cloud - native laboratory designed to support network security research and training.","Built on Google Cloud and adhering to GitOps methodologies, Cloudlab facilitates the the creation, testing, and deployment of secure, containerized workloads using Kubernetes and serverless architectures.","The lab integrates tools like Palo Alto Networks firewalls, Bridgecrew for \"Security as Code,\" and automated GitHub workflows to establish a robust Continuous Integration/Continuous Machine Learning pipeline.","By providing an adaptive and scalable environment, Cloudlab supports advanced security concepts such as role-based access control, Policy as Code, and container security.","This initiative enables data scientists and engineers to explore cutting-edge practices in a dynamic cloud-native ecosystem, fostering innovation and improving operational resilience in modern IT infrastructures."],"url":"http://arxiv.org/abs/2502.01966v1"}
{"created":"2025-02-04 03:13:09","title":"MPIC: Position-Independent Multimodal Context Caching System for Efficient MLLM Serving","abstract":"The context caching technique is employed to accelerate the Multimodal Large Language Model (MLLM) inference by prevailing serving platforms currently. However, this approach merely reuses the Key-Value (KV) cache of the initial sequence of prompt, resulting in full KV cache recomputation even if the prefix differs slightly. This becomes particularly inefficient in the context of interleaved text and images, as well as multimodal retrieval-augmented generation. This paper proposes position-independent caching as a more effective approach for multimodal information management. We have designed and implemented a caching system, named MPIC, to address both system-level and algorithm-level challenges. MPIC stores the KV cache on local or remote disks when receiving multimodal data, and calculates and loads the KV cache in parallel during inference. To mitigate accuracy degradation, we have incorporated integrated reuse and recompute mechanisms within the system. The experimental results demonstrate that MPIC can achieve up to 54% reduction in response time compared to existing context caching systems, while maintaining negligible or no accuracy loss.","sentences":["The context caching technique is employed to accelerate the Multimodal Large Language Model (MLLM) inference by prevailing serving platforms currently.","However, this approach merely reuses the Key-Value (KV) cache of the initial sequence of prompt, resulting in full KV cache recomputation even if the prefix differs slightly.","This becomes particularly inefficient in the context of interleaved text and images, as well as multimodal retrieval-augmented generation.","This paper proposes position-independent caching as a more effective approach for multimodal information management.","We have designed and implemented a caching system, named MPIC, to address both system-level and algorithm-level challenges.","MPIC stores the KV cache on local or remote disks when receiving multimodal data, and calculates and loads the KV cache in parallel during inference.","To mitigate accuracy degradation, we have incorporated integrated reuse and recompute mechanisms within the system.","The experimental results demonstrate that MPIC can achieve up to 54% reduction in response time compared to existing context caching systems, while maintaining negligible or no accuracy loss."],"url":"http://arxiv.org/abs/2502.01960v1"}
{"created":"2025-02-04 03:05:55","title":"DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents","abstract":"In this paper, we address the challenge of long-horizon visual planning tasks using Hierarchical Reinforcement Learning (HRL). Our key contribution is a Discrete Hierarchical Planning (DHP) method, an alternative to traditional distance-based approaches. We provide theoretical foundations for the method and demonstrate its effectiveness through extensive empirical evaluations.   Our agent recursively predicts subgoals in the context of a long-term goal and receives discrete rewards for constructing plans as compositions of abstract actions. The method introduces a novel advantage estimation strategy for tree trajectories, which inherently encourages shorter plans and enables generalization beyond the maximum tree depth. The learned policy function allows the agent to plan efficiently, requiring only $\\log N$ computational steps, making re-planning highly efficient. The agent, based on a soft-actor critic (SAC) framework, is trained using on-policy imagination data. Additionally, we propose a novel exploration strategy that enables the agent to generate relevant training examples for the planning modules. We evaluate our method on long-horizon visual planning tasks in a 25-room environment, where it significantly outperforms previous benchmarks at success rate and average episode length. Furthermore, an ablation study highlights the individual contributions of key modules to the overall performance.","sentences":["In this paper, we address the challenge of long-horizon visual planning tasks using Hierarchical Reinforcement Learning (HRL).","Our key contribution is a Discrete Hierarchical Planning (DHP) method, an alternative to traditional distance-based approaches.","We provide theoretical foundations for the method and demonstrate its effectiveness through extensive empirical evaluations.   ","Our agent recursively predicts subgoals in the context of a long-term goal and receives discrete rewards for constructing plans as compositions of abstract actions.","The method introduces a novel advantage estimation strategy for tree trajectories, which inherently encourages shorter plans and enables generalization beyond the maximum tree depth.","The learned policy function allows the agent to plan efficiently, requiring only $\\log N$ computational steps, making re-planning highly efficient.","The agent, based on a soft-actor critic (SAC) framework, is trained using on-policy imagination data.","Additionally, we propose a novel exploration strategy that enables the agent to generate relevant training examples for the planning modules.","We evaluate our method on long-horizon visual planning tasks in a 25-room environment, where it significantly outperforms previous benchmarks at success rate and average episode length.","Furthermore, an ablation study highlights the individual contributions of key modules to the overall performance."],"url":"http://arxiv.org/abs/2502.01956v1"}
{"created":"2025-02-04 02:30:36","title":"DAMO: Data- and Model-aware Alignment of Multi-modal LLMs","abstract":"Direct Preference Optimization (DPO) has shown effectiveness in aligning multi-modal large language models (MLLM) with human preferences. However, existing methods exhibit an imbalanced responsiveness to the data of varying hardness, tending to overfit on the easy-to-distinguish data while underfitting on the hard-to-distinguish data. In this paper, we propose Data- and Model-aware DPO (DAMO) to dynamically adjust the optimization process from two key aspects: (1) a data-aware strategy that incorporates data hardness, and (2) a model-aware strategy that integrates real-time model responses. By combining the two strategies, DAMO enables the model to effectively adapt to data with varying levels of hardness. Extensive experiments on five benchmarks demonstrate that DAMO not only significantly enhances the trustworthiness, but also improves the effectiveness over general tasks. For instance, on the Object HalBench, our DAMO-7B reduces response-level and mentioned-level hallucination by 90.0% and 95.3%, respectively, surpassing the performance of GPT-4V.","sentences":["Direct Preference Optimization (DPO) has shown effectiveness in aligning multi-modal large language models (MLLM) with human preferences.","However, existing methods exhibit an imbalanced responsiveness to the data of varying hardness, tending to overfit on the easy-to-distinguish data while underfitting on the hard-to-distinguish data.","In this paper, we propose Data- and Model-aware DPO (DAMO) to dynamically adjust the optimization process from two key aspects: (1) a data-aware strategy that incorporates data hardness, and (2) a model-aware strategy that integrates real-time model responses.","By combining the two strategies, DAMO enables the model to effectively adapt to data with varying levels of hardness.","Extensive experiments on five benchmarks demonstrate that DAMO not only significantly enhances the trustworthiness, but also improves the effectiveness over general tasks.","For instance, on the Object HalBench, our DAMO-7B reduces response-level and mentioned-level hallucination by 90.0% and 95.3%, respectively, surpassing the performance of GPT-4V."],"url":"http://arxiv.org/abs/2502.01943v1"}
{"created":"2025-02-04 02:20:52","title":"Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum Learning Approach","abstract":"We present a cost-effective new approach for generating denser depth maps for Autonomous Driving (AD) and Autonomous Vehicles (AVs) by integrating the images obtained from deep neural network (DNN) 4D radar detectors with conventional camera RGB images. Our approach introduces a novel pixel positional encoding algorithm inspired by Bartlett's spatial spectrum estimation technique. This algorithm transforms both radar depth maps and RGB images into a unified pixel image subspace called the Spatial Spectrum, facilitating effective learning based on their similarities and differences. Our method effectively leverages high-resolution camera images to train radar depth map generative models, addressing the limitations of conventional radar detectors in complex vehicular environments, thus sharpening the radar output. We develop spectrum estimation algorithms tailored for radar depth maps and RGB images, a comprehensive training framework for data-driven generative models, and a camera-radar deployment scheme for AV operation. Our results demonstrate that our approach also outperforms the state-of-the-art (SOTA) by 27.95% in terms of Unidirectional Chamfer Distance (UCD).","sentences":["We present a cost-effective new approach for generating denser depth maps for Autonomous Driving (AD) and Autonomous Vehicles (AVs) by integrating the images obtained from deep neural network (DNN) 4D radar detectors with conventional camera RGB images.","Our approach introduces a novel pixel positional encoding algorithm inspired by Bartlett's spatial spectrum estimation technique.","This algorithm transforms both radar depth maps and RGB images into a unified pixel image subspace called the Spatial Spectrum, facilitating effective learning based on their similarities and differences.","Our method effectively leverages high-resolution camera images to train radar depth map generative models, addressing the limitations of conventional radar detectors in complex vehicular environments, thus sharpening the radar output.","We develop spectrum estimation algorithms tailored for radar depth maps and RGB images, a comprehensive training framework for data-driven generative models, and a camera-radar deployment scheme for AV operation.","Our results demonstrate that our approach also outperforms the state-of-the-art (SOTA) by 27.95% in terms of Unidirectional Chamfer Distance (UCD)."],"url":"http://arxiv.org/abs/2502.01940v1"}
{"created":"2025-02-04 02:13:05","title":"A Comprehensive Study of Bug-Fix Patterns in Autonomous Driving Systems","abstract":"As autonomous driving systems (ADSes) become increasingly complex and integral to daily life, the importance of understanding the nature and mitigation of software bugs in these systems has grown correspondingly. Addressing the challenges of software maintenance in autonomous driving systems (e.g., handling real-time system decisions and ensuring safety-critical reliability) is crucial due to the unique combination of real-time decision-making requirements and the high stakes of operational failures in ADSes. The potential of automated tools in this domain is promising, yet there remains a gap in our comprehension of the challenges faced and the strategies employed during manual debugging and repair of such systems. In this paper, we present an empirical study that investigates bug-fix patterns in ADSes, with the aim of improving reliability and safety. We have analyzed the commit histories and bug reports of two major autonomous driving projects, Apollo and Autoware, from 1,331 bug fixes with the study of bug symptoms, root causes, and bug-fix patterns. Our study reveals several dominant bug-fix patterns, including those related to path planning, data flow, and configuration management. Additionally, we find that the frequency distribution of bug-fix patterns varies significantly depending on their nature and types and that certain categories of bugs are recurrent and more challenging to exterminate. Based on our findings, we propose a hierarchy of ADS bugs and two taxonomies of 15 syntactic bug-fix patterns and 27 semantic bug-fix patterns that offer guidance for bug identification and resolution. We also contribute a benchmark of 1,331 ADS bug-fix instances.","sentences":["As autonomous driving systems (ADSes) become increasingly complex and integral to daily life, the importance of understanding the nature and mitigation of software bugs in these systems has grown correspondingly.","Addressing the challenges of software maintenance in autonomous driving systems (e.g., handling real-time system decisions and ensuring safety-critical reliability) is crucial due to the unique combination of real-time decision-making requirements and the high stakes of operational failures in ADSes.","The potential of automated tools in this domain is promising, yet there remains a gap in our comprehension of the challenges faced and the strategies employed during manual debugging and repair of such systems.","In this paper, we present an empirical study that investigates bug-fix patterns in ADSes, with the aim of improving reliability and safety.","We have analyzed the commit histories and bug reports of two major autonomous driving projects, Apollo and Autoware, from 1,331 bug fixes with the study of bug symptoms, root causes, and bug-fix patterns.","Our study reveals several dominant bug-fix patterns, including those related to path planning, data flow, and configuration management.","Additionally, we find that the frequency distribution of bug-fix patterns varies significantly depending on their nature and types and that certain categories of bugs are recurrent and more challenging to exterminate.","Based on our findings, we propose a hierarchy of ADS bugs and two taxonomies of 15 syntactic bug-fix patterns and 27 semantic bug-fix patterns that offer guidance for bug identification and resolution.","We also contribute a benchmark of 1,331 ADS bug-fix instances."],"url":"http://arxiv.org/abs/2502.01937v1"}
{"created":"2025-02-04 01:46:26","title":"Improving Software Engineering Team Communication Through Stronger Social Networks","abstract":"Students working in teams in software engineering group project often communicate ineffectively, which reduces the quality of deliverables, and is therefore detrimental for project success. An important step towards addressing areas of improvement is identifying which changes to communication will improve team performance the most. We applied two different communication analysis techniques, triad census and socio-technical congruence, to data gathered from a two-semester software engineering group project. Triad census uses the presence of edges between groups of three nodes as a measure of network structure, while socio-technical congruence compares the fit of a team's communication to their technical dependencies. Our findings suggest that each team's triad census for a given sprint is promising as a predictor of the percentage of story points they pass, which is closely linked to project success. Meanwhile, socio-technical congruence is inadequate as the sole metric for predicting project success in this context. We discuss these findings, and their potential applications improve communication in a software engineering group project.","sentences":["Students working in teams in software engineering group project often communicate ineffectively, which reduces the quality of deliverables, and is therefore detrimental for project success.","An important step towards addressing areas of improvement is identifying which changes to communication will improve team performance the most.","We applied two different communication analysis techniques, triad census and socio-technical congruence, to data gathered from a two-semester software engineering group project.","Triad census uses the presence of edges between groups of three nodes as a measure of network structure, while socio-technical congruence compares the fit of a team's communication to their technical dependencies.","Our findings suggest that each team's triad census for a given sprint is promising as a predictor of the percentage of story points they pass, which is closely linked to project success.","Meanwhile, socio-technical congruence is inadequate as the sole metric for predicting project success in this context.","We discuss these findings, and their potential applications improve communication in a software engineering group project."],"url":"http://arxiv.org/abs/2502.01923v1"}
{"created":"2025-02-04 01:42:45","title":"LAST SToP For Modeling Asynchronous Time Series","abstract":"We present a novel prompt design for Large Language Models (LLMs) tailored to Asynchronous Time Series. Unlike regular time series, which assume values at evenly spaced time points, asynchronous time series consist of timestamped events occurring at irregular intervals, each described in natural language. Our approach effectively utilizes the rich natural language of event descriptions, allowing LLMs to benefit from their broad world knowledge for reasoning across different domains and tasks. This allows us to extend the scope of asynchronous time series analysis beyond forecasting to include tasks like anomaly detection and data imputation. We further introduce Stochastic Soft Prompting, a novel prompt-tuning mechanism that significantly improves model performance, outperforming existing fine-tuning methods such as QLoRA. Through extensive experiments on real world datasets, we demonstrate that our approach achieves state-of-the-art performance across different tasks and datasets.","sentences":["We present a novel prompt design for Large Language Models (LLMs) tailored to Asynchronous Time Series.","Unlike regular time series, which assume values at evenly spaced time points, asynchronous time series consist of timestamped events occurring at irregular intervals, each described in natural language.","Our approach effectively utilizes the rich natural language of event descriptions, allowing LLMs to benefit from their broad world knowledge for reasoning across different domains and tasks.","This allows us to extend the scope of asynchronous time series analysis beyond forecasting to include tasks like anomaly detection and data imputation.","We further introduce Stochastic Soft Prompting, a novel prompt-tuning mechanism that significantly improves model performance, outperforming existing fine-tuning methods such as QLoRA.","Through extensive experiments on real world datasets, we demonstrate that our approach achieves state-of-the-art performance across different tasks and datasets."],"url":"http://arxiv.org/abs/2502.01922v1"}
{"created":"2025-02-04 01:29:22","title":"Anomaly Detection via Autoencoder Composite Features and NCE","abstract":"Unsupervised anomaly detection is a challenging task. Autoencoders (AEs) or generative models are often employed to model the data distribution of normal inputs and subsequently identify anomalous, out-of-distribution inputs by high reconstruction error or low likelihood, respectively. However, AEs may generalize and achieve small reconstruction errors on abnormal inputs. We propose a decoupled training approach for anomaly detection that both an AE and a likelihood model trained with noise contrastive estimation (NCE). After training the AE, NCE estimates a probability density function, to serve as the anomaly score, on the joint space of the AE's latent representation combined with features of the reconstruction quality. To further reduce the false negative rate in NCE we systematically varying the reconstruction features to augment the training and optimize the contrastive Gaussian noise distribution. Experimental assessments on multiple benchmark datasets demonstrate that the proposed approach matches the performance of prevalent state-of-the-art anomaly detection algorithms.","sentences":["Unsupervised anomaly detection is a challenging task.","Autoencoders (AEs) or generative models are often employed to model the data distribution of normal inputs and subsequently identify anomalous, out-of-distribution inputs by high reconstruction error or low likelihood, respectively.","However, AEs may generalize and achieve small reconstruction errors on abnormal inputs.","We propose a decoupled training approach for anomaly detection that both an AE and a likelihood model trained with noise contrastive estimation (NCE).","After training the AE, NCE estimates a probability density function, to serve as the anomaly score, on the joint space of the AE's latent representation combined with features of the reconstruction quality.","To further reduce the false negative rate in NCE we systematically varying the reconstruction features to augment the training and optimize the contrastive Gaussian noise distribution.","Experimental assessments on multiple benchmark datasets demonstrate that the proposed approach matches the performance of prevalent state-of-the-art anomaly detection algorithms."],"url":"http://arxiv.org/abs/2502.01920v1"}
{"created":"2025-02-04 01:16:33","title":"Generalizable and Fast Surrogates: Model Predictive Control of Articulated Soft Robots using Physics-Informed Neural Networks","abstract":"Soft robots can revolutionize several applications with high demands on dexterity and safety. When operating these systems, real-time estimation and control require fast and accurate models. However, prediction with first-principles (FP) models is slow, and learned black-box models have poor generalizability. Physics-informed machine learning offers excellent advantages here, but it is currently limited to simple, often simulated systems without considering changes after training. We propose physics-informed neural networks (PINNs) for articulated soft robots (ASRs) with a focus on data efficiency. The amount of expensive real-world training data is reduced to a minimum - one dataset in one system domain. Two hours of data in different domains are used for a comparison against two gold-standard approaches: In contrast to a recurrent neural network, the PINN provides a high generalizability. The prediction speed of an accurate FP model is improved with the PINN by up to a factor of 466 at slightly reduced accuracy. This enables nonlinear model predictive control (MPC) of the pneumatic ASR. In nine dynamic MPC experiments, an average joint-tracking error of 1.3{\\deg} is achieved.","sentences":["Soft robots can revolutionize several applications with high demands on dexterity and safety.","When operating these systems, real-time estimation and control require fast and accurate models.","However, prediction with first-principles (FP) models is slow, and learned black-box models have poor generalizability.","Physics-informed machine learning offers excellent advantages here, but it is currently limited to simple, often simulated systems without considering changes after training.","We propose physics-informed neural networks (PINNs) for articulated soft robots (ASRs) with a focus on data efficiency.","The amount of expensive real-world training data is reduced to a minimum - one dataset in one system domain.","Two hours of data in different domains are used for a comparison against two gold-standard approaches: In contrast to a recurrent neural network, the PINN provides a high generalizability.","The prediction speed of an accurate FP model is improved with the PINN by up to a factor of 466 at slightly reduced accuracy.","This enables nonlinear model predictive control (MPC) of the pneumatic ASR.","In nine dynamic MPC experiments, an average joint-tracking error of 1.3{\\deg} is achieved."],"url":"http://arxiv.org/abs/2502.01916v1"}
{"created":"2025-02-04 01:05:12","title":"PATCH: a deep learning method to assess heterogeneity of artistic practice in historical paintings","abstract":"The history of art has seen significant shifts in the manner in which artworks are created, making understanding of creative processes a central question in technical art history. In the Renaissance and Early Modern period, paintings were largely produced by master painters directing workshops of apprentices who often contributed to projects. The masters varied significantly in artistic and managerial styles, meaning different combinations of artists and implements might be seen both between masters and within workshops or even individual canvases. Information on how different workshops were managed and the processes by which artworks were created remains elusive. Machine learning methods have potential to unearth new information about artists' creative processes by extending the analysis of brushwork to a microscopic scale. Analysis of workshop paintings, however, presents a challenge in that documentation of the artists and materials involved is sparse, meaning external examples are not available to train networks to recognize their contributions. Here we present a novel machine learning approach we call pairwise assignment training for classifying heterogeneity (PATCH) that is capable of identifying individual artistic practice regimes with no external training data, or \"ground truth.\" The method achieves unsupervised results by supervised means, and outperforms both simple statistical procedures and unsupervised machine learning methods. We apply this method to two historical paintings by the Spanish Renaissance master, El Greco: The Baptism of Christ and Christ on the Cross with Landscape, and our findings regarding the former potentially challenge previous work that has assigned the painting to workshop members. Further, the results of our analyses create a measure of heterogeneity of artistic practice that can be used to characterize artworks across time and space.","sentences":["The history of art has seen significant shifts in the manner in which artworks are created, making understanding of creative processes a central question in technical art history.","In the Renaissance and Early Modern period, paintings were largely produced by master painters directing workshops of apprentices who often contributed to projects.","The masters varied significantly in artistic and managerial styles, meaning different combinations of artists and implements might be seen both between masters and within workshops or even individual canvases.","Information on how different workshops were managed and the processes by which artworks were created remains elusive.","Machine learning methods have potential to unearth new information about artists' creative processes by extending the analysis of brushwork to a microscopic scale.","Analysis of workshop paintings, however, presents a challenge in that documentation of the artists and materials involved is sparse, meaning external examples are not available to train networks to recognize their contributions.","Here we present a novel machine learning approach we call pairwise assignment training for classifying heterogeneity (PATCH) that is capable of identifying individual artistic practice regimes with no external training data, or \"ground truth.\"","The method achieves unsupervised results by supervised means, and outperforms both simple statistical procedures and unsupervised machine learning methods.","We apply this method to two historical paintings by the Spanish Renaissance master, El Greco: The Baptism of Christ and Christ on the Cross with Landscape, and our findings regarding the former potentially challenge previous work that has assigned the painting to workshop members.","Further, the results of our analyses create a measure of heterogeneity of artistic practice that can be used to characterize artworks across time and space."],"url":"http://arxiv.org/abs/2502.01912v1"}
{"created":"2025-02-04 00:55:59","title":"A Multi-Objective Framework for Optimizing GPU-Enabled VM Placement in Cloud Data Centers with Multi-Instance GPU Technology","abstract":"The extensive use of GPUs in cloud computing and the growing need for multitenancy have driven the development of innovative solutions for efficient GPU resource management. Multi-Instance GPU (MIG) technology from NVIDIA enables shared GPU usage in cloud data centers by providing isolated instances. However, MIG placement rules often lead to fragmentation and suboptimal resource utilization. In this work, we formally model the MIG-enabled VM placement as a multi-objective Integer Linear Programming (ILP) problem aimed at maximizing request acceptance, minimizing active hardware usage, and reducing migration overhead. Building upon this formulation, we propose GRMU, a multi-stage placement framework designed to address MIG placement challenges. GRMU performs intra-GPU migrations for defragmentation of a single GPU and inter-GPU migrations for consolidation and resource efficiency. It also employs a quota-based partitioning approach to allocate GPUs into two distinct baskets: one for large-profile workloads and another for smaller-profile workloads. Each basket has predefined capacity limits, ensuring fair resource distribution and preventing large-profile workloads from monopolizing resources. Evaluations on a real-world Alibaba GPU cluster trace reveal that GRMU improves acceptance rates by 22%, reduces active hardware by 17%, and incurs migration for only 1% of MIG-enabled VMs, demonstrating its effectiveness in minimizing fragmentation and improving resource utilization.","sentences":["The extensive use of GPUs in cloud computing and the growing need for multitenancy have driven the development of innovative solutions for efficient GPU resource management.","Multi-Instance GPU (MIG) technology from NVIDIA enables shared GPU usage in cloud data centers by providing isolated instances.","However, MIG placement rules often lead to fragmentation and suboptimal resource utilization.","In this work, we formally model the MIG-enabled VM placement as a multi-objective Integer Linear Programming (ILP) problem aimed at maximizing request acceptance, minimizing active hardware usage, and reducing migration overhead.","Building upon this formulation, we propose GRMU, a multi-stage placement framework designed to address MIG placement challenges.","GRMU performs intra-GPU migrations for defragmentation of a single GPU and inter-GPU migrations for consolidation and resource efficiency.","It also employs a quota-based partitioning approach to allocate GPUs into two distinct baskets: one for large-profile workloads and another for smaller-profile workloads.","Each basket has predefined capacity limits, ensuring fair resource distribution and preventing large-profile workloads from monopolizing resources.","Evaluations on a real-world Alibaba GPU cluster trace reveal that GRMU improves acceptance rates by 22%, reduces active hardware by 17%, and incurs migration for only 1% of MIG-enabled VMs, demonstrating its effectiveness in minimizing fragmentation and improving resource utilization."],"url":"http://arxiv.org/abs/2502.01909v1"}
{"created":"2025-02-04 00:46:11","title":"Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models","abstract":"Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM). However, these inputs are inherently different: visual inputs are multi-dimensional and contextually rich, often pre-encoded by models like CLIP, while textual inputs lack this structure. In this paper, we propose Decomposed Attention (D-Attn), a novel method that processes visual and textual embeddings differently by decomposing the 1-D causal self-attention in LVLMs. After the attention decomposition, D-Attn diagonalizes visual-to-visual self-attention, reducing computation from $\\mathcal{O}(|V|^2)$ to $\\mathcal{O}(|V|)$ for $|V|$ visual embeddings without compromising performance. Moreover, D-Attn debiases positional encodings in textual-to-visual cross-attention, further enhancing visual understanding. Finally, we introduce an $\\alpha$-weighting strategy to merge visual and textual information, maximally preserving the pre-trained LLM's capabilities with minimal modifications. Extensive experiments and rigorous analyses validate the effectiveness of D-Attn, demonstrating significant improvements on multiple image benchmarks while significantly reducing computational costs. Code, data, and models will be publicly available.","sentences":["Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM).","However, these inputs are inherently different: visual inputs are multi-dimensional and contextually rich, often pre-encoded by models like CLIP, while textual inputs lack this structure.","In this paper, we propose Decomposed Attention (D-Attn), a novel method that processes visual and textual embeddings differently by decomposing the 1-D causal self-attention in LVLMs.","After the attention decomposition, D-Attn diagonalizes visual-to-visual self-attention, reducing computation from $\\mathcal{O}(|V|^2)$ to $\\mathcal{O}(|V|)$ for $|V|$ visual embeddings without compromising performance.","Moreover, D-Attn debiases positional encodings in textual-to-visual cross-attention, further enhancing visual understanding.","Finally, we introduce an $\\alpha$-weighting strategy to merge visual and textual information, maximally preserving the pre-trained LLM's capabilities with minimal modifications.","Extensive experiments and rigorous analyses validate the effectiveness of D-Attn, demonstrating significant improvements on multiple image benchmarks while significantly reducing computational costs.","Code, data, and models will be publicly available."],"url":"http://arxiv.org/abs/2502.01906v1"}
{"created":"2025-02-04 00:33:18","title":"Common Neighborhood Estimation over Bipartite Graphs under Local Differential Privacy","abstract":"Bipartite graphs, formed by two vertex layers, arise as a natural fit for modeling the relationships between two groups of entities. In bipartite graphs, common neighborhood computation between two vertices on the same vertex layer is a basic operator, which is easily solvable in general settings. However, it inevitably involves releasing the neighborhood information of vertices, posing a significant privacy risk for users in real-world applications. To protect edge privacy in bipartite graphs, in this paper, we study the problem of estimating the number of common neighbors of two vertices on the same layer under edge local differential privacy (edge LDP). The problem is challenging in the context of edge LDP since each vertex on the opposite layer of the query vertices can potentially be a common neighbor. To obtain efficient and accurate estimates, we propose a multiple-round framework that significantly reduces the candidate pool of common neighbors and enables the query vertices to construct unbiased estimators locally. Furthermore, we improve data utility by incorporating the estimators built from the neighbors of both query vertices and devise privacy budget allocation optimizations. These improve the estimator's robustness and consistency, particularly against query vertices with imbalanced degrees. Extensive experiments on 15 datasets validate the effectiveness and efficiency of our proposed techniques.","sentences":["Bipartite graphs, formed by two vertex layers, arise as a natural fit for modeling the relationships between two groups of entities.","In bipartite graphs, common neighborhood computation between two vertices on the same vertex layer is a basic operator, which is easily solvable in general settings.","However, it inevitably involves releasing the neighborhood information of vertices, posing a significant privacy risk for users in real-world applications.","To protect edge privacy in bipartite graphs, in this paper, we study the problem of estimating the number of common neighbors of two vertices on the same layer under edge local differential privacy (edge LDP).","The problem is challenging in the context of edge LDP since each vertex on the opposite layer of the query vertices can potentially be a common neighbor.","To obtain efficient and accurate estimates, we propose a multiple-round framework that significantly reduces the candidate pool of common neighbors and enables the query vertices to construct unbiased estimators locally.","Furthermore, we improve data utility by incorporating the estimators built from the neighbors of both query vertices and devise privacy budget allocation optimizations.","These improve the estimator's robustness and consistency, particularly against query vertices with imbalanced degrees.","Extensive experiments on 15 datasets validate the effectiveness and efficiency of our proposed techniques."],"url":"http://arxiv.org/abs/2502.01904v1"}
{"created":"2025-02-04 00:02:16","title":"INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy","abstract":"In this work, we present INTACT, a novel two-phase framework designed to enhance the robustness of deep neural networks (DNNs) against noisy LiDAR data in safety-critical perception tasks. INTACT combines meta-learning with adversarial curriculum training (ACT) to systematically address challenges posed by data corruption and sparsity in 3D point clouds. The meta-learning phase equips a teacher network with task-agnostic priors, enabling it to generate robust saliency maps that identify critical data regions. The ACT phase leverages these saliency maps to progressively expose a student network to increasingly complex noise patterns, ensuring targeted perturbation and improved noise resilience. INTACT's effectiveness is demonstrated through comprehensive evaluations on object detection, tracking, and classification benchmarks using diverse datasets, including KITTI, Argoverse, and ModelNet40. Results indicate that INTACT improves model robustness by up to 20% across all tasks, outperforming standard adversarial and curriculum training methods. This framework not only addresses the limitations of conventional training strategies but also offers a scalable and efficient solution for real-world deployment in resource-constrained safety-critical systems. INTACT's principled integration of meta-learning and adversarial training establishes a new paradigm for noise-tolerant 3D perception in safety-critical applications. INTACT improved KITTI Multiple Object Tracking Accuracy (MOTA) by 9.6% (64.1% -> 75.1%) and by 12.4% under Gaussian noise (52.5% -> 73.7%). Similarly, KITTI mean Average Precision (mAP) rose from 59.8% to 69.8% (50% point drop) and 49.3% to 70.9% (Gaussian noise), highlighting the framework's ability to enhance deep learning model resilience in safety-critical object tracking scenarios.","sentences":["In this work, we present INTACT, a novel two-phase framework designed to enhance the robustness of deep neural networks (DNNs) against noisy LiDAR data in safety-critical perception tasks.","INTACT combines meta-learning with adversarial curriculum training (ACT) to systematically address challenges posed by data corruption and sparsity in 3D point clouds.","The meta-learning phase equips a teacher network with task-agnostic priors, enabling it to generate robust saliency maps that identify critical data regions.","The ACT phase leverages these saliency maps to progressively expose a student network to increasingly complex noise patterns, ensuring targeted perturbation and improved noise resilience.","INTACT's effectiveness is demonstrated through comprehensive evaluations on object detection, tracking, and classification benchmarks using diverse datasets, including KITTI, Argoverse, and ModelNet40.","Results indicate that INTACT improves model robustness by up to 20% across all tasks, outperforming standard adversarial and curriculum training methods.","This framework not only addresses the limitations of conventional training strategies but also offers a scalable and efficient solution for real-world deployment in resource-constrained safety-critical systems.","INTACT's principled integration of meta-learning and adversarial training establishes a new paradigm for noise-tolerant 3D perception in safety-critical applications.","INTACT improved KITTI Multiple Object Tracking Accuracy (MOTA) by 9.6% (64.1% -> 75.1%) and by 12.4% under Gaussian noise (52.5% -> 73.7%).","Similarly, KITTI mean Average Precision (mAP) rose from 59.8% to 69.8% (50% point drop) and 49.3% to 70.9% (Gaussian noise), highlighting the framework's ability to enhance deep learning model resilience in safety-critical object tracking scenarios."],"url":"http://arxiv.org/abs/2502.01896v1"}
{"created":"2025-02-04 00:00:06","title":"SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset","abstract":"Bird's-eye view (BEV) perception for autonomous driving has garnered significant attention in recent years, in part because BEV representation facilitates the fusion of multi-sensor data. This enables a variety of perception tasks including BEV segmentation, a concise view of the environment that can be used to plan a vehicle's trajectory. However, this representation is not fully supported by existing datasets, and creation of new datasets can be a time-consuming endeavor. To address this problem, in this paper we introduce SimBEV, an extensively configurable and scalable randomized synthetic data generation tool that incorporates information from multiple sources to capture accurate BEV ground truth data, supports a comprehensive array of sensors, and enables a variety of perception tasks including BEV segmentation and 3D object detection. We use SimBEV to create the SimBEV dataset, a large collection of annotated perception data from diverse driving scenarios.","sentences":["Bird's-eye view (BEV) perception for autonomous driving has garnered significant attention in recent years, in part because BEV representation facilitates the fusion of multi-sensor data.","This enables a variety of perception tasks including BEV segmentation, a concise view of the environment that can be used to plan a vehicle's trajectory.","However, this representation is not fully supported by existing datasets, and creation of new datasets can be a time-consuming endeavor.","To address this problem, in this paper we introduce SimBEV, an extensively configurable and scalable randomized synthetic data generation tool that incorporates information from multiple sources to capture accurate BEV ground truth data, supports a comprehensive array of sensors, and enables a variety of perception tasks including BEV segmentation and 3D object detection.","We use SimBEV to create the SimBEV dataset, a large collection of annotated perception data from diverse driving scenarios."],"url":"http://arxiv.org/abs/2502.01894v1"}
