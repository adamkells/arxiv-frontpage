{"created":"2025-05-14 17:59:55","title":"UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing","abstract":"Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing both uni-modal events (i.e., those occurring exclusively in either the visual or acoustic modality of a video) and multi-modal events (i.e., those occurring in both modalities concurrently). Moreover, the prohibitive cost of annotating training data with the class labels of all these events, along with their start and end times, imposes constraints on the scalability of AVVP techniques unless they can be trained in a weakly-supervised setting, where only modality-agnostic, video-level labels are available in the training data. To this end, recently proposed approaches seek to generate segment-level pseudo-labels to better guide model training. However, the absence of inter-segment dependencies when generating these pseudo-labels and the general bias towards predicting labels that are absent in a segment limit their performance. This work proposes a novel approach towards overcoming these weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV). Additionally, our innovative approach factors in the uncertainty associated with these estimated pseudo-labels and incorporates a feature mixup based training regularization for improved training. Empirical results show that UWAV outperforms state-of-the-art methods for the AVVP task on multiple metrics, across two different datasets, attesting to its effectiveness and generalizability.","sentences":["Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing both uni-modal events (i.e., those occurring exclusively in either the visual or acoustic modality of a video) and multi-modal events (i.e., those occurring in both modalities concurrently).","Moreover, the prohibitive cost of annotating training data with the class labels of all these events, along with their start and end times, imposes constraints on the scalability of AVVP techniques unless they can be trained in a weakly-supervised setting, where only modality-agnostic, video-level labels are available in the training data.","To this end, recently proposed approaches seek to generate segment-level pseudo-labels to better guide model training.","However, the absence of inter-segment dependencies when generating these pseudo-labels and the general bias towards predicting labels that are absent in a segment limit their performance.","This work proposes a novel approach towards overcoming these weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV).","Additionally, our innovative approach factors in the uncertainty associated with these estimated pseudo-labels and incorporates a feature mixup based training regularization for improved training.","Empirical results show that UWAV outperforms state-of-the-art methods for the AVVP task on multiple metrics, across two different datasets, attesting to its effectiveness and generalizability."],"url":"http://arxiv.org/abs/2505.09615v1"}
{"created":"2025-05-14 17:59:35","title":"Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?","abstract":"Language model (LM) agents are increasingly used as autonomous decision-makers who need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established \"Blicket Test\" paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This \"disjunctive bias\" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not children-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.","sentences":["Language model (LM) agents are increasingly used as autonomous decision-makers who need to actively gather information to guide their decisions.","A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning.","Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions.","In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established \"Blicket Test\" paradigm from developmental psychology.","We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones.","This \"disjunctive bias\" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases.","Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data.","To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not children-like).","Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM.","This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning."],"url":"http://arxiv.org/abs/2505.09614v1"}
{"created":"2025-05-14 17:57:27","title":"LightLab: Controlling Light Sources in Images with Diffusion Models","abstract":"We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.","sentences":["We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image.","Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes.","Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting.","We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination.","Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color.","Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference."],"url":"http://arxiv.org/abs/2505.09608v1"}
{"created":"2025-05-14 17:56:26","title":"The Niche Connectivity Paradox: Multichrome Contagions Overcome Vaccine Hesitancy more effectively than Monochromacy","abstract":"The rise of vaccine hesitancy has caused a resurgence of vaccine-preventable diseases such as measles and pertussis, alongside widespread skepticism and refusals of COVID-19 vaccinations. While categorizing individuals as either supportive of or opposed to vaccines provides a convenient dichotomy of vaccine attitudes, vaccine hesitancy is far more complex and dynamic. It involves wavering individuals whose attitudes fluctuate -- those who may exhibit pro-vaccine attitudes at one time and anti-vaccine attitudes at another. Here, we identify and analyze multichrome contagions as potential targets for intervention by leveraging a dataset of known pro-vax and anti-vax Twitter users ($n =135$ million) and a large COVID-19 Twitter dataset ($n = 3.5$ billion; including close analysis of $1,563,472$ unique individuals). We reconstruct an evolving multiplex sentiment landscape using top co-spreading issues, characterizing them as monochrome and multichrome contagions, based on their conceptual overlap with vaccination. We demonstrate switchers as deliberative: they are more moderate, engage with a wider range of topics, and occupy more central positions in their networks. Further examination of their information consumption shows that their discourse often engages with progressive issues such as climate change, which can serve as avenues for multichrome contagion interventions to promote pro-vaccine attitudes. Using data-driven intervention simulations, we demonstrate a paradox of niche connectivity, where multichrome contagions with fragmented, non-overlapping communities generate the highest levels of diffusion for pro-vaccine attitudes. Our work offers insights into harnessing synergistic hitchhiking effect of multichrome contagions to drive desired attitude and behavior changes in network-based interventions, particularly for overcoming vaccine hesitancy.","sentences":["The rise of vaccine hesitancy has caused a resurgence of vaccine-preventable diseases such as measles and pertussis, alongside widespread skepticism and refusals of COVID-19 vaccinations.","While categorizing individuals as either supportive of or opposed to vaccines provides a convenient dichotomy of vaccine attitudes, vaccine hesitancy is far more complex and dynamic.","It involves wavering individuals whose attitudes fluctuate -- those who may exhibit pro-vaccine attitudes at one time and anti-vaccine attitudes at another.","Here, we identify and analyze multichrome contagions as potential targets for intervention by leveraging a dataset of known pro-vax and anti-vax Twitter users ($n =135$ million) and a large COVID-19 Twitter dataset ($n = 3.5$ billion; including close analysis of $1,563,472$ unique individuals).","We reconstruct an evolving multiplex sentiment landscape using top co-spreading issues, characterizing them as monochrome and multichrome contagions, based on their conceptual overlap with vaccination.","We demonstrate switchers as deliberative: they are more moderate, engage with a wider range of topics, and occupy more central positions in their networks.","Further examination of their information consumption shows that their discourse often engages with progressive issues such as climate change, which can serve as avenues for multichrome contagion interventions to promote pro-vaccine attitudes.","Using data-driven intervention simulations, we demonstrate a paradox of niche connectivity, where multichrome contagions with fragmented, non-overlapping communities generate the highest levels of diffusion for pro-vaccine attitudes.","Our work offers insights into harnessing synergistic hitchhiking effect of multichrome contagions to drive desired attitude and behavior changes in network-based interventions, particularly for overcoming vaccine hesitancy."],"url":"http://arxiv.org/abs/2505.09605v1"}
{"created":"2025-05-14 17:55:10","title":"DataMIL: Selecting Data for Robot Imitation Learning with Datamodels","abstract":"Recently, the robotics community has amassed ever larger and more diverse datasets to train generalist robot policies. However, while these policies achieve strong mean performance across a variety of tasks, they often underperform on individual, specialized tasks and require further tuning on newly acquired task-specific data. Combining task-specific data with carefully curated subsets of large prior datasets via co-training can produce better specialized policies, but selecting data naively may actually harm downstream performance. To address this, we introduce DataMIL, a policy-driven data selection framework built on the datamodels paradigm that reasons about data selection in an end-to-end manner, using the policy itself to identify which data points will most improve performance. Unlike standard practices that filter data using human notions of quality (e.g., based on semantic or visual similarity), DataMIL directly optimizes data selection for task success, allowing us to select data that enhance the policy while dropping data that degrade it. To avoid performing expensive rollouts in the environment during selection, we use a novel surrogate loss function on task-specific data, allowing us to use DataMIL in the real world without degrading performance. We validate our approach on a suite of more than 60 simulation and real-world manipulation tasks - most notably showing successful data selection from the Open X-Embodiment datasets-demonstrating consistent gains in success rates and superior performance over multiple baselines. Our results underscore the importance of end-to-end, performance-aware data selection for unlocking the potential of large prior datasets in robotics. More information at https://robin-lab.cs.utexas.edu/datamodels4imitation/","sentences":["Recently, the robotics community has amassed ever larger and more diverse datasets to train generalist robot policies.","However, while these policies achieve strong mean performance across a variety of tasks, they often underperform on individual, specialized tasks and require further tuning on newly acquired task-specific data.","Combining task-specific data with carefully curated subsets of large prior datasets via co-training can produce better specialized policies, but selecting data naively may actually harm downstream performance.","To address this, we introduce DataMIL, a policy-driven data selection framework built on the datamodels paradigm that reasons about data selection in an end-to-end manner, using the policy itself to identify which data points will most improve performance.","Unlike standard practices that filter data using human notions of quality (e.g., based on semantic or visual similarity), DataMIL directly optimizes data selection for task success, allowing us to select data that enhance the policy while dropping data that degrade it.","To avoid performing expensive rollouts in the environment during selection, we use a novel surrogate loss function on task-specific data, allowing us to use DataMIL in the real world without degrading performance.","We validate our approach on a suite of more than 60 simulation and real-world manipulation tasks - most notably showing successful data selection from the Open X-Embodiment datasets-demonstrating consistent gains in success rates and superior performance over multiple baselines.","Our results underscore the importance of end-to-end, performance-aware data selection for unlocking the potential of large prior datasets in robotics.","More information at https://robin-lab.cs.utexas.edu/datamodels4imitation/"],"url":"http://arxiv.org/abs/2505.09603v1"}
{"created":"2025-05-14 17:50:35","title":"Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware","abstract":"Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com","sentences":["Scaling robot learning requires vast and diverse datasets.","Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access.","We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware.","The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration.","R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion.","R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off.","Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies.","Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations.","Project page: https://real2render2real.com"],"url":"http://arxiv.org/abs/2505.09601v1"}
{"created":"2025-05-14 17:47:00","title":"How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference","abstract":"As large language models (LLMs) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential. However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates AI's environmental impact. To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards.","sentences":["As large language models (LLMs) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential.","However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates AI's environmental impact.","To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers.","Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations.","We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost.","Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency.","While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts.","These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset.","These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption.","Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards."],"url":"http://arxiv.org/abs/2505.09598v1"}
{"created":"2025-05-14 17:46:16","title":"MDTP -- An Adaptive Multi-Source Data Transfer Protocol","abstract":"Scientific data volume is growing in size, and as a direct result, the need for faster transfers is also increasing. The scientific community has sought to leverage parallel transfer methods using multi-threaded and multi-source download models to reduce download times. In multi-source transfers, a client downloads data from multiple replicated servers in parallel. Tools such as Aria2 and BitTorrent support such multi-source transfers and have shown improved transfer times.   In this work, we introduce Multi-Source Data Transfer Protocol, MDTP, which further improves multi-source transfer performance. MDTP logically divides a file request into smaller chunk requests and distributes the chunk requests across multiple servers. Chunk sizes are adapted based on each server's performance but selected in a way that ensures each round of requests completes around the same time. We formulate this chunk-size allocation problem as a variant of the bin-packing problem, where adaptive chunking efficiently fills the available capacity \"bins\" corresponding to each server.   Our evaluation shows that MDTP reduces transfer times by 10-22% compared to Aria2, the fastest alternative. Comparisons with other protocols, such as static chunking and BitTorrent, demonstrate even greater improvements. Additionally, we show that MDTP distributes load proportionally across all available replicas, not just the fastest ones, which improves throughput. Finally, we show MDTP maintains high throughput even when latency increases or bandwidth to the fastest server decreases.","sentences":["Scientific data volume is growing in size, and as a direct result, the need for faster transfers is also increasing.","The scientific community has sought to leverage parallel transfer methods using multi-threaded and multi-source download models to reduce download times.","In multi-source transfers, a client downloads data from multiple replicated servers in parallel.","Tools such as Aria2 and BitTorrent support such multi-source transfers and have shown improved transfer times.   ","In this work, we introduce Multi-Source Data Transfer Protocol, MDTP, which further improves multi-source transfer performance.","MDTP logically divides a file request into smaller chunk requests and distributes the chunk requests across multiple servers.","Chunk sizes are adapted based on each server's performance but selected in a way that ensures each round of requests completes around the same time.","We formulate this chunk-size allocation problem as a variant of the bin-packing problem, where adaptive chunking efficiently fills the available capacity \"bins\" corresponding to each server.   ","Our evaluation shows that MDTP reduces transfer times by 10-22% compared to Aria2, the fastest alternative.","Comparisons with other protocols, such as static chunking and BitTorrent, demonstrate even greater improvements.","Additionally, we show that MDTP distributes load proportionally across all available replicas, not just the fastest ones, which improves throughput.","Finally, we show MDTP maintains high throughput even when latency increases or bandwidth to the fastest server decreases."],"url":"http://arxiv.org/abs/2505.09597v1"}
{"created":"2025-05-14 17:42:50","title":"Online Isolation Forest","abstract":"The anomaly detection literature is abundant with offline methods, which require repeated access to data in memory, and impose impractical assumptions when applied to a streaming context. Existing online anomaly detection methods also generally fail to address these constraints, resorting to periodic retraining to adapt to the online context. We propose Online-iForest, a novel method explicitly designed for streaming conditions that seamlessly tracks the data generating process as it evolves over time. Experimental validation on real-world datasets demonstrated that Online-iForest is on par with online alternatives and closely rivals state-of-the-art offline anomaly detection techniques that undergo periodic retraining. Notably, Online-iForest consistently outperforms all competitors in terms of efficiency, making it a promising solution in applications where fast identification of anomalies is of primary importance such as cybersecurity, fraud and fault detection.","sentences":["The anomaly detection literature is abundant with offline methods, which require repeated access to data in memory, and impose impractical assumptions when applied to a streaming context.","Existing online anomaly detection methods also generally fail to address these constraints, resorting to periodic retraining to adapt to the online context.","We propose Online-iForest, a novel method explicitly designed for streaming conditions that seamlessly tracks the data generating process as it evolves over time.","Experimental validation on real-world datasets demonstrated that Online-iForest is on par with online alternatives and closely rivals state-of-the-art offline anomaly detection techniques that undergo periodic retraining.","Notably, Online-iForest consistently outperforms all competitors in terms of efficiency, making it a promising solution in applications where fast identification of anomalies is of primary importance such as cybersecurity, fraud and fault detection."],"url":"http://arxiv.org/abs/2505.09593v1"}
{"created":"2025-05-14 17:37:15","title":"Rhomboid Tiling for Geometric Graph Deep Learning","abstract":"Graph Neural Networks (GNNs) have proven effective for learning from graph-structured data through their neighborhood-based message passing framework. Many hierarchical graph clustering pooling methods modify this framework by introducing clustering-based strategies, enabling the construction of more expressive and powerful models. However, all of these message passing framework heavily rely on the connectivity structure of graphs, limiting their ability to capture the rich geometric features inherent in geometric graphs. To address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering method based on the rhomboid tiling structure, which performs clustering by leveraging the complex geometric information of the data and effectively extracts its higher-order geometric structures. Moreover, we design RTPool, a hierarchical graph clustering pooling model based on RT clustering for graph classification tasks. The proposed model demonstrates superior performance, outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.","sentences":["Graph Neural Networks (GNNs) have proven effective for learning from graph-structured data through their neighborhood-based message passing framework.","Many hierarchical graph clustering pooling methods modify this framework by introducing clustering-based strategies, enabling the construction of more expressive and powerful models.","However, all of these message passing framework heavily rely on the connectivity structure of graphs, limiting their ability to capture the rich geometric features inherent in geometric graphs.","To address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering method based on the rhomboid tiling structure, which performs clustering by leveraging the complex geometric information of the data and effectively extracts its higher-order geometric structures.","Moreover, we design RTPool, a hierarchical graph clustering pooling model based on RT clustering for graph classification tasks.","The proposed model demonstrates superior performance, outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets."],"url":"http://arxiv.org/abs/2505.09586v1"}
{"created":"2025-05-14 17:15:11","title":"SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures","abstract":"We study gradient flows for loss landscapes of fully connected feed forward neural networks with commonly used continuously differentiable activation functions such as the logistic, hyperbolic tangent, softplus or GELU function. We prove that the gradient flow either converges to a critical point or diverges to infinity while the loss converges to an asymptotic critical value. Moreover, we prove the existence of a threshold $\\varepsilon>0$ such that the loss value of any gradient flow initialized at most $\\varepsilon$ above the optimal level converges to it. For polynomial target functions and sufficiently big architecture and data set, we prove that the optimal loss value is zero and can only be realized asymptotically. From this setting, we deduce our main result that any gradient flow with sufficiently good initialization diverges to infinity. Our proof heavily relies on the geometry of o-minimal structures. We confirm these theoretical findings with numerical experiments and extend our investigation to real-world scenarios, where we observe an analogous behavior.","sentences":["We study gradient flows for loss landscapes of fully connected feed forward neural networks with commonly used continuously differentiable activation functions such as the logistic, hyperbolic tangent, softplus or GELU function.","We prove that the gradient flow either converges to a critical point or diverges to infinity while the loss converges to an asymptotic critical value.","Moreover, we prove the existence of a threshold $\\varepsilon>0$ such that the loss value of any gradient flow initialized at most $\\varepsilon$ above the optimal level converges to it.","For polynomial target functions and sufficiently big architecture and data set, we prove that the optimal loss value is zero and can only be realized asymptotically.","From this setting, we deduce our main result that any gradient flow with sufficiently good initialization diverges to infinity.","Our proof heavily relies on the geometry of o-minimal structures.","We confirm these theoretical findings with numerical experiments and extend our investigation to real-world scenarios, where we observe an analogous behavior."],"url":"http://arxiv.org/abs/2505.09572v1"}
{"created":"2025-05-14 17:07:30","title":"Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation","abstract":"Cardiac image segmentation is an important step in many cardiac image analysis and modeling tasks such as motion tracking or simulations of cardiac mechanics. While deep learning has greatly advanced segmentation in clinical settings, there is limited work on pre-clinical imaging, notably in porcine models, which are often used due to their anatomical and physiological similarity to humans. However, differences between species create a domain shift that complicates direct model transfer from human to pig data.   Recently, foundation models trained on large human datasets have shown promise for robust medical image segmentation; yet their applicability to porcine data remains largely unexplored. In this work, we investigate whether foundation models can generate sufficiently accurate pseudo-labels for pig cardiac CT and propose a simple self-training approach to iteratively refine these labels. Our method requires no manually annotated pig data, relying instead on iterative updates to improve segmentation quality. We demonstrate that this self-training process not only enhances segmentation accuracy but also smooths out temporal inconsistencies across consecutive frames. Although our results are encouraging, there remains room for improvement, for example by incorporating more sophisticated self-training strategies and by exploring additional foundation models and other cardiac imaging technologies.","sentences":["Cardiac image segmentation is an important step in many cardiac image analysis and modeling tasks such as motion tracking or simulations of cardiac mechanics.","While deep learning has greatly advanced segmentation in clinical settings",", there is limited work on pre-clinical imaging, notably in porcine models, which are often used due to their anatomical and physiological similarity to humans.","However, differences between species create a domain shift that complicates direct model transfer from human to pig data.   ","Recently, foundation models trained on large human datasets have shown promise for robust medical image segmentation; yet their applicability to porcine data remains largely unexplored.","In this work, we investigate whether foundation models can generate sufficiently accurate pseudo-labels for pig cardiac CT and propose a simple self-training approach to iteratively refine these labels.","Our method requires no manually annotated pig data, relying instead on iterative updates to improve segmentation quality.","We demonstrate that this self-training process not only enhances segmentation accuracy but also smooths out temporal inconsistencies across consecutive frames.","Although our results are encouraging, there remains room for improvement, for example by incorporating more sophisticated self-training strategies and by exploring additional foundation models and other cardiac imaging technologies."],"url":"http://arxiv.org/abs/2505.09564v1"}
{"created":"2025-05-14 16:21:45","title":"An Asymptotically Optimal Approximation Algorithm for Multiobjective Submodular Maximization at Scale","abstract":"Maximizing a single submodular set function subject to a cardinality constraint is a well-studied and central topic in combinatorial optimization. However, finding a set that maximizes multiple functions at the same time is much less understood, even though it is a formulation which naturally occurs in robust maximization or problems with fairness considerations such as fair influence maximization or fair allocation.   In this work, we consider the problem of maximizing the minimum over many submodular functions, which is known as multiobjective submodular maximization. All known polynomial-time approximation algorithms either obtain a weak approximation guarantee or rely on the evaluation of the multilinear extension. The latter is expensive to evaluate and renders such algorithms impractical. We bridge this gap and introduce the first scalable and practical algorithm that obtains the best-known approximation guarantee. We furthermore introduce a novel application fair centrality maximization and show how it can be addressed via multiobjective submodular maximization. In our experimental evaluation, we show that our algorithm outperforms known algorithms in terms of objective value and running time.","sentences":["Maximizing a single submodular set function subject to a cardinality constraint is a well-studied and central topic in combinatorial optimization.","However, finding a set that maximizes multiple functions at the same time is much less understood, even though it is a formulation which naturally occurs in robust maximization or problems with fairness considerations such as fair influence maximization or fair allocation.   ","In this work, we consider the problem of maximizing the minimum over many submodular functions, which is known as multiobjective submodular maximization.","All known polynomial-time approximation algorithms either obtain a weak approximation guarantee or rely on the evaluation of the multilinear extension.","The latter is expensive to evaluate and renders such algorithms impractical.","We bridge this gap and introduce the first scalable and practical algorithm that obtains the best-known approximation guarantee.","We furthermore introduce a novel application fair centrality maximization and show how it can be addressed via multiobjective submodular maximization.","In our experimental evaluation, we show that our algorithm outperforms known algorithms in terms of objective value and running time."],"url":"http://arxiv.org/abs/2505.09525v1"}
{"created":"2025-05-14 16:02:34","title":"Partnership through Play: Investigating How Long-Distance Couples Use Digital Games to Facilitate Intimacy","abstract":"Long-distance relationships (LDRs) have become more common in the last few decades, primarily among young adults pursuing educational or employment opportunities. A common way for couples in LDRs to spend time together is by playing multiplayer video games, which are often a shared hobby and therefore a preferred joint activity. However, games are relatively understudied in the context of relational maintenance for LDRs. In this work, we used a mixed-methods approach to collect data on the experiences of 13 couples in LDRs who frequently play games together. We investigated different values around various game mechanics and modalities and found significant differences in couple play styles, and also detail how couples appropriate game mechanics to express affection to each other virtually. We also created prototypes and design implications based on couples' needs surrounding the lack of physical sensation and memorabilia storage in most popular games.","sentences":["Long-distance relationships (LDRs) have become more common in the last few decades, primarily among young adults pursuing educational or employment opportunities.","A common way for couples in LDRs to spend time together is by playing multiplayer video games, which are often a shared hobby and therefore a preferred joint activity.","However, games are relatively understudied in the context of relational maintenance for LDRs.","In this work, we used a mixed-methods approach to collect data on the experiences of 13 couples in LDRs who frequently play games together.","We investigated different values around various game mechanics and modalities and found significant differences in couple play styles, and also detail how couples appropriate game mechanics to express affection to each other virtually.","We also created prototypes and design implications based on couples' needs surrounding the lack of physical sensation and memorabilia storage in most popular games."],"url":"http://arxiv.org/abs/2505.09509v1"}
{"created":"2025-05-14 15:53:14","title":"Towards Fair In-Context Learning with Tabular Foundation Models","abstract":"Tabular foundational models have exhibited strong in-context learning (ICL) capabilities on structured data, allowing them to make accurate predictions on test sets without parameter updates, using training examples as context. This emerging approach positions itself as a competitive alternative to traditional gradient-boosted tree methods. However, while biases in conventional machine learning models are well documented, it remains unclear how these biases manifest in tabular ICL. The paper investigates the fairness implications of tabular ICL and explores three preprocessing strategies--correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection--to address bias. Comprehensive experiments indicate that uncertainty-based demonstration selection consistently enhances group fairness of in-context predictions. The source code for reproducing the results of this work can be found at https://github.com/patrikken/Fair-TabICL.","sentences":["Tabular foundational models have exhibited strong in-context learning (ICL) capabilities on structured data, allowing them to make accurate predictions on test sets without parameter updates, using training examples as context.","This emerging approach positions itself as a competitive alternative to traditional gradient-boosted tree methods.","However, while biases in conventional machine learning models are well documented, it remains unclear how these biases manifest in tabular ICL.","The paper investigates the fairness implications of tabular ICL and explores three preprocessing strategies--correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection--to address bias.","Comprehensive experiments indicate that uncertainty-based demonstration selection consistently enhances group fairness of in-context predictions.","The source code for reproducing the results of this work can be found at https://github.com/patrikken/Fair-TabICL."],"url":"http://arxiv.org/abs/2505.09503v1"}
{"created":"2025-05-14 15:50:45","title":"Layered Unlearning for Adversarial Relearning","abstract":"Our goal is to understand how post-training methods, such as fine-tuning, alignment, and unlearning, modify language model behavior and representations. We are particularly interested in the brittle nature of these modifications that makes them easy to bypass through prompt engineering or relearning. Recent results suggest that post-training induces shallow context-dependent ``circuits'' that suppress specific response patterns. This could be one explanation for the brittleness of post-training. To test this hypothesis, we design an unlearning algorithm, Layered Unlearning (LU), that creates distinct inhibitory mechanisms for a growing subset of the data. By unlearning the first $i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU limits the ability of relearning on a subset of data to recover the full dataset. We evaluate LU through a combination of synthetic and large language model (LLM) experiments. We find that LU improves robustness to adversarial relearning for several different unlearning methods. Our results contribute to the state-of-the-art of machine unlearning and provide insight into the effect of post-training updates.","sentences":["Our goal is to understand how post-training methods, such as fine-tuning, alignment, and unlearning, modify language model behavior and representations.","We are particularly interested in the brittle nature of these modifications that makes them easy to bypass through prompt engineering or relearning.","Recent results suggest that post-training induces shallow context-dependent ``circuits'' that suppress specific response patterns.","This could be one explanation for the brittleness of post-training.","To test this hypothesis, we design an unlearning algorithm, Layered Unlearning (LU), that creates distinct inhibitory mechanisms for a growing subset of the data.","By unlearning the first $i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU limits the ability of relearning on a subset of data to recover the full dataset.","We evaluate LU through a combination of synthetic and large language model (LLM) experiments.","We find that LU improves robustness to adversarial relearning for several different unlearning methods.","Our results contribute to the state-of-the-art of machine unlearning and provide insight into the effect of post-training updates."],"url":"http://arxiv.org/abs/2505.09500v1"}
{"created":"2025-05-14 15:45:17","title":"Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput","abstract":"In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision-Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy. Leveraging advanced architectural enhancements and efficient computational strategies, Flash-VL 2B is designed to maximize throughput by reducing processing time while maintaining competitive performance across multiple vision-language benchmarks. Our approach includes tailored architectural choices, token compression mechanisms, data curation, training schemes, and a novel image processing technique called implicit semantic stitching that effectively balances computational load and model performance. Through extensive evaluations on 11 standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves state-of-the-art results in both speed and accuracy, making it a promising solution for deployment in resource-constrained environments and large-scale real-time applications.","sentences":["In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision-Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy.","Leveraging advanced architectural enhancements and efficient computational strategies, Flash-VL 2B is designed to maximize throughput by reducing processing time while maintaining competitive performance across multiple vision-language benchmarks.","Our approach includes tailored architectural choices, token compression mechanisms, data curation, training schemes, and a novel image processing technique called implicit semantic stitching that effectively balances computational load and model performance.","Through extensive evaluations on 11 standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves state-of-the-art results in both speed and accuracy, making it a promising solution for deployment in resource-constrained environments and large-scale real-time applications."],"url":"http://arxiv.org/abs/2505.09498v1"}
{"created":"2025-05-14 15:40:40","title":"Radon Exposure Dataset","abstract":"Exposure to elevated radon levels in the home is one of the leading causes of lung cancer in the world. The following study describes the creation of a comprehensive, state-level dataset designed to enable the modeling and prediction of household radon concentrations at Zip Code Tabulation Area (ZCTA) and sub-kilometer scales. Details include the data collection and processing involved in compiling physical and demographic factors for Pennsylvania and Utah. Attempting to mitigate this risk requires identifying the underlying geological causes and the populations that might be at risk. This work focuses on identifying at-risk populations throughout Pennsylvania and Utah, where radon levels are some of the highest in the country. The resulting dataset harmonizes geological and demographic factors from various sources and spatial resolutions, including temperature, geochemistry, and soil characteristics. Demographic variables such as the household heating fuel used, the age of building, and the housing type provide further insight into which populations could be most susceptible in areas with potentially high radon levels. This dataset also serves as a foundational resource for two other studies conducted by the authors. The resolution of the data provides a novel approach to predicting potential radon exposure, and the data processing conducted for these states can be scaled up to larger spatial resolutions (e.g., the Contiguous United States [CONUS]) and allow for a broad reclassification of radon exposure potential in the United States.","sentences":["Exposure to elevated radon levels in the home is one of the leading causes of lung cancer in the world.","The following study describes the creation of a comprehensive, state-level dataset designed to enable the modeling and prediction of household radon concentrations at Zip Code Tabulation Area (ZCTA) and sub-kilometer scales.","Details include the data collection and processing involved in compiling physical and demographic factors for Pennsylvania and Utah.","Attempting to mitigate this risk requires identifying the underlying geological causes and the populations that might be at risk.","This work focuses on identifying at-risk populations throughout Pennsylvania and Utah, where radon levels are some of the highest in the country.","The resulting dataset harmonizes geological and demographic factors from various sources and spatial resolutions, including temperature, geochemistry, and soil characteristics.","Demographic variables such as the household heating fuel used, the age of building, and the housing type provide further insight into which populations could be most susceptible in areas with potentially high radon levels.","This dataset also serves as a foundational resource for two other studies conducted by the authors.","The resolution of the data provides a novel approach to predicting potential radon exposure, and the data processing conducted for these states can be scaled up to larger spatial resolutions (e.g., the Contiguous United States [CONUS]) and allow for a broad reclassification of radon exposure potential in the United States."],"url":"http://arxiv.org/abs/2505.09489v1"}
{"created":"2025-05-14 15:36:44","title":"Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing","abstract":"Face Anti-Spoofing (FAS) is essential for the security of facial recognition systems in diverse scenarios such as payment processing and surveillance. Current multimodal FAS methods often struggle with effective generalization, mainly due to modality-specific biases and domain shifts. To address these challenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising and \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot generalization capability of CLIP, the MMDA framework effectively suppresses noise in multimodal data through denoising and alignment mechanisms, thereby significantly enhancing the generalization performance of cross-modal alignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential \\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the impacts of domain and modality noise by refining the attention mechanism based on extracted common noise features. Furthermore, the \\textbf{R}epresentation \\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the pre-trained CLIP model to align multi-domain multimodal data into a generalized representation space in a flexible manner, preserving intricate representations and enhancing the model's adaptability to various unseen conditions. We also design a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation (\\textbf{U-DSA}) module to enhance the adaptability of representations while maintaining generalization performance. These improvements not only enhance the framework's generalization capabilities but also boost its ability to represent complex representations. Our experimental results on four benchmark datasets under different evaluation protocols demonstrate that the MMDA framework outperforms existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy. The code will be released soon.","sentences":["Face Anti-Spoofing (FAS) is essential for the security of facial recognition systems in diverse scenarios such as payment processing and surveillance.","Current multimodal FAS methods often struggle with effective generalization, mainly due to modality-specific biases and domain shifts.","To address these challenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising and \\textbf{A}lignment (\\textbf{MMDA}) framework.","By leveraging the zero-shot generalization capability of CLIP, the MMDA framework effectively suppresses noise in multimodal data through denoising and alignment mechanisms, thereby significantly enhancing the generalization performance of cross-modal alignment.","The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential \\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the impacts of domain and modality noise by refining the attention mechanism based on extracted common noise features.","Furthermore, the \\textbf{R}epresentation \\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the pre-trained CLIP model to align multi-domain multimodal data into a generalized representation space in a flexible manner, preserving intricate representations and enhancing the model's adaptability to various unseen conditions.","We also design a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation (\\textbf{U-DSA}) module to enhance the adaptability of representations while maintaining generalization performance.","These improvements not only enhance the framework's generalization capabilities but also boost its ability to represent complex representations.","Our experimental results on four benchmark datasets under different evaluation protocols demonstrate that the MMDA framework outperforms existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy.","The code will be released soon."],"url":"http://arxiv.org/abs/2505.09484v1"}
{"created":"2025-05-14 15:13:59","title":"ARM SVE Unleashed: Performance and Insights Across HPC Applications on Nvidia Grace","abstract":"Vector architectures are essential for boosting computing throughput. ARM provides SVE as the next-generation length-agnostic vector extension beyond traditional fixed-length SIMD. This work provides a first study of the maturity and readiness of exploiting ARM and SVE in HPC. Using selected performance hardware events on the ARM Grace processor and analytical models, we derive new metrics to quantify the effectiveness of exploiting SVE vectorization to reduce executed instructions and improve performance speedup. We further propose an adapted roofline model that combines vector length and data elements to identify potential performance bottlenecks. Finally, we propose a decision tree for classifying the SVE-boosted performance in applications.","sentences":["Vector architectures are essential for boosting computing throughput.","ARM provides SVE as the next-generation length-agnostic vector extension beyond traditional fixed-length SIMD.","This work provides a first study of the maturity and readiness of exploiting ARM and SVE in HPC.","Using selected performance hardware events on the ARM Grace processor and analytical models, we derive new metrics to quantify the effectiveness of exploiting SVE vectorization to reduce executed instructions and improve performance speedup.","We further propose an adapted roofline model that combines vector length and data elements to identify potential performance bottlenecks.","Finally, we propose a decision tree for classifying the SVE-boosted performance in applications."],"url":"http://arxiv.org/abs/2505.09462v1"}
{"created":"2025-05-14 15:03:16","title":"SEGA-DCIM: Design Space Exploration-Guided Automatic Digital CIM Compiler with Multiple Precision Support","abstract":"Digital computing-in-memory (DCIM) has been a popular solution for addressing the memory wall problem in recent years. However, the DCIM design still heavily relies on manual efforts, and the optimization of DCIM is often based on human experience. These disadvantages limit the time to market while increasing the design difficulty of DCIMs. This work proposes a design space exploration-guided automatic DCIM compiler (SEGA-DCIM) with multiple precision support, including integer and floating-point data precision operations. SEGA-DCIM can automatically generate netlists and layouts of DCIM designs by leveraging a template-based method. With a multi-objective genetic algorithm (MOGA)-based design space explorer, SEGA-DCIM can easily select appropriate DCIM designs for a specific application considering the trade-offs among area, power, and delay. As demonstrated by the experimental results, SEGA-DCIM offers solutions with wide design space, including integer and floating-point precision designs, while maintaining competitive performance compared to state-of-the-art (SOTA) DCIMs.","sentences":["Digital computing-in-memory (DCIM) has been a popular solution for addressing the memory wall problem in recent years.","However, the DCIM design still heavily relies on manual efforts, and the optimization of DCIM is often based on human experience.","These disadvantages limit the time to market while increasing the design difficulty of DCIMs.","This work proposes a design space exploration-guided automatic DCIM compiler (SEGA-DCIM) with multiple precision support, including integer and floating-point data precision operations.","SEGA-DCIM can automatically generate netlists and layouts of DCIM designs by leveraging a template-based method.","With a multi-objective genetic algorithm (MOGA)-based design space explorer, SEGA-DCIM can easily select appropriate DCIM designs for a specific application considering the trade-offs among area, power, and delay.","As demonstrated by the experimental results, SEGA-DCIM offers solutions with wide design space, including integer and floating-point precision designs, while maintaining competitive performance compared to state-of-the-art (SOTA) DCIMs."],"url":"http://arxiv.org/abs/2505.09451v1"}
{"created":"2025-05-14 14:44:30","title":"CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios","abstract":"Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.","sentences":["Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations.","However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks.","Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency.","To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts.","Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations.","The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation.","Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools.","Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques."],"url":"http://arxiv.org/abs/2505.09436v1"}
{"created":"2025-05-14 14:42:07","title":"Decentralized Nonlinear Model Predictive Control-Based Flock Navigation with Real-Time Obstacle Avoidance in Unknown Obstructed Environments","abstract":"This work extends our prior work on the distributed nonlinear model predictive control (NMPC) for navigating a robot fleet following a certain flocking behavior in unknown obstructed environments with a more realistic local obstacle avoidance strategy. More specifically, we integrate the local obstacle avoidance constraint using point clouds into the NMPC framework. Here, each agent relies on data from its local sensor to perceive and respond to nearby obstacles. A point cloud processing technique is presented for both two-dimensional and three-dimensional point clouds to minimize the computational burden during the optimization. The process consists of directional filtering and down-sampling that significantly reduce the number of data points. The algorithm's performance is validated through realistic 3D simulations in Gazebo, and its practical feasibility is further explored via hardware-in-the-loop (HIL) simulations on embedded platforms.","sentences":["This work extends our prior work on the distributed nonlinear model predictive control (NMPC) for navigating a robot fleet following a certain flocking behavior in unknown obstructed environments with a more realistic local obstacle avoidance strategy.","More specifically, we integrate the local obstacle avoidance constraint using point clouds into the NMPC framework.","Here, each agent relies on data from its local sensor to perceive and respond to nearby obstacles.","A point cloud processing technique is presented for both two-dimensional and three-dimensional point clouds to minimize the computational burden during the optimization.","The process consists of directional filtering and down-sampling that significantly reduce the number of data points.","The algorithm's performance is validated through realistic 3D simulations in Gazebo, and its practical feasibility is further explored via hardware-in-the-loop (HIL) simulations on embedded platforms."],"url":"http://arxiv.org/abs/2505.09434v1"}
{"created":"2025-05-14 14:38:40","title":"Efficient LiDAR Reflectance Compression via Scanning Serialization","abstract":"Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods. To address this, we introduce SerLiC, a serialization-based neural compression framework to fully exploit the intrinsic characteristics of LiDAR reflectance. SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization, offering a device-centric perspective for reflectance analysis. Each point is then tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance, for effective dependencies exploration. For efficient sequential modeling, Mamba is incorporated with a dual parallelization scheme, enabling simultaneous autoregressive dependency capture and fast processing. Extensive experiments demonstrate that SerLiC attains over 2x volume reduction against the original reflectance data, outperforming the state-of-the-art method by up to 22% reduction of compressed bits while using only 2% of its parameters. Moreover, a lightweight version of SerLiC achieves > 10 fps (frames per second) with just 111K parameters, which is attractive for real-world applications.","sentences":["Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods.","To address this, we introduce SerLiC, a serialization-based neural compression framework to fully exploit the intrinsic characteristics of LiDAR reflectance.","SerLiC","first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization, offering a device-centric perspective for reflectance analysis.","Each point is then tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance, for effective dependencies exploration.","For efficient sequential modeling, Mamba is incorporated with a dual parallelization scheme, enabling simultaneous autoregressive dependency capture and fast processing.","Extensive experiments demonstrate that SerLiC attains over 2x volume reduction against the original reflectance data, outperforming the state-of-the-art method by up to 22% reduction of compressed bits while using only 2% of its parameters.","Moreover, a lightweight version of SerLiC achieves > 10 fps (frames per second) with just 111K parameters, which is attractive for real-world applications."],"url":"http://arxiv.org/abs/2505.09433v1"}
{"created":"2025-05-14 14:25:32","title":"Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion","abstract":"Recent studies have proved that imitation learning shows strong potential in the field of robotic manipulation. However, existing methods still struggle with precision manipulation task and rely on inefficient image/point cloud observations. In this paper, we explore to introduce SE(3) object pose into imitation learning and propose the pose-guided efficient imitation learning methods for robotic precise insertion task. First, we propose a precise insertion diffusion policy which utilizes the relative SE(3) pose as the observation-action pair. The policy models the source object SE(3) pose trajectory relative to the target object. Second, we explore to introduce the RGBD data to the pose-guided diffusion policy. Specifically, we design a goal-conditioned RGBD encoder to capture the discrepancy between the current state and the goal state. In addition, a pose-guided residual gated fusion method is proposed, which takes pose features as the backbone, and the RGBD features selectively compensate for pose feature deficiencies through an adaptive gating mechanism. Our methods are evaluated on 6 robotic precise insertion tasks, demonstrating competitive performance with only 7-10 demonstrations. Experiments demonstrate that the proposed methods can successfully complete precision insertion tasks with a clearance of about 0.01 mm. Experimental results highlight its superior efficiency and generalization capability compared to existing baselines. Code will be available at https://github.com/sunhan1997/PoseInsert.","sentences":["Recent studies have proved that imitation learning shows strong potential in the field of robotic manipulation.","However, existing methods still struggle with precision manipulation task and rely on inefficient image/point cloud observations.","In this paper, we explore to introduce SE(3) object pose into imitation learning and propose the pose-guided efficient imitation learning methods for robotic precise insertion task.","First, we propose a precise insertion diffusion policy which utilizes the relative SE(3) pose as the observation-action pair.","The policy models the source object SE(3) pose trajectory relative to the target object.","Second, we explore to introduce the RGBD data to the pose-guided diffusion policy.","Specifically, we design a goal-conditioned RGBD encoder to capture the discrepancy between the current state and the goal state.","In addition, a pose-guided residual gated fusion method is proposed, which takes pose features as the backbone, and the RGBD features selectively compensate for pose feature deficiencies through an adaptive gating mechanism.","Our methods are evaluated on 6 robotic precise insertion tasks, demonstrating competitive performance with only 7-10 demonstrations.","Experiments demonstrate that the proposed methods can successfully complete precision insertion tasks with a clearance of about 0.01 mm.","Experimental results highlight its superior efficiency and generalization capability compared to existing baselines.","Code will be available at https://github.com/sunhan1997/PoseInsert."],"url":"http://arxiv.org/abs/2505.09424v1"}
{"created":"2025-05-14 14:10:22","title":"FACTors: A New Dataset for Studying the Fact-checking Ecosystem","abstract":"Our fight against false information is spearheaded by fact-checkers. They investigate the veracity of claims and document their findings as fact-checking reports. With the rapid increase in the amount of false information circulating online, the use of automation in fact-checking processes aims to strengthen this ecosystem by enhancing scalability. Datasets containing fact-checked claims play a key role in developing such automated solutions. However, to the best of our knowledge, there is no fact-checking dataset at the ecosystem level, covering claims from a sufficiently long period of time and sourced from a wide range of actors reflecting the entire ecosystem that admittedly follows widely-accepted codes and principles of fact-checking. We present a new dataset FACTors, the first to fill this gap by presenting ecosystem-level data on fact-checking. It contains 118,112 claims from 117,993 fact-checking reports in English (co-)authored by 1,953 individuals and published during the period of 1995-2025 by 39 fact-checking organisations that are active signatories of the IFCN (International Fact-Checking Network) and/or EFCSN (European Fact-Checking Standards Network). It contains 7,327 overlapping claims investigated by multiple fact-checking organisations, corresponding to 2,977 unique claims. It allows to conduct new ecosystem-level studies of the fact-checkers (organisations and individuals). To demonstrate the usefulness of FACTors, we present three example applications, including a first-of-its-kind statistical analysis of the fact-checking ecosystem, examining the political inclinations of the fact-checking organisations, and attempting to assign a credibility score to each organisation based on the findings of the statistical analysis and political leanings. Our methods for constructing FACTors are generic and can be used to maintain a live dataset that can be updated dynamically.","sentences":["Our fight against false information is spearheaded by fact-checkers.","They investigate the veracity of claims and document their findings as fact-checking reports.","With the rapid increase in the amount of false information circulating online, the use of automation in fact-checking processes aims to strengthen this ecosystem by enhancing scalability.","Datasets containing fact-checked claims play a key role in developing such automated solutions.","However, to the best of our knowledge, there is no fact-checking dataset at the ecosystem level, covering claims from a sufficiently long period of time and sourced from a wide range of actors reflecting the entire ecosystem that admittedly follows widely-accepted codes and principles of fact-checking.","We present a new dataset FACTors, the first to fill this gap by presenting ecosystem-level data on fact-checking.","It contains 118,112 claims from 117,993 fact-checking reports in English (co-)authored by 1,953 individuals and published during the period of 1995-2025 by 39 fact-checking organisations that are active signatories of the IFCN (International Fact-Checking Network) and/or","EFCSN (European Fact-Checking Standards Network).","It contains 7,327 overlapping claims investigated by multiple fact-checking organisations, corresponding to 2,977 unique claims.","It allows to conduct new ecosystem-level studies of the fact-checkers (organisations and individuals).","To demonstrate the usefulness of FACTors, we present three example applications, including a first-of-its-kind statistical analysis of the fact-checking ecosystem, examining the political inclinations of the fact-checking organisations, and attempting to assign a credibility score to each organisation based on the findings of the statistical analysis and political leanings.","Our methods for constructing FACTors are generic and can be used to maintain a live dataset that can be updated dynamically."],"url":"http://arxiv.org/abs/2505.09414v1"}
{"created":"2025-05-14 14:02:49","title":"FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling","abstract":"Dynamic scene reconstruction for autonomous driving enables vehicles to perceive and interpret complex scene changes more precisely. Dynamic Neural Radiance Fields (NeRFs) have recently shown promising capability in scene modeling. However, many existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity. To address this, we propose FreeDriveRF, which reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs. We innovatively decouple dynamic and static parts at the early sampling level using semantic supervision, mitigating image blurring and artifacts. To overcome the challenges posed by object motion and occlusion in monocular camera, we introduce a warped ray-guided dynamic object rendering consistency loss, utilizing optical flow to better constrain the dynamic modeling process. Additionally, we incorporate estimated dynamic flow to constrain the pose optimization process, improving the stability and accuracy of unbounded scene reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of our method in dynamic scene modeling for autonomous driving.","sentences":["Dynamic scene reconstruction for autonomous driving enables vehicles to perceive and interpret complex scene changes more precisely.","Dynamic Neural Radiance Fields (NeRFs) have recently shown promising capability in scene modeling.","However, many existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity.","To address this, we propose FreeDriveRF, which reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs.","We innovatively decouple dynamic and static parts at the early sampling level using semantic supervision, mitigating image blurring and artifacts.","To overcome the challenges posed by object motion and occlusion in monocular camera, we introduce a warped ray-guided dynamic object rendering consistency loss, utilizing optical flow to better constrain the dynamic modeling process.","Additionally, we incorporate estimated dynamic flow to constrain the pose optimization process, improving the stability and accuracy of unbounded scene reconstruction.","Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of our method in dynamic scene modeling for autonomous driving."],"url":"http://arxiv.org/abs/2505.09406v1"}
{"created":"2025-05-14 14:01:54","title":"Wormhole Detection Based on Z-Score And Neighbor Table Comparison","abstract":"Wormhole attacks can cause serious disruptions to the network topology in disaster rescue opportunity networks.   By establishing false Wormhole(WH) links, malicious nodes can mislead legitimate paths in the network, further causing serious consequences such as traffic analysis attacks (i.e., by eavesdropping and monitoring exchanged traffic), denial of service (DoS) or selective packet loss attacks. This paper uses rescue equipment (vehicle-mounted base stations, rescue control centers, etc.) as an effective third-party auditor (TPA), and combines the commonly used Z-Score (Standard Score) data processing method to propose a new detection method based on pure mathematical statistics for detecting wormhole attacks. Finally, we perform a large number of simulations to evaluate the proposed method. Since our proposed strategy does not require auxiliary equipment such as GPS positioning and timers, as a pure data statistical analysis method, it is obviously more economically valuable, feasible, and practical than other strategies in disaster relief.","sentences":["Wormhole attacks can cause serious disruptions to the network topology in disaster rescue opportunity networks.   ","By establishing false Wormhole(WH) links, malicious nodes can mislead legitimate paths in the network, further causing serious consequences such as traffic analysis attacks (i.e., by eavesdropping and monitoring exchanged traffic), denial of service (DoS) or selective packet loss attacks.","This paper uses rescue equipment (vehicle-mounted base stations, rescue control centers, etc.)","as an effective third-party auditor (TPA), and combines the commonly used Z-Score (Standard Score) data processing method to propose a new detection method based on pure mathematical statistics for detecting wormhole attacks.","Finally, we perform a large number of simulations to evaluate the proposed method.","Since our proposed strategy does not require auxiliary equipment such as GPS positioning and timers, as a pure data statistical analysis method, it is obviously more economically valuable, feasible, and practical than other strategies in disaster relief."],"url":"http://arxiv.org/abs/2505.09405v1"}
{"created":"2025-05-14 13:48:36","title":"UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units","abstract":"Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion. However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist. To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions. Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape. The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy.","sentences":["Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion.","However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist.","To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs.","UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data.","Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions.","Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape.","The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each.","Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy."],"url":"http://arxiv.org/abs/2505.09393v1"}
{"created":"2025-05-14 13:48:33","title":"Mitigating Configuration Differences Between Development and Production Environments: A Catalog of Strategies","abstract":"Context: The Configuration Management of the development and production environments is an important aspect of IT operations. However, managing the configuration differences between these two environments can be challenging, leading to inconsistent behavior, unexpected errors, and increased downtime. Objective: In this study, we sought to investigate the strategies software companies employ to mitigate the configuration differences between the development and production environments. Our goal is to provide a comprehensive understanding of these strategies used to contribute to reducing the risk of configuration-related issues. Method: To achieve this goal, we interviewed 17 participants and leveraged the Thematic Analysis methodology to analyze the interview data. These participants shed some light on the current practices, processes, challenges, or issues they have encountered. Results: Based on the interviews, we systematically formulated and structured a catalog of eight strategies that explain how software producing companies mitigate these configuration differences. These strategies vary from 1) creating detailed configuration management plans, 2) using automation tools, and 3) developing processes to test and validate changes through containers and virtualization technologies. Conclusion: By implementing these strategies, companies can improve their ability to respond quickly and effectively to changes in the production environment. In addition, they can also ensure compliance with industry standards and regulations.","sentences":["Context: The Configuration Management of the development and production environments is an important aspect of IT operations.","However, managing the configuration differences between these two environments can be challenging, leading to inconsistent behavior, unexpected errors, and increased downtime.","Objective:","In this study, we sought to investigate the strategies software companies employ to mitigate the configuration differences between the development and production environments.","Our goal is to provide a comprehensive understanding of these strategies used to contribute to reducing the risk of configuration-related issues.","Method: To achieve this goal, we interviewed 17 participants and leveraged the Thematic Analysis methodology to analyze the interview data.","These participants shed some light on the current practices, processes, challenges, or issues they have encountered.","Results: Based on the interviews, we systematically formulated and structured a catalog of eight strategies that explain how software producing companies mitigate these configuration differences.","These strategies vary from 1) creating detailed configuration management plans, 2) using automation tools, and 3) developing processes to test and validate changes through containers and virtualization technologies.","Conclusion: By implementing these strategies, companies can improve their ability to respond quickly and effectively to changes in the production environment.","In addition, they can also ensure compliance with industry standards and regulations."],"url":"http://arxiv.org/abs/2505.09392v1"}
{"created":"2025-05-14 13:38:30","title":"FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization","abstract":"Federated semantic segmentation enables pixel-level classification in images through collaborative learning while maintaining data privacy. However, existing research commonly overlooks the fine-grained class relationships within the semantic space when addressing heterogeneous problems, particularly domain shift. This oversight results in ambiguities between class representation. To overcome this challenge, we propose a novel federated segmentation framework that strikes class consistency, termed FedSaaS. Specifically, we introduce class exemplars as a criterion for both local- and global-level class representations. On the server side, the uploaded class exemplars are leveraged to model class prototypes, which supervise global branch of clients, ensuring alignment with global-level representation. On the client side, we incorporate an adversarial mechanism to harmonize contributions of global and local branches, leading to consistent output. Moreover, multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space. Extensive experiments on several driving scene segmentation datasets demonstrate that our framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy and effectively addressing the class-consistency representation problem.","sentences":["Federated semantic segmentation enables pixel-level classification in images through collaborative learning while maintaining data privacy.","However, existing research commonly overlooks the fine-grained class relationships within the semantic space when addressing heterogeneous problems, particularly domain shift.","This oversight results in ambiguities between class representation.","To overcome this challenge, we propose a novel federated segmentation framework that strikes class consistency, termed FedSaaS.","Specifically, we introduce class exemplars as a criterion for both local- and global-level class representations.","On the server side, the uploaded class exemplars are leveraged to model class prototypes, which supervise global branch of clients, ensuring alignment with global-level representation.","On the client side, we incorporate an adversarial mechanism to harmonize contributions of global and local branches, leading to consistent output.","Moreover, multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space.","Extensive experiments on several driving scene segmentation datasets demonstrate that our framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy and effectively addressing the class-consistency representation problem."],"url":"http://arxiv.org/abs/2505.09385v1"}
{"created":"2025-05-14 13:33:38","title":"Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform","abstract":"Background: There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models. We evaluated the feasibility and effectiveness of running NeoMedSys for three months in real-world clinical settings and focused on improvement performance of an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage (ICH) detection.   Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. A pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway (site-1) with suspected traumatic brain injury (TBI) or patients with suspected stroke (site-2). We assessed ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining. Performance metrics included sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).   Results: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI. The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873). Model refinement stages were associated with notable gains, highlighting the value of real-time radiologist feedback.","sentences":["Background: There are many challenges and opportunities in the clinical deployment of AI tools in radiology.","The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models.","We evaluated the feasibility and effectiveness of running NeoMedSys for three months in real-world clinical settings and focused on improvement performance of an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage (ICH) detection.   ","Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems.","A pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway (site-1) with suspected traumatic brain injury (TBI) or patients with suspected stroke (site-2).","We assessed ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining.","Performance metrics included sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).   ","Results: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy.","Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI.","The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%).","The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).","Model refinement stages were associated with notable gains, highlighting the value of real-time radiologist feedback."],"url":"http://arxiv.org/abs/2505.09380v1"}
{"created":"2025-05-14 13:23:29","title":"A Dynamic Working Set Method for Compressed Sensing","abstract":"We propose a dynamic working set method (DWS) for the problem $\\min_{\\mathtt{x} \\in \\mathbb{R}^n} \\frac{1}{2}\\|\\mathtt{Ax}-\\mathtt{b}\\|^2 + \\eta\\|\\mathtt{x}\\|_1$ that arises from compressed sensing. DWS manages the working set while iteratively calling a regression solver to generate progressively better solutions. Our experiments show that DWS is more efficient than other state-of-the-art software in the context of compressed sensing. Scale space such that $\\|b\\|=1$. Let $s$ be the number of non-zeros in the unknown signal. We prove that for any given $\\varepsilon > 0$, DWS reaches a solution with an additive error $\\varepsilon/\\eta^2$ such that each call of the solver uses only $O(\\frac{1}{\\varepsilon}s\\log s \\log\\frac{1}{\\varepsilon})$ variables, and each intermediate solution has $O(\\frac{1}{\\varepsilon}s\\log s\\log\\frac{1}{\\varepsilon})$ non-zero coordinates.","sentences":["We propose a dynamic working set method (DWS) for the problem $\\min_{\\mathtt{x} \\in \\mathbb{R}^n} \\frac{1}{2}\\|\\mathtt{Ax}-\\mathtt{b}\\|^2 + \\eta\\|\\mathtt{x}\\|_1$ that arises from compressed sensing.","DWS manages the working set while iteratively calling a regression solver to generate progressively better solutions.","Our experiments show that DWS is more efficient than other state-of-the-art software in the context of compressed sensing.","Scale space such that $\\|b\\|=1$. Let $s$ be the number of non-zeros in the unknown signal.","We prove that for any given $\\varepsilon > 0$, DWS reaches a solution with an additive error $\\varepsilon/\\eta^2$ such that each call of the solver uses only $O(\\frac{1}{\\varepsilon}s\\log s \\log\\frac{1}{\\varepsilon})$ variables, and each intermediate solution has $O(\\frac{1}{\\varepsilon}s\\log s\\log\\frac{1}{\\varepsilon})$ non-zero coordinates."],"url":"http://arxiv.org/abs/2505.09370v1"}
{"created":"2025-05-14 13:18:57","title":"Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks","abstract":"Objective: This paper investigates the potential of learnable activation functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a lower-limb prosthesis. In addition, user-specific vs. pooled training data is evaluated to improve machine learning (ML) and Deep Learning (DL) performance for turn intent prediction.   Method: Inertial measurement unit (IMU) data from the shank were collected from five individuals with lower-limb amputation performing turning tasks in a laboratory setting. Ability to classify an upcoming turn was evaluated for Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models) assessed the effectiveness of learnable activation functions. Models were trained separately on user-specific and pooled data to evaluate the impact of training data on their performance.   Results: Learnable activation functions in KAN and FKAN did not yield significant improvement compared to MLP and CNN, respectively. Training on user-specific data yielded superior results compared to pooled data for ML models ($p < 0.05$). In contrast, no significant difference was observed between user-specific and pooled training for DL models.   Significance: These findings suggest that learnable activation functions may demonstrate distinct advantages in datasets involving more complex tasks and larger volumes. In addition, pooled training showed comparable performance to user-specific training in DL models, indicating that model training for prosthesis control can utilize data from multiple participants.","sentences":["Objective: This paper investigates the potential of learnable activation functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a lower-limb prosthesis.","In addition, user-specific vs. pooled training data is evaluated to improve machine learning (ML) and Deep Learning (DL) performance for turn intent prediction.   ","Method: Inertial measurement unit (IMU) data from the shank were collected from five individuals with lower-limb amputation performing turning tasks in a laboratory setting.","Ability to classify an upcoming turn was evaluated for Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN).","The comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models) assessed the effectiveness of learnable activation functions.","Models were trained separately on user-specific and pooled data to evaluate the impact of training data on their performance.   ","Results:","Learnable activation functions in KAN and FKAN did not yield significant improvement compared to MLP and CNN, respectively.","Training on user-specific data yielded superior results compared to pooled data for ML models ($p < 0.05$).","In contrast, no significant difference was observed between user-specific and pooled training for DL models.   ","Significance: These findings suggest that learnable activation functions may demonstrate distinct advantages in datasets involving more complex tasks and larger volumes.","In addition, pooled training showed comparable performance to user-specific training in DL models, indicating that model training for prosthesis control can utilize data from multiple participants."],"url":"http://arxiv.org/abs/2505.09366v1"}
{"created":"2025-05-14 13:07:03","title":"Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis","abstract":"The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io","sentences":["The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models.","In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning.","Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures.","Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets.","These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world.","In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition.","Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization.","Project page: https://marigoldcomputervision.github.io"],"url":"http://arxiv.org/abs/2505.09358v1"}
{"created":"2025-05-14 13:06:42","title":"APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression","abstract":"Precise initialization plays a critical role in the performance of localization algorithms, especially in the context of robotics, autonomous driving, and computer vision. Poor localization accuracy is often a consequence of inaccurate initial poses, particularly noticeable in GNSS-denied environments where GPS signals are primarily relied upon for initialization. Recent advances in leveraging deep neural networks for pose regression have led to significant improvements in both accuracy and robustness, especially in estimating complex spatial relationships and orientations. In this paper, we introduce APR-Transformer, a model architecture inspired by state-of-the-art methods, which predicts absolute pose (3D position and 3D orientation) using either image or LiDAR data. We demonstrate that our proposed method achieves state-of-the-art performance on established benchmark datasets such as the Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our experiments to include our custom complex APR-BeIntelli dataset. Additionally, we validate the reliability of our approach in GNSS-denied environments by deploying the model in real-time on an autonomous test vehicle. This showcases the practical feasibility and effectiveness of our approach. The source code is available at:https://github.com/GT-ARC/APR-Transformer.","sentences":["Precise initialization plays a critical role in the performance of localization algorithms, especially in the context of robotics, autonomous driving, and computer vision.","Poor localization accuracy is often a consequence of inaccurate initial poses, particularly noticeable in GNSS-denied environments where GPS signals are primarily relied upon for initialization.","Recent advances in leveraging deep neural networks for pose regression have led to significant improvements in both accuracy and robustness, especially in estimating complex spatial relationships and orientations.","In this paper, we introduce APR-Transformer, a model architecture inspired by state-of-the-art methods, which predicts absolute pose (3D position and 3D orientation) using either image or LiDAR data.","We demonstrate that our proposed method achieves state-of-the-art performance on established benchmark datasets such as the Radar Oxford Robot-Car and DeepLoc datasets.","Furthermore, we extend our experiments to include our custom complex APR-BeIntelli dataset.","Additionally, we validate the reliability of our approach in GNSS-denied environments by deploying the model in real-time on an autonomous test vehicle.","This showcases the practical feasibility and effectiveness of our approach.","The source code is available at:https://github.com/GT-ARC/APR-Transformer."],"url":"http://arxiv.org/abs/2505.09356v1"}
{"created":"2025-05-14 12:31:21","title":"Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition","abstract":"In this paper, we introduce MultiviewVLM, a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. Our architecture integrates pseudo-labels derived from generated textual prompts to guide implicit alignment of emotional semantics. To capture shared information across multi-views, we propose a joint embedding space that aligns multiview representations without requiring explicit supervision. We further enhance the discriminability of our model through a novel multiview contrastive learning strategy that leverages stable positive-negative pair sampling. A gradient-friendly loss function is introduced to promote smoother and more stable convergence, and the model is optimized for distributed training to ensure scalability. Extensive experiments demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications.","sentences":["In this paper, we introduce MultiviewVLM, a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data.","Our architecture integrates pseudo-labels derived from generated textual prompts to guide implicit alignment of emotional semantics.","To capture shared information across multi-views, we propose a joint embedding space that aligns multiview representations without requiring explicit supervision.","We further enhance the discriminability of our model through a novel multiview contrastive learning strategy that leverages stable positive-negative pair sampling.","A gradient-friendly loss function is introduced to promote smoother and more stable convergence, and the model is optimized for distributed training to ensure scalability.","Extensive experiments demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications."],"url":"http://arxiv.org/abs/2505.09336v1"}
{"created":"2025-05-14 12:25:41","title":"BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis","abstract":"Scaling up model and data size have demonstrated impressive performance improvement over a wide range of tasks. Despite extensive studies on scaling behaviors for general-purpose tasks, medical images exhibit substantial differences from natural data. It remains unclear the key factors in developing medical vision foundation models at scale due to the absence of an extensive understanding of scaling behavior in the medical domain. In this paper, we explored the scaling behavior across model sizes, training algorithms, data sizes, and imaging modalities in developing scalable medical vision foundation models by self-supervised learning. To support scalable pretraining, we introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a wide range of biomedical image modalities and anatomies. We observed that scaling up does provide benefits but varies across tasks. Additional analysis reveals several factors correlated with scaling benefits. Finally, we propose BioVFM, a large-scale medical vision foundation model pretrained on 21 million biomedical images, which outperforms the previous state-of-the-art foundation models across 12 medical benchmarks. Our results highlight that while scaling up is beneficial for pursuing better performance, task characteristics, data diversity, pretraining methods, and computational efficiency remain critical considerations for developing scalable medical foundation models.","sentences":["Scaling up model and data size have demonstrated impressive performance improvement over a wide range of tasks.","Despite extensive studies on scaling behaviors for general-purpose tasks, medical images exhibit substantial differences from natural data.","It remains unclear the key factors in developing medical vision foundation models at scale due to the absence of an extensive understanding of scaling behavior in the medical domain.","In this paper, we explored the scaling behavior across model sizes, training algorithms, data sizes, and imaging modalities in developing scalable medical vision foundation models by self-supervised learning.","To support scalable pretraining, we introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a wide range of biomedical image modalities and anatomies.","We observed that scaling up does provide benefits but varies across tasks.","Additional analysis reveals several factors correlated with scaling benefits.","Finally, we propose BioVFM, a large-scale medical vision foundation model pretrained on 21 million biomedical images, which outperforms the previous state-of-the-art foundation models across 12 medical benchmarks.","Our results highlight that while scaling up is beneficial for pursuing better performance, task characteristics, data diversity, pretraining methods, and computational efficiency remain critical considerations for developing scalable medical foundation models."],"url":"http://arxiv.org/abs/2505.09329v1"}
{"created":"2025-05-14 12:24:05","title":"SingNet: Towards a Large-Scale, Diverse, and In-the-Wild Singing Voice Dataset","abstract":"The lack of a publicly-available large-scale and diverse dataset has long been a significant bottleneck for singing voice applications like Singing Voice Synthesis (SVS) and Singing Voice Conversion (SVC). To tackle this problem, we present SingNet, an extensive, diverse, and in-the-wild singing voice dataset. Specifically, we propose a data processing pipeline to extract ready-to-use training data from sample packs and songs on the internet, forming 3000 hours of singing voices in various languages and styles. Furthermore, to facilitate the use and demonstrate the effectiveness of SingNet, we pre-train and open-source various state-of-the-art (SOTA) models on Wav2vec2, BigVGAN, and NSF-HiFiGAN based on our collected singing voice data. We also conduct benchmark experiments on Automatic Lyric Transcription (ALT), Neural Vocoder, and Singing Voice Conversion (SVC). Audio demos are available at: https://singnet-dataset.github.io/.","sentences":["The lack of a publicly-available large-scale and diverse dataset has long been a significant bottleneck for singing voice applications like Singing Voice Synthesis (SVS) and Singing Voice Conversion (SVC).","To tackle this problem, we present SingNet, an extensive, diverse, and in-the-wild singing voice dataset.","Specifically, we propose a data processing pipeline to extract ready-to-use training data from sample packs and songs on the internet, forming 3000 hours of singing voices in various languages and styles.","Furthermore, to facilitate the use and demonstrate the effectiveness of SingNet, we pre-train and open-source various state-of-the-art (SOTA) models on Wav2vec2, BigVGAN, and NSF-HiFiGAN based on our collected singing voice data.","We also conduct benchmark experiments on Automatic Lyric Transcription (ALT), Neural Vocoder, and Singing Voice Conversion (SVC).","Audio demos are available at: https://singnet-dataset.github.io/."],"url":"http://arxiv.org/abs/2505.09325v1"}
{"created":"2025-05-14 12:23:53","title":"Neural Video Compression using 2D Gaussian Splatting","abstract":"The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.","sentences":["The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc.","However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect.","Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods.","This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces.","However, their high computational demands currently limit their use in real-time applications like video conferencing.","To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting.","Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes.","In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space."],"url":"http://arxiv.org/abs/2505.09324v1"}
{"created":"2025-05-14 12:22:33","title":"Online Bin Packing with Item Size Estimates","abstract":"Imagine yourself moving to another place, and therefore, you need to pack all of your belongings into moving boxes with some capacity. In the classical bin packing model, you would try to minimize the number of boxes, knowing the exact size of each item you want to pack. In the online bin packing problem, you need to start packing the first item into a box, without knowing what other stuff is upcoming.   Both settings are somewhat unrealistic, as you are likely not willing to measure the exact size of all your belongings before packing the first item, but you are not completely clueless about what other stuff you have when you start packing. In this article, we introduce the online bin packing with estimates model, where you start packing with a rough idea about the upcoming item sizes in mind.   In this model, an algorithm receives a size estimate for every item in the input list together with an accuracy factor $\\delta$ in advance. Just as for regular online bin packing the items are then presented iteratively. The actual sizes of the items are allowed to deviate from the size estimate by a factor of $\\delta$. Once the actual size of an item is revealed the algorithm has to make an irrevocable decision on the question where to place it. This is the first time online bin packing is studied under this model.   This article has three main results: First, no algorithm can achieve a competitive ratio of less than $\\frac{4}{3}$, even for an arbitrary small factor $\\delta>0$. Second, we present an algorithm that is $1.5$-competitive for all $\\delta \\leq \\frac{1}{35}$. Finally, we design a strategy that yields a competitive ratio of $\\frac{4}{3}$ under the assumption that not more than two items can be placed in the same bin, which is best possible in this setting.","sentences":["Imagine yourself moving to another place, and therefore, you need to pack all of your belongings into moving boxes with some capacity.","In the classical bin packing model, you would try to minimize the number of boxes, knowing the exact size of each item you want to pack.","In the online bin packing problem, you need to start packing the first item into a box, without knowing what other stuff is upcoming.   ","Both settings are somewhat unrealistic, as you are likely not willing to measure the exact size of all your belongings before packing the first item, but you are not completely clueless about what other stuff you have when you start packing.","In this article, we introduce the online bin packing with estimates model, where you start packing with a rough idea about the upcoming item sizes in mind.   ","In this model, an algorithm receives a size estimate for every item in the input list together with an accuracy factor $\\delta$ in advance.","Just as for regular online bin packing the items are then presented iteratively.","The actual sizes of the items are allowed to deviate from the size estimate by a factor of $\\delta$.","Once the actual size of an item is revealed the algorithm has to make an irrevocable decision on the question where to place it.","This is the first time online bin packing is studied under this model.   ","This article has three main results:","First, no algorithm can achieve a competitive ratio of less than $\\frac{4}{3}$, even for an arbitrary small factor $\\delta>0$. Second, we present an algorithm that is $1.5$-competitive for all $\\delta \\leq \\frac{1}{35}$. Finally, we design a strategy that yields a competitive ratio of $\\frac{4}{3}$ under the assumption that not more than two items can be placed in the same bin, which is best possible in this setting."],"url":"http://arxiv.org/abs/2505.09321v1"}
{"created":"2025-05-14 12:16:40","title":"Statistical Modeling and Uncertainty Estimation of LLM Inference Systems","abstract":"Large Language Model (LLM) inference systems present significant challenges in statistical performance characterization due to dynamic workload variations, diverse hardware architectures, and complex interactions between model size, batch processing, and throughput requirements. Accurate statistical characterization enables better workload scheduling, adaptive resource provisioning, and cost-aware inference optimization, making it crucial for improving efficiency in large-scale AI deployments. Traditional analytical models provide explainability but cannot cover the vast diversity of real-world workloads, making it impossible to benchmark every scenario in advance. Machine learning (ML) approaches effectively predict performance for non-benchmarked cases but struggle when extrapolating beyond their observed training space. To address these limitations for LLM inference systems, we propose an Analytical with Learning Augmentation (ALA) framework that bridges analytical modeling with \\ml for robust statistical prediction and uncertainty estimation in LLM inference workloads. Our method employs an analytical throughput model with parameters estimated for benchmarked workloads, then extends to unobserved configurations using \\ml predictions. We enhance this with simulated annealing to exploit subsets of the workload data point combinations and develop an error predictor. Finally, we quantify uncertainty based on vector space similarity between new and observed workloads to ensure robust generalization. Through extensive experimentation on diverse LLM inference workloads, we demonstrate that our framework achieves low median errors while maintaining adaptability to new inference scenarios.","sentences":["Large Language Model (LLM) inference systems present significant challenges in statistical performance characterization due to dynamic workload variations, diverse hardware architectures, and complex interactions between model size, batch processing, and throughput requirements.","Accurate statistical characterization enables better workload scheduling, adaptive resource provisioning, and cost-aware inference optimization, making it crucial for improving efficiency in large-scale AI deployments.","Traditional analytical models provide explainability but cannot cover the vast diversity of real-world workloads, making it impossible to benchmark every scenario in advance.","Machine learning (ML) approaches effectively predict performance for non-benchmarked cases but struggle when extrapolating beyond their observed training space.","To address these limitations for LLM inference systems, we propose an Analytical with Learning Augmentation (ALA) framework that bridges analytical modeling with \\ml for robust statistical prediction and uncertainty estimation in LLM inference workloads.","Our method employs an analytical throughput model with parameters estimated for benchmarked workloads, then extends to unobserved configurations using \\ml predictions.","We enhance this with simulated annealing to exploit subsets of the workload data point combinations and develop an error predictor.","Finally, we quantify uncertainty based on vector space similarity between new and observed workloads to ensure robust generalization.","Through extensive experimentation on diverse LLM inference workloads, we demonstrate that our framework achieves low median errors while maintaining adaptability to new inference scenarios."],"url":"http://arxiv.org/abs/2505.09319v1"}
{"created":"2025-05-14 11:52:45","title":"Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model","abstract":"The Unconstrained Feature Model (UFM) is a mathematical framework that enables closed-form approximations for minimal training loss and related performance measures in deep neural networks (DNNs). This paper leverages the UFM to provide qualitative insights into neural multivariate regression, a critical task in imitation learning, robotics, and reinforcement learning. Specifically, we address two key questions: (1) How do multi-task models compare to multiple single-task models in terms of training performance? (2) Can whitening and normalizing regression targets improve training performance? The UFM theory predicts that multi-task models achieve strictly smaller training MSE than multiple single-task models when the same or stronger regularization is applied to the latter, and our empirical results confirm these findings. Regarding whitening and normalizing regression targets, the UFM theory predicts that they reduce training MSE when the average variance across the target dimensions is less than one, and our empirical results once again confirm these findings. These findings highlight the UFM as a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies.","sentences":["The Unconstrained Feature Model (UFM) is a mathematical framework that enables closed-form approximations for minimal training loss and related performance measures in deep neural networks (DNNs).","This paper leverages the UFM to provide qualitative insights into neural multivariate regression, a critical task in imitation learning, robotics, and reinforcement learning.","Specifically, we address two key questions: (1) How do multi-task models compare to multiple single-task models in terms of training performance?","(2) Can whitening and normalizing regression targets improve training performance?","The UFM theory predicts that multi-task models achieve strictly smaller training MSE than multiple single-task models when the same or stronger regularization is applied to the latter, and our empirical results confirm these findings.","Regarding whitening and normalizing regression targets, the UFM theory predicts that they reduce training MSE when the average variance across the target dimensions is less than one, and our empirical results once again confirm these findings.","These findings highlight the UFM as a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies."],"url":"http://arxiv.org/abs/2505.09308v1"}
{"created":"2025-05-14 11:42:09","title":"Predicting butterfly species presence from satellite imagery using soft contrastive regularisation","abstract":"The growing demand for scalable biodiversity monitoring methods has fuelled interest in remote sensing data, due to its widespread availability and extensive coverage. Traditionally, the application of remote sensing to biodiversity research has focused on mapping and monitoring habitats, but with increasing availability of large-scale citizen-science wildlife observation data, recent methods have started to explore predicting multi-species presence directly from satellite images. This paper presents a new data set for predicting butterfly species presence from satellite data in the United Kingdom. We experimentally optimise a Resnet-based model to predict multi-species presence from 4-band satellite images, and find that this model especially outperforms the mean rate baseline for locations with high species biodiversity. To improve performance, we develop a soft, supervised contrastive regularisation loss that is tailored to probabilistic labels (such as species-presence data), and demonstrate that this improves prediction accuracy. In summary, our new data set and contrastive regularisation method contribute to the open challenge of accurately predicting species biodiversity from remote sensing data, which is key for efficient biodiversity monitoring.","sentences":["The growing demand for scalable biodiversity monitoring methods has fuelled interest in remote sensing data, due to its widespread availability and extensive coverage.","Traditionally, the application of remote sensing to biodiversity research has focused on mapping and monitoring habitats, but with increasing availability of large-scale citizen-science wildlife observation data, recent methods have started to explore predicting multi-species presence directly from satellite images.","This paper presents a new data set for predicting butterfly species presence from satellite data in the United Kingdom.","We experimentally optimise a Resnet-based model to predict multi-species presence from 4-band satellite images, and find that this model especially outperforms the mean rate baseline for locations with high species biodiversity.","To improve performance, we develop a soft, supervised contrastive regularisation loss that is tailored to probabilistic labels (such as species-presence data), and demonstrate that this improves prediction accuracy.","In summary, our new data set and contrastive regularisation method contribute to the open challenge of accurately predicting species biodiversity from remote sensing data, which is key for efficient biodiversity monitoring."],"url":"http://arxiv.org/abs/2505.09306v1"}
{"created":"2025-05-14 11:22:54","title":"Toward Fair Federated Learning under Demographic Disparities and Data Imbalance","abstract":"Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities. Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect. We propose FedIDA (Fed erated Learning for Imbalance and D isparity A wareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling. FedIDA supports multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm. We provide theoretical analysis establishing fairness improvement bounds using Lipschitz continuity and concentration inequalities, and show that FedIDA reduces the variance of fairness metrics across test sets. Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare. The source code is available on GitHub.","sentences":["Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities.","Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect.","We propose FedIDA (Fed erated Learning for Imbalance and D isparity A wareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling.","FedIDA supports multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm.","We provide theoretical analysis establishing fairness improvement bounds using Lipschitz continuity and concentration inequalities, and show that FedIDA reduces the variance of fairness metrics across test sets.","Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare.","The source code is available on GitHub."],"url":"http://arxiv.org/abs/2505.09295v1"}
{"created":"2025-05-14 11:22:22","title":"On the Learning with Augmented Class via Forests","abstract":"Decision trees and forests have achieved successes in various real applications, most working with all testing classes known in training data. In this work, we focus on learning with augmented class via forests, where an augmented class may appear in testing data yet not in training data. We incorporate information of augmented class into trees' splitting, i.e., a new splitting criterion, called augmented Gini impurity, is introduced to exploit some unlabeled data from testing distribution. We then develop the approach named Learning with Augmented Class via Forests (LACForest), which constructs shallow forests based on the augmented Gini impurity and then splits forests with pseudo-labeled augmented instances for better performance. We also develop deep neural forests with a novel optimization objective based on our augmented Gini impurity, so as to utilize the representation power of neural networks for forests. Theoretically, we present the convergence analysis for augmented Gini impurity, and finally conduct experiments to verify the effectiveness of our approaches. The code is available at https://github.com/nju-xuf/LACForest/.","sentences":["Decision trees and forests have achieved successes in various real applications, most working with all testing classes known in training data.","In this work, we focus on learning with augmented class via forests, where an augmented class may appear in testing data yet not in training data.","We incorporate information of augmented class into trees' splitting, i.e., a new splitting criterion, called augmented Gini impurity, is introduced to exploit some unlabeled data from testing distribution.","We then develop the approach named Learning with Augmented Class via Forests (LACForest), which constructs shallow forests based on the augmented Gini impurity and then splits forests with pseudo-labeled augmented instances for better performance.","We also develop deep neural forests with a novel optimization objective based on our augmented Gini impurity, so as to utilize the representation power of neural networks for forests.","Theoretically, we present the convergence analysis for augmented Gini impurity, and finally conduct experiments to verify the effectiveness of our approaches.","The code is available at https://github.com/nju-xuf/LACForest/."],"url":"http://arxiv.org/abs/2505.09294v1"}
{"created":"2025-05-14 11:12:30","title":"Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features","abstract":"Digital textbooks are widely used in various educational contexts, such as university courses and online lectures. Such textbooks yield learning log data that have been used in numerous educational data mining (EDM) studies for student behavior analysis and performance prediction. However, these studies have faced challenges in integrating confidential data, such as academic records and learning logs, across schools due to privacy concerns. Consequently, analyses are often conducted with data limited to a single school, which makes developing high-performing and generalizable models difficult. This study proposes a method that combines federated learning and differential features to address these issues. Federated learning enables model training without centralizing data, thereby preserving student privacy. Differential features, which utilize relative values instead of absolute values, enhance model performance and generalizability. To evaluate the proposed method, a model for predicting at-risk students was trained using data from 1,136 students across 12 courses conducted over 4 years, and validated on hold-out test data from 5 other courses. Experimental results demonstrated that the proposed method addresses privacy concerns while achieving performance comparable to that of models trained via centralized learning in terms of Top-n precision, nDCG, and PR-AUC. Furthermore, using differential features improved prediction performance across all evaluation datasets compared to non-differential approaches. The trained models were also applicable for early prediction, achieving high performance in detecting at-risk students in earlier stages of the semester within the validation datasets.","sentences":["Digital textbooks are widely used in various educational contexts, such as university courses and online lectures.","Such textbooks yield learning log data that have been used in numerous educational data mining (EDM) studies for student behavior analysis and performance prediction.","However, these studies have faced challenges in integrating confidential data, such as academic records and learning logs, across schools due to privacy concerns.","Consequently, analyses are often conducted with data limited to a single school, which makes developing high-performing and generalizable models difficult.","This study proposes a method that combines federated learning and differential features to address these issues.","Federated learning enables model training without centralizing data, thereby preserving student privacy.","Differential features, which utilize relative values instead of absolute values, enhance model performance and generalizability.","To evaluate the proposed method, a model for predicting at-risk students was trained using data from 1,136 students across 12 courses conducted over 4 years, and validated on hold-out test data from 5 other courses.","Experimental results demonstrated that the proposed method addresses privacy concerns while achieving performance comparable to that of models trained via centralized learning in terms of Top-n precision, nDCG, and PR-AUC.","Furthermore, using differential features improved prediction performance across all evaluation datasets compared to non-differential approaches.","The trained models were also applicable for early prediction, achieving high performance in detecting at-risk students in earlier stages of the semester within the validation datasets."],"url":"http://arxiv.org/abs/2505.09287v1"}
{"created":"2025-05-14 11:11:17","title":"A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data","abstract":"Effectively analyzing online review data is essential across industries. However, many existing studies are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets. To address these limitations, we propose a multilingual, scalable, and unsupervised framework for cross-domain aspect detection. This framework is designed for multi-aspect labeling of multilingual and multi-domain review data. In this study, we apply automatic labeling to Korean and English review datasets spanning various domains and assess the quality of the generated labels through extensive experiments. Aspect category candidates are first extracted through clustering, and each review is then represented as an aspect-aware embedding vector using negative sampling. To evaluate the framework, we conduct multi-aspect labeling and fine-tune several pretrained language models to measure the effectiveness of the automatically generated labels. Results show that these models achieve high performance, demonstrating that the labels are suitable for training. Furthermore, comparisons with publicly available large language models highlight the framework's superior consistency and scalability when processing large-scale data. A human evaluation also confirms that the quality of the automatic labels is comparable to those created manually. This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments. Future research will explore automatic review summarization and the integration of artificial intelligence agents to further improve the efficiency and depth of review analysis.","sentences":["Effectively analyzing online review data is essential across industries.","However, many existing studies are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets.","To address these limitations, we propose a multilingual, scalable, and unsupervised framework for cross-domain aspect detection.","This framework is designed for multi-aspect labeling of multilingual and multi-domain review data.","In this study, we apply automatic labeling to Korean and English review datasets spanning various domains and assess the quality of the generated labels through extensive experiments.","Aspect category candidates are first extracted through clustering, and each review is then represented as an aspect-aware embedding vector using negative sampling.","To evaluate the framework, we conduct multi-aspect labeling and fine-tune several pretrained language models to measure the effectiveness of the automatically generated labels.","Results show that these models achieve high performance, demonstrating that the labels are suitable for training.","Furthermore, comparisons with publicly available large language models highlight the framework's superior consistency and scalability when processing large-scale data.","A human evaluation also confirms that the quality of the automatic labels is comparable to those created manually.","This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments.","Future research will explore automatic review summarization and the integration of artificial intelligence agents to further improve the efficiency and depth of review analysis."],"url":"http://arxiv.org/abs/2505.09286v1"}
{"created":"2025-05-14 11:09:15","title":"Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations","abstract":"Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations presents a fundamental challenge in scientific research. Recently, diffusion-based generative modeling shows promising potential for physical simulation. However, current approaches typically operate on on-grid data with preset spatiotemporal resolution, but struggle with the sparsely observed and continuous nature of real-world physical dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in Functional Tucker space, a novel framework that generates full-field evolution of physical dynamics from irregular sparse observations. SDIFT leverages the functional Tucker model as the latent space representer with proven universal approximation property, and represents observations as latent functions and Tucker core sequences. We then construct a sequential diffusion model with temporally augmented UNet in the functional Tucker space, denoising noise drawn from a Gaussian process to generate the sequence of core tensors.   At the posterior sampling stage, we propose a Message-Passing Posterior Sampling mechanism, enabling conditional generation of the entire sequence guided by observations at limited time steps. We validate SDIFT on three physical systems spanning astronomical (supernova explosions, light-year scale), environmental (ocean sound speed fields, kilometer scale), and molecular (organic liquid, millimeter scale) domains, demonstrating significant improvements in both reconstruction accuracy and computational efficiency compared to state-of-the-art approaches.","sentences":["Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations presents a fundamental challenge in scientific research.","Recently, diffusion-based generative modeling shows promising potential for physical simulation.","However, current approaches typically operate on on-grid data with preset spatiotemporal resolution, but struggle with the sparsely observed and continuous nature of real-world physical dynamics.","To fill the gaps, we present SDIFT, Sequential DIffusion in Functional Tucker space, a novel framework that generates full-field evolution of physical dynamics from irregular sparse observations.","SDIFT leverages the functional Tucker model as the latent space representer with proven universal approximation property, and represents observations as latent functions and Tucker core sequences.","We then construct a sequential diffusion model with temporally augmented UNet in the functional Tucker space, denoising noise drawn from a Gaussian process to generate the sequence of core tensors.   ","At the posterior sampling stage, we propose a Message-Passing Posterior Sampling mechanism, enabling conditional generation of the entire sequence guided by observations at limited time steps.","We validate SDIFT on three physical systems spanning astronomical (supernova explosions, light-year scale), environmental (ocean sound speed fields, kilometer scale), and molecular (organic liquid, millimeter scale) domains, demonstrating significant improvements in both reconstruction accuracy and computational efficiency compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2505.09284v1"}
{"created":"2025-05-14 10:59:09","title":"A drone that learns to efficiently find objects in agricultural fields: from simulation to the real world","abstract":"Drones are promising for data collection in precision agriculture, however, they are limited by their battery capacity. Efficient path planners are therefore required. This paper presents a drone path planner trained using Reinforcement Learning (RL) on an abstract simulation that uses object detections and uncertain prior knowledge. The RL agent controls the flight direction and can terminate the flight. By using the agent in combination with the drone's flight controller and a detection network to process camera images, it is possible to evaluate the performance of the agent on real-world data. In simulation, the agent yielded on average a 78% shorter flight path compared to a full coverage planner, at the cost of a 14% lower recall. On real-world data, the agent showed a 72% shorter flight path compared to a full coverage planner, however, at the cost of a 25% lower recall. The lower performance on real-world data was attributed to the real-world object distribution and the lower accuracy of prior knowledge, and shows potential for improvement. Overall, we concluded that for applications where it is not crucial to find all objects, such as weed detection, the learned-based path planner is suitable and efficient.","sentences":["Drones are promising for data collection in precision agriculture, however, they are limited by their battery capacity.","Efficient path planners are therefore required.","This paper presents a drone path planner trained using Reinforcement Learning (RL) on an abstract simulation that uses object detections and uncertain prior knowledge.","The RL agent controls the flight direction and can terminate the flight.","By using the agent in combination with the drone's flight controller and a detection network to process camera images, it is possible to evaluate the performance of the agent on real-world data.","In simulation, the agent yielded on average a 78% shorter flight path compared to a full coverage planner, at the cost of a 14% lower recall.","On real-world data, the agent showed a 72% shorter flight path compared to a full coverage planner, however, at the cost of a 25% lower recall.","The lower performance on real-world data was attributed to the real-world object distribution and the lower accuracy of prior knowledge, and shows potential for improvement.","Overall, we concluded that for applications where it is not crucial to find all objects, such as weed detection, the learned-based path planner is suitable and efficient."],"url":"http://arxiv.org/abs/2505.09278v1"}
{"created":"2025-05-14 10:49:07","title":"Privacy-Preserving Runtime Verification","abstract":"Runtime verification offers scalable solutions to improve the safety and reliability of systems. However, systems that require verification or monitoring by a third party to ensure compliance with a specification might contain sensitive information, causing privacy concerns when usual runtime verification approaches are used. Privacy is compromised if protected information about the system, or sensitive data that is processed by the system, is revealed. In addition, revealing the specification being monitored may undermine the essence of third-party verification.   In this work, we propose two novel protocols for the privacy-preserving runtime verification of systems against formal sequential specifications. In our first protocol, the monitor verifies whether the system satisfies the specification without learning anything else, though both parties are aware of the specification. Our second protocol ensures that the system remains oblivious to the monitored specification, while the monitor learns only whether the system satisfies the specification and nothing more. Our protocols adapt and improve existing techniques used in cryptography, and more specifically, multi-party computation.   The sequential specification defines the observation step of the monitor, whose granularity depends on the situation (e.g., banks may be monitored on a daily basis). Our protocols exchange a single message per observation step, after an initialisation phase. This design minimises communication overhead, enabling relatively lightweight privacy-preserving monitoring. We implement our approach for monitoring specifications described by register automata and evaluate it experimentally.","sentences":["Runtime verification offers scalable solutions to improve the safety and reliability of systems.","However, systems that require verification or monitoring by a third party to ensure compliance with a specification might contain sensitive information, causing privacy concerns when usual runtime verification approaches are used.","Privacy is compromised if protected information about the system, or sensitive data that is processed by the system, is revealed.","In addition, revealing the specification being monitored may undermine the essence of third-party verification.   ","In this work, we propose two novel protocols for the privacy-preserving runtime verification of systems against formal sequential specifications.","In our first protocol, the monitor verifies whether the system satisfies the specification without learning anything else, though both parties are aware of the specification.","Our second protocol ensures that the system remains oblivious to the monitored specification, while the monitor learns only whether the system satisfies the specification and nothing more.","Our protocols adapt and improve existing techniques used in cryptography, and more specifically, multi-party computation.   ","The sequential specification defines the observation step of the monitor, whose granularity depends on the situation (e.g., banks may be monitored on a daily basis).","Our protocols exchange a single message per observation step, after an initialisation phase.","This design minimises communication overhead, enabling relatively lightweight privacy-preserving monitoring.","We implement our approach for monitoring specifications described by register automata and evaluate it experimentally."],"url":"http://arxiv.org/abs/2505.09276v1"}
{"created":"2025-05-14 10:48:37","title":"Recent Advances in Medical Imaging Segmentation: A Survey","abstract":"Medical imaging is a cornerstone of modern healthcare, driving advancements in diagnosis, treatment planning, and patient care. Among its various tasks, segmentation remains one of the most challenging problem due to factors such as data accessibility, annotation complexity, structural variability, variation in medical imaging modalities, and privacy constraints. Despite recent progress, achieving robust generalization and domain adaptation remains a significant hurdle, particularly given the resource-intensive nature of some proposed models and their reliance on domain expertise. This survey explores cutting-edge advancements in medical image segmentation, focusing on methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and Universal Models. These approaches offer promising solutions to longstanding challenges. We provide a comprehensive overview of the theoretical foundations, state-of-the-art techniques, and recent applications of these methods. Finally, we discuss inherent limitations, unresolved issues, and future research directions aimed at enhancing the practicality and accessibility of segmentation models in medical imaging. We are maintaining a \\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub Repository} to continue tracking and updating innovations in this field.","sentences":["Medical imaging is a cornerstone of modern healthcare, driving advancements in diagnosis, treatment planning, and patient care.","Among its various tasks, segmentation remains one of the most challenging problem due to factors such as data accessibility, annotation complexity, structural variability, variation in medical imaging modalities, and privacy constraints.","Despite recent progress, achieving robust generalization and domain adaptation remains a significant hurdle, particularly given the resource-intensive nature of some proposed models and their reliance on domain expertise.","This survey explores cutting-edge advancements in medical image segmentation, focusing on methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and Universal Models.","These approaches offer promising solutions to longstanding challenges.","We provide a comprehensive overview of the theoretical foundations, state-of-the-art techniques, and recent applications of these methods.","Finally, we discuss inherent limitations, unresolved issues, and future research directions aimed at enhancing the practicality and accessibility of segmentation models in medical imaging.","We are maintaining a \\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub Repository} to continue tracking and updating innovations in this field."],"url":"http://arxiv.org/abs/2505.09274v1"}
{"created":"2025-05-14 10:25:06","title":"Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation","abstract":"Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.","sentences":["Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection.","Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data.","However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection.","To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models.","Specifically, our work is divided into three stages.","In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding.","In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures).","In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies.","Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec.","The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement in AU-PR metric on segmentation task, respectively.","The code and generated anomalous data are available at https://github.com/gaobb/AnoGen."],"url":"http://arxiv.org/abs/2505.09263v1"}
{"created":"2025-05-14 10:22:13","title":"Instantiating Standards: Enabling Standard-Driven Text TTP Extraction with Evolvable Memory","abstract":"Extracting MITRE ATT\\&CK Tactics, Techniques, and Procedures (TTPs) from natural language threat reports is crucial yet challenging. Existing methods primarily focus on performance metrics using data-driven approaches, often neglecting mechanisms to ensure faithful adherence to the official standard. This deficiency compromises reliability and consistency of TTP assignments, creating intelligence silos and contradictory threat assessments across organizations. To address this, we introduce a novel framework that converts abstract standard definitions into actionable, contextualized knowledge. Our method utilizes Large Language Model (LLM) to generate, update, and apply this knowledge. This framework populates an evolvable memory with dual-layer situational knowledge instances derived from labeled examples and official definitions. The first layer identifies situational contexts (e.g., \"Communication with C2 using encoded subdomains\"), while the second layer captures distinctive features that differentiate similar techniques (e.g., distinguishing T1132 \"Data Encoding\" from T1071 \"Application Layer Protocol\" based on whether the focus is on encoding methods or protocol usage). This structured approach provides a transparent basis for explainable TTP assignments and enhanced human oversight, while also helping to standardize other TTP extraction systems. Experiments show our framework (using Qwen2.5-32B) boosts Technique F1 scores by 11\\% over GPT-4o. Qualitative analysis confirms superior standardization, enhanced transparency, and improved explainability in real-world threat intelligence scenarios. To the best of our knowledge, this is the first work that uses the LLM to generate, update, and apply the a new knowledge for TTP extraction.","sentences":["Extracting MITRE ATT\\&CK Tactics, Techniques, and Procedures (TTPs) from natural language threat reports is crucial yet challenging.","Existing methods primarily focus on performance metrics using data-driven approaches, often neglecting mechanisms to ensure faithful adherence to the official standard.","This deficiency compromises reliability and consistency of TTP assignments, creating intelligence silos and contradictory threat assessments across organizations.","To address this, we introduce a novel framework that converts abstract standard definitions into actionable, contextualized knowledge.","Our method utilizes Large Language Model (LLM) to generate, update, and apply this knowledge.","This framework populates an evolvable memory with dual-layer situational knowledge instances derived from labeled examples and official definitions.","The first layer identifies situational contexts (e.g., \"Communication with C2 using encoded subdomains\"), while the second layer captures distinctive features that differentiate similar techniques (e.g., distinguishing T1132 \"Data Encoding\" from T1071 \"Application Layer Protocol\" based on whether the focus is on encoding methods or protocol usage).","This structured approach provides a transparent basis for explainable TTP assignments and enhanced human oversight, while also helping to standardize other TTP extraction systems.","Experiments show our framework (using Qwen2.5-32B) boosts Technique F1 scores by 11\\% over GPT-4o.","Qualitative analysis confirms superior standardization, enhanced transparency, and improved explainability in real-world threat intelligence scenarios.","To the best of our knowledge, this is the first work that uses the LLM to generate, update, and apply the a new knowledge for TTP extraction."],"url":"http://arxiv.org/abs/2505.09261v1"}
{"created":"2025-05-14 10:13:40","title":"Efficient Graph Embedding at Scale: Optimizing CPU-GPU-SSD Integration","abstract":"Graph embeddings provide continuous vector representations of nodes in a graph, which are widely applicable in community detection, recommendations, and various scientific fields. However, existing graph embedding systems either face scalability challenges due to the high cost of RAM and multiple GPUs, or rely on disk storage at the expense of I/O efficiency. In this paper, we propose Legend, a lightweight heterogeneous system for graph embedding that systematically redefines data management across CPU, GPU, and NVMe SSD resources. Legend is built on a foundation of efficient data placement and retrieval strategies tailored to the unique strengths of each hardware. Key innovations include a prefetch-friendly embedding loading strategy, enabling GPUs to directly prefetch data from SSDs with minimal I/O overhead, and a high-throughput GPU-SSD direct access driver optimized for graph embedding tasks. Furthermore, we propose a customized parallel execution strategy to maximize GPU utilization, ensuring efficient handling of billion-scale datasets. Extensive experiments demonstrate that Legend achieves up to 4.8x speedup compared to state-of-the-art systems. Moreover, Legend exhibits comparable performance on a single GPU to that of the state-of-the-art system using 4 GPUs on the billion-scale dataset.","sentences":["Graph embeddings provide continuous vector representations of nodes in a graph, which are widely applicable in community detection, recommendations, and various scientific fields.","However, existing graph embedding systems either face scalability challenges due to the high cost of RAM and multiple GPUs, or rely on disk storage at the expense of I/O efficiency.","In this paper, we propose Legend, a lightweight heterogeneous system for graph embedding that systematically redefines data management across CPU, GPU, and NVMe SSD resources.","Legend is built on a foundation of efficient data placement and retrieval strategies tailored to the unique strengths of each hardware.","Key innovations include a prefetch-friendly embedding loading strategy, enabling GPUs to directly prefetch data from SSDs with minimal I/O overhead, and a high-throughput GPU-SSD direct access driver optimized for graph embedding tasks.","Furthermore, we propose a customized parallel execution strategy to maximize GPU utilization, ensuring efficient handling of billion-scale datasets.","Extensive experiments demonstrate that Legend achieves up to 4.8x speedup compared to state-of-the-art systems.","Moreover, Legend exhibits comparable performance on a single GPU to that of the state-of-the-art system using 4 GPUs on the billion-scale dataset."],"url":"http://arxiv.org/abs/2505.09258v1"}
{"created":"2025-05-14 10:11:35","title":"Test-Time Augmentation for Pose-invariant Face Recognition","abstract":"The goal of this paper is to enhance face recognition performance by augmenting head poses during the testing phase. Existing methods often rely on training on frontalised images or learning pose-invariant representations, yet both approaches typically require re-training and testing for each dataset, involving a substantial amount of effort. In contrast, this study proposes Pose-TTA, a novel approach that aligns faces at inference time without additional training. To achieve this, we employ a portrait animator that transfers the source image identity into the pose of a driving image. Instead of frontalising a side-profile face -- which can introduce distortion -- Pose-TTA generates matching side-profile images for comparison, thereby reducing identity information loss. Furthermore, we propose a weighted feature aggregation strategy to address any distortions or biases arising from the synthetic data, thus enhancing the reliability of the augmented images. Extensive experiments on diverse datasets and with various pre-trained face recognition models demonstrate that Pose-TTA consistently improves inference performance. Moreover, our method is straightforward to integrate into existing face recognition pipelines, as it requires no retraining or fine-tuning of the underlying recognition models.","sentences":["The goal of this paper is to enhance face recognition performance by augmenting head poses during the testing phase.","Existing methods often rely on training on frontalised images or learning pose-invariant representations, yet both approaches typically require re-training and testing for each dataset, involving a substantial amount of effort.","In contrast, this study proposes Pose-TTA, a novel approach that aligns faces at inference time without additional training.","To achieve this, we employ a portrait animator that transfers the source image identity into the pose of a driving image.","Instead of frontalising a side-profile face -- which can introduce distortion -- Pose-TTA generates matching side-profile images for comparison, thereby reducing identity information loss.","Furthermore, we propose a weighted feature aggregation strategy to address any distortions or biases arising from the synthetic data, thus enhancing the reliability of the augmented images.","Extensive experiments on diverse datasets and with various pre-trained face recognition models demonstrate that Pose-TTA consistently improves inference performance.","Moreover, our method is straightforward to integrate into existing face recognition pipelines, as it requires no retraining or fine-tuning of the underlying recognition models."],"url":"http://arxiv.org/abs/2505.09256v1"}
{"created":"2025-05-14 09:51:43","title":"Structural Parameterization of Steiner Tree Packing","abstract":"Steiner Tree Packing (STP) is a notoriously hard problem in classical complexity theory, which is of practical relevance to VLSI circuit design. Previous research has approached this problem by providing heuristic or approximate algorithms. In this paper, we show the first FPT algorithms for STP parameterized by structural parameters of the input graph. In particular, we show that STP is fixed-parameter tractable by the tree-cut width as well as the fracture number of the input graph.   To achieve our results, we generalize techniques from Edge-Disjoint Paths (EDP) to Generalized Steiner Tree Packing (GSTP), which generalizes both STP and EDP. First, we derive the notion of the augmented graph for GSTP analogous to EDP. We then show that GSTP is FPT by (1) the tree-cut width of the augmented graph, (2) the fracture number of the augmented graph, (3) the slim tree-cut width of the input graph. The latter two results were previously known for EDP; our results generalize these to GSTP and improve the running time for the parameter fracture number. On the other hand, it was open whether EDP is FPT parameterized by the tree-cut width of the augmented graph, despite extensive research on the structural complexity of the problem. We settle this question affirmatively.","sentences":["Steiner Tree Packing (STP) is a notoriously hard problem in classical complexity theory, which is of practical relevance to VLSI circuit design.","Previous research has approached this problem by providing heuristic or approximate algorithms.","In this paper, we show the first FPT algorithms for STP parameterized by structural parameters of the input graph.","In particular, we show that STP is fixed-parameter tractable by the tree-cut width as well as the fracture number of the input graph.   ","To achieve our results, we generalize techniques from Edge-Disjoint Paths (EDP) to Generalized Steiner Tree Packing (GSTP), which generalizes both STP and EDP.","First, we derive the notion of the augmented graph for GSTP analogous to EDP.","We then show that GSTP is FPT by (1) the tree-cut width of the augmented graph, (2) the fracture number of the augmented graph, (3) the slim tree-cut width of the input graph.","The latter two results were previously known for EDP; our results generalize these to GSTP and improve the running time for the parameter fracture number.","On the other hand, it was open whether EDP is FPT parameterized by the tree-cut width of the augmented graph, despite extensive research on the structural complexity of the problem.","We settle this question affirmatively."],"url":"http://arxiv.org/abs/2505.09250v1"}
{"created":"2025-05-14 09:35:56","title":"Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases","abstract":"In many real-world settings, machine learning models and interactive systems have access to both structured knowledge, e.g., knowledge graphs or tables, and unstructured content, e.g., natural language documents. However, most rely on either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking unstructured content to nodes within structured data, thereby enabling new strategies for knowledge access and use. In this work, we present FocusedRetriever, a modular SKB-based framework for multi-hop question answering. It integrates components (VSS-based entity search, LLM-based generation of Cypher queries and pairwise re-ranking) in a way that enables it to outperform state-of-the-art methods across all three STaRK benchmark test sets, covering diverse domains and multiple performance metrics. The average first-hit rate exceeds that of the second-best method by 25.7%. FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to extract relational facts and entity attributes from unstructured text, (2) node set joins to filter answer candidates based on these extracted triplets and constraints, (3) vector similarity search to retrieve and rank relevant unstructured content, and (4) the contextual capabilities of LLMs to finally rank the top-k answers. For generality, we only incorporate base LLMs in FocusedRetriever in our evaluation. However, our analysis of intermediate results highlights several opportunities for further upgrades including finetuning. The source code is publicly available at https://github.com/kramerlab/FocusedRetriever .","sentences":["In many real-world settings, machine learning models and interactive systems have access to both structured knowledge, e.g., knowledge graphs or tables, and unstructured content, e.g., natural language documents.","However, most rely on either.","Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking unstructured content to nodes within structured data, thereby enabling new strategies for knowledge access and use.","In this work, we present FocusedRetriever, a modular SKB-based framework for multi-hop question answering.","It integrates components (VSS-based entity search, LLM-based generation of Cypher queries and pairwise re-ranking) in a way that enables it to outperform state-of-the-art methods across all three STaRK benchmark test sets, covering diverse domains and multiple performance metrics.","The average first-hit rate exceeds that of the second-best method by 25.7%.","FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to extract relational facts and entity attributes from unstructured text, (2) node set joins to filter answer candidates based on these extracted triplets and constraints, (3) vector similarity search to retrieve and rank relevant unstructured content, and (4) the contextual capabilities of LLMs to finally rank the top-k answers.","For generality, we only incorporate base LLMs in FocusedRetriever in our evaluation.","However, our analysis of intermediate results highlights several opportunities for further upgrades including finetuning.","The source code is publicly available at https://github.com/kramerlab/FocusedRetriever ."],"url":"http://arxiv.org/abs/2505.09246v1"}
{"created":"2025-05-14 09:25:21","title":"Approximate Cartesian Tree Matching with One Difference","abstract":"Cartesian tree pattern matching consists of finding all the factors of a text that have the same Cartesian tree than a given pattern. There already exist theoretical and practical solutions for the exact case. In this paper, we propose the first algorithms for solving approximate Cartesian tree pattern matching with one difference given a pattern of length m and a text of length n. We present a generic algorithm that find all the factors of the text that have the same Cartesian tree of the pattern with one difference, using different notions of differences. We show that this algorithm has a O(nM) worst-case complexity and that, for several random models, the algorithm has a linear average-case complexity. We also present an automaton based algorithm, adapting [PALP19], that can be generalized to deal with more than one difference.","sentences":["Cartesian tree pattern matching consists of finding all the factors of a text that have the same Cartesian tree than a given pattern.","There already exist theoretical and practical solutions for the exact case.","In this paper, we propose the first algorithms for solving approximate Cartesian tree pattern matching with one difference given a pattern of length m and a text of length n. We present a generic algorithm that find all the factors of the text that have the same Cartesian tree of the pattern with one difference, using different notions of differences.","We show that this algorithm has a O(nM) worst-case complexity and that, for several random models, the algorithm has a linear average-case complexity.","We also present an automaton based algorithm, adapting [PALP19], that can be generalized to deal with more than one difference."],"url":"http://arxiv.org/abs/2505.09236v1"}
{"created":"2025-05-14 09:16:19","title":"A Standardized Benchmark Set of Clustering Problem Instances for Comparing Black-Box Optimizers","abstract":"One key challenge in optimization is the selection of a suitable set of benchmark problems. A common goal is to find functions which are representative of a class of real-world optimization problems in order to ensure findings on the benchmarks will translate to relevant problem domains. While some problem characteristics are well-covered by popular benchmarking suites, others are often overlooked. One example of such a problem characteristic is permutation invariance, where the search space consists of a set of symmetrical search regions. This type of problem occurs e.g. when a set of solutions has to be found, but the ordering within this set does not matter. The data clustering problem, often seen in machine learning contexts, is a clear example of such an optimization landscape, and has thus been proposed as a base from which optimization benchmarks can be created. In addition to the symmetry aspect, these clustering problems also contain potential regions of neutrality, which can provide an additional challenge to optimization algorithms. In this paper, we present a standardized benchmark suite for the evaluation of continuous black-box optimization algorithms, based on data clustering problems. To gain insight into the diversity of the benchmark set, both internally and in comparison to existing suites, we perform a benchmarking study of a set of modular CMA-ES configurations, as well as an analysis using exploratory landscape analysis. Our benchmark set is open-source and integrated with the IOHprofiler benchmarking framework to encourage its use in future research.","sentences":["One key challenge in optimization is the selection of a suitable set of benchmark problems.","A common goal is to find functions which are representative of a class of real-world optimization problems in order to ensure findings on the benchmarks will translate to relevant problem domains.","While some problem characteristics are well-covered by popular benchmarking suites, others are often overlooked.","One example of such a problem characteristic is permutation invariance, where the search space consists of a set of symmetrical search regions.","This type of problem occurs e.g. when a set of solutions has to be found, but the ordering within this set does not matter.","The data clustering problem, often seen in machine learning contexts, is a clear example of such an optimization landscape, and has thus been proposed as a base from which optimization benchmarks can be created.","In addition to the symmetry aspect, these clustering problems also contain potential regions of neutrality, which can provide an additional challenge to optimization algorithms.","In this paper, we present a standardized benchmark suite for the evaluation of continuous black-box optimization algorithms, based on data clustering problems.","To gain insight into the diversity of the benchmark set, both internally and in comparison to existing suites, we perform a benchmarking study of a set of modular CMA-ES configurations, as well as an analysis using exploratory landscape analysis.","Our benchmark set is open-source and integrated with the IOHprofiler benchmarking framework to encourage its use in future research."],"url":"http://arxiv.org/abs/2505.09233v1"}
{"created":"2025-05-14 08:46:24","title":"Ethical Aspects of the Use of Social Robots in Elderly Care -- A Systematic Qualitative Review","abstract":"Background: The use of social robotics in elderly care is increasingly discussed as one way of meeting emerging care needs due to scarce resources. While many potential benefits are associated with robotic care technologies, there is a variety of ethical challenges. To support steps towards a responsible implementation and use, this review develops an overview on ethical aspects of the use of social robots in elderly care from a decision-makers' perspective.   Methods: Electronic databases were queried using a comprehensive search strategy based on the key concepts of \"ethical aspects\", \"social robotics\" and \"elderly care\". Abstract and title screening was conducted by two authors independently. Full-text screening was conducted by one author following a joint consolidation phase. Data was extracted using MAXQDA24 by one author, based on a consolidated coding framework. Analysis was performed through modified qualitative content analysis.   Results: A total of 1,518 publications were screened, and 248 publications were included. We have organized our analysis in a scheme of ethical hazards, ethical opportunities and unsettled questions, identifying at least 60 broad ethical aspects affecting three different stakeholder groups. While some ethical issues are well-known and broadly discussed our analysis shows a plethora of potentially relevant aspects, often only marginally recognized, that are worthy of consideration from a practical perspective.   Discussion: The findings highlight the need for a contextual and detailed evaluation of implementation scenarios. To make use of the vast knowledge of the ethical discourse, we hypothesize that decision-makers need to understand the specific nature of this discourse to be able to engage in careful ethical deliberation.","sentences":["Background: The use of social robotics in elderly care is increasingly discussed as one way of meeting emerging care needs due to scarce resources.","While many potential benefits are associated with robotic care technologies, there is a variety of ethical challenges.","To support steps towards a responsible implementation and use, this review develops an overview on ethical aspects of the use of social robots in elderly care from a decision-makers' perspective.   ","Methods: Electronic databases were queried using a comprehensive search strategy based on the key concepts of \"ethical aspects\", \"social robotics\" and \"elderly care\".","Abstract and title screening was conducted by two authors independently.","Full-text screening was conducted by one author following a joint consolidation phase.","Data was extracted using MAXQDA24 by one author, based on a consolidated coding framework.","Analysis was performed through modified qualitative content analysis.   ","Results: A total of 1,518 publications were screened, and 248 publications were included.","We have organized our analysis in a scheme of ethical hazards, ethical opportunities and unsettled questions, identifying at least 60 broad ethical aspects affecting three different stakeholder groups.","While some ethical issues are well-known and broadly discussed our analysis shows a plethora of potentially relevant aspects, often only marginally recognized, that are worthy of consideration from a practical perspective.   ","Discussion: The findings highlight the need for a contextual and detailed evaluation of implementation scenarios.","To make use of the vast knowledge of the ethical discourse, we hypothesize that decision-makers need to understand the specific nature of this discourse to be able to engage in careful ethical deliberation."],"url":"http://arxiv.org/abs/2505.09224v1"}
{"created":"2025-05-14 08:42:46","title":"Securing P4 Programs by Information Flow Control","abstract":"Software-Defined Networking (SDN) has transformed network architectures by decoupling the control and data-planes, enabling fine-grained control over packet processing and forwarding. P4, a language designed for programming data-plane devices, allows developers to define custom packet processing behaviors directly on programmable network devices. This provides greater control over packet forwarding, inspection, and modification. However, the increased flexibility provided by P4 also brings significant security challenges, particularly in managing sensitive data and preventing information leakage within the data-plane.   This paper presents a novel security type system for analyzing information flow in P4 programs that combines security types with interval analysis. The proposed type system allows the specification of security policies in terms of input and output packet bit fields rather than program variables. We formalize this type system and prove it sound, guaranteeing that well-typed programs satisfy noninterference. Our prototype implementation, Tap4s, is evaluated on several use cases, demonstrating its effectiveness in detecting security violations and information leakages.","sentences":["Software-Defined Networking (SDN) has transformed network architectures by decoupling the control and data-planes, enabling fine-grained control over packet processing and forwarding.","P4, a language designed for programming data-plane devices, allows developers to define custom packet processing behaviors directly on programmable network devices.","This provides greater control over packet forwarding, inspection, and modification.","However, the increased flexibility provided by P4 also brings significant security challenges, particularly in managing sensitive data and preventing information leakage within the data-plane.   ","This paper presents a novel security type system for analyzing information flow in P4 programs that combines security types with interval analysis.","The proposed type system allows the specification of security policies in terms of input and output packet bit fields rather than program variables.","We formalize this type system and prove it sound, guaranteeing that well-typed programs satisfy noninterference.","Our prototype implementation, Tap4s, is evaluated on several use cases, demonstrating its effectiveness in detecting security violations and information leakages."],"url":"http://arxiv.org/abs/2505.09221v1"}
{"created":"2025-05-14 07:52:54","title":"Educational impacts of generative artificial intelligence on learning and performance of engineering students in China","abstract":"With the rapid advancement of generative artificial intelligence(AI), its potential applications in higher education have attracted significant attention. This study investigated how 148 students from diverse engineering disciplines and regions across China used generative AI, focusing on its impact on their learning experience and the opportunities and challenges it poses in engineering education. Based on the surveyed data, we explored four key areas: the frequency and application scenarios of AI use among engineering students, its impact on students' learning and performance, commonly encountered challenges in using generative AI, and future prospects for its adoption in engineering education. The results showed that more than half of the participants reported a positive impact of generative AI on their learning efficiency, initiative, and creativity, with nearly half believing it also enhanced their independent thinking. However, despite acknowledging improved study efficiency, many felt their actual academic performance remained largely unchanged and expressed concerns about the accuracy and domain-specific reliability of generative AI. Our findings provide a first-hand insight into the current benefits and challenges generative AI brings to students, particularly Chinese engineering students, while offering several recommendations, especially from the students' perspective, for effectively integrating generative AI into engineering education.","sentences":["With the rapid advancement of generative artificial intelligence(AI), its potential applications in higher education have attracted significant attention.","This study investigated how 148 students from diverse engineering disciplines and regions across China used generative AI, focusing on its impact on their learning experience and the opportunities and challenges it poses in engineering education.","Based on the surveyed data, we explored four key areas: the frequency and application scenarios of AI use among engineering students, its impact on students' learning and performance, commonly encountered challenges in using generative AI, and future prospects for its adoption in engineering education.","The results showed that more than half of the participants reported a positive impact of generative AI on their learning efficiency, initiative, and creativity, with nearly half believing it also enhanced their independent thinking.","However, despite acknowledging improved study efficiency, many felt their actual academic performance remained largely unchanged and expressed concerns about the accuracy and domain-specific reliability of generative AI.","Our findings provide a first-hand insight into the current benefits and challenges generative AI brings to students, particularly Chinese engineering students, while offering several recommendations, especially from the students' perspective, for effectively integrating generative AI into engineering education."],"url":"http://arxiv.org/abs/2505.09208v1"}
{"created":"2025-05-14 07:34:36","title":"HMamba: Hyperbolic Mamba for Sequential Recommendation","abstract":"Sequential recommendation systems have become a cornerstone of personalized services, adept at modeling the temporal evolution of user preferences by capturing dynamic interaction sequences. Existing approaches predominantly rely on traditional models, including RNNs and Transformers. Despite their success in local pattern recognition, Transformer-based methods suffer from quadratic computational complexity and a tendency toward superficial attention patterns, limiting their ability to infer enduring preference hierarchies in sequential recommendation data. Recent advances in Mamba-based sequential models introduce linear-time efficiency but remain constrained by Euclidean geometry, failing to leverage the intrinsic hyperbolic structure of recommendation data. To bridge this gap, we propose Hyperbolic Mamba, a novel architecture that unifies the efficiency of Mamba's selective state space mechanism with hyperbolic geometry's hierarchical representational power. Our framework introduces (1) a hyperbolic selective state space that maintains curvature-aware sequence modeling and (2) stabilized Riemannian operations to enable scalable training. Experiments across four benchmarks demonstrate that Hyperbolic Mamba achieves 3-11% improvement while retaining Mamba's linear-time efficiency, enabling real-world deployment. This work establishes a new paradigm for efficient, hierarchy-aware sequential modeling.","sentences":["Sequential recommendation systems have become a cornerstone of personalized services, adept at modeling the temporal evolution of user preferences by capturing dynamic interaction sequences.","Existing approaches predominantly rely on traditional models, including RNNs and Transformers.","Despite their success in local pattern recognition, Transformer-based methods suffer from quadratic computational complexity and a tendency toward superficial attention patterns, limiting their ability to infer enduring preference hierarchies in sequential recommendation data.","Recent advances in Mamba-based sequential models introduce linear-time efficiency but remain constrained by Euclidean geometry, failing to leverage the intrinsic hyperbolic structure of recommendation data.","To bridge this gap, we propose Hyperbolic Mamba, a novel architecture that unifies the efficiency of Mamba's selective state space mechanism with hyperbolic geometry's hierarchical representational power.","Our framework introduces (1) a hyperbolic selective state space that maintains curvature-aware sequence modeling and (2) stabilized Riemannian operations to enable scalable training.","Experiments across four benchmarks demonstrate that Hyperbolic Mamba achieves 3-11% improvement while retaining Mamba's linear-time efficiency, enabling real-world deployment.","This work establishes a new paradigm for efficient, hierarchy-aware sequential modeling."],"url":"http://arxiv.org/abs/2505.09205v1"}
{"created":"2025-05-14 07:21:17","title":"SHACL-DS: A SHACL extension to validate RDF dataset","abstract":"The Shapes Constraint Language (SHACL) provides a powerful mechanism for validating RDF data against shape constraints, but is inherently designed for single-graph validation. This limitation makes SHACL unsuitable for natively validating RDF datasets comprising multiple named graphs. To address this gap, developers must build solutions on top of SHACL, applying a shapes graph to each RDF dataset or combinations thereof using bespoke code. However, these approaches may lead to information loss, such as the named graph from which the data originates. This paper introduces SHACL-DS, an extension to SHACL that enables validation of RDF datasets. The extension adds a layer on top of SHACL, and the only disruptive change is the execution of SPARQL queries in, e.g., SPARQL-based constraints. The contributions are a SHACL-DS specification, a prototype implementation, and a set of test cases illustrating its use and providing future developers guidance in building SHACL-DS engines. This work lays the foundation for integrating dataset-level features into SHACL and encourages further exploration of advanced RDF dataset validation techniques.","sentences":["The Shapes Constraint Language (SHACL) provides a powerful mechanism for validating RDF data against shape constraints, but is inherently designed for single-graph validation.","This limitation makes SHACL unsuitable for natively validating RDF datasets comprising multiple named graphs.","To address this gap, developers must build solutions on top of SHACL, applying a shapes graph to each RDF dataset or combinations thereof using bespoke code.","However, these approaches may lead to information loss, such as the named graph from which the data originates.","This paper introduces SHACL-DS, an extension to SHACL that enables validation of RDF datasets.","The extension adds a layer on top of SHACL, and the only disruptive change is the execution of SPARQL queries in, e.g., SPARQL-based constraints.","The contributions are a SHACL-DS specification, a prototype implementation, and a set of test cases illustrating its use and providing future developers guidance in building SHACL-DS engines.","This work lays the foundation for integrating dataset-level features into SHACL and encourages further exploration of advanced RDF dataset validation techniques."],"url":"http://arxiv.org/abs/2505.09198v1"}
{"created":"2025-05-14 06:39:01","title":"Zero-shot Quantization: A Comprehensive Survey","abstract":"Network quantization has proven to be a powerful approach to reduce the memory and computational demands of deep learning models for deployment on resource-constrained devices. However, traditional quantization methods often rely on access to training data, which is impractical in many real-world scenarios due to privacy, security, or regulatory constraints. Zero-shot Quantization (ZSQ) emerges as a promising solution, achieving quantization without requiring any real data. In this paper, we provide a comprehensive overview of ZSQ methods and their recent advancements. First, we provide a formal definition of the ZSQ problem and highlight the key challenges. Then, we categorize the existing ZSQ methods into classes based on data generation strategies, and analyze their motivations, core ideas, and key takeaways. Lastly, we suggest future research directions to address the remaining limitations and advance the field of ZSQ. To the best of our knowledge, this paper is the first in-depth survey on ZSQ.","sentences":["Network quantization has proven to be a powerful approach to reduce the memory and computational demands of deep learning models for deployment on resource-constrained devices.","However, traditional quantization methods often rely on access to training data, which is impractical in many real-world scenarios due to privacy, security, or regulatory constraints.","Zero-shot Quantization (ZSQ) emerges as a promising solution, achieving quantization without requiring any real data.","In this paper, we provide a comprehensive overview of ZSQ methods and their recent advancements.","First, we provide a formal definition of the ZSQ problem and highlight the key challenges.","Then, we categorize the existing ZSQ methods into classes based on data generation strategies, and analyze their motivations, core ideas, and key takeaways.","Lastly, we suggest future research directions to address the remaining limitations and advance the field of ZSQ.","To the best of our knowledge, this paper is the first in-depth survey on ZSQ."],"url":"http://arxiv.org/abs/2505.09188v1"}
{"created":"2025-05-14 06:13:23","title":"Optimizing Urban Critical Green Space Development Using Machine Learning","abstract":"This paper presents a novel framework for prioritizing urban green space development in Tehran using diverse socio-economic, environmental, and sensitivity indices. The indices were derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the Weather Research & Forecasting (WRF) model. The WRF model was used to estimate the air temperature at a 1 km resolution due to insufficient meteorological stations, yielding RMSE and MAE values of 0.96{\\deg}C and 0.92{\\deg}C, respectively. After data preparation, several machine learning models were used for binary vegetation cover classification including XGBoost, LightGBM, Random Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94% in Overall Accuracy, Recall, and F1-score. Then, the probability of areas lacking vegetation cover was assessed using socio-economic, environmental and sensitivity indices. This resulted in the RF generating an urban green space development prioritization map. Feature Importance Analysis revealed that the most significant indices were nightly land surface temperature (LST) and sensitive population. Finally, the framework performance was validated through microclimate simulation to assess the critical areas after and before the green space development by green roofs. The simulation demonstrated reducing air temperature by up to 0.67{\\deg}C after utilizing the green roof technology in critical areas. As a result, this framework provides a valuable tool for urban planners to develop green spaces.","sentences":["This paper presents a novel framework for prioritizing urban green space development in Tehran using diverse socio-economic, environmental, and sensitivity indices.","The indices were derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the Weather Research & Forecasting (WRF) model.","The WRF model was used to estimate the air temperature at a 1 km resolution due to insufficient meteorological stations, yielding RMSE and MAE values of 0.96{\\deg}C and 0.92{\\deg}C, respectively.","After data preparation, several machine learning models were used for binary vegetation cover classification including XGBoost, LightGBM, Random Forest (RF) and Extra Trees.","RF achieved the highest performance, exceeding 94% in Overall Accuracy, Recall, and F1-score.","Then, the probability of areas lacking vegetation cover was assessed using socio-economic, environmental and sensitivity indices.","This resulted in the RF generating an urban green space development prioritization map.","Feature Importance Analysis revealed that the most significant indices were nightly land surface temperature (LST) and sensitive population.","Finally, the framework performance was validated through microclimate simulation to assess the critical areas after and before the green space development by green roofs.","The simulation demonstrated reducing air temperature by up to 0.67{\\deg}C after utilizing the green roof technology in critical areas.","As a result, this framework provides a valuable tool for urban planners to develop green spaces."],"url":"http://arxiv.org/abs/2505.09175v1"}
{"created":"2025-05-14 06:13:14","title":"Quotient Complex Transformer (QCformer) for Perovskite Data Analysis","abstract":"The discovery of novel functional materials is crucial in addressing the challenges of sustainable energy generation and climate change. Hybrid organic-inorganic perovskites (HOIPs) have gained attention for their exceptional optoelectronic properties in photovoltaics. Recently, geometric deep learning, particularly graph neural networks (GNNs), has shown strong potential in predicting material properties and guiding material design. However, traditional GNNs often struggle to capture the periodic structures and higher-order interactions prevalent in such systems. To address these limitations, we propose a novel representation based on quotient complexes (QCs) and introduce the Quotient Complex Transformer (QCformer) for material property prediction. A material structure is modeled as a quotient complex, which encodes both pairwise and many-body interactions via simplices of varying dimensions and captures material periodicity through a quotient operation. Our model leverages higher-order features defined on simplices and processes them using a simplex-based Transformer module. We pretrain QCformer on benchmark datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP datasets. The results show that QCformer outperforms state-of-the-art models in HOIP property prediction, demonstrating its effectiveness. The quotient complex representation and QCformer model together contribute a powerful new tool for predictive modeling of perovskite materials.","sentences":["The discovery of novel functional materials is crucial in addressing the challenges of sustainable energy generation and climate change.","Hybrid organic-inorganic perovskites (HOIPs) have gained attention for their exceptional optoelectronic properties in photovoltaics.","Recently, geometric deep learning, particularly graph neural networks (GNNs), has shown strong potential in predicting material properties and guiding material design.","However, traditional GNNs often struggle to capture the periodic structures and higher-order interactions prevalent in such systems.","To address these limitations, we propose a novel representation based on quotient complexes (QCs) and introduce the Quotient Complex Transformer (QCformer) for material property prediction.","A material structure is modeled as a quotient complex, which encodes both pairwise and many-body interactions via simplices of varying dimensions and captures material periodicity through a quotient operation.","Our model leverages higher-order features defined on simplices and processes them using a simplex-based Transformer module.","We pretrain QCformer on benchmark datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP datasets.","The results show that QCformer outperforms state-of-the-art models in HOIP property prediction, demonstrating its effectiveness.","The quotient complex representation and QCformer model together contribute a powerful new tool for predictive modeling of perovskite materials."],"url":"http://arxiv.org/abs/2505.09174v1"}
{"created":"2025-05-14 05:45:22","title":"A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning","abstract":"Current applications of self-supervised learning to wireless channel representation often borrow paradigms developed for text and image processing, without fully addressing the unique characteristics and constraints of wireless communications. Aiming to fill this gap, we first propose WiMAE (Wireless Masked Autoencoder), a transformer-based encoder-decoder foundation model pretrained on a realistic open-source multi-antenna wireless channel dataset. Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by incorporating a contrastive learning objective alongside the reconstruction task in a unified multi-task framework. By warm-starting from pretrained WiMAE weights and generating positive pairs via noise injection, the contrastive component enables the model to capture both structural and discriminative features, enhancing representation quality beyond what reconstruction alone can achieve. Through extensive evaluation on unseen scenarios, we demonstrate the effectiveness of both approaches across multiple downstream tasks, with ContraWiMAE showing further improvements in linear separability and adaptability in diverse wireless environments. Comparative evaluations against a state-of-the-art wireless channel foundation model confirm the superior performance and data efficiency of our models, highlighting their potential as powerful baselines for future research in self-supervised wireless channel representation learning.","sentences":["Current applications of self-supervised learning to wireless channel representation often borrow paradigms developed for text and image processing, without fully addressing the unique characteristics and constraints of wireless communications.","Aiming to fill this gap, we first propose WiMAE (Wireless Masked Autoencoder), a transformer-based encoder-decoder foundation model pretrained on a realistic open-source multi-antenna wireless channel dataset.","Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by incorporating a contrastive learning objective alongside the reconstruction task in a unified multi-task framework.","By warm-starting from pretrained WiMAE weights and generating positive pairs via noise injection, the contrastive component enables the model to capture both structural and discriminative features, enhancing representation quality beyond what reconstruction alone can achieve.","Through extensive evaluation on unseen scenarios, we demonstrate the effectiveness of both approaches across multiple downstream tasks, with ContraWiMAE showing further improvements in linear separability and adaptability in diverse wireless environments.","Comparative evaluations against a state-of-the-art wireless channel foundation model confirm the superior performance and data efficiency of our models, highlighting their potential as powerful baselines for future research in self-supervised wireless channel representation learning."],"url":"http://arxiv.org/abs/2505.09160v1"}
{"created":"2025-05-14 05:32:55","title":"AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection","abstract":"Current multimodal large language models (MLLMs) struggle to understand circuit schematics due to their limited recognition capabilities. This could be attributed to the lack of high-quality schematic-netlist training data. Existing work such as AMSnet applies schematic parsing to generate netlists. However, these methods rely on hard-coded heuristics and are difficult to apply to complex or noisy schematics in this paper. We therefore propose a novel net detection mechanism based on segmentation with high robustness. The proposed method also recovers positional information, allowing digital reconstruction of schematics. We then expand AMSnet dataset with schematic images from various sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information for circuit components and nets, whereas AMSnet only includes 792 circuits with SPICE netlists but no digital schematics.","sentences":["Current multimodal large language models (MLLMs) struggle to understand circuit schematics due to their limited recognition capabilities.","This could be attributed to the lack of high-quality schematic-netlist training data.","Existing work such as AMSnet applies schematic parsing to generate netlists.","However, these methods rely on hard-coded heuristics and are difficult to apply to complex or noisy schematics in this paper.","We therefore propose a novel net detection mechanism based on segmentation with high robustness.","The proposed method also recovers positional information, allowing digital reconstruction of schematics.","We then expand AMSnet dataset with schematic images from various sources and create AMSnet 2.0.","AMSnet 2.0 contains 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information for circuit components and nets, whereas AMSnet only includes 792 circuits with SPICE netlists but no digital schematics."],"url":"http://arxiv.org/abs/2505.09155v1"}
{"created":"2025-05-14 05:04:31","title":"Robot-Assisted Drone Recovery on a Wavy Surface Using Error-State Kalman Filter and Receding Horizon Model Predictive Control","abstract":"Recovering a drone on a disturbed water surface remains a significant challenge in maritime robotics. In this paper, we propose a unified framework for Robot-Assisted Drone Recovery on a Wavy Surface that addresses two major tasks: Firstly, accurate prediction of a moving drone's position under wave-induced disturbances using an Error-State Kalman Filter (ESKF), and secondly, effective motion planning for a manipulator via Receding Horizon Control (RHC). Specifically, the ESKF predicts the drone's future position 0.5s ahead, while the manipulator plans a capture trajectory in real time, thus overcoming not only wave-induced base motions but also limited torque constraints. We provide a system design that comprises a manipulator subsystem and a UAV subsystem. On the UAV side, we detail how position control and suspended payload strategies are implemented. On the manipulator side, we show how an RHC scheme outperforms traditional low-level control algorithms. Simulation and real-world experiments - using wave-disturbed motion data - demonstrate that our approach achieves a high success rate - above 95% and outperforms conventional baseline methods by up to 10% in efficiency and 20% in precision. The results underscore the feasibility and robustness of our system, which achieves state-of-the-art (SOTA) performance and offers a practical solution for maritime drone operations.","sentences":["Recovering a drone on a disturbed water surface remains a significant challenge in maritime robotics.","In this paper, we propose a unified framework for Robot-Assisted Drone Recovery on a Wavy Surface that addresses two major tasks: Firstly, accurate prediction of a moving drone's position under wave-induced disturbances using an Error-State Kalman Filter (ESKF), and secondly, effective motion planning for a manipulator via Receding Horizon Control (RHC).","Specifically, the ESKF predicts the drone's future position 0.5s ahead, while the manipulator plans a capture trajectory in real time, thus overcoming not only wave-induced base motions but also limited torque constraints.","We provide a system design that comprises a manipulator subsystem and a UAV subsystem.","On the UAV side, we detail how position control and suspended payload strategies are implemented.","On the manipulator side, we show how an RHC scheme outperforms traditional low-level control algorithms.","Simulation and real-world experiments - using wave-disturbed motion data - demonstrate that our approach achieves a high success rate - above 95% and outperforms conventional baseline methods by up to 10% in efficiency and 20% in precision.","The results underscore the feasibility and robustness of our system, which achieves state-of-the-art (SOTA) performance and offers a practical solution for maritime drone operations."],"url":"http://arxiv.org/abs/2505.09145v1"}
{"created":"2025-05-14 05:03:09","title":"Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation","abstract":"We present Latent Theory of Mind (LatentToM), a decentralized diffusion policy architecture for collaborative robot manipulation. Our policy allows multiple manipulators with their own perception and computation to collaborate with each other towards a common task goal with or without explicit communication. Our key innovation lies in allowing each agent to maintain two latent representations: an ego embedding specific to the robot, and a consensus embedding trained to be common to both robots, despite their different sensor streams and poses. We further let each robot train a decoder to infer the other robot's ego embedding from their consensus embedding, akin to theory of mind in latent space. Training occurs centrally, with all the policies' consensus encoders supervised by a loss inspired by sheaf theory, a mathematical theory for clustering data on a topological manifold. Specifically, we introduce a first-order cohomology loss to enforce sheaf-consistent alignment of the consensus embeddings. To preserve the expressiveness of the consensus embedding, we further propose structural constraints based on theory of mind and a directional consensus mechanism. Execution can be fully distributed, requiring no explicit communication between policies. In which case, the information is exchanged implicitly through each robot's sensor stream by observing the actions of the other robots and their effects on the scene. Alternatively, execution can leverage direct communication to share the robots' consensus embeddings, where the embeddings are shared once during each inference step and are aligned using the sheaf Laplacian. In our hardware experiments, LatentToM outperforms a naive decentralized diffusion baseline, and shows comparable performance with a state-of-the-art centralized diffusion policy for bi-manual manipulation. Project website: https://stanfordmsl.github.io/LatentToM/.","sentences":["We present Latent Theory of Mind (LatentToM), a decentralized diffusion policy architecture for collaborative robot manipulation.","Our policy allows multiple manipulators with their own perception and computation to collaborate with each other towards a common task goal with or without explicit communication.","Our key innovation lies in allowing each agent to maintain two latent representations: an ego embedding specific to the robot, and a consensus embedding trained to be common to both robots, despite their different sensor streams and poses.","We further let each robot train a decoder to infer the other robot's ego embedding from their consensus embedding, akin to theory of mind in latent space.","Training occurs centrally, with all the policies' consensus encoders supervised by a loss inspired by sheaf theory, a mathematical theory for clustering data on a topological manifold.","Specifically, we introduce a first-order cohomology loss to enforce sheaf-consistent alignment of the consensus embeddings.","To preserve the expressiveness of the consensus embedding, we further propose structural constraints based on theory of mind and a directional consensus mechanism.","Execution can be fully distributed, requiring no explicit communication between policies.","In which case, the information is exchanged implicitly through each robot's sensor stream by observing the actions of the other robots and their effects on the scene.","Alternatively, execution can leverage direct communication to share the robots' consensus embeddings, where the embeddings are shared once during each inference step and are aligned using the sheaf Laplacian.","In our hardware experiments, LatentToM outperforms a naive decentralized diffusion baseline, and shows comparable performance with a state-of-the-art centralized diffusion policy for bi-manual manipulation.","Project website: https://stanfordmsl.github.io/LatentToM/."],"url":"http://arxiv.org/abs/2505.09144v1"}
{"created":"2025-05-14 04:43:36","title":"Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models","abstract":"Vision-language models (VLMs) offer flexible object detection through natural language prompts but suffer from performance variability depending on prompt phrasing. In this paper, we introduce a method for automated prompt refinement using a novel metric called the Contrastive Class Alignment Score (CCAS), which ranks prompts based on their semantic alignment with a target object class while penalizing similarity to confounding classes. Our method generates diverse prompt candidates via a large language model and filters them through CCAS, computed using prompt embeddings from a sentence transformer. We evaluate our approach on challenging object categories, demonstrating that our automatic selection of high-precision prompts improves object detection accuracy without the need for additional model training or labeled data. This scalable and model-agnostic pipeline offers a principled alternative to manual prompt engineering for VLM-based detection systems.","sentences":["Vision-language models (VLMs) offer flexible object detection through natural language prompts but suffer from performance variability depending on prompt phrasing.","In this paper, we introduce a method for automated prompt refinement using a novel metric called the Contrastive Class Alignment Score (CCAS), which ranks prompts based on their semantic alignment with a target object class while penalizing similarity to confounding classes.","Our method generates diverse prompt candidates via a large language model and filters them through CCAS, computed using prompt embeddings from a sentence transformer.","We evaluate our approach on challenging object categories, demonstrating that our automatic selection of high-precision prompts improves object detection accuracy without the need for additional model training or labeled data.","This scalable and model-agnostic pipeline offers a principled alternative to manual prompt engineering for VLM-based detection systems."],"url":"http://arxiv.org/abs/2505.09139v1"}
{"created":"2025-05-14 04:35:26","title":"Scaling Gaussian Process Regression with Full Derivative Observations","abstract":"We present a scalable Gaussian Process (GP) method that can fit and predict full derivative observations called DSoftKI. It extends SoftKI, a method that approximates a kernel via softmax interpolation from learned interpolation point locations, to the setting with derivatives. DSoftKI enhances SoftKI's interpolation scheme to incorporate the directional orientation of interpolation points relative to the data. This enables the construction of a scalable approximate kernel, including its first and second-order derivatives, through interpolation. We evaluate DSoftKI on a synthetic function benchmark and high-dimensional molecular force field prediction (100-1000 dimensions), demonstrating that DSoftKI is accurate and can scale to larger datasets with full derivative observations than previously possible.","sentences":["We present a scalable Gaussian Process (GP) method that can fit and predict full derivative observations called DSoftKI.","It extends SoftKI, a method that approximates a kernel via softmax interpolation from learned interpolation point locations, to the setting with derivatives.","DSoftKI enhances SoftKI's interpolation scheme to incorporate the directional orientation of interpolation points relative to the data.","This enables the construction of a scalable approximate kernel, including its first and second-order derivatives, through interpolation.","We evaluate DSoftKI on a synthetic function benchmark and high-dimensional molecular force field prediction (100-1000 dimensions), demonstrating that DSoftKI is accurate and can scale to larger datasets with full derivative observations than previously possible."],"url":"http://arxiv.org/abs/2505.09134v1"}
{"created":"2025-05-14 04:29:09","title":"Fair Clustering via Alignment","abstract":"Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute. While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice. To resolve these limitations, we propose a new fair clustering algorithm based on a novel decomposition of the fair K-means clustering objective function. The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space. A key advantage of FCA is that it theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice. Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.","sentences":["Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute.","While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice.","To resolve these limitations, we propose a new fair clustering algorithm based on a novel decomposition of the fair K-means clustering objective function.","The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space.","A key advantage of FCA is that it theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice.","Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability."],"url":"http://arxiv.org/abs/2505.09131v1"}
{"created":"2025-05-14 04:24:37","title":"WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes","abstract":"The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.","sentences":["The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges.","In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions.","The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames.","The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data.","The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value.","The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future."],"url":"http://arxiv.org/abs/2505.09129v1"}
{"created":"2025-05-14 04:09:28","title":"Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance","abstract":"Big model has emerged as a new research paradigm that can be applied to various down-stream tasks with only minor effort for domain adaption. Correspondingly, this study tackles Camouflaged Object Detection (COD) leveraging the Segment Anything Model (SAM). The previous studies declared that SAM is not workable for COD but this study reveals that SAM works if promoted properly, for which we devise a new framework to render point promotions: First, we develop the Promotion Point Targeting Network (PPT-net) to leverage multi-scale features in predicting the probabilities of camouflaged objects' presences at given candidate points over the image. Then, we develop a key point selection (KPS) algorithm to deploy both positive and negative point promotions contrastively to SAM to guide the segmentation. It is the first work to facilitate big model for COD and achieves plausible results experimentally over the existing methods on 3 data sets under 6 metrics. This study demonstrates an off-the-shelf methodology for COD by leveraging SAM, which gains advantage over designing professional models from scratch, not only in performance, but also in turning the problem to a less challenging task, that is, seeking informative but not exactly precise promotions.","sentences":["Big model has emerged as a new research paradigm that can be applied to various down-stream tasks with only minor effort for domain adaption.","Correspondingly, this study tackles Camouflaged Object Detection (COD) leveraging the Segment Anything Model (SAM).","The previous studies declared that SAM is not workable for COD but this study reveals that SAM works if promoted properly, for which we devise a new framework to render point promotions:","First, we develop the Promotion Point Targeting Network (PPT-net) to leverage multi-scale features in predicting the probabilities of camouflaged objects' presences at given candidate points over the image.","Then, we develop a key point selection (KPS) algorithm to deploy both positive and negative point promotions contrastively to SAM to guide the segmentation.","It is the first work to facilitate big model for COD and achieves plausible results experimentally over the existing methods on 3 data sets under 6 metrics.","This study demonstrates an off-the-shelf methodology for COD by leveraging SAM, which gains advantage over designing professional models from scratch, not only in performance, but also in turning the problem to a less challenging task, that is, seeking informative but not exactly precise promotions."],"url":"http://arxiv.org/abs/2505.09123v1"}
{"created":"2025-05-14 03:45:16","title":"Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer","abstract":"Decision Transformers (DT) play a crucial role in modern reinforcement learning, leveraging offline datasets to achieve impressive results across various domains. However, DT requires high-quality, comprehensive data to perform optimally. In real-world applications, the lack of training data and the scarcity of optimal behaviours make training on offline datasets challenging, as suboptimal data can hinder performance. To address this, we propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel framework inspired by counterfactual reasoning. CRDT enhances DT ability to reason beyond known data by generating and utilizing counterfactual experiences, enabling improved decision-making in unseen scenarios. Experiments across Atari and D4RL benchmarks, including scenarios with limited data and altered dynamics, demonstrate that CRDT outperforms conventional DT approaches. Additionally, reasoning counterfactually allows the DT agent to obtain stitching abilities, combining suboptimal trajectories, without architectural modifications. These results highlight the potential of counterfactual reasoning to enhance reinforcement learning agents' performance and generalization capabilities.","sentences":["Decision Transformers (DT) play a crucial role in modern reinforcement learning, leveraging offline datasets to achieve impressive results across various domains.","However, DT requires high-quality, comprehensive data to perform optimally.","In real-world applications, the lack of training data and the scarcity of optimal behaviours make training on offline datasets challenging, as suboptimal data can hinder performance.","To address this, we propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel framework inspired by counterfactual reasoning.","CRDT enhances DT ability to reason beyond known data by generating and utilizing counterfactual experiences, enabling improved decision-making in unseen scenarios.","Experiments across Atari and D4RL benchmarks, including scenarios with limited data and altered dynamics, demonstrate that CRDT outperforms conventional DT approaches.","Additionally, reasoning counterfactually allows the DT agent to obtain stitching abilities, combining suboptimal trajectories, without architectural modifications.","These results highlight the potential of counterfactual reasoning to enhance reinforcement learning agents' performance and generalization capabilities."],"url":"http://arxiv.org/abs/2505.09114v1"}
{"created":"2025-05-14 03:36:36","title":"Toward Malicious Clients Detection in Federated Learning","abstract":"Federated learning (FL) enables multiple clients to collaboratively train a global machine learning model without sharing their raw data. However, the decentralized nature of FL introduces vulnerabilities, particularly to poisoning attacks, where malicious clients manipulate their local models to disrupt the training process. While Byzantine-robust aggregation rules have been developed to mitigate such attacks, they remain inadequate against more advanced threats. In response, recent advancements have focused on FL detection techniques to identify potentially malicious participants. Unfortunately, these methods often misclassify numerous benign clients as threats or rely on unrealistic assumptions about the server's capabilities. In this paper, we propose a novel algorithm, SafeFL, specifically designed to accurately identify malicious clients in FL. The SafeFL approach involves the server collecting a series of global models to generate a synthetic dataset, which is then used to distinguish between malicious and benign models based on their behavior. Extensive testing demonstrates that SafeFL outperforms existing methods, offering superior efficiency and accuracy in detecting malicious clients.","sentences":["Federated learning (FL) enables multiple clients to collaboratively train a global machine learning model without sharing their raw data.","However, the decentralized nature of FL introduces vulnerabilities, particularly to poisoning attacks, where malicious clients manipulate their local models to disrupt the training process.","While Byzantine-robust aggregation rules have been developed to mitigate such attacks, they remain inadequate against more advanced threats.","In response, recent advancements have focused on FL detection techniques to identify potentially malicious participants.","Unfortunately, these methods often misclassify numerous benign clients as threats or rely on unrealistic assumptions about the server's capabilities.","In this paper, we propose a novel algorithm, SafeFL, specifically designed to accurately identify malicious clients in FL.","The SafeFL approach involves the server collecting a series of global models to generate a synthetic dataset, which is then used to distinguish between malicious and benign models based on their behavior.","Extensive testing demonstrates that SafeFL outperforms existing methods, offering superior efficiency and accuracy in detecting malicious clients."],"url":"http://arxiv.org/abs/2505.09110v1"}
{"created":"2025-05-14 03:34:30","title":"FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis","abstract":"Due to the deformability of garments, generating a large amount of high-quality data for robotic garment manipulation tasks is highly challenging. In this paper, we present a synthetic garment dataset that can be used for robotic garment folding. We begin by constructing geometric garment templates based on keypoints and applying generative models to generate realistic texture patterns. Leveraging these keypoint annotations, we generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning. To improve robustness, we propose KG-DAgger, which uses a keypoint-based strategy to generate demonstration data for recovering from failures. KG-DAgger significantly improves the model performance, boosting the real-world success rate by 25\\%. After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75\\% success rate in the real world. Experiments in both simulation and real-world settings validate the effectiveness of our proposed framework.","sentences":["Due to the deformability of garments, generating a large amount of high-quality data for robotic garment manipulation tasks is highly challenging.","In this paper, we present a synthetic garment dataset that can be used for robotic garment folding.","We begin by constructing geometric garment templates based on keypoints and applying generative models to generate realistic texture patterns.","Leveraging these keypoint annotations, we generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning.","To improve robustness, we propose KG-DAgger, which uses a keypoint-based strategy to generate demonstration data for recovering from failures.","KG-DAgger significantly improves the model performance, boosting the real-world success rate by 25\\%.","After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75\\% success rate in the real world.","Experiments in both simulation and real-world settings validate the effectiveness of our proposed framework."],"url":"http://arxiv.org/abs/2505.09109v1"}
{"created":"2025-05-14 03:09:21","title":"Imitation Learning for Adaptive Control of a Virtual Soft Exoglove","abstract":"The use of wearable robots has been widely adopted in rehabilitation training for patients with hand motor impairments. However, the uniqueness of patients' muscle loss is often overlooked. Leveraging reinforcement learning and a biologically accurate musculoskeletal model in simulation, we propose a customized wearable robotic controller that is able to address specific muscle deficits and to provide compensation for hand-object manipulation tasks. Video data of a same subject performing human grasping tasks is used to train a manipulation model through learning from demonstration. This manipulation model is subsequently fine-tuned to perform object-specific interaction tasks. The muscle forces in the musculoskeletal manipulation model are then weakened to simulate neurological motor impairments, which are later compensated by the actuation of a virtual wearable robotics glove. Results shows that integrating the virtual wearable robotic glove provides shared assistance to support the hand manipulator with weakened muscle forces. The learned exoglove controller achieved an average of 90.5\\% of the original manipulation proficiency.","sentences":["The use of wearable robots has been widely adopted in rehabilitation training for patients with hand motor impairments.","However, the uniqueness of patients' muscle loss is often overlooked.","Leveraging reinforcement learning and a biologically accurate musculoskeletal model in simulation, we propose a customized wearable robotic controller that is able to address specific muscle deficits and to provide compensation for hand-object manipulation tasks.","Video data of a same subject performing human grasping tasks is used to train a manipulation model through learning from demonstration.","This manipulation model is subsequently fine-tuned to perform object-specific interaction tasks.","The muscle forces in the musculoskeletal manipulation model are then weakened to simulate neurological motor impairments, which are later compensated by the actuation of a virtual wearable robotics glove.","Results shows that integrating the virtual wearable robotic glove provides shared assistance to support the hand manipulator with weakened muscle forces.","The learned exoglove controller achieved an average of 90.5\\% of the original manipulation proficiency."],"url":"http://arxiv.org/abs/2505.09099v1"}
{"created":"2025-05-14 02:53:50","title":"OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions","abstract":"Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its real-world performance remains underexplored due to proprietary systems and limited data access. This paper presents OpenLKA, the first open, large-scale dataset for LKA evaluation and improvement. It includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing in Tampa, Florida and global contributions from the Comma.ai driving community. The dataset spans a wide range of challenging scenarios, including complex road geometries, degraded lane markings, adverse weather, lighting conditions and surrounding traffic. The dataset is multimodal, comprising: i) full CAN bus streams, decoded using custom reverse-engineered DBC files to extract key LKA events (e.g., system disengagements, lane detection failures); ii) synchronized high-resolution dash-cam video; iii) real-time outputs from Openpilot, providing accurate estimates of road curvature and lane positioning; iv) enhanced scene annotations generated by Vision Language Models, describing lane visibility, pavement quality, weather, lighting, and traffic conditions. By integrating vehicle-internal signals with high-fidelity perception and rich semantic context, OpenLKA provides a comprehensive platform for benchmarking the real-world performance of production LKA systems, identifying safety-critical operational scenarios, and assessing the readiness of current road infrastructure for autonomous driving. The dataset is publicly available at: https://github.com/OpenLKA/OpenLKA.","sentences":["Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its real-world performance remains underexplored due to proprietary systems and limited data access.","This paper presents OpenLKA, the first open, large-scale dataset for LKA evaluation and improvement.","It includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing in Tampa, Florida and global contributions from the Comma.ai driving community.","The dataset spans a wide range of challenging scenarios, including complex road geometries, degraded lane markings, adverse weather, lighting conditions and surrounding traffic.","The dataset is multimodal, comprising: i) full CAN bus streams, decoded using custom reverse-engineered DBC files to extract key LKA events (e.g., system disengagements, lane detection failures); ii) synchronized high-resolution dash-cam video; iii) real-time outputs from Openpilot, providing accurate estimates of road curvature and lane positioning; iv) enhanced scene annotations generated by Vision Language Models, describing lane visibility, pavement quality, weather, lighting, and traffic conditions.","By integrating vehicle-internal signals with high-fidelity perception and rich semantic context, OpenLKA provides a comprehensive platform for benchmarking the real-world performance of production LKA systems, identifying safety-critical operational scenarios, and assessing the readiness of current road infrastructure for autonomous driving.","The dataset is publicly available at: https://github.com/OpenLKA/OpenLKA."],"url":"http://arxiv.org/abs/2505.09092v1"}
{"created":"2025-05-14 02:52:16","title":"DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis","abstract":"In recent years, generative adversarial networks (GANs) have made significant progress in generating audio sequences. However, these models typically rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences, and lead to mode collapse during conditional generation. To address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN), a novel GAN architecture that incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation. This innovative approach enhances the model's ability to capture and reproduce intricate audio patterns. In particular, our proposed model features a DPN module for multi-resolution generation utilizing deformable convolution operations, allowing for adaptive receptive fields that improve the quality and fidelity of the synthetic audio. Additionally, we enhance the discriminator network using deformable convolution to better distinguish between real and generated samples, further refining the audio quality. We trained two versions of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M parameters). For evaluation, we use five different datasets, covering both speech synthesis and music generation tasks, to demonstrate the efficiency of the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers superior performance on both out-of-distribution and noisy data, showcasing its robustness and adaptability. Trained across various datasets, DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics, and exhibits increased robustness in synthesized audio.","sentences":["In recent years, generative adversarial networks (GANs) have made significant progress in generating audio sequences.","However, these models typically rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences, and lead to mode collapse during conditional generation.","To address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN), a novel GAN architecture that incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation.","This innovative approach enhances the model's ability to capture and reproduce intricate audio patterns.","In particular, our proposed model features a DPN module for multi-resolution generation utilizing deformable convolution operations, allowing for adaptive receptive fields that improve the quality and fidelity of the synthetic audio.","Additionally, we enhance the discriminator network using deformable convolution to better distinguish between real and generated samples, further refining the audio quality.","We trained two versions of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M parameters).","For evaluation, we use five different datasets, covering both speech synthesis and music generation tasks, to demonstrate the efficiency of the DPN-GAN.","The experimental results demonstrate that DPN-GAN delivers superior performance on both out-of-distribution and noisy data, showcasing its robustness and adaptability.","Trained across various datasets, DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics, and exhibits increased robustness in synthesized audio."],"url":"http://arxiv.org/abs/2505.09091v1"}
{"created":"2025-05-14 02:39:10","title":"Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision","abstract":"Recent advancements in deep neural networks (DNNs), particularly large-scale language models, have demonstrated remarkable capabilities in image and natural language understanding. Although scaling up model parameters with increasing volume of training data has progressively improved DNN capabilities, achieving complex cognitive abilities - such as understanding abstract concepts, reasoning, and adapting to novel scenarios, which are intrinsic to human cognition - remains a major challenge. In this study, we show that brain-in-the-loop supervised learning, utilizing a small set of brain signals, can effectively transfer human conceptual structures to DNNs, significantly enhancing their comprehension of abstract and even unseen concepts. Experimental results further indicate that the enhanced cognitive capabilities lead to substantial performance gains in challenging tasks, including few-shot/zero-shot learning and out-of-distribution recognition, while also yielding highly interpretable concept representations. These findings highlight that human-in-the-loop supervision can effectively augment the complex cognitive abilities of large models, offering a promising pathway toward developing more human-like cognitive abilities in artificial systems.","sentences":["Recent advancements in deep neural networks (DNNs), particularly large-scale language models, have demonstrated remarkable capabilities in image and natural language understanding.","Although scaling up model parameters with increasing volume of training data has progressively improved DNN capabilities, achieving complex cognitive abilities - such as understanding abstract concepts, reasoning, and adapting to novel scenarios, which are intrinsic to human cognition - remains a major challenge.","In this study, we show that brain-in-the-loop supervised learning, utilizing a small set of brain signals, can effectively transfer human conceptual structures to DNNs, significantly enhancing their comprehension of abstract and even unseen concepts.","Experimental results further indicate that the enhanced cognitive capabilities lead to substantial performance gains in challenging tasks, including few-shot/zero-shot learning and out-of-distribution recognition, while also yielding highly interpretable concept representations.","These findings highlight that human-in-the-loop supervision can effectively augment the complex cognitive abilities of large models, offering a promising pathway toward developing more human-like cognitive abilities in artificial systems."],"url":"http://arxiv.org/abs/2505.09085v1"}
{"created":"2025-05-14 02:35:47","title":"CEC-Zero: Chinese Error Correction Solution Based on LLM","abstract":"Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.","sentences":["Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC).","While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization.","This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision.","By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models.","Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications.","This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models."],"url":"http://arxiv.org/abs/2505.09082v1"}
{"created":"2025-05-14 02:17:53","title":"2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition","abstract":"Despite recent advances in facial recognition, there remains a fundamental issue concerning degradations in performance due to substantial perspective (pose) differences between enrollment and query (probe) imagery. Therefore, we propose a novel domain adaptive framework to facilitate improved performances across large discrepancies in pose by enabling image-based (2D) representations to infer properties of inherently pose invariant point cloud (3D) representations. Specifically, our proposed framework achieves better pose invariance by using (1) a shared (joint) attention mapping to emphasize common patterns that are most correlated between 2D facial images and 3D facial data and (2) a joint entropy regularizing loss to promote better consistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D and 3D representations$\\unicode{x2014}$by leveraging both attention maps. This framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms competitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$) TAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and 1.57$\\unicode{x0025}$, respectively.","sentences":["Despite recent advances in facial recognition, there remains a fundamental issue concerning degradations in performance due to substantial perspective (pose) differences between enrollment and query (probe) imagery.","Therefore, we propose a novel domain adaptive framework to facilitate improved performances across large discrepancies in pose by enabling image-based (2D) representations to infer properties of inherently pose invariant point cloud (3D) representations.","Specifically, our proposed framework achieves better pose invariance by using (1) a shared (joint) attention mapping to emphasize common patterns that are most correlated between 2D facial images and 3D facial data and (2) a joint entropy regularizing loss to promote better consistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D and 3D representations$\\unicode{x2014}$by leveraging both attention maps.","This framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms competitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$) TAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and 1.57$\\unicode{x0025}$, respectively."],"url":"http://arxiv.org/abs/2505.09073v1"}
{"created":"2025-05-14 02:09:38","title":"A Novel 6-axis Force/Torque Sensor Using Inductance Sensors","abstract":"This paper presents a novel six-axis force/torque (F/T) sensor based on inductive sensing technology. Unlike conventional strain gauge-based sensors that require direct contact and external amplification, the proposed sensor utilizes non-contact inductive measurements to estimate force via displacement of a conductive target. A compact, fully integrated architecture is achieved by incorporating a CAN-FD based signal processing module directly onto the PCB, enabling high-speed data acquisition at up to 4~kHz without external DAQ systems. The sensing mechanism is modeled and calibrated through a rational function fitting approach, which demonstrated superior performance in terms of root mean square error (RMSE), coefficient of determination ($R^2$), and linearity error compared to other nonlinear models. Static and repeatability experiments validate the sensor's accuracy, achieving a resolution of 0.03~N and quantization levels exceeding 55,000 steps, surpassing that of commercial sensors. The sensor also exhibits low crosstalk, high sensitivity, and robust noise characteristics. Its performance and structure make it suitable for precision robotic applications, especially in scenarios where compactness, non-contact operation, and integrated processing are essential.","sentences":["This paper presents a novel six-axis force/torque (F/T) sensor based on inductive sensing technology.","Unlike conventional strain gauge-based sensors that require direct contact and external amplification, the proposed sensor utilizes non-contact inductive measurements to estimate force via displacement of a conductive target.","A compact, fully integrated architecture is achieved by incorporating a CAN-FD based signal processing module directly onto the PCB, enabling high-speed data acquisition at up to 4~kHz without external DAQ systems.","The sensing mechanism is modeled and calibrated through a rational function fitting approach, which demonstrated superior performance in terms of root mean square error (RMSE), coefficient of determination ($R^2$), and linearity error compared to other nonlinear models.","Static and repeatability experiments validate the sensor's accuracy, achieving a resolution of 0.03~N and quantization levels exceeding 55,000 steps, surpassing that of commercial sensors.","The sensor also exhibits low crosstalk, high sensitivity, and robust noise characteristics.","Its performance and structure make it suitable for precision robotic applications, especially in scenarios where compactness, non-contact operation, and integrated processing are essential."],"url":"http://arxiv.org/abs/2505.09069v1"}
{"created":"2025-05-14 01:48:26","title":"Single-shot prediction of parametric partial differential equations","abstract":"We introduce Flexi-VAE, a data-driven framework for efficient single-shot forecasting of nonlinear parametric partial differential equations (PDEs), eliminating the need for iterative time-stepping while maintaining high accuracy and stability. Flexi-VAE incorporates a neural propagator that advances latent representations forward in time, aligning latent evolution with physical state reconstruction in a variational autoencoder setting. We evaluate two propagation strategies, the Direct Concatenation Propagator (DCP) and the Positional Encoding Propagator (PEP), and demonstrate, through representation-theoretic analysis, that DCP offers superior long-term generalization by fostering disentangled and physically meaningful latent spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal that propagated latent states reside in regions of lower decoder sensitivity and more stable local geometry than those derived via direct encoding, enhancing robustness for long-horizon predictions. We validate Flexi-VAE on canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D advection-diffusion equation, achieving accurate forecasts across wide parametric ranges. The model delivers over 50x CPU and 90x GPU speedups compared to autoencoder-LSTM baselines for large temporal shifts. These results position Flexi-VAE as a scalable and interpretable surrogate modeling tool for accelerating high-fidelity simulations in computational fluid dynamics (CFD) and other parametric PDE-driven applications, with extensibility to higher-dimensional and more complex systems.","sentences":["We introduce Flexi-VAE, a data-driven framework for efficient single-shot forecasting of nonlinear parametric partial differential equations (PDEs), eliminating the need for iterative time-stepping while maintaining high accuracy and stability.","Flexi-VAE incorporates a neural propagator that advances latent representations forward in time, aligning latent evolution with physical state reconstruction in a variational autoencoder setting.","We evaluate two propagation strategies, the Direct Concatenation Propagator (DCP) and the Positional Encoding Propagator (PEP), and demonstrate, through representation-theoretic analysis, that DCP offers superior long-term generalization by fostering disentangled and physically meaningful latent spaces.","Geometric diagnostics, including Jacobian spectral analysis, reveal that propagated latent states reside in regions of lower decoder sensitivity and more stable local geometry than those derived via direct encoding, enhancing robustness for long-horizon predictions.","We validate Flexi-VAE on canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D advection-diffusion equation, achieving accurate forecasts across wide parametric ranges.","The model delivers over 50x CPU and 90x GPU speedups compared to autoencoder-LSTM baselines for large temporal shifts.","These results position Flexi-VAE as a scalable and interpretable surrogate modeling tool for accelerating high-fidelity simulations in computational fluid dynamics (CFD) and other parametric PDE-driven applications, with extensibility to higher-dimensional and more complex systems."],"url":"http://arxiv.org/abs/2505.09063v1"}
{"created":"2025-05-14 01:19:44","title":"EcoSphere: A Decision-Support Tool for Automated Carbon Emission and Cost Optimization in Sustainable Urban Development","abstract":"The construction industry is a major contributor to global greenhouse gas emissions, with embodied carbon being a key component. This study develops EcoSphere, an innovative software designed to evaluate and balance embodied and operational carbon emissions with construction and environmental costs in urban planning. Using high-resolution data from the National Structure Inventory, combined with computer vision and natural language processing applied to Google Street View and satellite imagery, EcoSphere categorizes buildings by structural and material characteristics with a bottom-up approach, creating a baseline emissions dataset. By simulating policy scenarios and mitigation strategies, EcoSphere provides policymakers and non-experts with actionable insights for sustainable development in cities and provide them with a vision of the environmental and financial results of their decisions. Case studies in Chicago and Indianapolis showcase how EcoSphere aids in assessing policy impacts on carbon emissions and costs, supporting data-driven progress toward carbon neutrality.","sentences":["The construction industry is a major contributor to global greenhouse gas emissions, with embodied carbon being a key component.","This study develops EcoSphere, an innovative software designed to evaluate and balance embodied and operational carbon emissions with construction and environmental costs in urban planning.","Using high-resolution data from the National Structure Inventory, combined with computer vision and natural language processing applied to Google Street View and satellite imagery, EcoSphere categorizes buildings by structural and material characteristics with a bottom-up approach, creating a baseline emissions dataset.","By simulating policy scenarios and mitigation strategies, EcoSphere provides policymakers and non-experts with actionable insights for sustainable development in cities and provide them with a vision of the environmental and financial results of their decisions.","Case studies in Chicago and Indianapolis showcase how EcoSphere aids in assessing policy impacts on carbon emissions and costs, supporting data-driven progress toward carbon neutrality."],"url":"http://arxiv.org/abs/2505.09054v1"}
{"created":"2025-05-14 00:41:44","title":"RT-cache: Efficient Robot Trajectory Retrieval System","abstract":"This paper introduces RT-cache, a novel trajectorymemory pipeline that accelerates real-world robot inference by leveraging big-data retrieval and learning from experience. While modern Vision-Language-Action (VLA) models can handle diverse robotic tasks, they often incur high per-step inference costs, resulting in significant latency, sometimes minutes per task. In contrast, RT-cache stores a large-scale Memory of previously successful robot trajectories and retrieves relevant multistep motion snippets, drastically reducing inference overhead. By integrating a Memory Builder with a Trajectory Retrieval, we develop an efficient retrieval process that remains tractable even for extremely large datasets. RT-cache flexibly accumulates real-world experiences and replays them whenever the current scene matches past states, adapting quickly to new or unseen environments with only a few additional samples. Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks both faster and more successfully than a baseline lacking retrieval, suggesting a practical, data-driven solution for real-time manipulation.","sentences":["This paper introduces RT-cache, a novel trajectorymemory pipeline that accelerates real-world robot inference by leveraging big-data retrieval and learning from experience.","While modern Vision-Language-Action (VLA) models can handle diverse robotic tasks, they often incur high per-step inference costs, resulting in significant latency, sometimes minutes per task.","In contrast, RT-cache stores a large-scale Memory of previously successful robot trajectories and retrieves relevant multistep motion snippets, drastically reducing inference overhead.","By integrating a Memory Builder with a Trajectory Retrieval, we develop an efficient retrieval process that remains tractable even for extremely large datasets.","RT-cache flexibly accumulates real-world experiences and replays them whenever the current scene matches past states, adapting quickly to new or unseen environments with only a few additional samples.","Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks both faster and more successfully than a baseline lacking retrieval, suggesting a practical, data-driven solution for real-time manipulation."],"url":"http://arxiv.org/abs/2505.09040v1"}
{"created":"2025-05-14 00:39:47","title":"Atomic Consistency Preference Optimization for Long-Form Question Answering","abstract":"Large Language Models (LLMs) frequently produce factoid hallucinations - plausible yet incorrect answers. A common mitigation strategy is model alignment, which improves factual accuracy by training on curated factual and non-factual pairs. However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness, which may not always be accessible. To address this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision. ACPO leverages atomic consistency signals, i.e., the agreement of individual facts across multiple stochastic responses, to identify high- and low-quality data pairs for model alignment. By eliminating the need for costly GPT calls, ACPO provides a scalable and efficient approach to improving factoid question-answering. Despite being self-supervised, empirical results demonstrate that ACPO outperforms FactAlign, a strong supervised alignment baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its effectiveness in enhancing factual reliability without relying on external models or knowledge bases.","sentences":["Large Language Models (LLMs) frequently produce factoid hallucinations - plausible yet incorrect answers.","A common mitigation strategy is model alignment, which improves factual accuracy by training on curated factual and non-factual pairs.","However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness, which may not always be accessible.","To address this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision.","ACPO leverages atomic consistency signals, i.e., the agreement of individual facts across multiple stochastic responses, to identify high- and low-quality data pairs for model alignment.","By eliminating the need for costly GPT calls, ACPO provides a scalable and efficient approach to improving factoid question-answering.","Despite being self-supervised, empirical results demonstrate that ACPO outperforms FactAlign, a strong supervised alignment baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its effectiveness in enhancing factual reliability without relying on external models or knowledge bases."],"url":"http://arxiv.org/abs/2505.09039v1"}
{"created":"2025-05-14 00:04:23","title":"FocusE: A semantic extension of FocusST","abstract":"To analyse and verify the safety and security properties of interactive systems, a formal specification might be necessary. There are many types of formal languages and frameworks. The decision regarding what type of formal specification should be applied in each particular case depends on many factors. One of the approaches to specify interactive systems formally is to present them as a composition of components processing data and control streams. In this short paper, we present FocusE, a formal approach for modelling event-based streams. The proposed approach is based on a formal language FocusST, and can be seen as its semantic extension.","sentences":["To analyse and verify the safety and security properties of interactive systems, a formal specification might be necessary.","There are many types of formal languages and frameworks.","The decision regarding what type of formal specification should be applied in each particular case depends on many factors.","One of the approaches to specify interactive systems formally is to present them as a composition of components processing data and control streams.","In this short paper, we present FocusE, a formal approach for modelling event-based streams.","The proposed approach is based on a formal language FocusST, and can be seen as its semantic extension."],"url":"http://arxiv.org/abs/2505.09032v1"}
{"created":"2025-05-13 23:31:32","title":"AI-Mediated Code Comment Improvement","abstract":"This paper describes an approach to improve code comments along different quality axes by rewriting those comments with customized Artificial Intelligence (AI)-based tools. We conduct an empirical study followed by grounded theory qualitative analysis to determine the quality axes to improve. Then we propose a procedure using a Large Language Model (LLM) to rewrite existing code comments along the quality axes. We implement our procedure using GPT-4o, then distil the results into a smaller model capable of being run in-house, so users can maintain data custody. We evaluate both our approach using GPT-4o and the distilled model versions. We show in an evaluation how our procedure improves code comments along the quality axes. We release all data and source code in an online repository for reproducibility.","sentences":["This paper describes an approach to improve code comments along different quality axes by rewriting those comments with customized Artificial Intelligence (AI)-based tools.","We conduct an empirical study followed by grounded theory qualitative analysis to determine the quality axes to improve.","Then we propose a procedure using a Large Language Model (LLM) to rewrite existing code comments along the quality axes.","We implement our procedure using GPT-4o, then distil the results into a smaller model capable of being run in-house, so users can maintain data custody.","We evaluate both our approach using GPT-4o and the distilled model versions.","We show in an evaluation how our procedure improves code comments along the quality axes.","We release all data and source code in an online repository for reproducibility."],"url":"http://arxiv.org/abs/2505.09021v1"}
{"created":"2025-05-13 23:12:54","title":"Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction","abstract":"Effective dietary monitoring is critical for managing Type 2 diabetes, yet accurately estimating caloric intake remains a major challenge. While continuous glucose monitors (CGMs) offer valuable physiological data, they often fall short in capturing the full nutritional profile of meals due to inter-individual and meal-specific variability. In this work, we introduce a multimodal deep learning framework that jointly leverages CGM time-series data, Demographic/Microbiome, and pre-meal food images to enhance caloric estimation. Our model utilizes attention based encoding and a convolutional feature extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome data followed by a late fusion strategy for joint reasoning. We evaluate our approach on a curated dataset of over 40 participants, incorporating synchronized CGM, Demographic and Microbiome data and meal photographs with standardized caloric labels. Our model achieves a Root Mean Squared Relative Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These findings demonstrate the potential of multimodal sensing to improve automated dietary assessment tools for chronic disease management.","sentences":["Effective dietary monitoring is critical for managing Type 2 diabetes, yet accurately estimating caloric intake remains a major challenge.","While continuous glucose monitors (CGMs) offer valuable physiological data, they often fall short in capturing the full nutritional profile of meals due to inter-individual and meal-specific variability.","In this work, we introduce a multimodal deep learning framework that jointly leverages CGM time-series data, Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.","Our model utilizes attention based encoding and a convolutional feature extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome data followed by a late fusion strategy for joint reasoning.","We evaluate our approach on a curated dataset of over 40 participants, incorporating synchronized CGM, Demographic and Microbiome data and meal photographs with standardized caloric labels.","Our model achieves a Root Mean Squared Relative Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%.","These findings demonstrate the potential of multimodal sensing to improve automated dietary assessment tools for chronic disease management."],"url":"http://arxiv.org/abs/2505.09018v1"}
{"created":"2025-05-13 22:57:39","title":"Fully Dynamic Euclidean Bi-Chromatic Matching in Sublinear Update Time","abstract":"We consider the Euclidean bi-chromatic matching problem in the dynamic setting, where the goal is to efficiently process point insertions and deletions while maintaining a high-quality solution. Computing the minimum cost bi-chromatic matching is one of the core problems in geometric optimization that has found many applications, most notably in estimating Wasserstein distance between two distributions. In this work, we present the first fully dynamic algorithm for Euclidean bi-chromatic matching with sub-linear update time. For any fixed $\\varepsilon > 0$, our algorithm achieves $O(1/\\varepsilon)$-approximation and handles updates in $O(n^{\\varepsilon})$ time. Our experiments show that our algorithm enables effective monitoring of the distributional drift in the Wasserstein distance on real and synthetic data sets, while outperforming the runtime of baseline approximations by orders of magnitudes.","sentences":["We consider the Euclidean bi-chromatic matching problem in the dynamic setting, where the goal is to efficiently process point insertions and deletions while maintaining a high-quality solution.","Computing the minimum cost bi-chromatic matching is one of the core problems in geometric optimization that has found many applications, most notably in estimating Wasserstein distance between two distributions.","In this work, we present the first fully dynamic algorithm for Euclidean bi-chromatic matching with sub-linear update time.","For any fixed $\\varepsilon > 0$, our algorithm achieves $O(1/\\varepsilon)$-approximation and handles updates in $O(n^{\\varepsilon})$ time.","Our experiments show that our algorithm enables effective monitoring of the distributional drift in the Wasserstein distance on real and synthetic data sets, while outperforming the runtime of baseline approximations by orders of magnitudes."],"url":"http://arxiv.org/abs/2505.09010v1"}
{"created":"2025-05-13 22:49:15","title":"Vendi Information Gain: An Alternative To Mutual Information For Science And Machine Learning","abstract":"In his 1948 seminal paper A Mathematical Theory of Communication that birthed information theory, Claude Shannon introduced mutual information (MI), which he called \"rate of transmission\", as a way to quantify information gain (IG) and defined it as the difference between the marginal and conditional entropy of a random variable. While MI has become a standard tool in science and engineering, it has several shortcomings. First, MI is often intractable - it requires a density over samples with tractable Shannon entropy - and existing techniques for approximating it often fail, especially in high dimensions. Moreover, in settings where MI is tractable, its symmetry and insensitivity to sample similarity are undesirable. In this paper, we propose the Vendi Information Gain (VIG), a novel alternative to MI that leverages the Vendi scores, a flexible family of similarity-based diversity metrics. We call the logarithm of the VS the Vendi entropy and define VIG as the difference between the marginal and conditional Vendi entropy of a variable. Being based on the VS, VIG accounts for similarity. Furthermore, VIG generalizes MI and recovers it under the assumption that the samples are completely dissimilar. Importantly, VIG only requires samples and not a probability distribution over them. Finally, it is asymmetric, a desideratum for a good measure of IG that MI fails to meet. VIG extends information theory to settings where MI completely fails. For example, we use VIG to describe a novel, unified framework for active data acquisition, a popular paradigm of modern data-driven science. We demonstrate the advantages of VIG over MI in diverse applications, including in cognitive science to model human response times to external stimuli and in epidemiology to learn epidemic processes and identify disease hotspots in different countries via level-set estimation.","sentences":["In his 1948 seminal paper A Mathematical Theory of Communication that birthed information theory, Claude Shannon introduced mutual information (MI), which he called \"rate of transmission\", as a way to quantify information gain (IG) and defined it as the difference between the marginal and conditional entropy of a random variable.","While MI has become a standard tool in science and engineering, it has several shortcomings.","First, MI is often intractable - it requires a density over samples with tractable Shannon entropy - and existing techniques for approximating it often fail, especially in high dimensions.","Moreover, in settings where MI is tractable, its symmetry and insensitivity to sample similarity are undesirable.","In this paper, we propose the Vendi Information Gain (VIG), a novel alternative to MI that leverages the Vendi scores, a flexible family of similarity-based diversity metrics.","We call the logarithm of the VS the Vendi entropy and define VIG as the difference between the marginal and conditional Vendi entropy of a variable.","Being based on the VS, VIG accounts for similarity.","Furthermore, VIG generalizes MI and recovers it under the assumption that the samples are completely dissimilar.","Importantly, VIG only requires samples and not a probability distribution over them.","Finally, it is asymmetric, a desideratum for a good measure of IG that MI fails to meet.","VIG extends information theory to settings where MI completely fails.","For example, we use VIG to describe a novel, unified framework for active data acquisition, a popular paradigm of modern data-driven science.","We demonstrate the advantages of VIG over MI in diverse applications, including in cognitive science to model human response times to external stimuli and in epidemiology to learn epidemic processes and identify disease hotspots in different countries via level-set estimation."],"url":"http://arxiv.org/abs/2505.09007v1"}
{"created":"2025-05-13 22:26:19","title":"Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking","abstract":"In recent years, visual tracking methods based on convolutional neural networks and Transformers have achieved remarkable performance and have been successfully applied in fields such as autonomous driving. However, the numerous security issues exposed by deep learning models have gradually affected the reliable application of visual tracking methods in real-world scenarios. Therefore, how to reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks has become a critical problem that needs to be addressed. To this end, we propose an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking. This method integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing, which can significantly enhance the transferability and attack effectiveness of adversarial examples. AMGA randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model. This paradigm minimizes the gap between white- and black-box adversarial attacks, thus achieving excellent attack performance in black-box scenarios. Extensive experimental results on large-scale datasets such as OTB2015, LaSOT, and GOT-10k demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples. Codes and data are available at https://github.com/pgao-lab/AMGA.","sentences":["In recent years, visual tracking methods based on convolutional neural networks and Transformers have achieved remarkable performance and have been successfully applied in fields such as autonomous driving.","However, the numerous security issues exposed by deep learning models have gradually affected the reliable application of visual tracking methods in real-world scenarios.","Therefore, how to reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks has become a critical problem that needs to be addressed.","To this end, we propose an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking.","This method integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing, which can significantly enhance the transferability and attack effectiveness of adversarial examples.","AMGA randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model.","This paradigm minimizes the gap between white- and black-box adversarial attacks, thus achieving excellent attack performance in black-box scenarios.","Extensive experimental results on large-scale datasets such as OTB2015, LaSOT, and GOT-10k demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples.","Codes and data are available at https://github.com/pgao-lab/AMGA."],"url":"http://arxiv.org/abs/2505.08999v1"}
{"created":"2025-05-13 22:09:10","title":"Dataflow & Tiling Strategies in Edge-AI FPGA Accelerators: A Comprehensive Literature Review","abstract":"Edge-AI applications demand high-throughput, low-latency inference on FPGAs under tight resource and power constraints. This survey provides a comprehensive review of two key architectural decisions for FPGA-based neural network accelerators: (i) the dataflow (the order and manner in which data is moved and reused on chip), and (ii) the tiling/blocking strategy (how large tensors are partitioned to fit on-chip). We first present a broadened taxonomy of canonical dataflow styles: Weight-Stationary, Output-Stationary, Row-Stationary, and No-Local-Reuse, including formal definitions, pseudocode/diagrams, and real FPGA examples. We then discuss analytical frameworks (MAESTRO, Timeloop) and compare them with a concise feature table, illustrating how they model reuse, performance, and hardware costs. Next, we detail multi-level tiling and loop unrolling/pipelining strategies for FPGAs, clarifying how each memory tier (registers, LUTRAM, BRAM, HBM) can be exploited. Our four case studies - FINN, FINN-R, FlightLLM, and SSR - highlight distinct dataflows (from binary streaming to hybrid sparse transformations) and tiling patterns. We include a unified comparison matrix covering platform, precision, throughput, resource utilization, and energy efficiency, plus small block diagrams for each design. We conclude by examining design automation trade-offs among HLS, DSL, and hand-coded RTL, offering a \"lessons learned\" summary box, and charting future research directions in partial reconfiguration, hybrid dataflows, and domain-specific compiler flows for next-generation edge AI FPGA accelerators.","sentences":["Edge-AI applications demand high-throughput, low-latency inference on FPGAs under tight resource and power constraints.","This survey provides a comprehensive review of two key architectural decisions for FPGA-based neural network accelerators: (i) the dataflow (the order and manner in which data is moved and reused on chip), and (ii) the tiling/blocking strategy (how large tensors are partitioned to fit on-chip).","We first present a broadened taxonomy of canonical dataflow styles: Weight-Stationary, Output-Stationary, Row-Stationary, and No-Local-Reuse, including formal definitions, pseudocode/diagrams, and real FPGA examples.","We then discuss analytical frameworks (MAESTRO, Timeloop) and compare them with a concise feature table, illustrating how they model reuse, performance, and hardware costs.","Next, we detail multi-level tiling and loop unrolling/pipelining strategies for FPGAs, clarifying how each memory tier (registers, LUTRAM, BRAM, HBM) can be exploited.","Our four case studies - FINN, FINN-R, FlightLLM, and SSR - highlight distinct dataflows (from binary streaming to hybrid sparse transformations) and tiling patterns.","We include a unified comparison matrix covering platform, precision, throughput, resource utilization, and energy efficiency, plus small block diagrams for each design.","We conclude by examining design automation trade-offs among HLS, DSL, and hand-coded RTL, offering a \"lessons learned\" summary box, and charting future research directions in partial reconfiguration, hybrid dataflows, and domain-specific compiler flows for next-generation edge AI FPGA accelerators."],"url":"http://arxiv.org/abs/2505.08992v1"}
{"created":"2025-05-13 21:49:56","title":"Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret","abstract":"We consider the problem of online prediction for an unknown, non-explosive linear stochastic system. With a known system model, the optimal predictor is the celebrated Kalman filter. In the case of unknown systems, existing approaches based on recursive least squares and its variants may suffer from degraded performance due to the highly imbalanced nature of the regression model. This imbalance can easily lead to overfitting and thus degrade prediction accuracy. We tackle this problem by injecting an inductive bias into the regression model via {exponential forgetting}. While exponential forgetting is a common wisdom in online learning, it is typically used for re-weighting data. In contrast, our approach focuses on balancing the regression model. This achieves a better trade-off between {regression} and {regularization errors}, and simultaneously reduces the {accumulation error}. With new proof techniques, we also provide a sharper logarithmic regret bound of $O(\\log^3 N)$, where $N$ is the number of observations.","sentences":["We consider the problem of online prediction for an unknown, non-explosive linear stochastic system.","With a known system model, the optimal predictor is the celebrated Kalman filter.","In the case of unknown systems, existing approaches based on recursive least squares and its variants may suffer from degraded performance due to the highly imbalanced nature of the regression model.","This imbalance can easily lead to overfitting and thus degrade prediction accuracy.","We tackle this problem by injecting an inductive bias into the regression model via {exponential forgetting}.","While exponential forgetting is a common wisdom in online learning, it is typically used for re-weighting data.","In contrast, our approach focuses on balancing the regression model.","This achieves a better trade-off between {regression} and {regularization errors}, and simultaneously reduces the {accumulation error}.","With new proof techniques, we also provide a sharper logarithmic regret bound of $O(\\log^3 N)$, where $N$ is the number of observations."],"url":"http://arxiv.org/abs/2505.08982v1"}
{"created":"2025-05-13 21:39:40","title":"SaFARi: State-Space Models for Frame-Agnostic Representation","abstract":"State-Space Models (SSMs) have re-emerged as a powerful tool for online function approximation, and as the backbone of machine learning models for long-range dependent data. However, to date, only a few polynomial bases have been explored for this purpose, and the state-of-the-art implementations were built upon the best of a few limited options. In this paper, we present a generalized method for building an SSM with any frame or basis, rather than being restricted to polynomials. This framework encompasses the approach known as HiPPO, but also permits an infinite diversity of other possible \"species\" within the SSM architecture. We dub this approach SaFARi: SSMs for Frame-Agnostic Representation.","sentences":["State-Space Models (SSMs) have re-emerged as a powerful tool for online function approximation, and as the backbone of machine learning models for long-range dependent data.","However, to date, only a few polynomial bases have been explored for this purpose, and the state-of-the-art implementations were built upon the best of a few limited options.","In this paper, we present a generalized method for building an SSM with any frame or basis, rather than being restricted to polynomials.","This framework encompasses the approach known as HiPPO, but also permits an infinite diversity of other possible \"species\" within the SSM architecture.","We dub this approach SaFARi: SSMs for Frame-Agnostic Representation."],"url":"http://arxiv.org/abs/2505.08977v1"}
{"created":"2025-05-13 21:27:52","title":"Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training","abstract":"In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination. We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework. PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training. Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM. During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders. We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data.","sentences":["In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination.","We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework.","PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training.","Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM.","During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss.","We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders.","We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP.","In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data."],"url":"http://arxiv.org/abs/2505.08971v1"}
{"created":"2025-05-13 20:49:04","title":"Even Faster Algorithm for the Chamfer Distance","abstract":"For two d-dimensional point sets A, B of size up to n, the Chamfer distance from A to B is defined as CH(A,B) = \\sum_{a \\in A} \\min_{b \\in B} \\|a-b\\|. The Chamfer distance is a widely used measure for quantifying dissimilarity between sets of points, used in many machine learning and computer vision applications. A recent work of Bakshi et al, NeuriPS'23, gave the first near-linear time (1+eps)-approximate algorithm, with a running time of O(ndlog(n)/eps^2). In this paper we improve the running time further, to O(nd(loglog(n)+log(1/eps))/eps^2). When eps is a constant, this reduces the gap between the upper bound and the trivial Omega(dn) lower bound significantly, from O(log n) to O(loglog n).","sentences":["For two d-dimensional point sets A, B of size up to n, the Chamfer distance from A to B is defined as CH(A,B) = \\sum_{a \\in A} \\min_{b \\in B} \\|a-b\\|.","The Chamfer distance is a widely used measure for quantifying dissimilarity between sets of points, used in many machine learning and computer vision applications.","A recent work of Bakshi et al, NeuriPS'23, gave the first near-linear time (1+eps)-approximate algorithm, with a running time of O(ndlog(n)/eps^2).","In this paper we improve the running time further, to O(nd(loglog(n)+log(1/eps))/eps^2).","When eps is a constant, this reduces the gap between the upper bound and the trivial Omega(dn) lower bound significantly, from O(log n) to O(loglog n)."],"url":"http://arxiv.org/abs/2505.08957v1"}
{"created":"2025-05-13 20:10:00","title":"ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers","abstract":"Predicting the future citation rates of academic papers is an important step toward the automation of research evaluation and the acceleration of scientific progress. We present $\\textbf{ForeCite}$, a simple but powerful framework to append pre-trained causal language models with a linear head for average monthly citation rate prediction. Adapting transformers for regression tasks, ForeCite achieves a test correlation of $\\rho = 0.826$ on a curated dataset of 900K+ biomedical papers published between 2000 and 2024, a 27-point improvement over the previous state-of-the-art. Comprehensive scaling-law analysis reveals consistent gains across model sizes and data volumes, while temporal holdout experiments confirm practical robustness. Gradient-based saliency heatmaps suggest a potentially undue reliance on titles and abstract texts. These results establish a new state-of-the-art in forecasting the long-term influence of academic research and lay the groundwork for the automated, high-fidelity evaluation of scientific contributions.","sentences":["Predicting the future citation rates of academic papers is an important step toward the automation of research evaluation and the acceleration of scientific progress.","We present $\\textbf{ForeCite}$, a simple but powerful framework to append pre-trained causal language models with a linear head for average monthly citation rate prediction.","Adapting transformers for regression tasks, ForeCite achieves a test correlation of $\\rho = 0.826$ on a curated dataset of 900K+ biomedical papers published between 2000 and 2024, a 27-point improvement over the previous state-of-the-art.","Comprehensive scaling-law analysis reveals consistent gains across model sizes and data volumes, while temporal holdout experiments confirm practical robustness.","Gradient-based saliency heatmaps suggest a potentially undue reliance on titles and abstract texts.","These results establish a new state-of-the-art in forecasting the long-term influence of academic research and lay the groundwork for the automated, high-fidelity evaluation of scientific contributions."],"url":"http://arxiv.org/abs/2505.08941v1"}
{"created":"2025-05-13 20:09:22","title":"NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach","abstract":"The characterization of exoplanetary atmospheres through spectral analysis is a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration with the European Space Agency's (ESA) Ariel mission, provided an opportunity to explore machine learning techniques for extracting atmospheric compositions from simulated spectral data. In this work, we focus on a data-centric business approach, prioritizing generalization over competition-specific optimization. We briefly outline multiple experimental axes, including feature extraction, signal transformation, and heteroskedastic uncertainty modeling. Our experiments demonstrate that uncertainty estimation plays a crucial role in the Gaussian Log-Likelihood (GLL) score, impacting performance by several percentage points. Despite improving the GLL score by 11%, our results highlight the inherent limitations of tabular modeling and feature engineering for this task, as well as the constraints of a business-driven approach within a Kaggle-style competition framework. Our findings emphasize the trade-offs between model simplicity, interpretability, and generalization in astrophysical data analysis.","sentences":["The characterization of exoplanetary atmospheres through spectral analysis is a complex challenge.","The NeurIPS 2024 Ariel Data Challenge, in collaboration with the European Space Agency's (ESA) Ariel mission, provided an opportunity to explore machine learning techniques for extracting atmospheric compositions from simulated spectral data.","In this work, we focus on a data-centric business approach, prioritizing generalization over competition-specific optimization.","We briefly outline multiple experimental axes, including feature extraction, signal transformation, and heteroskedastic uncertainty modeling.","Our experiments demonstrate that uncertainty estimation plays a crucial role in the Gaussian Log-Likelihood (GLL) score, impacting performance by several percentage points.","Despite improving the GLL score by 11%, our results highlight the inherent limitations of tabular modeling and feature engineering for this task, as well as the constraints of a business-driven approach within a Kaggle-style competition framework.","Our findings emphasize the trade-offs between model simplicity, interpretability, and generalization in astrophysical data analysis."],"url":"http://arxiv.org/abs/2505.08940v1"}
{"created":"2025-05-13 19:52:50","title":"Predictive Digital Twins with Quantified Uncertainty for Patient-Specific Decision Making in Oncology","abstract":"Quantifying the uncertainty in predictive models is critical for establishing trust and enabling risk-informed decision making for personalized medicine. In contrast to one-size-fits-all approaches that seek to mitigate risk at the population level, digital twins enable personalized modeling thereby potentially improving individual patient outcomes. Realizing digital twins in biomedicine requires scalable and efficient methods to integrate patient data with mechanistic models of disease progression. This study develops an end-to-end data-to-decisions methodology that combines longitudinal non-invasive imaging data with mechanistic models to estimate and predict spatiotemporal tumor progression accounting for patient-specific anatomy. Through the solution of a statistical inverse problem, imaging data inform the spatially varying parameters of a reaction-diffusion model of tumor progression. An efficient parallel implementation of the forward model coupled with a scalable approximation of the Bayesian posterior distribution enables rigorous, but tractable, quantification of uncertainty due to the sparse, noisy measurements. The methodology is verified on a virtual patient with synthetic data to control for model inadequacy, noise level, and the frequency of data collection. The application to decision-making is illustrated by evaluating the importance of imaging frequency and formulating an optimal experimental design question. The clinical relevance is demonstrated through a model validation study on a cohort of patients with publicly available longitudinal imaging data.","sentences":["Quantifying the uncertainty in predictive models is critical for establishing trust and enabling risk-informed decision making for personalized medicine.","In contrast to one-size-fits-all approaches that seek to mitigate risk at the population level, digital twins enable personalized modeling thereby potentially improving individual patient outcomes.","Realizing digital twins in biomedicine requires scalable and efficient methods to integrate patient data with mechanistic models of disease progression.","This study develops an end-to-end data-to-decisions methodology that combines longitudinal non-invasive imaging data with mechanistic models to estimate and predict spatiotemporal tumor progression accounting for patient-specific anatomy.","Through the solution of a statistical inverse problem, imaging data inform the spatially varying parameters of a reaction-diffusion model of tumor progression.","An efficient parallel implementation of the forward model coupled with a scalable approximation of the Bayesian posterior distribution enables rigorous, but tractable, quantification of uncertainty due to the sparse, noisy measurements.","The methodology is verified on a virtual patient with synthetic data to control for model inadequacy, noise level, and the frequency of data collection.","The application to decision-making is illustrated by evaluating the importance of imaging frequency and formulating an optimal experimental design question.","The clinical relevance is demonstrated through a model validation study on a cohort of patients with publicly available longitudinal imaging data."],"url":"http://arxiv.org/abs/2505.08927v1"}
