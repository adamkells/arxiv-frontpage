{"created":"2023-12-20 18:56:45","title":"Deep Learning on 3D Neural Fields","abstract":"In recent years, Neural Fields (NFs) have emerged as an effective tool for encoding diverse continuous signals such as images, videos, audio, and 3D shapes. When applied to 3D data, NFs offer a solution to the fragmentation and limitations associated with prevalent discrete representations. However, given that NFs are essentially neural networks, it remains unclear whether and how they can be seamlessly integrated into deep learning pipelines for solving downstream tasks. This paper addresses this research problem and introduces nf2vec, a framework capable of generating a compact latent representation for an input NF in a single inference pass. We demonstrate that nf2vec effectively embeds 3D objects represented by the input NFs and showcase how the resulting embeddings can be employed in deep learning pipelines to successfully address various tasks, all while processing exclusively NFs. We test this framework on several NFs used to represent 3D surfaces, such as unsigned/signed distance and occupancy fields. Moreover, we demonstrate the effectiveness of our approach with more complex NFs that encompass both geometry and appearance of 3D objects such as neural radiance fields.","sentences":["In recent years, Neural Fields (NFs) have emerged as an effective tool for encoding diverse continuous signals such as images, videos, audio, and 3D shapes.","When applied to 3D data, NFs offer a solution to the fragmentation and limitations associated with prevalent discrete representations.","However, given that NFs are essentially neural networks, it remains unclear whether and how they can be seamlessly integrated into deep learning pipelines for solving downstream tasks.","This paper addresses this research problem and introduces nf2vec, a framework capable of generating a compact latent representation for an input NF in a single inference pass.","We demonstrate that nf2vec effectively embeds 3D objects represented by the input NFs and showcase how the resulting embeddings can be employed in deep learning pipelines to successfully address various tasks, all while processing exclusively NFs.","We test this framework on several NFs used to represent 3D surfaces, such as unsigned/signed distance and occupancy fields.","Moreover, we demonstrate the effectiveness of our approach with more complex NFs that encompass both geometry and appearance of 3D objects such as neural radiance fields."],"url":"http://arxiv.org/abs/2312.13277v1"}
{"created":"2023-12-20 18:53:18","title":"SoK: A Broad Comparative Evaluation of Software Debloating Tools","abstract":"Software debloating tools seek to improve the program security and performance by removing unnecessary code, called bloat. While many techniques have been proposed, several barriers to their adoption have emerged. Namely, debloating tools are highly specialized, making it difficult for adopters to find the right type of tool for their needs. This is further hindered by a lack of established metrics and comparative evaluations between tools. To close this gap, we surveyed of 10 years of debloating literature and several tools currently under commercial development to systematize the debloating ecosystem's knowledge. We then conducted a broad comparative evaluation of 10 debloating tools to determine their relative strengths and weaknesses. Our evaluation, conducted on a diverse set of 20 benchmark programs, measures tools across 16 performance, security, correctness, and usability metrics.   Our evaluation surfaces several concerning findings that contradict the prevailing narrative in debloating literature. First, debloating tools lack the required maturity to be used on real-world software, evidenced by a slim 21% overall success rate for creating passable debloated versions of medium- and high-complexity benchmarks. Second, debloating tools struggle to produce sound and robust programs. Using our novel differential fuzzing tool, DIFFER, we discovered that only 13% of our debloating attempts produced a sound and robust debloated program. Finally, our results indicate that debloating tools typically do not improve the performance or security posture of debloated programs by a significant degree. We believe that our contributions in this paper will help potential adopters better understand the landscape of tools and will motivate future research and development of more capable debloating tools. To this end, we have made our benchmark set, data, and custom tools publicly available.","sentences":["Software debloating tools seek to improve the program security and performance by removing unnecessary code, called bloat.","While many techniques have been proposed, several barriers to their adoption have emerged.","Namely, debloating tools are highly specialized, making it difficult for adopters to find the right type of tool for their needs.","This is further hindered by a lack of established metrics and comparative evaluations between tools.","To close this gap, we surveyed of 10 years of debloating literature and several tools currently under commercial development to systematize the debloating ecosystem's knowledge.","We then conducted a broad comparative evaluation of 10 debloating tools to determine their relative strengths and weaknesses.","Our evaluation, conducted on a diverse set of 20 benchmark programs, measures tools across 16 performance, security, correctness, and usability metrics.   ","Our evaluation surfaces several concerning findings that contradict the prevailing narrative in debloating literature.","First, debloating tools lack the required maturity to be used on real-world software, evidenced by a slim 21% overall success rate for creating passable debloated versions of medium- and high-complexity benchmarks.","Second, debloating tools struggle to produce sound and robust programs.","Using our novel differential fuzzing tool, DIFFER, we discovered that only 13% of our debloating attempts produced a sound and robust debloated program.","Finally, our results indicate that debloating tools typically do not improve the performance or security posture of debloated programs by a significant degree.","We believe that our contributions in this paper will help potential adopters better understand the landscape of tools and will motivate future research and development of more capable debloating tools.","To this end, we have made our benchmark set, data, and custom tools publicly available."],"url":"http://arxiv.org/abs/2312.13274v1"}
{"created":"2023-12-20 18:41:44","title":"dIR -- Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models","abstract":"Data is stored in both structured and unstructured form. Querying both, to power natural language conversations, is a challenge. This paper introduces dIR, Discrete Information Retrieval, providing a unified interface to query both free text and structured knowledge. Specifically, a Large Language Model (LLM) transforms text into expressive representation. After the text is extracted into columnar form, it can then be queried via a text-to-SQL Semantic Parser, with an LLM converting natural language into SQL. Where desired, such conversation may be effected by a multi-step reasoning conversational agent. We validate our approach via a proprietary question/answer data set, concluding that dIR makes a whole new class of queries on free text possible when compared to traditionally fine-tuned dense-embedding-model-based Information Retrieval (IR) and SQL-based Knowledge Bases (KB). For sufficiently complex queries, dIR can succeed where no other method stands a chance.","sentences":["Data is stored in both structured and unstructured form.","Querying both, to power natural language conversations, is a challenge.","This paper introduces dIR, Discrete Information Retrieval, providing a unified interface to query both free text and structured knowledge.","Specifically, a Large Language Model (LLM) transforms text into expressive representation.","After the text is extracted into columnar form, it can then be queried via a text-to-SQL Semantic Parser, with an LLM converting natural language into SQL.","Where desired, such conversation may be effected by a multi-step reasoning conversational agent.","We validate our approach via a proprietary question/answer data set, concluding that dIR makes a whole new class of queries on free text possible when compared to traditionally fine-tuned dense-embedding-model-based Information Retrieval (IR) and SQL-based Knowledge Bases (KB).","For sufficiently complex queries, dIR can succeed where no other method stands a chance."],"url":"http://arxiv.org/abs/2312.13264v1"}
{"created":"2023-12-20 18:27:53","title":"Conditional Image Generation with Pretrained Generative Model","abstract":"In recent years, diffusion models have gained popularity for their ability to generate higher-quality images in comparison to GAN models. However, like any other large generative models, these models require a huge amount of data, computational resources, and meticulous tuning for successful training. This poses a significant challenge, rendering it infeasible for most individuals. As a result, the research community has devised methods to leverage pre-trained unconditional diffusion models with additional guidance for the purpose of conditional image generative. These methods enable conditional image generations on diverse inputs and, most importantly, circumvent the need for training the diffusion model. In this paper, our objective is to reduce the time-required and computational overhead introduced by the addition of guidance in diffusion models -- while maintaining comparable image quality. We propose a set of methods based on our empirical analysis, demonstrating a reduction in computation time by approximately threefold.","sentences":["In recent years, diffusion models have gained popularity for their ability to generate higher-quality images in comparison to GAN models.","However, like any other large generative models, these models require a huge amount of data, computational resources, and meticulous tuning for successful training.","This poses a significant challenge, rendering it infeasible for most individuals.","As a result, the research community has devised methods to leverage pre-trained unconditional diffusion models with additional guidance for the purpose of conditional image generative.","These methods enable conditional image generations on diverse inputs and, most importantly, circumvent the need for training the diffusion model.","In this paper, our objective is to reduce the time-required and computational overhead introduced by the addition of guidance in diffusion models -- while maintaining comparable image quality.","We propose a set of methods based on our empirical analysis, demonstrating a reduction in computation time by approximately threefold."],"url":"http://arxiv.org/abs/2312.13253v1"}
{"created":"2023-12-20 18:00:16","title":"Diffusion Models With Learned Adaptive Noise","abstract":"Diffusion models have gained traction as powerful algorithms for synthesizing high-quality images. Central to these algorithms is the diffusion process, which maps data to noise according to equations inspired by thermodynamics and can significantly impact performance. A widely held assumption is that the ELBO objective of a diffusion model is invariant to the noise process (Kingma et al.,2021). In this work, we dispel this assumption -- we propose multivariate learned adaptive noise (MuLAN), a learned diffusion process that applies Gaussian noise at different rates across an image. Our method consists of three components -- a multivariate noise schedule, instance-conditional diffusion, and auxiliary variables -- which ensure that the learning objective is no longer invariant to the choice of the noise schedule as in previous works. Our work is grounded in Bayesian inference and casts the learned diffusion process as an approximate variational posterior that yields a tighter lower bound on marginal likelihood. Empirically, MuLAN sets a new state-of-the-art in density estimation on CIFAR-10 and ImageNet compared to classical diffusion. Code is available at https://github.com/s-sahoo/MuLAN","sentences":["Diffusion models have gained traction as powerful algorithms for synthesizing high-quality images.","Central to these algorithms is the diffusion process, which maps data to noise according to equations inspired by thermodynamics and can significantly impact performance.","A widely held assumption is that the ELBO objective of a diffusion model is invariant to the noise process (Kingma et al.,2021).","In this work, we dispel this assumption -- we propose multivariate learned adaptive noise (MuLAN), a learned diffusion process that applies Gaussian noise at different rates across an image.","Our method consists of three components -- a multivariate noise schedule, instance-conditional diffusion, and auxiliary variables -- which ensure that the learning objective is no longer invariant to the choice of the noise schedule as in previous works.","Our work is grounded in Bayesian inference and casts the learned diffusion process as an approximate variational posterior that yields a tighter lower bound on marginal likelihood.","Empirically, MuLAN sets a new state-of-the-art in density estimation on CIFAR-10 and ImageNet compared to classical diffusion.","Code is available at https://github.com/s-sahoo/MuLAN"],"url":"http://arxiv.org/abs/2312.13236v1"}
{"created":"2023-12-20 17:53:24","title":"Benchmarks for Retrospective Automated Driving System Crash Rate Analysis Using Police-Reported Crash Data","abstract":"With fully automated driving systems (ADS; SAE level 4) ride-hailing services expanding in the US, we are now approaching an inflection point, where the process of retrospectively evaluating ADS safety impact can start to yield statistically credible conclusions. An ADS safety impact measurement requires a comparison to a \"benchmark\" crash rate. This study aims to address, update, and extend the existing literature by leveraging police-reported crashes to generate human crash rates for multiple geographic areas with current ADS deployments. All of the data leveraged is publicly accessible, and the benchmark determination methodology is intended to be repeatable and transparent. Generating a benchmark that is comparable to ADS crash data is associated with certain challenges, including data selection, handling underreporting and reporting thresholds, identifying the population of drivers and vehicles to compare against, choosing an appropriate severity level to assess, and matching crash and mileage exposure data. Consequently, we identify essential steps when generating benchmarks, and present our analyses amongst a backdrop of existing ADS benchmark literature. One analysis presented is the usage of established underreporting correction methodology to publicly available human driver police-reported data to improve comparability to publicly available ADS crash data. We also identify important dependencies in controlling for geographic region, road type, and vehicle type, and show how failing to control for these features can bias results. This body of work aims to contribute to the ability of the community - researchers, regulators, industry, and experts - to reach consensus on how to estimate accurate benchmarks.","sentences":["With fully automated driving systems (ADS; SAE level 4) ride-hailing services expanding in the US, we are now approaching an inflection point, where the process of retrospectively evaluating ADS safety impact can start to yield statistically credible conclusions.","An ADS safety impact measurement requires a comparison to a \"benchmark\" crash rate.","This study aims to address, update, and extend the existing literature by leveraging police-reported crashes to generate human crash rates for multiple geographic areas with current ADS deployments.","All of the data leveraged is publicly accessible, and the benchmark determination methodology is intended to be repeatable and transparent.","Generating a benchmark that is comparable to ADS crash data is associated with certain challenges, including data selection, handling underreporting and reporting thresholds, identifying the population of drivers and vehicles to compare against, choosing an appropriate severity level to assess, and matching crash and mileage exposure data.","Consequently, we identify essential steps when generating benchmarks, and present our analyses amongst a backdrop of existing ADS benchmark literature.","One analysis presented is the usage of established underreporting correction methodology to publicly available human driver police-reported data to improve comparability to publicly available ADS crash data.","We also identify important dependencies in controlling for geographic region, road type, and vehicle type, and show how failing to control for these features can bias results.","This body of work aims to contribute to the ability of the community - researchers, regulators, industry, and experts - to reach consensus on how to estimate accurate benchmarks."],"url":"http://arxiv.org/abs/2312.13228v1"}
{"created":"2023-12-20 17:47:52","title":"Automated DevOps Pipeline Generation for Code Repositories using Large Language Models","abstract":"Automating software development processes through the orchestration of GitHub Action workflows has revolutionized the efficiency and agility of software delivery pipelines. This paper presents a detailed investigation into the use of Large Language Models (LLMs) specifically, GPT 3.5 and GPT 4 to generate and evaluate GitHub Action workflows for DevOps tasks. Our methodology involves data collection from public GitHub repositories, prompt engineering for LLM utilization, and evaluation metrics encompassing exact match scores, BLEU scores, and a novel DevOps Aware score. The research scrutinizes the proficiency of GPT 3.5 and GPT 4 in generating GitHub workflows, while assessing the influence of various prompt elements in constructing the most efficient pipeline. Results indicate substantial advancements in GPT 4, particularly in DevOps awareness and syntax correctness. The research introduces a GitHub App built on Probot, empowering users to automate workflow generation within GitHub ecosystem. This study contributes insights into the evolving landscape of AI-driven automation in DevOps practices.","sentences":["Automating software development processes through the orchestration of GitHub Action workflows has revolutionized the efficiency and agility of software delivery pipelines.","This paper presents a detailed investigation into the use of Large Language Models (LLMs) specifically, GPT 3.5 and GPT 4 to generate and evaluate GitHub Action workflows for DevOps tasks.","Our methodology involves data collection from public GitHub repositories, prompt engineering for LLM utilization, and evaluation metrics encompassing exact match scores, BLEU scores, and a novel DevOps Aware score.","The research scrutinizes the proficiency of GPT 3.5 and GPT 4 in generating GitHub workflows, while assessing the influence of various prompt elements in constructing the most efficient pipeline.","Results indicate substantial advancements in GPT 4, particularly in DevOps awareness and syntax correctness.","The research introduces a GitHub App built on Probot, empowering users to automate workflow generation within GitHub ecosystem.","This study contributes insights into the evolving landscape of AI-driven automation in DevOps practices."],"url":"http://arxiv.org/abs/2312.13225v1"}
{"created":"2023-12-20 17:46:48","title":"StableKD: Breaking Inter-block Optimization Entanglement for Stable Knowledge Distillation","abstract":"Knowledge distillation (KD) has been recognized as an effective tool to compress and accelerate models. However, current KD approaches generally suffer from an accuracy drop and/or an excruciatingly long distillation process. In this paper, we tackle the issue by first providing a new insight into a phenomenon that we call the Inter-Block Optimization Entanglement (IBOE), which makes the conventional end-to-end KD approaches unstable with noisy gradients. We then propose StableKD, a novel KD framework that breaks the IBOE and achieves more stable optimization. StableKD distinguishes itself through two operations: Decomposition and Recomposition, where the former divides a pair of teacher and student networks into several blocks for separate distillation, and the latter progressively merges them back, evolving towards end-to-end distillation. We conduct extensive experiments on CIFAR100, Imagewoof, and ImageNet datasets with various teacher-student pairs. Compared to other KD approaches, our simple yet effective StableKD greatly boosts the model accuracy by 1% ~ 18%, speeds up the convergence up to 10 times, and outperforms them with only 40% of the training data.","sentences":["Knowledge distillation (KD) has been recognized as an effective tool to compress and accelerate models.","However, current KD approaches generally suffer from an accuracy drop and/or an excruciatingly long distillation process.","In this paper, we tackle the issue by first providing a new insight into a phenomenon that we call the Inter-Block Optimization Entanglement (IBOE), which makes the conventional end-to-end KD approaches unstable with noisy gradients.","We then propose StableKD, a novel KD framework that breaks the IBOE and achieves more stable optimization.","StableKD","distinguishes itself through two operations: Decomposition and Recomposition, where the former divides a pair of teacher and student networks into several blocks for separate distillation, and the latter progressively merges them back, evolving towards end-to-end distillation.","We conduct extensive experiments on CIFAR100, Imagewoof, and ImageNet datasets with various teacher-student pairs.","Compared to other KD approaches, our simple yet effective StableKD","greatly boosts the model accuracy by 1% ~ 18%, speeds up the convergence up to 10 times, and outperforms them with only 40% of the training data."],"url":"http://arxiv.org/abs/2312.13223v1"}
{"created":"2023-12-20 17:36:36","title":"FiFAR: A Fraud Detection Dataset for Learning to Defer","abstract":"Public dataset limitations have significantly hindered the development and benchmarking of learning to defer (L2D) algorithms, which aim to optimally combine human and AI capabilities in hybrid decision-making systems. In such systems, human availability and domain-specific concerns introduce difficulties, while obtaining human predictions for training and evaluation is costly. Financial fraud detection is a high-stakes setting where algorithms and human experts often work in tandem; however, there are no publicly available datasets for L2D concerning this important application of human-AI teaming. To fill this gap in L2D research, we introduce the Financial Fraud Alert Review Dataset (FiFAR), a synthetic bank account fraud detection dataset, containing the predictions of a team of 50 highly complex and varied synthetic fraud analysts, with varied bias and feature dependence. We also provide a realistic definition of human work capacity constraints, an aspect of L2D systems that is often overlooked, allowing for extensive testing of assignment systems under real-world conditions. We use our dataset to develop a capacity-aware L2D method and rejection learning approach under realistic data availability conditions, and benchmark these baselines under an array of 300 distinct testing scenarios. We believe that this dataset will serve as a pivotal instrument in facilitating a systematic, rigorous, reproducible, and transparent evaluation and comparison of L2D methods, thereby fostering the development of more synergistic human-AI collaboration in decision-making systems. The public dataset and detailed synthetic expert information are available at: https://github.com/feedzai/fifar-dataset","sentences":["Public dataset limitations have significantly hindered the development and benchmarking of learning to defer (L2D) algorithms, which aim to optimally combine human and AI capabilities in hybrid decision-making systems.","In such systems, human availability and domain-specific concerns introduce difficulties, while obtaining human predictions for training and evaluation is costly.","Financial fraud detection is a high-stakes setting where algorithms and human experts often work in tandem; however, there are no publicly available datasets for L2D concerning this important application of human-AI teaming.","To fill this gap in L2D research, we introduce the Financial Fraud Alert Review Dataset (FiFAR), a synthetic bank account fraud detection dataset, containing the predictions of a team of 50 highly complex and varied synthetic fraud analysts, with varied bias and feature dependence.","We also provide a realistic definition of human work capacity constraints, an aspect of L2D systems that is often overlooked, allowing for extensive testing of assignment systems under real-world conditions.","We use our dataset to develop a capacity-aware L2D method and rejection learning approach under realistic data availability conditions, and benchmark these baselines under an array of 300 distinct testing scenarios.","We believe that this dataset will serve as a pivotal instrument in facilitating a systematic, rigorous, reproducible, and transparent evaluation and comparison of L2D methods, thereby fostering the development of more synergistic human-AI collaboration in decision-making systems.","The public dataset and detailed synthetic expert information are available at: https://github.com/feedzai/fifar-dataset"],"url":"http://arxiv.org/abs/2312.13218v1"}
{"created":"2023-12-20 16:45:26","title":"Task-oriented Semantics-aware Communications for Robotic Waypoint Transmission: the Value and Age of Information Approach","abstract":"The ultra-reliable and low-latency communication (URLLC) service of the fifth-generation (5G) mobile communication network struggles to support safe robot operation. Nowadays, the sixth-generation (6G) mobile communication network is proposed to provide hyper-reliable and low-latency communication to enable safer control for robots. However, current 5G/ 6G research mainly focused on improving communication performance, while the robotics community mostly assumed communication to be ideal. To jointly consider communication and robotic control with a focus on the specific robotic task, we propose task-oriented and semantics-aware communication in robotic control (TSRC) to exploit the context of data and its importance in achieving the task at both transmitter and receiver. At the transmitter, we propose a deep reinforcement learning algorithm to generate optimal control and command (C&C) data and a proactive repetition scheme (DeepPro) to increase the successful transmission probability. At the receiver, we design the value of information (VoI) and age of information (AoI) based queue ordering mechanism (VA-QOM) to reorganize the queue based on the semantic information extracted from the AoI and the VoI. The simulation results validate that our proposed TSRC framework achieves a 91.5% improvement in the mean square error compared to the traditional unmanned aerial vehicle control framework.","sentences":["The ultra-reliable and low-latency communication (URLLC) service of the fifth-generation (5G) mobile communication network struggles to support safe robot operation.","Nowadays, the sixth-generation (6G) mobile communication network is proposed to provide hyper-reliable and low-latency communication to enable safer control for robots.","However, current 5G/ 6G research mainly focused on improving communication performance, while the robotics community mostly assumed communication to be ideal.","To jointly consider communication and robotic control with a focus on the specific robotic task, we propose task-oriented and semantics-aware communication in robotic control (TSRC) to exploit the context of data and its importance in achieving the task at both transmitter and receiver.","At the transmitter, we propose a deep reinforcement learning algorithm to generate optimal control and command (C&C) data and a proactive repetition scheme (DeepPro) to increase the successful transmission probability.","At the receiver, we design the value of information (VoI) and age of information (AoI) based queue ordering mechanism (VA-QOM) to reorganize the queue based on the semantic information extracted from the AoI and the VoI.","The simulation results validate that our proposed TSRC framework achieves a 91.5% improvement in the mean square error compared to the traditional unmanned aerial vehicle control framework."],"url":"http://arxiv.org/abs/2312.13182v1"}
{"created":"2023-12-20 16:38:23","title":"$\\mathcal{O}(\\log\\log{n})$ Passes is Optimal for Semi-Streaming Maximal Independent Set","abstract":"In the semi-streaming model for processing massive graphs, an algorithm makes multiple passes over the edges of a given $n$-vertex graph and is tasked with computing the solution to a problem using $O(n \\cdot \\text{polylog}(n))$ space. Semi-streaming algorithms for Maximal Independent Set (MIS) that run in $O(\\log\\log{n})$ passes have been known for almost a decade, however, the best lower bounds can only rule out single-pass algorithms. We close this large gap by proving that the current algorithms are optimal: Any semi-streaming algorithm for finding an MIS with constant probability of success requires $\\Omega(\\log\\log{n})$ passes. This settles the complexity of this fundamental problem in the semi-streaming model, and constitutes one of the first optimal multi-pass lower bounds in this model.   We establish our result by proving an optimal round vs communication tradeoff for the (multi-party) communication complexity of MIS. The key ingredient of this result is a new technique, called hierarchical embedding, for performing round elimination: we show how to pack many but small hard $(r-1)$-round instances of the problem into a single $r$-round instance, in a way that enforces any $r$-round protocol to effectively solve all these $(r-1)$-round instances also. These embeddings are obtained via a novel application of results from extremal graph theory -- in particular dense graphs with many disjoint unique shortest paths -- together with a newly designed graph product, and are analyzed via information-theoretic tools such as direct-sum and message compression arguments.","sentences":["In the semi-streaming model for processing massive graphs, an algorithm makes multiple passes over the edges of a given $n$-vertex graph and is tasked with computing the solution to a problem using $O(n \\cdot \\text{polylog}(n))$ space.","Semi-streaming algorithms for Maximal Independent Set (MIS) that run in $O(\\log\\log{n})$ passes have been known for almost a decade, however, the best lower bounds can only rule out single-pass algorithms.","We close this large gap by proving that the current algorithms are optimal: Any semi-streaming algorithm for finding an MIS with constant probability of success requires $\\Omega(\\log\\log{n})$ passes.","This settles the complexity of this fundamental problem in the semi-streaming model, and constitutes one of the first optimal multi-pass lower bounds in this model.   ","We establish our result by proving an optimal round vs communication tradeoff for the (multi-party) communication complexity of MIS.","The key ingredient of this result is a new technique, called hierarchical embedding, for performing round elimination: we show how to pack many but small hard $(r-1)$-round instances of the problem into a single $r$-round instance, in a way that enforces any $r$-round protocol to effectively solve all these $(r-1)$-round instances also.","These embeddings are obtained via a novel application of results from extremal graph theory -- in particular dense graphs with many disjoint unique shortest paths -- together with a newly designed graph product, and are analyzed via information-theoretic tools such as direct-sum and message compression arguments."],"url":"http://arxiv.org/abs/2312.13178v1"}
{"created":"2023-12-20 16:33:15","title":"Learning Fair Policies for Multi-stage Selection Problems from Observational Data","abstract":"We consider the problem of learning fair policies for multi-stage selection problems from observational data. This problem arises in several high-stakes domains such as company hiring, loan approval, or bail decisions where outcomes (e.g., career success, loan repayment, recidivism) are only observed for those selected. We propose a multi-stage framework that can be augmented with various fairness constraints, such as demographic parity or equal opportunity. This problem is a highly intractable infinite chance-constrained program involving the unknown joint distribution of covariates and outcomes. Motivated by the potential impact of selection decisions on people's lives and livelihoods, we propose to focus on interpretable linear selection rules. Leveraging tools from causal inference and sample average approximation, we obtain an asymptotically consistent solution to this selection problem by solving a mixed binary conic optimization problem, which can be solved using standard off-the-shelf solvers. We conduct extensive computational experiments on a variety of datasets adapted from the UCI repository on which we show that our proposed approaches can achieve an 11.6% improvement in precision and a 38% reduction in the measure of unfairness compared to the existing selection policy.","sentences":["We consider the problem of learning fair policies for multi-stage selection problems from observational data.","This problem arises in several high-stakes domains such as company hiring, loan approval, or bail decisions where outcomes (e.g., career success, loan repayment, recidivism) are only observed for those selected.","We propose a multi-stage framework that can be augmented with various fairness constraints, such as demographic parity or equal opportunity.","This problem is a highly intractable infinite chance-constrained program involving the unknown joint distribution of covariates and outcomes.","Motivated by the potential impact of selection decisions on people's lives and livelihoods, we propose to focus on interpretable linear selection rules.","Leveraging tools from causal inference and sample average approximation, we obtain an asymptotically consistent solution to this selection problem by solving a mixed binary conic optimization problem, which can be solved using standard off-the-shelf solvers.","We conduct extensive computational experiments on a variety of datasets adapted from the UCI repository on which we show that our proposed approaches can achieve an 11.6% improvement in precision and a 38% reduction in the measure of unfairness compared to the existing selection policy."],"url":"http://arxiv.org/abs/2312.13173v1"}
{"created":"2023-12-20 16:18:51","title":"Gappy local conformal auto-encoders for heterogeneous data fusion: in praise of rigidity","abstract":"Fusing measurements from multiple, heterogeneous, partial sources, observing a common object or process, poses challenges due to the increasing availability of numbers and types of sensors. In this work we propose, implement and validate an end-to-end computational pipeline in the form of a multiple-auto-encoder neural network architecture for this task. The inputs to the pipeline are several sets of partial observations, and the result is a globally consistent latent space, harmonizing (rigidifying, fusing) all measurements. The key enabler is the availability of multiple slightly perturbed measurements of each instance:, local measurement, \"bursts\", that allows us to estimate the local distortion induced by each instrument. We demonstrate the approach in a sequence of examples, starting with simple two-dimensional data sets and proceeding to a Wi-Fi localization problem and to the solution of a \"dynamical puzzle\" arising in spatio-temporal observations of the solutions of Partial Differential Equations.","sentences":["Fusing measurements from multiple, heterogeneous, partial sources, observing a common object or process, poses challenges due to the increasing availability of numbers and types of sensors.","In this work we propose, implement and validate an end-to-end computational pipeline in the form of a multiple-auto-encoder neural network architecture for this task.","The inputs to the pipeline are several sets of partial observations, and the result is a globally consistent latent space, harmonizing (rigidifying, fusing) all measurements.","The key enabler is the availability of multiple slightly perturbed measurements of each instance:, local measurement, \"bursts\", that allows us to estimate the local distortion induced by each instrument.","We demonstrate the approach in a sequence of examples, starting with simple two-dimensional data sets and proceeding to a Wi-Fi localization problem and to the solution of a \"dynamical puzzle\" arising in spatio-temporal observations of the solutions of Partial Differential Equations."],"url":"http://arxiv.org/abs/2312.13155v1"}
{"created":"2023-12-20 16:16:29","title":"Neural Stochastic Differential Equations with Change Points: A Generative Adversarial Approach","abstract":"Stochastic differential equations (SDEs) have been widely used to model real world random phenomena. Existing works mainly focus on the case where the time series is modeled by a single SDE, which might be restrictive for modeling time series with distributional shift. In this work, we propose a change point detection algorithm for time series modeled as neural SDEs. Given a time series dataset, the proposed method jointly learns the unknown change points and the parameters of distinct neural SDE models corresponding to each change point. Specifically, the SDEs are learned under the framework of generative adversarial networks (GANs) and the change points are detected based on the output of the GAN discriminator in a forward pass. At each step of the proposed algorithm, the change points and the SDE model parameters are updated in an alternating fashion. Numerical results on both synthetic and real datasets are provided to validate the performance of our algorithm in comparison to classical change point detection benchmarks, standard GAN-based neural SDEs, and other state-of-the-art deep generative models for time series data.","sentences":["Stochastic differential equations (SDEs) have been widely used to model real world random phenomena.","Existing works mainly focus on the case where the time series is modeled by a single SDE, which might be restrictive for modeling time series with distributional shift.","In this work, we propose a change point detection algorithm for time series modeled as neural SDEs.","Given a time series dataset, the proposed method jointly learns the unknown change points and the parameters of distinct neural SDE models corresponding to each change point.","Specifically, the SDEs are learned under the framework of generative adversarial networks (GANs) and the change points are detected based on the output of the GAN discriminator in a forward pass.","At each step of the proposed algorithm, the change points and the SDE model parameters are updated in an alternating fashion.","Numerical results on both synthetic and real datasets are provided to validate the performance of our algorithm in comparison to classical change point detection benchmarks, standard GAN-based neural SDEs, and other state-of-the-art deep generative models for time series data."],"url":"http://arxiv.org/abs/2312.13152v1"}
{"created":"2023-12-20 16:08:29","title":"FLASH-TB: Integrating Arc-Flags and Trip-Based Public Transit Routing","abstract":"We present FLASH-TB, a journey planning algorithm for public transit networks that combines Trip-Based Public Transit Routing (TB) with the Arc-Flags speedup technique. The basic idea is simple: The network is partitioned into a configurable number of cells. For each cell and each possible transfer between two vehicles, the algorithm precomputes a flag that indicates whether the transfer is required to reach the cell. During a query, only flagged transfers are explored. Our algorithm improves upon previous attempts to apply Arc-Flags to public transit networks, which saw limited success due to conflicting rules for pruning the search space. We show that these rules can be reconciled while still producing correct results. Because the number of cells is configurable, FLASH-TB offers a tradeoff between query time and memory consumption. It is significantly more space-efficient than existing techniques with a comparable preprocessing time, which store generalized shortest-path trees: to match their query performance, it requires up to two orders of magnitude less memory. The fastest configuration of FLASH-TB achieves a speedup of more than two orders of magnitude over TB, offering sub-millisecond query times even on large countrywide networks.","sentences":["We present FLASH-TB, a journey planning algorithm for public transit networks that combines Trip-Based Public Transit Routing (TB) with the Arc-Flags speedup technique.","The basic idea is simple: The network is partitioned into a configurable number of cells.","For each cell and each possible transfer between two vehicles, the algorithm precomputes a flag that indicates whether the transfer is required to reach the cell.","During a query, only flagged transfers are explored.","Our algorithm improves upon previous attempts to apply Arc-Flags to public transit networks, which saw limited success due to conflicting rules for pruning the search space.","We show that these rules can be reconciled while still producing correct results.","Because the number of cells is configurable, FLASH-TB offers a tradeoff between query time and memory consumption.","It is significantly more space-efficient than existing techniques with a comparable preprocessing time, which store generalized shortest-path trees: to match their query performance, it requires up to two orders of magnitude less memory.","The fastest configuration of FLASH-TB achieves a speedup of more than two orders of magnitude over TB, offering sub-millisecond query times even on large countrywide networks."],"url":"http://arxiv.org/abs/2312.13146v1"}
{"created":"2023-12-20 16:04:02","title":"Underwater Acoustic Signal Recognition Based on Salient Features","abstract":"With the rapid advancement of technology, the recognition of underwater acoustic signals in complex environments has become increasingly crucial. Currently, mainstream underwater acoustic signal recognition relies primarily on time-frequency analysis to extract spectral features, finding widespread applications in the field. However, existing recognition methods heavily depend on expert systems, facing limitations such as restricted knowledge bases and challenges in handling complex relationships. These limitations stem from the complexity and maintenance difficulties associated with rules or inference engines. Recognizing the potential advantages of deep learning in handling intricate relationships, this paper proposes a method utilizing neural networks for underwater acoustic signal recognition. The proposed approach involves continual learning of features extracted from spectra for the classification of underwater acoustic signals. Deep learning models can automatically learn abstract features from data and continually adjust weights during training to enhance classification performance.","sentences":["With the rapid advancement of technology, the recognition of underwater acoustic signals in complex environments has become increasingly crucial.","Currently, mainstream underwater acoustic signal recognition relies primarily on time-frequency analysis to extract spectral features, finding widespread applications in the field.","However, existing recognition methods heavily depend on expert systems, facing limitations such as restricted knowledge bases and challenges in handling complex relationships.","These limitations stem from the complexity and maintenance difficulties associated with rules or inference engines.","Recognizing the potential advantages of deep learning in handling intricate relationships, this paper proposes a method utilizing neural networks for underwater acoustic signal recognition.","The proposed approach involves continual learning of features extracted from spectra for the classification of underwater acoustic signals.","Deep learning models can automatically learn abstract features from data and continually adjust weights during training to enhance classification performance."],"url":"http://arxiv.org/abs/2312.13143v1"}
{"created":"2023-12-20 16:02:25","title":"Augment on Manifold: Mixup Regularization with UMAP","abstract":"Data augmentation techniques play an important role in enhancing the performance of deep learning models. Despite their proven benefits in computer vision tasks, their application in the other domains remains limited. This paper proposes a Mixup regularization scheme, referred to as UMAP Mixup, designed for \"on-manifold\" automated data augmentation for deep learning predictive models. The proposed approach ensures that the Mixup operations result in synthesized samples that lie on the data manifold of the features and labels by utilizing a dimensionality reduction technique known as uniform manifold approximation and projection. Evaluations across diverse regression tasks show that UMAP Mixup is competitive with or outperforms other Mixup variants, show promise for its potential as an effective tool for enhancing the generalization performance of deep learning models.","sentences":["Data augmentation techniques play an important role in enhancing the performance of deep learning models.","Despite their proven benefits in computer vision tasks, their application in the other domains remains limited.","This paper proposes a Mixup regularization scheme, referred to as UMAP Mixup, designed for \"on-manifold\" automated data augmentation for deep learning predictive models.","The proposed approach ensures that the Mixup operations result in synthesized samples that lie on the data manifold of the features and labels by utilizing a dimensionality reduction technique known as uniform manifold approximation and projection.","Evaluations across diverse regression tasks show that UMAP Mixup is competitive with or outperforms other Mixup variants, show promise for its potential as an effective tool for enhancing the generalization performance of deep learning models."],"url":"http://arxiv.org/abs/2312.13141v1"}
{"created":"2023-12-20 16:00:43","title":"Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation","abstract":"Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io","sentences":["Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations.","In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training.","We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation.","GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states.","It predicts robot actions as well as future images in an end-to-end manner.","Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset.","We perform extensive experiments on the challenging CALVIN benchmark and a real robot.","On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%.","In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%.","In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects.","We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation.","Project page: https://GR1-Manipulation.github.io"],"url":"http://arxiv.org/abs/2312.13139v1"}
{"created":"2023-12-20 15:38:59","title":"Prometheus: Infrastructure Security Posture Analysis with AI-generated Attack Graphs","abstract":"The rampant occurrence of cybersecurity breaches imposes substantial limitations on the progress of network infrastructures, leading to compromised data, financial losses, potential harm to individuals, and disruptions in essential services. The current security landscape demands the urgent development of a holistic security assessment solution that encompasses vulnerability analysis and investigates the potential exploitation of these vulnerabilities as attack paths. In this paper, we propose Prometheus, an advanced system designed to provide a detailed analysis of the security posture of computing infrastructures. Using user-provided information, such as device details and software versions, Prometheus performs a comprehensive security assessment. This assessment includes identifying associated vulnerabilities and constructing potential attack graphs that adversaries can exploit. Furthermore, Prometheus evaluates the exploitability of these attack paths and quantifies the overall security posture through a scoring mechanism. The system takes a holistic approach by analyzing security layers encompassing hardware, system, network, and cryptography. Furthermore, Prometheus delves into the interconnections between these layers, exploring how vulnerabilities in one layer can be leveraged to exploit vulnerabilities in others. In this paper, we present the end-to-end pipeline implemented in Prometheus, showcasing the systematic approach adopted for conducting this thorough security analysis.","sentences":["The rampant occurrence of cybersecurity breaches imposes substantial limitations on the progress of network infrastructures, leading to compromised data, financial losses, potential harm to individuals, and disruptions in essential services.","The current security landscape demands the urgent development of a holistic security assessment solution that encompasses vulnerability analysis and investigates the potential exploitation of these vulnerabilities as attack paths.","In this paper, we propose Prometheus, an advanced system designed to provide a detailed analysis of the security posture of computing infrastructures.","Using user-provided information, such as device details and software versions, Prometheus performs a comprehensive security assessment.","This assessment includes identifying associated vulnerabilities and constructing potential attack graphs that adversaries can exploit.","Furthermore, Prometheus evaluates the exploitability of these attack paths and quantifies the overall security posture through a scoring mechanism.","The system takes a holistic approach by analyzing security layers encompassing hardware, system, network, and cryptography.","Furthermore, Prometheus delves into the interconnections between these layers, exploring how vulnerabilities in one layer can be leveraged to exploit vulnerabilities in others.","In this paper, we present the end-to-end pipeline implemented in Prometheus, showcasing the systematic approach adopted for conducting this thorough security analysis."],"url":"http://arxiv.org/abs/2312.13119v1"}
{"created":"2023-12-20 15:22:34","title":"Optimizing Ego Vehicle Trajectory Prediction: The Graph Enhancement Approach","abstract":"Predicting the trajectory of an ego vehicle is a critical component of autonomous driving systems. Current state-of-the-art methods typically rely on Deep Neural Networks (DNNs) and sequential models to process front-view images for future trajectory prediction. However, these approaches often struggle with perspective issues affecting object features in the scene. To address this, we advocate for the use of Bird's Eye View (BEV) perspectives, which offer unique advantages in capturing spatial relationships and object homogeneity. In our work, we leverage Graph Neural Networks (GNNs) and positional encoding to represent objects in a BEV, achieving competitive performance compared to traditional DNN-based methods. While the BEV-based approach loses some detailed information inherent to front-view images, we balance this by enriching the BEV data by representing it as a graph where relationships between the objects in a scene are captured effectively.","sentences":["Predicting the trajectory of an ego vehicle is a critical component of autonomous driving systems.","Current state-of-the-art methods typically rely on Deep Neural Networks (DNNs) and sequential models to process front-view images for future trajectory prediction.","However, these approaches often struggle with perspective issues affecting object features in the scene.","To address this, we advocate for the use of Bird's Eye View (BEV) perspectives, which offer unique advantages in capturing spatial relationships and object homogeneity.","In our work, we leverage Graph Neural Networks (GNNs) and positional encoding to represent objects in a BEV, achieving competitive performance compared to traditional DNN-based methods.","While the BEV-based approach loses some detailed information inherent to front-view images, we balance this by enriching the BEV data by representing it as a graph where relationships between the objects in a scene are captured effectively."],"url":"http://arxiv.org/abs/2312.13104v1"}
{"created":"2023-12-20 15:20:33","title":"Exploring Multimodal Large Language Models for Radiology Report Error-checking","abstract":"This paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports. We created an evaluation dataset from two real-world radiology datasets (MIMIC-CXR and IU-Xray), with 1,000 subsampled reports each. A subset of original reports was modified to contain synthetic errors by introducing various type of mistakes. The evaluation contained two difficulty levels: SIMPLE for binary error-checking and COMPLEX for identifying error types. LLaVA (Large Language and Visual Assistant) variant models, including our instruction-tuned model, were used for the evaluation. Additionally, a domain expert evaluation was conducted on a small test set. At the SIMPLE level, the LLaVA v1.5 model outperformed other publicly available models. Instruction tuning significantly enhanced performance by 47.4% and 25.4% on MIMIC-CXR and IU-Xray data, respectively. The model also surpassed the domain experts accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets (N=21) of the test set where a clinician did not achieve the correct conclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases. This study marks a promising step toward utilizing multi-modal LLMs to enhance diagnostic accuracy in radiology. The ensemble model demonstrated comparable performance to clinicians, even capturing errors overlooked by humans. Nevertheless, future work is needed to improve the model ability to identify the types of inconsistency.","sentences":["This paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports.","We created an evaluation dataset from two real-world radiology datasets (MIMIC-CXR and IU-Xray), with 1,000 subsampled reports each.","A subset of original reports was modified to contain synthetic errors by introducing various type of mistakes.","The evaluation contained two difficulty levels: SIMPLE for binary error-checking and COMPLEX for identifying error types.","LLaVA (Large Language and Visual Assistant) variant models, including our instruction-tuned model, were used for the evaluation.","Additionally, a domain expert evaluation was conducted on a small test set.","At the SIMPLE level, the LLaVA v1.5 model outperformed other publicly available models.","Instruction tuning significantly enhanced performance by 47.4% and 25.4% on MIMIC-CXR and IU-Xray data, respectively.","The model also surpassed the domain experts accuracy in the MIMIC-CXR dataset by 1.67%.","Notably, among the subsets (N=21) of the test set where a clinician did not achieve the correct conclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases.","This study marks a promising step toward utilizing multi-modal LLMs to enhance diagnostic accuracy in radiology.","The ensemble model demonstrated comparable performance to clinicians, even capturing errors overlooked by humans.","Nevertheless, future work is needed to improve the model ability to identify the types of inconsistency."],"url":"http://arxiv.org/abs/2312.13103v1"}
{"created":"2023-12-20 15:20:25","title":"SpecNeRF: Gaussian Directional Encoding for Specular Reflections","abstract":"Neural radiance fields have achieved remarkable performance in modeling the appearance of 3D scenes. However, existing approaches still struggle with the view-dependent appearance of glossy surfaces, especially under complex lighting of indoor environments. Unlike existing methods, which typically assume distant lighting like an environment map, we propose a learnable Gaussian directional encoding to better model the view-dependent effects under near-field lighting conditions. Importantly, our new directional encoding captures the spatially-varying nature of near-field lighting and emulates the behavior of prefiltered environment maps. As a result, it enables the efficient evaluation of preconvolved specular color at any 3D location with varying roughness coefficients. We further introduce a data-driven geometry prior that helps alleviate the shape radiance ambiguity in reflection modeling. We show that our Gaussian directional encoding and geometry prior significantly improve the modeling of challenging specular reflections in neural radiance fields, which helps decompose appearance into more physically meaningful components.","sentences":["Neural radiance fields have achieved remarkable performance in modeling the appearance of 3D scenes.","However, existing approaches still struggle with the view-dependent appearance of glossy surfaces, especially under complex lighting of indoor environments.","Unlike existing methods, which typically assume distant lighting like an environment map, we propose a learnable Gaussian directional encoding to better model the view-dependent effects under near-field lighting conditions.","Importantly, our new directional encoding captures the spatially-varying nature of near-field lighting and emulates the behavior of prefiltered environment maps.","As a result, it enables the efficient evaluation of preconvolved specular color at any 3D location with varying roughness coefficients.","We further introduce a data-driven geometry prior that helps alleviate the shape radiance ambiguity in reflection modeling.","We show that our Gaussian directional encoding and geometry prior significantly improve the modeling of challenging specular reflections in neural radiance fields, which helps decompose appearance into more physically meaningful components."],"url":"http://arxiv.org/abs/2312.13102v1"}
{"created":"2023-12-20 15:18:51","title":"SEER-ZSL: Semantic Encoder-Enhanced Representations for Generalized Zero-Shot Learning","abstract":"Generalized Zero-Shot Learning (GZSL) recognizes unseen classes by transferring knowledge from the seen classes, depending on the inherent interactions between visual and semantic data. However, the discrepancy between well-prepared training data and unpredictable real-world test scenarios remains a significant challenge. This paper introduces a dual strategy to address the generalization gap. Firstly, we incorporate semantic information through an innovative encoder. This encoder effectively integrates class-specific semantic information by targeting the performance disparity, enhancing the produced features to enrich the semantic space for class-specific attributes. Secondly, we refine our generative capabilities using a novel compositional loss function. This approach generates discriminative classes, effectively classifying both seen and unseen classes. In addition, we extend the exploitation of the learned latent space by utilizing controlled semantic inputs, ensuring the robustness of the model in varying environments. This approach yields a model that outperforms the state-of-the-art models in terms of both generalization and diverse settings, notably without requiring hyperparameter tuning or domain-specific adaptations. We also propose a set of novel evaluation metrics to provide a more detailed assessment of the reliability and reproducibility of the results. The complete code is made available on https://github.com/william-heyden/SEER-ZeroShotLearning/.","sentences":["Generalized Zero-Shot Learning (GZSL) recognizes unseen classes by transferring knowledge from the seen classes, depending on the inherent interactions between visual and semantic data.","However, the discrepancy between well-prepared training data and unpredictable real-world test scenarios remains a significant challenge.","This paper introduces a dual strategy to address the generalization gap.","Firstly, we incorporate semantic information through an innovative encoder.","This encoder effectively integrates class-specific semantic information by targeting the performance disparity, enhancing the produced features to enrich the semantic space for class-specific attributes.","Secondly, we refine our generative capabilities using a novel compositional loss function.","This approach generates discriminative classes, effectively classifying both seen and unseen classes.","In addition, we extend the exploitation of the learned latent space by utilizing controlled semantic inputs, ensuring the robustness of the model in varying environments.","This approach yields a model that outperforms the state-of-the-art models in terms of both generalization and diverse settings, notably without requiring hyperparameter tuning or domain-specific adaptations.","We also propose a set of novel evaluation metrics to provide a more detailed assessment of the reliability and reproducibility of the results.","The complete code is made available on https://github.com/william-heyden/SEER-ZeroShotLearning/."],"url":"http://arxiv.org/abs/2312.13100v1"}
{"created":"2023-12-20 15:12:53","title":"MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading","abstract":"Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods. We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrisic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects. The project website and the dataset are available on the following link: https://ubisoftlaforge.github.io/character/mosar","sentences":["Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem.","Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle.","Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion.","Moreover, training solely with this type of data leads to poor generalization with in-the-wild images.","This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images.","We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets.","This is achieved using a novel differentiable shading formulation.","We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars.","As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods.","We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrisic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects.","The project website and the dataset are available on the following link: https://ubisoftlaforge.github.io/character/mosar"],"url":"http://arxiv.org/abs/2312.13091v1"}
{"created":"2023-12-20 15:04:52","title":"Pyreal: A Framework for Interpretable ML Explanations","abstract":"Users in many domains use machine learning (ML) predictions to help them make decisions. Effective ML-based decision-making often requires explanations of ML models and their predictions. While there are many algorithms that explain models, generating explanations in a format that is comprehensible and useful to decision-makers is a nontrivial task that can require extensive development overhead. We developed Pyreal, a highly extensible system with a corresponding Python implementation for generating a variety of interpretable ML explanations. Pyreal converts data and explanations between the feature spaces expected by the model, relevant explanation algorithms, and human users, allowing users to generate interpretable explanations in a low-code manner. Our studies demonstrate that Pyreal generates more useful explanations than existing systems while remaining both easy-to-use and efficient.","sentences":["Users in many domains use machine learning (ML) predictions to help them make decisions.","Effective ML-based decision-making often requires explanations of ML models and their predictions.","While there are many algorithms that explain models, generating explanations in a format that is comprehensible and useful to decision-makers is a nontrivial task that can require extensive development overhead.","We developed Pyreal, a highly extensible system with a corresponding Python implementation for generating a variety of interpretable ML explanations.","Pyreal converts data and explanations between the feature spaces expected by the model, relevant explanation algorithms, and human users, allowing users to generate interpretable explanations in a low-code manner.","Our studies demonstrate that Pyreal generates more useful explanations than existing systems while remaining both easy-to-use and efficient."],"url":"http://arxiv.org/abs/2312.13084v1"}
{"created":"2023-12-20 14:56:09","title":"How to Integrate Digital Twin and Virtual Reality in Robotics Systems? Design and Implementation for Providing Robotics Maintenance Services in Data Centers","abstract":"In the context of Industry 4.0, the physical and digital worlds are closely connected, and robots are widely used to achieve system automation. Digital twin solutions have contributed significantly to the growth of Industry 4.0. Combining various technologies is a trend that aims to improve system performance. For example, digital twinning can be combined with virtual reality in automated systems. This paper proposes a new concept to articulate this combination, which has mainly been implemented in engineering research projects. However, there are currently no guidelines, plans, or concepts to articulate this combination. The concept will be implemented in data centers, which are crucial for enabling virtual tasks in our daily lives. Due to the COVID-19 pandemic, there has been a surge in demand for services such as e-commerce and videoconferencing. Regular maintenance is necessary to ensure uninterrupted and reliable services. Manual maintenance strategies may not be sufficient to meet the current high demand, and innovative approaches are needed to address the problem. This paper presents a novel approach to data center maintenance: real-time monitoring by an autonomous robot. The robot is integrated with digital twins of assets and a virtual reality interface that allows human personnel to control it and respond to alarms. This methodology enables faster, more cost-effective, and higher quality data center maintenance. It has been validated in a real data centre and can be used for intelligent monitoring and management through joint data sources. The method has potential applications in other automated systems.","sentences":["In the context of Industry 4.0, the physical and digital worlds are closely connected, and robots are widely used to achieve system automation.","Digital twin solutions have contributed significantly to the growth of Industry 4.0.","Combining various technologies is a trend that aims to improve system performance.","For example, digital twinning can be combined with virtual reality in automated systems.","This paper proposes a new concept to articulate this combination, which has mainly been implemented in engineering research projects.","However, there are currently no guidelines, plans, or concepts to articulate this combination.","The concept will be implemented in data centers, which are crucial for enabling virtual tasks in our daily lives.","Due to the COVID-19 pandemic, there has been a surge in demand for services such as e-commerce and videoconferencing.","Regular maintenance is necessary to ensure uninterrupted and reliable services.","Manual maintenance strategies may not be sufficient to meet the current high demand, and innovative approaches are needed to address the problem.","This paper presents a novel approach to data center maintenance: real-time monitoring by an autonomous robot.","The robot is integrated with digital twins of assets and a virtual reality interface that allows human personnel to control it and respond to alarms.","This methodology enables faster, more cost-effective, and higher quality data center maintenance.","It has been validated in a real data centre and can be used for intelligent monitoring and management through joint data sources.","The method has potential applications in other automated systems."],"url":"http://arxiv.org/abs/2312.13076v1"}
{"created":"2023-12-20 14:52:07","title":"Point Deformable Network with Enhanced Normal Embedding for Point Cloud Analysis","abstract":"Recently MLP-based methods have shown strong performance in point cloud analysis. Simple MLP architectures are able to learn geometric features in local point groups yet fail to model long-range dependencies directly. In this paper, we propose Point Deformable Network (PDNet), a concise MLP-based network that can capture long-range relations with strong representation ability. Specifically, we put forward Point Deformable Aggregation Module (PDAM) to improve representation capability in both long-range dependency and adaptive aggregation among points. For each query point, PDAM aggregates information from deformable reference points rather than points in limited local areas. The deformable reference points are generated data-dependent, and we initialize them according to the input point positions. Additional offsets and modulation scalars are learned on the whole point features, which shift the deformable reference points to the regions of interest. We also suggest estimating the normal vector for point clouds and applying Enhanced Normal Embedding (ENE) to the geometric extractors to improve the representation ability of single-point. Extensive experiments and ablation studies on various benchmarks demonstrate the effectiveness and superiority of our PDNet.","sentences":["Recently MLP-based methods have shown strong performance in point cloud analysis.","Simple MLP architectures are able to learn geometric features in local point groups yet fail to model long-range dependencies directly.","In this paper, we propose Point Deformable Network (PDNet), a concise MLP-based network that can capture long-range relations with strong representation ability.","Specifically, we put forward Point Deformable Aggregation Module (PDAM) to improve representation capability in both long-range dependency and adaptive aggregation among points.","For each query point, PDAM aggregates information from deformable reference points rather than points in limited local areas.","The deformable reference points are generated data-dependent, and we initialize them according to the input point positions.","Additional offsets and modulation scalars are learned on the whole point features, which shift the deformable reference points to the regions of interest.","We also suggest estimating the normal vector for point clouds and applying Enhanced Normal Embedding (ENE) to the geometric extractors to improve the representation ability of single-point.","Extensive experiments and ablation studies on various benchmarks demonstrate the effectiveness and superiority of our PDNet."],"url":"http://arxiv.org/abs/2312.13071v1"}
{"created":"2023-12-20 14:09:13","title":"Advancing SQL Injection Detection for High-Speed Data Centers: A Novel Approach Using Cascaded NLP","abstract":"Detecting SQL Injection (SQLi) attacks is crucial for web-based data center security, but it is challenging to balance accuracy and computational efficiency, especially in high-speed networks. Traditional methods struggle with this balance, while NLP-based approaches, although accurate, are computationally intensive.   We introduce a novel cascade SQLi detection method, blending classical and transformer-based NLP models, achieving a 99.86% detection accuracy with significantly lower computational demands-20 times faster than using transformer-based models alone. Our approach is tested in a realistic setting and compared with 35 other methods, including Machine Learning-based and transformer models like BERT, on a dataset of over 30,000 SQL sentences.   Our results show that this hybrid method effectively detects SQLi in high-traffic environments, offering efficient and accurate protection against SQLi vulnerabilities with computational efficiency. The code is available at https://github.com/gdrlab/cascaded-sqli-detection .","sentences":["Detecting SQL Injection (SQLi) attacks is crucial for web-based data center security, but it is challenging to balance accuracy and computational efficiency, especially in high-speed networks.","Traditional methods struggle with this balance, while NLP-based approaches, although accurate, are computationally intensive.   ","We introduce a novel cascade SQLi detection method, blending classical and transformer-based NLP models, achieving a 99.86% detection accuracy with significantly lower computational demands-20 times faster than using transformer-based models alone.","Our approach is tested in a realistic setting and compared with 35 other methods, including Machine Learning-based and transformer models like BERT, on a dataset of over 30,000 SQL sentences.   ","Our results show that this hybrid method effectively detects SQLi in high-traffic environments, offering efficient and accurate protection against SQLi vulnerabilities with computational efficiency.","The code is available at https://github.com/gdrlab/cascaded-sqli-detection ."],"url":"http://arxiv.org/abs/2312.13041v1"}
{"created":"2023-12-20 14:04:57","title":"AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting","abstract":"Automated machine learning (AutoML) streamlines the creation of ML models. While most methods select the \"best\" model based on predictive quality, it's crucial to acknowledge other aspects, such as interpretability and resource consumption. This holds particular importance in the context of deep neural networks (DNNs), as these models are often perceived as computationally intensive black boxes. In the challenging domain of time series forecasting, DNNs achieve stunning results, but specialized approaches for automatically selecting models are scarce. In this paper, we propose AutoXPCR - a novel method for automated and explainable multi-objective model selection. Our approach leverages meta-learning to estimate any model's performance along PCR criteria, which encompass (P)redictive error, (C)omplexity, and (R)esource demand. Explainability is addressed on multiple levels, as our interactive framework can prioritize less complex models and provide by-product explanations of recommendations. We demonstrate practical feasibility by deploying AutoXPCR on over 1000 configurations across 114 data sets from various domains. Our method clearly outperforms other model selection approaches - on average, it only requires 20% of computation costs for recommending models with 90% of the best-possible quality.","sentences":["Automated machine learning (AutoML) streamlines the creation of ML models.","While most methods select the \"best\" model based on predictive quality, it's crucial to acknowledge other aspects, such as interpretability and resource consumption.","This holds particular importance in the context of deep neural networks (DNNs), as these models are often perceived as computationally intensive black boxes.","In the challenging domain of time series forecasting, DNNs achieve stunning results, but specialized approaches for automatically selecting models are scarce.","In this paper, we propose AutoXPCR - a novel method for automated and explainable multi-objective model selection.","Our approach leverages meta-learning to estimate any model's performance along PCR criteria, which encompass (P)redictive error, (C)omplexity, and (R)esource demand.","Explainability is addressed on multiple levels, as our interactive framework can prioritize less complex models and provide by-product explanations of recommendations.","We demonstrate practical feasibility by deploying AutoXPCR on over 1000 configurations across 114 data sets from various domains.","Our method clearly outperforms other model selection approaches - on average, it only requires 20% of computation costs for recommending models with 90% of the best-possible quality."],"url":"http://arxiv.org/abs/2312.13038v1"}
{"created":"2023-12-20 13:55:56","title":"A self-attention-based differentially private tabular GAN with high data utility","abstract":"Generative Adversarial Networks (GANs) have become a ubiquitous technology for data generation, with their prowess in image generation being well-established. However, their application in generating tabular data has been less than ideal. Furthermore, attempting to incorporate differential privacy technology into these frameworks has often resulted in a degradation of data utility. To tackle these challenges, this paper introduces DP-SACTGAN, a novel Conditional Generative Adversarial Network (CGAN) framework for differentially private tabular data generation, aiming to surmount these obstacles. Experimental findings demonstrate that DP-SACTGAN not only accurately models the distribution of the original data but also effectively satisfies the requirements of differential privacy.","sentences":["Generative Adversarial Networks (GANs) have become a ubiquitous technology for data generation, with their prowess in image generation being well-established.","However, their application in generating tabular data has been less than ideal.","Furthermore, attempting to incorporate differential privacy technology into these frameworks has often resulted in a degradation of data utility.","To tackle these challenges, this paper introduces DP-SACTGAN, a novel Conditional Generative Adversarial Network (CGAN) framework for differentially private tabular data generation, aiming to surmount these obstacles.","Experimental findings demonstrate that DP-SACTGAN not only accurately models the distribution of the original data but also effectively satisfies the requirements of differential privacy."],"url":"http://arxiv.org/abs/2312.13031v1"}
{"created":"2023-12-20 13:50:26","title":"Doubly Perturbed Task-Free Continual Learning","abstract":"Task-free online continual learning (TF-CL) is a challenging problem where the model incrementally learns tasks without explicit task information. Although training with entire data from the past, present as well as future is considered as the gold standard, naive approaches in TF-CL with the current samples may be conflicted with learning with samples in the future, leading to catastrophic forgetting and poor plasticity. Thus, a proactive consideration of an unseen future sample in TF-CL becomes imperative. Motivated by this intuition, we propose a novel TF-CL framework considering future samples and show that injecting adversarial perturbations on both input data and decision-making is effective. Then, we propose a novel method named Doubly Perturbed Continual Learning (DPCL) to efficiently implement these input and decision-making perturbations. Specifically, for input perturbation, we propose an approximate perturbation method that injects noise into the input data as well as the feature vector and then interpolates the two perturbed samples. For decision-making process perturbation, we devise multiple stochastic classifiers. We also investigate a memory management scheme and learning rate scheduling reflecting our proposed double perturbations. We demonstrate that our proposed method outperforms the state-of-the-art baseline methods by large margins on various TF-CL benchmarks.","sentences":["Task-free online continual learning (TF-CL) is a challenging problem where the model incrementally learns tasks without explicit task information.","Although training with entire data from the past, present as well as future is considered as the gold standard, naive approaches in TF-CL with the current samples may be conflicted with learning with samples in the future, leading to catastrophic forgetting and poor plasticity.","Thus, a proactive consideration of an unseen future sample in TF-CL becomes imperative.","Motivated by this intuition, we propose a novel TF-CL framework considering future samples and show that injecting adversarial perturbations on both input data and decision-making is effective.","Then, we propose a novel method named Doubly Perturbed Continual Learning (DPCL) to efficiently implement these input and decision-making perturbations.","Specifically, for input perturbation, we propose an approximate perturbation method that injects noise into the input data as well as the feature vector and then interpolates the two perturbed samples.","For decision-making process perturbation, we devise multiple stochastic classifiers.","We also investigate a memory management scheme and learning rate scheduling reflecting our proposed double perturbations.","We demonstrate that our proposed method outperforms the state-of-the-art baseline methods by large margins on various TF-CL benchmarks."],"url":"http://arxiv.org/abs/2312.13027v1"}
{"created":"2023-12-20 13:01:25","title":"Accelerator-driven Data Arrangement to Minimize Transformers Run-time on Multi-core Architectures","abstract":"The increasing complexity of transformer models in artificial intelligence expands their computational costs, memory usage, and energy consumption. Hardware acceleration tackles the ensuing challenges by designing processors and accelerators tailored for transformer models, supporting their computation hotspots with high efficiency. However, memory bandwidth can hinder improvements in hardware accelerators. Against this backdrop, in this paper we propose a novel memory arrangement strategy, governed by the hardware accelerator's kernel size, which effectively minimizes off-chip data access. This arrangement is particularly beneficial for end-to-end transformer model inference, where most of the computation is based on general matrix multiplication (GEMM) operations. Additionally, we address the overhead of non-GEMM operations in transformer models within the scope of this memory data arrangement. Our study explores the implementation and effectiveness of the proposed accelerator-driven data arrangement approach in both single- and multi-core systems. Our evaluation demonstrates that our approach can achieve up to a 2.8x speed increase when executing inferences employing state-of-the-art transformers.","sentences":["The increasing complexity of transformer models in artificial intelligence expands their computational costs, memory usage, and energy consumption.","Hardware acceleration tackles the ensuing challenges by designing processors and accelerators tailored for transformer models, supporting their computation hotspots with high efficiency.","However, memory bandwidth can hinder improvements in hardware accelerators.","Against this backdrop, in this paper we propose a novel memory arrangement strategy, governed by the hardware accelerator's kernel size, which effectively minimizes off-chip data access.","This arrangement is particularly beneficial for end-to-end transformer model inference, where most of the computation is based on general matrix multiplication (GEMM) operations.","Additionally, we address the overhead of non-GEMM operations in transformer models within the scope of this memory data arrangement.","Our study explores the implementation and effectiveness of the proposed accelerator-driven data arrangement approach in both single- and multi-core systems.","Our evaluation demonstrates that our approach can achieve up to a 2.8x speed increase when executing inferences employing state-of-the-art transformers."],"url":"http://arxiv.org/abs/2312.13000v1"}
{"created":"2023-12-20 12:59:31","title":"Machine Mindset: An MBTI Exploration of Large Language Models","abstract":"We present a novel approach for integrating Myers-Briggs Type Indicator (MBTI) personality traits into large language models (LLMs), addressing the challenges of personality consistency in personalized AI. Our method, \"Machine Mindset,\" involves a two-phase fine-tuning and Direct Preference Optimization (DPO) to embed MBTI traits into LLMs. This approach ensures that models internalize these traits, offering a stable and consistent personality profile. We demonstrate the effectiveness of our models across various domains, showing alignment between model performance and their respective MBTI traits. The paper highlights significant contributions in the development of personality datasets and a new training methodology for personality integration in LLMs, enhancing the potential for personalized AI applications. We also open-sourced our model and part of the data at \\url{https://github.com/PKU-YuanGroup/Machine-Mindset}.","sentences":["We present a novel approach for integrating Myers-Briggs Type Indicator (MBTI) personality traits into large language models (LLMs), addressing the challenges of personality consistency in personalized AI.","Our method, \"Machine Mindset,\" involves a two-phase fine-tuning and Direct Preference Optimization (DPO) to embed MBTI traits into LLMs.","This approach ensures that models internalize these traits, offering a stable and consistent personality profile.","We demonstrate the effectiveness of our models across various domains, showing alignment between model performance and their respective MBTI traits.","The paper highlights significant contributions in the development of personality datasets and a new training methodology for personality integration in LLMs, enhancing the potential for personalized AI applications.","We also open-sourced our model and part of the data at \\url{https://github.com/PKU-YuanGroup/Machine-Mindset}."],"url":"http://arxiv.org/abs/2312.12999v1"}
{"created":"2023-12-20 12:46:44","title":"Benchmarking and Analyzing In-context Learning, Fine-tuning and Supervised Learning for Biomedical Knowledge Curation: a focused study on chemical entities of biological interest","abstract":"Automated knowledge curation for biomedical ontologies is key to ensure that they remain comprehensive, high-quality and up-to-date. In the era of foundational language models, this study compares and analyzes three NLP paradigms for curation tasks: in-context learning (ICL), fine-tuning (FT), and supervised learning (ML). Using the Chemical Entities of Biological Interest (ChEBI) database as a model ontology, three curation tasks were devised. For ICL, three prompting strategies were employed with GPT-4, GPT-3.5, BioGPT. PubmedBERT was chosen for the FT paradigm. For ML, six embedding models were utilized for training Random Forest and Long-Short Term Memory models. Five setups were designed to assess ML and FT model performance across different data availability scenarios.Datasets for curation tasks included: task 1 (620,386), task 2 (611,430), and task 3 (617,381), maintaining a 50:50 positive versus negative ratio. For ICL models, GPT-4 achieved best accuracy scores of 0.916, 0.766 and 0.874 for tasks 1-3 respectively. In a direct comparison, ML (trained on ~260,000 triples) outperformed ICL in accuracy across all tasks. (accuracy differences: +.11, +.22 and +.17). Fine-tuned PubmedBERT performed similarly to leading ML models in tasks 1 & 2 (F1 differences: -.014 and +.002), but worse in task 3 (-.048). Simulations revealed performance declines in both ML and FT models with smaller and higher imbalanced training data. where ICL (particularly GPT-4) excelled in tasks 1 & 3. GPT-4 excelled in tasks 1 and 3 with less than 6,000 triples, surpassing ML/FT. ICL underperformed ML/FT in task 2.ICL-augmented foundation models can be good assistants for knowledge curation with correct prompting, however, not making ML and FT paradigms obsolete. The latter two require task-specific data to beat ICL. In such cases, ML relies on small pretrained embeddings, minimizing computational demands.","sentences":["Automated knowledge curation for biomedical ontologies is key to ensure that they remain comprehensive, high-quality and up-to-date.","In the era of foundational language models, this study compares and analyzes three NLP paradigms for curation tasks: in-context learning (ICL), fine-tuning (FT), and supervised learning (ML).","Using the Chemical Entities of Biological Interest (ChEBI) database as a model ontology, three curation tasks were devised.","For ICL, three prompting strategies were employed with GPT-4, GPT-3.5, BioGPT.","PubmedBERT was chosen for the FT paradigm.","For ML, six embedding models were utilized for training Random Forest and Long-Short Term Memory models.","Five setups were designed to assess ML and FT model performance across different data availability scenarios.","Datasets for curation tasks included: task 1 (620,386), task 2 (611,430), and task 3 (617,381), maintaining a 50:50 positive versus negative ratio.","For ICL models, GPT-4 achieved best accuracy scores of 0.916, 0.766 and 0.874 for tasks 1-3 respectively.","In a direct comparison, ML (trained on ~260,000 triples) outperformed ICL in accuracy across all tasks.","(accuracy differences: +.11, +.22 and +.17).","Fine-tuned PubmedBERT performed similarly to leading ML models in tasks 1 & 2 (F1 differences: -.014 and +.002), but worse in task 3 (-.048).","Simulations revealed performance declines in both ML and FT models with smaller and higher imbalanced training data.","where ICL (particularly GPT-4) excelled in tasks 1 & 3.","GPT-4 excelled in tasks 1 and 3 with less than 6,000 triples, surpassing ML/FT.","ICL underperformed ML/FT in task 2.ICL-augmented foundation models can be good assistants for knowledge curation with correct prompting, however, not making ML and FT paradigms obsolete.","The latter two require task-specific data to beat ICL.","In such cases, ML relies on small pretrained embeddings, minimizing computational demands."],"url":"http://arxiv.org/abs/2312.12989v1"}
{"created":"2023-12-20 12:34:54","title":"Collaborative Optimization of the Age of Information under Partial Observability","abstract":"The significance of the freshness of sensor and control data at the receiver side, often referred to as Age of Information (AoI), is fundamentally constrained by contention for limited network resources. Evidently, network congestion is detrimental for AoI, where this congestion is partly self-induced by the sensor transmission process in addition to the contention from other transmitting sensors. In this work, we devise a decentralized AoI-minimizing transmission policy for a number of sensor agents sharing capacity-limited, non-FIFO duplex channels that introduce random delays in communication with a common receiver. By implementing the same policy, however with no explicit inter-agent communication, the agents minimize the expected AoI in this partially observable system. We cater to the partial observability due to random channel delays by designing a bootstrap particle filter that independently maintains a belief over the AoI of each agent. We also leverage mean-field control approximations and reinforcement learning to derive scalable and optimal solutions for minimizing the expected AoI collaboratively.","sentences":["The significance of the freshness of sensor and control data at the receiver side, often referred to as Age of Information (AoI), is fundamentally constrained by contention for limited network resources.","Evidently, network congestion is detrimental for AoI, where this congestion is partly self-induced by the sensor transmission process in addition to the contention from other transmitting sensors.","In this work, we devise a decentralized AoI-minimizing transmission policy for a number of sensor agents sharing capacity-limited, non-FIFO duplex channels that introduce random delays in communication with a common receiver.","By implementing the same policy, however with no explicit inter-agent communication, the agents minimize the expected AoI in this partially observable system.","We cater to the partial observability due to random channel delays by designing a bootstrap particle filter that independently maintains a belief over the AoI of each agent.","We also leverage mean-field control approximations and reinforcement learning to derive scalable and optimal solutions for minimizing the expected AoI collaboratively."],"url":"http://arxiv.org/abs/2312.12977v1"}
{"created":"2023-12-20 12:31:28","title":"Sparse Mean Field Load Balancing in Large Localized Queueing Systems","abstract":"Scalable load balancing algorithms are of great interest in cloud networks and data centers, necessitating the use of tractable techniques to compute optimal load balancing policies for good performance. However, most existing scalable techniques, especially asymptotically scaling methods based on mean field theory, have not been able to model large queueing networks with strong locality. Meanwhile, general multi-agent reinforcement learning techniques can be hard to scale and usually lack a theoretical foundation. In this work, we address this challenge by leveraging recent advances in sparse mean field theory to learn a near-optimal load balancing policy in sparsely connected queueing networks in a tractable manner, which may be preferable to global approaches in terms of communication overhead. Importantly, we obtain a general load balancing framework for a large class of sparse bounded-degree topologies. By formulating a novel mean field control problem in the context of graphs with bounded degree, we reduce the otherwise difficult multi-agent problem to a single-agent problem. Theoretically, the approach is justified by approximation guarantees. Empirically, the proposed methodology performs well on several realistic and scalable network topologies. Moreover, we compare it with a number of well-known load balancing heuristics and with existing scalable multi-agent reinforcement learning methods. Overall, we obtain a tractable approach for load balancing in highly localized networks.","sentences":["Scalable load balancing algorithms are of great interest in cloud networks and data centers, necessitating the use of tractable techniques to compute optimal load balancing policies for good performance.","However, most existing scalable techniques, especially asymptotically scaling methods based on mean field theory, have not been able to model large queueing networks with strong locality.","Meanwhile, general multi-agent reinforcement learning techniques can be hard to scale and usually lack a theoretical foundation.","In this work, we address this challenge by leveraging recent advances in sparse mean field theory to learn a near-optimal load balancing policy in sparsely connected queueing networks in a tractable manner, which may be preferable to global approaches in terms of communication overhead.","Importantly, we obtain a general load balancing framework for a large class of sparse bounded-degree topologies.","By formulating a novel mean field control problem in the context of graphs with bounded degree, we reduce the otherwise difficult multi-agent problem to a single-agent problem.","Theoretically, the approach is justified by approximation guarantees.","Empirically, the proposed methodology performs well on several realistic and scalable network topologies.","Moreover, we compare it with a number of well-known load balancing heuristics and with existing scalable multi-agent reinforcement learning methods.","Overall, we obtain a tractable approach for load balancing in highly localized networks."],"url":"http://arxiv.org/abs/2312.12973v1"}
{"created":"2023-12-20 12:08:39","title":"Far- and Near-Field Channel Measurements and Characterization in the Terahertz Band Using a Virtual Antenna Array","abstract":"Extremely large-scale antenna array (ELAA) technologies consisting of ultra-massive multiple-input-multiple-output (UM-MIMO) or reconfigurable intelligent surfaces (RISs), are emerging to meet the demand of wireless systems in sixth-generation and beyond communications for enhanced coverage and extreme data rates up to Terabits per second. For ELAA operating at Terahertz (THz) frequencies, the Rayleigh distance expands, and users are likely to be located in both far-field (FF) and near-field (NF) regions. On one hand, new features like NF propagation and spatial non-stationarity need to be characterized. On the other hand, the transition of properties near the FF and NF boundary is worth exploring. In this paper, a complete experimental analysis of far- and near-field channel characteristics using a THz virtual antenna array is provided based on measurement of the multi-input-single-output channel with the virtual uniform planar array (UPA) structure of at most 4096 elements. In particular, non-linear phase change is observed in the NF, and the Rayleigh criterion regarding the maximum phase error is verified. Then, a new cross-field path loss model is proposed, which is compatible with both FF and NF cases based on the UPA structure. Besides, multi-path fading is discovered in both NF and FF regions.","sentences":["Extremely large-scale antenna array (ELAA) technologies consisting of ultra-massive multiple-input-multiple-output (UM-MIMO) or reconfigurable intelligent surfaces (RISs), are emerging to meet the demand of wireless systems in sixth-generation and beyond communications for enhanced coverage and extreme data rates up to Terabits per second.","For ELAA operating at Terahertz (THz) frequencies, the Rayleigh distance expands, and users are likely to be located in both far-field (FF) and near-field (NF) regions.","On one hand, new features like NF propagation and spatial non-stationarity need to be characterized.","On the other hand, the transition of properties near the FF and NF boundary is worth exploring.","In this paper, a complete experimental analysis of far- and near-field channel characteristics using a THz virtual antenna array is provided based on measurement of the multi-input-single-output channel with the virtual uniform planar array (UPA) structure of at most 4096 elements.","In particular, non-linear phase change is observed in the NF, and the Rayleigh criterion regarding the maximum phase error is verified.","Then, a new cross-field path loss model is proposed, which is compatible with both FF and NF cases based on the UPA structure.","Besides, multi-path fading is discovered in both NF and FF regions."],"url":"http://arxiv.org/abs/2312.12964v1"}
{"created":"2023-12-20 11:51:49","title":"TADAP: Trajectory-Aided Drivable area Auto-labeling with Pre-trained self-supervised features in winter driving conditions","abstract":"Detection of the drivable area in all conditions is crucial for autonomous driving and advanced driver assistance systems. However, the amount of labeled data in adverse driving conditions is limited, especially in winter, and supervised methods generalize poorly to conditions outside the training distribution. For easy adaption to all conditions, the need for human annotation should be removed from the learning process. In this paper, Trajectory-Aided Drivable area Auto-labeling with Pre-trained self-supervised features (TADAP) is presented for automated annotation of the drivable area in winter driving conditions. A sample of the drivable area is extracted based on the trajectory estimate from the global navigation satellite system. Similarity with the sample area is determined based on pre-trained self-supervised visual features. Image areas similar to the sample area are considered to be drivable. These TADAP labels were evaluated with a novel winter-driving dataset, collected in varying driving scenes. A prediction model trained with the TADAP labels achieved a +9.6 improvement in intersection over union compared to the previous state-of-the-art of self-supervised drivable area detection.","sentences":["Detection of the drivable area in all conditions is crucial for autonomous driving and advanced driver assistance systems.","However, the amount of labeled data in adverse driving conditions is limited, especially in winter, and supervised methods generalize poorly to conditions outside the training distribution.","For easy adaption to all conditions, the need for human annotation should be removed from the learning process.","In this paper, Trajectory-Aided Drivable area Auto-labeling with Pre-trained self-supervised features (TADAP) is presented for automated annotation of the drivable area in winter driving conditions.","A sample of the drivable area is extracted based on the trajectory estimate from the global navigation satellite system.","Similarity with the sample area is determined based on pre-trained self-supervised visual features.","Image areas similar to the sample area are considered to be drivable.","These TADAP labels were evaluated with a novel winter-driving dataset, collected in varying driving scenes.","A prediction model trained with the TADAP labels achieved a +9.6 improvement in intersection over union compared to the previous state-of-the-art of self-supervised drivable area detection."],"url":"http://arxiv.org/abs/2312.12954v1"}
{"created":"2023-12-20 11:43:33","title":"Class Conditional Time Series Generation with Structured Noise Space GAN","abstract":"This paper introduces Structured Noise Space GAN (SNS-GAN), a novel approach in the field of generative modeling specifically tailored for class-conditional generation in both image and time series data. It addresses the challenge of effectively integrating class labels into generative models without requiring structural modifications to the network. The SNS-GAN method embeds class conditions within the generator's noise space, simplifying the training process and enhancing model versatility. The model's efficacy is demonstrated through qualitative validations in the image domain and superior performance in time series generation compared to baseline models. This research opens new avenues for the application of GANs in various domains, including but not limited to time series and image data generation.","sentences":["This paper introduces Structured Noise Space GAN (SNS-GAN), a novel approach in the field of generative modeling specifically tailored for class-conditional generation in both image and time series data.","It addresses the challenge of effectively integrating class labels into generative models without requiring structural modifications to the network.","The SNS-GAN method embeds class conditions within the generator's noise space, simplifying the training process and enhancing model versatility.","The model's efficacy is demonstrated through qualitative validations in the image domain and superior performance in time series generation compared to baseline models.","This research opens new avenues for the application of GANs in various domains, including but not limited to time series and image data generation."],"url":"http://arxiv.org/abs/2312.12946v1"}
{"created":"2023-12-20 11:33:12","title":"On the Energy Consumption of UAV Edge Computing in Non-Terrestrial Networks","abstract":"During the last few years, the use of Unmanned Aerial Vehicles (UAVs) equipped with sensors and cameras has emerged as a cutting-edge technology to provide services such as surveillance, infrastructure inspections, and target acquisition. However, this approach requires UAVs to process data onboard, mainly for person/object detection and recognition, which may pose significant energy constraints as UAVs are battery-powered. A possible solution can be the support of Non-Terrestrial Networks (NTNs) for edge computing. In particular, UAVs can partially offload data (e.g., video acquisitions from onboard sensors) to more powerful upstream High Altitude Platforms (HAPs) or satellites acting as edge computing servers to increase the battery autonomy compared to local processing, even though at the expense of some data transmission delays. Accordingly, in this study we model the energy consumption of UAVs, HAPs, and satellites considering the energy for data processing, offloading, and hovering. Then, we investigate whether data offloading can improve the system performance. Simulations demonstrate that edge computing can improve both UAV autonomy and end-to-end delay compared to onboard processing in many configurations.","sentences":["During the last few years, the use of Unmanned Aerial Vehicles (UAVs) equipped with sensors and cameras has emerged as a cutting-edge technology to provide services such as surveillance, infrastructure inspections, and target acquisition.","However, this approach requires UAVs to process data onboard, mainly for person/object detection and recognition, which may pose significant energy constraints as UAVs are battery-powered.","A possible solution can be the support of Non-Terrestrial Networks (NTNs) for edge computing.","In particular, UAVs can partially offload data (e.g., video acquisitions from onboard sensors) to more powerful upstream High Altitude Platforms (HAPs) or satellites acting as edge computing servers to increase the battery autonomy compared to local processing, even though at the expense of some data transmission delays.","Accordingly, in this study we model the energy consumption of UAVs, HAPs, and satellites considering the energy for data processing, offloading, and hovering.","Then, we investigate whether data offloading can improve the system performance.","Simulations demonstrate that edge computing can improve both UAV autonomy and end-to-end delay compared to onboard processing in many configurations."],"url":"http://arxiv.org/abs/2312.12940v1"}
{"created":"2023-12-20 11:27:46","title":"Robust Loss Functions for Training Decision Trees with Noisy Labels","abstract":"We consider training decision trees using noisily labeled data, focusing on loss functions that can lead to robust learning algorithms. Our contributions are threefold. First, we offer novel theoretical insights on the robustness of many existing loss functions in the context of decision tree learning. We show that some of the losses belong to a class of what we call conservative losses, and the conservative losses lead to an early stopping behavior during training and noise-tolerant predictions during testing. Second, we introduce a framework for constructing robust loss functions, called distribution losses. These losses apply percentile-based penalties based on an assumed margin distribution, and they naturally allow adapting to different noise rates via a robustness parameter. In particular, we introduce a new loss called the negative exponential loss, which leads to an efficient greedy impurity-reduction learning algorithm. Lastly, our experiments on multiple datasets and noise settings validate our theoretical insight and the effectiveness of our adaptive negative exponential loss.","sentences":["We consider training decision trees using noisily labeled data, focusing on loss functions that can lead to robust learning algorithms.","Our contributions are threefold.","First, we offer novel theoretical insights on the robustness of many existing loss functions in the context of decision tree learning.","We show that some of the losses belong to a class of what we call conservative losses, and the conservative losses lead to an early stopping behavior during training and noise-tolerant predictions during testing.","Second, we introduce a framework for constructing robust loss functions, called distribution losses.","These losses apply percentile-based penalties based on an assumed margin distribution, and they naturally allow adapting to different noise rates via a robustness parameter.","In particular, we introduce a new loss called the negative exponential loss, which leads to an efficient greedy impurity-reduction learning algorithm.","Lastly, our experiments on multiple datasets and noise settings validate our theoretical insight and the effectiveness of our adaptive negative exponential loss."],"url":"http://arxiv.org/abs/2312.12937v1"}
{"created":"2023-12-20 10:58:43","title":"Secure Authentication Mechanism for Cluster based Vehicular Adhoc Network (VANET): A Survey","abstract":"Vehicular Ad Hoc Networks (VANETs) play a crucial role in Intelligent Transportation Systems (ITS) by facilitating communication between vehicles and infrastructure. This communication aims to enhance road safety, improve traffic efficiency, and enhance passenger comfort. The secure and reliable exchange of information is paramount to ensure the integrity and confidentiality of data, while the authentication of vehicles and messages is essential to prevent unauthorized access and malicious activities. This survey paper presents a comprehensive analysis of existing authentication mechanisms proposed for cluster-based VANETs. The strengths, weaknesses, and suitability of these mechanisms for various scenarios are carefully examined. Additionally, the integration of secure key management techniques is discussed to enhance the overall authentication process. Cluster-based VANETs are formed by dividing the network into smaller groups or clusters, with designated cluster heads comprising one or more vehicles. Furthermore, this paper identifies gaps in the existing literature through an exploration of previous surveys. Several schemes based on different methods are critically evaluated, considering factors such as throughput, detection rate, security, packet delivery ratio, and end-to-end delay. To provide optimal solutions for authentication in cluster-based VANETs, this paper highlights AI- and ML-based routing-based schemes. These approaches leverage artificial intelligence and machine learning techniques to enhance authentication within the cluster-based VANET network. Finally, this paper explores the open research challenges that exist in the realm of authentication for cluster-based Vehicular Adhoc Networks, shedding light on areas that require further investigation and development.","sentences":["Vehicular Ad Hoc Networks (VANETs) play a crucial role in Intelligent Transportation Systems (ITS) by facilitating communication between vehicles and infrastructure.","This communication aims to enhance road safety, improve traffic efficiency, and enhance passenger comfort.","The secure and reliable exchange of information is paramount to ensure the integrity and confidentiality of data, while the authentication of vehicles and messages is essential to prevent unauthorized access and malicious activities.","This survey paper presents a comprehensive analysis of existing authentication mechanisms proposed for cluster-based VANETs.","The strengths, weaknesses, and suitability of these mechanisms for various scenarios are carefully examined.","Additionally, the integration of secure key management techniques is discussed to enhance the overall authentication process.","Cluster-based VANETs are formed by dividing the network into smaller groups or clusters, with designated cluster heads comprising one or more vehicles.","Furthermore, this paper identifies gaps in the existing literature through an exploration of previous surveys.","Several schemes based on different methods are critically evaluated, considering factors such as throughput, detection rate, security, packet delivery ratio, and end-to-end delay.","To provide optimal solutions for authentication in cluster-based VANETs, this paper highlights AI- and ML-based routing-based schemes.","These approaches leverage artificial intelligence and machine learning techniques to enhance authentication within the cluster-based VANET network.","Finally, this paper explores the open research challenges that exist in the realm of authentication for cluster-based Vehicular Adhoc Networks, shedding light on areas that require further investigation and development."],"url":"http://arxiv.org/abs/2312.12925v1"}
{"created":"2023-12-20 10:56:50","title":"Improving Data Minimization through Decentralized Data Architectures","abstract":"In this research project, we investigate an alternative to the standard cloud-centralized data architecture. Specifically, we aim to leave part of the application data under the control of the individual data owners in decentralized personal data stores. Our primary goal is to increase data minimization, i. e., enabling more sensitive personal data to be under the control of its owners while providing a straightforward and efficient framework to design architectures that allow applications to run and data to be analyzed. To serve this purpose, the centralized part of the schema contains aggregating views over this decentralized data. We propose to design a declarative language that extends SQL, for architects to specify different kinds of tables and views at the schema level, along with sensitive columns and their minimum granularity level of their aggregations. Local updates need to be reflected in the centralized views while ensuring privacy throughout intermediate calculations; for this we pursue the integration of distributed materialized view maintenance and multi-party computation (MPC) techniques. We finally aim to implement this system, where the personal data stores could either live in mobile devices or encrypted cloud storage, in order to evaluate its performance properties.","sentences":["In this research project, we investigate an alternative to the standard cloud-centralized data architecture.","Specifically, we aim to leave part of the application data under the control of the individual data owners in decentralized personal data stores.","Our primary goal is to increase data minimization, i. e., enabling more sensitive personal data to be under the control of its owners while providing a straightforward and efficient framework to design architectures that allow applications to run and data to be analyzed.","To serve this purpose, the centralized part of the schema contains aggregating views over this decentralized data.","We propose to design a declarative language that extends SQL, for architects to specify different kinds of tables and views at the schema level, along with sensitive columns and their minimum granularity level of their aggregations.","Local updates need to be reflected in the centralized views while ensuring privacy throughout intermediate calculations; for this we pursue the integration of distributed materialized view maintenance and multi-party computation (MPC) techniques.","We finally aim to implement this system, where the personal data stores could either live in mobile devices or encrypted cloud storage, in order to evaluate its performance properties."],"url":"http://arxiv.org/abs/2312.12923v1"}
{"created":"2023-12-20 10:53:53","title":"Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors","abstract":"To combat the potential misuse of Natural Language Generation (NLG) technology, a variety of algorithms have been developed for the detection of AI-generated texts. Traditionally, this task is treated as a binary classification problem. Although supervised learning has demonstrated promising results, acquiring labeled data for detection purposes poses real-world challenges and the risk of overfitting. In an effort to address these issues, we delve into the realm of zero-shot machine-generated text detection. Existing zero-shot detectors, typically designed for specific tasks or topics, often assume uniform testing scenarios, limiting their practicality. In our research, we explore various advanced Large Language Models (LLMs) and their specialized variants, contributing to this field in several ways. In empirical studies, we uncover a significant correlation between topics and detection performance. Secondly, we delve into the influence of topic shifts on zero-shot detectors. These investigations shed light on the adaptability and robustness of these detection methods across diverse topics.","sentences":["To combat the potential misuse of Natural Language Generation (NLG) technology, a variety of algorithms have been developed for the detection of AI-generated texts.","Traditionally, this task is treated as a binary classification problem.","Although supervised learning has demonstrated promising results, acquiring labeled data for detection purposes poses real-world challenges and the risk of overfitting.","In an effort to address these issues, we delve into the realm of zero-shot machine-generated text detection.","Existing zero-shot detectors, typically designed for specific tasks or topics, often assume uniform testing scenarios, limiting their practicality.","In our research, we explore various advanced Large Language Models (LLMs) and their specialized variants, contributing to this field in several ways.","In empirical studies, we uncover a significant correlation between topics and detection performance.","Secondly, we delve into the influence of topic shifts on zero-shot detectors.","These investigations shed light on the adaptability and robustness of these detection methods across diverse topics."],"url":"http://arxiv.org/abs/2312.12918v1"}
{"created":"2023-12-20 10:49:49","title":"Produce Once, Utilize Twice for Anomaly Detection","abstract":"Visual anomaly detection aims at classifying and locating the regions that deviate from the normal appearance. Embedding-based methods and reconstruction-based methods are two main approaches for this task. However, they are either not efficient or not precise enough for the industrial detection. To deal with this problem, we derive POUTA (Produce Once Utilize Twice for Anomaly detection), which improves both the accuracy and efficiency by reusing the discriminant information potential in the reconstructive network. We observe that the encoder and decoder representations of the reconstructive network are able to stand for the features of the original and reconstructed image respectively. And the discrepancies between the symmetric reconstructive representations provides roughly accurate anomaly information. To refine this information, a coarse-to-fine process is proposed in POUTA, which calibrates the semantics of each discriminative layer by the high-level representations and supervision loss. Equipped with the above modules, POUTA is endowed with the ability to provide a more precise anomaly location than the prior arts. Besides, the representation reusage also enables to exclude the feature extraction process in the discriminative network, which reduces the parameters and improves the efficiency. Extensive experiments show that, POUTA is superior or comparable to the prior methods with even less cost. Furthermore, POUTA also achieves better performance than the state-of-the-art few-shot anomaly detection methods without any special design, showing that POUTA has strong ability to learn representations inherent in the training data.","sentences":["Visual anomaly detection aims at classifying and locating the regions that deviate from the normal appearance.","Embedding-based methods and reconstruction-based methods are two main approaches for this task.","However, they are either not efficient or not precise enough for the industrial detection.","To deal with this problem, we derive POUTA (Produce Once Utilize Twice for Anomaly detection), which improves both the accuracy and efficiency by reusing the discriminant information potential in the reconstructive network.","We observe that the encoder and decoder representations of the reconstructive network are able to stand for the features of the original and reconstructed image respectively.","And the discrepancies between the symmetric reconstructive representations provides roughly accurate anomaly information.","To refine this information, a coarse-to-fine process is proposed in POUTA, which calibrates the semantics of each discriminative layer by the high-level representations and supervision loss.","Equipped with the above modules, POUTA is endowed with the ability to provide a more precise anomaly location than the prior arts.","Besides, the representation reusage also enables to exclude the feature extraction process in the discriminative network, which reduces the parameters and improves the efficiency.","Extensive experiments show that, POUTA is superior or comparable to the prior methods with even less cost.","Furthermore, POUTA also achieves better performance than the state-of-the-art few-shot anomaly detection methods without any special design, showing that POUTA has strong ability to learn representations inherent in the training data."],"url":"http://arxiv.org/abs/2312.12913v1"}
{"created":"2023-12-20 10:47:05","title":"A Survey on Scheduling the Task in Fog Computing Environment","abstract":"With the rapid increase in the Internet of Things (IoT), the amount of data produced and processed is also increased. Cloud Computing facilitates the storage, processing, and analysis of data as needed. However, cloud computing devices are located far away from the IoT devices. Fog computing has emerged as a small cloud computing paradigm that is near to the edge devices and handles the task very efficiently. Fog nodes have a small storage capability than the cloud node but it is designed and deployed near to the edge device so that request must be accessed efficiently and executes in time. In this survey paper we have investigated and analysed the main challenges and issues raised in scheduling the task in fog computing environment. To the best of our knowledge there is no comprehensive survey paper on challenges in task scheduling of fog computing paradigm. In this survey paper research is conducted from 2018 to 2021 and most of the paper selection is done from 2020-2021. Moreover, this survey paper organizes the task scheduling approaches and technically plans the identified challenges and issues. Based on the identified issues, we have highlighted the future work directions in the field of task scheduling in fog computing environment.","sentences":["With the rapid increase in the Internet of Things (IoT), the amount of data produced and processed is also increased.","Cloud Computing facilitates the storage, processing, and analysis of data as needed.","However, cloud computing devices are located far away from the IoT devices.","Fog computing has emerged as a small cloud computing paradigm that is near to the edge devices and handles the task very efficiently.","Fog nodes have a small storage capability than the cloud node but it is designed and deployed near to the edge device so that request must be accessed efficiently and executes in time.","In this survey paper we have investigated and analysed the main challenges and issues raised in scheduling the task in fog computing environment.","To the best of our knowledge there is no comprehensive survey paper on challenges in task scheduling of fog computing paradigm.","In this survey paper research is conducted from 2018 to 2021 and most of the paper selection is done from 2020-2021.","Moreover, this survey paper organizes the task scheduling approaches and technically plans the identified challenges and issues.","Based on the identified issues, we have highlighted the future work directions in the field of task scheduling in fog computing environment."],"url":"http://arxiv.org/abs/2312.12910v1"}
{"created":"2023-12-20 10:36:53","title":"DXP: Billing Data Preparation for Big Data Analytics","abstract":"In this paper, we present the data preparation activities that we performed for the Digital Experience Platform (DXP) project, commissioned and supervised by Doxee S.p.A.. DXP manages the billing data of the users of different companies operating in various sectors (electricity and gas, telephony, pay TV, etc.). This data has to be processed to provide services to the users (e.g., interactive billing), but mainly to provide analytics to the companies (e.g., churn prediction or user segmentation). We focus on the design of the data preparation pipeline, describing the challenges that we had to overcome in order to get the billing data ready to perform analysis on it. We illustrate the lessons learned by highlighting the key points that could be transferred to similar projects. Moreover, we report some interesting results and considerations derived from the preliminary analysis of the prepared data, also pointing out some possible future directions for the ongoing project, spacing from big data integration to privacy-preserving temporal record linkage.","sentences":["In this paper, we present the data preparation activities that we performed for the Digital Experience Platform (DXP) project, commissioned and supervised by Doxee S.p.A..","DXP manages the billing data of the users of different companies operating in various sectors (electricity and gas, telephony, pay TV, etc.).","This data has to be processed to provide services to the users (e.g., interactive billing), but mainly to provide analytics to the companies (e.g., churn prediction or user segmentation).","We focus on the design of the data preparation pipeline, describing the challenges that we had to overcome in order to get the billing data ready to perform analysis on it.","We illustrate the lessons learned by highlighting the key points that could be transferred to similar projects.","Moreover, we report some interesting results and considerations derived from the preliminary analysis of the prepared data, also pointing out some possible future directions for the ongoing project, spacing from big data integration to privacy-preserving temporal record linkage."],"url":"http://arxiv.org/abs/2312.12902v1"}
{"created":"2023-12-20 09:46:42","title":"BSL: Understanding and Improving Softmax Loss for Recommendation","abstract":"Loss functions steer the optimization direction of recommendation models and are critical to model performance, but have received relatively little attention in recent recommendation research. Among various losses, we find Softmax loss (SL) stands out for not only achieving remarkable accuracy but also better robustness and fairness. Nevertheless, the current literature lacks a comprehensive explanation for the efficacy of SL. Toward addressing this research gap, we conduct theoretical analyses on SL and uncover three insights: 1) Optimizing SL is equivalent to performing Distributionally Robust Optimization (DRO) on the negative data, thereby learning against perturbations on the negative distribution and yielding robustness to noisy negatives. 2) Comparing with other loss functions, SL implicitly penalizes the prediction variance, resulting in a smaller gap between predicted values and and thus producing fairer results. Building on these insights, we further propose a novel loss function Bilateral SoftMax Loss (BSL) that extends the advantage of SL to both positive and negative sides. BSL augments SL by applying the same Log-Expectation-Exp structure to positive examples as is used for negatives, making the model robust to the noisy positives as well. Remarkably, BSL is simple and easy-to-implement -- requiring just one additional line of code compared to SL. Experiments on four real-world datasets and three representative backbones demonstrate the effectiveness of our proposal. The code is available at https://github.com/junkangwu/BSL","sentences":["Loss functions steer the optimization direction of recommendation models and are critical to model performance, but have received relatively little attention in recent recommendation research.","Among various losses, we find Softmax loss (SL) stands out for not only achieving remarkable accuracy but also better robustness and fairness.","Nevertheless, the current literature lacks a comprehensive explanation for the efficacy of SL.","Toward addressing this research gap, we conduct theoretical analyses on SL and uncover three insights: 1) Optimizing SL is equivalent to performing Distributionally Robust Optimization (DRO) on the negative data, thereby learning against perturbations on the negative distribution and yielding robustness to noisy negatives.","2) Comparing with other loss functions, SL implicitly penalizes the prediction variance, resulting in a smaller gap between predicted values and and thus producing fairer results.","Building on these insights, we further propose a novel loss function Bilateral SoftMax Loss (BSL) that extends the advantage of SL to both positive and negative sides.","BSL augments SL by applying the same Log-Expectation-Exp structure to positive examples as is used for negatives, making the model robust to the noisy positives as well.","Remarkably, BSL is simple and easy-to-implement -- requiring just one additional line of code compared to SL.","Experiments on four real-world datasets and three representative backbones demonstrate the effectiveness of our proposal.","The code is available at https://github.com/junkangwu/BSL"],"url":"http://arxiv.org/abs/2312.12882v1"}
{"created":"2023-12-20 09:39:55","title":"Relightable and Animatable Neural Avatars from Videos","abstract":"Lightweight creation of 3D digital avatars is a highly desirable but challenging task. With only sparse videos of a person under unknown illumination, we propose a method to create relightable and animatable neural avatars, which can be used to synthesize photorealistic images of humans under novel viewpoints, body poses, and lighting. The key challenge here is to disentangle the geometry, material of the clothed body, and lighting, which becomes more difficult due to the complex geometry and shadow changes caused by body motions. To solve this ill-posed problem, we propose novel techniques to better model the geometry and shadow changes. For geometry change modeling, we propose an invertible deformation field, which helps to solve the inverse skinning problem and leads to better geometry quality. To model the spatial and temporal varying shading cues, we propose a pose-aware part-wise light visibility network to estimate light occlusion. Extensive experiments on synthetic and real datasets show that our approach reconstructs high-quality geometry and generates realistic shadows under different body poses. Code and data are available at \\url{https://wenbin-lin.github.io/RelightableAvatar-page/}.","sentences":["Lightweight creation of 3D digital avatars is a highly desirable but challenging task.","With only sparse videos of a person under unknown illumination, we propose a method to create relightable and animatable neural avatars, which can be used to synthesize photorealistic images of humans under novel viewpoints, body poses, and lighting.","The key challenge here is to disentangle the geometry, material of the clothed body, and lighting, which becomes more difficult due to the complex geometry and shadow changes caused by body motions.","To solve this ill-posed problem, we propose novel techniques to better model the geometry and shadow changes.","For geometry change modeling, we propose an invertible deformation field, which helps to solve the inverse skinning problem and leads to better geometry quality.","To model the spatial and temporal varying shading cues, we propose a pose-aware part-wise light visibility network to estimate light occlusion.","Extensive experiments on synthetic and real datasets show that our approach reconstructs high-quality geometry and generates realistic shadows under different body poses.","Code and data are available at \\url{https://wenbin-lin.github.io/RelightableAvatar-page/}."],"url":"http://arxiv.org/abs/2312.12877v1"}
{"created":"2023-12-20 09:38:38","title":"Deep-Unfolding-Based Joint Activity and Data Detection for Grant-Free Transmission in Cell-Free Communication Systems","abstract":"Massive grant-free transmission and cell-free wireless communication systems have emerged as pivotal enablers for massive machine-type communication. This paper proposes a deep-unfolding-based joint activity and data detection (DU-JAD) algorithm for massive grant-free transmission in cell-free systems. We first formulate a joint activity and data detection optimization problem, which we solve approximately using forward-backward splitting (FBS). We then apply deep unfolding to FBS to optimize algorithm parameters using machine learning. In order to improve data detection (DD) performance, reduce algorithm complexity, and enhance active user detection (AUD), we employ a momentum strategy, an approximate posterior mean estimator, and a novel soft-output AUD module, respectively. Simulation results confirm the efficacy of DU-JAD for AUD and DD.","sentences":["Massive grant-free transmission and cell-free wireless communication systems have emerged as pivotal enablers for massive machine-type communication.","This paper proposes a deep-unfolding-based joint activity and data detection (DU-JAD) algorithm for massive grant-free transmission in cell-free systems.","We first formulate a joint activity and data detection optimization problem, which we solve approximately using forward-backward splitting (FBS).","We then apply deep unfolding to FBS to optimize algorithm parameters using machine learning.","In order to improve data detection (DD) performance, reduce algorithm complexity, and enhance active user detection (AUD), we employ a momentum strategy, an approximate posterior mean estimator, and a novel soft-output AUD module, respectively.","Simulation results confirm the efficacy of DU-JAD for AUD and DD."],"url":"http://arxiv.org/abs/2312.12874v1"}
{"created":"2023-12-20 09:37:06","title":"Integration and Performance Analysis of Artificial Intelligence and Computer Vision Based on Deep Learning Algorithms","abstract":"This paper focuses on the analysis of the application effectiveness of the integration of deep learning and computer vision technologies. Deep learning achieves a historic breakthrough by constructing hierarchical neural networks, enabling end-to-end feature learning and semantic understanding of images. The successful experiences in the field of computer vision provide strong support for training deep learning algorithms. The tight integration of these two fields has given rise to a new generation of advanced computer vision systems, significantly surpassing traditional methods in tasks such as machine vision image classification and object detection. In this paper, typical image classification cases are combined to analyze the superior performance of deep neural network models while also pointing out their limitations in generalization and interpretability, proposing directions for future improvements. Overall, the efficient integration and development trend of deep learning with massive visual data will continue to drive technological breakthroughs and application expansion in the field of computer vision, making it possible to build truly intelligent machine vision systems. This deepening fusion paradigm will powerfully promote unprecedented tasks and functions in computer vision, providing stronger development momentum for related disciplines and industries.","sentences":["This paper focuses on the analysis of the application effectiveness of the integration of deep learning and computer vision technologies.","Deep learning achieves a historic breakthrough by constructing hierarchical neural networks, enabling end-to-end feature learning and semantic understanding of images.","The successful experiences in the field of computer vision provide strong support for training deep learning algorithms.","The tight integration of these two fields has given rise to a new generation of advanced computer vision systems, significantly surpassing traditional methods in tasks such as machine vision image classification and object detection.","In this paper, typical image classification cases are combined to analyze the superior performance of deep neural network models while also pointing out their limitations in generalization and interpretability, proposing directions for future improvements.","Overall, the efficient integration and development trend of deep learning with massive visual data will continue to drive technological breakthroughs and application expansion in the field of computer vision, making it possible to build truly intelligent machine vision systems.","This deepening fusion paradigm will powerfully promote unprecedented tasks and functions in computer vision, providing stronger development momentum for related disciplines and industries."],"url":"http://arxiv.org/abs/2312.12872v1"}
{"created":"2023-12-20 09:34:28","title":"Effect Size Estimation for Duration Recommendation in Online Experiments: Leveraging Hierarchical Models and Objective Utility Approaches","abstract":"The selection of the assumed effect size (AES) critically determines the duration of an experiment, and hence its accuracy and efficiency. Traditionally, experimenters determine AES based on domain knowledge. However, this method becomes impractical for online experimentation services managing numerous experiments, and a more automated approach is hence of great demand. We initiate the study of data-driven AES selection in for online experimentation services by introducing two solutions. The first employs a three-layer Gaussian Mixture Model considering the heteroskedasticity across experiments, and it seeks to estimate the true expected effect size among positive experiments. The second method, grounded in utility theory, aims to determine the optimal effect size by striking a balance between the experiment's cost and the precision of decision-making. Through comparisons with baseline methods using both simulated and real data, we showcase the superior performance of the proposed approaches.","sentences":["The selection of the assumed effect size (AES) critically determines the duration of an experiment, and hence its accuracy and efficiency.","Traditionally, experimenters determine AES based on domain knowledge.","However, this method becomes impractical for online experimentation services managing numerous experiments, and a more automated approach is hence of great demand.","We initiate the study of data-driven AES selection in for online experimentation services by introducing two solutions.","The first employs a three-layer Gaussian Mixture Model considering the heteroskedasticity across experiments, and it seeks to estimate the true expected effect size among positive experiments.","The second method, grounded in utility theory, aims to determine the optimal effect size by striking a balance between the experiment's cost and the precision of decision-making.","Through comparisons with baseline methods using both simulated and real data, we showcase the superior performance of the proposed approaches."],"url":"http://arxiv.org/abs/2312.12871v1"}
{"created":"2023-12-20 09:27:41","title":"RadEdit: stress-testing biomedical vision models via diffusion image editing","abstract":"Biomedical imaging datasets are often small and biased, meaning that real-world performance of predictive models can be substantially lower than expected from internal testing. This work proposes using generative image editing to simulate dataset shifts and diagnose failure modes of biomedical vision models; this can be used in advance of deployment to assess readiness, potentially reducing cost and patient harm. Existing editing methods can produce undesirable changes, with spurious correlations learned due to the co-occurrence of disease and treatment interventions, limiting practical applicability. To address this, we train a text-to-image diffusion model on multiple chest X-ray datasets and introduce a new editing method RadEdit that uses multiple masks, if present, to constrain changes and ensure consistency in the edited images. We consider three types of dataset shifts: acquisition shift, manifestation shift, and population shift, and demonstrate that our approach can diagnose failures and quantify model robustness without additional data collection, complementing more qualitative tools for explainable AI.","sentences":["Biomedical imaging datasets are often small and biased, meaning that real-world performance of predictive models can be substantially lower than expected from internal testing.","This work proposes using generative image editing to simulate dataset shifts and diagnose failure modes of biomedical vision models; this can be used in advance of deployment to assess readiness, potentially reducing cost and patient harm.","Existing editing methods can produce undesirable changes, with spurious correlations learned due to the co-occurrence of disease and treatment interventions, limiting practical applicability.","To address this, we train a text-to-image diffusion model on multiple chest X-ray datasets and introduce a new editing method RadEdit that uses multiple masks, if present, to constrain changes and ensure consistency in the edited images.","We consider three types of dataset shifts: acquisition shift, manifestation shift, and population shift, and demonstrate that our approach can diagnose failures and quantify model robustness without additional data collection, complementing more qualitative tools for explainable AI."],"url":"http://arxiv.org/abs/2312.12865v1"}
{"created":"2023-12-20 09:27:09","title":"Federated Learning While Providing Model as a Service: Joint Training and Inference Optimization","abstract":"While providing machine learning model as a service to process users' inference requests, online applications can periodically upgrade the model utilizing newly collected data. Federated learning (FL) is beneficial for enabling the training of models across distributed clients while keeping the data locally. However, existing work has overlooked the coexistence of model training and inference under clients' limited resources. This paper focuses on the joint optimization of model training and inference to maximize inference performance at clients. Such an optimization faces several challenges. The first challenge is to characterize the clients' inference performance when clients may partially participate in FL. To resolve this challenge, we introduce a new notion of age of model (AoM) to quantify client-side model freshness, based on which we use FL's global model convergence error as an approximate measure of inference performance. The second challenge is the tight coupling among clients' decisions, including participation probability in FL, model download probability, and service rates. Toward the challenges, we propose an online problem approximation to reduce the problem complexity and optimize the resources to balance the needs of model training and inference. Experimental results demonstrate that the proposed algorithm improves the average inference accuracy by up to 12%.","sentences":["While providing machine learning model as a service to process users' inference requests, online applications can periodically upgrade the model utilizing newly collected data.","Federated learning (FL) is beneficial for enabling the training of models across distributed clients while keeping the data locally.","However, existing work has overlooked the coexistence of model training and inference under clients' limited resources.","This paper focuses on the joint optimization of model training and inference to maximize inference performance at clients.","Such an optimization faces several challenges.","The first challenge is to characterize the clients' inference performance when clients may partially participate in FL.","To resolve this challenge, we introduce a new notion of age of model (AoM) to quantify client-side model freshness, based on which we use FL's global model convergence error as an approximate measure of inference performance.","The second challenge is the tight coupling among clients' decisions, including participation probability in FL, model download probability, and service rates.","Toward the challenges, we propose an online problem approximation to reduce the problem complexity and optimize the resources to balance the needs of model training and inference.","Experimental results demonstrate that the proposed algorithm improves the average inference accuracy by up to 12%."],"url":"http://arxiv.org/abs/2312.12863v1"}
{"created":"2023-12-20 09:06:18","title":"CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models","abstract":"As an indispensable ingredient of intelligence, commonsense reasoning is crucial for large language models (LLMs) in real-world scenarios. In this paper, we propose CORECODE, a dataset that contains abundant commonsense knowledge manually annotated on dyadic dialogues, to evaluate the commonsense reasoning and commonsense conflict detection capabilities of Chinese LLMs. We categorize commonsense knowledge in everyday conversations into three dimensions: entity, event, and social interaction. For easy and consistent annotation, we standardize the form of commonsense knowledge annotation in open-domain dialogues as \"domain: slot = value\". A total of 9 domains and 37 slots are defined to capture diverse commonsense knowledge. With these pre-defined domains and slots, we collect 76,787 commonsense knowledge annotations from 19,700 dialogues through crowdsourcing. To evaluate and enhance the commonsense reasoning capability for LLMs on the curated dataset, we establish a series of dialogue-level reasoning and detection tasks, including commonsense knowledge filling, commonsense knowledge generation, commonsense conflict phrase detection, domain identification, slot identification, and event causal inference. A wide variety of existing open-source Chinese LLMs are evaluated with these tasks on our dataset. Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting. We release the data and codes of CORECODE at https://github.com/danshi777/CORECODE to promote commonsense reasoning evaluation and study of LLMs in the context of daily conversations.","sentences":["As an indispensable ingredient of intelligence, commonsense reasoning is crucial for large language models (LLMs) in real-world scenarios.","In this paper, we propose CORECODE, a dataset that contains abundant commonsense knowledge manually annotated on dyadic dialogues, to evaluate the commonsense reasoning and commonsense conflict detection capabilities of Chinese LLMs.","We categorize commonsense knowledge in everyday conversations into three dimensions: entity, event, and social interaction.","For easy and consistent annotation, we standardize the form of commonsense knowledge annotation in open-domain dialogues as \"domain: slot = value\".","A total of 9 domains and 37 slots are defined to capture diverse commonsense knowledge.","With these pre-defined domains and slots, we collect 76,787 commonsense knowledge annotations from 19,700 dialogues through crowdsourcing.","To evaluate and enhance the commonsense reasoning capability for LLMs on the curated dataset, we establish a series of dialogue-level reasoning and detection tasks, including commonsense knowledge filling, commonsense knowledge generation, commonsense conflict phrase detection, domain identification, slot identification, and event causal inference.","A wide variety of existing open-source Chinese LLMs are evaluated with these tasks on our dataset.","Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting.","We release the data and codes of CORECODE at https://github.com/danshi777/CORECODE to promote commonsense reasoning evaluation and study of LLMs in the context of daily conversations."],"url":"http://arxiv.org/abs/2312.12853v1"}
{"created":"2023-12-20 09:06:06","title":"Language Resources for Dutch Large Language Modelling","abstract":"Despite the rapid expansion of types of large language models, there remains a notable gap in models specifically designed for the Dutch language. This gap is not only a shortage in terms of pretrained Dutch models but also in terms of data, and benchmarks and leaderboards. This work provides a small step to improve the situation. First, we introduce two fine-tuned variants of the Llama 2 13B model. We first fine-tuned Llama 2 using Dutch-specific web-crawled data and subsequently refined this model further on multiple synthetic instruction and chat datasets. These datasets as well as the model weights are made available. In addition, we provide a leaderboard to keep track of the performance of (Dutch) models on a number of generation tasks, and we include results of a number of state-of-the-art models, including our own. Finally we provide a critical conclusion on what we believe is needed to push forward Dutch language models and the whole eco-system around the models.","sentences":["Despite the rapid expansion of types of large language models, there remains a notable gap in models specifically designed for the Dutch language.","This gap is not only a shortage in terms of pretrained Dutch models but also in terms of data, and benchmarks and leaderboards.","This work provides a small step to improve the situation.","First, we introduce two fine-tuned variants of the Llama 2 13B model.","We first fine-tuned Llama 2 using Dutch-specific web-crawled data and subsequently refined this model further on multiple synthetic instruction and chat datasets.","These datasets as well as the model weights are made available.","In addition, we provide a leaderboard to keep track of the performance of (Dutch) models on a number of generation tasks, and we include results of a number of state-of-the-art models, including our own.","Finally we provide a critical conclusion on what we believe is needed to push forward Dutch language models and the whole eco-system around the models."],"url":"http://arxiv.org/abs/2312.12852v1"}
{"created":"2023-12-20 08:51:58","title":"Causal Discovery under Identifiable Heteroscedastic Noise Model","abstract":"Capturing the underlying structural causal relations represented by Directed Acyclic Graphs (DAGs) has been a fundamental task in various AI disciplines. Causal DAG learning via the continuous optimization framework has recently achieved promising performance in terms of both accuracy and efficiency. However, most methods make strong assumptions of homoscedastic noise, i.e., exogenous noises have equal variances across variables, observations, or even both. The noises in real data usually violate both assumptions due to the biases introduced by different data collection processes. To address the issue of heteroscedastic noise, we introduce relaxed and implementable sufficient conditions, proving the identifiability of a general class of SEM subject to these conditions. Based on the identifiable general SEM, we propose a novel formulation for DAG learning that accounts for the variation in noise variance across variables and observations. We then propose an effective two-phase iterative DAG learning algorithm to address the increasing optimization difficulties and to learn a causal DAG from data with heteroscedastic variable noise under varying variance. We show significant empirical gains of the proposed approaches over state-of-the-art methods on both synthetic data and real data.","sentences":["Capturing the underlying structural causal relations represented by Directed Acyclic Graphs (DAGs) has been a fundamental task in various AI disciplines.","Causal DAG learning via the continuous optimization framework has recently achieved promising performance in terms of both accuracy and efficiency.","However, most methods make strong assumptions of homoscedastic noise, i.e., exogenous noises have equal variances across variables, observations, or even both.","The noises in real data usually violate both assumptions due to the biases introduced by different data collection processes.","To address the issue of heteroscedastic noise, we introduce relaxed and implementable sufficient conditions, proving the identifiability of a general class of SEM subject to these conditions.","Based on the identifiable general SEM, we propose a novel formulation for DAG learning that accounts for the variation in noise variance across variables and observations.","We then propose an effective two-phase iterative DAG learning algorithm to address the increasing optimization difficulties and to learn a causal DAG from data with heteroscedastic variable noise under varying variance.","We show significant empirical gains of the proposed approaches over state-of-the-art methods on both synthetic data and real data."],"url":"http://arxiv.org/abs/2312.12844v1"}
{"created":"2023-12-20 08:47:21","title":"Comparing Machine Learning Algorithms by Union-Free Generic Depth","abstract":"We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we provide two examples of classifier comparisons on samples of standard benchmark data sets. Our results demonstrate promisingly the wide variety of different analysis approaches based on ufg methods. Furthermore, the examples outline that our approach differs substantially from existing benchmarking approaches, and thus adds a new perspective to the vivid debate on classifier comparison.","sentences":["We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions.","Despite intensive studies in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders.","We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth.","Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures.","Concretely, we provide two examples of classifier comparisons on samples of standard benchmark data sets.","Our results demonstrate promisingly the wide variety of different analysis approaches based on ufg methods.","Furthermore, the examples outline that our approach differs substantially from existing benchmarking approaches, and thus adds a new perspective to the vivid debate on classifier comparison."],"url":"http://arxiv.org/abs/2312.12839v1"}
{"created":"2023-12-20 08:42:57","title":"FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation Against Heterogeneous Annotation Noise","abstract":"Federated learning (FL) has emerged as a promising paradigm for training segmentation models on decentralized medical data, owing to its privacy-preserving property. However, existing research overlooks the prevalent annotation noise encountered in real-world medical datasets, which limits the performance ceilings of FL. In this paper, we, for the first time, identify and tackle this problem. For problem formulation, we propose a contour evolution for modeling non-independent and identically distributed (Non-IID) noise across pixels within each client and then extend it to the case of multi-source data to form a heterogeneous noise model (\\textit{i.e.}, Non-IID annotation noise across clients). For robust learning from annotations with such two-level Non-IID noise, we emphasize the importance of data quality in model aggregation, allowing high-quality clients to have a greater impact on FL. To achieve this, we propose \\textbf{Fed}erated learning with \\textbf{A}nnotation qu\\textbf{A}lity-aware \\textbf{A}ggregat\\textbf{I}on, named \\textbf{FedA$^3$I}, by introducing a quality factor based on client-wise noise estimation. Specifically, noise estimation at each client is accomplished through the Gaussian mixture model and then incorporated into model aggregation in a layer-wise manner to up-weight high-quality clients. Extensive experiments on two real-world medical image segmentation datasets demonstrate the superior performance of FedA$^3$I against the state-of-the-art approaches in dealing with cross-client annotation noise. The code is available at \\color{blue}{https://github.com/wnn2000/FedAAAI}.","sentences":["Federated learning (FL) has emerged as a promising paradigm for training segmentation models on decentralized medical data, owing to its privacy-preserving property.","However, existing research overlooks the prevalent annotation noise encountered in real-world medical datasets, which limits the performance ceilings of FL.","In this paper, we, for the first time, identify and tackle this problem.","For problem formulation, we propose a contour evolution for modeling non-independent and identically distributed (Non-IID) noise across pixels within each client and then extend it to the case of multi-source data to form a heterogeneous noise model (\\textit{i.e.}, Non-IID annotation noise across clients).","For robust learning from annotations with such two-level Non-IID noise, we emphasize the importance of data quality in model aggregation, allowing high-quality clients to have a greater impact on FL.","To achieve this, we propose \\textbf{Fed}erated learning with \\textbf{A}nnotation qu\\textbf{A}lity-aware \\textbf{A}ggregat\\textbf{I}on, named \\textbf{FedA$^3$I}, by introducing a quality factor based on client-wise noise estimation.","Specifically, noise estimation at each client is accomplished through the Gaussian mixture model and then incorporated into model aggregation in a layer-wise manner to up-weight high-quality clients.","Extensive experiments on two real-world medical image segmentation datasets demonstrate the superior performance of FedA$^3$I against the state-of-the-art approaches in dealing with cross-client annotation noise.","The code is available at \\color{blue}{https://github.com/wnn2000/FedAAAI}."],"url":"http://arxiv.org/abs/2312.12838v1"}
{"created":"2023-12-20 08:36:55","title":"Near-Optimal Resilient Aggregation Rules for Distributed Learning Using 1-Center and 1-Mean Clustering with Outliers","abstract":"Byzantine machine learning has garnered considerable attention in light of the unpredictable faults that can occur in large-scale distributed learning systems. The key to secure resilience against Byzantine machines in distributed learning is resilient aggregation mechanisms. Although abundant resilient aggregation rules have been proposed, they are designed in ad-hoc manners, imposing extra barriers on comparing, analyzing, and improving the rules across performance criteria. This paper studies near-optimal aggregation rules using clustering in the presence of outliers. Our outlier-robust clustering approach utilizes geometric properties of the update vectors provided by workers. Our analysis show that constant approximations to the 1-center and 1-mean clustering problems with outliers provide near-optimal resilient aggregators for metric-based criteria, which have been proven to be crucial in the homogeneous and heterogeneous cases respectively. In addition, we discuss two contradicting types of attacks under which no single aggregation rule is guaranteed to improve upon the naive average. Based on the discussion, we propose a two-phase resilient aggregation framework. We run experiments for image classification using a non-convex loss function. The proposed algorithms outperform previously known aggregation rules by a large margin with both homogeneous and heterogeneous data distributions among non-faulty workers. Code and appendix are available at https://github.com/jerry907/AAAI24-RASHB.","sentences":["Byzantine machine learning has garnered considerable attention in light of the unpredictable faults that can occur in large-scale distributed learning systems.","The key to secure resilience against Byzantine machines in distributed learning is resilient aggregation mechanisms.","Although abundant resilient aggregation rules have been proposed, they are designed in ad-hoc manners, imposing extra barriers on comparing, analyzing, and improving the rules across performance criteria.","This paper studies near-optimal aggregation rules using clustering in the presence of outliers.","Our outlier-robust clustering approach utilizes geometric properties of the update vectors provided by workers.","Our analysis show that constant approximations to the 1-center and 1-mean clustering problems with outliers provide near-optimal resilient aggregators for metric-based criteria, which have been proven to be crucial in the homogeneous and heterogeneous cases respectively.","In addition, we discuss two contradicting types of attacks under which no single aggregation rule is guaranteed to improve upon the naive average.","Based on the discussion, we propose a two-phase resilient aggregation framework.","We run experiments for image classification using a non-convex loss function.","The proposed algorithms outperform previously known aggregation rules by a large margin with both homogeneous and heterogeneous data distributions among non-faulty workers.","Code and appendix are available at https://github.com/jerry907/AAAI24-RASHB."],"url":"http://arxiv.org/abs/2312.12835v1"}
{"created":"2023-12-20 08:28:36","title":"Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data","abstract":"Large Language Models (LLMs) have performed well on various reasoning tasks, but their inaccessibility and numerous parameters hinder wide application in practice. One promising way is distilling the reasoning ability from LLMs to small models by the generated chain-of-thought reasoning paths. In some cases, however, LLMs may produce incorrect reasoning chains, especially when facing complex mathematical problems. Previous studies only transfer knowledge from positive samples and drop the synthesized data with wrong answers. In this work, we illustrate the merit of negative data and propose a model specialization framework to distill LLMs with negative samples besides positive ones. The framework consists of three progressive steps, covering from training to inference stages, to absorb knowledge from negative data. We conduct extensive experiments across arithmetic reasoning tasks to demonstrate the role of negative data in distillation from LLM.","sentences":["Large Language Models (LLMs) have performed well on various reasoning tasks, but their inaccessibility and numerous parameters hinder wide application in practice.","One promising way is distilling the reasoning ability from LLMs to small models by the generated chain-of-thought reasoning paths.","In some cases, however, LLMs may produce incorrect reasoning chains, especially when facing complex mathematical problems.","Previous studies only transfer knowledge from positive samples and drop the synthesized data with wrong answers.","In this work, we illustrate the merit of negative data and propose a model specialization framework to distill LLMs with negative samples besides positive ones.","The framework consists of three progressive steps, covering from training to inference stages, to absorb knowledge from negative data.","We conduct extensive experiments across arithmetic reasoning tasks to demonstrate the role of negative data in distillation from LLM."],"url":"http://arxiv.org/abs/2312.12832v1"}
{"created":"2023-12-20 07:22:38","title":"Graph-Based Generalization of Galam Model: Convergence Time and Influential Nodes","abstract":"We study a graph-based generalization of the Galam opinion formation model. Consider a simple connected graph which represents a social network. Each node in the graph is colored either blue or white, which indicates a positive or negative opinion on a new product or a topic. In each discrete-time round, all nodes are assigned randomly to groups of different sizes, where the node(s) in each group form a clique in the underlying graph. All the nodes simultaneously update their color to the majority color in their group. If there is a tie, each node in the group chooses one of the two colors uniformly at random. Investigating the convergence time of the model, our experiments show that the convergence time is a logarithm function of the number of nodes for a complete graph and a quadratic function for a cycle graph. We also study the various strategies for selecting a set of seed nodes to maximize the final cascade of one of the two colors, motivated by viral marketing. We consider the algorithms where the seed nodes are selected based on the graph structure (nodes' centrality measures such as degree, betweenness, and closeness) and the individual's characteristics (activeness and stubbornness). We provide a comparison of such strategies by conducting experiments on different real-world and synthetic networks.","sentences":["We study a graph-based generalization of the Galam opinion formation model.","Consider a simple connected graph which represents a social network.","Each node in the graph is colored either blue or white, which indicates a positive or negative opinion on a new product or a topic.","In each discrete-time round, all nodes are assigned randomly to groups of different sizes, where the node(s) in each group form a clique in the underlying graph.","All the nodes simultaneously update their color to the majority color in their group.","If there is a tie, each node in the group chooses one of the two colors uniformly at random.","Investigating the convergence time of the model, our experiments show that the convergence time is a logarithm function of the number of nodes for a complete graph and a quadratic function for a cycle graph.","We also study the various strategies for selecting a set of seed nodes to maximize the final cascade of one of the two colors, motivated by viral marketing.","We consider the algorithms where the seed nodes are selected based on the graph structure (nodes' centrality measures such as degree, betweenness, and closeness) and the individual's characteristics (activeness and stubbornness).","We provide a comparison of such strategies by conducting experiments on different real-world and synthetic networks."],"url":"http://arxiv.org/abs/2312.12811v1"}
{"created":"2023-12-20 06:52:38","title":"Multi-stages attention Breast cancer classification based on nonlinear spiking neural P neurons with autapses","abstract":"Breast cancer(BC) is a prevalent type of malignant tumor in women. Early diagnosis and treatment are vital for enhancing the patients' survival rate. Downsampling in deep networks may lead to loss of information, so for compensating the detail and edge information and allowing convolutional neural networks to pay more attention to seek the lesion region, we propose a multi-stages attention architecture based on NSNP neurons with autapses. First, unlike the single-scale attention acquisition methods of existing methods, we set up spatial attention acquisition at each feature map scale of the convolutional network to obtain an fusion global information on attention guidance. Then we introduce a new type of NSNP variants called NSNP neurons with autapses. Specifically, NSNP systems are modularized as feature encoders, recoding the features extracted from convolutional neural network as well as the fusion of attention information and preserve the key characteristic elements in feature maps. This ensures the retention of valuable data while gradually transforming high-dimensional complicated info into low-dimensional ones. The proposed method is evaluated on the public dataset BreakHis at various magnifications and classification tasks. It achieves a classification accuracy of 96.32% at all magnification cases, outperforming state-of-the-art methods. Ablation studies are also performed, verifying the proposed model's efficacy. The source code is available at XhuBobYoung/Breast-cancer-Classification.","sentences":["Breast cancer(BC) is a prevalent type of malignant tumor in women.","Early diagnosis and treatment are vital for enhancing the patients' survival rate.","Downsampling in deep networks may lead to loss of information, so for compensating the detail and edge information and allowing convolutional neural networks to pay more attention to seek the lesion region, we propose a multi-stages attention architecture based on NSNP neurons with autapses.","First, unlike the single-scale attention acquisition methods of existing methods, we set up spatial attention acquisition at each feature map scale of the convolutional network to obtain an fusion global information on attention guidance.","Then we introduce a new type of NSNP variants called NSNP neurons with autapses.","Specifically, NSNP systems are modularized as feature encoders, recoding the features extracted from convolutional neural network as well as the fusion of attention information and preserve the key characteristic elements in feature maps.","This ensures the retention of valuable data while gradually transforming high-dimensional complicated info into low-dimensional ones.","The proposed method is evaluated on the public dataset BreakHis at various magnifications and classification tasks.","It achieves a classification accuracy of 96.32% at all magnification cases, outperforming state-of-the-art methods.","Ablation studies are also performed, verifying the proposed model's efficacy.","The source code is available at XhuBobYoung/Breast-cancer-Classification."],"url":"http://arxiv.org/abs/2312.12804v1"}
{"created":"2023-12-20 06:37:58","title":"Join Sampling under Acyclic Degree Constraints and (Cyclic) Subgraph Sampling","abstract":"Given a join with an acyclic set of degree constraints, we show how to draw a uniformly random sample from the join result in $O(\\mathit{polymat}/ \\max \\{1, \\mathrm{OUT} \\})$ expected time after a preprocessing of $O(\\mathrm{IN})$ expected time, where $\\mathrm{IN}$, $\\mathrm{OUT}$, and $\\mathit{polymat}$ are the join's input size, output size, and polymatroid bound, respectively. This compares favorably with the state of the art (Deng et al.\\ and Kim et al., both in PODS'23), which states that a uniformly random sample can be drawn in $\\tilde{O}(\\mathrm{AGM} / \\max \\{1, \\mathrm{OUT}\\})$ expected time after a preprocessing phase of $\\tilde{O}(\\mathrm{IN})$ expected time, where $\\mathrm{AGM}$ is the join's AGM bound.   We then utilize our techniques to tackle {\\em directed subgraph sampling}. Let $G = (V, E)$ be a directed data graph where each vertex has an out-degree at most $\\lambda$, and let $P$ be a directed pattern graph with $O(1)$ vertices. The objective is to uniformly sample an occurrence of $P$ in $G$. The problem can be modeled as join sampling with input size $\\mathrm{IN} = \\Theta(|E|)$ but, whenever $P$ contains cycles, the converted join has {\\em cyclic} degree constraints. We show that it is always possible to throw away certain degree constraints such that (i) the remaining constraints are acyclic and (ii) the new join has asymptotically the same polymatroid bound $\\mathit{polymat}$ as the old one. Combining this finding with our new join sampling solution yields an algorithm to sample from the original (cyclic) join (thereby yielding a uniformly random occurrence of $P$) in $O(\\mathit{polymat}/ \\max \\{1, \\mathrm{OUT}\\})$ expected time after $O(|E|)$ expected-time preprocessing. We also prove similar results for {\\em undirected subgraph sampling} and demonstrate how our techniques can be significantly simplified in that scenario.","sentences":["Given a join with an acyclic set of degree constraints, we show how to draw a uniformly random sample from the join result in $O(\\mathit{polymat}/ \\max \\{1, \\mathrm{OUT} \\})$ expected time after a preprocessing of $O(\\mathrm{IN})$ expected time, where $\\mathrm{IN}$, $\\mathrm{OUT}$, and $\\mathit{polymat}$ are the join's input size, output size, and polymatroid bound, respectively.","This compares favorably with the state of the art (Deng et al.\\ and Kim et al., both in PODS'23), which states that a uniformly random sample can be drawn in $\\tilde{O}(\\mathrm{AGM} / \\max \\{1, \\mathrm{OUT}\\})$ expected time after a preprocessing phase of $\\tilde{O}(\\mathrm{IN})$ expected time, where $\\mathrm{AGM}$ is the join's AGM bound.   ","We then utilize our techniques to tackle {\\em directed subgraph sampling}.","Let $G = (V, E)$ be a directed data graph where each vertex has an out-degree at most $\\lambda$, and let $P$ be a directed pattern graph with $O(1)$ vertices.","The objective is to uniformly sample an occurrence of $P$ in $G$. The problem can be modeled as join sampling with input size $\\mathrm{IN} = \\Theta(|E|)$ but, whenever $P$ contains cycles, the converted join has {\\em cyclic} degree constraints.","We show that it is always possible to throw away certain degree constraints such that (i) the remaining constraints are acyclic and (ii) the new join has asymptotically the same polymatroid bound $\\mathit{polymat}$ as the old one.","Combining this finding with our new join sampling solution yields an algorithm to sample from the original (cyclic) join (thereby yielding a uniformly random occurrence of $P$) in $O(\\mathit{polymat}/ \\max \\{1, \\mathrm{OUT}\\})$ expected time after $O(|E|)$ expected-time preprocessing.","We also prove similar results for {\\em undirected subgraph sampling} and demonstrate how our techniques can be significantly simplified in that scenario."],"url":"http://arxiv.org/abs/2312.12797v1"}
{"created":"2023-12-20 06:34:15","title":"Bandit Sequential Posted Pricing via Half-Concavity","abstract":"Sequential posted pricing auctions are popular because of their simplicity in practice and their tractability in theory. A usual assumption in their study is that the Bayesian prior distributions of the buyers are known to the seller, while in reality these priors can only be accessed from historical data. To overcome this assumption, we study sequential posted pricing in the bandit learning model, where the seller interacts with $n$ buyers over $T$ rounds: In each round the seller posts $n$ prices for the $n$ buyers and the first buyer with a valuation higher than the price takes the item. The only feedback that the seller receives in each round is the revenue.   Our main results obtain nearly-optimal regret bounds for single-item sequential posted pricing in the bandit learning model. In particular, we achieve an $\\tilde{O}(\\mathsf{poly}(n)\\sqrt{T})$ regret for buyers with (Myerson's) regular distributions and an $\\tilde{O}(\\mathsf{poly}(n)T^{{2}/{3}})$ regret for buyers with general distributions, both of which are tight in the number of rounds $T$. Our result for regular distributions was previously not known even for the single-buyer setting and relies on a new half-concavity property of the revenue function in the value space. For $n$ sequential buyers, our technique is to run a generalized single-buyer algorithm for all the buyers and to carefully bound the regret from the sub-optimal pricing of the suffix buyers.","sentences":["Sequential posted pricing auctions are popular because of their simplicity in practice and their tractability in theory.","A usual assumption in their study is that the Bayesian prior distributions of the buyers are known to the seller, while in reality these priors can only be accessed from historical data.","To overcome this assumption, we study sequential posted pricing in the bandit learning model, where the seller interacts with $n$ buyers over $T$ rounds: In each round the seller posts $n$ prices for the $n$ buyers and the first buyer with a valuation higher than the price takes the item.","The only feedback that the seller receives in each round is the revenue.   ","Our main results obtain nearly-optimal regret bounds for single-item sequential posted pricing in the bandit learning model.","In particular, we achieve an $\\tilde{O}(\\mathsf{poly}(n)\\sqrt{T})$ regret for buyers with (Myerson's) regular distributions and an $\\tilde{O}(\\mathsf{poly}(n)T^{{2}/{3}})$ regret for buyers with general distributions, both of which are tight in the number of rounds $T$. Our result for regular distributions was previously not known even for the single-buyer setting and relies on a new half-concavity property of the revenue function in the value space.","For $n$ sequential buyers, our technique is to run a generalized single-buyer algorithm for all the buyers and to carefully bound the regret from the sub-optimal pricing of the suffix buyers."],"url":"http://arxiv.org/abs/2312.12794v1"}
{"created":"2023-12-20 06:10:27","title":"Fast Cell Library Characterization for Design Technology Co-Optimization Based on Graph Neural Networks","abstract":"Design technology co-optimization (DTCO) plays a critical role in achieving optimal power, performance, and area (PPA) for advanced semiconductor process development. Cell library characterization is essential in DTCO flow, but traditional methods are time-consuming and costly. To overcome these challenges, we propose a graph neural network (GNN)-based machine learning model for rapid and accurate cell library characterization. Our model incorporates cell structures and demonstrates high prediction accuracy across various process-voltage-temperature (PVT) corners and technology parameters. Validation with 512 unseen technology corners and over one million test data points shows accurate predictions of delay, power, and input pin capacitance for 33 types of cells, with a mean absolute percentage error (MAPE) $\\le$ 0.95% and a speed-up of 100X compared with SPICE simulations. Additionally, we investigate system-level metrics such as worst negative slack (WNS), leakage power, and dynamic power using predictions obtained from the GNN-based model on unseen corners. Our model achieves precise predictions, with absolute error $\\le$3.0 ps for WNS, percentage errors $\\le$0.60% for leakage power, and $\\le$0.99% for dynamic power, when compared to golden reference. With the developed model, we further proposed a fine-grained drive strength interpolation methodology to enhance PPA for small-to-medium-scale designs, resulting in an approximate 1-3% improvement.","sentences":["Design technology co-optimization (DTCO) plays a critical role in achieving optimal power, performance, and area (PPA) for advanced semiconductor process development.","Cell library characterization is essential in DTCO flow, but traditional methods are time-consuming and costly.","To overcome these challenges, we propose a graph neural network (GNN)-based machine learning model for rapid and accurate cell library characterization.","Our model incorporates cell structures and demonstrates high prediction accuracy across various process-voltage-temperature (PVT) corners and technology parameters.","Validation with 512 unseen technology corners and over one million test data points shows accurate predictions of delay, power, and input pin capacitance for 33 types of cells, with a mean absolute percentage error (MAPE) $\\le$ 0.95% and a speed-up of 100X compared with SPICE simulations.","Additionally, we investigate system-level metrics such as worst negative slack (WNS), leakage power, and dynamic power using predictions obtained from the GNN-based model on unseen corners.","Our model achieves precise predictions, with absolute error $\\le$3.0 ps for WNS, percentage errors $\\le$0.60% for leakage power, and $\\le$0.99% for dynamic power, when compared to golden reference.","With the developed model, we further proposed a fine-grained drive strength interpolation methodology to enhance PPA for small-to-medium-scale designs, resulting in an approximate 1-3% improvement."],"url":"http://arxiv.org/abs/2312.12784v1"}
{"created":"2023-12-20 05:34:12","title":"Collaborative business intelligence virtual assistant","abstract":"The present-day business landscape necessitates novel methodologies that integrate intelligent technologies and tools capable of swiftly providing precise and dependable information for decision-making purposes. Contemporary society is characterized by vast amounts of accumulated data across various domains, which hold considerable potential for informing and guiding decision-making processes. However, these data are typically collected and stored by disparate and unrelated software systems, stored in diverse formats, and offer varying levels of accessibility and security. To address the challenges associated with processing such large volumes of data, organizations often rely on data analysts. Nonetheless, a significant hurdle in harnessing the benefits of accumulated data lies in the lack of direct communication between technical specialists, decision-makers, and business process analysts. To overcome this issue, the application of collaborative business intelligence (CBI) emerges as a viable solution. This research focuses on the applications of data mining and aims to model CBI processes within distributed virtual teams through the interaction of users and a CBI Virtual Assistant. The proposed virtual assistant for CBI endeavors to enhance data exploration accessibility for a wider range of users and streamline the time and effort required for data analysis. The key contributions of this study encompass: 1) a reference model representing collaborative BI, inspired by linguistic theory; 2) an approach that enables the transformation of user queries into executable commands, thereby facilitating their utilization within data exploration software; and 3) the primary workflow of a conversational agent designed for data analytics.","sentences":["The present-day business landscape necessitates novel methodologies that integrate intelligent technologies and tools capable of swiftly providing precise and dependable information for decision-making purposes.","Contemporary society is characterized by vast amounts of accumulated data across various domains, which hold considerable potential for informing and guiding decision-making processes.","However, these data are typically collected and stored by disparate and unrelated software systems, stored in diverse formats, and offer varying levels of accessibility and security.","To address the challenges associated with processing such large volumes of data, organizations often rely on data analysts.","Nonetheless, a significant hurdle in harnessing the benefits of accumulated data lies in the lack of direct communication between technical specialists, decision-makers, and business process analysts.","To overcome this issue, the application of collaborative business intelligence (CBI) emerges as a viable solution.","This research focuses on the applications of data mining and aims to model CBI processes within distributed virtual teams through the interaction of users and a CBI Virtual Assistant.","The proposed virtual assistant for CBI endeavors to enhance data exploration accessibility for a wider range of users and streamline the time and effort required for data analysis.","The key contributions of this study encompass: 1) a reference model representing collaborative BI, inspired by linguistic theory; 2) an approach that enables the transformation of user queries into executable commands, thereby facilitating their utilization within data exploration software; and 3) the primary workflow of a conversational agent designed for data analytics."],"url":"http://arxiv.org/abs/2312.12778v1"}
{"created":"2023-12-20 05:18:12","title":"Data Extraction, Transformation, and Loading Process Automation for Algorithmic Trading Machine Learning Modelling and Performance Optimization","abstract":"A data warehouse efficiently prepares data for effective and fast data analysis and modelling using machine learning algorithms. This paper discusses existing solutions for the Data Extraction, Transformation, and Loading (ETL) process and automation for algorithmic trading algorithms. Integrating the Data Warehouses and, in the future, the Data Lakes with the Machine Learning Algorithms gives enormous opportunities in research when performance and data processing time become critical non-functional requirements.","sentences":["A data warehouse efficiently prepares data for effective and fast data analysis and modelling using machine learning algorithms.","This paper discusses existing solutions for the Data Extraction, Transformation, and Loading (ETL) process and automation for algorithmic trading algorithms.","Integrating the Data Warehouses and, in the future, the Data Lakes with the Machine Learning Algorithms gives enormous opportunities in research when performance and data processing time become critical non-functional requirements."],"url":"http://arxiv.org/abs/2312.12774v1"}
{"created":"2023-12-20 05:16:04","title":"Realistic Rainy Weather Simulation for LiDARs in CARLA Simulator","abstract":"Employing data augmentation methods to enhance perception performance in adverse weather has attracted considerable attention recently. Most of the LiDAR augmentation methods post-process the existing dataset by physics-based models or machine-learning methods. However, due to the limited environmental annotations and the fixed vehicle trajectories in the existing dataset, it is challenging to edit the scene and expand the diversity of traffic flow and scenario. To this end, we propose a simulator-based physical modeling approach to augment LiDAR data in rainy weather in order to improve the perception performance of LiDAR in this scenario. We complete the modeling task of the rainy weather in the CARLA simulator and establish a pipeline for LiDAR data collection. In particular, we pay special attention to the spray and splash rolled up by the wheels of surrounding vehicles in rain and complete the simulation of this special scenario through the Spray Emitter method we developed. In addition, we examine the influence of different weather conditions on the intensity of the LiDAR echo, develop a prediction network for the intensity of the LiDAR echo, and complete the simulation of 4-feat LiDAR point cloud data. In the experiment, we observe that the model augmented by the synthetic data improves the object detection task's performance in the rainy sequence of the Waymo Open Dataset. Both the code and the dataset will be made publicly available at https://github.com/PJLab-ADG/PCSim#rainypcsim.","sentences":["Employing data augmentation methods to enhance perception performance in adverse weather has attracted considerable attention recently.","Most of the LiDAR augmentation methods post-process the existing dataset by physics-based models or machine-learning methods.","However, due to the limited environmental annotations and the fixed vehicle trajectories in the existing dataset, it is challenging to edit the scene and expand the diversity of traffic flow and scenario.","To this end, we propose a simulator-based physical modeling approach to augment LiDAR data in rainy weather in order to improve the perception performance of LiDAR in this scenario.","We complete the modeling task of the rainy weather in the CARLA simulator and establish a pipeline for LiDAR data collection.","In particular, we pay special attention to the spray and splash rolled up by the wheels of surrounding vehicles in rain and complete the simulation of this special scenario through the Spray Emitter method we developed.","In addition, we examine the influence of different weather conditions on the intensity of the LiDAR echo, develop a prediction network for the intensity of the LiDAR echo, and complete the simulation of 4-feat LiDAR point cloud data.","In the experiment, we observe that the model augmented by the synthetic data improves the object detection task's performance in the rainy sequence of the Waymo Open Dataset.","Both the code and the dataset will be made publicly available at https://github.com/PJLab-ADG/PCSim#rainypcsim."],"url":"http://arxiv.org/abs/2312.12772v1"}
{"created":"2023-12-20 05:02:40","title":"IOPS: An Unified SpMM Accelerator Based on Inner-Outer-Hybrid Product","abstract":"Sparse matrix multiplication (SpMM) is widely applied to numerous domains, such as graph processing, machine learning, and data analytics. However, inner product based SpMM induces redundant zero-element computing for mismatched nonzero operands, while outer product based approach lacks input reuse across Process Elements (PEs) and poor output locality for accumulating partial sum (psum) matrices. Besides, current works only focus on sparse-sparse matrix multiplication (SSMM) or sparse-dense matrix multiplication (SDMM), rarely performing efficiently for both. To address these problems, this paper proposes an unified SpMM accelerator, called IOPS, hybridizing inner with outer products. It reuses the input matrix among PEs with inner product dataflow, and removes zero-element calculations with outer product approach in each PE, which can efficiently process SSMM and SDMM. Moreover, an address mapping method is designed to accumulate the irregular sparse psum matrices, reducing the latency and DRAM access of psum accumulating. Furthermore, an adaptive partition strategy is proposed to tile the input matrices based on their sparsity ratios, effectively utilizing the storage of architecture and reducing DRAM access. Compared with the SSMM accelerator, SpArch, we achieve 1.7x~6.3x energy efficiency and 1.2x~4.4x resource efficiency, with 1.4x~2.1x DRAM access saving.","sentences":["Sparse matrix multiplication (SpMM) is widely applied to numerous domains, such as graph processing, machine learning, and data analytics.","However, inner product based SpMM induces redundant zero-element computing for mismatched nonzero operands, while outer product based approach lacks input reuse across Process Elements (PEs) and poor output locality for accumulating partial sum (psum) matrices.","Besides, current works only focus on sparse-sparse matrix multiplication (SSMM) or sparse-dense matrix multiplication (SDMM), rarely performing efficiently for both.","To address these problems, this paper proposes an unified SpMM accelerator, called IOPS, hybridizing inner with outer products.","It reuses the input matrix among PEs with inner product dataflow, and removes zero-element calculations with outer product approach in each PE, which can efficiently process SSMM and SDMM.","Moreover, an address mapping method is designed to accumulate the irregular sparse psum matrices, reducing the latency and DRAM access of psum accumulating.","Furthermore, an adaptive partition strategy is proposed to tile the input matrices based on their sparsity ratios, effectively utilizing the storage of architecture and reducing DRAM access.","Compared with the SSMM accelerator, SpArch, we achieve 1.7x~6.3x energy efficiency and 1.2x~4.4x resource efficiency, with 1.4x~2.1x DRAM access saving."],"url":"http://arxiv.org/abs/2312.12766v1"}
{"created":"2023-12-20 04:15:01","title":"Human-Centred Learning Analytics and AI in Education: a Systematic Literature Review","abstract":"The rapid expansion of Learning Analytics (LA) and Artificial Intelligence in Education (AIED) offers new scalable, data-intensive systems but also raises concerns about data privacy and agency. Excluding stakeholders -- like students and teachers -- from the design process can potentially lead to mistrust and inadequately aligned tools. Despite a shift towards human-centred design in recent LA and AIED research, there remain gaps in our understanding of the importance of human control, safety, reliability, and trustworthiness in the design and implementation of these systems. We conducted a systematic literature review to explore these concerns and gaps. We analysed 108 papers to provide insights about i) the current state of human-centred LA/AIED research; ii) the extent to which educational stakeholders have contributed to the design process of human-centred LA/AIED systems; iii) the current balance between human control and computer automation of such systems; and iv) the extent to which safety, reliability and trustworthiness have been considered in the literature. Results indicate some consideration of human control in LA/AIED system design, but limited end-user involvement in actual design. Based on these findings, we recommend: 1) carefully balancing stakeholders' involvement in designing and deploying LA/AIED systems throughout all design phases, 2) actively involving target end-users, especially students, to delineate the balance between human control and automation, and 3) exploring safety, reliability, and trustworthiness as principles in future human-centred LA/AIED systems.","sentences":["The rapid expansion of Learning Analytics (LA) and Artificial Intelligence in Education (AIED) offers new scalable, data-intensive systems but also raises concerns about data privacy and agency.","Excluding stakeholders -- like students and teachers -- from the design process can potentially lead to mistrust and inadequately aligned tools.","Despite a shift towards human-centred design in recent LA and AIED research, there remain gaps in our understanding of the importance of human control, safety, reliability, and trustworthiness in the design and implementation of these systems.","We conducted a systematic literature review to explore these concerns and gaps.","We analysed 108 papers to provide insights about i) the current state of human-centred LA/AIED research; ii) the extent to which educational stakeholders have contributed to the design process of human-centred LA/AIED systems; iii) the current balance between human control and computer automation of such systems; and iv) the extent to which safety, reliability and trustworthiness have been considered in the literature.","Results indicate some consideration of human control in LA/AIED system design, but limited end-user involvement in actual design.","Based on these findings, we recommend: 1) carefully balancing stakeholders' involvement in designing and deploying LA/AIED systems throughout all design phases, 2) actively involving target end-users, especially students, to delineate the balance between human control and automation, and 3) exploring safety, reliability, and trustworthiness as principles in future human-centred LA/AIED systems."],"url":"http://arxiv.org/abs/2312.12751v1"}
{"created":"2023-12-20 03:40:45","title":"ChatFDA: Medical Records Risk Assessment","abstract":"In healthcare, the emphasis on patient safety and the minimization of medical errors cannot be overstated. Despite concerted efforts, many healthcare systems, especially in low-resource regions, still grapple with preventing these errors effectively. This study explores a pioneering application aimed at addressing this challenge by assisting caregivers in gauging potential risks derived from medical notes. The application leverages data from openFDA, delivering real-time, actionable insights regarding prescriptions. Preliminary analyses conducted on the MIMIC-III \\cite{mimic} dataset affirm a proof of concept highlighting a reduction in medical errors and an amplification in patient safety. This tool holds promise for drastically enhancing healthcare outcomes in settings with limited resources. To bolster reproducibility and foster further research, the codebase underpinning our methodology is accessible on https://github.com/autonlab/2023.hackAuton/tree/main/prescription_checker. This is a submission for the 30th HackAuton CMU.","sentences":["In healthcare, the emphasis on patient safety and the minimization of medical errors cannot be overstated.","Despite concerted efforts, many healthcare systems, especially in low-resource regions, still grapple with preventing these errors effectively.","This study explores a pioneering application aimed at addressing this challenge by assisting caregivers in gauging potential risks derived from medical notes.","The application leverages data from openFDA, delivering real-time, actionable insights regarding prescriptions.","Preliminary analyses conducted on the MIMIC-III \\cite{mimic} dataset affirm a proof of concept highlighting a reduction in medical errors and an amplification in patient safety.","This tool holds promise for drastically enhancing healthcare outcomes in settings with limited resources.","To bolster reproducibility and foster further research, the codebase underpinning our methodology is accessible on https://github.com/autonlab/2023.hackAuton/tree/main/prescription_checker.","This is a submission for the 30th HackAuton CMU."],"url":"http://arxiv.org/abs/2312.12746v1"}
{"created":"2023-12-20 03:18:50","title":"Learning and Forgetting Unsafe Examples in Large Language Models","abstract":"As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the \"ForgetFilter\" algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.","sentences":["As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data.","We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content.","Drawing inspiration from the discrepancies in forgetting, we introduce the \"ForgetFilter\" algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data.","We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning.","ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score."],"url":"http://arxiv.org/abs/2312.12736v1"}
{"created":"2023-12-20 03:16:34","title":"MetaSegNet: Metadata-collaborative Vision-Language Representation Learning for Semantic Segmentation of Remote Sensing Images","abstract":"Semantic segmentation of remote sensing images plays a vital role in a wide range of Earth Observation (EO) applications, such as land use land cover mapping, environment monitoring, and sustainable development. Driven by rapid developments in Artificial Intelligence (AI), deep learning (DL) has emerged as the mainstream tool for semantic segmentation and achieved many breakthroughs in the field of remote sensing. However, the existing DL-based methods mainly focus on unimodal visual data while ignoring the rich multimodal information involved in the real world, usually demonstrating weak reliability and generlization. Inspired by the success of Vision Transformers and large language models, we propose a novel metadata-collaborative multimodal segmentation network (MetaSegNet) that applies vision-language representation learning for semantic segmentation of remote sensing images. Unlike the common model structure that only uses unimodal visual data, we extract the key characteristic (i.e. the climate zone) from freely available remote sensing image metadata and transfer it into knowledge-based text prompts via the generic ChatGPT. Then, we construct an image encoder, a text encoder and a crossmodal attention fusion subnetwork to extract the image and text feature and apply image-text interaction. Benefiting from such a design, the proposed MetaSegNet demonstrates superior generalization and achieves competitive accuracy with state-of-the-art semantic segmentation methods on the large-scale OpenEarthMap dataset (68.6% mIoU) and Potsdam dataset (93.3% mean F1 score) as well as LoveDA dataset (52.2% mIoU).","sentences":["Semantic segmentation of remote sensing images plays a vital role in a wide range of Earth Observation (EO) applications, such as land use land cover mapping, environment monitoring, and sustainable development.","Driven by rapid developments in Artificial Intelligence (AI), deep learning (DL) has emerged as the mainstream tool for semantic segmentation and achieved many breakthroughs in the field of remote sensing.","However, the existing DL-based methods mainly focus on unimodal visual data while ignoring the rich multimodal information involved in the real world, usually demonstrating weak reliability and generlization.","Inspired by the success of Vision Transformers and large language models, we propose a novel metadata-collaborative multimodal segmentation network (MetaSegNet) that applies vision-language representation learning for semantic segmentation of remote sensing images.","Unlike the common model structure that only uses unimodal visual data, we extract the key characteristic (i.e. the climate zone) from freely available remote sensing image metadata and transfer it into knowledge-based text prompts via the generic ChatGPT.","Then, we construct an image encoder, a text encoder and a crossmodal attention fusion subnetwork to extract the image and text feature and apply image-text interaction.","Benefiting from such a design, the proposed MetaSegNet demonstrates superior generalization and achieves competitive accuracy with state-of-the-art semantic segmentation methods on the large-scale OpenEarthMap dataset (68.6% mIoU) and Potsdam dataset (93.3% mean F1 score) as well as LoveDA dataset (52.2% mIoU)."],"url":"http://arxiv.org/abs/2312.12735v1"}
{"created":"2023-12-20 03:09:50","title":"Strassen's Matrix Multiplication Algorithm Is Still Faster","abstract":"Recently, reinforcement algorithms discovered new algorithms that really jump-started a wave of excitements and a flourishing of publications. However, there is little on implementations, applications, and, especially, no absolute performance and, we show here they are not here to replace Strassen's original fast matrix multiplication yet. We present Matrix Flow, this is a simple Python project for the automatic formulation, design, implementation, code generation, and execution of fast matrix multiplication algorithms for CPUs, using BLAS interface GPUs, and in the future other accelerators. We shall not play with module-2 (Z2) algorithms and, for simplicity, we present only square double-precision matrices. By means of factorizing the operand matrices we can express many algorithms and prove them correct. These algorithms are represented by Data Flows and matrix data partitions: a Directed Acyclic Graph. We show that Strassen's original algorithm is still the top choice even for modern GPUs. We also address error analysis in double precision, because integer computations are correct, always","sentences":["Recently, reinforcement algorithms discovered new algorithms that really jump-started a wave of excitements and a flourishing of publications.","However, there is little on implementations, applications, and, especially, no absolute performance and, we show here they are not here to replace Strassen's original fast matrix multiplication yet.","We present Matrix Flow, this is a simple Python project for the automatic formulation, design, implementation, code generation, and execution of fast matrix multiplication algorithms for CPUs, using BLAS interface GPUs, and in the future other accelerators.","We shall not play with module-2 (Z2) algorithms and, for simplicity, we present only square double-precision matrices.","By means of factorizing the operand matrices we can express many algorithms and prove them correct.","These algorithms are represented by Data Flows and matrix data partitions: a Directed Acyclic Graph.","We show that Strassen's original algorithm is still the top choice even for modern GPUs.","We also address error analysis in double precision, because integer computations are correct, always"],"url":"http://arxiv.org/abs/2312.12732v1"}
{"created":"2023-12-20 03:03:06","title":"Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach","abstract":"This paper studies bandit problems where an agent has access to offline data that might be utilized to potentially improve the estimation of each arm's reward distribution. A major obstacle in this setting is the existence of compound biases from the observational data. Ignoring these biases and blindly fitting a model with the biased data could even negatively affect the online learning phase. In this work, we formulate this problem from a causal perspective. First, we categorize the biases into confounding bias and selection bias based on the causal structure they imply. Next, we extract the causal bound for each arm that is robust towards compound biases from biased observational data. The derived bounds contain the ground truth mean reward and can effectively guide the bandit agent to learn a nearly-optimal decision policy. We also conduct regret analysis in both contextual and non-contextual bandit settings and show that prior causal bounds could help consistently reduce the asymptotic regret.","sentences":["This paper studies bandit problems where an agent has access to offline data that might be utilized to potentially improve the estimation of each arm's reward distribution.","A major obstacle in this setting is the existence of compound biases from the observational data.","Ignoring these biases and blindly fitting a model with the biased data could even negatively affect the online learning phase.","In this work, we formulate this problem from a causal perspective.","First, we categorize the biases into confounding bias and selection bias based on the causal structure they imply.","Next, we extract the causal bound for each arm that is robust towards compound biases from biased observational data.","The derived bounds contain the ground truth mean reward and can effectively guide the bandit agent to learn a nearly-optimal decision policy.","We also conduct regret analysis in both contextual and non-contextual bandit settings and show that prior causal bounds could help consistently reduce the asymptotic regret."],"url":"http://arxiv.org/abs/2312.12731v1"}
{"created":"2023-12-20 02:40:28","title":"Progressive Poisoned Data Isolation for Training-time Backdoor Defense","abstract":"Deep Neural Networks (DNN) are susceptible to backdoor attacks where malicious attackers manipulate the model's predictions via data poisoning. It is hence imperative to develop a strategy for training a clean model using a potentially poisoned dataset. Previous training-time defense mechanisms typically employ an one-time isolation process, often leading to suboptimal isolation outcomes. In this study, we present a novel and efficacious defense method, termed Progressive Isolation of Poisoned Data (PIPD), that progressively isolates poisoned data to enhance the isolation accuracy and mitigate the risk of benign samples being misclassified as poisoned ones. Once the poisoned portion of the dataset has been identified, we introduce a selective training process to train a clean model. Through the implementation of these techniques, we ensure that the trained model manifests a significantly diminished attack success rate against the poisoned data. Extensive experiments on multiple benchmark datasets and DNN models, assessed against nine state-of-the-art backdoor attacks, demonstrate the superior performance of our PIPD method for backdoor defense. For instance, our PIPD achieves an average True Positive Rate (TPR) of 99.95% and an average False Positive Rate (FPR) of 0.06% for diverse attacks over CIFAR-10 dataset, markedly surpassing the performance of state-of-the-art methods.","sentences":["Deep Neural Networks (DNN) are susceptible to backdoor attacks where malicious attackers manipulate the model's predictions via data poisoning.","It is hence imperative to develop a strategy for training a clean model using a potentially poisoned dataset.","Previous training-time defense mechanisms typically employ an one-time isolation process, often leading to suboptimal isolation outcomes.","In this study, we present a novel and efficacious defense method, termed Progressive Isolation of Poisoned Data (PIPD), that progressively isolates poisoned data to enhance the isolation accuracy and mitigate the risk of benign samples being misclassified as poisoned ones.","Once the poisoned portion of the dataset has been identified, we introduce a selective training process to train a clean model.","Through the implementation of these techniques, we ensure that the trained model manifests a significantly diminished attack success rate against the poisoned data.","Extensive experiments on multiple benchmark datasets and DNN models, assessed against nine state-of-the-art backdoor attacks, demonstrate the superior performance of our PIPD method for backdoor defense.","For instance, our PIPD achieves an average True Positive Rate (TPR) of 99.95% and an average False Positive Rate (FPR) of 0.06% for diverse attacks over CIFAR-10 dataset, markedly surpassing the performance of state-of-the-art methods."],"url":"http://arxiv.org/abs/2312.12724v1"}
{"created":"2023-12-20 02:34:11","title":"Fine-Grained Knowledge Selection and Restoration for Non-Exemplar Class Incremental Learning","abstract":"Non-exemplar class incremental learning aims to learn both the new and old tasks without accessing any training data from the past. This strict restriction enlarges the difficulty of alleviating catastrophic forgetting since all techniques can only be applied to current task data. Considering this challenge, we propose a novel framework of fine-grained knowledge selection and restoration. The conventional knowledge distillation-based methods place too strict constraints on the network parameters and features to prevent forgetting, which limits the training of new tasks. To loose this constraint, we proposed a novel fine-grained selective patch-level distillation to adaptively balance plasticity and stability. Some task-agnostic patches can be used to preserve the decision boundary of the old task. While some patches containing the important foreground are favorable for learning the new task.   Moreover, we employ a task-agnostic mechanism to generate more realistic prototypes of old tasks with the current task sample for reducing classifier bias for fine-grained knowledge restoration. Extensive experiments on CIFAR100, TinyImageNet and ImageNet-Subset demonstrate the effectiveness of our method. Code is available at https://github.com/scok30/vit-cil.","sentences":["Non-exemplar class incremental learning aims to learn both the new and old tasks without accessing any training data from the past.","This strict restriction enlarges the difficulty of alleviating catastrophic forgetting since all techniques can only be applied to current task data.","Considering this challenge, we propose a novel framework of fine-grained knowledge selection and restoration.","The conventional knowledge distillation-based methods place too strict constraints on the network parameters and features to prevent forgetting, which limits the training of new tasks.","To loose this constraint, we proposed a novel fine-grained selective patch-level distillation to adaptively balance plasticity and stability.","Some task-agnostic patches can be used to preserve the decision boundary of the old task.","While some patches containing the important foreground are favorable for learning the new task.   ","Moreover, we employ a task-agnostic mechanism to generate more realistic prototypes of old tasks with the current task sample for reducing classifier bias for fine-grained knowledge restoration.","Extensive experiments on CIFAR100, TinyImageNet and ImageNet-Subset demonstrate the effectiveness of our method.","Code is available at https://github.com/scok30/vit-cil."],"url":"http://arxiv.org/abs/2312.12722v1"}
{"created":"2023-12-20 02:30:39","title":"Cross-Modal Reasoning with Event Correlation for Video Question Answering","abstract":"Video Question Answering (VideoQA) is a very attractive and challenging research direction aiming to understand complex semantics of heterogeneous data from two domains, i.e., the spatio-temporal video content and the word sequence in question. Although various attention mechanisms have been utilized to manage contextualized representations by modeling intra- and inter-modal relationships of the two modalities, one limitation of the predominant VideoQA methods is the lack of reasoning with event correlation, that is, sensing and analyzing relationships among abundant and informative events contained in the video. In this paper, we introduce the dense caption modality as a new auxiliary and distill event-correlated information from it to infer the correct answer. To this end, we propose a novel end-to-end trainable model, Event-Correlated Graph Neural Networks (EC-GNNs), to perform cross-modal reasoning over information from the three modalities (i.e., caption, video, and question). Besides the exploitation of a brand new modality, we employ cross-modal reasoning modules for explicitly modeling inter-modal relationships and aggregating relevant information across different modalities, and we propose a question-guided self-adaptive multi-modal fusion module to collect the question-oriented and event-correlated evidence through multi-step reasoning. We evaluate our model on two widely-used benchmark datasets and conduct an ablation study to justify the effectiveness of each proposed component.","sentences":["Video Question Answering (VideoQA) is a very attractive and challenging research direction aiming to understand complex semantics of heterogeneous data from two domains, i.e., the spatio-temporal video content and the word sequence in question.","Although various attention mechanisms have been utilized to manage contextualized representations by modeling intra- and inter-modal relationships of the two modalities, one limitation of the predominant VideoQA methods is the lack of reasoning with event correlation, that is, sensing and analyzing relationships among abundant and informative events contained in the video.","In this paper, we introduce the dense caption modality as a new auxiliary and distill event-correlated information from it to infer the correct answer.","To this end, we propose a novel end-to-end trainable model, Event-Correlated Graph Neural Networks (EC-GNNs), to perform cross-modal reasoning over information from the three modalities (i.e., caption, video, and question).","Besides the exploitation of a brand new modality, we employ cross-modal reasoning modules for explicitly modeling inter-modal relationships and aggregating relevant information across different modalities, and we propose a question-guided self-adaptive multi-modal fusion module to collect the question-oriented and event-correlated evidence through multi-step reasoning.","We evaluate our model on two widely-used benchmark datasets and conduct an ablation study to justify the effectiveness of each proposed component."],"url":"http://arxiv.org/abs/2312.12721v1"}
{"created":"2023-12-20 02:29:31","title":"AdvST: Revisiting Data Augmentations for Single Domain Generalization","abstract":"Single domain generalization (SDG) aims to train a robust model against unknown target domain shifts using data from a single source domain. Data augmentation has been proven an effective approach to SDG. However, the utility of standard augmentations, such as translate, or invert, has not been fully exploited in SDG; practically, these augmentations are used as a part of a data preprocessing procedure. Although it is intuitive to use many such augmentations to boost the robustness of a model to out-of-distribution domain shifts, we lack a principled approach to harvest the benefit brought from multiple these augmentations. Here, we conceptualize standard data augmentations with learnable parameters as semantics transformations that can manipulate certain semantics of a sample, such as the geometry or color of an image. Then, we propose Adversarial learning with Semantics Transformations (AdvST) that augments the source domain data with semantics transformations and learns a robust model with the augmented data. We theoretically show that AdvST essentially optimizes a distributionally robust optimization objective defined on a set of semantics distributions induced by the parameters of semantics transformations. We demonstrate that AdvST can produce samples that expand the coverage on target domain data. Compared with the state-of-the-art methods, AdvST, despite being a simple method, is surprisingly competitive and achieves the best average SDG performance on the Digits, PACS, and DomainNet datasets. Our code is available at https://github.com/gtzheng/AdvST.","sentences":["Single domain generalization (SDG) aims to train a robust model against unknown target domain shifts using data from a single source domain.","Data augmentation has been proven an effective approach to SDG.","However, the utility of standard augmentations, such as translate, or invert, has not been fully exploited in SDG; practically, these augmentations are used as a part of a data preprocessing procedure.","Although it is intuitive to use many such augmentations to boost the robustness of a model to out-of-distribution domain shifts, we lack a principled approach to harvest the benefit brought from multiple these augmentations.","Here, we conceptualize standard data augmentations with learnable parameters as semantics transformations that can manipulate certain semantics of a sample, such as the geometry or color of an image.","Then, we propose Adversarial learning with Semantics Transformations (AdvST) that augments the source domain data with semantics transformations and learns a robust model with the augmented data.","We theoretically show that AdvST essentially optimizes a distributionally robust optimization objective defined on a set of semantics distributions induced by the parameters of semantics transformations.","We demonstrate that AdvST can produce samples that expand the coverage on target domain data.","Compared with the state-of-the-art methods, AdvST, despite being a simple method, is surprisingly competitive and achieves the best average SDG performance on the Digits, PACS, and DomainNet datasets.","Our code is available at https://github.com/gtzheng/AdvST."],"url":"http://arxiv.org/abs/2312.12720v1"}
{"created":"2023-12-20 02:22:54","title":"DoDo-Code: a Deep Levenshtein Distance Embedding-based Code for IDS Channel and DNA Storage","abstract":"Recently, DNA storage has emerged as a promising data storage solution, offering significant advantages in storage density, maintenance cost efficiency, and parallel replication capability. Mathematically, the DNA storage pipeline can be viewed as an insertion, deletion, and substitution (IDS) channel. Because of the mathematical terra incognita of the Levenshtein distance, designing an IDS-correcting code is still a challenge. In this paper, we propose an innovative approach that utilizes deep Levenshtein distance embedding to bypass these mathematical challenges. By representing the Levenshtein distance between two sequences as a conventional distance between their corresponding embedding vectors, the inherent structural property of Levenshtein distance is revealed in the friendly embedding space. Leveraging this embedding space, we introduce the DoDo-Code, an IDS-correcting code that incorporates deep embedding of Levenshtein distance, deep embedding-based codeword search, and deep embedding-based segment correcting. To address the requirements of DNA storage, we also present a preliminary algorithm for long sequence decoding. As far as we know, the DoDo-Code is the first IDS-correcting code designed using plausible deep learning methodologies, potentially paving the way for a new direction in error-correcting code research. It is also the first IDS code that exhibits characteristics of being `optimal' in terms of redundancy, significantly outperforming the mainstream IDS-correcting codes of the Varshamov-Tenengolts code family in code rate.","sentences":["Recently, DNA storage has emerged as a promising data storage solution, offering significant advantages in storage density, maintenance cost efficiency, and parallel replication capability.","Mathematically, the DNA storage pipeline can be viewed as an insertion, deletion, and substitution (IDS) channel.","Because of the mathematical terra incognita of the Levenshtein distance, designing an IDS-correcting code is still a challenge.","In this paper, we propose an innovative approach that utilizes deep Levenshtein distance embedding to bypass these mathematical challenges.","By representing the Levenshtein distance between two sequences as a conventional distance between their corresponding embedding vectors, the inherent structural property of Levenshtein distance is revealed in the friendly embedding space.","Leveraging this embedding space, we introduce the DoDo-Code, an IDS-correcting code that incorporates deep embedding of Levenshtein distance, deep embedding-based codeword search, and deep embedding-based segment correcting.","To address the requirements of DNA storage, we also present a preliminary algorithm for long sequence decoding.","As far as we know, the DoDo-Code is the first IDS-correcting code designed using plausible deep learning methodologies, potentially paving the way for a new direction in error-correcting code research.","It is also the first IDS code that exhibits characteristics of being `optimal' in terms of redundancy, significantly outperforming the mainstream IDS-correcting codes of the Varshamov-Tenengolts code family in code rate."],"url":"http://arxiv.org/abs/2312.12717v1"}
{"created":"2023-12-20 02:22:49","title":"BloomVQA: Assessing Hierarchical Multi-modal Comprehension","abstract":"We propose a novel VQA dataset, based on picture stories designed for educating young children, that aims to facilitate comprehensive evaluation and characterization of vision-language models on comprehension tasks. Unlike current VQA datasets that often focus on fact-based memorization and simple reasoning tasks without principled scientific grounding, we collect data containing tasks reflecting different levels of comprehension and underlying cognitive processes, as laid out in Bloom's Taxonomy, a classic framework widely adopted in education research. The proposed BloomVQA dataset can be mapped to a hierarchical graph-based representation of visual stories, enabling automatic data augmentation and novel measures characterizing model consistency across the underlying taxonomy. We demonstrate graded evaluation and reliability analysis based on our proposed consistency metrics on state-of-the-art vision-language models. Our results suggest that, while current models achieve the most gain on low-level comprehension tasks, they generally fall short on high-level tasks requiring more advanced comprehension and cognitive skills, as 38.0% drop in VQA accuracy is observed comparing lowest and highest level tasks. Furthermore, current models show consistency patterns misaligned with human comprehension in various scenarios, suggesting emergent structures of model behaviors.","sentences":["We propose a novel VQA dataset, based on picture stories designed for educating young children, that aims to facilitate comprehensive evaluation and characterization of vision-language models on comprehension tasks.","Unlike current VQA datasets that often focus on fact-based memorization and simple reasoning tasks without principled scientific grounding, we collect data containing tasks reflecting different levels of comprehension and underlying cognitive processes, as laid out in Bloom's Taxonomy, a classic framework widely adopted in education research.","The proposed BloomVQA dataset can be mapped to a hierarchical graph-based representation of visual stories, enabling automatic data augmentation and novel measures characterizing model consistency across the underlying taxonomy.","We demonstrate graded evaluation and reliability analysis based on our proposed consistency metrics on state-of-the-art vision-language models.","Our results suggest that, while current models achieve the most gain on low-level comprehension tasks, they generally fall short on high-level tasks requiring more advanced comprehension and cognitive skills, as 38.0% drop in VQA accuracy is observed comparing lowest and highest level tasks.","Furthermore, current models show consistency patterns misaligned with human comprehension in various scenarios, suggesting emergent structures of model behaviors."],"url":"http://arxiv.org/abs/2312.12716v1"}
{"created":"2023-12-20 02:19:54","title":"Response Enhanced Semi-Supervised Dialogue Query Generation","abstract":"Leveraging vast and continually updated knowledge from the Internet has been considered an important ability for a dialogue system. Therefore, the dialogue query generation task is proposed for generating search queries from dialogue histories, which will be submitted to a search engine for retrieving relevant websites on the Internet. In this regard, previous efforts were devoted to collecting conversations with annotated queries and training a query producer (QP) via standard supervised learning. However, these studies still face the challenges of data scarcity and domain adaptation. To address these issues, in this paper, we propose a semi-supervised learning framework -- SemiDQG, to improve model performance with unlabeled conversations. Based on the observation that the search query is typically related to the topic of dialogue response, we train a response-augmented query producer (RA) to provide rich and effective training signals for QP. We first apply a similarity-based query selection strategy to select high-quality RA-generated pseudo queries, which are used to construct pseudo instances for training QP and RA. Then, we adopt the REINFORCE algorithm to further enhance QP, with RA-provided rewards as fine-grained training signals. Experimental results and in-depth analysis of three benchmarks show the effectiveness of our framework in cross-domain and low-resource scenarios. Particularly, SemiDQG significantly surpasses ChatGPT and competitive baselines. Our code is available at \\url{https://github.com/DeepLearnXMU/SemiDQG}.","sentences":["Leveraging vast and continually updated knowledge from the Internet has been considered an important ability for a dialogue system.","Therefore, the dialogue query generation task is proposed for generating search queries from dialogue histories, which will be submitted to a search engine for retrieving relevant websites on the Internet.","In this regard, previous efforts were devoted to collecting conversations with annotated queries and training a query producer (QP) via standard supervised learning.","However, these studies still face the challenges of data scarcity and domain adaptation.","To address these issues, in this paper, we propose a semi-supervised learning framework -- SemiDQG, to improve model performance with unlabeled conversations.","Based on the observation that the search query is typically related to the topic of dialogue response, we train a response-augmented query producer (RA) to provide rich and effective training signals for QP.","We first apply a similarity-based query selection strategy to select high-quality RA-generated pseudo queries, which are used to construct pseudo instances for training QP and RA.","Then, we adopt the REINFORCE algorithm to further enhance QP, with RA-provided rewards as fine-grained training signals.","Experimental results and in-depth analysis of three benchmarks show the effectiveness of our framework in cross-domain and low-resource scenarios.","Particularly, SemiDQG significantly surpasses ChatGPT and competitive baselines.","Our code is available at \\url{https://github.com/DeepLearnXMU/SemiDQG}."],"url":"http://arxiv.org/abs/2312.12713v1"}
{"created":"2023-12-20 02:03:15","title":"Optimizing Distributed Training on Frontier for Large Language Models","abstract":"Large language models (LLM) are showing tremendous success as foundation models, and many downstream applications benefit from fine-tuning. Prior works on loss scaling have demonstrated that the larger LLMs perform better than their smaller counterparts. However, training LLMs with billions of parameters requires considerable computational resources; to train a one trillion GPT-style model on 20 trillion tokens, we need to perform 120 million exaflops. Frontier is the world's first and fastest exascale supercomputer for open science and is equipped with 75264 MI250X GPUs. This work explores efficient distributed strategies such as tensor parallelism, pipeline parallelism, and sharded data parallelism to train a trillion-parameter model on the Frontier exascale supercomputer. We analyze these distributed training techniques and associated parameters individually to decide which techniques to use and what associated parameters to select for a particular technique. We perform hyperparameter tuning on these techniques to understand their complex interplay. Combined with these two tuning efforts, we have found optimal strategies to train three models of size 22B, 175B, and 1T parameters with $38.38\\%$ , $36.14\\%$ , and $31.96\\%$ achieved throughput. For training the 175B parameter model and 1T model, we have achieved $100\\%$ weak scaling efficiency and $89\\%$ and $87\\%$ strong scaling efficiency, respectively. Our work presents a set of strategies for distributed training of LLMs through experimental findings and hyperparameter tuning.","sentences":["Large language models (LLM) are showing tremendous success as foundation models, and many downstream applications benefit from fine-tuning.","Prior works on loss scaling have demonstrated that the larger LLMs perform better than their smaller counterparts.","However, training LLMs with billions of parameters requires considerable computational resources; to train a one trillion GPT-style model on 20 trillion tokens, we need to perform 120 million exaflops.","Frontier is the world's first and fastest exascale supercomputer for open science and is equipped with 75264 MI250X GPUs.","This work explores efficient distributed strategies such as tensor parallelism, pipeline parallelism, and sharded data parallelism to train a trillion-parameter model on the Frontier exascale supercomputer.","We analyze these distributed training techniques and associated parameters individually to decide which techniques to use and what associated parameters to select for a particular technique.","We perform hyperparameter tuning on these techniques to understand their complex interplay.","Combined with these two tuning efforts, we have found optimal strategies to train three models of size 22B, 175B, and 1T parameters with $38.38\\%$ , $36.14\\%$ , and $31.96\\%$ achieved throughput.","For training the 175B parameter model and 1T model, we have achieved $100\\%$ weak scaling efficiency and $89\\%$ and $87\\%$ strong scaling efficiency, respectively.","Our work presents a set of strategies for distributed training of LLMs through experimental findings and hyperparameter tuning."],"url":"http://arxiv.org/abs/2312.12705v1"}
{"created":"2023-12-20 01:59:48","title":"Federated Learning with Extremely Noisy Clients via Negative Distillation","abstract":"Federated learning (FL) has shown remarkable success in cooperatively training deep models, while typically struggling with noisy labels. Advanced works propose to tackle label noise by a re-weighting strategy with a strong assumption, i.e., mild label noise. However, it may be violated in many real-world FL scenarios because of highly contaminated clients, resulting in extreme noise ratios, e.g., $>$90%. To tackle extremely noisy clients, we study the robustness of the re-weighting strategy, showing a pessimistic conclusion: minimizing the weight of clients trained over noisy data outperforms re-weighting strategies. To leverage models trained on noisy clients, we propose a novel approach, called negative distillation (FedNed). FedNed first identifies noisy clients and employs rather than discards the noisy clients in a knowledge distillation manner. In particular, clients identified as noisy ones are required to train models using noisy labels and pseudo-labels obtained by global models. The model trained on noisy labels serves as a `bad teacher' in knowledge distillation, aiming to decrease the risk of providing incorrect information. Meanwhile, the model trained on pseudo-labels is involved in model aggregation if not identified as a noisy client. Consequently, through pseudo-labeling, FedNed gradually increases the trustworthiness of models trained on noisy clients, while leveraging all clients for model aggregation through negative distillation. To verify the efficacy of FedNed, we conduct extensive experiments under various settings, demonstrating that FedNed can consistently outperform baselines and achieve state-of-the-art performance. Our code is available at https://github.com/linChen99/FedNed.","sentences":["Federated learning (FL) has shown remarkable success in cooperatively training deep models, while typically struggling with noisy labels.","Advanced works propose to tackle label noise by a re-weighting strategy with a strong assumption, i.e., mild label noise.","However, it may be violated in many real-world FL scenarios because of highly contaminated clients, resulting in extreme noise ratios, e.g., $>$90%.","To tackle extremely noisy clients, we study the robustness of the re-weighting strategy, showing a pessimistic conclusion: minimizing the weight of clients trained over noisy data outperforms re-weighting strategies.","To leverage models trained on noisy clients, we propose a novel approach, called negative distillation (FedNed).","FedNed first identifies noisy clients and employs rather than discards the noisy clients in a knowledge distillation manner.","In particular, clients identified as noisy ones are required to train models using noisy labels and pseudo-labels obtained by global models.","The model trained on noisy labels serves as a `bad teacher' in knowledge distillation, aiming to decrease the risk of providing incorrect information.","Meanwhile, the model trained on pseudo-labels is involved in model aggregation if not identified as a noisy client.","Consequently, through pseudo-labeling, FedNed gradually increases the trustworthiness of models trained on noisy clients, while leveraging all clients for model aggregation through negative distillation.","To verify the efficacy of FedNed, we conduct extensive experiments under various settings, demonstrating that FedNed can consistently outperform baselines and achieve state-of-the-art performance.","Our code is available at https://github.com/linChen99/FedNed."],"url":"http://arxiv.org/abs/2312.12703v1"}
{"created":"2023-12-20 01:20:24","title":"CodeLL: A Lifelong Learning Dataset to Support the Co-Evolution of Data and Language Models of Code","abstract":"Motivated by recent work on lifelong learning applications for language models (LMs) of code, we introduce CodeLL, a lifelong learning dataset focused on code changes. Our contribution addresses a notable research gap marked by the absence of a long-term temporal dimension in existing code change datasets, limiting their suitability in lifelong learning scenarios. In contrast, our dataset aims to comprehensively capture code changes across the entire release history of open-source software repositories. In this work, we introduce an initial version of CodeLL, comprising 71 machine-learning-based projects mined from Software Heritage. This dataset enables the extraction and in-depth analysis of code changes spanning 2,483 releases at both the method and API levels. CodeLL enables researchers studying the behaviour of LMs in lifelong fine-tuning settings for learning code changes. Additionally, the dataset can help studying data distribution shifts within software repositories and the evolution of API usages over time.","sentences":["Motivated by recent work on lifelong learning applications for language models (LMs) of code, we introduce CodeLL, a lifelong learning dataset focused on code changes.","Our contribution addresses a notable research gap marked by the absence of a long-term temporal dimension in existing code change datasets, limiting their suitability in lifelong learning scenarios.","In contrast, our dataset aims to comprehensively capture code changes across the entire release history of open-source software repositories.","In this work, we introduce an initial version of CodeLL, comprising 71 machine-learning-based projects mined from Software Heritage.","This dataset enables the extraction and in-depth analysis of code changes spanning 2,483 releases at both the method and API levels.","CodeLL enables researchers studying the behaviour of LMs in lifelong fine-tuning settings for learning code changes.","Additionally, the dataset can help studying data distribution shifts within software repositories and the evolution of API usages over time."],"url":"http://arxiv.org/abs/2312.12492v1"}
{"created":"2023-12-20 00:49:52","title":"Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?","abstract":"The vast majority of today's large language models are English-centric, having been pretrained predominantly on English text. Yet, in order to meet user expectations, models need to be able to respond appropriately in multiple languages once deployed in downstream applications. Given limited exposure to other languages during pretraining, cross-lingual transfer is important for achieving decent performance in non-English settings. In this work, we investigate just how much multilinguality is required during finetuning to elicit strong cross-lingual generalisation across a range of tasks and target languages. We find that, compared to English-only finetuning, multilingual instruction tuning with as few as three languages significantly improves a model's cross-lingual transfer abilities on generative tasks that assume input/output language agreement, while being of less importance for highly structured tasks. Our code and data is available at https://github.com/ZurichNLP/multilingual-instruction-tuning.","sentences":["The vast majority of today's large language models are English-centric, having been pretrained predominantly on English text.","Yet, in order to meet user expectations, models need to be able to respond appropriately in multiple languages once deployed in downstream applications.","Given limited exposure to other languages during pretraining, cross-lingual transfer is important for achieving decent performance in non-English settings.","In this work, we investigate just how much multilinguality is required during finetuning to elicit strong cross-lingual generalisation across a range of tasks and target languages.","We find that, compared to English-only finetuning, multilingual instruction tuning with as few as three languages significantly improves a model's cross-lingual transfer abilities on generative tasks that assume input/output language agreement, while being of less importance for highly structured tasks.","Our code and data is available at https://github.com/ZurichNLP/multilingual-instruction-tuning."],"url":"http://arxiv.org/abs/2312.12683v1"}
{"created":"2023-12-20 00:45:27","title":"Imitation of Life: A Search Engine for Biologically Inspired Design","abstract":"Biologically Inspired Design (BID), or Biomimicry, is a problem-solving methodology that applies analogies from nature to solve engineering challenges. For example, Speedo engineers designed swimsuits based on shark skin. Finding relevant biological solutions for real-world problems poses significant challenges, both due to the limited biological knowledge engineers and designers typically possess and to the limited BID resources. Existing BID datasets are hand-curated and small, and scaling them up requires costly human annotations.   In this paper, we introduce BARcode (Biological Analogy Retriever), a search engine for automatically mining bio-inspirations from the web at scale. Using advances in natural language understanding and data programming, BARcode identifies potential inspirations for engineering challenges. Our experiments demonstrate that BARcode can retrieve inspirations that are valuable to engineers and designers tackling real-world problems, as well as recover famous historical BID examples. We release data and code; we view BARcode as a step towards addressing the challenges that have historically hindered the practical application of BID to engineering innovation.","sentences":["Biologically Inspired Design (BID), or Biomimicry, is a problem-solving methodology that applies analogies from nature to solve engineering challenges.","For example, Speedo engineers designed swimsuits based on shark skin.","Finding relevant biological solutions for real-world problems poses significant challenges, both due to the limited biological knowledge engineers and designers typically possess and to the limited BID resources.","Existing BID datasets are hand-curated and small, and scaling them up requires costly human annotations.   ","In this paper, we introduce BARcode (Biological Analogy Retriever), a search engine for automatically mining bio-inspirations from the web at scale.","Using advances in natural language understanding and data programming, BARcode identifies potential inspirations for engineering challenges.","Our experiments demonstrate that BARcode can retrieve inspirations that are valuable to engineers and designers tackling real-world problems, as well as recover famous historical BID examples.","We release data and code; we view BARcode as a step towards addressing the challenges that have historically hindered the practical application of BID to engineering innovation."],"url":"http://arxiv.org/abs/2312.12681v1"}
{"created":"2023-12-20 00:27:10","title":"Comparison of Waymo Rider-Only Crash Data to Human Benchmarks at 7.1 Million Miles","abstract":"This paper examines the safety performance of the Waymo Driver, an SAE level 4 automated driving system (ADS) used in a rider-only (RO) ride-hailing application without a human driver, either in the vehicle or remotely. ADS crash data was derived from NHTSA's Standing General Order (SGO) reporting over 7.14 million RO miles through the end of October 2023 in Phoenix, AZ, San Francisco, CA, and Los Angeles, CA. This study is one of the first to compare overall crashed vehicle rates using only RO data (as opposed to ADS testing with a human behind the wheel) to a human benchmark that also corrects for biases caused by underreporting and unequal reporting thresholds reported in the literature. When considering all locations together, the any-injury-reported crashed vehicle rate was 0.41 incidents per million miles (IPMM) for the ADS vs 2.78 IPMM for the human benchmark, an 85% reduction or a 6.8 times lower rate. Police-reported crashed vehicle rates for all locations together were 2.1 IPMM for the ADS vs. 4.85 IPMM for the human benchmark, a 57% reduction or 2.3 times lower rate. Police-reported and any-injury-reported crashed vehicle rate reductions for the ADS were statistically significant when compared in San Francisco and Phoenix as well as combined across all locations. The comparison in Los Angeles, which to date has low mileage and no reported events, was not statistically significant. In general, the Waymo ADS had a lower any property damage or injury rate than the human benchmarks. Given imprecision in the benchmark estimate and multiple potential sources of underreporting biasing the benchmarks, caution should be taken when interpreting the results of the any property damage or injury comparison. Together, these crash-rate results should be interpreted as a directional and continuous confidence growth indicator, together with other methodologies, in a safety case approach.","sentences":["This paper examines the safety performance of the Waymo Driver, an SAE level 4 automated driving system (ADS) used in a rider-only (RO) ride-hailing application without a human driver, either in the vehicle or remotely.","ADS crash data was derived from NHTSA's Standing General Order (SGO) reporting over 7.14 million RO miles through the end of October 2023 in Phoenix, AZ, San Francisco, CA, and Los Angeles, CA.","This study is one of the first to compare overall crashed vehicle rates using only RO data (as opposed to ADS testing with a human behind the wheel) to a human benchmark that also corrects for biases caused by underreporting and unequal reporting thresholds reported in the literature.","When considering all locations together, the any-injury-reported crashed vehicle rate was 0.41 incidents per million miles (IPMM) for the ADS vs 2.78 IPMM for the human benchmark, an 85% reduction or a 6.8 times lower rate.","Police-reported crashed vehicle rates for all locations together were 2.1 IPMM for the ADS vs. 4.85 IPMM for the human benchmark, a 57% reduction or 2.3 times lower rate.","Police-reported and any-injury-reported crashed vehicle rate reductions for the ADS were statistically significant when compared in San Francisco and Phoenix as well as combined across all locations.","The comparison in Los Angeles, which to date has low mileage and no reported events, was not statistically significant.","In general, the Waymo ADS had a lower any property damage or injury rate than the human benchmarks.","Given imprecision in the benchmark estimate and multiple potential sources of underreporting biasing the benchmarks, caution should be taken when interpreting the results of the any property damage or injury comparison.","Together, these crash-rate results should be interpreted as a directional and continuous confidence growth indicator, together with other methodologies, in a safety case approach."],"url":"http://arxiv.org/abs/2312.12675v1"}
{"created":"2023-12-20 00:07:43","title":"Categorical, Ratio, and Professorial Data: The Case for Reciprocal Rank","abstract":"Search engine results pages are usually abstracted as binary relevance vectors and hence are categorical data, meaning that only a limited set of operations is permitted, most notably tabulation of occurrence frequencies, with determination of medians and averages not possible. To compare retrieval systems it is thus usual to make use of a categorical-to-numeric effectiveness mapping. A previous paper has argued that any desired categorical-to-numeric mapping may be used, provided only that there is an argued connection between each category of SERP and the score that is assigned to that category by the mapping. Further, once that plausible connection has been established, then the mapped values can be treated as real-valued observations on a ratio scale, allowing the computation of averages. This article is written in support of that point of view, and to respond to ongoing claims that SERP scores may only be averaged if very restrictive conditions are imposed on the effectiveness mapping.","sentences":["Search engine results pages are usually abstracted as binary relevance vectors and hence are categorical data, meaning that only a limited set of operations is permitted, most notably tabulation of occurrence frequencies, with determination of medians and averages not possible.","To compare retrieval systems it is thus usual to make use of a categorical-to-numeric effectiveness mapping.","A previous paper has argued that any desired categorical-to-numeric mapping may be used, provided only that there is an argued connection between each category of SERP and the score that is assigned to that category by the mapping.","Further, once that plausible connection has been established, then the mapped values can be treated as real-valued observations on a ratio scale, allowing the computation of averages.","This article is written in support of that point of view, and to respond to ongoing claims that SERP scores may only be averaged if very restrictive conditions are imposed on the effectiveness mapping."],"url":"http://arxiv.org/abs/2312.12672v1"}
{"created":"2023-12-19 23:56:49","title":"On the Role of Server Momentum in Federated Learning","abstract":"Federated Averaging (FedAvg) is known to experience convergence issues when encountering significant clients system heterogeneity and data heterogeneity. Server momentum has been proposed as an effective mitigation. However, existing server momentum works are restrictive in the momentum formulation, do not properly schedule hyperparameters and focus only on system homogeneous settings, which leaves the role of server momentum still an under-explored problem. In this paper, we propose a general framework for server momentum, that (a) covers a large class of momentum schemes that are unexplored in federated learning (FL), (b) enables a popular stagewise hyperparameter scheduler, (c) allows heterogeneous and asynchronous local computing. We provide rigorous convergence analysis for the proposed framework. To our best knowledge, this is the first work that thoroughly analyzes the performances of server momentum with a hyperparameter scheduler and system heterogeneity. Extensive experiments validate the effectiveness of our proposed framework.","sentences":["Federated Averaging (FedAvg) is known to experience convergence issues when encountering significant clients system heterogeneity and data heterogeneity.","Server momentum has been proposed as an effective mitigation.","However, existing server momentum works are restrictive in the momentum formulation, do not properly schedule hyperparameters and focus only on system homogeneous settings, which leaves the role of server momentum still an under-explored problem.","In this paper, we propose a general framework for server momentum, that (a) covers a large class of momentum schemes that are unexplored in federated learning (FL), (b) enables a popular stagewise hyperparameter scheduler, (c) allows heterogeneous and asynchronous local computing.","We provide rigorous convergence analysis for the proposed framework.","To our best knowledge, this is the first work that thoroughly analyzes the performances of server momentum with a hyperparameter scheduler and system heterogeneity.","Extensive experiments validate the effectiveness of our proposed framework."],"url":"http://arxiv.org/abs/2312.12670v1"}
{"created":"2023-12-19 23:48:43","title":"Convolutional Channel-wise Competitive Learning for the Forward-Forward Algorithm","abstract":"The Forward-Forward (FF) Algorithm has been recently proposed to alleviate the issues of backpropagation (BP) commonly used to train deep neural networks. However, its current formulation exhibits limitations such as the generation of negative data, slower convergence, and inadequate performance on complex tasks. In this paper, we take the main ideas of FF and improve them by leveraging channel-wise competitive learning in the context of convolutional neural networks for image classification tasks. A layer-wise loss function is introduced that promotes competitive learning and eliminates the need for negative data construction. To enhance both the learning of compositional features and feature space partitioning, a channel-wise feature separator and extractor block is proposed that complements the competitive learning process. Our method outperforms recent FF-based models on image classification tasks, achieving testing errors of 0.58%, 7.69%, 21.89%, and 48.77% on MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 respectively. Our approach bridges the performance gap between FF learning and BP methods, indicating the potential of our proposed approach to learn useful representations in a layer-wise modular fashion, enabling more efficient and flexible learning.","sentences":["The Forward-Forward (FF) Algorithm has been recently proposed to alleviate the issues of backpropagation (BP) commonly used to train deep neural networks.","However, its current formulation exhibits limitations such as the generation of negative data, slower convergence, and inadequate performance on complex tasks.","In this paper, we take the main ideas of FF and improve them by leveraging channel-wise competitive learning in the context of convolutional neural networks for image classification tasks.","A layer-wise loss function is introduced that promotes competitive learning and eliminates the need for negative data construction.","To enhance both the learning of compositional features and feature space partitioning, a channel-wise feature separator and extractor block is proposed that complements the competitive learning process.","Our method outperforms recent FF-based models on image classification tasks, achieving testing errors of 0.58%, 7.69%, 21.89%, and 48.77% on MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 respectively.","Our approach bridges the performance gap between FF learning and BP methods, indicating the potential of our proposed approach to learn useful representations in a layer-wise modular fashion, enabling more efficient and flexible learning."],"url":"http://arxiv.org/abs/2312.12668v1"}
{"created":"2023-12-19 23:39:33","title":"Incremental Semi-supervised Federated Learning for Health Inference via Mobile Sensing","abstract":"Mobile sensing appears as a promising solution for health inference problem (e.g., influenza-like symptom recognition) by leveraging diverse smart sensors to capture fine-grained information about human behaviors and ambient contexts. Centralized training of machine learning models can place mobile users' sensitive information under privacy risks due to data breach and misexploitation. Federated Learning (FL) enables mobile devices to collaboratively learn global models without the exposure of local private data. However, there are challenges of on-device FL deployment using mobile sensing: 1) long-term and continuously collected mobile sensing data may exhibit domain shifts as sensing objects (e.g. humans) have varying behaviors as a result of internal and/or external stimulus; 2) model retraining using all available data may increase computation and memory burden; and 3) the sparsity of annotated crowd-sourced data causes supervised FL to lack robustness. In this work, we propose FedMobile, an incremental semi-supervised federated learning algorithm, to train models semi-supervisedly and incrementally in a decentralized online fashion. We evaluate FedMobile using a real-world mobile sensing dataset for influenza-like symptom recognition. Our empirical results show that FedMobile-trained models achieve the best results in comparison to the selected baseline methods.","sentences":["Mobile sensing appears as a promising solution for health inference problem (e.g., influenza-like symptom recognition) by leveraging diverse smart sensors to capture fine-grained information about human behaviors and ambient contexts.","Centralized training of machine learning models can place mobile users' sensitive information under privacy risks due to data breach and misexploitation.","Federated Learning (FL) enables mobile devices to collaboratively learn global models without the exposure of local private data.","However, there are challenges of on-device FL deployment using mobile sensing: 1) long-term and continuously collected mobile sensing data may exhibit domain shifts as sensing objects (e.g. humans) have varying behaviors as a result of internal and/or external stimulus; 2) model retraining using all available data may increase computation and memory burden; and 3) the sparsity of annotated crowd-sourced data causes supervised FL to lack robustness.","In this work, we propose FedMobile, an incremental semi-supervised federated learning algorithm, to train models semi-supervisedly and incrementally in a decentralized online fashion.","We evaluate FedMobile using a real-world mobile sensing dataset for influenza-like symptom recognition.","Our empirical results show that FedMobile-trained models achieve the best results in comparison to the selected baseline methods."],"url":"http://arxiv.org/abs/2312.12666v1"}
{"created":"2023-12-19 23:22:47","title":"Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pretraining","abstract":"Contrastive Language-Image Pretraining has emerged as a prominent approach for training vision and text encoders with uncurated image-text pairs from the web. To enhance data-efficiency, recent efforts have introduced additional supervision terms that involve random-augmented views of the image. However, since the image augmentation process is unaware of its text counterpart, this procedure could cause various degrees of image-text misalignments during training. Prior methods either disregarded this discrepancy or introduced external models to mitigate the impact of misalignments during training. In contrast, we propose a novel metric learning approach that capitalizes on these misalignments as an additional training source, which we term \"Misalign, Contrast then Distill (MCD)\". Unlike previous methods that treat augmented images and their text counterparts as simple positive pairs, MCD predicts the continuous scales of misalignment caused by the augmentation. Our extensive experimental results show that our proposed MCD achieves state-of-the-art transferability in multiple classification and retrieval downstream datasets.","sentences":["Contrastive Language-Image Pretraining has emerged as a prominent approach for training vision and text encoders with uncurated image-text pairs from the web.","To enhance data-efficiency, recent efforts have introduced additional supervision terms that involve random-augmented views of the image.","However, since the image augmentation process is unaware of its text counterpart, this procedure could cause various degrees of image-text misalignments during training.","Prior methods either disregarded this discrepancy or introduced external models to mitigate the impact of misalignments during training.","In contrast, we propose a novel metric learning approach that capitalizes on these misalignments as an additional training source, which we term \"Misalign, Contrast then Distill (MCD)\".","Unlike previous methods that treat augmented images and their text counterparts as simple positive pairs, MCD predicts the continuous scales of misalignment caused by the augmentation.","Our extensive experimental results show that our proposed MCD achieves state-of-the-art transferability in multiple classification and retrieval downstream datasets."],"url":"http://arxiv.org/abs/2312.12661v1"}
{"created":"2023-12-19 23:21:19","title":"Is post-editing really faster than human translation?","abstract":"Time efficiency is paramount for the localisation industry, which demands ever-faster turnaround times. However, translation speed is largely underresearched, and there is a lack of clarity about how language service providers (LSPs) can evaluate the performance of their post-editing (PE) and human translation (HT) services. This study constitutes the first large-scale investigation of translation and revision speed in HT and in the PE of neural machine translation, based on real-world data from an LSP. It uses an exploratory data analysis approach to investigate data for 90 million words translated by 879 linguists across 11 language pairs, over 2.5 years. The results of this research indicate that (a) PE is usually but not always faster than HT; (b) average speed values may be misleading; (c) translation speed is highly variable; and (d) edit distance cannot be used as a proxy for post-editing productivity, because it does not correlate strongly with speed.","sentences":["Time efficiency is paramount for the localisation industry, which demands ever-faster turnaround times.","However, translation speed is largely underresearched, and there is a lack of clarity about how language service providers (LSPs) can evaluate the performance of their post-editing (PE) and human translation (HT) services.","This study constitutes the first large-scale investigation of translation and revision speed in HT and in the PE of neural machine translation, based on real-world data from an LSP.","It uses an exploratory data analysis approach to investigate data for 90 million words translated by 879 linguists across 11 language pairs, over 2.5 years.","The results of this research indicate that (a) PE is usually but not always faster than HT; (b) average speed values may be misleading; (c) translation speed is highly variable; and (d) edit distance cannot be used as a proxy for post-editing productivity, because it does not correlate strongly with speed."],"url":"http://arxiv.org/abs/2312.12660v1"}
{"created":"2023-12-19 23:11:06","title":"Expediting Contrastive Language-Image Pretraining via Self-distilled Encoders","abstract":"Recent advances in vision language pretraining (VLP) have been largely attributed to the large-scale data collected from the web. However, uncurated dataset contains weakly correlated image-text pairs, causing data inefficiency. To address the issue, knowledge distillation have been explored at the expense of extra image and text momentum encoders to generate teaching signals for misaligned image-text pairs. In this paper, our goal is to resolve the misalignment problem with an efficient distillation framework. To this end, we propose ECLIPSE: Expediting Contrastive Language-Image Pretraining with Self-distilled Encoders. ECLIPSE features a distinctive distillation architecture wherein a shared text encoder is utilized between an online image encoder and a momentum image encoder. This strategic design choice enables the distillation to operate within a unified projected space of text embedding, resulting in better performance. Based on the unified text embedding space, ECLIPSE compensates for the additional computational cost of the momentum image encoder by expediting the online image encoder. Through our extensive experiments, we validate that there is a sweet spot between expedition and distillation where the partial view from the expedited online image encoder interacts complementarily with the momentum teacher. As a result, ECLIPSE outperforms its counterparts while achieving substantial acceleration in inference speed.","sentences":["Recent advances in vision language pretraining (VLP) have been largely attributed to the large-scale data collected from the web.","However, uncurated dataset contains weakly correlated image-text pairs, causing data inefficiency.","To address the issue, knowledge distillation have been explored at the expense of extra image and text momentum encoders to generate teaching signals for misaligned image-text pairs.","In this paper, our goal is to resolve the misalignment problem with an efficient distillation framework.","To this end, we propose ECLIPSE: Expediting Contrastive Language-Image Pretraining with Self-distilled Encoders.","ECLIPSE features a distinctive distillation architecture wherein a shared text encoder is utilized between an online image encoder and a momentum image encoder.","This strategic design choice enables the distillation to operate within a unified projected space of text embedding, resulting in better performance.","Based on the unified text embedding space, ECLIPSE compensates for the additional computational cost of the momentum image encoder by expediting the online image encoder.","Through our extensive experiments, we validate that there is a sweet spot between expedition and distillation where the partial view from the expedited online image encoder interacts complementarily with the momentum teacher.","As a result, ECLIPSE outperforms its counterparts while achieving substantial acceleration in inference speed."],"url":"http://arxiv.org/abs/2312.12659v1"}
{"created":"2023-12-19 23:04:56","title":"The Convex Landscape of Neural Networks: Characterizing Global Optima and Stationary Points via Lasso Models","abstract":"Due to the non-convex nature of training Deep Neural Network (DNN) models, their effectiveness relies on the use of non-convex optimization heuristics. Traditional methods for training DNNs often require costly empirical methods to produce successful models and do not have a clear theoretical foundation. In this study, we examine the use of convex optimization theory and sparse recovery models to refine the training process of neural networks and provide a better interpretation of their optimal weights. We focus on training two-layer neural networks with piecewise linear activations and demonstrate that they can be formulated as a finite-dimensional convex program. These programs include a regularization term that promotes sparsity, which constitutes a variant of group Lasso. We first utilize semi-infinite programming theory to prove strong duality for finite width neural networks and then we express these architectures equivalently as high dimensional convex sparse recovery models. Remarkably, the worst-case complexity to solve the convex program is polynomial in the number of samples and number of neurons when the rank of the data matrix is bounded, which is the case in convolutional networks. To extend our method to training data of arbitrary rank, we develop a novel polynomial-time approximation scheme based on zonotope subsampling that comes with a guaranteed approximation ratio. We also show that all the stationary of the nonconvex training objective can be characterized as the global optimum of a subsampled convex program. Our convex models can be trained using standard convex solvers without resorting to heuristics or extensive hyper-parameter tuning unlike non-convex methods. Through extensive numerical experiments, we show that convex models can outperform traditional non-convex methods and are not sensitive to optimizer hyperparameters.","sentences":["Due to the non-convex nature of training Deep Neural Network (DNN) models, their effectiveness relies on the use of non-convex optimization heuristics.","Traditional methods for training DNNs often require costly empirical methods to produce successful models and do not have a clear theoretical foundation.","In this study, we examine the use of convex optimization theory and sparse recovery models to refine the training process of neural networks and provide a better interpretation of their optimal weights.","We focus on training two-layer neural networks with piecewise linear activations and demonstrate that they can be formulated as a finite-dimensional convex program.","These programs include a regularization term that promotes sparsity, which constitutes a variant of group Lasso.","We first utilize semi-infinite programming theory to prove strong duality for finite width neural networks","and then we express these architectures equivalently as high dimensional convex sparse recovery models.","Remarkably, the worst-case complexity to solve the convex program is polynomial in the number of samples and number of neurons when the rank of the data matrix is bounded, which is the case in convolutional networks.","To extend our method to training data of arbitrary rank, we develop a novel polynomial-time approximation scheme based on zonotope subsampling that comes with a guaranteed approximation ratio.","We also show that all the stationary of the nonconvex training objective can be characterized as the global optimum of a subsampled convex program.","Our convex models can be trained using standard convex solvers without resorting to heuristics or extensive hyper-parameter tuning unlike non-convex methods.","Through extensive numerical experiments, we show that convex models can outperform traditional non-convex methods and are not sensitive to optimizer hyperparameters."],"url":"http://arxiv.org/abs/2312.12657v1"}
{"created":"2023-12-19 22:57:13","title":"Can Transformers Learn Sequential Function Classes In Context?","abstract":"In-context learning (ICL) has revolutionized the capabilities of transformer models in NLP. In our project, we extend the understanding of the mechanisms underpinning ICL by exploring whether transformers can learn from sequential, non-textual function class data distributions. We introduce a novel sliding window sequential function class and employ toy-sized transformers with a GPT-2 architecture to conduct our experiments. Our analysis indicates that these models can indeed leverage ICL when trained on non-textual sequential function classes. Additionally, our experiments with randomized y-label sequences highlights that transformers retain some ICL capabilities even when the label associations are obfuscated. We provide evidence that transformers can reason with and understand sequentiality encoded within function classes, as reflected by the effective learning of our proposed tasks. Our results also show that the performance deteriorated with increasing randomness in the labels, though not to the extent one might expect, implying a potential robustness of learned sequentiality against label noise. Future research may want to look into how previous explanations of transformers, such as induction heads and task vectors, relate to sequentiality in ICL in these toy examples. Our investigation lays the groundwork for further research into how transformers process and perceive sequential data.","sentences":["In-context learning (ICL) has revolutionized the capabilities of transformer models in NLP.","In our project, we extend the understanding of the mechanisms underpinning ICL by exploring whether transformers can learn from sequential, non-textual function class data distributions.","We introduce a novel sliding window sequential function class and employ toy-sized transformers with a GPT-2 architecture to conduct our experiments.","Our analysis indicates that these models can indeed leverage ICL when trained on non-textual sequential function classes.","Additionally, our experiments with randomized y-label sequences highlights that transformers retain some ICL capabilities even when the label associations are obfuscated.","We provide evidence that transformers can reason with and understand sequentiality encoded within function classes, as reflected by the effective learning of our proposed tasks.","Our results also show that the performance deteriorated with increasing randomness in the labels, though not to the extent one might expect, implying a potential robustness of learned sequentiality against label noise.","Future research may want to look into how previous explanations of transformers, such as induction heads and task vectors, relate to sequentiality in ICL in these toy examples.","Our investigation lays the groundwork for further research into how transformers process and perceive sequential data."],"url":"http://arxiv.org/abs/2312.12655v1"}
{"created":"2023-12-19 22:53:59","title":"FairFlow Protocol: Equitable Maximal Extractable Value (MEV) mitigation in Ethereum","abstract":"Ethereum has emerged as a leading platform for decentralized applications (dApps) due to its robust smart contract capabilities. One of the critical issues in the Ethereum ecosystem is Maximal Extractable Value (MEV), a concept that has gained significant attention in the blockchain community. However, MEV has remained a major challenge with significant implications for the platform's operation and integrity. This paper introduces the FairFlow protocol, a novel framework designed to mitigate the effects of MEV within Ethereum's existing infrastructure. The protocol aims to provide a more equitable environment, preventing exploitation by miners or validators, and protecting user data. The combined approach of auction-based block space allocation and randomized transaction ordering significantly reduces the potential for MEV exploitation.","sentences":["Ethereum has emerged as a leading platform for decentralized applications (dApps) due to its robust smart contract capabilities.","One of the critical issues in the Ethereum ecosystem is Maximal Extractable Value (MEV), a concept that has gained significant attention in the blockchain community.","However, MEV has remained a major challenge with significant implications for the platform's operation and integrity.","This paper introduces the FairFlow protocol, a novel framework designed to mitigate the effects of MEV within Ethereum's existing infrastructure.","The protocol aims to provide a more equitable environment, preventing exploitation by miners or validators, and protecting user data.","The combined approach of auction-based block space allocation and randomized transaction ordering significantly reduces the potential for MEV exploitation."],"url":"http://arxiv.org/abs/2312.12654v1"}
{"created":"2023-12-19 22:52:51","title":"Toxic Bias: Perspective API Misreads German as More Toxic","abstract":"Proprietary public APIs play a crucial and growing role as research tools among social scientists. Among such APIs, Google's machine learning-based Perspective API is extensively utilized for assessing the toxicity of social media messages, providing both an important resource for researchers and automatic content moderation. However, this paper exposes an important bias in Perspective API concerning German language text. Through an in-depth examination of several datasets, we uncover intrinsic language biases within the multilingual model of Perspective API. We find that the toxicity assessment of German content produces significantly higher toxicity levels than other languages. This finding is robust across various translations, topics, and data sources, and has significant consequences for both research and moderation strategies that rely on Perspective API. For instance, we show that, on average, four times more tweets and users would be moderated when using the German language compared to their English translation. Our findings point to broader risks associated with the widespread use of proprietary APIs within the computational social sciences.","sentences":["Proprietary public APIs play a crucial and growing role as research tools among social scientists.","Among such APIs, Google's machine learning-based Perspective API is extensively utilized for assessing the toxicity of social media messages, providing both an important resource for researchers and automatic content moderation.","However, this paper exposes an important bias in Perspective API concerning German language text.","Through an in-depth examination of several datasets, we uncover intrinsic language biases within the multilingual model of Perspective API.","We find that the toxicity assessment of German content produces significantly higher toxicity levels than other languages.","This finding is robust across various translations, topics, and data sources, and has significant consequences for both research and moderation strategies that rely on Perspective API.","For instance, we show that, on average, four times more tweets and users would be moderated when using the German language compared to their English translation.","Our findings point to broader risks associated with the widespread use of proprietary APIs within the computational social sciences."],"url":"http://arxiv.org/abs/2312.12651v1"}
{"created":"2023-12-19 22:45:57","title":"IS-DARTS: Stabilizing DARTS through Precise Measurement on Candidate Importance","abstract":"Among existing Neural Architecture Search methods, DARTS is known for its efficiency and simplicity. This approach applies continuous relaxation of network representation to construct a weight-sharing supernet and enables the identification of excellent subnets in just a few GPU days. However, performance collapse in DARTS results in deteriorating architectures filled with parameter-free operations and remains a great challenge to the robustness. To resolve this problem, we reveal that the fundamental reason is the biased estimation of the candidate importance in the search space through theoretical and experimental analysis, and more precisely select operations via information-based measurements. Furthermore, we demonstrate that the excessive concern over the supernet and inefficient utilization of data in bi-level optimization also account for suboptimal results. We adopt a more realistic objective focusing on the performance of subnets and simplify it with the help of the information-based measurements. Finally, we explain theoretically why progressively shrinking the width of the supernet is necessary and reduce the approximation error of optimal weights in DARTS. Our proposed method, named IS-DARTS, comprehensively improves DARTS and resolves the aforementioned problems. Extensive experiments on NAS-Bench-201 and DARTS-based search space demonstrate the effectiveness of IS-DARTS.","sentences":["Among existing Neural Architecture Search methods, DARTS is known for its efficiency and simplicity.","This approach applies continuous relaxation of network representation to construct a weight-sharing supernet and enables the identification of excellent subnets in just a few GPU days.","However, performance collapse in DARTS results in deteriorating architectures filled with parameter-free operations and remains a great challenge to the robustness.","To resolve this problem, we reveal that the fundamental reason is the biased estimation of the candidate importance in the search space through theoretical and experimental analysis, and more precisely select operations via information-based measurements.","Furthermore, we demonstrate that the excessive concern over the supernet and inefficient utilization of data in bi-level optimization also account for suboptimal results.","We adopt a more realistic objective focusing on the performance of subnets and simplify it with the help of the information-based measurements.","Finally, we explain theoretically why progressively shrinking the width of the supernet is necessary and reduce the approximation error of optimal weights in DARTS.","Our proposed method, named IS-DARTS, comprehensively improves DARTS and resolves the aforementioned problems.","Extensive experiments on NAS-Bench-201 and DARTS-based search space demonstrate the effectiveness of IS-DARTS."],"url":"http://arxiv.org/abs/2312.12648v1"}
{"created":"2023-12-19 22:33:17","title":"MotionScript: Natural Language Descriptions for Expressive 3D Human Motions","abstract":"This paper proposes MotionScript, a motion-to-text conversion algorithm and natural language representation for human body motions. MotionScript aims to describe movements in greater detail and with more accuracy than previous natural language approaches. Many motion datasets describe relatively objective and simple actions with little variation on the way they are expressed (e.g. sitting, walking, dribbling a ball). But for expressive actions that contain a diversity of movements in the class (e.g. being sad, dancing), or for actions outside the domain of standard motion capture datasets (e.g. stylistic walking, sign-language), more specific and granular natural language descriptions are needed. Our proposed MotionScript descriptions differ from existing natural language representations in that it provides direct descriptions in natural language instead of simple action labels or high-level human captions. To the best of our knowledge, this is the first attempt at translating 3D motions to natural language descriptions without requiring training data. Our experiments show that when MotionScript representations are used in a text-to-motion neural task, body movements are more accurately reconstructed, and large language models can be used to generate unseen complex motions.","sentences":["This paper proposes MotionScript, a motion-to-text conversion algorithm and natural language representation for human body motions.","MotionScript aims to describe movements in greater detail and with more accuracy than previous natural language approaches.","Many motion datasets describe relatively objective and simple actions with little variation on the way they are expressed (e.g. sitting, walking, dribbling a ball).","But for expressive actions that contain a diversity of movements in the class (e.g. being sad, dancing), or for actions outside the domain of standard motion capture datasets (e.g. stylistic walking, sign-language), more specific and granular natural language descriptions are needed.","Our proposed MotionScript descriptions differ from existing natural language representations in that it provides direct descriptions in natural language instead of simple action labels or high-level human captions.","To the best of our knowledge, this is the first attempt at translating 3D motions to natural language descriptions without requiring training data.","Our experiments show that when MotionScript representations are used in a text-to-motion neural task, body movements are more accurately reconstructed, and large language models can be used to generate unseen complex motions."],"url":"http://arxiv.org/abs/2312.12634v1"}
{"created":"2023-12-19 22:01:01","title":"Building a Llama2-finetuned LLM for Odia Language Utilizing Domain Knowledge Instruction Set","abstract":"Building LLMs for languages other than English is in great demand due to the unavailability and performance of multilingual LLMs, such as understanding the local context. The problem is critical for low-resource languages due to the need for instruction sets. In a multilingual country like India, there is a need for LLMs supporting Indic languages to provide generative AI and LLM-based technologies and services to its citizens.   This paper presents our approach of i) generating a large Odia instruction set, including domain knowledge data suitable for LLM fine-tuning, and ii) building a Llama2-finetuned model tailored for enhanced performance in the Odia domain. The proposed work will help researchers build an instruction set and LLM, particularly for Indic languages. We will release the model and instruction set for the public for research and noncommercial purposes.","sentences":["Building LLMs for languages other than English is in great demand due to the unavailability and performance of multilingual LLMs, such as understanding the local context.","The problem is critical for low-resource languages due to the need for instruction sets.","In a multilingual country like India, there is a need for LLMs supporting Indic languages to provide generative AI and LLM-based technologies and services to its citizens.   ","This paper presents our approach of i) generating a large Odia instruction set, including domain knowledge data suitable for LLM fine-tuning, and ii) building a Llama2-finetuned model tailored for enhanced performance in the Odia domain.","The proposed work will help researchers build an instruction set and LLM, particularly for Indic languages.","We will release the model and instruction set for the public for research and noncommercial purposes."],"url":"http://arxiv.org/abs/2312.12624v1"}
{"created":"2023-12-19 21:56:24","title":"\"It Can Relate to Real Lives\": Attitudes and Expectations in Justice-Centered Data Structures & Algorithms for Non-Majors","abstract":"Prior work has argued for a more justice-centered approach to postsecondary computing education by emphasizing ethics, identity, and political vision. In this experience report, we examine how postsecondary students of diverse gender and racial identities experience a justice-centered Data Structures and Algorithms designed for undergraduate non-computer science majors. Through a quantitative and qualitative analysis of two quarters of student survey data collected at the start and end of each quarter, we report on student attitudes and expectations.   Across the class, we found a significant increase in the following attitudes: computing confidence and sense of belonging. While women, non-binary, and other students not identifying as men (WNB+) also increased in these areas, they still reported significantly lower confidence and sense of belonging than men at the end of the quarter. Black, Latinx, Middle Eastern and North African, Native American, and Pacific Islander (BLMNPI) students had no significant differences compared to white and Asian students.   We also analyzed end-of-quarter student self-reflections on their fulfillment of expectations prior to taking the course. While the majority of students reported a positive overall sentiment about the course and many students specifically appreciated the justice-centered approach, some desired more practice with program implementation and interview preparation. We discuss implications for practice and articulate a political vision for holding both appreciation for computing ethics and a desire for professional preparation together through iterative design.","sentences":["Prior work has argued for a more justice-centered approach to postsecondary computing education by emphasizing ethics, identity, and political vision.","In this experience report, we examine how postsecondary students of diverse gender and racial identities experience a justice-centered Data Structures and Algorithms designed for undergraduate non-computer science majors.","Through a quantitative and qualitative analysis of two quarters of student survey data collected at the start and end of each quarter, we report on student attitudes and expectations.   ","Across the class, we found a significant increase in the following attitudes: computing confidence and sense of belonging.","While women, non-binary, and other students not identifying as men (WNB+) also increased in these areas, they still reported significantly lower confidence and sense of belonging than men at the end of the quarter.","Black, Latinx, Middle Eastern and North African, Native American, and Pacific Islander (BLMNPI) students had no significant differences compared to white and Asian students.   ","We also analyzed end-of-quarter student self-reflections on their fulfillment of expectations prior to taking the course.","While the majority of students reported a positive overall sentiment about the course and many students specifically appreciated the justice-centered approach, some desired more practice with program implementation and interview preparation.","We discuss implications for practice and articulate a political vision for holding both appreciation for computing ethics and a desire for professional preparation together through iterative design."],"url":"http://arxiv.org/abs/2312.12620v1"}
{"created":"2023-12-19 21:18:14","title":"Studying the Practices of Testing Machine Learning Software in the Wild","abstract":"Background: We are witnessing an increasing adoption of machine learning (ML), especially deep learning (DL) algorithms in many software systems, including safety-critical systems such as health care systems or autonomous driving vehicles. Ensuring the software quality of these systems is yet an open challenge for the research community, mainly due to the inductive nature of ML software systems. Traditionally, software systems were constructed deductively, by writing down the rules that govern the behavior of the system as program code. However, for ML software, these rules are inferred from training data. Few recent research advances in the quality assurance of ML systems have adapted different concepts from traditional software testing, such as mutation testing, to help improve the reliability of ML software systems. However, it is unclear if any of these proposed testing techniques from research are adopted in practice. There is little empirical evidence about the testing strategies of ML engineers. Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing practices in the wild, to identify the ML properties being tested, the followed testing strategies, and their implementation throughout the ML workflow. Method: First, we systematically summarized the different testing strategies (e.g., Oracle Approximation), the tested ML properties (e.g., Correctness, Bias, and Fairness), and the testing methods (e.g., Unit test) from the literature. Then, we conducted a study to understand the practices of testing ML software. Results: In our findings: 1) we identified four (4) major categories of testing strategy including Grey-box, White-box, Black-box, and Heuristic-based techniques that are used by the ML engineers to find software bugs. 2) We identified 16 ML properties that are tested in the ML workflow.","sentences":["Background: We are witnessing an increasing adoption of machine learning (ML), especially deep learning (DL) algorithms in many software systems, including safety-critical systems such as health care systems or autonomous driving vehicles.","Ensuring the software quality of these systems is yet an open challenge for the research community, mainly due to the inductive nature of ML software systems.","Traditionally, software systems were constructed deductively, by writing down the rules that govern the behavior of the system as program code.","However, for ML software, these rules are inferred from training data.","Few recent research advances in the quality assurance of ML systems have adapted different concepts from traditional software testing, such as mutation testing, to help improve the reliability of ML software systems.","However, it is unclear if any of these proposed testing techniques from research are adopted in practice.","There is little empirical evidence about the testing strategies of ML engineers.","Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing practices in the wild, to identify the ML properties being tested, the followed testing strategies, and their implementation throughout the ML workflow.","Method:","First, we systematically summarized the different testing strategies (e.g., Oracle Approximation), the tested ML properties (e.g., Correctness, Bias, and Fairness), and the testing methods (e.g., Unit test) from the literature.","Then, we conducted a study to understand the practices of testing ML software.","Results:","In our findings: 1) we identified four (4) major categories of testing strategy including Grey-box, White-box, Black-box, and Heuristic-based techniques that are used by the ML engineers to find software bugs.","2) We identified 16 ML properties that are tested in the ML workflow."],"url":"http://arxiv.org/abs/2312.12604v1"}
{"created":"2023-12-19 21:09:54","title":"Behind the Intent of Extract Method Refactoring: A Systematic Literature Review","abstract":"Code refactoring is widely recognized as an essential software engineering practice to improve the understandability and maintainability of the source code. The Extract Method refactoring is considered as \"Swiss army knife\" of refactorings, as developers often apply it to improve their code quality. In recent years, several studies attempted to recommend Extract Method refactorings allowing the collection, analysis, and revelation of actionable data-driven insights about refactoring practices within software projects. In this paper, we aim at reviewing the current body of knowledge on existing Extract Method refactoring research and explore their limitations and potential improvement opportunities for future research efforts. Hence, researchers and practitioners begin to be aware of the state-of-the-art and identify new research opportunities in this context. We review the body of knowledge related to Extract Method refactoring in the form of a systematic literature review (SLR). After compiling an initial pool of 1,367 papers, we conducted a systematic selection and our final pool included 83 primary studies. We define three sets of research questions and systematically develop and refine a classification schema based on several criteria including their methodology, applicability, and degree of automation. The results construct a catalog of 83 Extract Method approaches indicating that several techniques have been proposed in the literature. Our results show that: (i) 38.6% of Extract Method refactoring studies primarily focus on addressing code clones; (ii) Several of the Extract Method tools incorporate the developer's involvement in the decision-making process when applying the method extraction, and (iii) the existing benchmarks are heterogeneous and do not contain the same type of information, making standardizing them for the purpose of benchmarking difficult.","sentences":["Code refactoring is widely recognized as an essential software engineering practice to improve the understandability and maintainability of the source code.","The Extract Method refactoring is considered as \"Swiss army knife\" of refactorings, as developers often apply it to improve their code quality.","In recent years, several studies attempted to recommend Extract Method refactorings allowing the collection, analysis, and revelation of actionable data-driven insights about refactoring practices within software projects.","In this paper, we aim at reviewing the current body of knowledge on existing Extract Method refactoring research and explore their limitations and potential improvement opportunities for future research efforts.","Hence, researchers and practitioners begin to be aware of the state-of-the-art and identify new research opportunities in this context.","We review the body of knowledge related to Extract Method refactoring in the form of a systematic literature review (SLR).","After compiling an initial pool of 1,367 papers, we conducted a systematic selection and our final pool included 83 primary studies.","We define three sets of research questions and systematically develop and refine a classification schema based on several criteria including their methodology, applicability, and degree of automation.","The results construct a catalog of 83 Extract Method approaches indicating that several techniques have been proposed in the literature.","Our results show that: (i) 38.6% of Extract Method refactoring studies primarily focus on addressing code clones; (ii) Several of the Extract Method tools incorporate the developer's involvement in the decision-making process when applying the method extraction, and (iii) the existing benchmarks are heterogeneous and do not contain the same type of information, making standardizing them for the purpose of benchmarking difficult."],"url":"http://arxiv.org/abs/2312.12600v1"}
{"created":"2023-12-19 20:49:28","title":"Robust Machine Learning by Transforming and Augmenting Imperfect Training Data","abstract":"Machine Learning (ML) is an expressive framework for turning data into computer programs. Across many problem domains -- both in industry and policy settings -- the types of computer programs needed for accurate prediction or optimal control are difficult to write by hand. On the other hand, collecting instances of desired system behavior may be relatively more feasible. This makes ML broadly appealing, but also induces data sensitivities that often manifest as unexpected failure modes during deployment. In this sense, the training data available tend to be imperfect for the task at hand. This thesis explores several data sensitivities of modern machine learning and how to address them. We begin by discussing how to prevent ML from codifying prior human discrimination measured in the training data, where we take a fair representation learning approach. We then discuss the problem of learning from data containing spurious features, which provide predictive fidelity during training but are unreliable upon deployment. Here we observe that insofar as standard training methods tend to learn such features, this propensity can be leveraged to search for partitions of training data that expose this inconsistency, ultimately promoting learning algorithms invariant to spurious features. Finally, we turn our attention to reinforcement learning from data with insufficient coverage over all possible states and actions. To address the coverage issue, we discuss how causal priors can be used to model the single-step dynamics of the setting where data are collected. This enables a new type of data augmentation where observed trajectories are stitched together to produce new but plausible counterfactual trajectories.","sentences":["Machine Learning (ML) is an expressive framework for turning data into computer programs.","Across many problem domains -- both in industry and policy settings -- the types of computer programs needed for accurate prediction or optimal control are difficult to write by hand.","On the other hand, collecting instances of desired system behavior may be relatively more feasible.","This makes ML broadly appealing, but also induces data sensitivities that often manifest as unexpected failure modes during deployment.","In this sense, the training data available tend to be imperfect for the task at hand.","This thesis explores several data sensitivities of modern machine learning and how to address them.","We begin by discussing how to prevent ML from codifying prior human discrimination measured in the training data, where we take a fair representation learning approach.","We then discuss the problem of learning from data containing spurious features, which provide predictive fidelity during training but are unreliable upon deployment.","Here we observe that insofar as standard training methods tend to learn such features, this propensity can be leveraged to search for partitions of training data that expose this inconsistency, ultimately promoting learning algorithms invariant to spurious features.","Finally, we turn our attention to reinforcement learning from data with insufficient coverage over all possible states and actions.","To address the coverage issue, we discuss how causal priors can be used to model the single-step dynamics of the setting where data are collected.","This enables a new type of data augmentation where observed trajectories are stitched together to produce new but plausible counterfactual trajectories."],"url":"http://arxiv.org/abs/2312.12597v1"}
{"created":"2023-12-19 20:35:08","title":"An Empirical study of Unsupervised Neural Machine Translation: analyzing NMT output, model's behavior and sentences' contribution","abstract":"Unsupervised Neural Machine Translation (UNMT) focuses on improving NMT results under the assumption there is no human translated parallel data, yet little work has been done so far in highlighting its advantages compared to supervised methods and analyzing its output in aspects other than translation accuracy. We focus on three very diverse languages, French, Gujarati, and Kazakh, and train bilingual NMT models, to and from English, with various levels of supervision, in high- and low- resource setups, measure quality of the NMT output and compare the generated sequences' word order and semantic similarity to source and reference sentences. We also use Layer-wise Relevance Propagation to evaluate the source and target sentences' contribution to the result, expanding the findings of previous works to the UNMT paradigm.","sentences":["Unsupervised Neural Machine Translation (UNMT) focuses on improving NMT results under the assumption there is no human translated parallel data, yet little work has been done so far in highlighting its advantages compared to supervised methods and analyzing its output in aspects other than translation accuracy.","We focus on three very diverse languages, French, Gujarati, and Kazakh, and train bilingual NMT models, to and from English, with various levels of supervision, in high- and low- resource setups, measure quality of the NMT output and compare the generated sequences' word order and semantic similarity to source and reference sentences.","We also use Layer-wise Relevance Propagation to evaluate the source and target sentences' contribution to the result, expanding the findings of previous works to the UNMT paradigm."],"url":"http://arxiv.org/abs/2312.12588v1"}
