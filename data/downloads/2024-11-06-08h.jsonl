{"created":"2024-11-05 18:58:15","title":"Classification Done Right for Vision-Language Pre-Training","abstract":"We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection. Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP does. SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks. We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP. https://github.com/x-cls/superclass","sentences":["We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data.","Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection.","Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP does.","SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks.","We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP.","https://github.com/x-cls/superclass"],"url":"http://arxiv.org/abs/2411.03313v1"}
{"created":"2024-11-05 18:01:12","title":"LLMs for Domain Generation Algorithm Detection","abstract":"This work analyzes the use of large language models (LLMs) for detecting domain generation algorithms (DGAs). We perform a detailed evaluation of two important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT), showing how they can improve detection. SFT increases performance by using domain-specific data, whereas ICL helps the detection model to quickly adapt to new threats without requiring much retraining. We use Meta's Llama3 8B model, on a custom dataset with 68 malware families and normal domains, covering several hard-to-detect schemes, including recent word-based DGAs. Results proved that LLM-based methods can achieve competitive results in DGA detection. In particular, the SFT-based LLM DGA detector outperforms state-of-the-art models using attention layers, achieving 94% accuracy with a 4% false positive rate (FPR) and excelling at detecting word-based DGA domains.","sentences":["This work analyzes the use of large language models (LLMs) for detecting domain generation algorithms (DGAs).","We perform a detailed evaluation of two important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT), showing how they can improve detection.","SFT increases performance by using domain-specific data, whereas ICL helps the detection model to quickly adapt to new threats without requiring much retraining.","We use Meta's Llama3 8B model, on a custom dataset with 68 malware families and normal domains, covering several hard-to-detect schemes, including recent word-based DGAs.","Results proved that LLM-based methods can achieve competitive results in DGA detection.","In particular, the SFT-based LLM DGA detector outperforms state-of-the-art models using attention layers, achieving 94% accuracy with a 4% false positive rate (FPR) and excelling at detecting word-based DGA domains."],"url":"http://arxiv.org/abs/2411.03307v1"}
{"created":"2024-11-05 17:56:27","title":"Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor","abstract":"We present the first static-obstacle avoidance method for quadrotors using just an onboard, monocular event camera. Quadrotors are capable of fast and agile flight in cluttered environments when piloted manually, but vision-based autonomous flight in unknown environments is difficult in part due to the sensor limitations of traditional onboard cameras. Event cameras, however, promise nearly zero motion blur and high dynamic range, but produce a very large volume of events under significant ego-motion and further lack a continuous-time sensor model in simulation, making direct sim-to-real transfer not possible. By leveraging depth prediction as a pretext task in our learning framework, we can pre-train a reactive obstacle avoidance events-to-control policy with approximated, simulated events and then fine-tune the perception component with limited events-and-depth real-world data to achieve obstacle avoidance in indoor and outdoor settings. We demonstrate this across two quadrotor-event camera platforms in multiple settings and find, contrary to traditional vision-based works, that low speeds (1m/s) make the task harder and more prone to collisions, while high speeds (5m/s) result in better event-based depth estimation and avoidance. We also find that success rates in outdoor scenes can be significantly higher than in certain indoor scenes.","sentences":["We present the first static-obstacle avoidance method for quadrotors using just an onboard, monocular event camera.","Quadrotors are capable of fast and agile flight in cluttered environments when piloted manually, but vision-based autonomous flight in unknown environments is difficult in part due to the sensor limitations of traditional onboard cameras.","Event cameras, however, promise nearly zero motion blur and high dynamic range, but produce a very large volume of events under significant ego-motion and further lack a continuous-time sensor model in simulation, making direct sim-to-real transfer not possible.","By leveraging depth prediction as a pretext task in our learning framework, we can pre-train a reactive obstacle avoidance events-to-control policy with approximated, simulated events and then fine-tune the perception component with limited events-and-depth real-world data to achieve obstacle avoidance in indoor and outdoor settings.","We demonstrate this across two quadrotor-event camera platforms in multiple settings and find, contrary to traditional vision-based works, that low speeds (1m/s) make the task harder and more prone to collisions, while high speeds (5m/s) result in better event-based depth estimation and avoidance.","We also find that success rates in outdoor scenes can be significantly higher than in certain indoor scenes."],"url":"http://arxiv.org/abs/2411.03303v1"}
{"created":"2024-11-05 17:50:39","title":"Concurrent Composition for Continual Mechanisms","abstract":"A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC `21, NeurIPS `22, STOC `23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism, when differential privacy is measured using $f$-DP and the adversary is adaptive. We extend their work to the $\\textit{continual observation setting,}$ where the data is arriving online in a potentially adaptive manner. More specifically, we show that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism, where the adversary is adaptive. We show this result for $f$-DP, which also implies the result for pure DP and $(\\epsilon, \\delta)$-DP.","sentences":["A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC `21, NeurIPS `22, STOC `23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism, when differential privacy is measured using $f$-DP and the adversary is adaptive.","We extend their work to the $\\textit{continual observation setting,}$ where the data is arriving online in a potentially adaptive manner.","More specifically, we show that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism, where the adversary is adaptive.","We show this result for $f$-DP, which also implies the result for pure DP and $(\\epsilon, \\delta)$-DP."],"url":"http://arxiv.org/abs/2411.03299v1"}
{"created":"2024-11-05 17:41:14","title":"Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning","abstract":"We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning. Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states. Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from object keypoint manifold gradient in the original training data. The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations. We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of $\\textbf{77.7\\%}$ over the base policy in OOD. Project Website: https://sites.google.com/view/ocr-penn","sentences":["We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning.","Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states.","Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from object keypoint manifold gradient in the original training data.","The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations.","We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of $\\textbf{77.7\\%}$ over the base policy in OOD.","Project Website: https://sites.google.com/view/ocr-penn"],"url":"http://arxiv.org/abs/2411.03294v1"}
{"created":"2024-11-05 17:38:03","title":"Data-Driven Sampling Based Stochastic MPC for Skid-Steer Mobile Robot Navigation","abstract":"Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs. This enables us to develop an adaptive, uncertainty-informed navigation formulation. We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method. This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control. Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance. Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other. We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework. In simulations, our approach shows superior tracking accuracy and obstacle avoidance. We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation. The GPU implementation of the proposed method and supplementary video footage are available at https: //stochasticmppi.github.io.","sentences":["Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers.","In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs.","This enables us to develop an adaptive, uncertainty-informed navigation formulation.","We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method.","This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control.","Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance.","Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other.","We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework.","In simulations, our approach shows superior tracking accuracy and obstacle avoidance.","We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation.","The GPU implementation of the proposed method and supplementary video footage are available at https: //stochasticmppi.github.io."],"url":"http://arxiv.org/abs/2411.03289v1"}
{"created":"2024-11-05 17:33:39","title":"SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents","abstract":"While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity. To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency. Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking. Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs. Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization. Code and data will be available at: https://github.com/David-Li0406/SMoA.","sentences":["While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity.","To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs.","Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency.","Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking.","Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs.","Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization.","Code and data will be available at: https://github.com/David-Li0406/SMoA."],"url":"http://arxiv.org/abs/2411.03284v1"}
{"created":"2024-11-05 17:16:56","title":"Graph-Based Semi-Supervised Segregated Lipschitz Learning","abstract":"This paper presents an approach to semi-supervised learning for the classification of data using the Lipschitz Learning on graphs. We develop a graph-based semi-supervised learning framework that leverages the properties of the infinity Laplacian to propagate labels in a dataset where only a few samples are labeled. By extending the theory of spatial segregation from the Laplace operator to the infinity Laplace operator, both in continuum and discrete settings, our approach provides a robust method for dealing with class imbalance, a common challenge in machine learning. Experimental validation on several benchmark datasets demonstrates that our method not only improves classification accuracy compared to existing methods but also ensures efficient label propagation in scenarios with limited labeled data.","sentences":["This paper presents an approach to semi-supervised learning for the classification of data using the Lipschitz Learning on graphs.","We develop a graph-based semi-supervised learning framework that leverages the properties of the infinity Laplacian to propagate labels in a dataset where only a few samples are labeled.","By extending the theory of spatial segregation from the Laplace operator to the infinity Laplace operator, both in continuum and discrete settings, our approach provides a robust method for dealing with class imbalance, a common challenge in machine learning.","Experimental validation on several benchmark datasets demonstrates that our method not only improves classification accuracy compared to existing methods but also ensures efficient label propagation in scenarios with limited labeled data."],"url":"http://arxiv.org/abs/2411.03273v1"}
{"created":"2024-11-05 17:02:29","title":"Proxy-informed Bayesian transfer learning with unknown sources","abstract":"Generalization outside the scope of one's training data requires leveraging prior knowledge about the effects that transfer, and the effects that don't, between different data sources. Bayesian transfer learning is a principled paradigm for specifying this knowledge, and refining it on the basis of data from the source (training) and target (prediction) tasks. We address the challenging transfer learning setting where the learner (i) cannot fine-tune in the target task, and (ii) does not know which source data points correspond to the same task (i.e., the data sources are unknown). We propose a proxy-informed robust method for probabilistic transfer learning (PROMPT), which provides a posterior predictive estimate tailored to the structure of the target task, without requiring the learner have access to any outcome information from the target task. Instead, PROMPT relies on the availability of proxy information. PROMPT uses the same proxy information for two purposes: (i) estimation of effects specific to the target task, and (ii) construction of a robust reweighting of the source data for estimation of effects that transfer between tasks. We provide theoretical results on the effect of this reweighting on the risk of negative transfer, and demonstrate application of PROMPT in two synthetic settings.","sentences":["Generalization outside the scope of one's training data requires leveraging prior knowledge about the effects that transfer, and the effects that don't, between different data sources.","Bayesian transfer learning is a principled paradigm for specifying this knowledge, and refining it on the basis of data from the source (training) and target (prediction) tasks.","We address the challenging transfer learning setting where the learner (i) cannot fine-tune in the target task, and (ii) does not know which source data points correspond to the same task (i.e., the data sources are unknown).","We propose a proxy-informed robust method for probabilistic transfer learning (PROMPT), which provides a posterior predictive estimate tailored to the structure of the target task, without requiring the learner have access to any outcome information from the target task.","Instead, PROMPT relies on the availability of proxy information.","PROMPT uses the same proxy information for two purposes: (i) estimation of effects specific to the target task, and (ii) construction of a robust reweighting of the source data for estimation of effects that transfer between tasks.","We provide theoretical results on the effect of this reweighting on the risk of negative transfer, and demonstrate application of PROMPT in two synthetic settings."],"url":"http://arxiv.org/abs/2411.03263v1"}
{"created":"2024-11-05 16:50:54","title":"Discovering Data Structures: Nearest Neighbor Search and Beyond","abstract":"We propose a general framework for end-to-end learning of data structures. Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity. Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures/algorithms. We first apply this framework to the problem of nearest neighbor search. In several settings, we are able to reverse-engineer the learned data structures and query algorithms. For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search. In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, they have elements of locality-sensitive hashing. The model can also learn useful representations of high-dimensional data and exploit them to design effective data structures. We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could also be a powerful discovery tool for new problems.","sentences":["We propose a general framework for end-to-end learning of data structures.","Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity.","Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures/algorithms.","We first apply this framework to the problem of nearest neighbor search.","In several settings, we are able to reverse-engineer the learned data structures and query algorithms.","For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search.","In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, they have elements of locality-sensitive hashing.","The model can also learn useful representations of high-dimensional data and exploit them to design effective data structures.","We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could also be a powerful discovery tool for new problems."],"url":"http://arxiv.org/abs/2411.03253v1"}
{"created":"2024-11-05 16:47:53","title":"DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models","abstract":"Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2-7 percent in certain cases. The data and code will be publicly available upon completion of internal review.","sentences":["Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis.","However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data.","To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module.","As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution.","Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2-7 percent in certain cases.","The data and code will be publicly available upon completion of internal review."],"url":"http://arxiv.org/abs/2411.03250v1"}
{"created":"2024-11-05 16:37:30","title":"Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution","abstract":"Recovering high-quality depth maps from compressed sources has gained significant attention due to the limitations of consumer-grade depth cameras and the bandwidth restrictions during data transmission. However, current methods still suffer from two challenges. First, bit-depth compression produces a uniform depth representation in regions with subtle variations, hindering the recovery of detailed information. Second, densely distributed random noise reduces the accuracy of estimating the global geometric structure of the scene. To address these challenges, we propose a novel framework, termed geometry-decoupled network (GDNet), for compressed depth map super-resolution that decouples the high-quality depth map reconstruction process by handling global and detailed geometric features separately. To be specific, we propose the fine geometry detail encoder (FGDE), which is designed to aggregate fine geometry details in high-resolution low-level image features while simultaneously enriching them with complementary information from low-resolution context-level image features. In addition, we develop the global geometry encoder (GGE) that aims at suppressing noise and extracting global geometric information effectively via constructing compact feature representation in a low-rank space. We conduct experiments on multiple benchmark datasets, demonstrating that our GDNet significantly outperforms current methods in terms of geometric consistency and detail recovery. In the ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st place award. Our codes will be available.","sentences":["Recovering high-quality depth maps from compressed sources has gained significant attention due to the limitations of consumer-grade depth cameras and the bandwidth restrictions during data transmission.","However, current methods still suffer from two challenges.","First, bit-depth compression produces a uniform depth representation in regions with subtle variations, hindering the recovery of detailed information.","Second, densely distributed random noise reduces the accuracy of estimating the global geometric structure of the scene.","To address these challenges, we propose a novel framework, termed geometry-decoupled network (GDNet), for compressed depth map super-resolution that decouples the high-quality depth map reconstruction process by handling global and detailed geometric features separately.","To be specific, we propose the fine geometry detail encoder (FGDE), which is designed to aggregate fine geometry details in high-resolution low-level image features while simultaneously enriching them with complementary information from low-resolution context-level image features.","In addition, we develop the global geometry encoder (GGE) that aims at suppressing noise and extracting global geometric information effectively via constructing compact feature representation in a low-rank space.","We conduct experiments on multiple benchmark datasets, demonstrating that our GDNet significantly outperforms current methods in terms of geometric consistency and detail recovery.","In the ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st place award.","Our codes will be available."],"url":"http://arxiv.org/abs/2411.03239v1"}
{"created":"2024-11-05 16:36:51","title":"On the Detection of Non-Cooperative RISs: Scan B-Testing via Deep Support Vector Data Description","abstract":"In this paper, we study the problem of promptly detecting the presence of non-cooperative activity from one or more Reconfigurable Intelligent Surfaces (RISs) with unknown characteristics lying in the vicinity of a Multiple-Input Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division Multiplexing (OFDM) transmissions. We first present a novel wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces, which captures both the effect of the RIS actuation time on the channel in the frequency domain as well as the difference between changing phase configurations during or among transmissions. Considering that RISs may operate under the coordination of a third-party system, and thus, may negatively impact the communication of the intended MIMO OFDM system, we present a novel RIS activity detection framework that is unaware of the distribution of the phase configuration of any of the non-cooperative RISs. In particular, capitalizing on the knowledge of the data distribution at the multi-antenna receiver, we design a novel online change point detection statistic that combines a deep support vector data description model with the scan $B$-test. The presented numerical investigations demonstrate the improved detection accuracy as well as decreased computational complexity of the proposed RIS detection approach over existing change point detection schemes.","sentences":["In this paper, we study the problem of promptly detecting the presence of non-cooperative activity from one or more Reconfigurable Intelligent Surfaces (RISs) with unknown characteristics lying in the vicinity of a Multiple-Input Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division Multiplexing (OFDM) transmissions.","We first present a novel wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces, which captures both the effect of the RIS actuation time on the channel in the frequency domain as well as the difference between changing phase configurations during or among transmissions.","Considering that RISs may operate under the coordination of a third-party system, and thus, may negatively impact the communication of the intended MIMO OFDM system, we present a novel RIS activity detection framework that is unaware of the distribution of the phase configuration of any of the non-cooperative RISs.","In particular, capitalizing on the knowledge of the data distribution at the multi-antenna receiver, we design a novel online change point detection statistic that combines a deep support vector data description model with the scan $B$-test.","The presented numerical investigations demonstrate the improved detection accuracy as well as decreased computational complexity of the proposed RIS detection approach over existing change point detection schemes."],"url":"http://arxiv.org/abs/2411.03237v1"}
{"created":"2024-11-05 16:23:19","title":"Formal Logic-guided Robust Federated Learning against Poisoning Attacks","abstract":"Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning. However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance. Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems. However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data. In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants. Unlike traditional model-centric defenses, FLORAL leverages logical reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates. Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates. Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior. Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications. Notably, FLORAL reduced the prediction error by 93.27\\% in the best-case scenario compared to the second-best baseline. Our code is available at \\url{https://anonymous.4open.science/r/FLORAL-Robust-FTS}.","sentences":["Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning.","However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance.","Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems.","However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data.","In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants.","Unlike traditional model-centric defenses, FLORAL leverages logical reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates.","Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates.","Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior.","Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications.","Notably, FLORAL reduced the prediction error by 93.27\\% in the best-case scenario compared to the second-best baseline.","Our code is available at \\url{https://anonymous.4open.science/r/FLORAL-Robust-FTS}."],"url":"http://arxiv.org/abs/2411.03231v1"}
{"created":"2024-11-05 16:19:43","title":"Tight Sampling Bounds for Eigenvalue Approximation","abstract":"We consider the problem of estimating the spectrum of a symmetric bounded entry (not necessarily PSD) matrix via entrywise sampling. This problem was introduced by [Bhattacharjee, Dexter, Drineas, Musco, Ray '22], where it was shown that one can obtain an $\\epsilon n$ additive approximation to all eigenvalues of $A$ by sampling a principal submatrix of dimension $\\frac{\\text{poly}(\\log n)}{\\epsilon^3}$. We improve their analysis by showing that it suffices to sample a principal submatrix of dimension $\\tilde{O}(\\frac{1}{\\epsilon^2})$ (with no dependence on $n$). This matches known lower bounds and therefore resolves the sample complexity of this problem up to $\\log\\frac{1}{\\epsilon}$ factors. Using similar techniques, we give a tight $\\tilde{O}(\\frac{1}{\\epsilon^2})$ bound for obtaining an additive $\\epsilon\\|A\\|_F$ approximation to the spectrum of $A$ via squared row-norm sampling, improving on the previous best $\\tilde{O}(\\frac{1}{\\epsilon^{8}})$ bound. We also address the problem of approximating the top eigenvector for a bounded entry, PSD matrix $A.$ In particular, we show that sampling $O(\\frac{1}{\\epsilon})$ columns of $A$ suffices to produce a unit vector $u$ with $u^T A u \\geq \\lambda_1(A) - \\epsilon n$. This matches what one could achieve via the sampling bound of [Musco, Musco'17] for the special case of approximating the top eigenvector, but does not require adaptivity.   As additional applications, we observe that our sampling results can be used to design a faster eigenvalue estimation sketch for dense matrices resolving a question of [Swartworth, Woodruff'23], and can also be combined with [Musco, Musco'17] to achieve $O(1/\\epsilon^3)$ (adaptive) sample complexity for approximating the spectrum of a bounded entry PSD matrix to $\\epsilon n$ additive error.","sentences":["We consider the problem of estimating the spectrum of a symmetric bounded entry (not necessarily PSD) matrix via entrywise sampling.","This problem was introduced by [Bhattacharjee, Dexter, Drineas, Musco, Ray '22], where it was shown that one can obtain an $\\epsilon n$ additive approximation to all eigenvalues of $A$ by sampling a principal submatrix of dimension $\\frac{\\text{poly}(\\log n)}{\\epsilon^3}$.","We improve their analysis by showing that it suffices to sample a principal submatrix of dimension $\\tilde{O}(\\frac{1}{\\epsilon^2})$ (with no dependence on $n$).","This matches known lower bounds and therefore resolves the sample complexity of this problem up to $\\log\\frac{1}{\\epsilon}$ factors.","Using similar techniques, we give a tight $\\tilde{O}(\\frac{1}{\\epsilon^2})$ bound for obtaining an additive $\\epsilon\\|A\\|_F$ approximation to the spectrum of $A$ via squared row-norm sampling, improving on the previous best $\\tilde{O}(\\frac{1}{\\epsilon^{8}})$ bound.","We also address the problem of approximating the top eigenvector for a bounded entry, PSD matrix $A.$","In particular, we show that sampling $O(\\frac{1}{\\epsilon})$ columns of $A$ suffices to produce a unit vector $u$ with $u^T","A u \\geq \\lambda_1(A) - \\epsilon n$. This matches what one could achieve via the sampling bound of [Musco, Musco'17] for the special case of approximating the top eigenvector, but does not require adaptivity.   ","As additional applications, we observe that our sampling results can be used to design a faster eigenvalue estimation sketch for dense matrices resolving a question of [Swartworth, Woodruff'23], and can also be combined with [Musco, Musco'17] to achieve $O(1/\\epsilon^3)$ (adaptive) sample complexity for approximating the spectrum of a bounded entry PSD matrix to $\\epsilon n$ additive error."],"url":"http://arxiv.org/abs/2411.03227v1"}
{"created":"2024-11-05 16:15:25","title":"Interpretable Predictive Models for Healthcare via Rational Logistic Regression","abstract":"The healthcare sector has experienced a rapid accumulation of digital data recently, especially in the form of electronic health records (EHRs). EHRs constitute a precious resource that IS researchers could utilize for clinical applications (e.g., morbidity prediction). Deep learning seems like the obvious choice to exploit this surfeit of data. However, numerous studies have shown that deep learning does not enjoy the same kind of success on EHR data as it has in other domains; simple models like logistic regression are frequently as good as sophisticated deep learning ones. Inspired by this observation, we develop a novel model called rational logistic regression (RLR) that has standard logistic regression (LR) as its special case (and thus inherits LR's inductive bias that aligns with EHR data). RLR has rational series as its theoretical underpinnings, works on longitudinal time-series data, and learns interpretable patterns. Empirical comparisons on real-world clinical tasks demonstrate RLR's efficacy.","sentences":["The healthcare sector has experienced a rapid accumulation of digital data recently, especially in the form of electronic health records (EHRs).","EHRs constitute a precious resource that IS researchers could utilize for clinical applications (e.g., morbidity prediction).","Deep learning seems like the obvious choice to exploit this surfeit of data.","However, numerous studies have shown that deep learning does not enjoy the same kind of success on EHR data as it has in other domains; simple models like logistic regression are frequently as good as sophisticated deep learning ones.","Inspired by this observation, we develop a novel model called rational logistic regression (RLR) that has standard logistic regression (LR) as its special case (and thus inherits LR's inductive bias that aligns with EHR data).","RLR has rational series as its theoretical underpinnings, works on longitudinal time-series data, and learns interpretable patterns.","Empirical comparisons on real-world clinical tasks demonstrate RLR's efficacy."],"url":"http://arxiv.org/abs/2411.03224v1"}
{"created":"2024-11-05 16:12:12","title":"Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation","abstract":"Earth Observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph Neural Networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO, to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs' applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the paper by comparing them with other popular architectures like transformers and analyzing their potential synergies.","sentences":["Earth Observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures.","Graph Neural Networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain.","Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data.","To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs.","Then, we summarize the generic problems in EO, to which GNNs can offer potential solutions.","Following this, we explore a broad spectrum of GNNs' applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling.","The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks.","Furthermore, we highlight methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research.","While acknowledging that GNNs are not a universal solution, we conclude the paper by comparing them with other popular architectures like transformers and analyzing their potential synergies."],"url":"http://arxiv.org/abs/2411.03223v1"}
{"created":"2024-11-05 15:53:59","title":"GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis","abstract":"Recent advancements in Generative AI offer promising capabilities for spatial analysis. Despite their potential, the integration of generative AI with established GIS platforms remains underexplored. In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example. Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters. The implementation of this framework resulted in the development of a \"GIS Copilot\" that allows GIS users to interact with QGIS using natural language commands for spatial analysis. The GIS Copilot was evaluated based on three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps. The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks. This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise. While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes.","sentences":["Recent advancements in Generative AI offer promising capabilities for spatial analysis.","Despite their potential, the integration of generative AI with established GIS platforms remains underexplored.","In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example.","Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters.","The implementation of this framework resulted in the development of a \"GIS Copilot\" that allows GIS users to interact with QGIS using natural language commands for spatial analysis.","The GIS Copilot was evaluated based on three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps.","The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks.","This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise.","While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes."],"url":"http://arxiv.org/abs/2411.03205v1"}
{"created":"2024-11-05 15:31:40","title":"Design-Reality Gap Analysis of Health Information Systems Failure","abstract":"This study investigates the factors contributing to the failure of Health Information Systems (HIS) in a public hospital in South Africa. While HIS have the potential to improve healthcare delivery by integrating services and enhancing effectiveness, failures can lead to service interruptions, revenue loss, data loss, administrative difficulties, and reputational damage. Using semi-structured interviews with key stakeholders, we employed a hybrid data analysis approach combining deductive analysis based on the Design- Reality Gap Model and inductive thematic analysis. Our findings highlight several factors contributing to HIS failures, including system capacity constraints, inadequate IT risk management, and critical skills gaps. Despite these challenges, end users perceive HIS positively and recommend its implementation for streamlining daily processes. This study underscores the importance of addressing design-reality gaps to improve HIS outcomes in public healthcare settings.","sentences":["This study investigates the factors contributing to the failure of Health Information Systems (HIS) in a public hospital in South Africa.","While HIS have the potential to improve healthcare delivery by integrating services and enhancing effectiveness, failures can lead to service interruptions, revenue loss, data loss, administrative difficulties, and reputational damage.","Using semi-structured interviews with key stakeholders, we employed a hybrid data analysis approach combining deductive analysis based on the Design- Reality Gap Model and inductive thematic analysis.","Our findings highlight several factors contributing to HIS failures, including system capacity constraints, inadequate IT risk management, and critical skills gaps.","Despite these challenges, end users perceive HIS positively and recommend its implementation for streamlining daily processes.","This study underscores the importance of addressing design-reality gaps to improve HIS outcomes in public healthcare settings."],"url":"http://arxiv.org/abs/2411.03187v1"}
{"created":"2024-11-05 15:22:11","title":"ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression","abstract":"As a core component in modern data centers, key-value cache provides high-throughput and low-latency services for high-speed data processing. The effectiveness of a key-value cache relies on its ability of accommodating the needed data. However, expanding the cache capacity is often more difficult than commonly expected because of many practical constraints, such as server costs, cooling issues, rack space, and even human resource expenses. A potential solution is compression, which virtually extends the cache capacity by condensing data in cache. In practice, this seemingly simple idea has not gained much traction in key-value cache system design, due to several critical issues: the compression-unfriendly index structure, severe read/write amplification, wasteful decompression operations, and heavy computing cost. This paper presents a hybrid DRAM-SSD cache design to realize a systematic integration of data compression in key-value cache. By treating compression as an essential component, we have redesigned the indexing structure, data management, and leveraged the emerging computational SSD hardware for collaborative optimizations. We have developed a prototype, called ZipCache. Our experimental results show that ZipCache can achieve up to 72.4% higher throughput and 42.4% lower latency, while reducing the write amplification by up to 26.2 times.","sentences":["As a core component in modern data centers, key-value cache provides high-throughput and low-latency services for high-speed data processing.","The effectiveness of a key-value cache relies on its ability of accommodating the needed data.","However, expanding the cache capacity is often more difficult than commonly expected because of many practical constraints, such as server costs, cooling issues, rack space, and even human resource expenses.","A potential solution is compression, which virtually extends the cache capacity by condensing data in cache.","In practice, this seemingly simple idea has not gained much traction in key-value cache system design, due to several critical issues: the compression-unfriendly index structure, severe read/write amplification, wasteful decompression operations, and heavy computing cost.","This paper presents a hybrid DRAM-SSD cache design to realize a systematic integration of data compression in key-value cache.","By treating compression as an essential component, we have redesigned the indexing structure, data management, and leveraged the emerging computational SSD hardware for collaborative optimizations.","We have developed a prototype, called ZipCache.","Our experimental results show that ZipCache can achieve up to 72.4% higher throughput and 42.4% lower latency, while reducing the write amplification by up to 26.2 times."],"url":"http://arxiv.org/abs/2411.03174v1"}
{"created":"2024-11-05 15:18:02","title":"Pre-trained Visual Dynamics Representations for Efficient Policy Learning","abstract":"Pre-training for Reinforcement Learning (RL) with purely video data is a valuable yet challenging problem. Although in-the-wild videos are readily available and inhere a vast amount of prior world knowledge, the absence of action annotations and the common domain gap with downstream tasks hinder utilizing videos for RL pre-training. To address the challenge of pre-training with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to bridge the domain gap between videos and downstream tasks for efficient policy learning. By adopting video prediction as a pre-training task, we use a Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual dynamics representations. The pre-trained visual dynamics representations capture the visual dynamics prior knowledge in the videos. This abstract prior knowledge can be readily adapted to downstream tasks and aligned with executable actions through online adaptation. We conduct experiments on a series of robotics visual control tasks and verify that PVDR is an effective form for pre-training with videos to promote policy learning.","sentences":["Pre-training for Reinforcement Learning (RL) with purely video data is a valuable yet challenging problem.","Although in-the-wild videos are readily available and inhere a vast amount of prior world knowledge, the absence of action annotations and the common domain gap with downstream tasks hinder utilizing videos for RL pre-training.","To address the challenge of pre-training with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to bridge the domain gap between videos and downstream tasks for efficient policy learning.","By adopting video prediction as a pre-training task, we use a Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual dynamics representations.","The pre-trained visual dynamics representations capture the visual dynamics prior knowledge in the videos.","This abstract prior knowledge can be readily adapted to downstream tasks and aligned with executable actions through online adaptation.","We conduct experiments on a series of robotics visual control tasks and verify that PVDR is an effective form for pre-training with videos to promote policy learning."],"url":"http://arxiv.org/abs/2411.03169v1"}
{"created":"2024-11-05 14:08:43","title":"Fully Dynamic $k$-Median with Near-Optimal Update Time and Recourse","abstract":"In metric $k$-clustering, we are given as input a set of $n$ points in a general metric space, and we have to pick $k$ centers and cluster the input points around these chosen centers, so as to minimize an appropriate objective function. In recent years, significant effort has been devoted to the study of metric $k$-clustering problems in a dynamic setting, where the input keeps changing via updates (point insertions/deletions), and we have to maintain a good clustering throughout these updates. The performance of such a dynamic algorithm is measured in terms of three parameters: (i) Approximation ratio, which signifies the quality of the maintained solution, (ii) Recourse, which signifies how stable the maintained solution is, and (iii) Update time, which signifies the efficiency of the algorithm.   We consider the metric $k$-median problem, where the objective is the sum of the distances of the points to their nearest centers. We design the first dynamic algorithm for this problem with near-optimal guarantees across all three performance measures (up to a constant factor in approximation ratio, and polylogarithmic factors in recourse and update time). Specifically, we obtain a $O(1)$-approximation algorithm for dynamic metric $k$-median with $\\tilde{O}(1)$ recourse and $\\tilde{O}(k)$ update time. Prior to our work, the state-of-the-art here was the recent result of [Bhattacharya et al., FOCS'24], who obtained $O(\\epsilon^{-1})$-approximation ratio with $\\tilde{O}(k^{\\epsilon})$ recourse and $\\tilde{O}(k^{1+\\epsilon})$ update time.   We achieve our results by carefully synthesizing the concept of robust centers introduced in [Fichtenberger et al., SODA'21] along with the randomized local search subroutine from [Bhattacharya et al., FOCS'24], in addition to several key technical insights of our own.","sentences":["In metric $k$-clustering, we are given as input a set of $n$ points in a general metric space, and we have to pick $k$ centers and cluster the input points around these chosen centers, so as to minimize an appropriate objective function.","In recent years, significant effort has been devoted to the study of metric $k$-clustering problems in a dynamic setting, where the input keeps changing via updates (point insertions/deletions), and we have to maintain a good clustering throughout these updates.","The performance of such a dynamic algorithm is measured in terms of three parameters: (i) Approximation ratio, which signifies the quality of the maintained solution, (ii) Recourse, which signifies how stable the maintained solution is, and (iii) Update time, which signifies the efficiency of the algorithm.   ","We consider the metric $k$-median problem, where the objective is the sum of the distances of the points to their nearest centers.","We design the first dynamic algorithm for this problem with near-optimal guarantees across all three performance measures (up to a constant factor in approximation ratio, and polylogarithmic factors in recourse and update time).","Specifically, we obtain a $O(1)$-approximation algorithm for dynamic metric $k$-median with $\\tilde{O}(1)$ recourse and $\\tilde{O}(k)$ update time.","Prior to our work, the state-of-the-art here was the recent result of [Bhattacharya et al., FOCS'24], who obtained $O(\\epsilon^{-1})$-approximation ratio with $\\tilde{O}(k^{\\epsilon})$ recourse and $\\tilde{O}(k^{1+\\epsilon})$ update time.   ","We achieve our results by carefully synthesizing the concept of robust centers introduced in [Fichtenberger et al., SODA'21] along with the randomized local search subroutine from [Bhattacharya et al., FOCS'24], in addition to several key technical insights of our own."],"url":"http://arxiv.org/abs/2411.03121v1"}
{"created":"2024-11-05 13:50:09","title":"Evaluating Machine Learning Models against Clinical Protocols for Enhanced Interpretability and Continuity of Care","abstract":"In clinical practice, decision-making relies heavily on established protocols, often formalised as rules. Concurrently, Machine Learning (ML) models, trained on clinical data, aspire to integrate into medical decision-making processes. However, despite the growing number of ML applications, their adoption into clinical practice remains limited. Two critical concerns arise, relevant to the notions of consistency and continuity of care: (a) accuracy - the ML model, albeit more accurate, might introduce errors that would not have occurred by applying the protocol; (b) interpretability - ML models operating as black boxes might make predictions based on relationships that contradict established clinical knowledge. In this context, the literature suggests using ML models integrating domain knowledge for improved accuracy and interpretability. However, there is a lack of appropriate metrics for comparing ML models with clinical rules in addressing these challenges. Accordingly, in this article, we first propose metrics to assess the accuracy of ML models with respect to the established protocol. Secondly, we propose an approach to measure the distance of explanations provided by two rule sets, with the goal of comparing the explanation similarity between clinical rule-based systems and rules extracted from ML models. The approach is validated on the Pima Indians Diabetes dataset by training two neural networks - one exclusively on data, and the other integrating a clinical protocol. Our findings demonstrate that the integrated ML model achieves comparable performance to that of a fully data-driven model while exhibiting superior accuracy relative to the clinical protocol, ensuring enhanced continuity of care. Furthermore, we show that our integrated model provides explanations for predictions that align more closely with the clinical protocol compared to the data-driven model.","sentences":["In clinical practice, decision-making relies heavily on established protocols, often formalised as rules.","Concurrently, Machine Learning (ML) models, trained on clinical data, aspire to integrate into medical decision-making processes.","However, despite the growing number of ML applications, their adoption into clinical practice remains limited.","Two critical concerns arise, relevant to the notions of consistency and continuity of care: (a) accuracy - the ML model, albeit more accurate, might introduce errors that would not have occurred by applying the protocol; (b) interpretability - ML models operating as black boxes might make predictions based on relationships that contradict established clinical knowledge.","In this context, the literature suggests using ML models integrating domain knowledge for improved accuracy and interpretability.","However, there is a lack of appropriate metrics for comparing ML models with clinical rules in addressing these challenges.","Accordingly, in this article, we first propose metrics to assess the accuracy of ML models with respect to the established protocol.","Secondly, we propose an approach to measure the distance of explanations provided by two rule sets, with the goal of comparing the explanation similarity between clinical rule-based systems and rules extracted from ML models.","The approach is validated on the Pima Indians Diabetes dataset by training two neural networks - one exclusively on data, and the other integrating a clinical protocol.","Our findings demonstrate that the integrated ML model achieves comparable performance to that of a fully data-driven model while exhibiting superior accuracy relative to the clinical protocol, ensuring enhanced continuity of care.","Furthermore, we show that our integrated model provides explanations for predictions that align more closely with the clinical protocol compared to the data-driven model."],"url":"http://arxiv.org/abs/2411.03105v1"}
{"created":"2024-11-05 13:44:25","title":"Local Lesion Generation is Effective for Capsule Endoscopy Image Data Augmentation in a Limited Data Setting","abstract":"Limited medical imaging datasets challenge deep learning models by increasing risks of overfitting and reduced generalization, particularly in Generative Adversarial Networks (GANs), where discriminators may overfit, leading to training divergence. This constraint also impairs classification models trained on small datasets. Generative Data Augmentation (GDA) addresses this by expanding training datasets with synthetic data, although it requires training a generative model. We propose and evaluate two local lesion generation approaches to address the challenge of augmenting small medical image datasets. The first approach employs the Poisson Image Editing algorithm, a classical image processing technique, to create realistic image composites that outperform current state-of-the-art methods. The second approach introduces a novel generative method, leveraging a fine-tuned Image Inpainting GAN to synthesize realistic lesions within specified regions of real training images. A comprehensive comparison of the two proposed methods demonstrates that effective local lesion generation in a data-constrained setting allows for reaching new state-of-the-art results in capsule endoscopy lesion classification. Combination of our techniques achieves a macro F1-score of 33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule endoscopy. To the best of our knowledge, this work is the first to apply a fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that an image-conditional GAN can be adapted effectively to limited datasets to generate high-quality examples, facilitating effective data augmentation. Additionally, we show that combining this GAN-based approach with classical image processing techniques further enhances the results.","sentences":["Limited medical imaging datasets challenge deep learning models by increasing risks of overfitting and reduced generalization, particularly in Generative Adversarial Networks (GANs), where discriminators may overfit, leading to training divergence.","This constraint also impairs classification models trained on small datasets.","Generative Data Augmentation (GDA) addresses this by expanding training datasets with synthetic data, although it requires training a generative model.","We propose and evaluate two local lesion generation approaches to address the challenge of augmenting small medical image datasets.","The first approach employs the Poisson Image Editing algorithm, a classical image processing technique, to create realistic image composites that outperform current state-of-the-art methods.","The second approach introduces a novel generative method, leveraging a fine-tuned Image Inpainting GAN to synthesize realistic lesions within specified regions of real training images.","A comprehensive comparison of the two proposed methods demonstrates that effective local lesion generation in a data-constrained setting allows for reaching new state-of-the-art results in capsule endoscopy lesion classification.","Combination of our techniques achieves a macro F1-score of 33.07%, surpassing the previous best result by 7.84 percentage points (p.p.)","on the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule endoscopy.","To the best of our knowledge, this work is the first to apply a fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that an image-conditional GAN can be adapted effectively to limited datasets to generate high-quality examples, facilitating effective data augmentation.","Additionally, we show that combining this GAN-based approach with classical image processing techniques further enhances the results."],"url":"http://arxiv.org/abs/2411.03098v1"}
{"created":"2024-11-05 13:32:28","title":"On Differentially Private Linear Algebra","abstract":"We introduce efficient differentially private (DP) algorithms for several linear algebraic tasks, including solving linear equalities over arbitrary fields, linear inequalities over the reals, and computing affine spans and convex hulls. As an application, we obtain efficient DP algorithms for learning halfspaces and affine subspaces. Our algorithms addressing equalities are strongly polynomial, whereas those addressing inequalities are weakly polynomial. Furthermore, this distinction is inevitable: no DP algorithm for linear programming can be strongly polynomial-time efficient.","sentences":["We introduce efficient differentially private (DP) algorithms for several linear algebraic tasks, including solving linear equalities over arbitrary fields, linear inequalities over the reals, and computing affine spans and convex hulls.","As an application, we obtain efficient DP algorithms for learning halfspaces and affine subspaces.","Our algorithms addressing equalities are strongly polynomial, whereas those addressing inequalities are weakly polynomial.","Furthermore, this distinction is inevitable: no DP algorithm for linear programming can be strongly polynomial-time efficient."],"url":"http://arxiv.org/abs/2411.03087v1"}
{"created":"2024-11-05 13:30:27","title":"Speech Separation with Pretrained Frontend to Minimize Domain Mismatch","abstract":"Speech separation seeks to separate individual speech signals from a speech mixture. Typically, most separation models are trained on synthetic data due to the unavailability of target reference in real-world cocktail party scenarios. As a result, there exists a domain gap between real and synthetic data when deploying speech separation models in real-world applications. In this paper, we propose a self-supervised domain-invariant pretrained (DIP) frontend that is exposed to mixture data without the need for target reference speech. The DIP frontend utilizes a Siamese network with two innovative pretext tasks, mixture predictive coding (MPC) and mixture invariant coding (MIC), to capture shared contextual cues between real and synthetic unlabeled mixtures. Subsequently, we freeze the DIP frontend as a feature extractor when training the downstream speech separation models on synthetic data. By pretraining the DIP frontend with the contextual cues, we expect that the speech separation skills learned from synthetic data can be effectively transferred to real data. To benefit from the DIP frontend, we introduce a novel separation pipeline to align the feature resolution of the separation models. We evaluate the speech separation quality on standard benchmarks and real-world datasets. The results confirm the superiority of our DIP frontend over existing speech separation models. This study underscores the potential of large-scale pretraining to enhance the quality and intelligibility of speech separation in real-world applications.","sentences":["Speech separation seeks to separate individual speech signals from a speech mixture.","Typically, most separation models are trained on synthetic data due to the unavailability of target reference in real-world cocktail party scenarios.","As a result, there exists a domain gap between real and synthetic data when deploying speech separation models in real-world applications.","In this paper, we propose a self-supervised domain-invariant pretrained (DIP) frontend that is exposed to mixture data without the need for target reference speech.","The DIP frontend utilizes a Siamese network with two innovative pretext tasks, mixture predictive coding (MPC) and mixture invariant coding (MIC), to capture shared contextual cues between real and synthetic unlabeled mixtures.","Subsequently, we freeze the DIP frontend as a feature extractor when training the downstream speech separation models on synthetic data.","By pretraining the DIP frontend with the contextual cues, we expect that the speech separation skills learned from synthetic data can be effectively transferred to real data.","To benefit from the DIP frontend, we introduce a novel separation pipeline to align the feature resolution of the separation models.","We evaluate the speech separation quality on standard benchmarks and real-world datasets.","The results confirm the superiority of our DIP frontend over existing speech separation models.","This study underscores the potential of large-scale pretraining to enhance the quality and intelligibility of speech separation in real-world applications."],"url":"http://arxiv.org/abs/2411.03085v1"}
{"created":"2024-11-05 13:26:31","title":"Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data","abstract":"This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated train-ng datasets. We propose a self-supervising teacher-student pipeline, in which a relatively simple teacher classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a student network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and teach 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process GP to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled datasets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.","sentences":["This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated train-ng datasets.","We propose a self-supervising teacher-student pipeline, in which a relatively simple teacher classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a student network based on a modified YOLOv3 architecture.","Firstly, 3D object detection with back projection is used to automatically extract and teach 2D detection and localisation information to the student network.","Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition.","Thirdly, we use a Gaussian Process GP to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization.","The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data.","Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications.","The end-to-end network is also capable of real-time processing, needed for robotics applications.","Our method can be applied to many important industrial tasks, where labelled datasets are typically unavailable.","In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes.","This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations."],"url":"http://arxiv.org/abs/2411.03082v1"}
{"created":"2024-11-05 13:24:56","title":"Utilizing Precise and Complete Code Context to Guide LLM in Automatic False Positive Mitigation","abstract":"Static Application Security Testing(SAST) tools are crucial for early bug detection and code quality but often generate false positives that slow development. Automating false positive mitigation is thus essential for advancing SAST tools. Past efforts use static/dynamic analysis or machine learning. The advent of Large Language Models, adept at understanding natural language and code, offers promising ways to improve the accuracy and usability of SAST tools. However, existing LLM-based methods need improvement in two key areas: first, extracted code snippets related to warnings are often cluttered with irrelevant control and data flows, reducing precision; second, critical code contexts are often missing, leading to incomplete representations that can mislead LLMs and cause inaccurate assessments. To ensure the use of precise and complete code context, thereby avoiding misguidance and enabling LLMs to reach accurate conclusions, we propose LLM4FPM. One of its core components is eCPG-Slicer, which builds an extended code property graph and extracts line-level, precise code context. Moreover, LLM4FPM incorporates FARF algorithm, which builds a file reference graph and then efficiently detects all files related to a warning in linear time, enabling eCPG-Slicer to gather complete code context across these files. We evaluate LLM4FPM on Juliet dataset, where it comprehensively outperforms the baseline, achieving an F1 score above 99% across various CWEs. LLM4FPM leverages a free, open-source model, avoiding costly alternatives and reducing inspection costs by up to $2758 per run on Juliet, with an average inspection time of 4.7 seconds per warning. Our work emphasizes the critical impact of precise and complete code context and highlights the potential of combining program analysis with LLMs, improving the quality and efficiency of software development.","sentences":["Static Application Security Testing(SAST) tools are crucial for early bug detection and code quality but often generate false positives that slow development.","Automating false positive mitigation is thus essential for advancing SAST tools.","Past efforts use static/dynamic analysis or machine learning.","The advent of Large Language Models, adept at understanding natural language and code, offers promising ways to improve the accuracy and usability of SAST tools.","However, existing LLM-based methods need improvement in two key areas: first, extracted code snippets related to warnings are often cluttered with irrelevant control and data flows, reducing precision; second, critical code contexts are often missing, leading to incomplete representations that can mislead LLMs and cause inaccurate assessments.","To ensure the use of precise and complete code context, thereby avoiding misguidance and enabling LLMs to reach accurate conclusions, we propose LLM4FPM.","One of its core components is eCPG-Slicer, which builds an extended code property graph and extracts line-level, precise code context.","Moreover, LLM4FPM incorporates FARF algorithm, which builds a file reference graph and then efficiently detects all files related to a warning in linear time, enabling eCPG-Slicer to gather complete code context across these files.","We evaluate LLM4FPM on Juliet dataset, where it comprehensively outperforms the baseline, achieving an F1 score above 99% across various CWEs.","LLM4FPM leverages a free, open-source model, avoiding costly alternatives and reducing inspection costs by up to $2758 per run on Juliet, with an average inspection time of 4.7 seconds per warning.","Our work emphasizes the critical impact of precise and complete code context and highlights the potential of combining program analysis with LLMs, improving the quality and efficiency of software development."],"url":"http://arxiv.org/abs/2411.03079v1"}
{"created":"2024-11-05 13:09:47","title":"Multi-dimensional Approximate Counting","abstract":"The celebrated Morris counter uses $\\log_2\\log_2 n + O(\\log_2 \\sigma^{-1})$ bits to count up to $n$ with a relative error $\\sigma$, where if $\\hat{\\lambda}$ is the estimate of the current count $\\lambda$, then $\\mathbb{E}|\\hat{\\lambda}-\\lambda|^2 <\\sigma^2\\lambda^2$. A natural generalization is \\emph{multi-dimensional} approximate counting. Let $d\\geq 1$ be the dimension. The count vector $x\\in \\mathbb{N}^d$ is incremented entry-wisely over a stream of coordinates $(w_1,\\ldots,w_n)\\in [d]^n$, where upon receiving $w_k\\in[d]$, $x_{w_k}\\gets x_{w_k}+1$. A \\emph{$d$-dimensional approximate counter} is required to count $d$ coordinates simultaneously and return an estimate $\\hat{x}$ of the count vector $x$. Aden-Ali, Han, Nelson, and Yu \\cite{aden2022amortized} showed that the trivial solution of using $d$ Morris counters that track $d$ coordinates separately is already optimal in space, \\emph{if each entry only allows error relative to itself}, i.e., $\\mathbb{E}|\\hat{x}_j-x_j|^2<\\sigma^2|x_j|^2$ for each $j\\in [d]$. However, for another natural error metric -- the \\emph{Euclidean mean squared error} $\\mathbb{E} |\\hat{x}-x|^2$ -- we show that using $d$ separate Morris counters is sub-optimal.   In this work, we present a simple and optimal $d$-dimensional counter with Euclidean relative error $\\sigma$, i.e., $\\mathbb{E} |\\hat{x}-x|^2 <\\sigma^2|x|^2$ where $|x|=\\sqrt{\\sum_{j=1}^d x_j^2}$, with a matching lower bound. The upper and lower bounds are proved with ideas that are strikingly simple. The upper bound is constructed with a certain variable-length integer encoding and the lower bound is derived from a straightforward volumetric estimation of sphere covering.","sentences":["The celebrated Morris counter uses $\\log_2\\log_2 n + O(\\log_2 \\sigma^{-1})$ bits to count up to $n$ with a relative error $\\sigma$, where if $\\hat{\\lambda}$ is the estimate of the current count $\\lambda$, then $\\mathbb{E}|\\hat{\\lambda}-\\lambda|^2 <\\sigma^2\\lambda^2$. A natural generalization is \\emph{multi-dimensional} approximate counting.","Let $d\\geq 1$ be the dimension.","The count vector $x\\in \\mathbb{N}^d$ is incremented entry-wisely over a stream of coordinates $(w_1,\\ldots,w_n)\\in","[d]^n$, where upon receiving $w_k\\in[d]$, $x_{w_k}\\gets x_{w_k}+1$.","A \\emph{$d$-dimensional approximate counter} is required to count $d$ coordinates simultaneously and return an estimate $\\hat{x}$ of the count vector $x$. Aden-Ali, Han, Nelson, and Yu \\cite{aden2022amortized} showed that the trivial solution of using $d$ Morris counters that track $d$ coordinates separately is already optimal in space, \\emph{if each entry only allows error relative to itself}, i.e., $\\mathbb{E}|\\hat{x}_j-x_j|^2<\\sigma^2|x_j|^2$ for each $j\\in [d]$. However, for another natural error metric -- the \\emph{Euclidean mean squared error} $\\mathbb{E} |\\hat{x}-x|^2$ -- we show that using $d$ separate Morris counters is sub-optimal.   ","In this work, we present a simple and optimal $d$-dimensional counter with Euclidean relative error $\\sigma$, i.e., $\\mathbb{E} |\\hat{x}-x|^2 <\\sigma^2|x|^2$ where $|x|=\\sqrt{\\sum_{j=1}^d x_j^2}$, with a matching lower bound.","The upper and lower bounds are proved with ideas that are strikingly simple.","The upper bound is constructed with a certain variable-length integer encoding and the lower bound is derived from a straightforward volumetric estimation of sphere covering."],"url":"http://arxiv.org/abs/2411.03071v1"}
{"created":"2024-11-05 13:04:05","title":"Alpha and Prejudice: Improving $\u03b1$-sized Worst-case Fairness via Intrinsic Reweighting","abstract":"Worst-case fairness with off-the-shelf demographics achieves group parity by maximizing the model utility of the worst-off group. Nevertheless, demographic information is often unavailable in practical scenarios, which impedes the use of such a direct max-min formulation. Recent advances have reframed this learning problem by introducing the lower bound of minimal partition ratio, denoted as $\\alpha$, as side information, referred to as ``$\\alpha$-sized worst-case fairness'' in this paper. We first justify the practical significance of this setting by presenting noteworthy evidence from the data privacy perspective, which has been overlooked by existing research. Without imposing specific requirements on loss functions, we propose reweighting the training samples based on their intrinsic importance to fairness. Given the global nature of the worst-case formulation, we further develop a stochastic learning scheme to simplify the training process without compromising model performance. Additionally, we address the issue of outliers and provide a robust variant to handle potential outliers during model training. Our theoretical analysis and experimental observations reveal the connections between the proposed approaches and existing ``fairness-through-reweighting'' studies, with extensive experimental results on fairness benchmarks demonstrating the superiority of our methods.","sentences":["Worst-case fairness with off-the-shelf demographics achieves group parity by maximizing the model utility of the worst-off group.","Nevertheless, demographic information is often unavailable in practical scenarios, which impedes the use of such a direct max-min formulation.","Recent advances have reframed this learning problem by introducing the lower bound of minimal partition ratio, denoted as $\\alpha$, as side information, referred to as ``$\\alpha$-sized worst-case fairness'' in this paper.","We first justify the practical significance of this setting by presenting noteworthy evidence from the data privacy perspective, which has been overlooked by existing research.","Without imposing specific requirements on loss functions, we propose reweighting the training samples based on their intrinsic importance to fairness.","Given the global nature of the worst-case formulation, we further develop a stochastic learning scheme to simplify the training process without compromising model performance.","Additionally, we address the issue of outliers and provide a robust variant to handle potential outliers during model training.","Our theoretical analysis and experimental observations reveal the connections between the proposed approaches and existing ``fairness-through-reweighting'' studies, with extensive experimental results on fairness benchmarks demonstrating the superiority of our methods."],"url":"http://arxiv.org/abs/2411.03068v1"}
{"created":"2024-11-05 12:47:30","title":"Enhancing DP-SGD through Non-monotonous Adaptive Scaling Gradient Weight","abstract":"In the domain of deep learning, the challenge of protecting sensitive data while maintaining model utility is significant. Traditional Differential Privacy (DP) techniques such as Differentially Private Stochastic Gradient Descent (DP-SGD) typically employ strategies like direct or per-sample adaptive gradient clipping. These methods, however, compromise model accuracy due to their critical influence on gradient handling, particularly neglecting the significant contribution of small gradients during later training stages. In this paper, we introduce an enhanced version of DP-SGD, named Differentially Private Per-sample Adaptive Scaling Clipping (DP-PSASC). This approach replaces traditional clipping with non-monotonous adaptive gradient scaling, which alleviates the need for intensive threshold setting and rectifies the disproportionate weighting of smaller gradients. Our contribution is twofold. First, we develop a novel gradient scaling technique that effectively assigns proper weights to gradients, particularly small ones, thus improving learning under differential privacy. Second, we integrate a momentum-based method into DP-PSASC to reduce bias from stochastic sampling, enhancing convergence rates. Our theoretical and empirical analyses confirm that DP-PSASC preserves privacy and delivers superior performance across diverse datasets, setting new standards for privacy-sensitive applications.","sentences":["In the domain of deep learning, the challenge of protecting sensitive data while maintaining model utility is significant.","Traditional Differential Privacy (DP) techniques such as Differentially Private Stochastic Gradient Descent (DP-SGD) typically employ strategies like direct or per-sample adaptive gradient clipping.","These methods, however, compromise model accuracy due to their critical influence on gradient handling, particularly neglecting the significant contribution of small gradients during later training stages.","In this paper, we introduce an enhanced version of DP-SGD, named Differentially Private Per-sample Adaptive Scaling Clipping (DP-PSASC).","This approach replaces traditional clipping with non-monotonous adaptive gradient scaling, which alleviates the need for intensive threshold setting and rectifies the disproportionate weighting of smaller gradients.","Our contribution is twofold.","First, we develop a novel gradient scaling technique that effectively assigns proper weights to gradients, particularly small ones, thus improving learning under differential privacy.","Second, we integrate a momentum-based method into DP-PSASC to reduce bias from stochastic sampling, enhancing convergence rates.","Our theoretical and empirical analyses confirm that DP-PSASC preserves privacy and delivers superior performance across diverse datasets, setting new standards for privacy-sensitive applications."],"url":"http://arxiv.org/abs/2411.03059v1"}
{"created":"2024-11-05 12:42:42","title":"ATM: Improving Model Merging by Alternating Tuning and Merging","abstract":"Model merging has recently emerged as a cost-efficient paradigm for multi-task learning. Among current approaches, task arithmetic stands out for its simplicity and effectiveness. In this paper, we motivate the effectiveness of task vectors by linking them to multi-task gradients. We show that in a single-epoch scenario, task vectors are mathematically equivalent to the gradients obtained via gradient descent in a multi-task setting, and still approximate these gradients in subsequent epochs. Furthermore, we show that task vectors perform optimally when equality is maintained, and their effectiveness is largely driven by the first epoch's gradient. Building on this insight, we propose viewing model merging as a single step in an iterative process that Alternates between Tuning and Merging (ATM). This method acts as a bridge between model merging and multi-task gradient descent, achieving state-of-the-art results with the same data and computational requirements. We extensively evaluate ATM across diverse settings, achieving up to 20% higher accuracy in computer vision and NLP tasks, compared to the best baselines.Finally, we provide both empirical and theoretical support for its effectiveness, demonstrating increased orthogonality between task vectors and proving that ATM minimizes an upper bound on the loss obtained by jointly finetuning all tasks.","sentences":["Model merging has recently emerged as a cost-efficient paradigm for multi-task learning.","Among current approaches, task arithmetic stands out for its simplicity and effectiveness.","In this paper, we motivate the effectiveness of task vectors by linking them to multi-task gradients.","We show that in a single-epoch scenario, task vectors are mathematically equivalent to the gradients obtained via gradient descent in a multi-task setting, and still approximate these gradients in subsequent epochs.","Furthermore, we show that task vectors perform optimally when equality is maintained, and their effectiveness is largely driven by the first epoch's gradient.","Building on this insight, we propose viewing model merging as a single step in an iterative process that Alternates between Tuning and Merging (ATM).","This method acts as a bridge between model merging and multi-task gradient descent, achieving state-of-the-art results with the same data and computational requirements.","We extensively evaluate ATM across diverse settings, achieving up to 20% higher accuracy in computer vision and NLP tasks, compared to the best baselines.","Finally, we provide both empirical and theoretical support for its effectiveness, demonstrating increased orthogonality between task vectors and proving that ATM minimizes an upper bound on the loss obtained by jointly finetuning all tasks."],"url":"http://arxiv.org/abs/2411.03055v1"}
{"created":"2024-11-05 12:31:20","title":"UNet: A Generic and Reliable Multi-UAV Communication and Networking Architecture for Heterogeneous Applications","abstract":"The rapid growth of UAV applications necessitates a robust communication and networking architecture capable of addressing the diverse requirements of various applications concurrently, rather than relying on application-specific solutions. This paper proposes a generic and reliable multi-UAV communication and networking architecture designed to support the varying demands of heterogeneous applications, including short-range and long-range communication, star and mesh topologies, different data rates, and multiple wireless standards. Our architecture accommodates both adhoc and infrastructure networks, ensuring seamless connectivity throughout the network. Additionally, we present the design of a multi-protocol UAV gateway that enables interoperability among various communication protocols. Furthermore, we introduce a data processing and service layer framework with a graphical user interface of a ground control station that facilitates remote control and monitoring from any location at any time. We practically implemented the proposed architecture and evaluated its performance using different metrics, demonstrating its effectiveness.","sentences":["The rapid growth of UAV applications necessitates a robust communication and networking architecture capable of addressing the diverse requirements of various applications concurrently, rather than relying on application-specific solutions.","This paper proposes a generic and reliable multi-UAV communication and networking architecture designed to support the varying demands of heterogeneous applications, including short-range and long-range communication, star and mesh topologies, different data rates, and multiple wireless standards.","Our architecture accommodates both adhoc and infrastructure networks, ensuring seamless connectivity throughout the network.","Additionally, we present the design of a multi-protocol UAV gateway that enables interoperability among various communication protocols.","Furthermore, we introduce a data processing and service layer framework with a graphical user interface of a ground control station that facilitates remote control and monitoring from any location at any time.","We practically implemented the proposed architecture and evaluated its performance using different metrics, demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2411.03048v1"}
{"created":"2024-11-05 12:30:07","title":"GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details","abstract":"Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches. Project page: https://garverselod.github.io/","sentences":["Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images.","However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses.","In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image.","Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data.","Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists.","In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model.","We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details.","This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space.","To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism.","We evaluate our method on a massive amount of in-the-wild images.","Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches.","Project page: https://garverselod.github.io/"],"url":"http://arxiv.org/abs/2411.03047v1"}
{"created":"2024-11-05 12:22:51","title":"Self-Compositional Data Augmentation for Scientific Keyphrase Generation","abstract":"State-of-the-art models for keyphrase generation require large amounts of training data to achieve good performance. However, obtaining keyphrase-labeled documents can be challenging and costly. To address this issue, we present a self-compositional data augmentation method. More specifically, we measure the relatedness of training documents based on their shared keyphrases, and combine similar documents to generate synthetic samples. The advantage of our method lies in its ability to create additional training samples that keep domain coherence, without relying on external data or resources. Our results on multiple datasets spanning three different domains, demonstrate that our method consistently improves keyphrase generation. A qualitative analysis of the generated keyphrases for the Computer Science domain confirms this improvement towards their representativity property.","sentences":["State-of-the-art models for keyphrase generation require large amounts of training data to achieve good performance.","However, obtaining keyphrase-labeled documents can be challenging and costly.","To address this issue, we present a self-compositional data augmentation method.","More specifically, we measure the relatedness of training documents based on their shared keyphrases, and combine similar documents to generate synthetic samples.","The advantage of our method lies in its ability to create additional training samples that keep domain coherence, without relying on external data or resources.","Our results on multiple datasets spanning three different domains, demonstrate that our method consistently improves keyphrase generation.","A qualitative analysis of the generated keyphrases for the Computer Science domain confirms this improvement towards their representativity property."],"url":"http://arxiv.org/abs/2411.03039v1"}
{"created":"2024-11-05 12:16:52","title":"Top-k Stabbing Interval Queries","abstract":"We investigate a weighted variant of the interval stabbing problem, where the goal is to design an efficient data structure for a given set $\\mathcal{I}$ of weighted intervals such that, for a query point $q$ and an integer $k>0$, we can report the $k$ intervals with largest weights among those stabbed by $q$. In this paper, we present a linear space solution with $O(\\log n + k)$ query time. Moreover, we also present another trade-off for the problem.","sentences":["We investigate a weighted variant of the interval stabbing problem, where the goal is to design an efficient data structure for a given set $\\mathcal{I}$ of weighted intervals such that, for a query point $q$ and an integer $k>0$, we can report the $k$ intervals with largest weights among those stabbed by $q$. In this paper, we present a linear space solution with $O(\\log n + k)$ query time.","Moreover, we also present another trade-off for the problem."],"url":"http://arxiv.org/abs/2411.03037v1"}
{"created":"2024-11-05 12:14:57","title":"HumanVLM: Foundation for Human-Scene Vision-Language Model","abstract":"Human-scene vision-language tasks are increasingly prevalent in diverse social applications, yet recent advancements predominantly rely on models specifically tailored to individual tasks. Emerging research indicates that large vision-language models (VLMs) can enhance performance across various downstream vision-language understanding tasks. However, general-domain models often underperform in specialized fields. This study introduces a domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model (HumanVLM), designed to provide a foundation for human-scene Vision-Language tasks. Specifically, (1) we create a large-scale human-scene multimodal image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate domain-specific alignment; (2) develop a captioning approach for human-centered images, capturing human faces, bodies, and backgrounds, and construct a high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs) that contain as much detailed information as possible about human; (3) Using HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments, we then evaluate our HumanVLM across varous downstream tasks, where it demonstrates superior overall performance among multimodal models of comparable scale, particularly excelling in human-related tasks and significantly outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM, alongside the data introduced, will stimulate the research in human-around fields.","sentences":["Human-scene vision-language tasks are increasingly prevalent in diverse social applications, yet recent advancements predominantly rely on models specifically tailored to individual tasks.","Emerging research indicates that large vision-language models (VLMs) can enhance performance across various downstream vision-language understanding tasks.","However, general-domain models often underperform in specialized fields.","This study introduces a domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model (HumanVLM), designed to provide a foundation for human-scene Vision-Language tasks.","Specifically, (1) we create a large-scale human-scene multimodal image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate domain-specific alignment; (2) develop a captioning approach for human-centered images, capturing human faces, bodies, and backgrounds, and construct a high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs) that contain as much detailed information as possible about human; (3) Using HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM.","In the experiments, we then evaluate our HumanVLM across varous downstream tasks, where it demonstrates superior overall performance among multimodal models of comparable scale, particularly excelling in human-related tasks and significantly outperforming similar models, including Qwen2VL and ChatGPT-4o.","HumanVLM, alongside the data introduced, will stimulate the research in human-around fields."],"url":"http://arxiv.org/abs/2411.03034v1"}
{"created":"2024-11-05 11:46:27","title":"DA-MoE: Addressing Depth-Sensitivity in Graph-Level Analysis through Mixture of Experts","abstract":"Graph neural networks (GNNs) are gaining popularity for processing graph-structured data. In real-world scenarios, graph data within the same dataset can vary significantly in scale. This variability leads to depth-sensitivity, where the optimal depth of GNN layers depends on the scale of the graph data. Empirically, fewer layers are sufficient for message passing in smaller graphs, while larger graphs typically require deeper networks to capture long-range dependencies and global features. However, existing methods generally use a fixed number of GNN layers to generate representations for all graphs, overlooking the depth-sensitivity issue in graph structure data. To address this challenge, we propose the depth adaptive mixture of expert (DA-MoE) method, which incorporates two main improvements to GNN backbone: \\textbf{1)} DA-MoE employs different GNN layers, each considered an expert with its own parameters. Such a design allows the model to flexibly aggregate information at different scales, effectively addressing the depth-sensitivity issue in graph data. \\textbf{2)} DA-MoE utilizes GNN to capture the structural information instead of the linear projections in the gating network. Thus, the gating network enables the model to capture complex patterns and dependencies within the data. By leveraging these improvements, each expert in DA-MoE specifically learns distinct graph patterns at different scales. Furthermore, comprehensive experiments on the TU dataset and open graph benchmark (OGB) have shown that DA-MoE consistently surpasses existing baselines on various tasks, including graph, node, and link-level analyses. The code are available at \\url{https://github.com/Celin-Yao/DA-MoE}.","sentences":["Graph neural networks (GNNs) are gaining popularity for processing graph-structured data.","In real-world scenarios, graph data within the same dataset can vary significantly in scale.","This variability leads to depth-sensitivity, where the optimal depth of GNN layers depends on the scale of the graph data.","Empirically, fewer layers are sufficient for message passing in smaller graphs, while larger graphs typically require deeper networks to capture long-range dependencies and global features.","However, existing methods generally use a fixed number of GNN layers to generate representations for all graphs, overlooking the depth-sensitivity issue in graph structure data.","To address this challenge, we propose the depth adaptive mixture of expert (DA-MoE) method, which incorporates two main improvements to GNN backbone: \\textbf{1)} DA-MoE employs different GNN layers, each considered an expert with its own parameters.","Such a design allows the model to flexibly aggregate information at different scales, effectively addressing the depth-sensitivity issue in graph data.","\\textbf{2)} DA-MoE utilizes GNN to capture the structural information instead of the linear projections in the gating network.","Thus, the gating network enables the model to capture complex patterns and dependencies within the data.","By leveraging these improvements, each expert in DA-MoE specifically learns distinct graph patterns at different scales.","Furthermore, comprehensive experiments on the TU dataset and open graph benchmark (OGB) have shown that DA-MoE consistently surpasses existing baselines on various tasks, including graph, node, and link-level analyses.","The code are available at \\url{https://github.com/Celin-Yao/DA-MoE}."],"url":"http://arxiv.org/abs/2411.03025v1"}
{"created":"2024-11-05 11:44:00","title":"Testing Generalizability in Causal Inference","abstract":"Ensuring robust model performance across diverse real-world scenarios requires addressing both transportability across domains with covariate shifts and extrapolation beyond observed data ranges. However, there is no formal procedure for statistically evaluating generalizability in machine learning algorithms, particularly in causal inference. Existing methods often rely on arbitrary metrics like AUC or MSE and focus predominantly on toy datasets, providing limited insights into real-world applicability. To address this gap, we propose a systematic and quantitative framework for evaluating model generalizability under covariate distribution shifts, specifically within causal inference settings. Our approach leverages the frugal parameterization, allowing for flexible simulations from fully and semi-synthetic benchmarks, offering comprehensive evaluations for both mean and distributional regression methods. By basing simulations on real data, our method ensures more realistic evaluations, which is often missing in current work relying on simplified datasets. Furthermore, using simulations and statistical testing, our framework is robust and avoids over-reliance on conventional metrics. Grounded in real-world data, it provides realistic insights into model performance, bridging the gap between synthetic evaluations and practical applications.","sentences":["Ensuring robust model performance across diverse real-world scenarios requires addressing both transportability across domains with covariate shifts and extrapolation beyond observed data ranges.","However, there is no formal procedure for statistically evaluating generalizability in machine learning algorithms, particularly in causal inference.","Existing methods often rely on arbitrary metrics like AUC or MSE and focus predominantly on toy datasets, providing limited insights into real-world applicability.","To address this gap, we propose a systematic and quantitative framework for evaluating model generalizability under covariate distribution shifts, specifically within causal inference settings.","Our approach leverages the frugal parameterization, allowing for flexible simulations from fully and semi-synthetic benchmarks, offering comprehensive evaluations for both mean and distributional regression methods.","By basing simulations on real data, our method ensures more realistic evaluations, which is often missing in current work relying on simplified datasets.","Furthermore, using simulations and statistical testing, our framework is robust and avoids over-reliance on conventional metrics.","Grounded in real-world data, it provides realistic insights into model performance, bridging the gap between synthetic evaluations and practical applications."],"url":"http://arxiv.org/abs/2411.03021v1"}
{"created":"2024-11-05 11:42:26","title":"FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses","abstract":"Federated Learning is a privacy preserving decentralized machine learning paradigm designed to collaboratively train models across multiple clients by exchanging gradients to the server and keeping private data local. Nevertheless, recent research has revealed that the security of Federated Learning is compromised, as private ground truth data can be recovered through a gradient inversion technique known as Deep Leakage. While these attacks are crafted with a focus on applications in Federated Learning, they generally are not evaluated in realistic scenarios. This paper introduces the FEDLAD Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), a comprehensive benchmark for evaluating Deep Leakage attacks and defenses within a realistic Federated context. By implementing a unified benchmark that encompasses multiple state-of-the-art Deep Leakage techniques and various defense strategies, our framework facilitates the evaluation and comparison of the efficacy of these methods across different datasets and training states. This work highlights a crucial trade-off between privacy and model accuracy in Federated Learning and aims to advance the understanding of security challenges in decentralized machine learning systems, stimulate future research, and enhance reproducibility in evaluating Deep Leakage attacks and defenses.","sentences":["Federated Learning is a privacy preserving decentralized machine learning paradigm designed to collaboratively train models across multiple clients by exchanging gradients to the server and keeping private data local.","Nevertheless, recent research has revealed that the security of Federated Learning is compromised, as private ground truth data can be recovered through a gradient inversion technique known as Deep Leakage.","While these attacks are crafted with a focus on applications in Federated Learning, they generally are not evaluated in realistic scenarios.","This paper introduces the FEDLAD Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), a comprehensive benchmark for evaluating Deep Leakage attacks and defenses within a realistic Federated context.","By implementing a unified benchmark that encompasses multiple state-of-the-art Deep Leakage techniques and various defense strategies, our framework facilitates the evaluation and comparison of the efficacy of these methods across different datasets and training states.","This work highlights a crucial trade-off between privacy and model accuracy in Federated Learning and aims to advance the understanding of security challenges in decentralized machine learning systems, stimulate future research, and enhance reproducibility in evaluating Deep Leakage attacks and defenses."],"url":"http://arxiv.org/abs/2411.03019v1"}
{"created":"2024-11-05 11:41:00","title":"Rozproszone Wykrywanie Zaj\u0119to\u015bci Widma Oparte na Uczeniu Federacyjnym","abstract":"Spectrum occupancy detection is a key enabler for dynamic spectrum access, where machine learning algorithms are successfully utilized for detection improvement. However, the main challenge is limited access to labeled data about users transmission presence needed in supervised learning models. We present a distributed federated learning approach that addresses this challenge for sensors without access to learning data. The paper discusses the results of the conducted hardware experiment, where FL has been applied for DVB-T signal detection.","sentences":["Spectrum occupancy detection is a key enabler for dynamic spectrum access, where machine learning algorithms are successfully utilized for detection improvement.","However, the main challenge is limited access to labeled data about users transmission presence needed in supervised learning models.","We present a distributed federated learning approach that addresses this challenge for sensors without access to learning data.","The paper discusses the results of the conducted hardware experiment, where FL has been applied for DVB-T signal detection."],"url":"http://arxiv.org/abs/2411.03017v1"}
{"created":"2024-11-05 11:28:56","title":"Constitutive Models for Active Skeletal Muscle: Review, Comparison, and Application in a Novel Continuum Shoulder Model","abstract":"The shoulder joint is one of the functionally and anatomically most sophisticated articular systems in the human body. Both complex movement patterns and the stabilization of the highly mobile joint rely on intricate three-dimensional interactions among various components. Continuum-based finite element models can capture such complexity, and are thus particularly relevant in shoulder biomechanics. Considering their role as active joint stabilizers and force generators, skeletal muscles require special attention regarding their constitutive description. In this contribution, we propose a constitutive description to model active skeletal muscle within complex musculoskeletal systems, focusing on a novel continuum shoulder model. We thoroughly review existing material models before analyzing three selected ones in detail: an active-stress, an active-strain, and a generalized active-strain approach. To establish a basis for comparison, we identify the material parameters based on experimental stress-strain data obtained under various active and passive loading conditions. We discuss the concepts to incorporate active contractile behavior from a mathematical and physiological perspective, address analytical and numerical challenges arising from the mathematical formulations, and analyze the included biophysical principles of force generation in terms of physiological correctness and relevance for human shoulder modeling. Based on these insights, we present an improved constitutive model combining the studied models' most promising and relevant features. Using the example of a fusiform muscle, we investigate force generation, deformation, and kinematics during active isometric and free contractions. Eventually, we demonstrate the applicability of the suggested material model in a novel continuum mechanical model of the human shoulder.","sentences":["The shoulder joint is one of the functionally and anatomically most sophisticated articular systems in the human body.","Both complex movement patterns and the stabilization of the highly mobile joint rely on intricate three-dimensional interactions among various components.","Continuum-based finite element models can capture such complexity, and are thus particularly relevant in shoulder biomechanics.","Considering their role as active joint stabilizers and force generators, skeletal muscles require special attention regarding their constitutive description.","In this contribution, we propose a constitutive description to model active skeletal muscle within complex musculoskeletal systems, focusing on a novel continuum shoulder model.","We thoroughly review existing material models before analyzing three selected ones in detail: an active-stress, an active-strain, and a generalized active-strain approach.","To establish a basis for comparison, we identify the material parameters based on experimental stress-strain data obtained under various active and passive loading conditions.","We discuss the concepts to incorporate active contractile behavior from a mathematical and physiological perspective, address analytical and numerical challenges arising from the mathematical formulations, and analyze the included biophysical principles of force generation in terms of physiological correctness and relevance for human shoulder modeling.","Based on these insights, we present an improved constitutive model combining the studied models' most promising and relevant features.","Using the example of a fusiform muscle, we investigate force generation, deformation, and kinematics during active isometric and free contractions.","Eventually, we demonstrate the applicability of the suggested material model in a novel continuum mechanical model of the human shoulder."],"url":"http://arxiv.org/abs/2411.03015v1"}
{"created":"2024-11-05 11:25:12","title":"Leveraging Large Language Models in Code Question Answering: Baselines and Issues","abstract":"Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.","sentences":["Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product.","This paper presents a work devoted to using large language models for question answering over source code in Python.","The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code.","To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries.","The model answers were also analyzed for errors manually.","We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis.","The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets.","In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values.","The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions.","The training and evaluation code is publicly available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering."],"url":"http://arxiv.org/abs/2411.03012v1"}
{"created":"2024-11-05 11:19:30","title":"Set-Membership Estimation for Fault Diagnosis of Nonlinear Systems","abstract":"This paper introduces a Fault Diagnosis (Detection, Isolation, and Estimation) method using Set-Membership Estimation (SME) designed for a class of nonlinear systems that are linear to the fault parameters. The methodology advances fault diagnosis by continuously evaluating an estimate of the fault parameter and a feasible parameter set where the true fault parameter belongs. Unlike previous SME approaches, in this work, we address nonlinear systems subjected to both input and output uncertainties by utilizing inclusion functions and interval arithmetic. Additionally, we present an approach to outer-approximate the polytopic description of the feasible parameter set by effectively balancing approximation accuracy with computational efficiency resulting in improved fault detectability. Lastly, we introduce adaptive regularization of the parameter estimates to enhance the estimation process when the input-output data are sparse or non-informative, enhancing fault identifiability. We demonstrate the effectiveness of this method in simulations involving an Autonomous Surface Vehicle in both a path-following and a realistic collision avoidance scenario, underscoring its potential to enhance safety and reliability in critical applications.","sentences":["This paper introduces a Fault Diagnosis (Detection, Isolation, and Estimation) method using Set-Membership Estimation (SME) designed for a class of nonlinear systems that are linear to the fault parameters.","The methodology advances fault diagnosis by continuously evaluating an estimate of the fault parameter and a feasible parameter set where the true fault parameter belongs.","Unlike previous SME approaches, in this work, we address nonlinear systems subjected to both input and output uncertainties by utilizing inclusion functions and interval arithmetic.","Additionally, we present an approach to outer-approximate the polytopic description of the feasible parameter set by effectively balancing approximation accuracy with computational efficiency resulting in improved fault detectability.","Lastly, we introduce adaptive regularization of the parameter estimates to enhance the estimation process when the input-output data are sparse or non-informative, enhancing fault identifiability.","We demonstrate the effectiveness of this method in simulations involving an Autonomous Surface Vehicle in both a path-following and a realistic collision avoidance scenario, underscoring its potential to enhance safety and reliability in critical applications."],"url":"http://arxiv.org/abs/2411.03011v1"}
{"created":"2024-11-05 11:18:43","title":"Learning-based Lossless Event Data Compression","abstract":"Emerging event cameras acquire visual information by detecting time domain brightness changes asynchronously at the pixel level and, unlike conventional cameras, are able to provide high temporal resolution, very high dynamic range, low latency, and low power consumption. Considering the huge amount of data involved, efficient compression solutions are very much needed. In this context, this paper presents a novel deep-learning-based lossless event data compression scheme based on octree partitioning and a learned hyperprior model. The proposed method arranges the event stream as a 3D volume and employs an octree structure for adaptive partitioning. A deep neural network-based entropy model, using a hyperprior, is then applied. Experimental results demonstrate that the proposed method outperforms traditional lossless data compression techniques in terms of compression ratio and bits per event.","sentences":["Emerging event cameras acquire visual information by detecting time domain brightness changes asynchronously at the pixel level and, unlike conventional cameras, are able to provide high temporal resolution, very high dynamic range, low latency, and low power consumption.","Considering the huge amount of data involved, efficient compression solutions are very much needed.","In this context, this paper presents a novel deep-learning-based lossless event data compression scheme based on octree partitioning and a learned hyperprior model.","The proposed method arranges the event stream as a 3D volume and employs an octree structure for adaptive partitioning.","A deep neural network-based entropy model, using a hyperprior, is then applied.","Experimental results demonstrate that the proposed method outperforms traditional lossless data compression techniques in terms of compression ratio and bits per event."],"url":"http://arxiv.org/abs/2411.03010v1"}
{"created":"2024-11-05 11:12:25","title":"Data Quality Awareness: A Journey from Traditional Data Management to Data Science Systems","abstract":"Artificial intelligence (AI) has transformed various fields, significantly impacting our daily lives. A major factor in AI success is high-quality data. In this paper, we present a comprehensive review of the evolution of data quality (DQ) awareness from traditional data management systems to modern data-driven AI systems, which are integral to data science. We synthesize the existing literature, highlighting the quality challenges and techniques that have evolved from traditional data management to data science including big data and ML fields. As data science systems support a wide range of activities, our focus in this paper lies specifically in the analytics aspect driven by machine learning. We use the cause-effect connection between the quality challenges of ML and those of big data to allow a more thorough understanding of emerging DQ challenges and the related quality awareness techniques in data science systems. To the best of our knowledge, our paper is the first to provide a review of DQ awareness spanning traditional and emergent data science systems. We hope that readers will find this journey through the evolution of data quality awareness insightful and valuable.","sentences":["Artificial intelligence (AI) has transformed various fields, significantly impacting our daily lives.","A major factor in AI success is high-quality data.","In this paper, we present a comprehensive review of the evolution of data quality (DQ) awareness from traditional data management systems to modern data-driven AI systems, which are integral to data science.","We synthesize the existing literature, highlighting the quality challenges and techniques that have evolved from traditional data management to data science including big data and ML fields.","As data science systems support a wide range of activities, our focus in this paper lies specifically in the analytics aspect driven by machine learning.","We use the cause-effect connection between the quality challenges of ML and those of big data to allow a more thorough understanding of emerging DQ challenges and the related quality awareness techniques in data science systems.","To the best of our knowledge, our paper is the first to provide a review of DQ awareness spanning traditional and emergent data science systems.","We hope that readers will find this journey through the evolution of data quality awareness insightful and valuable."],"url":"http://arxiv.org/abs/2411.03007v1"}
{"created":"2024-11-05 11:05:53","title":"Controlling for Unobserved Confounding with Large Language Model Classification of Patient Smoking Status","abstract":"Causal understanding is a fundamental goal of evidence-based medicine. When randomization is impossible, causal inference methods allow the estimation of treatment effects from retrospective analysis of observational data. However, such analyses rely on a number of assumptions, often including that of no unobserved confounding. In many practical settings, this assumption is violated when important variables are not explicitly measured in the clinical record. Prior work has proposed to address unobserved confounding with machine learning by imputing unobserved variables and then correcting for the classifier's mismeasurement. When such a classifier can be trained and the necessary assumptions are met, this method can recover an unbiased estimate of a causal effect. However, such work has been limited to synthetic data, simple classifiers, and binary variables. This paper extends this methodology by using a large language model trained on clinical notes to predict patients' smoking status, which would otherwise be an unobserved confounder. We then apply a measurement error correction on the categorical predicted smoking status to estimate the causal effect of transthoracic echocardiography on mortality in the MIMIC dataset.","sentences":["Causal understanding is a fundamental goal of evidence-based medicine.","When randomization is impossible, causal inference methods allow the estimation of treatment effects from retrospective analysis of observational data.","However, such analyses rely on a number of assumptions, often including that of no unobserved confounding.","In many practical settings, this assumption is violated when important variables are not explicitly measured in the clinical record.","Prior work has proposed to address unobserved confounding with machine learning by imputing unobserved variables and then correcting for the classifier's mismeasurement.","When such a classifier can be trained and the necessary assumptions are met, this method can recover an unbiased estimate of a causal effect.","However, such work has been limited to synthetic data, simple classifiers, and binary variables.","This paper extends this methodology by using a large language model trained on clinical notes to predict patients' smoking status, which would otherwise be an unobserved confounder.","We then apply a measurement error correction on the categorical predicted smoking status to estimate the causal effect of transthoracic echocardiography on mortality in the MIMIC dataset."],"url":"http://arxiv.org/abs/2411.03004v1"}
{"created":"2024-11-05 11:00:55","title":"Precise Drive with VLM: First Prize Solution for PRCV 2024 Drive LM challenge","abstract":"This technical report outlines the methodologies we applied for the PRCV Challenge, focusing on cognition and decision-making in driving scenarios. We employed InternVL-2.0, a pioneering open-source multi-modal model, and enhanced it by refining both the model input and training methodologies. For the input data, we strategically concatenated and formatted the multi-view images. It is worth mentioning that we utilized the coordinates of the original images without transformation. In terms of model training, we initially pre-trained the model on publicly available autonomous driving scenario datasets to bolster its alignment capabilities of the challenge tasks, followed by fine-tuning on the DriveLM-nuscenes Dataset. During the fine-tuning phase, we innovatively modified the loss function to enhance the model's precision in predicting coordinate values. These approaches ensure that our model possesses advanced cognitive and decision-making capabilities in driving scenarios. Consequently, our model achieved a score of 0.6064, securing the first prize on the competition's final results.","sentences":["This technical report outlines the methodologies we applied for the PRCV Challenge, focusing on cognition and decision-making in driving scenarios.","We employed InternVL-2.0, a pioneering open-source multi-modal model, and enhanced it by refining both the model input and training methodologies.","For the input data, we strategically concatenated and formatted the multi-view images.","It is worth mentioning that we utilized the coordinates of the original images without transformation.","In terms of model training, we initially pre-trained the model on publicly available autonomous driving scenario datasets to bolster its alignment capabilities of the challenge tasks, followed by fine-tuning on the DriveLM-nuscenes Dataset.","During the fine-tuning phase, we innovatively modified the loss function to enhance the model's precision in predicting coordinate values.","These approaches ensure that our model possesses advanced cognitive and decision-making capabilities in driving scenarios.","Consequently, our model achieved a score of 0.6064, securing the first prize on the competition's final results."],"url":"http://arxiv.org/abs/2411.02999v1"}
{"created":"2024-11-05 10:58:37","title":"PV-faultNet: Optimized CNN Architecture to detect defects resulting efficient PV production","abstract":"The global shift towards renewable energy has pushed PV cell manufacturing as a pivotal point as they are the fundamental building block of green energy. However, the manufacturing process is complex enough to lose its purpose due to probable defects experienced during the time impacting the overall efficiency. However, at the moment, manual inspection is being conducted to detect the defects that can cause bias, leading to time and cost inefficiency. Even if automated solutions have also been proposed, most of them are resource-intensive, proving ineffective in production environments. In that context, this study presents PV-faultNet, a lightweight Convolutional Neural Network (CNN) architecture optimized for efficient and real-time defect detection in photovoltaic (PV) cells, designed to be deployable on resource-limited production devices. Addressing computational challenges in industrial PV manufacturing environments, the model includes only 2.92 million parameters, significantly reducing processing demands without sacrificing accuracy. Comprehensive data augmentation techniques were implemented to tackle data scarcity, thus enhancing model generalization and maintaining a balance between precision and recall. The proposed model achieved high performance with 91\\% precision, 89\\% recall, and a 90\\% F1 score, demonstrating its effectiveness for scalable quality control in PV production.","sentences":["The global shift towards renewable energy has pushed PV cell manufacturing as a pivotal point as they are the fundamental building block of green energy.","However, the manufacturing process is complex enough to lose its purpose due to probable defects experienced during the time impacting the overall efficiency.","However, at the moment, manual inspection is being conducted to detect the defects that can cause bias, leading to time and cost inefficiency.","Even if automated solutions have also been proposed, most of them are resource-intensive, proving ineffective in production environments.","In that context, this study presents PV-faultNet, a lightweight Convolutional Neural Network (CNN) architecture optimized for efficient and real-time defect detection in photovoltaic (PV) cells, designed to be deployable on resource-limited production devices.","Addressing computational challenges in industrial PV manufacturing environments, the model includes only 2.92 million parameters, significantly reducing processing demands without sacrificing accuracy.","Comprehensive data augmentation techniques were implemented to tackle data scarcity, thus enhancing model generalization and maintaining a balance between precision and recall.","The proposed model achieved high performance with 91\\% precision, 89\\% recall, and a 90\\% F1 score, demonstrating its effectiveness for scalable quality control in PV production."],"url":"http://arxiv.org/abs/2411.02997v1"}
{"created":"2024-11-05 10:55:29","title":"SUDS: A Strategy for Unsupervised Drift Sampling","abstract":"Supervised machine learning often encounters concept drift, where the data distribution changes over time, degrading model performance. Existing drift detection methods focus on identifying these shifts but often overlook the challenge of acquiring labeled data for model retraining after a shift occurs. We present the Strategy for Drift Sampling (SUDS), a novel method that selects homogeneous samples for retraining using existing drift detection algorithms, thereby enhancing model adaptability to evolving data. SUDS seamlessly integrates with current drift detection techniques. We also introduce the Harmonized Annotated Data Accuracy Metric (HADAM), a metric that evaluates classifier performance in relation to the quantity of annotated data required to achieve the stated performance, thereby taking into account the difficulty of acquiring labeled data. Our contributions are twofold: SUDS combines drift detection with strategic sampling to improve the retraining process, and HADAM provides a metric that balances classifier performance with the amount of labeled data, ensuring efficient resource utilization. Empirical results demonstrate the efficacy of SUDS in optimizing labeled data use in dynamic environments, significantly improving the performance of machine learning applications in real-world scenarios. Our code is open source and available at https://github.com/cfellicious/SUDS/","sentences":["Supervised machine learning often encounters concept drift, where the data distribution changes over time, degrading model performance.","Existing drift detection methods focus on identifying these shifts but often overlook the challenge of acquiring labeled data for model retraining after a shift occurs.","We present the Strategy for Drift Sampling (SUDS), a novel method that selects homogeneous samples for retraining using existing drift detection algorithms, thereby enhancing model adaptability to evolving data.","SUDS seamlessly integrates with current drift detection techniques.","We also introduce the Harmonized Annotated Data Accuracy Metric (HADAM), a metric that evaluates classifier performance in relation to the quantity of annotated data required to achieve the stated performance, thereby taking into account the difficulty of acquiring labeled data.","Our contributions are twofold: SUDS combines drift detection with strategic sampling to improve the retraining process, and HADAM provides a metric that balances classifier performance with the amount of labeled data, ensuring efficient resource utilization.","Empirical results demonstrate the efficacy of SUDS in optimizing labeled data use in dynamic environments, significantly improving the performance of machine learning applications in real-world scenarios.","Our code is open source and available at https://github.com/cfellicious/SUDS/"],"url":"http://arxiv.org/abs/2411.02995v1"}
{"created":"2024-11-05 10:42:09","title":"Counting random $k$-SAT near the satisfiability threshold","abstract":"We present efficient counting and sampling algorithms for random $k$-SAT when the clause density satisfies $\\alpha \\le \\frac{2^k}{\\mathrm{poly}(k)}.$ In particular, the exponential term $2^k$ matches the satisfiability threshold $\\Theta(2^k)$ for the existence of a solution and the (conjectured) algorithmic threshold $2^k (\\ln k) / k$ for efficiently finding a solution. Previously, the best-known counting and sampling algorithms required far more restricted densities $\\alpha\\lesssim 2^{k/3}$ [He, Wu, Yang, SODA '23]. Notably, our result goes beyond the lower bound $d\\gtrsim 2^{k/2}$ for worst-case $k$-SAT with bounded-degree $d$ [Bez\\'akov\\'a et al, SICOMP '19], showing that for counting and sampling, the average-case random $k$-SAT model is computationally much easier than the worst-case model.   At the heart of our approach is a new refined analysis of the recent novel coupling procedure by [Wang, Yin, FOCS '24], utilizing the structural properties of random constraint satisfaction problems (CSPs). Crucially, our analysis avoids reliance on the $2$-tree structure used in prior works, which cannot extend beyond the worst-case threshold $2^{k/2}$. Instead, we employ a witness tree similar to that used in the analysis of the Moser-Tardos algorithm [Moser, Tardos, JACM '10] for the Lov\\'{a}sz Local lemma, which may be of independent interest. Our new analysis provides a universal framework for efficient counting and sampling for random atomic CSPs, including, for example, random hypergraph colorings. At the same time, it immediately implies as corollaries several structural and probabilistic properties of random CSPs that have been widely studied but rarely justified, including replica symmetry and non-reconstruction.","sentences":["We present efficient counting and sampling algorithms for random $k$-SAT when the clause density satisfies $\\alpha \\le \\frac{2^k}{\\mathrm{poly}(k)}.$","In particular, the exponential term $2^k$ matches the satisfiability threshold $\\Theta(2^k)$ for the existence of a solution and the (conjectured) algorithmic threshold $2^k (\\ln k) / k$ for efficiently finding a solution.","Previously, the best-known counting and sampling algorithms required far more restricted densities $\\alpha\\lesssim 2^{k/3}$","[He, Wu, Yang, SODA '23].","Notably, our result goes beyond the lower bound $d\\gtrsim 2^{k/2}$ for worst-case $k$-SAT with bounded-degree $d$ [Bez\\'akov\\'a et al, SICOMP '19], showing that for counting and sampling, the average-case random $k$-SAT model is computationally much easier than the worst-case model.   ","At the heart of our approach is a new refined analysis of the recent novel coupling procedure by [Wang, Yin, FOCS '24], utilizing the structural properties of random constraint satisfaction problems (CSPs).","Crucially, our analysis avoids reliance on the $2$-tree structure used in prior works, which cannot extend beyond the worst-case threshold $2^{k/2}$. Instead, we employ a witness tree similar to that used in the analysis of the Moser-Tardos algorithm","[Moser, Tardos, JACM '10] for the Lov\\'{a}sz Local lemma, which may be of independent interest.","Our new analysis provides a universal framework for efficient counting and sampling for random atomic CSPs, including, for example, random hypergraph colorings.","At the same time, it immediately implies as corollaries several structural and probabilistic properties of random CSPs that have been widely studied but rarely justified, including replica symmetry and non-reconstruction."],"url":"http://arxiv.org/abs/2411.02980v1"}
{"created":"2024-11-05 10:18:53","title":"[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI","abstract":"We present an outline of the first large language model (LLM) based chatbot application in the context of patient-reported outcome measures (PROMs) for diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable patients to provide feedback about their quality of life and treatment progress via an interactive application. The proposed framework offers significant advantages over the current approach, which encompasses only qualitative collection of survey data or a static survey with limited answer options. Using the PROBot LLM-PROM application, patients will be asked tailored questions about their individual challenges, and can give more detailed feedback on the progress of their treatment. Based on this input, we will use machine learning to infer conventional PROM scores, which can be used by clinicians to evaluate the treatment status. The goal of the application is to improve adherence to the healthcare system and treatments, and thus ultimately reduce cases of subsequent vision impairment. The approach needs to be further validated using a survey and a clinical study.","sentences":["We present an outline of the first large language model (LLM) based chatbot application in the context of patient-reported outcome measures (PROMs) for diabetic retinopathy.","By utilizing the capabilities of current LLMs, we enable patients to provide feedback about their quality of life and treatment progress via an interactive application.","The proposed framework offers significant advantages over the current approach, which encompasses only qualitative collection of survey data or a static survey with limited answer options.","Using the PROBot LLM-PROM application, patients will be asked tailored questions about their individual challenges, and can give more detailed feedback on the progress of their treatment.","Based on this input, we will use machine learning to infer conventional PROM scores, which can be used by clinicians to evaluate the treatment status.","The goal of the application is to improve adherence to the healthcare system and treatments, and thus ultimately reduce cases of subsequent vision impairment.","The approach needs to be further validated using a survey and a clinical study."],"url":"http://arxiv.org/abs/2411.02973v1"}
{"created":"2024-11-05 10:16:14","title":"Exploring Seasonal Variability in the Context of Neural Radiance Fields for 3D Reconstruction on Satellite Imagery","abstract":"In this work, the seasonal predictive capabilities of Neural Radiance Fields (NeRF) applied to satellite images are investigated. Focusing on the utilization of satellite data, the study explores how Sat-NeRF, a novel approach in computer vision, performs in predicting seasonal variations across different months. Through comprehensive analysis and visualization, the study examines the model's ability to capture and predict seasonal changes, highlighting specific challenges and strengths. Results showcase the impact of the sun direction on predictions, revealing nuanced details in seasonal transitions, such as snow cover, color accuracy, and texture representation in different landscapes. Given these results, we propose Planet-NeRF, an extension to Sat-NeRF capable of incorporating seasonal variability through a set of month embedding vectors. Comparative evaluations reveal that Planet-NeRF outperforms prior models in the case where seasonal changes are present. The extensive evaluation combined with the proposed method offers promising avenues for future research in this domain.","sentences":["In this work, the seasonal predictive capabilities of Neural Radiance Fields (NeRF) applied to satellite images are investigated.","Focusing on the utilization of satellite data, the study explores how Sat-NeRF, a novel approach in computer vision, performs in predicting seasonal variations across different months.","Through comprehensive analysis and visualization, the study examines the model's ability to capture and predict seasonal changes, highlighting specific challenges and strengths.","Results showcase the impact of the sun direction on predictions, revealing nuanced details in seasonal transitions, such as snow cover, color accuracy, and texture representation in different landscapes.","Given these results, we propose Planet-NeRF, an extension to Sat-NeRF capable of incorporating seasonal variability through a set of month embedding vectors.","Comparative evaluations reveal that Planet-NeRF outperforms prior models in the case where seasonal changes are present.","The extensive evaluation combined with the proposed method offers promising avenues for future research in this domain."],"url":"http://arxiv.org/abs/2411.02972v1"}
{"created":"2024-11-05 10:13:23","title":"Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation","abstract":"LiDAR Semantic Segmentation is a fundamental task in autonomous driving perception consisting of associating each LiDAR point to a semantic label. Fully-supervised models have widely tackled this task, but they require labels for each scan, which either limits their domain or requires impractical amounts of expensive annotations. Camera images, which are generally recorded alongside LiDAR pointclouds, can be processed by the widely available 2D foundation models, which are generic and dataset-agnostic. However, distilling knowledge from 2D data to improve LiDAR perception raises domain adaptation challenges. For example, the classical perspective projection suffers from the parallax effect produced by the position shift between both sensors at their respective capture times. We propose a Semi-Supervised Learning setup to leverage unlabeled LiDAR pointclouds alongside distilled knowledge from the camera images. To self-supervise our model on the unlabeled scans, we add an auxiliary NeRF head and cast rays from the camera viewpoint over the unlabeled voxel features. The NeRF head predicts densities and semantic logits at each sampled ray location which are used for rendering pixel semantics. Concurrently, we query the Segment-Anything (SAM) foundation model with the camera image to generate a set of unlabeled generic masks. We fuse the masks with the rendered pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel predictions. During inference, we drop the NeRF head and run our model with only LiDAR. We show the effectiveness of our approach in three public LiDAR Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.","sentences":["LiDAR Semantic Segmentation is a fundamental task in autonomous driving perception consisting of associating each LiDAR point to a semantic label.","Fully-supervised models have widely tackled this task, but they require labels for each scan, which either limits their domain or requires impractical amounts of expensive annotations.","Camera images, which are generally recorded alongside LiDAR pointclouds, can be processed by the widely available 2D foundation models, which are generic and dataset-agnostic.","However, distilling knowledge from 2D data to improve LiDAR perception raises domain adaptation challenges.","For example, the classical perspective projection suffers from the parallax effect produced by the position shift between both sensors at their respective capture times.","We propose a Semi-Supervised Learning setup to leverage unlabeled LiDAR pointclouds alongside distilled knowledge from the camera images.","To self-supervise our model on the unlabeled scans, we add an auxiliary NeRF head and cast rays from the camera viewpoint over the unlabeled voxel features.","The NeRF head predicts densities and semantic logits at each sampled ray location which are used for rendering pixel semantics.","Concurrently, we query the Segment-Anything (SAM) foundation model with the camera image to generate a set of unlabeled generic masks.","We fuse the masks with the rendered pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel predictions.","During inference, we drop the NeRF head and run our model with only LiDAR.","We show the effectiveness of our approach in three public LiDAR Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI."],"url":"http://arxiv.org/abs/2411.02969v1"}
{"created":"2024-11-05 09:53:52","title":"IMUDiffusion: A Diffusion Model for Multivariate Time Series Synthetisation for Inertial Motion Capturing Systems","abstract":"Kinematic sensors are often used to analyze movement behaviors in sports and daily activities due to their ease of use and lack of spatial restrictions, unlike video-based motion capturing systems. Still, the generation, and especially the labeling of motion data for specific activities can be time-consuming and costly. Additionally, many models struggle with limited data, which limits their performance in recognizing complex movement patterns. To address those issues, generating synthetic data can help expand the diversity and variability. In this work, we propose IMUDiffusion, a probabilistic diffusion model specifically designed for multivariate time series generation. Our approach enables the generation of high-quality time series sequences which accurately capture the dynamics of human activities. Moreover, by joining our dataset with synthetic data, we achieve a significant improvement in the performance of our baseline human activity classifier. In some cases, we are able to improve the macro F1-score by almost 30%. IMUDiffusion provides a valuable tool for generating realistic human activity movements and enhance the robustness of models in scenarios with limited training data.","sentences":["Kinematic sensors are often used to analyze movement behaviors in sports and daily activities due to their ease of use and lack of spatial restrictions, unlike video-based motion capturing systems.","Still, the generation, and especially the labeling of motion data for specific activities can be time-consuming and costly.","Additionally, many models struggle with limited data, which limits their performance in recognizing complex movement patterns.","To address those issues, generating synthetic data can help expand the diversity and variability.","In this work, we propose IMUDiffusion, a probabilistic diffusion model specifically designed for multivariate time series generation.","Our approach enables the generation of high-quality time series sequences which accurately capture the dynamics of human activities.","Moreover, by joining our dataset with synthetic data, we achieve a significant improvement in the performance of our baseline human activity classifier.","In some cases, we are able to improve the macro F1-score by almost 30%.","IMUDiffusion provides a valuable tool for generating realistic human activity movements and enhance the robustness of models in scenarios with limited training data."],"url":"http://arxiv.org/abs/2411.02954v1"}
{"created":"2024-11-05 09:45:57","title":"A scalable generative model for dynamical system reconstruction from neuroimaging data","abstract":"Data-driven inference of the generative dynamics underlying a set of observed time series is of growing interest in machine learning and the natural sciences. In neuroscience, such methods promise to alleviate the need to handcraft models based on biophysical principles and allow to automatize the inference of inter-individual differences in brain dynamics. Recent breakthroughs in training techniques for state space models (SSMs) specifically geared toward dynamical systems (DS) reconstruction (DSR) enable to recover the underlying system including its geometrical (attractor) and long-term statistical invariants from even short time series. These techniques are based on control-theoretic ideas, like modern variants of teacher forcing (TF), to ensure stable loss gradient propagation while training. However, as it currently stands, these techniques are not directly applicable to data modalities where current observations depend on an entire history of previous states due to a signal's filtering properties, as common in neuroscience (and physiology more generally). Prominent examples are the blood oxygenation level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or Ca$^{2+}$ imaging data. Such types of signals render the SSM's decoder model non-invertible, a requirement for previous TF-based methods. Here, exploiting the recent success of control techniques for training SSMs, we propose a novel algorithm that solves this problem and scales exceptionally well with model dimensionality and filter length. We demonstrate its efficiency in reconstructing dynamical systems, including their state space geometry and long-term temporal properties, from just short BOLD time series.","sentences":["Data-driven inference of the generative dynamics underlying a set of observed time series is of growing interest in machine learning and the natural sciences.","In neuroscience, such methods promise to alleviate the need to handcraft models based on biophysical principles and allow to automatize the inference of inter-individual differences in brain dynamics.","Recent breakthroughs in training techniques for state space models (SSMs) specifically geared toward dynamical systems (DS) reconstruction (DSR) enable to recover the underlying system including its geometrical (attractor) and long-term statistical invariants from even short time series.","These techniques are based on control-theoretic ideas, like modern variants of teacher forcing (TF), to ensure stable loss gradient propagation while training.","However, as it currently stands, these techniques are not directly applicable to data modalities where current observations depend on an entire history of previous states due to a signal's filtering properties, as common in neuroscience (and physiology more generally).","Prominent examples are the blood oxygenation level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or Ca$^{2+}$ imaging data.","Such types of signals render the SSM's decoder model non-invertible, a requirement for previous TF-based methods.","Here, exploiting the recent success of control techniques for training SSMs, we propose a novel algorithm that solves this problem and scales exceptionally well with model dimensionality and filter length.","We demonstrate its efficiency in reconstructing dynamical systems, including their state space geometry and long-term temporal properties, from just short BOLD time series."],"url":"http://arxiv.org/abs/2411.02949v1"}
{"created":"2024-11-05 09:44:53","title":"Grounding Natural Language to SQL Translation with Data-Based Self-Explanations","abstract":"Natural Language Interfaces for Databases empower non-technical users to interact with data using natural language (NL). Advanced approaches, utilizing either neural sequence-to-sequence or more recent sophisticated large-scale language models, typically implement NL to SQL (NL2SQL) translation in an end-to-end fashion. However, like humans, these end-to-end translation models may not always generate the best SQL output on their first try. In this paper, we propose CycleSQL, an iterative framework designed for end-to-end translation models to autonomously generate the best output through self-evaluation. The main idea of CycleSQL is to introduce data-grounded NL explanations of query results as self-provided feedback, and use the feedback to validate the correctness of the translation iteratively, hence improving the overall translation accuracy. Extensive experiments, including quantitative and qualitative evaluations, are conducted to study CycleSQL by applying it to seven existing translation models on five widely used benchmarks. The results show that 1) the feedback loop introduced in CycleSQL can consistently improve the performance of existing models, and in particular, by applying CycleSQL to RESDSQL, obtains a translation accuracy of 82.0% (+2.6%) on the validation set, and 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL explanations can also provide insightful information for users, aiding in the comprehension of translation results and consequently enhancing the interpretability of NL2SQL translation.","sentences":["Natural Language Interfaces for Databases empower non-technical users to interact with data using natural language (NL).","Advanced approaches, utilizing either neural sequence-to-sequence or more recent sophisticated large-scale language models, typically implement NL to SQL (NL2SQL) translation in an end-to-end fashion.","However, like humans, these end-to-end translation models may not always generate the best SQL output on their first try.","In this paper, we propose CycleSQL, an iterative framework designed for end-to-end translation models to autonomously generate the best output through self-evaluation.","The main idea of CycleSQL is to introduce data-grounded NL explanations of query results as self-provided feedback, and use the feedback to validate the correctness of the translation iteratively, hence improving the overall translation accuracy.","Extensive experiments, including quantitative and qualitative evaluations, are conducted to study CycleSQL by applying it to seven existing translation models on five widely used benchmarks.","The results show that 1) the feedback loop introduced in CycleSQL can consistently improve the performance of existing models, and in particular, by applying CycleSQL to RESDSQL, obtains a translation accuracy of 82.0% (+2.6%) on the validation set, and 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL explanations can also provide insightful information for users, aiding in the comprehension of translation results and consequently enhancing the interpretability of NL2SQL translation."],"url":"http://arxiv.org/abs/2411.02948v1"}
{"created":"2024-11-05 09:44:15","title":"Time-Causal VAE: Robust Financial Time Series Generator","abstract":"We build a time-causal variational autoencoder (TC-VAE) for robust generation of financial time series data. Our approach imposes a causality constraint on the encoder and decoder networks, ensuring a causal transport from the real market time series to the fake generated time series. Specifically, we prove that the TC-VAE loss provides an upper bound on the causal Wasserstein distance between market distributions and generated distributions. Consequently, the TC-VAE loss controls the discrepancy between optimal values of various dynamic stochastic optimization problems under real and generated distributions. To further enhance the model's ability to approximate the latent representation of the real market distribution, we integrate a RealNVP prior into the TC-VAE framework. Finally, extensive numerical experiments show that TC-VAE achieves promising results on both synthetic and real market data. This is done by comparing real and generated distributions according to various statistical distances, demonstrating the effectiveness of the generated data for downstream financial optimization tasks, as well as showcasing that the generated data reproduces stylized facts of real financial market data.","sentences":["We build a time-causal variational autoencoder (TC-VAE) for robust generation of financial time series data.","Our approach imposes a causality constraint on the encoder and decoder networks, ensuring a causal transport from the real market time series to the fake generated time series.","Specifically, we prove that the TC-VAE loss provides an upper bound on the causal Wasserstein distance between market distributions and generated distributions.","Consequently, the TC-VAE loss controls the discrepancy between optimal values of various dynamic stochastic optimization problems under real and generated distributions.","To further enhance the model's ability to approximate the latent representation of the real market distribution, we integrate a RealNVP prior into the TC-VAE framework.","Finally, extensive numerical experiments show that TC-VAE achieves promising results on both synthetic and real market data.","This is done by comparing real and generated distributions according to various statistical distances, demonstrating the effectiveness of the generated data for downstream financial optimization tasks, as well as showcasing that the generated data reproduces stylized facts of real financial market data."],"url":"http://arxiv.org/abs/2411.02947v1"}
{"created":"2024-11-05 09:43:49","title":"Instant Resonance: Dual Strategy Enhances the Data Consensus Success Rate of Blockchain Threshold Signature Oracles","abstract":"With the rapid development of Decentralized Finance (DeFi) and Real-World Assets (RWA), the importance of blockchain oracles in real-time data acquisition has become increasingly prominent. Using cryptographic techniques, threshold signature oracles can achieve consensus on data from multiple nodes and provide corresponding proofs to ensure the credibility and security of the information. However, in real-time data acquisition, threshold signature methods face challenges such as data inconsistency and low success rates in heterogeneous environments, which limit their practical application potential. To address these issues, this paper proposes an innovative dual-strategy approach to enhance the success rate of data consensus in blockchain threshold signature oracles. Firstly, we introduce a Representative Enhanced Aggregation Strategy (REP-AG) that improves the representativeness of data submitted by nodes, ensuring consistency with data from other nodes, and thereby enhancing the usability of threshold signatures. Additionally, we present a Timing Optimization Strategy (TIM-OPT) that dynamically adjusts the timing of nodes' access to data sources to maximize consensus success rates. Experimental results indicate that REP-AG improves the aggregation success rate by approximately 56.6\\% compared to the optimal baseline, while the implementation of TIM-OPT leads to an average increase of approximately 32.9\\% in consensus success rates across all scenarios.","sentences":["With the rapid development of Decentralized Finance (DeFi) and Real-World Assets (RWA), the importance of blockchain oracles in real-time data acquisition has become increasingly prominent.","Using cryptographic techniques, threshold signature oracles can achieve consensus on data from multiple nodes and provide corresponding proofs to ensure the credibility and security of the information.","However, in real-time data acquisition, threshold signature methods face challenges such as data inconsistency and low success rates in heterogeneous environments, which limit their practical application potential.","To address these issues, this paper proposes an innovative dual-strategy approach to enhance the success rate of data consensus in blockchain threshold signature oracles.","Firstly, we introduce a Representative Enhanced Aggregation Strategy (REP-AG) that improves the representativeness of data submitted by nodes, ensuring consistency with data from other nodes, and thereby enhancing the usability of threshold signatures.","Additionally, we present a Timing Optimization Strategy (TIM-OPT) that dynamically adjusts the timing of nodes' access to data sources to maximize consensus success rates.","Experimental results indicate that REP-AG improves the aggregation success rate by approximately 56.6\\% compared to the optimal baseline, while the implementation of TIM-OPT leads to an average increase of approximately 32.9\\% in consensus success rates across all scenarios."],"url":"http://arxiv.org/abs/2411.02945v1"}
{"created":"2024-11-05 09:37:23","title":"Capturing research literature attitude towards Sustainable Development Goals: an LLM-based topic modeling approach","abstract":"The world is facing a multitude of challenges that hinder the development of human civilization and the well-being of humanity on the planet. The Sustainable Development Goals (SDGs) were formulated by the United Nations in 2015 to address these global challenges by 2030. Natural language processing techniques can help uncover discussions on SDGs within research literature. We propose a completely automated pipeline to 1) fetch content from the Scopus database and prepare datasets dedicated to five groups of SDGs; 2) perform topic modeling, a statistical technique used to identify topics in large collections of textual data; and 3) enable topic exploration through keywords-based search and topic frequency time series extraction. For topic modeling, we leverage the stack of BERTopic scaled up to be applied on large corpora of textual documents (we find hundreds of topics on hundreds of thousands of documents), introducing i) a novel LLM-based embeddings computation for representing scientific abstracts in the continuous space and ii) a hyperparameter optimizer to efficiently find the best configuration for any new big datasets. We additionally produce the visualization of results on interactive dashboards reporting topics' temporal evolution. Results are made inspectable and explorable, contributing to the interpretability of the topic modeling process. Our proposed LLM-based topic modeling pipeline for big-text datasets allows users to capture insights on the evolution of the attitude toward SDGs within scientific abstracts in the 2006-2023 time span. All the results are reproducible by using our system; the workflow can be generalized to be applied at any point in time to any big corpus of textual documents.","sentences":["The world is facing a multitude of challenges that hinder the development of human civilization and the well-being of humanity on the planet.","The Sustainable Development Goals (SDGs) were formulated by the United Nations in 2015 to address these global challenges by 2030.","Natural language processing techniques can help uncover discussions on SDGs within research literature.","We propose a completely automated pipeline to 1) fetch content from the Scopus database and prepare datasets dedicated to five groups of SDGs; 2) perform topic modeling, a statistical technique used to identify topics in large collections of textual data; and 3) enable topic exploration through keywords-based search and topic frequency time series extraction.","For topic modeling, we leverage the stack of BERTopic scaled up to be applied on large corpora of textual documents (we find hundreds of topics on hundreds of thousands of documents), introducing i) a novel LLM-based embeddings computation for representing scientific abstracts in the continuous space and ii) a hyperparameter optimizer to efficiently find the best configuration for any new big datasets.","We additionally produce the visualization of results on interactive dashboards reporting topics' temporal evolution.","Results are made inspectable and explorable, contributing to the interpretability of the topic modeling process.","Our proposed LLM-based topic modeling pipeline for big-text datasets allows users to capture insights on the evolution of the attitude toward SDGs within scientific abstracts in the 2006-2023 time span.","All the results are reproducible by using our system; the workflow can be generalized to be applied at any point in time to any big corpus of textual documents."],"url":"http://arxiv.org/abs/2411.02943v1"}
{"created":"2024-11-05 09:35:02","title":"Constant Approximation for Weighted Nash Social Welfare with Submodular Valuations","abstract":"We study the problem of assigning items to agents so as to maximize the \\emph{weighted} Nash Social Welfare (NSW) under submodular valuations. The best-known result for the problem is an $O(nw_{\\max})$-approximation due to Garg, Husic, Li, Vega, and Vondrak~\\cite{GHL23}, where $w_{\\max}$ is the maximum weight over all agents. Obtaining a constant approximation algorithm is an open problem in the field that has recently attracted considerable attention.   We give the first such algorithm for the problem, thus solving the open problem in the affirmative. Our algorithm is based on the natural Configuration LP for the problem, which was introduced recently by Feng and Li~\\cite{FL24} for the additive valuation case. Our rounding algorithm is similar to that of Li \\cite{Li25} developed for the unrelated machine scheduling problem to minimize weighted completion time. Roughly speaking, we designate the largest item in each configuration as a large item and the remaining items as small items. So, every agent gets precisely 1 fractional large item in the configuration LP solution. With the rounding algorithm in \\cite{Li25}, we can ensure that in the obtained solution, every agent gets precisely 1 large item, and the assignments of small items are negatively correlated.","sentences":["We study the problem of assigning items to agents so as to maximize the \\emph{weighted} Nash Social Welfare (NSW) under submodular valuations.","The best-known result for the problem is an $O(nw_{\\max})$-approximation due to Garg, Husic, Li, Vega, and Vondrak~\\cite{GHL23}, where $w_{\\max}$ is the maximum weight over all agents.","Obtaining a constant approximation algorithm is an open problem in the field that has recently attracted considerable attention.   ","We give the first such algorithm for the problem, thus solving the open problem in the affirmative.","Our algorithm is based on the natural Configuration LP for the problem, which was introduced recently by Feng and Li~\\cite{FL24} for the additive valuation case.","Our rounding algorithm is similar to that of Li \\cite{Li25} developed for the unrelated machine scheduling problem to minimize weighted completion time.","Roughly speaking, we designate the largest item in each configuration as a large item and the remaining items as small items.","So, every agent gets precisely 1 fractional large item in the configuration LP solution.","With the rounding algorithm in \\cite{Li25}, we can ensure that in the obtained solution, every agent gets precisely 1 large item, and the assignments of small items are negatively correlated."],"url":"http://arxiv.org/abs/2411.02942v1"}
{"created":"2024-11-05 09:34:05","title":"A Mamba Foundation Model for Time Series Forecasting","abstract":"Time series foundation models have demonstrated strong performance in zero-shot learning, making them well-suited for predicting rapidly evolving patterns in real-world applications where relevant training data are scarce. However, most of these models rely on the Transformer architecture, which incurs quadratic complexity as input length increases. To address this, we introduce TSMamba, a linear-complexity foundation model for time series forecasting built on the Mamba architecture. The model captures temporal dependencies through both forward and backward Mamba encoders, achieving high prediction accuracy. To reduce reliance on large datasets and lower training costs, TSMamba employs a two-stage transfer learning process that leverages pretrained Mamba LLMs, allowing effective time series modeling with a moderate training set. In the first stage, the forward and backward backbones are optimized via patch-wise autoregressive prediction; in the second stage, the model trains a prediction head and refines other components for long-term forecasting. While the backbone assumes channel independence to manage varying channel numbers across datasets, a channel-wise compressed attention module is introduced to capture cross-channel dependencies during fine-tuning on specific multivariate datasets. Experiments show that TSMamba's zero-shot performance is comparable to state-of-the-art time series foundation models, despite using significantly less training data. It also achieves competitive or superior full-shot performance compared to task-specific prediction models. The code will be made publicly available.","sentences":["Time series foundation models have demonstrated strong performance in zero-shot learning, making them well-suited for predicting rapidly evolving patterns in real-world applications where relevant training data are scarce.","However, most of these models rely on the Transformer architecture, which incurs quadratic complexity as input length increases.","To address this, we introduce TSMamba, a linear-complexity foundation model for time series forecasting built on the Mamba architecture.","The model captures temporal dependencies through both forward and backward Mamba encoders, achieving high prediction accuracy.","To reduce reliance on large datasets and lower training costs, TSMamba employs a two-stage transfer learning process that leverages pretrained Mamba LLMs, allowing effective time series modeling with a moderate training set.","In the first stage, the forward and backward backbones are optimized via patch-wise autoregressive prediction; in the second stage, the model trains a prediction head and refines other components for long-term forecasting.","While the backbone assumes channel independence to manage varying channel numbers across datasets, a channel-wise compressed attention module is introduced to capture cross-channel dependencies during fine-tuning on specific multivariate datasets.","Experiments show that TSMamba's zero-shot performance is comparable to state-of-the-art time series foundation models, despite using significantly less training data.","It also achieves competitive or superior full-shot performance compared to task-specific prediction models.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2411.02941v1"}
{"created":"2024-11-05 09:32:26","title":"A Post-Training Enhanced Optimization Approach for Small Language Models","abstract":"This paper delves into the continuous post-training optimization methods for small language models, and proposes a continuous post-training alignment data construction method for small language models. The core of this method is based on the data guidance of large models, optimizing the diversity and accuracy of alignment data. In addition, to verify the effectiveness of the methods in this paper, we used Qwen2-0.5B-Instruct model as the baseline model for small language models, using the alignment dataset constructed by our proposed method, we trained and compared several groups of experiments, including SFT (Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky optimization) post-training experiment, as well as SFT-KTO two-stage post-training experiment and model weight fusion experiment. Finally, we evaluated and analyzed the performance of post-training models, and confirmed that the continuous post-training optimization method proposed by us can significantly improve the performance of small language models.","sentences":["This paper delves into the continuous post-training optimization methods for small language models, and proposes a continuous post-training alignment data construction method for small language models.","The core of this method is based on the data guidance of large models, optimizing the diversity and accuracy of alignment data.","In addition, to verify the effectiveness of the methods in this paper, we used Qwen2-0.5B-Instruct model as the baseline model for small language models, using the alignment dataset constructed by our proposed method, we trained and compared several groups of experiments, including SFT (Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky optimization) post-training experiment, as well as SFT-KTO two-stage post-training experiment and model weight fusion experiment.","Finally, we evaluated and analyzed the performance of post-training models, and confirmed that the continuous post-training optimization method proposed by us can significantly improve the performance of small language models."],"url":"http://arxiv.org/abs/2411.02939v1"}
{"created":"2024-11-05 09:24:59","title":"Mapping Africa Settlements: High Resolution Urban and Rural Map by Deep Learning and Satellite Imagery","abstract":"Accurate Land Use and Land Cover (LULC) maps are essential for understanding the drivers of sustainable development, in terms of its complex interrelationships between human activities and natural resources. However, existing LULC maps often lack precise urban and rural classifications, particularly in diverse regions like Africa. This study presents a novel construction of a high-resolution rural-urban map using deep learning techniques and satellite imagery. We developed a deep learning model based on the DeepLabV3 architecture, which was trained on satellite imagery from Landsat-8 and the ESRI LULC dataset, augmented with human settlement data from the GHS-SMOD. The model utilizes semantic segmentation to classify land into detailed categories, including urban and rural areas, at a 10-meter resolution. Our findings demonstrate that incorporating LULC along with urban and rural classifications significantly enhances the model's ability to accurately distinguish between urban, rural, and non-human settlement areas. Therefore, our maps can support more informed decision-making for policymakers, researchers, and stakeholders. We release a continent wide urban-rural map, covering the period 2016 and 2022.","sentences":["Accurate Land Use and Land Cover (LULC) maps are essential for understanding the drivers of sustainable development, in terms of its complex interrelationships between human activities and natural resources.","However, existing LULC maps often lack precise urban and rural classifications, particularly in diverse regions like Africa.","This study presents a novel construction of a high-resolution rural-urban map using deep learning techniques and satellite imagery.","We developed a deep learning model based on the DeepLabV3 architecture, which was trained on satellite imagery from Landsat-8 and the ESRI LULC dataset, augmented with human settlement data from the GHS-SMOD.","The model utilizes semantic segmentation to classify land into detailed categories, including urban and rural areas, at a 10-meter resolution.","Our findings demonstrate that incorporating LULC along with urban and rural classifications significantly enhances the model's ability to accurately distinguish between urban, rural, and non-human settlement areas.","Therefore, our maps can support more informed decision-making for policymakers, researchers, and stakeholders.","We release a continent wide urban-rural map, covering the period 2016 and 2022."],"url":"http://arxiv.org/abs/2411.02935v1"}
{"created":"2024-11-05 09:23:27","title":"P-MOSS: Learned Scheduling For Indexes Over NUMA Servers Using Low-Level Hardware Statistics","abstract":"Ever since the Dennard scaling broke down in the early 2000s and the frequency of the CPU stalled, vendors have started to increase the core count in each CPU chip at the expense of introducing heterogeneity, thus ushering the era of NUMA processors. Since then, the heterogeneity in the design space of hardware has only increased to the point that DBMS performance may vary significantly up to an order of magnitude in modern servers. An important factor that affects performance includes the location of the logical cores where the DBMS queries are scheduled, and the locations of the data that the queries access. This paper introduces P-MOSS, a learned spatial scheduling framework that schedules query execution to certain logical cores, and places data accordingly to certain integrated memory controllers (IMC), to integrate hardware consciousness into the system. In the spirit of hardware-software synergy, P-MOSS solely guides its scheduling decision based on low-level hardware statistics collected by performance monitoring counters with the aid of a Decision Transformer. Experimental evaluation is performed in the context of the B-tree and R-tree indexes. Performance results demonstrate that P-MOSS has up to 6x improvement over traditional schedules in terms of query throughput.","sentences":["Ever since the Dennard scaling broke down in the early 2000s and the frequency of the CPU stalled, vendors have started to increase the core count in each CPU chip at the expense of introducing heterogeneity, thus ushering the era of NUMA processors.","Since then, the heterogeneity in the design space of hardware has only increased to the point that DBMS performance may vary significantly up to an order of magnitude in modern servers.","An important factor that affects performance includes the location of the logical cores where the DBMS queries are scheduled, and the locations of the data that the queries access.","This paper introduces P-MOSS, a learned spatial scheduling framework that schedules query execution to certain logical cores, and places data accordingly to certain integrated memory controllers (IMC), to integrate hardware consciousness into the system.","In the spirit of hardware-software synergy, P-MOSS solely guides its scheduling decision based on low-level hardware statistics collected by performance monitoring counters with the aid of a Decision Transformer.","Experimental evaluation is performed in the context of the B-tree and R-tree indexes.","Performance results demonstrate that P-MOSS has up to 6x improvement over traditional schedules in terms of query throughput."],"url":"http://arxiv.org/abs/2411.02933v1"}
{"created":"2024-11-05 09:22:08","title":"Textual Aesthetics in Large Language Models","abstract":"Image aesthetics is a crucial metric in the field of image generation. However, textual aesthetics has not been sufficiently explored. With the widespread application of large language models (LLMs), previous work has primarily focused on the correctness of content and the helpfulness of responses. Nonetheless, providing responses with textual aesthetics is also an important factor for LLMs, which can offer a cleaner layout and ensure greater consistency and coherence in content. In this work, we introduce a pipeline for aesthetics polishing and help construct a textual aesthetics dataset named TexAes. We propose a textual aesthetics-powered fine-tuning method based on direct preference optimization, termed TAPO, which leverages textual aesthetics without compromising content correctness. Additionally, we develop two evaluation methods for textual aesthetics based on text and image analysis, respectively. Our experiments demonstrate that using textual aesthetics data and employing the TAPO fine-tuning method not only improves aesthetic scores but also enhances performance on general evaluation datasets such as AlpacalEval and Anera-hard.","sentences":["Image aesthetics is a crucial metric in the field of image generation.","However, textual aesthetics has not been sufficiently explored.","With the widespread application of large language models (LLMs), previous work has primarily focused on the correctness of content and the helpfulness of responses.","Nonetheless, providing responses with textual aesthetics is also an important factor for LLMs, which can offer a cleaner layout and ensure greater consistency and coherence in content.","In this work, we introduce a pipeline for aesthetics polishing and help construct a textual aesthetics dataset named TexAes.","We propose a textual aesthetics-powered fine-tuning method based on direct preference optimization, termed TAPO, which leverages textual aesthetics without compromising content correctness.","Additionally, we develop two evaluation methods for textual aesthetics based on text and image analysis, respectively.","Our experiments demonstrate that using textual aesthetics data and employing the TAPO fine-tuning method not only improves aesthetic scores but also enhances performance on general evaluation datasets such as AlpacalEval and Anera-hard."],"url":"http://arxiv.org/abs/2411.02930v1"}
{"created":"2024-11-05 09:13:53","title":"Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic Encryption for Collaborative Anti-Money Laundering","abstract":"Combating money laundering has become increasingly complex with the rise of cybercrime and digitalization of financial transactions. Graph-based machine learning techniques have emerged as promising tools for Anti-Money Laundering (AML) detection, capturing intricate relationships within money laundering networks. However, the effectiveness of AML solutions is hindered by data silos within financial institutions, limiting collaboration and overall efficacy. This research presents a novel privacy-preserving approach for collaborative AML machine learning, facilitating secure data sharing across institutions and borders while preserving privacy and regulatory compliance. Leveraging Fully Homomorphic Encryption (FHE), computations are directly performed on encrypted data, ensuring the confidentiality of financial data. Notably, FHE over the Torus (TFHE) was integrated with graph-based machine learning using Zama Concrete ML. The research contributes two key privacy-preserving pipelines. First, the development of a privacy-preserving Graph Neural Network (GNN) pipeline was explored. Optimization techniques like quantization and pruning were used to render the GNN FHE-compatible. Second, a privacy-preserving graph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was successfully developed. Experiments demonstrated strong predictive performance, with the XGBoost model consistently achieving over 99% accuracy, F1-score, precision, and recall on the balanced AML dataset in both unencrypted and FHE-encrypted inference settings. On the imbalanced dataset, the incorporation of graph-based features improved the F1-score by 8%. The research highlights the need to balance the trade-off between privacy and computational efficiency.","sentences":["Combating money laundering has become increasingly complex with the rise of cybercrime and digitalization of financial transactions.","Graph-based machine learning techniques have emerged as promising tools for Anti-Money Laundering (AML) detection, capturing intricate relationships within money laundering networks.","However, the effectiveness of AML solutions is hindered by data silos within financial institutions, limiting collaboration and overall efficacy.","This research presents a novel privacy-preserving approach for collaborative AML machine learning, facilitating secure data sharing across institutions and borders while preserving privacy and regulatory compliance.","Leveraging Fully Homomorphic Encryption (FHE), computations are directly performed on encrypted data, ensuring the confidentiality of financial data.","Notably, FHE over the Torus (TFHE) was integrated with graph-based machine learning using Zama Concrete ML.","The research contributes two key privacy-preserving pipelines.","First, the development of a privacy-preserving Graph Neural Network (GNN) pipeline was explored.","Optimization techniques like quantization and pruning were used to render the GNN FHE-compatible.","Second, a privacy-preserving graph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was successfully developed.","Experiments demonstrated strong predictive performance, with the XGBoost model consistently achieving over 99% accuracy, F1-score, precision, and recall on the balanced AML dataset in both unencrypted and FHE-encrypted inference settings.","On the imbalanced dataset, the incorporation of graph-based features improved the F1-score by 8%.","The research highlights the need to balance the trade-off between privacy and computational efficiency."],"url":"http://arxiv.org/abs/2411.02926v1"}
{"created":"2024-11-05 09:10:39","title":"Theoretically Guaranteed Distribution Adaptable Learning","abstract":"In many open environment applications, data are collected in the form of a stream, which exhibits an evolving distribution over time. How to design algorithms to track these evolving data distributions with provable guarantees, particularly in terms of the generalization ability, remains a formidable challenge. To handle this crucial but rarely studied problem and take a further step toward robust artificial intelligence, we propose a novel framework called Distribution Adaptable Learning (DAL). It enables the model to effectively track the evolving data distributions. By Encoding Feature Marginal Distribution Information (EFMDI), we broke the limitations of optimal transport to characterize the environmental changes and enable model reuse across diverse data distributions. It can enhance the reusable and evolvable properties of DAL in accommodating evolving distributions. Furthermore, to obtain the model interpretability, we not only analyze the generalization error bound of the local step in the evolution process, but also investigate the generalization error bound associated with the entire classifier trajectory of the evolution based on the Fisher-Rao distance. For demonstration, we also present two special cases within the framework, together with their optimizations and convergence analyses. Experimental results over both synthetic and real-world data distribution evolving tasks validate the effectiveness and practical utility of the proposed framework.","sentences":["In many open environment applications, data are collected in the form of a stream, which exhibits an evolving distribution over time.","How to design algorithms to track these evolving data distributions with provable guarantees, particularly in terms of the generalization ability, remains a formidable challenge.","To handle this crucial but rarely studied problem and take a further step toward robust artificial intelligence, we propose a novel framework called Distribution Adaptable Learning (DAL).","It enables the model to effectively track the evolving data distributions.","By Encoding Feature Marginal Distribution Information (EFMDI), we broke the limitations of optimal transport to characterize the environmental changes and enable model reuse across diverse data distributions.","It can enhance the reusable and evolvable properties of DAL in accommodating evolving distributions.","Furthermore, to obtain the model interpretability, we not only analyze the generalization error bound of the local step in the evolution process, but also investigate the generalization error bound associated with the entire classifier trajectory of the evolution based on the Fisher-Rao distance.","For demonstration, we also present two special cases within the framework, together with their optimizations and convergence analyses.","Experimental results over both synthetic and real-world data distribution evolving tasks validate the effectiveness and practical utility of the proposed framework."],"url":"http://arxiv.org/abs/2411.02921v1"}
{"created":"2024-11-05 09:08:46","title":"Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization","abstract":"Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts. The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition. In this paper, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes. Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples. Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information. Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers. In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization. Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets.","sentences":["Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts.","The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition.","In this paper, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes.","Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples.","Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information.","Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers.","In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization.","Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets."],"url":"http://arxiv.org/abs/2411.02920v1"}
{"created":"2024-11-05 08:48:25","title":"Photon: Federated LLM Pre-Training","abstract":"Scaling large language models (LLMs) demands extensive data and computing resources, which are traditionally constrained to data centers by the high-bandwidth requirements of distributed training. Low-bandwidth methods like federated learning (FL) could enable collaborative training of larger models across weakly-connected GPUs if they can effectively be used for pre-training. To achieve this, we introduce Photon, the first complete system for federated end-to-end LLM training, leveraging cross-silo FL for global-scale training with minimal communication overheads. Using Photon, we train the first federated family of decoder-only LLMs from scratch. We show that: (1) Photon can train model sizes up to 7B in a federated fashion while reaching an even better perplexity than centralized pre-training; (2) Photon model training time decreases with available compute, achieving a similar compute-time trade-off to centralized; and (3) Photon outperforms the wall-time of baseline distributed training methods by 35% via communicating 64x-512xless. Our proposal is robust to data heterogeneity and converges twice as fast as previous methods like DiLoCo. This surprising data efficiency stems from a unique approach combining small client batch sizes with extremely high learning rates, enabled by federated averaging's robustness to hyperparameters. Photon thus represents the first economical system for global internet-wide LLM pre-training.","sentences":["Scaling large language models (LLMs) demands extensive data and computing resources, which are traditionally constrained to data centers by the high-bandwidth requirements of distributed training.","Low-bandwidth methods like federated learning (FL) could enable collaborative training of larger models across weakly-connected GPUs if they can effectively be used for pre-training.","To achieve this, we introduce Photon, the first complete system for federated end-to-end LLM training, leveraging cross-silo FL for global-scale training with minimal communication overheads.","Using Photon, we train the first federated family of decoder-only LLMs from scratch.","We show that: (1) Photon can train model sizes up to 7B in a federated fashion while reaching an even better perplexity than centralized pre-training; (2) Photon model training time decreases with available compute, achieving a similar compute-time trade-off to centralized; and (3) Photon outperforms the wall-time of baseline distributed training methods by 35% via communicating 64x-512xless.","Our proposal is robust to data heterogeneity and converges twice as fast as previous methods like DiLoCo.","This surprising data efficiency stems from a unique approach combining small client batch sizes with extremely high learning rates, enabled by federated averaging's robustness to hyperparameters.","Photon thus represents the first economical system for global internet-wide LLM pre-training."],"url":"http://arxiv.org/abs/2411.02908v1"}
{"created":"2024-11-05 08:35:08","title":"Membership Inference Attacks against Large Vision-Language Models","abstract":"Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR\\'enyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.","sentences":["Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios.","However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets.","Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies.","In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection.","Then, we propose a novel MIA pipeline specifically designed for token-level image detection.","Lastly, we present a new metric called MaxR\\'enyi-K%, which is based on the confidence of the model output and applies to both text and image data.","We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs.","Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA."],"url":"http://arxiv.org/abs/2411.02902v1"}
{"created":"2024-11-05 07:17:06","title":"iAnomaly: A Toolkit for Generating Performance Anomaly Datasets in Edge-Cloud Integrated Computing Environments","abstract":"Microservice architectures are increasingly used to modularize IoT applications and deploy them in distributed and heterogeneous edge computing environments. Over time, these microservice-based IoT applications are susceptible to performance anomalies caused by resource hogging (e.g., CPU or memory), resource contention, etc., which can negatively impact their Quality of Service and violate their Service Level Agreements. Existing research on performance anomaly detection in edge computing environments is limited primarily due to the absence of publicly available edge performance anomaly datasets or due to the lack of accessibility of real edge setups to generate necessary data. To address this gap, we propose iAnomaly: a full-system emulator equipped with open-source tools and fully automated dataset generation capabilities to generate labeled normal and anomaly data based on user-defined configurations. We also release a performance anomaly dataset generated using iAnomaly, which captures performance data for several microservice-based IoT applications with heterogeneous QoS and resource requirements while introducing a variety of anomalies. This dataset effectively represents the characteristics found in real edge environments, and the anomalous data in the dataset adheres to the required standards of a high-quality performance anomaly dataset.","sentences":["Microservice architectures are increasingly used to modularize IoT applications and deploy them in distributed and heterogeneous edge computing environments.","Over time, these microservice-based IoT applications are susceptible to performance anomalies caused by resource hogging (e.g., CPU or memory), resource contention, etc., which can negatively impact their Quality of Service and violate their Service Level Agreements.","Existing research on performance anomaly detection in edge computing environments is limited primarily due to the absence of publicly available edge performance anomaly datasets or due to the lack of accessibility of real edge setups to generate necessary data.","To address this gap, we propose iAnomaly: a full-system emulator equipped with open-source tools and fully automated dataset generation capabilities to generate labeled normal and anomaly data based on user-defined configurations.","We also release a performance anomaly dataset generated using iAnomaly, which captures performance data for several microservice-based IoT applications with heterogeneous QoS and resource requirements while introducing a variety of anomalies.","This dataset effectively represents the characteristics found in real edge environments, and the anomalous data in the dataset adheres to the required standards of a high-quality performance anomaly dataset."],"url":"http://arxiv.org/abs/2411.02868v1"}
{"created":"2024-11-05 07:12:50","title":"Double Whammy: Stealthy Data Manipulation aided Reconstruction Attack on Graph Federated Learning","abstract":"Recent research has constructed successful graph reconstruction attack (GRA) on GFL. But these attacks are still challenged in aspects of effectiveness and stealth. To address the issues, we propose the first Data Manipulation aided Reconstruction attack on GFL, dubbed as DMan4Rec. The malicious client is born to manipulate its locally collected data to enhance graph stealing privacy from benign ones, so as to construct double whammy on GFL. It differs from previous work in three terms: (1) effectiveness - to fully utilize the sparsity and feature smoothness of the graph, novel penalty terms are designed adaptive to diverse similarity functions for connected and unconnected node pairs, as well as incorporation label smoothing on top of the original cross-entropy loss. (2) scalability - DMan4Rec is capable of both white-box and black-box attacks via training a supervised model to infer the posterior probabilities obtained from limited queries (3) stealthiness - by manipulating the malicious client's node features, it can maintain the overall graph structure's invariance and conceal the attack. Comprehensive experiments on four real datasets and three GNN models demonstrate that DMan4Rec achieves the state-of-the-art (SOTA) attack performance, e.g., the attack AUC and precision improved by 9.2% and 10.5% respectively compared with the SOTA baselines. Particularly, DMan4Rec achieves an AUC score and a precision score of up to 99.59% and 99.56%, respectively in black-box setting. Nevertheless, the complete overlap of the distribution graphs supports the stealthiness of the attack. Besides, DMan4Rec still beats the defensive GFL, which alarms a new threat to GFL.","sentences":["Recent research has constructed successful graph reconstruction attack (GRA) on GFL.","But these attacks are still challenged in aspects of effectiveness and stealth.","To address the issues, we propose the first Data Manipulation aided Reconstruction attack on GFL, dubbed as DMan4Rec.","The malicious client is born to manipulate its locally collected data to enhance graph stealing privacy from benign ones, so as to construct double whammy on GFL.","It differs from previous work in three terms: (1) effectiveness - to fully utilize the sparsity and feature smoothness of the graph, novel penalty terms are designed adaptive to diverse similarity functions for connected and unconnected node pairs, as well as incorporation label smoothing on top of the original cross-entropy loss.","(2) scalability - DMan4Rec is capable of both white-box and black-box attacks via training a supervised model to infer the posterior probabilities obtained from limited queries (3) stealthiness - by manipulating the malicious client's node features, it can maintain the overall graph structure's invariance and conceal the attack.","Comprehensive experiments on four real datasets and three GNN models demonstrate that DMan4Rec achieves the state-of-the-art (SOTA) attack performance, e.g., the attack AUC and precision improved by 9.2% and 10.5% respectively compared with the SOTA baselines.","Particularly, DMan4Rec achieves an AUC score and a precision score of up to 99.59% and 99.56%, respectively in black-box setting.","Nevertheless, the complete overlap of the distribution graphs supports the stealthiness of the attack.","Besides, DMan4Rec still beats the defensive GFL, which alarms a new threat to GFL."],"url":"http://arxiv.org/abs/2411.02866v1"}
{"created":"2024-11-05 06:59:05","title":"Analyzing Poverty through Intra-Annual Time-Series: A Wavelet Transform Approach","abstract":"Reducing global poverty is a key objective of the Sustainable Development Goals (SDGs). Achieving this requires high-frequency, granular data to capture neighborhood-level changes, particularly in data scarce regions such as low- and middle-income countries. To fill in the data gaps, recent computer vision methods combining machine learning (ML) with earth observation (EO) data to improve poverty estimation. However, while much progress have been made, they often omit intra-annual variations, which are crucial for estimating poverty in agriculturally dependent countries. We explored the impact of integrating intra-annual NDVI information with annual multi-spectral data on model accuracy. To evaluate our method, we created a simulated dataset using Landsat imagery and nighttime light data to evaluate EO-ML methods that use intra-annual EO data. Additionally, we evaluated our method against the Demographic and Health Survey (DHS) dataset across Africa. Our results indicate that integrating specific NDVI-derived features with multi-spectral data provides valuable insights for poverty analysis, emphasizing the importance of retaining intra-annual information.","sentences":["Reducing global poverty is a key objective of the Sustainable Development Goals (SDGs).","Achieving this requires high-frequency, granular data to capture neighborhood-level changes, particularly in data scarce regions such as low- and middle-income countries.","To fill in the data gaps, recent computer vision methods combining machine learning (ML) with earth observation (EO) data to improve poverty estimation.","However, while much progress have been made, they often omit intra-annual variations, which are crucial for estimating poverty in agriculturally dependent countries.","We explored the impact of integrating intra-annual NDVI information with annual multi-spectral data on model accuracy.","To evaluate our method, we created a simulated dataset using Landsat imagery and nighttime light data to evaluate EO-ML methods that use intra-annual EO data.","Additionally, we evaluated our method against the Demographic and Health Survey (DHS) dataset across Africa.","Our results indicate that integrating specific NDVI-derived features with multi-spectral data provides valuable insights for poverty analysis, emphasizing the importance of retaining intra-annual information."],"url":"http://arxiv.org/abs/2411.02855v1"}
{"created":"2024-11-05 06:59:02","title":"SpiDR: A Reconfigurable Digital Compute-in-Memory Spiking Neural Network Accelerator for Event-based Perception","abstract":"Spiking Neural Networks (SNNs), with their inherent recurrence, offer an efficient method for processing the asynchronous temporal data generated by Dynamic Vision Sensors (DVS), making them well-suited for event-based vision applications. However, existing SNN accelerators suffer from limitations in adaptability to diverse neuron models, bit precisions and network sizes, inefficient membrane potential (Vmem) handling, and limited sparse optimizations. In response to these challenges, we propose a scalable and reconfigurable digital compute-in-memory (CIM) SNN accelerator \\chipname with a set of key features: 1) It uses in-memory computations and reconfigurable operating modes to minimize data movement associated with weight and Vmem data structures while efficiently adapting to different workloads. 2) It supports multiple weight/Vmem bit precision values, enabling a trade-off between accuracy and energy efficiency and enhancing adaptability to diverse application demands. 3) A zero-skipping mechanism for sparse inputs significantly reduces energy usage by leveraging the inherent sparsity of spikes without introducing high overheads for low sparsity. 4) Finally, the asynchronous handshaking mechanism maintains the computational efficiency of the pipeline for variable execution times of different computation units. We fabricated \\chipname in 65 nm Taiwan Semiconductor Manufacturing Company (TSMC) low-power (LP) technology. It demonstrates competitive performance (scaled to the same technology node) to other digital SNN accelerators proposed in the recent literature and supports advanced reconfigurability. It achieves up to 5 TOPS/W energy efficiency at 95% input sparsity with 4-bit weights and 7-bit Vmem precision.","sentences":["Spiking Neural Networks (SNNs), with their inherent recurrence, offer an efficient method for processing the asynchronous temporal data generated by Dynamic Vision Sensors (DVS), making them well-suited for event-based vision applications.","However, existing SNN accelerators suffer from limitations in adaptability to diverse neuron models, bit precisions and network sizes, inefficient membrane potential (Vmem) handling, and limited sparse optimizations.","In response to these challenges, we propose a scalable and reconfigurable digital compute-in-memory (CIM) SNN accelerator \\chipname with a set of key features: 1) It uses in-memory computations and reconfigurable operating modes to minimize data movement associated with weight and Vmem data structures while efficiently adapting to different workloads.","2) It supports multiple weight/Vmem bit precision values, enabling a trade-off between accuracy and energy efficiency and enhancing adaptability to diverse application demands.","3) A zero-skipping mechanism for sparse inputs significantly reduces energy usage by leveraging the inherent sparsity of spikes without introducing high overheads for low sparsity.","4) Finally, the asynchronous handshaking mechanism maintains the computational efficiency of the pipeline for variable execution times of different computation units.","We fabricated \\chipname in 65 nm Taiwan Semiconductor Manufacturing Company (TSMC) low-power (LP) technology.","It demonstrates competitive performance (scaled to the same technology node) to other digital SNN accelerators proposed in the recent literature and supports advanced reconfigurability.","It achieves up to 5 TOPS/W energy efficiency at 95% input sparsity with 4-bit weights and 7-bit Vmem precision."],"url":"http://arxiv.org/abs/2411.02854v1"}
{"created":"2024-11-05 06:44:15","title":"WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African clean water access, sanitation and hygiene","abstract":"This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate rural African communities on clean water access, sanitation, and hygiene (WASH) principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach to address the limitations of previous approaches with limited reach or missing contextualization. The paper details the development process, employing Design Science Research Methodology. The evaluation consisted of two phases: content validation by four WASH experts and community validation by potential users. Content validation confirmed WASHtsApp's ability to provide accurate and relevant WASH-related information. Community validation indicated high user acceptance and perceived usefulness of the chatbot. The paper concludes by discussing the potential for further development, including incorporating local languages and user data analysis for targeted interventions. It also proposes future research cycles focused on wider deployment and leveraging user data for educational purposes.","sentences":["This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate rural African communities on clean water access, sanitation, and hygiene (WASH) principles.","WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach to address the limitations of previous approaches with limited reach or missing contextualization.","The paper details the development process, employing Design Science Research Methodology.","The evaluation consisted of two phases: content validation by four WASH experts and community validation by potential users.","Content validation confirmed WASHtsApp's ability to provide accurate and relevant WASH-related information.","Community validation indicated high user acceptance and perceived usefulness of the chatbot.","The paper concludes by discussing the potential for further development, including incorporating local languages and user data analysis for targeted interventions.","It also proposes future research cycles focused on wider deployment and leveraging user data for educational purposes."],"url":"http://arxiv.org/abs/2411.02850v1"}
{"created":"2024-11-05 06:42:51","title":"Adversarial multi-task underwater acoustic target recognition: towards robustness against various influential factors","abstract":"Underwater acoustic target recognition based on passive sonar faces numerous challenges in practical maritime applications. One of the main challenges lies in the susceptibility of signal characteristics to diverse environmental conditions and data acquisition configurations, which can lead to instability in recognition systems. While significant efforts have been dedicated to addressing these influential factors in other domains of underwater acoustics, they are often neglected in the field of underwater acoustic target recognition. To overcome this limitation, this study designs auxiliary tasks that model influential factors (e.g., source range, water column depth, or wind speed) based on available annotations and adopts a multi-task framework to connect these factors to the recognition task. Furthermore, we integrate an adversarial learning mechanism into the multi-task framework to prompt the model to extract representations that are robust against influential factors. Through extensive experiments and analyses on the ShipsEar dataset, our proposed adversarial multi-task model demonstrates its capacity to effectively model the influential factors and achieve state-of-the-art performance on the 12-class recognition task.","sentences":["Underwater acoustic target recognition based on passive sonar faces numerous challenges in practical maritime applications.","One of the main challenges lies in the susceptibility of signal characteristics to diverse environmental conditions and data acquisition configurations, which can lead to instability in recognition systems.","While significant efforts have been dedicated to addressing these influential factors in other domains of underwater acoustics, they are often neglected in the field of underwater acoustic target recognition.","To overcome this limitation, this study designs auxiliary tasks that model influential factors (e.g., source range, water column depth, or wind speed) based on available annotations and adopts a multi-task framework to connect these factors to the recognition task.","Furthermore, we integrate an adversarial learning mechanism into the multi-task framework to prompt the model to extract representations that are robust against influential factors.","Through extensive experiments and analyses on the ShipsEar dataset, our proposed adversarial multi-task model demonstrates its capacity to effectively model the influential factors and achieve state-of-the-art performance on the 12-class recognition task."],"url":"http://arxiv.org/abs/2411.02848v1"}
{"created":"2024-11-05 06:34:29","title":"Max-Distance Sparsification for Diversification and Clustering","abstract":"Let $\\mathcal{D}$ be a set family that is the solution domain of some combinatorial problem. The \\emph{max-min diversification problem on $\\mathcal{D}$} is the problem to select $k$ sets from $\\mathcal{D}$ such that the Hamming distance between any two selected sets is at least $d$. FPT algorithms parameterized by $k,l:=\\max_{D\\in \\mathcal{D}}|D|$ and $k,d$ have been actively studied recently for several specific domains.   This paper provides unified algorithmic frameworks to solve this problem. Specifically, for each parameterization $k,l$ and $k,d$, we provide an FPT oracle algorithm for the max-min diversification problem using oracles related to $\\mathcal{D}$. We then demonstrate that our frameworks generalize most of the existing domain-specific tractability results and provide the first FPT algorithms for several domains.   Our main technical breakthrough is introducing the notion of \\emph{max-distance sparsifier} of $\\mathcal{D}$, a domain on which the max-min diversification problem is equivalent to the same problem on the original domain $\\mathcal{D}$. The core of our framework is to design FPT oracle algorithms that construct a constant-size max-distance sparsifier of $\\mathcal{D}$. Using max-distance sparsifiers, we provide FPT algorithms for the max-min and max-sum diversification problems on $\\mathcal{D}$, as well as $k$-center and $k$-sum-of-radii clustering problems on $\\mathcal{D}$, which are also natural problems in the context of diversification and have their own interests.","sentences":["Let $\\mathcal{D}$ be a set family that is the solution domain of some combinatorial problem.","The \\emph{max-min diversification problem on $\\mathcal{D}$} is the problem to select $k$ sets from $\\mathcal{D}$ such that the Hamming distance between any two selected sets is at least $d$. FPT algorithms parameterized by $k,l:=\\max_{D\\in \\mathcal{D}}|D|$ and $k,d$ have been actively studied recently for several specific domains.   ","This paper provides unified algorithmic frameworks to solve this problem.","Specifically, for each parameterization $k,l$ and $k,d$, we provide an FPT oracle algorithm for the max-min diversification problem using oracles related to $\\mathcal{D}$. We then demonstrate that our frameworks generalize most of the existing domain-specific tractability results and provide the first FPT algorithms for several domains.   ","Our main technical breakthrough is introducing the notion of \\emph{max-distance sparsifier} of $\\mathcal{D}$, a domain on which the max-min diversification problem is equivalent to the same problem on the original domain $\\mathcal{D}$. The core of our framework is to design FPT oracle algorithms that construct a constant-size max-distance sparsifier of $\\mathcal{D}$. Using max-distance sparsifiers, we provide FPT algorithms for the max-min and max-sum diversification problems on $\\mathcal{D}$, as well as $k$-center and $k$-sum-of-radii clustering problems on $\\mathcal{D}$, which are also natural problems in the context of diversification and have their own interests."],"url":"http://arxiv.org/abs/2411.02845v1"}
{"created":"2024-11-05 06:23:44","title":"Test-Time Dynamic Image Fusion","abstract":"The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources. Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field. Is it possible to conduct dynamic image fusion with a clear theoretical justification? In this paper, we give our solution from a generalization perspective. We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm. It provably reduces the upper bound of generalization error. Specifically, we decompose the fused image into multiple components corresponding to its source data. The decomposed components represent the effective information from the source data, thus the gap between them reflects the Relative Dominability (RD) of the uni-source data in constructing the fusion image. Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss. Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results. Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority. Our code is available at https://github.com/Yinan-Xia/TTD.","sentences":["The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources.","Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field.","Is it possible to conduct dynamic image fusion with a clear theoretical justification?","In this paper, we give our solution from a generalization perspective.","We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm.","It provably reduces the upper bound of generalization error.","Specifically, we decompose the fused image into multiple components corresponding to its source data.","The decomposed components represent the effective information from the source data, thus the gap between them reflects the Relative Dominability (RD) of the uni-source data in constructing the fusion image.","Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss.","Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results.","Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority.","Our code is available at https://github.com/Yinan-Xia/TTD."],"url":"http://arxiv.org/abs/2411.02840v1"}
{"created":"2024-11-05 06:21:17","title":"On the Comparison between Multi-modal and Single-modal Contrastive Learning","abstract":"Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning. By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit impressive robustness and transferability. Despite its empirical success, the theoretical understanding is still in its infancy, especially regarding its comparison with single-modal contrastive learning. In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning. Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function. Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning. Through the cooperation between the two modalities, multi-modal learning can achieve better feature learning, leading to improvements in performance in downstream tasks compared to single-modal learning. Our analysis provides a unified framework that can characterize the optimization and generalization of both single-modal and multi-modal contrastive learning. Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings.","sentences":["Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning.","By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit impressive robustness and transferability.","Despite its empirical success, the theoretical understanding is still in its infancy, especially regarding its comparison with single-modal contrastive learning.","In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning.","Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function.","Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning.","Through the cooperation between the two modalities, multi-modal learning can achieve better feature learning, leading to improvements in performance in downstream tasks compared to single-modal learning.","Our analysis provides a unified framework that can characterize the optimization and generalization of both single-modal and multi-modal contrastive learning.","Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings."],"url":"http://arxiv.org/abs/2411.02837v1"}
{"created":"2024-11-05 05:44:33","title":"Faster Exact and Parameterized Algorithm for Feedback Vertex Set in Bipartite Tournaments","abstract":"A {\\em bipartite tournament} is a directed graph $T:=(A \\cup B, E)$ such that every pair of vertices $(a,b), a\\in A,b\\in B$ are connected by an arc, and no arc connects two vertices of $A$ or two vertices of $B$. A {\\em feedback vertex set} is a set $S$ of vertices in $T$ such that $T - S$ is acyclic. In this article we consider the {\\sc Feedback Vertex Set} problem in bipartite tournaments. Here the input is a bipartite tournament $T$ on $n$ vertices together with an integer $k$, and the task is to determine whether $T$ has a feedback vertex set of size at most $k$. We give a new algorithm for {\\sc Feedback Vertex Set in Bipartite Tournaments}. The running time of our algorithm is upper-bounded by $O(1.6181^k + n^{O(1)})$, improving over the previously best known algorithm with running time $2^kk^{O(1)} + n^{O(1)}$ [Hsiao, ISAAC 2011]. As a by-product, we also obtain the fastest currently known exact exponential-time algorithm for the problem, with running time $O(1.3820^n)$.","sentences":["A {\\em bipartite tournament} is a directed graph $T:=(A \\cup B, E)$ such that every pair of vertices $(a,b), a\\in A,b\\in B$ are connected by an arc, and no arc connects two vertices of $A$ or two vertices of $B$. A {\\em feedback vertex set} is a set $S$ of vertices in $T$ such that $T - S$ is acyclic.","In this article we consider the {\\sc Feedback Vertex Set} problem in bipartite tournaments.","Here the input is a bipartite tournament $T$ on $n$ vertices together with an integer $k$, and the task is to determine whether $T$ has a feedback vertex set of size at most $k$. We give a new algorithm for {\\sc Feedback Vertex Set in Bipartite Tournaments}.","The running time of our algorithm is upper-bounded by $O(1.6181^k + n^{O(1)})$, improving over the previously best known algorithm with running time $2^kk^{O(1)}","+ n^{O(1)}$","[Hsiao, ISAAC 2011].","As a by-product, we also obtain the fastest currently known exact exponential-time algorithm for the problem, with running time $O(1.3820^n)$."],"url":"http://arxiv.org/abs/2411.02821v1"}
{"created":"2024-11-05 05:41:41","title":"DroidSpeak: Enhancing Cross-LLM Communication","abstract":"In multi-agent systems utilizing Large Language Models (LLMs), communication between agents traditionally relies on natural language. This communication often includes the full context of the query so far, which can introduce significant prefill-phase latency, especially with long contexts.   We introduce DroidSpeak, a novel framework to target this cross-LLM communication by leveraging the reuse of intermediate data, such as input embeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the need to reprocess entire contexts for fine-tuned versions of the same foundational model. This approach allows faster context integration while maintaining the quality of task performance. Experimental evaluations demonstrate DroidSpeak's ability to significantly accelerate inter-agent communication, achieving up to a 2.78x speedup in prefill latency with negligible loss in accuracy. Our findings underscore the potential to create more efficient and scalable multi-agent systems.","sentences":["In multi-agent systems utilizing Large Language Models (LLMs), communication between agents traditionally relies on natural language.","This communication often includes the full context of the query so far, which can introduce significant prefill-phase latency, especially with long contexts.   ","We introduce DroidSpeak, a novel framework to target this cross-LLM communication by leveraging the reuse of intermediate data, such as input embeddings (E-cache) and key-value caches (KV-cache).","We efficiently bypass the need to reprocess entire contexts for fine-tuned versions of the same foundational model.","This approach allows faster context integration while maintaining the quality of task performance.","Experimental evaluations demonstrate DroidSpeak's ability to significantly accelerate inter-agent communication, achieving up to a 2.78x speedup in prefill latency with negligible loss in accuracy.","Our findings underscore the potential to create more efficient and scalable multi-agent systems."],"url":"http://arxiv.org/abs/2411.02820v1"}
{"created":"2024-11-05 05:36:17","title":"LiVOS: Light Video Object Segmentation with Gated Linear Matching","abstract":"Semi-supervised video object segmentation (VOS) has been largely driven by space-time memory (STM) networks, which store past frame features in a spatiotemporal memory to segment the current frame via softmax attention. However, STM networks face memory limitations due to the quadratic complexity of softmax matching, restricting their applicability as video length and resolution increase. To address this, we propose LiVOS, a lightweight memory network that employs linear matching via linear attention, reformulating memory matching into a recurrent process that reduces the quadratic attention matrix to a constant-size, spatiotemporal-agnostic 2D state. To enhance selectivity, we introduce gated linear matching, where a data-dependent gate matrix is multiplied with the state matrix to control what information to retain or discard. Experiments on diverse benchmarks demonstrated the effectiveness of our method. It achieved 64.8 J&F on MOSE and 85.1 J&F on DAVIS, surpassing all non-STM methods and narrowing the gap with STM-based approaches. For longer and higher-resolution videos, it matched STM-based methods with 53% less GPU memory and supports 4096p inference on a 32G consumer-grade GPU--a previously cost-prohibitive capability--opening the door for long and high-resolution video foundation models.","sentences":["Semi-supervised video object segmentation (VOS) has been largely driven by space-time memory (STM) networks, which store past frame features in a spatiotemporal memory to segment the current frame via softmax attention.","However, STM networks face memory limitations due to the quadratic complexity of softmax matching, restricting their applicability as video length and resolution increase.","To address this, we propose LiVOS, a lightweight memory network that employs linear matching via linear attention, reformulating memory matching into a recurrent process that reduces the quadratic attention matrix to a constant-size, spatiotemporal-agnostic 2D state.","To enhance selectivity, we introduce gated linear matching, where a data-dependent gate matrix is multiplied with the state matrix to control what information to retain or discard.","Experiments on diverse benchmarks demonstrated the effectiveness of our method.","It achieved 64.8 J&F on MOSE and 85.1 J&F on DAVIS, surpassing all non-STM methods and narrowing the gap with STM-based approaches.","For longer and higher-resolution videos, it matched STM-based methods with 53% less GPU memory and supports 4096p inference on a 32G consumer-grade GPU--a previously cost-prohibitive capability--opening the door for long and high-resolution video foundation models."],"url":"http://arxiv.org/abs/2411.02818v1"}
{"created":"2024-11-05 05:30:39","title":"Conditional Vendi Score: An Information-Theoretic Approach to Diversity Evaluation of Prompt-based Generative Models","abstract":"Text-conditioned generation models are commonly evaluated based on the quality of the generated data and its alignment with the input text prompt. On the other hand, several applications of prompt-based generative models require sufficient diversity in the generated data to ensure the models' capability of generating image and video samples possessing a variety of features. However, most existing diversity metrics are designed for unconditional generative models, and thus cannot distinguish the diversity arising from variations in text prompts and that contributed by the generative model itself. In this work, our goal is to quantify the prompt-induced and model-induced diversity in samples generated by prompt-based models. We propose an information-theoretic approach for internal diversity quantification, where we decompose the kernel-based entropy $H(X)$ of the generated data $X$ into the sum of the conditional entropy $H(X|T)$, given text variable $T$, and the mutual information $I(X; T)$ between the text and data variables. We introduce the \\emph{Conditional-Vendi} score based on $H(X|T)$ to quantify the internal diversity of the model and the \\emph{Information-Vendi} score based on $I(X; T)$ to measure the statistical relevance between the generated data and text prompts. We provide theoretical results to statistically interpret these scores and relate them to the unconditional Vendi score. We conduct several numerical experiments to show the correlation between the Conditional-Vendi score and the internal diversity of text-conditioned generative models. The codebase is available at \\href{https://github.com/mjalali/conditional-vendi}{https://github.com/mjalali/conditional-vendi}.","sentences":["Text-conditioned generation models are commonly evaluated based on the quality of the generated data and its alignment with the input text prompt.","On the other hand, several applications of prompt-based generative models require sufficient diversity in the generated data to ensure the models' capability of generating image and video samples possessing a variety of features.","However, most existing diversity metrics are designed for unconditional generative models, and thus cannot distinguish the diversity arising from variations in text prompts and that contributed by the generative model itself.","In this work, our goal is to quantify the prompt-induced and model-induced diversity in samples generated by prompt-based models.","We propose an information-theoretic approach for internal diversity quantification, where we decompose the kernel-based entropy $H(X)$ of the generated data $X$ into the sum of the conditional entropy $H(X|T)$, given text variable $T$, and the mutual information $I(X; T)$ between the text and data variables.","We introduce the \\emph{Conditional-Vendi} score based on $H(X|T)$ to quantify the internal diversity of the model and the \\emph{Information-Vendi} score based on $I(X; T)$ to measure the statistical relevance between the generated data and text prompts.","We provide theoretical results to statistically interpret these scores and relate them to the unconditional Vendi score.","We conduct several numerical experiments to show the correlation between the Conditional-Vendi score and the internal diversity of text-conditioned generative models.","The codebase is available at \\href{https://github.com/mjalali/conditional-vendi}{https://github.com/mjalali/conditional-vendi}."],"url":"http://arxiv.org/abs/2411.02817v1"}
{"created":"2024-11-05 05:19:09","title":"Sparse Orthogonal Parameters Tuning for Continual Learning","abstract":"Continual learning methods based on pre-trained models (PTM) have recently gained attention which adapt to successive downstream tasks without catastrophic forgetting. These methods typically refrain from updating the pre-trained parameters and instead employ additional adapters, prompts, and classifiers. In this paper, we from a novel perspective investigate the benefit of sparse orthogonal parameters for continual learning. We found that merging sparse orthogonality of models learned from multiple streaming tasks has great potential in addressing catastrophic forgetting. Leveraging this insight, we propose a novel yet effective method called SoTU (Sparse Orthogonal Parameters TUning). We hypothesize that the effectiveness of SoTU lies in the transformation of knowledge learned from multiple domains into the fusion of orthogonal delta parameters. Experimental evaluations on diverse CL benchmarks demonstrate the effectiveness of the proposed approach. Notably, SoTU achieves optimal feature representation for streaming data without necessitating complex classifier designs, making it a Plug-and-Play solution.","sentences":["Continual learning methods based on pre-trained models (PTM) have recently gained attention which adapt to successive downstream tasks without catastrophic forgetting.","These methods typically refrain from updating the pre-trained parameters and instead employ additional adapters, prompts, and classifiers.","In this paper, we from a novel perspective investigate the benefit of sparse orthogonal parameters for continual learning.","We found that merging sparse orthogonality of models learned from multiple streaming tasks has great potential in addressing catastrophic forgetting.","Leveraging this insight, we propose a novel yet effective method called SoTU (Sparse Orthogonal Parameters TUning).","We hypothesize that the effectiveness of SoTU lies in the transformation of knowledge learned from multiple domains into the fusion of orthogonal delta parameters.","Experimental evaluations on diverse CL benchmarks demonstrate the effectiveness of the proposed approach.","Notably, SoTU achieves optimal feature representation for streaming data without necessitating complex classifier designs, making it a Plug-and-Play solution."],"url":"http://arxiv.org/abs/2411.02813v1"}
{"created":"2024-11-05 04:52:20","title":"Query-Efficient Adversarial Attack Against Vertical Federated Graph Learning","abstract":"Graph neural network (GNN) has captured wide attention due to its capability of graph representation learning for graph-structured data. However, the distributed data silos limit the performance of GNN. Vertical federated learning (VFL), an emerging technique to process distributed data, successfully makes GNN possible to handle the distributed graph-structured data. Despite the prosperous development of vertical federated graph learning (VFGL), the robustness of VFGL against the adversarial attack has not been explored yet. Although numerous adversarial attacks against centralized GNNs are proposed, their attack performance is challenged in the VFGL scenario. To the best of our knowledge, this is the first work to explore the adversarial attack against VFGL. A query-efficient hybrid adversarial attack framework is proposed to significantly improve the centralized adversarial attacks against VFGL, denoted as NA2, short for Neuron-based Adversarial Attack. Specifically, a malicious client manipulates its local training data to improve its contribution in a stealthy fashion. Then a shadow model is established based on the manipulated data to simulate the behavior of the server model in VFGL. As a result, the shadow model can improve the attack success rate of various centralized attacks with a few queries. Extensive experiments on five real-world benchmarks demonstrate that NA2 improves the performance of the centralized adversarial attacks against VFGL, achieving state-of-the-art performance even under potential adaptive defense where the defender knows the attack method. Additionally, we provide interpretable experiments of the effectiveness of NA2 via sensitive neurons identification and visualization of t-SNE.","sentences":["Graph neural network (GNN) has captured wide attention due to its capability of graph representation learning for graph-structured data.","However, the distributed data silos limit the performance of GNN.","Vertical federated learning (VFL), an emerging technique to process distributed data, successfully makes GNN possible to handle the distributed graph-structured data.","Despite the prosperous development of vertical federated graph learning (VFGL), the robustness of VFGL against the adversarial attack has not been explored yet.","Although numerous adversarial attacks against centralized GNNs are proposed, their attack performance is challenged in the VFGL scenario.","To the best of our knowledge, this is the first work to explore the adversarial attack against VFGL.","A query-efficient hybrid adversarial attack framework is proposed to significantly improve the centralized adversarial attacks against VFGL, denoted as NA2, short for Neuron-based Adversarial Attack.","Specifically, a malicious client manipulates its local training data to improve its contribution in a stealthy fashion.","Then a shadow model is established based on the manipulated data to simulate the behavior of the server model in VFGL.","As a result, the shadow model can improve the attack success rate of various centralized attacks with a few queries.","Extensive experiments on five real-world benchmarks demonstrate that NA2 improves the performance of the centralized adversarial attacks against VFGL, achieving state-of-the-art performance even under potential adaptive defense where the defender knows the attack method.","Additionally, we provide interpretable experiments of the effectiveness of NA2 via sensitive neurons identification and visualization of t-SNE."],"url":"http://arxiv.org/abs/2411.02809v1"}
{"created":"2024-11-05 04:20:06","title":"ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather Condition by Unified Image-Adaptive Processing","abstract":"We propose an image-adaptive object detection method for adverse weather conditions such as fog and low-light. Our framework employs differentiable preprocessing filters to perform image enhancement suitable for later-stage object detections. Our framework introduces two differentiable filters: a B\\'ezier curve-based pixel-wise (BPW) filter and a kernel-based local (KBL) filter. These filters unify the functions of classical image processing filters and improve performance of object detection. We also propose a domain-agnostic data augmentation strategy using the BPW filter. Our method does not require data-specific customization of the filter combinations, parameter ranges, and data augmentation. We evaluate our proposed approach, called Enhanced Robustness by Unified Image Processing (ERUP)-YOLO, by applying it to the YOLOv3 detector. Experiments on adverse weather datasets demonstrate that our proposed filters match or exceed the expressiveness of conventional methods and our ERUP-YOLO achieved superior performance in a wide range of adverse weather conditions, including fog and low-light conditions.","sentences":["We propose an image-adaptive object detection method for adverse weather conditions such as fog and low-light.","Our framework employs differentiable preprocessing filters to perform image enhancement suitable for later-stage object detections.","Our framework introduces two differentiable filters: a B\\'ezier curve-based pixel-wise (BPW) filter and a kernel-based local (KBL) filter.","These filters unify the functions of classical image processing filters and improve performance of object detection.","We also propose a domain-agnostic data augmentation strategy using the BPW filter.","Our method does not require data-specific customization of the filter combinations, parameter ranges, and data augmentation.","We evaluate our proposed approach, called Enhanced Robustness by Unified Image Processing (ERUP)-YOLO, by applying it to the YOLOv3 detector.","Experiments on adverse weather datasets demonstrate that our proposed filters match or exceed the expressiveness of conventional methods and our ERUP-YOLO achieved superior performance in a wide range of adverse weather conditions, including fog and low-light conditions."],"url":"http://arxiv.org/abs/2411.02799v1"}
{"created":"2024-11-05 04:10:59","title":"Specialized Foundation Models Struggle to Beat Supervised Baselines","abstract":"Following its success for vision and text, the \"foundation model\" (FM) paradigm -- pretraining large models on massive data, then fine-tuning on target tasks -- has rapidly expanded to domains in the sciences, engineering, healthcare, and beyond. Has this achieved what the original FMs accomplished, i.e. the supplanting of traditional supervised learning in their domains? To answer we look at three modalities -- genomics, satellite imaging, and time series -- with multiple recent FMs and compare them to a standard supervised learning workflow: model development, hyperparameter tuning, and training, all using only data from the target task. Across these three specialized domains, we find that it is consistently possible to train simple supervised models -- no more complicated than a lightly modified wide ResNet or UNet -- that match or even outperform the latest foundation models. Our work demonstrates that the benefits of large-scale pretraining have yet to be realized in many specialized areas, reinforces the need to compare new FMs to strong, well-tuned baselines, and introduces two new, easy-to-use, open-source, and automated workflows for doing so.","sentences":["Following its success for vision and text, the \"foundation model\" (FM) paradigm -- pretraining large models on massive data, then fine-tuning on target tasks -- has rapidly expanded to domains in the sciences, engineering, healthcare, and beyond.","Has this achieved what the original FMs accomplished, i.e. the supplanting of traditional supervised learning in their domains?","To answer we look at three modalities -- genomics, satellite imaging, and time series -- with multiple recent FMs and compare them to a standard supervised learning workflow: model development, hyperparameter tuning, and training, all using only data from the target task.","Across these three specialized domains, we find that it is consistently possible to train simple supervised models -- no more complicated than a lightly modified wide ResNet or UNet -- that match or even outperform the latest foundation models.","Our work demonstrates that the benefits of large-scale pretraining have yet to be realized in many specialized areas, reinforces the need to compare new FMs to strong, well-tuned baselines, and introduces two new, easy-to-use, open-source, and automated workflows for doing so."],"url":"http://arxiv.org/abs/2411.02796v1"}
{"created":"2024-11-05 04:08:59","title":"Real-Time Text Detection with Similar Mask in Traffic, Industrial, and Natural Scenes","abstract":"Texts on the intelligent transportation scene include mass information. Fully harnessing this information is one of the critical drivers for advancing intelligent transportation. Unlike the general scene, detecting text in transportation has extra demand, such as a fast inference speed, except for high accuracy. Most existing real-time text detection methods are based on the shrink mask, which loses some geometry semantic information and needs complex post-processing. In addition, the previous method usually focuses on correct output, which ignores feature correction and lacks guidance during the intermediate process. To this end, we propose an efficient multi-scene text detector that contains an effective text representation similar mask (SM) and a feature correction module (FCM). Unlike previous methods, the former aims to preserve the geometric information of the instances as much as possible. Its post-progressing saves 50$\\%$ of the time, accurately and efficiently reconstructing text contours. The latter encourages false positive features to move away from the positive feature center, optimizing the predictions from the feature level. Some ablation studies demonstrate the efficiency of the SM and the effectiveness of the FCM. Moreover, the deficiency of existing traffic datasets (such as the low-quality annotation or closed source data unavailability) motivated us to collect and annotate a traffic text dataset, which introduces motion blur. In addition, to validate the scene robustness of the SM-Net, we conduct experiments on traffic, industrial, and natural scene datasets. Extensive experiments verify it achieves (SOTA) performance on several benchmarks. The code and dataset are available at: \\url{https://github.com/fengmulin/SMNet}.","sentences":["Texts on the intelligent transportation scene include mass information.","Fully harnessing this information is one of the critical drivers for advancing intelligent transportation.","Unlike the general scene, detecting text in transportation has extra demand, such as a fast inference speed, except for high accuracy.","Most existing real-time text detection methods are based on the shrink mask, which loses some geometry semantic information and needs complex post-processing.","In addition, the previous method usually focuses on correct output, which ignores feature correction and lacks guidance during the intermediate process.","To this end, we propose an efficient multi-scene text detector that contains an effective text representation similar mask (SM) and a feature correction module (FCM).","Unlike previous methods, the former aims to preserve the geometric information of the instances as much as possible.","Its post-progressing saves 50$\\%$ of the time, accurately and efficiently reconstructing text contours.","The latter encourages false positive features to move away from the positive feature center, optimizing the predictions from the feature level.","Some ablation studies demonstrate the efficiency of the SM and the effectiveness of the FCM.","Moreover, the deficiency of existing traffic datasets (such as the low-quality annotation or closed source data unavailability) motivated us to collect and annotate a traffic text dataset, which introduces motion blur.","In addition, to validate the scene robustness of the SM-Net, we conduct experiments on traffic, industrial, and natural scene datasets.","Extensive experiments verify it achieves (SOTA) performance on several benchmarks.","The code and dataset are available at: \\url{https://github.com/fengmulin/SMNet}."],"url":"http://arxiv.org/abs/2411.02794v1"}
{"created":"2024-11-05 03:55:25","title":"Memory Augmented Cross-encoders for Controllable Personalized Search","abstract":"Personalized search represents a problem where retrieval models condition on historical user interaction data in order to improve retrieval results. However, personalization is commonly perceived as opaque and not amenable to control by users. Further, personalization necessarily limits the space of items that users are exposed to. Therefore, prior work notes a tension between personalization and users' ability for discovering novel items. While discovery of novel items in personalization setups may be resolved through search result diversification, these approaches do little to allow user control over personalization. Therefore, in this paper, we introduce an approach for controllable personalized search. Our model, CtrlCE presents a novel cross-encoder model augmented with an editable memory constructed from users historical items. Our proposed memory augmentation allows cross-encoder models to condition on large amounts of historical user data and supports interaction from users permitting control over personalization. Further, controllable personalization for search must account for queries which don't require personalization, and in turn user control. For this, we introduce a calibrated mixing model which determines when personalization is necessary. This allows system designers using CtrlCE to only obtain user input for control when necessary. In multiple datasets of personalized search, we show CtrlCE to result in effective personalization as well as fulfill various key goals for controllable personalized search.","sentences":["Personalized search represents a problem where retrieval models condition on historical user interaction data in order to improve retrieval results.","However, personalization is commonly perceived as opaque and not amenable to control by users.","Further, personalization necessarily limits the space of items that users are exposed to.","Therefore, prior work notes a tension between personalization and users' ability for discovering novel items.","While discovery of novel items in personalization setups may be resolved through search result diversification, these approaches do little to allow user control over personalization.","Therefore, in this paper, we introduce an approach for controllable personalized search.","Our model, CtrlCE presents a novel cross-encoder model augmented with an editable memory constructed from users historical items.","Our proposed memory augmentation allows cross-encoder models to condition on large amounts of historical user data and supports interaction from users permitting control over personalization.","Further, controllable personalization for search must account for queries which don't require personalization, and in turn user control.","For this, we introduce a calibrated mixing model which determines when personalization is necessary.","This allows system designers using CtrlCE to only obtain user input for control when necessary.","In multiple datasets of personalized search, we show CtrlCE to result in effective personalization as well as fulfill various key goals for controllable personalized search."],"url":"http://arxiv.org/abs/2411.02790v1"}
{"created":"2024-11-05 03:52:36","title":"Advancing Robust Underwater Acoustic Target Recognition through Multi-task Learning and Multi-Gate Mixture-of-Experts","abstract":"Underwater acoustic target recognition has emerged as a prominent research area within the field of underwater acoustics. However, the current availability of authentic underwater acoustic signal recordings remains limited, which hinders data-driven acoustic recognition models from learning robust patterns of targets from a limited set of intricate underwater signals, thereby compromising their stability in practical applications. To overcome these limitations, this study proposes a recognition framework called M3 (Multi-task, Multi-gate, Multi-expert) to enhance the model's ability to capture robust patterns by making it aware of the inherent properties of targets. In this framework, an auxiliary task that focuses on target properties, such as estimating target size, is designed. The auxiliary task then shares parameters with the recognition task to realize multi-task learning. This paradigm allows the model to concentrate on shared information across tasks and identify robust patterns of targets in a regularized manner, thereby enhancing the model's generalization ability. Moreover, M3 incorporates multi-expert and multi-gate mechanisms, allowing for the allocation of distinct parameter spaces to various underwater signals. This enables the model to process intricate signal patterns in a fine-grained and differentiated manner. To evaluate the effectiveness of M3, extensive experiments were implemented on the ShipsEar underwater ship-radiated noise dataset. The results substantiate that M3 has the ability to outperform the most advanced single-task recognition models, thereby achieving the state-of-the-art performance.","sentences":["Underwater acoustic target recognition has emerged as a prominent research area within the field of underwater acoustics.","However, the current availability of authentic underwater acoustic signal recordings remains limited, which hinders data-driven acoustic recognition models from learning robust patterns of targets from a limited set of intricate underwater signals, thereby compromising their stability in practical applications.","To overcome these limitations, this study proposes a recognition framework called M3 (Multi-task, Multi-gate, Multi-expert) to enhance the model's ability to capture robust patterns by making it aware of the inherent properties of targets.","In this framework, an auxiliary task that focuses on target properties, such as estimating target size, is designed.","The auxiliary task then shares parameters with the recognition task to realize multi-task learning.","This paradigm allows the model to concentrate on shared information across tasks and identify robust patterns of targets in a regularized manner, thereby enhancing the model's generalization ability.","Moreover, M3 incorporates multi-expert and multi-gate mechanisms, allowing for the allocation of distinct parameter spaces to various underwater signals.","This enables the model to process intricate signal patterns in a fine-grained and differentiated manner.","To evaluate the effectiveness of M3, extensive experiments were implemented on the ShipsEar underwater ship-radiated noise dataset.","The results substantiate that M3 has the ability to outperform the most advanced single-task recognition models, thereby achieving the state-of-the-art performance."],"url":"http://arxiv.org/abs/2411.02787v1"}
{"created":"2024-11-05 03:45:17","title":"How much is a noisy image worth? Data Scaling Laws for Ambient Diffusion","abstract":"The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more than $80$ models on data with different corruption levels across three datasets ranging from $30,000$ to $\\approx 1.3$M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g.~$10\\%$ of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments.","sentences":["The quality of generative models depends on the quality of the data they are trained on.","Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints.","Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data.","We study this phenomenon at scale by training more than $80$ models on data with different corruption levels across three datasets ranging from $30,000$ to $\\approx 1.3$M samples.","We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data.","Yet, a combination of a small set of clean data (e.g.~$10\\%$ of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance.","We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances.","Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample.","Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments."],"url":"http://arxiv.org/abs/2411.02780v1"}
{"created":"2024-11-05 03:44:54","title":"Advancing Recycling Efficiency: A Comparative Analysis of Deep Learning Models in Waste Classification","abstract":"With the ongoing increase in the worldwide population and escalating consumption habits,there's a surge in the amount of waste produced.The situation poses considerable challenges for waste management and the optimization of recycling operations.The research tackles the pressing issue of waste classification for recycling by analyzing various deep learning models,including Convolutional Neural Network(CNN),AlexNet,ResNet,ResNet50 plus Support Vector Machine(SVM),and transformers,across a wide array of waste categories.The research meticulously compares these models on several targets like parameters settings,category accuracy,total accuracy and model parameters to establish a uniform evaluation criterion.This research presents a novel method that incorporates SVM with deep learning frameworks,particularly ResNet50.The results indicate the method significantly boosts accuracy in complex waste categories.Moreover,the transformer model outshines others in average accuracy,showcasing its aptitude for intricate classification tasks.To improve performance in poorly performing categories,the research advocates for enlarging the dataset,employing data augmentation,and leveraging sophisticated models such as transformers,along with refining training methodologies.The research paves the way for future advancements in multi-category waste recycling and underscores the pivotal role of deep learning in promoting environmental sustainability.","sentences":["With the ongoing increase in the worldwide population and escalating consumption habits,there's a surge in the amount of waste produced.","The situation poses considerable challenges for waste management and the optimization of recycling operations.","The research tackles the pressing issue of waste classification for recycling by analyzing various deep learning models,including Convolutional Neural Network(CNN),AlexNet,ResNet,ResNet50 plus Support Vector Machine(SVM),and transformers,across a wide array of waste categories.","The research meticulously compares these models on several targets like parameters settings,category accuracy,total accuracy and model parameters to establish a uniform evaluation criterion.","This research presents a novel method that incorporates SVM with deep learning frameworks,particularly ResNet50.The results indicate the method significantly boosts accuracy in complex waste categories.","Moreover,the transformer model outshines others in average accuracy,showcasing its aptitude for intricate classification tasks.","To improve performance in poorly performing categories,the research advocates for enlarging the dataset,employing data augmentation,and leveraging sophisticated models such as transformers,along with refining training methodologies.","The research paves the way for future advancements in multi-category waste recycling and underscores the pivotal role of deep learning in promoting environmental sustainability."],"url":"http://arxiv.org/abs/2411.02779v1"}
{"created":"2024-11-05 03:38:16","title":"Brewing Vodka: Distilling Pure Knowledge for Lightweight Threat Detection in Audit Logs","abstract":"Advanced Persistent Threats (APTs) are continuously evolving, leveraging their stealthiness and persistence to put increasing pressure on current provenance-based Intrusion Detection Systems (IDS). This evolution exposes several critical issues: (1) The dense interaction between malicious and benign nodes within provenance graphs introduces neighbor noise, hindering effective detection; (2) The complex prediction mechanisms of existing APTs detection models lead to the insufficient utilization of prior knowledge embedded in the data; (3) The high computational cost makes detection impractical.   To address these challenges, we propose Vodka, a lightweight threat detection system built on a knowledge distillation framework, capable of node-level detection within audit log provenance graphs. Specifically, Vodka applies graph Laplacian regularization to reduce neighbor noise, obtaining smoothed and denoised graph signals. Subsequently, Vodka employs a teacher model based on GNNs to extract knowledge, which is then distilled into a lightweight student model. The student model is designed as a trainable combination of a feature transformation module and a personalized PageRank random walk label propagation module, with the former capturing feature knowledge and the latter learning label and structural knowledge. After distillation, the student model benefits from the knowledge of the teacher model to perform precise threat detection. Finally, Vodka reconstructs attack paths from anomalous nodes, providing insight into the attackers' strategies. We evaluate Vodka through extensive experiments on three public datasets and compare its performance against several state-of-the-art IDS solutions. The results demonstrate that Vodka achieves outstanding detection accuracy across all scenarios and the detection time is 1.4 to 5.2 times faster than the current state-of-the-art methods.","sentences":["Advanced Persistent Threats (APTs) are continuously evolving, leveraging their stealthiness and persistence to put increasing pressure on current provenance-based Intrusion Detection Systems (IDS).","This evolution exposes several critical issues: (1) The dense interaction between malicious and benign nodes within provenance graphs introduces neighbor noise, hindering effective detection; (2) The complex prediction mechanisms of existing APTs detection models lead to the insufficient utilization of prior knowledge embedded in the data; (3) The high computational cost makes detection impractical.   ","To address these challenges, we propose Vodka, a lightweight threat detection system built on a knowledge distillation framework, capable of node-level detection within audit log provenance graphs.","Specifically, Vodka applies graph Laplacian regularization to reduce neighbor noise, obtaining smoothed and denoised graph signals.","Subsequently, Vodka employs a teacher model based on GNNs to extract knowledge, which is then distilled into a lightweight student model.","The student model is designed as a trainable combination of a feature transformation module and a personalized PageRank random walk label propagation module, with the former capturing feature knowledge and the latter learning label and structural knowledge.","After distillation, the student model benefits from the knowledge of the teacher model to perform precise threat detection.","Finally, Vodka reconstructs attack paths from anomalous nodes, providing insight into the attackers' strategies.","We evaluate Vodka through extensive experiments on three public datasets and compare its performance against several state-of-the-art IDS solutions.","The results demonstrate that Vodka achieves outstanding detection accuracy across all scenarios and the detection time is 1.4 to 5.2 times faster than the current state-of-the-art methods."],"url":"http://arxiv.org/abs/2411.02775v1"}
{"created":"2024-11-05 03:34:53","title":"FedBlock: A Blockchain Approach to Federated Learning against Backdoor Attacks","abstract":"Federated Learning (FL) is a machine learning method for training with private data locally stored in distributed machines without gathering them into one place for central learning. Despite its promises, FL is prone to critical security risks. First, because FL depends on a central server to aggregate local training models, this is a single point of failure. The server might function maliciously. Second, due to its distributed nature, FL might encounter backdoor attacks by participating clients. They can poison the local model before submitting to the server. Either type of attack, on the server or the client side, would severely degrade learning accuracy. We propose FedBlock, a novel blockchain-based FL framework that addresses both of these security risks. FedBlock is uniquely desirable in that it involves only smart contract programming, thus deployable atop any blockchain network. Our framework is substantiated with a comprehensive evaluation study using real-world datasets. Its robustness against backdoor attacks is competitive with the literature of FL backdoor defense. The latter, however, does not address the server risk as we do.","sentences":["Federated Learning (FL) is a machine learning method for training with private data locally stored in distributed machines without gathering them into one place for central learning.","Despite its promises, FL is prone to critical security risks.","First, because FL depends on a central server to aggregate local training models, this is a single point of failure.","The server might function maliciously.","Second, due to its distributed nature, FL might encounter backdoor attacks by participating clients.","They can poison the local model before submitting to the server.","Either type of attack, on the server or the client side, would severely degrade learning accuracy.","We propose FedBlock, a novel blockchain-based FL framework that addresses both of these security risks.","FedBlock is uniquely desirable in that it involves only smart contract programming, thus deployable atop any blockchain network.","Our framework is substantiated with a comprehensive evaluation study using real-world datasets.","Its robustness against backdoor attacks is competitive with the literature of FL backdoor defense.","The latter, however, does not address the server risk as we do."],"url":"http://arxiv.org/abs/2411.02773v1"}
{"created":"2024-11-05 03:26:26","title":"One-Stage-TFS: Thai One-Stage Fingerspelling Dataset for Fingerspelling Recognition Frameworks","abstract":"The Thai One-Stage Fingerspelling (One-Stage-TFS) dataset is a comprehensive resource designed to advance research in hand gesture recognition, explicitly focusing on the recognition of Thai sign language. This dataset comprises 7,200 images capturing 15 one-stage consonant gestures performed by undergraduate students from Rajabhat Maha Sarakham University, Thailand. The contributors include both expert students from the Special Education Department with proficiency in Thai sign language and students from other departments without prior sign language experience. Images were collected between July and December 2021 using a DSLR camera, with contributors demonstrating hand gestures against both simple and complex backgrounds. The One-Stage-TFS dataset presents challenges in detecting and recognizing hand gestures, offering opportunities to develop novel end-to-end recognition frameworks. Researchers can utilize this dataset to explore deep learning methods, such as YOLO, EfficientDet, RetinaNet, and Detectron, for hand detection, followed by feature extraction and recognition using techniques like convolutional neural networks, transformers, and adaptive feature fusion networks. The dataset is accessible via the Mendeley Data repository and supports a wide range of applications in computer science, including deep learning, computer vision, and pattern recognition, thereby encouraging further innovation and exploration in these fields.","sentences":["The Thai One-Stage Fingerspelling (One-Stage-TFS) dataset is a comprehensive resource designed to advance research in hand gesture recognition, explicitly focusing on the recognition of Thai sign language.","This dataset comprises 7,200 images capturing 15 one-stage consonant gestures performed by undergraduate students from Rajabhat Maha Sarakham University, Thailand.","The contributors include both expert students from the Special Education Department with proficiency in Thai sign language and students from other departments without prior sign language experience.","Images were collected between July and December 2021 using a DSLR camera, with contributors demonstrating hand gestures against both simple and complex backgrounds.","The One-Stage-TFS dataset presents challenges in detecting and recognizing hand gestures, offering opportunities to develop novel end-to-end recognition frameworks.","Researchers can utilize this dataset to explore deep learning methods, such as YOLO, EfficientDet, RetinaNet, and Detectron, for hand detection, followed by feature extraction and recognition using techniques like convolutional neural networks, transformers, and adaptive feature fusion networks.","The dataset is accessible via the Mendeley Data repository and supports a wide range of applications in computer science, including deep learning, computer vision, and pattern recognition, thereby encouraging further innovation and exploration in these fields."],"url":"http://arxiv.org/abs/2411.02768v1"}
{"created":"2024-11-05 03:20:14","title":"Fast, robust approximate message passing","abstract":"We give a fast, spectral procedure for implementing approximate-message passing (AMP) algorithms robustly. For any quadratic optimization problem over symmetric matrices $X$ with independent subgaussian entries, and any separable AMP algorithm $\\mathcal A$, our algorithm performs a spectral pre-processing step and then mildly modifies the iterates of $\\mathcal A$. If given the perturbed input $X + E \\in \\mathbb R^{n \\times n}$ for any $E$ supported on a $\\varepsilon n \\times \\varepsilon n$ principal minor, our algorithm outputs a solution $\\hat v$ which is guaranteed to be close to the output of $\\mathcal A$ on the uncorrupted $X$, with $\\|\\mathcal A(X) - \\hat v\\|_2 \\le f(\\varepsilon) \\|\\mathcal A(X)\\|_2$ where $f(\\varepsilon) \\to 0$ as $\\varepsilon \\to 0$ depending only on $\\varepsilon$.","sentences":["We give a fast, spectral procedure for implementing approximate-message passing (AMP) algorithms robustly.","For any quadratic optimization problem over symmetric matrices $X$ with independent subgaussian entries, and any separable AMP algorithm $\\mathcal A$, our algorithm performs a spectral pre-processing step and then mildly modifies the iterates of $\\mathcal A$.","If given the perturbed input $X + E \\in \\mathbb R^{n \\times n}$ for any $E$ supported on a $\\varepsilon n \\times \\varepsilon n$ principal minor, our algorithm outputs a solution $\\hat v$ which is guaranteed to be close to the output of $\\mathcal A$ on the uncorrupted $X$, with $\\|\\mathcal A(X) - \\hat v\\|_2 \\le f(\\varepsilon) \\|\\mathcal A(X)\\|_2$ where $f(\\varepsilon) \\to 0$ as $\\varepsilon \\to 0$ depending only on $\\varepsilon$."],"url":"http://arxiv.org/abs/2411.02764v1"}
{"created":"2024-11-05 03:02:01","title":"Energy Efficient and Balanced Task Assignment Strategy for Multi-UAV Patrol Inspection System in Mobile Edge Computing Network","abstract":"This paper considers a patrol inspection scenario where multiple unmanned aerial vehicles (UAVs) are adopted to traverse multiple predetermined cruise points for data collection. The UAVs are connected to cellular networks and they would offload the collected data to the ground base stations (GBSs) for data processing within the constrained duration. This paper proposes a balanced task assignment strategy among patrol UAVs and an energy-efficient trajectory design method. Through jointly optimizing the cruise point assignment, communication scheduling, computational allocation, and UAV trajectory, a novel solution can be obtained to balance the multiple UAVs' task completion time and minimize the total energy consumption. Firstly, we propose a novel clustering method that considers geometry topology, communication rate, and offload volume; it can determine each UAV's cruise points and balance the UAVs' patrol task. Secondly, a hybrid Time-Energy traveling salesman problem is formulated to analyze the cruise point traversal sequence, and the energy-efficient UAV trajectory can be designed by adopting the successive convex approximation (SCA) technique and block coordinate descent (BCD) scheme. The numerical results demonstrate that the proposed balanced task assignment strategy can efficiently balance the multiple UAVs' tasks. Moreover, the min-max task completion time and total energy consumption performance of the proposed solution outperform that of the current conventional approach.","sentences":["This paper considers a patrol inspection scenario where multiple unmanned aerial vehicles (UAVs) are adopted to traverse multiple predetermined cruise points for data collection.","The UAVs are connected to cellular networks and they would offload the collected data to the ground base stations (GBSs) for data processing within the constrained duration.","This paper proposes a balanced task assignment strategy among patrol UAVs and an energy-efficient trajectory design method.","Through jointly optimizing the cruise point assignment, communication scheduling, computational allocation, and UAV trajectory, a novel solution can be obtained to balance the multiple UAVs' task completion time and minimize the total energy consumption.","Firstly, we propose a novel clustering method that considers geometry topology, communication rate, and offload volume; it can determine each UAV's cruise points and balance the UAVs' patrol task.","Secondly, a hybrid Time-Energy traveling salesman problem is formulated to analyze the cruise point traversal sequence, and the energy-efficient UAV trajectory can be designed by adopting the successive convex approximation (SCA) technique and block coordinate descent (BCD) scheme.","The numerical results demonstrate that the proposed balanced task assignment strategy can efficiently balance the multiple UAVs' tasks.","Moreover, the min-max task completion time and total energy consumption performance of the proposed solution outperform that of the current conventional approach."],"url":"http://arxiv.org/abs/2411.02757v1"}
{"created":"2024-11-05 02:50:47","title":"Label Critic: Design Data Before Models","abstract":"As medical datasets rapidly expand, creating detailed annotations of different body structures becomes increasingly expensive and time-consuming. We consider that requesting radiologists to create detailed annotations is unnecessarily burdensome and that pre-existing AI models can largely automate this process. Following the spirit don't use a sledgehammer on a nut, we find that, rather than creating annotations from scratch, radiologists only have to review and edit errors if the Best-AI Labels have mistakes. To obtain the Best-AI Labels among multiple AI Labels, we developed an automatic tool, called Label Critic, that can assess label quality through tireless pairwise comparisons. Extensive experiments demonstrate that, when incorporated with our developed Image-Prompt pairs, pre-existing Large Vision-Language Models (LVLM), trained on natural images and texts, achieve 96.5% accuracy when choosing the best label in a pair-wise comparison, without extra fine-tuning. By transforming the manual annotation task (30-60 min/scan) into an automatic comparison task (15 sec/scan), we effectively reduce the manual efforts required from radiologists by an order of magnitude. When the Best-AI Labels are sufficiently accurate (81% depending on body structures), they will be directly adopted as the gold-standard annotations for the dataset, with lower-quality AI Labels automatically discarded. Label Critic can also check the label quality of a single AI Label with 71.8% accuracy when no alternatives are available for comparison, prompting radiologists to review and edit if the estimated quality is low (19% depending on body structures).","sentences":["As medical datasets rapidly expand, creating detailed annotations of different body structures becomes increasingly expensive and time-consuming.","We consider that requesting radiologists to create detailed annotations is unnecessarily burdensome and that pre-existing AI models can largely automate this process.","Following the spirit don't use a sledgehammer on a nut, we find that, rather than creating annotations from scratch, radiologists only have to review and edit errors if the Best-AI Labels have mistakes.","To obtain the Best-AI Labels among multiple AI Labels, we developed an automatic tool, called Label Critic, that can assess label quality through tireless pairwise comparisons.","Extensive experiments demonstrate that, when incorporated with our developed Image-Prompt pairs, pre-existing Large Vision-Language Models (LVLM), trained on natural images and texts, achieve 96.5% accuracy when choosing the best label in a pair-wise comparison, without extra fine-tuning.","By transforming the manual annotation task (30-60 min/scan) into an automatic comparison task (15 sec/scan), we effectively reduce the manual efforts required from radiologists by an order of magnitude.","When the Best-AI Labels are sufficiently accurate (81% depending on body structures), they will be directly adopted as the gold-standard annotations for the dataset, with lower-quality AI Labels automatically discarded.","Label Critic can also check the label quality of a single AI Label with 71.8% accuracy when no alternatives are available for comparison, prompting radiologists to review and edit if the estimated quality is low (19% depending on body structures)."],"url":"http://arxiv.org/abs/2411.02753v1"}
{"created":"2024-11-05 02:46:14","title":"Sampling Permutations Satisfying Constraints within and beyond the Local Lemma Regime","abstract":"Sampling a random permutation with restricted positions, or equivalently approximating the permanent of a 0-1 matrix, is a fundamental problem in computer science, with several notable results attained through the years. In this paper, we first improves the running time of the algorithms for a single permutation. We propose a fast approximation algorithm for the permanent of $\\gamma$-dense 0-1 matrix, with an expected running time of $\\tilde{O}\\left(n^{2+(1-\\gamma)/(2\\gamma - 1)}\\right)$. Our result removes the $n^4$ term from the previous best runtime and provides an improvement for $\\gamma \\geq 0.6$. When $\\gamma = o(1)$, our runtime is $\\tilde{\\Theta}(n^2)$, which is nearly optimal for this problem. The core of our proof is to demonstrate that the Sinkhorn algorithm, a fundamental tool in matrix scaling, can achieve maximum accuracy of $1/\\text{poly}(n)$ for dense matrices in $O(\\log n)$ iterations.   We further introduce a general model called permutations with disjunctive constraints (PDC) for handling multiple constrained permutations. We propose a novel Markov chain-based algorithm for sampling nearly uniform solutions of PDC within a Lov${\\'a}$sz Local Lemma (LLL)-like regime by a novel sampling framework called correlated factorization. For uniform PDC formulas, where all constraints are of the same length and all permutations are of equal size, our algorithm runs in nearly linear time with respect to the number of variables.","sentences":["Sampling a random permutation with restricted positions, or equivalently approximating the permanent of a 0-1 matrix, is a fundamental problem in computer science, with several notable results attained through the years.","In this paper, we first improves the running time of the algorithms for a single permutation.","We propose a fast approximation algorithm for the permanent of $\\gamma$-dense 0-1 matrix, with an expected running time of $\\tilde{O}\\left(n^{2+(1-\\gamma)/(2\\gamma - 1)}\\right)$. Our result removes the $n^4$ term from the previous best runtime and provides an improvement for $\\gamma \\geq 0.6$. When $\\gamma = o(1)$, our runtime is $\\tilde{\\Theta}(n^2)$, which is nearly optimal for this problem.","The core of our proof is to demonstrate that the Sinkhorn algorithm, a fundamental tool in matrix scaling, can achieve maximum accuracy of $1/\\text{poly}(n)$ for dense matrices in $O(\\log n)$ iterations.   ","We further introduce a general model called permutations with disjunctive constraints (PDC) for handling multiple constrained permutations.","We propose a novel Markov chain-based algorithm for sampling nearly uniform solutions of PDC within a Lov${\\'a}$sz Local Lemma (LLL)-like regime by a novel sampling framework called correlated factorization.","For uniform PDC formulas, where all constraints are of the same length and all permutations are of equal size, our algorithm runs in nearly linear time with respect to the number of variables."],"url":"http://arxiv.org/abs/2411.02750v1"}
{"created":"2024-11-05 02:29:51","title":"Sensitivity Lower Bounds for Approximaiton Algorithms","abstract":"Sensitivity measures how much the output of an algorithm changes, in terms of Hamming distance, when part of the input is modified. While approximation algorithms with low sensitivity have been developed for many problems, no sensitivity lower bounds were previously known for approximation algorithms. In this work, we establish the first polynomial lower bound on the sensitivity of (randomized) approximation algorithms for constraint satisfaction problems (CSPs) by adapting the probabilistically checkable proof (PCP) framework to preserve sensitivity lower bounds. From this, we derive polynomial sensitivity lower bounds for approximation algorithms for a variety of problems, including maximum clique, minimum vertex cover, and maximum cut.   Given the connection between sensitivity and distributed algorithms, our sensitivity lower bounds also allow us to recover various round complexity lower bounds for distributed algorithms in the LOCAL model. Additionally, we present new lower bounds for distributed CSPs.","sentences":["Sensitivity measures how much the output of an algorithm changes, in terms of Hamming distance, when part of the input is modified.","While approximation algorithms with low sensitivity have been developed for many problems, no sensitivity lower bounds were previously known for approximation algorithms.","In this work, we establish the first polynomial lower bound on the sensitivity of (randomized) approximation algorithms for constraint satisfaction problems (CSPs) by adapting the probabilistically checkable proof (PCP) framework to preserve sensitivity lower bounds.","From this, we derive polynomial sensitivity lower bounds for approximation algorithms for a variety of problems, including maximum clique, minimum vertex cover, and maximum cut.   ","Given the connection between sensitivity and distributed algorithms, our sensitivity lower bounds also allow us to recover various round complexity lower bounds for distributed algorithms in the LOCAL model.","Additionally, we present new lower bounds for distributed CSPs."],"url":"http://arxiv.org/abs/2411.02744v1"}
{"created":"2024-11-05 02:16:23","title":"An information-matching approach to optimal experimental design and active learning","abstract":"The efficacy of mathematical models heavily depends on the quality of the training data, yet collecting sufficient data is often expensive and challenging. Many modeling applications require inferring parameters only as a means to predict other quantities of interest (QoI). Because models often contain many unidentifiable (sloppy) parameters, QoIs often depend on a relatively small number of parameter combinations. Therefore, we introduce an information-matching criterion based on the Fisher Information Matrix to select the most informative training data from a candidate pool. This method ensures that the selected data contain sufficient information to learn only those parameters that are needed to constrain downstream QoIs. It is formulated as a convex optimization problem, making it scalable to large models and datasets. We demonstrate the effectiveness of this approach across various modeling problems in diverse scientific fields, including power systems and underwater acoustics. Finally, we use information-matching as a query function within an Active Learning loop for material science applications. In all these applications, we find that a relatively small set of optimal training data can provide the necessary information for achieving precise predictions. These results are encouraging for diverse future applications, particularly active learning in large machine learning models.","sentences":["The efficacy of mathematical models heavily depends on the quality of the training data, yet collecting sufficient data is often expensive and challenging.","Many modeling applications require inferring parameters only as a means to predict other quantities of interest (QoI).","Because models often contain many unidentifiable (sloppy) parameters, QoIs often depend on a relatively small number of parameter combinations.","Therefore, we introduce an information-matching criterion based on the Fisher Information Matrix to select the most informative training data from a candidate pool.","This method ensures that the selected data contain sufficient information to learn only those parameters that are needed to constrain downstream QoIs.","It is formulated as a convex optimization problem, making it scalable to large models and datasets.","We demonstrate the effectiveness of this approach across various modeling problems in diverse scientific fields, including power systems and underwater acoustics.","Finally, we use information-matching as a query function within an Active Learning loop for material science applications.","In all these applications, we find that a relatively small set of optimal training data can provide the necessary information for achieving precise predictions.","These results are encouraging for diverse future applications, particularly active learning in large machine learning models."],"url":"http://arxiv.org/abs/2411.02740v1"}
{"created":"2024-11-05 02:03:12","title":"DDFAV: Remote Sensing Large Vision Language Models Dataset and Evaluation Benchmark","abstract":"With the rapid development of large vision language models (LVLMs), these models have shown excellent results in various multimodal tasks. Since LVLMs are prone to hallucinations and there are currently few datasets and evaluation methods specifically designed for remote sensing, their performance is typically poor when applied to remote sensing tasks. To address these issues, this paper introduces a high quality remote sensing LVLMs dataset, DDFAV, created using data augmentation and data mixing strategies. Next, a training instruction set is produced based on some high-quality remote sensing images selected from the proposed dataset. Finally, we develop a remote sensing LVLMs hallucination evaluation method RSPOPE based on the proposed dataset and evaluate the zero-shot capabilities of different LVLMs. Our proposed dataset, instruction set, and evaluation method files are available at https://github.com/HaodongLi2024/rspope.","sentences":["With the rapid development of large vision language models (LVLMs), these models have shown excellent results in various multimodal tasks.","Since LVLMs are prone to hallucinations and there are currently few datasets and evaluation methods specifically designed for remote sensing, their performance is typically poor when applied to remote sensing tasks.","To address these issues, this paper introduces a high quality remote sensing LVLMs dataset, DDFAV, created using data augmentation and data mixing strategies.","Next, a training instruction set is produced based on some high-quality remote sensing images selected from the proposed dataset.","Finally, we develop a remote sensing LVLMs hallucination evaluation method RSPOPE based on the proposed dataset and evaluate the zero-shot capabilities of different LVLMs.","Our proposed dataset, instruction set, and evaluation method files are available at https://github.com/HaodongLi2024/rspope."],"url":"http://arxiv.org/abs/2411.02733v1"}
{"created":"2024-11-05 01:58:31","title":"A Natural Language Processing Approach to Support Biomedical Data Harmonization: Leveraging Large Language Models","abstract":"Biomedical research requires large, diverse samples to produce unbiased results. Automated methods for matching variables across datasets can accelerate this process. Research in this area has been limited, primarily focusing on lexical matching and ontology based semantic matching. We aimed to develop new methods, leveraging large language models (LLM) and ensemble learning, to automate variable matching. Methods: We utilized data from two GERAS cohort (European and Japan) studies to develop variable matching methods. We first manually created a dataset by matching 352 EU variables with 1322 candidate JP variables, where matched variable pairs were positive and unmatched pairs were negative instances. Using this dataset, we developed and evaluated two types of natural language processing (NLP) methods, which matched variables based on variable labels and definitions from data dictionaries: (1) LLM-based and (2) fuzzy matching. We then developed an ensemble-learning method, using the Random Forest model, to integrate individual NLP methods. RF was trained and evaluated on 50 trials. Each trial had a random split (4:1) of training and test sets, with the model's hyperparameters optimized through cross-validation on the training set. For each EU variable, 1322 candidate JP variables were ranked based on NLP-derived similarity scores or RF's probability scores, denoting their likelihood to match the EU variable. Ranking performance was measured by top-n hit ratio (HRn) and mean reciprocal rank (MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30 and 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less than 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived features contributed most to RF's performance. One major cause of errors in automatic variable matching was ambiguous variable definitions within data dictionaries.","sentences":["Biomedical research requires large, diverse samples to produce unbiased results.","Automated methods for matching variables across datasets can accelerate this process.","Research in this area has been limited, primarily focusing on lexical matching and ontology based semantic matching.","We aimed to develop new methods, leveraging large language models (LLM) and ensemble learning, to automate variable matching.","Methods: We utilized data from two GERAS cohort (European and Japan) studies to develop variable matching methods.","We first manually created a dataset by matching 352 EU variables with 1322 candidate JP variables, where matched variable pairs were positive and unmatched pairs were negative instances.","Using this dataset, we developed and evaluated two types of natural language processing (NLP) methods, which matched variables based on variable labels and definitions from data dictionaries: (1) LLM-based and (2) fuzzy matching.","We then developed an ensemble-learning method, using the Random Forest model, to integrate individual NLP methods.","RF was trained and evaluated on 50 trials.","Each trial had a random split (4:1) of training and test sets, with the model's hyperparameters optimized through cross-validation on the training set.","For each EU variable, 1322 candidate JP variables were ranked based on NLP-derived similarity scores or RF's probability scores, denoting their likelihood to match the EU variable.","Ranking performance was measured by top-n hit ratio (HRn) and mean reciprocal rank (MRR).","Results:E5 performed best among individual methods, achieving 0.90 HR-30 and 0.70 MRR.","RF performed better than E5 on all metrics over 50 trials (P less than 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73.","LLM-derived features contributed most to RF's performance.","One major cause of errors in automatic variable matching was ambiguous variable definitions within data dictionaries."],"url":"http://arxiv.org/abs/2411.02730v1"}
{"created":"2024-11-05 01:55:07","title":"Compositional simulation-based inference for time series","abstract":"Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this approach avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time-series data. Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time. We propose an SBI framework that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions. We then compose these local results to obtain a posterior over parameters that align with the entire time series observation. We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation. We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology. Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million dimensions in the data domain.","sentences":["Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference.","While this approach avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time-series data.","Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time.","We propose an SBI framework that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions.","We then compose these local results to obtain a posterior over parameters that align with the entire time series observation.","We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation.","We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology.","Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million dimensions in the data domain."],"url":"http://arxiv.org/abs/2411.02728v1"}
{"created":"2024-11-05 01:31:05","title":"Derivative-Guided Symbolic Execution","abstract":"We consider the formulation of a symbolic execution (SE) procedure for functional programs that interact with effectful, opaque libraries. Our procedure allows specifications of libraries and abstract data type (ADT) methods that are expressed in Linear Temporal Logic over Finite Traces (LTLf), interpreting them as symbolic finite automata (SFAs) to enable intelligent specification-guided path exploration in this setting. We apply our technique to facilitate the falsification of complex data structure safety properties in terms of effectful operations made by ADT methods on underlying opaque representation type(s). Specifications naturally characterize admissible traces of temporally-ordered events that ADT methods (and the library methods they depend upon) are allowed to perform. We show how to use these specifications to construct feasible symbolic input states for the corresponding methods, as well as how to encode safety properties in terms of this formalism. More importantly, we incorporate the notion of symbolic derivatives, a mechanism that allows the SE procedure to intelligently underapproximate the set of precondition states it needs to explore, based on the automata structures implicit in the provided specifications and the safety property that is to be falsified. Intuitively, derivatives enable symbolic execution to exploit temporal constraints defined by trace-based specifications to quickly prune unproductive paths and discover feasible error states. Experimental results on a wide-range of challenging ADT implementations demonstrate the effectiveness of our approach.","sentences":["We consider the formulation of a symbolic execution (SE) procedure for functional programs that interact with effectful, opaque libraries.","Our procedure allows specifications of libraries and abstract data type (ADT) methods that are expressed in Linear Temporal Logic over Finite Traces (LTLf), interpreting them as symbolic finite automata (SFAs) to enable intelligent specification-guided path exploration in this setting.","We apply our technique to facilitate the falsification of complex data structure safety properties in terms of effectful operations made by ADT methods on underlying opaque representation type(s).","Specifications naturally characterize admissible traces of temporally-ordered events that ADT methods (and the library methods they depend upon) are allowed to perform.","We show how to use these specifications to construct feasible symbolic input states for the corresponding methods, as well as how to encode safety properties in terms of this formalism.","More importantly, we incorporate the notion of symbolic derivatives, a mechanism that allows the SE procedure to intelligently underapproximate the set of precondition states it needs to explore, based on the automata structures implicit in the provided specifications and the safety property that is to be falsified.","Intuitively, derivatives enable symbolic execution to exploit temporal constraints defined by trace-based specifications to quickly prune unproductive paths and discover feasible error states.","Experimental results on a wide-range of challenging ADT implementations demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2411.02716v1"}
{"created":"2024-11-05 01:27:25","title":"CIT: Rethinking Class-incremental Semantic Segmentation with a Class Independent Transformation","abstract":"Class-incremental semantic segmentation (CSS) requires that a model learn to segment new classes without forgetting how to segment previous ones: this is typically achieved by distilling the current knowledge and incorporating the latest data. However, bypassing iterative distillation by directly transferring outputs of initial classes to the current learning task is not supported in existing class-specific CSS methods. Via Softmax, they enforce dependency between classes and adjust the output distribution at each learning step, resulting in a large probability distribution gap between initial and current tasks. We introduce a simple, yet effective Class Independent Transformation (CIT) that converts the outputs of existing semantic segmentation models into class-independent forms with negligible cost or performance loss. By utilizing class-independent predictions facilitated by CIT, we establish an accumulative distillation framework, ensuring equitable incorporation of all class information. We conduct extensive experiments on various segmentation architectures, including DeepLabV3, Mask2Former, and SegViTv2. Results from these experiments show minimal task forgetting across different datasets, with less than 5% for ADE20K in the most challenging 11 task configurations and less than 1% across all configurations for the PASCAL VOC 2012 dataset.","sentences":["Class-incremental semantic segmentation (CSS) requires that a model learn to segment new classes without forgetting how to segment previous ones: this is typically achieved by distilling the current knowledge and incorporating the latest data.","However, bypassing iterative distillation by directly transferring outputs of initial classes to the current learning task is not supported in existing class-specific CSS methods.","Via Softmax, they enforce dependency between classes and adjust the output distribution at each learning step, resulting in a large probability distribution gap between initial and current tasks.","We introduce a simple, yet effective Class Independent Transformation (CIT) that converts the outputs of existing semantic segmentation models into class-independent forms with negligible cost or performance loss.","By utilizing class-independent predictions facilitated by CIT, we establish an accumulative distillation framework, ensuring equitable incorporation of all class information.","We conduct extensive experiments on various segmentation architectures, including DeepLabV3, Mask2Former, and SegViTv2.","Results from these experiments show minimal task forgetting across different datasets, with less than 5% for ADE20K in the most challenging 11 task configurations and less than 1% across all configurations for the PASCAL VOC 2012 dataset."],"url":"http://arxiv.org/abs/2411.02715v1"}
{"created":"2024-11-05 01:24:37","title":"V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization","abstract":"Large vision-language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs.   We tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context. Our code is publicly available at https://github.com/YuxiXie/V-DPO.","sentences":["Large vision-language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content.","Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs.   ","We tackle this issue of hallucination by mitigating such over-reliance through preference learning.","We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time.","To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples.","Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks.","Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context.","Our code is publicly available at https://github.com/YuxiXie/V-DPO."],"url":"http://arxiv.org/abs/2411.02712v1"}
{"created":"2024-11-05 01:21:28","title":"Self-Supervised Multi-View Learning for Disentangled Music Audio Representations","abstract":"Self-supervised learning (SSL) offers a powerful way to learn robust, generalizable representations without labeled data. In music, where labeled data is scarce, existing SSL methods typically use generated supervision and multi-view redundancy to create pretext tasks. However, these approaches often produce entangled representations and lose view-specific information. We propose a novel self-supervised multi-view learning framework for audio designed to incentivize separation between private and shared representation spaces. A case study on audio disentanglement in a controlled setting demonstrates the effectiveness of our method.","sentences":["Self-supervised learning (SSL) offers a powerful way to learn robust, generalizable representations without labeled data.","In music, where labeled data is scarce, existing SSL methods typically use generated supervision and multi-view redundancy to create pretext tasks.","However, these approaches often produce entangled representations and lose view-specific information.","We propose a novel self-supervised multi-view learning framework for audio designed to incentivize separation between private and shared representation spaces.","A case study on audio disentanglement in a controlled setting demonstrates the effectiveness of our method."],"url":"http://arxiv.org/abs/2411.02711v1"}
{"created":"2024-11-05 01:12:17","title":"Carbon price fluctuation prediction using blockchain information A new hybrid machine learning approach","abstract":"In this study, the novel hybrid machine learning approach is proposed in carbon price fluctuation prediction. Specifically, a research framework integrating DILATED Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) neural network algorithm is proposed. The advantage of the combined framework is that it can make feature extraction more efficient. Then, based on the DILATED CNN-LSTM framework, the L1 and L2 parameter norm penalty as regularization method is adopted to predict. Referring to the characteristics of high correlation between energy indicator price and blockchain information in previous literature, and we primarily includes indicators related to blockchain information through regularization process. Based on the above methods, this paper uses a dataset containing an amount of data to carry out the carbon price prediction. The experimental results show that the DILATED CNN-LSTM framework is superior to the traditional CNN-LSTM architecture. Blockchain information can effectively predict the price. Since parameter norm penalty as regularization, Ridge Regression (RR) as L2 regularization is better than Smoothly Clipped Absolute Deviation Penalty (SCAD) as L1 regularization in price forecasting. Thus, the proposed RR-DILATED CNN-LSTM approach can effectively and accurately predict the fluctuation trend of the carbon price. Therefore, the new forecasting methods and theoretical ecology proposed in this study provide a new basis for trend prediction and evaluating digital assets policy represented by the carbon price for both the academia and practitioners.","sentences":["In this study, the novel hybrid machine learning approach is proposed in carbon price fluctuation prediction.","Specifically, a research framework integrating DILATED Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) neural network algorithm is proposed.","The advantage of the combined framework is that it can make feature extraction more efficient.","Then, based on the DILATED CNN-LSTM framework, the L1 and L2 parameter norm penalty as regularization method is adopted to predict.","Referring to the characteristics of high correlation between energy indicator price and blockchain information in previous literature, and we primarily includes indicators related to blockchain information through regularization process.","Based on the above methods, this paper uses a dataset containing an amount of data to carry out the carbon price prediction.","The experimental results show that the DILATED CNN-LSTM framework is superior to the traditional CNN-LSTM architecture.","Blockchain information can effectively predict the price.","Since parameter norm penalty as regularization, Ridge Regression (RR) as L2 regularization is better than Smoothly Clipped Absolute Deviation Penalty (SCAD) as L1 regularization in price forecasting.","Thus, the proposed RR-DILATED CNN-LSTM approach can effectively and accurately predict the fluctuation trend of the carbon price.","Therefore, the new forecasting methods and theoretical ecology proposed in this study provide a new basis for trend prediction and evaluating digital assets policy represented by the carbon price for both the academia and practitioners."],"url":"http://arxiv.org/abs/2411.02709v1"}
{"created":"2024-11-05 01:11:28","title":"Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios","abstract":"Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency in their responses is essential for developing trustworthy multimodal intelligence. However, existing benchmarks include many samples where all MLLMs \\textit{exhibit high response uncertainty when encountering misleading information}, requiring even 5-15 response attempts per sample to effectively assess uncertainty. Therefore, we propose a two-stage pipeline: first, we collect MLLMs' responses without misleading information, and then gather misleading ones via specific misleading instructions. By calculating the misleading rate, and capturing both correct-to-incorrect and incorrect-to-correct shifts between the two sets of responses, we can effectively metric the model's response uncertainty. Eventually, we establish a \\textbf{\\underline{M}}ultimodal \\textbf{\\underline{U}}ncertainty \\textbf{\\underline{B}}enchmark (\\textbf{MUB}) that employs both explicit and implicit misleading instructions to comprehensively assess the vulnerability of MLLMs across diverse domains. Our experiments reveal that all open-source and close-source MLLMs are highly susceptible to misleading instructions, with an average misleading rate exceeding 86\\%. To enhance the robustness of MLLMs, we further fine-tune all open-source MLLMs by incorporating explicit and implicit misleading data, which demonstrates a significant reduction in misleading rates. Our code is available at: \\href{https://github.com/Yunkai696/MUB}{https://github.com/Yunkai696/MUB}","sentences":["Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency in their responses is essential for developing trustworthy multimodal intelligence.","However, existing benchmarks include many samples where all MLLMs \\textit{exhibit high response uncertainty when encountering misleading information}, requiring even 5-15 response attempts per sample to effectively assess uncertainty.","Therefore, we propose a two-stage pipeline: first, we collect MLLMs' responses without misleading information, and then gather misleading ones via specific misleading instructions.","By calculating the misleading rate, and capturing both correct-to-incorrect and incorrect-to-correct shifts between the two sets of responses, we can effectively metric the model's response uncertainty.","Eventually, we establish a \\textbf{\\underline{M}}ultimodal \\textbf{\\underline{U}}ncertainty \\textbf{\\underline{B}}enchmark (\\textbf{MUB}) that employs both explicit and implicit misleading instructions to comprehensively assess the vulnerability of MLLMs across diverse domains.","Our experiments reveal that all open-source and close-source MLLMs are highly susceptible to misleading instructions, with an average misleading rate exceeding 86\\%.","To enhance the robustness of MLLMs, we further fine-tune all open-source MLLMs by incorporating explicit and implicit misleading data, which demonstrates a significant reduction in misleading rates.","Our code is available at: \\href{https://github.com/Yunkai696/MUB}{https://github.com/Yunkai696/MUB}"],"url":"http://arxiv.org/abs/2411.02708v1"}
