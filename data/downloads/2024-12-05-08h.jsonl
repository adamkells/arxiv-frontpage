{"created":"2024-12-04 18:59:05","title":"The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control","abstract":"We present The Matrix, the first foundational realistic world simulator capable of generating continuous 720p high-fidelity real-scene video streams with real-time, responsive control in both first- and third-person perspectives, enabling immersive exploration of richly dynamic environments. Trained on limited supervised data from AAA games like Forza Horizon 5 and Cyberpunk 2077, complemented by large-scale unsupervised footage from real-world settings like Tokyo streets, The Matrix allows users to traverse diverse terrains -- deserts, grasslands, water bodies, and urban landscapes -- in continuous, uncut hour-long sequences. Operating at 16 FPS, the system supports real-time interactivity and demonstrates zero-shot generalization, translating virtual game environments to real-world contexts where collecting continuous movement data is often infeasible. For example, The Matrix can simulate a BMW X3 driving through an office setting--an environment present in neither gaming data nor real-world sources. This approach showcases the potential of AAA game data to advance robust world models, bridging the gap between simulations and real-world applications in scenarios with limited data.","sentences":["We present The Matrix, the first foundational realistic world simulator capable of generating continuous 720p high-fidelity real-scene video streams with real-time, responsive control in both first- and third-person perspectives, enabling immersive exploration of richly dynamic environments.","Trained on limited supervised data from AAA games like Forza Horizon 5 and Cyberpunk 2077, complemented by large-scale unsupervised footage from real-world settings like Tokyo streets, The Matrix allows users to traverse diverse terrains -- deserts, grasslands, water bodies, and urban landscapes -- in continuous, uncut hour-long sequences.","Operating at 16 FPS, the system supports real-time interactivity and demonstrates zero-shot generalization, translating virtual game environments to real-world contexts where collecting continuous movement data is often infeasible.","For example, The Matrix can simulate a BMW X3 driving through an office setting--an environment present in neither gaming data nor real-world sources.","This approach showcases the potential of AAA game data to advance robust world models, bridging the gap between simulations and real-world applications in scenarios with limited data."],"url":"http://arxiv.org/abs/2412.03568v1"}
{"created":"2024-12-04 18:58:21","title":"FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes","abstract":"We propose FreeSim, a camera simulation method for autonomous driving. FreeSim emphasizes high-quality rendering from viewpoints beyond the recorded ego trajectories. In such viewpoints, previous methods have unacceptable degradation because the training data of these viewpoints is unavailable. To address such data scarcity, we first propose a generative enhancement model with a matched data construction strategy. The resulting model can generate high-quality images in a viewpoint slightly deviated from the recorded trajectories, conditioned on the degraded rendering of this viewpoint. We then propose a progressive reconstruction strategy, which progressively adds generated images of unrecorded views into the reconstruction process, starting from slightly off-trajectory viewpoints and moving progressively farther away. With this progressive generation-reconstruction pipeline, FreeSim supports high-quality off-trajectory view synthesis under large deviations of more than 3 meters.","sentences":["We propose FreeSim, a camera simulation method for autonomous driving.","FreeSim emphasizes high-quality rendering from viewpoints beyond the recorded ego trajectories.","In such viewpoints, previous methods have unacceptable degradation because the training data of these viewpoints is unavailable.","To address such data scarcity, we first propose a generative enhancement model with a matched data construction strategy.","The resulting model can generate high-quality images in a viewpoint slightly deviated from the recorded trajectories, conditioned on the degraded rendering of this viewpoint.","We then propose a progressive reconstruction strategy, which progressively adds generated images of unrecorded views into the reconstruction process, starting from slightly off-trajectory viewpoints and moving progressively farther away.","With this progressive generation-reconstruction pipeline, FreeSim supports high-quality off-trajectory view synthesis under large deviations of more than 3 meters."],"url":"http://arxiv.org/abs/2412.03566v1"}
{"created":"2024-12-04 18:52:40","title":"MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation","abstract":"This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.","sentences":["This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image.","Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability.","At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes.","The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation.","During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability.","MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models."],"url":"http://arxiv.org/abs/2412.03558v1"}
{"created":"2024-12-04 18:50:08","title":"Imagine360: Immersive 360 Video Generation from Perspective Anchor","abstract":"$360^\\circ$ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in $360^\\circ$ video format, we seek to lift standard perspective videos into $360^\\circ$ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-$360^\\circ$ video generation framework that creates high-quality $360^\\circ$ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited $360^\\circ$ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for $360^\\circ$ video generation, with motion module and spatial LoRA layers fine-tuned on extended web $360^\\circ$ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art $360^\\circ$ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive $360^\\circ$ video creation.","sentences":["$360^\\circ$ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees.","To achieve more user-friendly and personalized content creation in $360^\\circ$ video format, we seek to lift standard perspective videos into $360^\\circ$ equirectangular videos.","To this end, we introduce Imagine360, the first perspective-to-$360^\\circ$ video generation framework that creates high-quality $360^\\circ$ videos with rich and diverse motion patterns from video anchors.","Imagine360","learns fine-grained spherical visual and motion patterns from limited $360^\\circ$ video data with several key designs.","1)","Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for $360^\\circ$ video generation, with motion module and spatial LoRA layers fine-tuned on extended web $360^\\circ$ videos.","2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres.","3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames.","Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art $360^\\circ$ video generation methods.","We believe Imagine360 holds promise for advancing personalized, immersive $360^\\circ$ video creation."],"url":"http://arxiv.org/abs/2412.03552v1"}
{"created":"2024-12-04 18:47:11","title":"Teaching an Old Dog New Tricks: Verifiable FHE Using Commodity Hardware","abstract":"We present Argos, a simple approach for adding verifiability to fully homomorphic encryption (FHE) schemes using trusted hardware. Traditional approaches to verifiable FHE require expensive cryptographic proofs, which incur an overhead of up to seven orders of magnitude on top of FHE, making them impractical.   With Argos, we show that trusted hardware can be securely used to provide verifiability for FHE computations, with minimal overhead relative to the baseline FHE computation. An important contribution of Argos is showing that the major security pitfall associated with trusted hardware, microarchitectural side channels, can be completely mitigated by excluding any secrets from the CPU and the memory hierarchy. This is made possible by focusing on building a platform that only enforces program and data integrity and not confidentiality (which is sufficient for verifiable FHE, since all data remain encrypted at all times). All secrets related to the attestation mechanism are kept in a separate coprocessor (e.g., a TPM) inaccessible to any software-based attacker. Relying on a discrete TPM typically incurs significant performance overhead, which is why (insecure) software-based TPMs are used in practice. As a second contribution, we show that for FHE applications, the attestation protocol can be adapted to only incur a fixed cost.   Argos requires no dedicated hardware extensions and is supported on commodity processors from 2008 onward. Our prototype implementation introduces 6% overhead to the FHE evaluation, and 8% for more complex protocols. In particular, we show that Argos can be adapted for real-world applications of FHE, such as PIR and PSI. By demonstrating how to combine cryptography with trusted hardware, Argos paves the way for widespread deployment of FHE-based protocols beyond the semi-honest setting, without the overhead of cryptographic proofs.","sentences":["We present Argos, a simple approach for adding verifiability to fully homomorphic encryption (FHE) schemes using trusted hardware.","Traditional approaches to verifiable FHE require expensive cryptographic proofs, which incur an overhead of up to seven orders of magnitude on top of FHE, making them impractical.   ","With Argos, we show that trusted hardware can be securely used to provide verifiability for FHE computations, with minimal overhead relative to the baseline FHE computation.","An important contribution of Argos is showing that the major security pitfall associated with trusted hardware, microarchitectural side channels, can be completely mitigated by excluding any secrets from the CPU and the memory hierarchy.","This is made possible by focusing on building a platform that only enforces program and data integrity and not confidentiality (which is sufficient for verifiable FHE, since all data remain encrypted at all times).","All secrets related to the attestation mechanism are kept in a separate coprocessor (e.g., a TPM) inaccessible to any software-based attacker.","Relying on a discrete TPM typically incurs significant performance overhead, which is why (insecure) software-based TPMs are used in practice.","As a second contribution, we show that for FHE applications, the attestation protocol can be adapted to only incur a fixed cost.   ","Argos requires no dedicated hardware extensions and is supported on commodity processors from 2008 onward.","Our prototype implementation introduces 6% overhead to the FHE evaluation, and 8% for more complex protocols.","In particular, we show that Argos can be adapted for real-world applications of FHE, such as PIR and PSI.","By demonstrating how to combine cryptography with trusted hardware, Argos paves the way for widespread deployment of FHE-based protocols beyond the semi-honest setting, without the overhead of cryptographic proofs."],"url":"http://arxiv.org/abs/2412.03550v1"}
{"created":"2024-12-04 18:45:35","title":"Perception Tokens Enhance Visual Reasoning in Multimodal Language Models","abstract":"Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produce intermediate depth or boxes to reason over. Finetuning MLMs on relevant data doesn't generalize well and outsourcing computation to specialized vision tools is too compute-intensive and memory-inefficient. To address this, we introduce Perception Tokens, intrinsic image representations designed to assist reasoning tasks where language is insufficient. Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models. For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively. We propose AURORA, a training method that augments MLMs with perception tokens for improved reasoning over visual inputs. AURORA leverages a VQVAE to transform intermediate image representations, such as depth maps into a tokenized format and bounding box tokens, which is then used in a multi-task training framework. AURORA achieves notable improvements across counting benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench, outperforming finetuning approaches in generalization across datasets. It also improves on relative depth: over +6% on BLINK. With perception tokens, AURORA expands the scope of MLMs beyond language-based reasoning, paving the way for more effective visual reasoning capabilities.","sentences":["Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel.","Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection.","Yet, MLMs can not produce intermediate depth or boxes to reason over.","Finetuning MLMs on relevant data doesn't generalize well and outsourcing computation to specialized vision tools is too compute-intensive and memory-inefficient.","To address this, we introduce Perception Tokens, intrinsic image representations designed to assist reasoning tasks where language is insufficient.","Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models.","For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively.","We propose AURORA, a training method that augments MLMs with perception tokens for improved reasoning over visual inputs.","AURORA leverages a VQVAE to transform intermediate image representations, such as depth maps into a tokenized format and bounding box tokens, which is then used in a multi-task training framework.","AURORA achieves notable improvements across counting benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench, outperforming finetuning approaches in generalization across datasets.","It also improves on relative depth: over +6% on BLINK.","With perception tokens, AURORA expands the scope of MLMs beyond language-based reasoning, paving the way for more effective visual reasoning capabilities."],"url":"http://arxiv.org/abs/2412.03548v1"}
{"created":"2024-12-04 18:26:13","title":"A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences","abstract":"The rapid advancement of large language models (LLMs) has opened new boundaries in the extraction and synthesis of medical knowledge, particularly within evidence synthesis. This paper reviews the state-of-the-art applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of documents. While LLMs demonstrate remarkable potential, significant challenges remain, including issues related to hallucinations, contextual understanding, and the ability to generalize across diverse medical tasks. We highlight critical gaps in the current research literature, particularly the need for unified benchmarks to standardize evaluations and ensure reliability in real-world applications. In addition, we propose directions for future research, emphasizing the integration of state-of-the-art techniques such as retrieval-augmented generation (RAG) to enhance LLM performance in evidence synthesis. By addressing these challenges and utilizing the strengths of LLMs, we aim to improve access to medical literature and facilitate meaningful discoveries in healthcare.","sentences":["The rapid advancement of large language models (LLMs) has opened new boundaries in the extraction and synthesis of medical knowledge, particularly within evidence synthesis.","This paper reviews the state-of-the-art applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of documents.","While LLMs demonstrate remarkable potential, significant challenges remain, including issues related to hallucinations, contextual understanding, and the ability to generalize across diverse medical tasks.","We highlight critical gaps in the current research literature, particularly the need for unified benchmarks to standardize evaluations and ensure reliability in real-world applications.","In addition, we propose directions for future research, emphasizing the integration of state-of-the-art techniques such as retrieval-augmented generation (RAG) to enhance LLM performance in evidence synthesis.","By addressing these challenges and utilizing the strengths of LLMs, we aim to improve access to medical literature and facilitate meaningful discoveries in healthcare."],"url":"http://arxiv.org/abs/2412.03531v1"}
{"created":"2024-12-04 18:15:41","title":"FANAL -- Financial Activity News Alerting Language Modeling Framework","abstract":"In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events. This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories. FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned with ORPO (Odds Ratio Preference Optimization) for superior class-wise probability calibration and alignment with financial event relevance. We evaluate FANAL's performance against leading large language models, including GPT-4o, Llama-3.1 8B, and Phi-3, demonstrating its superior accuracy and cost efficiency. This framework sets a new standard for financial intelligence and responsiveness, significantly outstripping existing models in both performance and affordability.","sentences":["In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events.","This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories.","FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned with ORPO (Odds Ratio Preference Optimization) for superior class-wise probability calibration and alignment with financial event relevance.","We evaluate FANAL's performance against leading large language models, including GPT-4o, Llama-3.1 8B, and Phi-3, demonstrating its superior accuracy and cost efficiency.","This framework sets a new standard for financial intelligence and responsiveness, significantly outstripping existing models in both performance and affordability."],"url":"http://arxiv.org/abs/2412.03527v1"}
{"created":"2024-12-04 17:58:03","title":"NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images","abstract":"Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.","sentences":["Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data.","However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views.","In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment.","NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training.","Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility.","Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems."],"url":"http://arxiv.org/abs/2412.03517v1"}
{"created":"2024-12-04 17:55:33","title":"Distillation of Diffusion Features for Semantic Correspondence","abstract":"Semantic correspondence, the task of determining relationships between different parts of images, underpins various applications including 3D reconstruction, image-to-image translation, object tracking, and visual place recognition. Recent studies have begun to explore representations learned in large generative image models for semantic correspondence, demonstrating promising results. Building on this progress, current state-of-the-art methods rely on combining multiple large models, resulting in high computational demands and reduced efficiency. In this work, we address this challenge by proposing a more computationally efficient approach. We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency. We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost. Furthermore, we demonstrate that by incorporating 3D data, we are able to further improve performance, without the need for human-annotated correspondences. Overall, our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications, such as semantic video correspondence. Our code and weights are publicly available on our project page.","sentences":["Semantic correspondence, the task of determining relationships between different parts of images, underpins various applications including 3D reconstruction, image-to-image translation, object tracking, and visual place recognition.","Recent studies have begun to explore representations learned in large generative image models for semantic correspondence, demonstrating promising results.","Building on this progress, current state-of-the-art methods rely on combining multiple large models, resulting in high computational demands and reduced efficiency.","In this work, we address this challenge by proposing a more computationally efficient approach.","We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency.","We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost.","Furthermore, we demonstrate that by incorporating 3D data, we are able to further improve performance, without the need for human-annotated correspondences.","Overall, our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications, such as semantic video correspondence.","Our code and weights are publicly available on our project page."],"url":"http://arxiv.org/abs/2412.03512v1"}
{"created":"2024-12-04 17:39:01","title":"Soft Checksums to Flag Untrustworthy Machine Learning Surrogate Predictions and Application to Atomic Physics Simulations","abstract":"Trained neural networks (NN) are attractive as surrogate models to replace costly calculations in physical simulations, but are often unknowingly applied to states not adequately represented in the training dataset. We present the novel technique of soft checksums for scientific machine learning, a general-purpose method to differentiate between trustworthy predictions with small errors on in-distribution (ID) data points, and untrustworthy predictions with large errors on out-of-distribution (OOD) data points. By adding a check node to the existing output layer, we train the model to learn the chosen checksum function encoded within the NN predictions and show that violations of this function correlate with high prediction errors. As the checksum function depends only on the NN predictions, we can calculate the checksum error for any prediction with a single forward pass, incurring negligible time and memory costs. Additionally, we find that incorporating the checksum function into the loss function and exposing the NN to OOD data points during the training process improves separation between ID and OOD predictions. By applying soft checksums to a physically complex and high-dimensional non-local thermodynamic equilibrium atomic physics dataset, we show that a well-chosen threshold checksum error can effectively separate ID and OOD predictions.","sentences":["Trained neural networks (NN) are attractive as surrogate models to replace costly calculations in physical simulations, but are often unknowingly applied to states not adequately represented in the training dataset.","We present the novel technique of soft checksums for scientific machine learning, a general-purpose method to differentiate between trustworthy predictions with small errors on in-distribution (ID) data points, and untrustworthy predictions with large errors on out-of-distribution (OOD) data points.","By adding a check node to the existing output layer, we train the model to learn the chosen checksum function encoded within the NN predictions and show that violations of this function correlate with high prediction errors.","As the checksum function depends only on the NN predictions, we can calculate the checksum error for any prediction with a single forward pass, incurring negligible time and memory costs.","Additionally, we find that incorporating the checksum function into the loss function and exposing the NN to OOD data points during the training process improves separation between ID and OOD predictions.","By applying soft checksums to a physically complex and high-dimensional non-local thermodynamic equilibrium atomic physics dataset, we show that a well-chosen threshold checksum error can effectively separate ID and OOD predictions."],"url":"http://arxiv.org/abs/2412.03497v1"}
{"created":"2024-12-04 17:26:30","title":"Data Fusion of Semantic and Depth Information in the Context of Object Detection","abstract":"Considerable study has already been conducted regarding autonomous driving in modern era. An autonomous driving system must be extremely good at detecting objects surrounding the car to ensure safety. In this paper, classification, and estimation of an object's (pedestrian) position (concerning an ego 3D coordinate system) are studied and the distance between the ego vehicle and the object in the context of autonomous driving is measured. To classify the object, faster Region-based Convolution Neural Network (R-CNN) with inception v2 is utilized. First, a network is trained with customized dataset to estimate the reference position of objects as well as the distance from the vehicle. From camera calibration to computing the distance, cutting-edge technologies of computer vision algorithms in a series of processes are applied to generate a 3D reference point of the region of interest. The foremost step in this process is generating a disparity map using the concept of stereo vision.","sentences":["Considerable study has already been conducted regarding autonomous driving in modern era.","An autonomous driving system must be extremely good at detecting objects surrounding the car to ensure safety.","In this paper, classification, and estimation of an object's (pedestrian) position (concerning an ego 3D coordinate system) are studied and the distance between the ego vehicle and the object in the context of autonomous driving is measured.","To classify the object, faster Region-based Convolution Neural Network (R-CNN) with inception v2 is utilized.","First, a network is trained with customized dataset to estimate the reference position of objects as well as the distance from the vehicle.","From camera calibration to computing the distance, cutting-edge technologies of computer vision algorithms in a series of processes are applied to generate a 3D reference point of the region of interest.","The foremost step in this process is generating a disparity map using the concept of stereo vision."],"url":"http://arxiv.org/abs/2412.03490v1"}
{"created":"2024-12-04 17:24:35","title":"Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective","abstract":"The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction. In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case. We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain.","sentences":["The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction.","In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes.","Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain.","Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case.","We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation.","We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain."],"url":"http://arxiv.org/abs/2412.03487v1"}
{"created":"2024-12-04 17:20:01","title":"Convolutional Neural Networks and Mixture of Experts for Intrusion Detection in 5G Networks and beyond","abstract":"The advent of 6G/NextG networks comes along with a series of benefits, including extreme capacity, reliability, and efficiency. However, these networks may become vulnerable to new security threats. Therefore, 6G/NextG networks must be equipped with advanced Artificial Intelligence algorithms, in order to evade these attacks. Existing studies on the intrusion detection task rely on the train of shallow machine learning classifiers, including Logistic Regression, Decision Trees, and so on, yielding suboptimal performance. Others are based on deep neural networks consisting of static components, which are not conditional on the input. This limits their representation power and efficiency. To resolve these issues, we present the first study integrating Mixture of Experts (MoE) for identifying malicious traffic. Specifically, we use network traffic data and convert the 1D array of features into a 2D matrix. Next, we pass this matrix through convolutional neural network (CNN) layers followed by batch normalization and max pooling layers. After obtaining the representation vector via the CNN layers, a sparsely gated MoE layer is used. This layer consists of a set of experts (dense layers) and a router, where the router assigns weights to the output of each expert. Sparsity is achieved by choosing the most relevant experts of the total ones. Finally, we perform a series of ablation experiments to prove the effectiveness of our proposed model. Experiments are conducted on the 5G-NIDD dataset, a network intrusion detection dataset generated from a real 5G test network. Results show that our introduced approach reaches weighted F1-score up to 99.95% achieving comparable performance to existing approaches. Findings also show that our proposed model achieves multiple advantages over state-of-the-art approaches.","sentences":["The advent of 6G/NextG networks comes along with a series of benefits, including extreme capacity, reliability, and efficiency.","However, these networks may become vulnerable to new security threats.","Therefore, 6G/NextG networks must be equipped with advanced Artificial Intelligence algorithms, in order to evade these attacks.","Existing studies on the intrusion detection task rely on the train of shallow machine learning classifiers, including Logistic Regression, Decision Trees, and so on, yielding suboptimal performance.","Others are based on deep neural networks consisting of static components, which are not conditional on the input.","This limits their representation power and efficiency.","To resolve these issues, we present the first study integrating Mixture of Experts (MoE) for identifying malicious traffic.","Specifically, we use network traffic data and convert the 1D array of features into a 2D matrix.","Next, we pass this matrix through convolutional neural network (CNN) layers followed by batch normalization and max pooling layers.","After obtaining the representation vector via the CNN layers, a sparsely gated MoE layer is used.","This layer consists of a set of experts (dense layers) and a router, where the router assigns weights to the output of each expert.","Sparsity is achieved by choosing the most relevant experts of the total ones.","Finally, we perform a series of ablation experiments to prove the effectiveness of our proposed model.","Experiments are conducted on the 5G-NIDD dataset, a network intrusion detection dataset generated from a real 5G test network.","Results show that our introduced approach reaches weighted F1-score up to 99.95% achieving comparable performance to existing approaches.","Findings also show that our proposed model achieves multiple advantages over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2412.03483v1"}
{"created":"2024-12-04 16:59:37","title":"Cluster Specific Representation Learning","abstract":"Representation learning aims to extract meaningful lower-dimensional embeddings from data, known as representations. Despite its widespread application, there is no established definition of a ``good'' representation. Typically, the representation quality is evaluated based on its performance in downstream tasks such as clustering, de-noising, etc. However, this task-specific approach has a limitation where a representation that performs well for one task may not necessarily be effective for another. This highlights the need for a more agnostic formulation, which is the focus of our work. We propose a downstream-agnostic formulation: when inherent clusters exist in the data, the representations should be specific to each cluster. Under this idea, we develop a meta-algorithm that jointly learns cluster-specific representations and cluster assignments. As our approach is easy to integrate with any representation learning framework, we demonstrate its effectiveness in various setups, including Autoencoders, Variational Autoencoders, Contrastive learning models, and Restricted Boltzmann Machines. We qualitatively compare our cluster-specific embeddings to standard embeddings and downstream tasks such as de-noising and clustering. While our method slightly increases runtime and parameters compared to the standard model, the experiments clearly show that it extracts the inherent cluster structures in the data, resulting in improved performance in relevant applications.","sentences":["Representation learning aims to extract meaningful lower-dimensional embeddings from data, known as representations.","Despite its widespread application, there is no established definition of a ``good'' representation.","Typically, the representation quality is evaluated based on its performance in downstream tasks such as clustering, de-noising, etc.","However, this task-specific approach has a limitation where a representation that performs well for one task may not necessarily be effective for another.","This highlights the need for a more agnostic formulation, which is the focus of our work.","We propose a downstream-agnostic formulation: when inherent clusters exist in the data, the representations should be specific to each cluster.","Under this idea, we develop a meta-algorithm that jointly learns cluster-specific representations and cluster assignments.","As our approach is easy to integrate with any representation learning framework, we demonstrate its effectiveness in various setups, including Autoencoders, Variational Autoencoders, Contrastive learning models, and Restricted Boltzmann Machines.","We qualitatively compare our cluster-specific embeddings to standard embeddings and downstream tasks such as de-noising and clustering.","While our method slightly increases runtime and parameters compared to the standard model, the experiments clearly show that it extracts the inherent cluster structures in the data, resulting in improved performance in relevant applications."],"url":"http://arxiv.org/abs/2412.03471v1"}
{"created":"2024-12-04 16:56:20","title":"Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning","abstract":"Multimodal models typically combine a powerful large language model (LLM) with a vision encoder and are then trained on multimodal data via instruction tuning. While this process adapts LLMs to multimodal settings, it remains unclear whether this adaptation compromises their original language reasoning capabilities. In this work, we explore the effects of multimodal instruction tuning on language reasoning performance. We focus on LLaVA, a leading multimodal framework that integrates LLMs such as Vicuna or Mistral with the CLIP vision encoder. We compare the performance of the original LLMs with their multimodal-adapted counterparts across eight language reasoning tasks. Our experiments yield several key insights. First, the impact of multimodal learning varies between Vicuna and Mistral: we observe a degradation in language reasoning for Mistral but improvements for Vicuna across most tasks. Second, while multimodal instruction learning consistently degrades performance on mathematical reasoning tasks (e.g., GSM8K), it enhances performance on commonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that a training-free model merging technique can effectively mitigate the language reasoning degradation observed in multimodal-adapted Mistral and even improve performance on visual tasks.","sentences":["Multimodal models typically combine a powerful large language model (LLM) with a vision encoder and are then trained on multimodal data via instruction tuning.","While this process adapts LLMs to multimodal settings, it remains unclear whether this adaptation compromises their original language reasoning capabilities.","In this work, we explore the effects of multimodal instruction tuning on language reasoning performance.","We focus on LLaVA, a leading multimodal framework that integrates LLMs such as Vicuna or Mistral with the CLIP vision encoder.","We compare the performance of the original LLMs with their multimodal-adapted counterparts across eight language reasoning tasks.","Our experiments yield several key insights.","First, the impact of multimodal learning varies between Vicuna and Mistral: we observe a degradation in language reasoning for Mistral but improvements for Vicuna across most tasks.","Second, while multimodal instruction learning consistently degrades performance on mathematical reasoning tasks (e.g., GSM8K), it enhances performance on commonsense reasoning tasks (e.g., CommonsenseQA).","Finally, we demonstrate that a training-free model merging technique can effectively mitigate the language reasoning degradation observed in multimodal-adapted Mistral and even improve performance on visual tasks."],"url":"http://arxiv.org/abs/2412.03467v1"}
{"created":"2024-12-04 16:51:59","title":"Multi-Momentum Observer Contact Estimation for Bipedal Robots","abstract":"As bipedal robots become more and more popular in commercial and industrial settings, the ability to control them with a high degree of reliability is critical. To that end, this paper considers how to accurately estimate which feet are currently in contact with the ground so as to avoid improper control actions that could jeopardize the stability of the robot. Additionally, modern algorithms for estimating the position and orientation of a robot's base frame rely heavily on such contact mode estimates. Dedicated contact sensors on the feet can be used to estimate this contact mode, but these sensors are prone to noise, time delays, damage/yielding from repeated impacts with the ground, and are not available on every robot. To overcome these limitations, we propose a momentum observer based method for contact mode estimation that does not rely on such contact sensors. Often, momentum observers assume that the robot's base frame can be treated as an inertial frame. However, since many humanoids' legs represent a significant portion of the overall mass, the proposed method instead utilizes multiple simultaneous dynamic models. Each of these models assumes a different contact condition. A given contact assumption is then used to constrain the full dynamics in order to avoid assuming that either the body is an inertial frame or that a fully accurate estimate of body velocity is known. The (dis)agreement between each model's estimates and measurements is used to determine which contact mode is most likely using a Markov-style fusion method. The proposed method produces contact detection accuracy of up to 98.44% with a low noise simulation and 77.12% when utilizing data collect on the Sarcos Guardian XO robot (a hybrid humanoid/exoskeleton).","sentences":["As bipedal robots become more and more popular in commercial and industrial settings, the ability to control them with a high degree of reliability is critical.","To that end, this paper considers how to accurately estimate which feet are currently in contact with the ground so as to avoid improper control actions that could jeopardize the stability of the robot.","Additionally, modern algorithms for estimating the position and orientation of a robot's base frame rely heavily on such contact mode estimates.","Dedicated contact sensors on the feet can be used to estimate this contact mode, but these sensors are prone to noise, time delays, damage/yielding from repeated impacts with the ground, and are not available on every robot.","To overcome these limitations, we propose a momentum observer based method for contact mode estimation that does not rely on such contact sensors.","Often, momentum observers assume that the robot's base frame can be treated as an inertial frame.","However, since many humanoids' legs represent a significant portion of the overall mass, the proposed method instead utilizes multiple simultaneous dynamic models.","Each of these models assumes a different contact condition.","A given contact assumption is then used to constrain the full dynamics in order to avoid assuming that either the body is an inertial frame or that a fully accurate estimate of body velocity is known.","The (dis)agreement between each model's estimates and measurements is used to determine which contact mode is most likely using a Markov-style fusion method.","The proposed method produces contact detection accuracy of up to 98.44% with a low noise simulation and 77.12% when utilizing data collect on the Sarcos Guardian XO robot (a hybrid humanoid/exoskeleton)."],"url":"http://arxiv.org/abs/2412.03462v1"}
{"created":"2024-12-04 16:38:07","title":"PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes","abstract":"This paper presents PlanarSplatting, an ultra-fast and accurate surface reconstruction approach for multiview indoor images. We take the 3D planes as the main objective due to their compactness and structural expressiveness in indoor scenes, and develop an explicit optimization framework that learns to fit the expected surface of indoor scenes by splatting the 3D planes into 2.5D depth and normal maps. As our PlanarSplatting operates directly on the 3D plane primitives, it eliminates the dependencies on 2D/3D plane detection and plane matching and tracking for planar surface reconstruction. Furthermore, the essential merits of plane-based representation plus CUDA-based implementation of planar splatting functions, PlanarSplatting reconstructs an indoor scene in 3 minutes while having significantly better geometric accuracy. Thanks to our ultra-fast reconstruction speed, the largest quantitative evaluation on the ScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated the advantages of our method. We believe that our accurate and ultrafast planar surface reconstruction method will be applied in the structured data curation for surface reconstruction in the future. The code of our CUDA implementation will be publicly available. Project page: https://icetttb.github.io/PlanarSplatting/","sentences":["This paper presents PlanarSplatting, an ultra-fast and accurate surface reconstruction approach for multiview indoor images.","We take the 3D planes as the main objective due to their compactness and structural expressiveness in indoor scenes, and develop an explicit optimization framework that learns to fit the expected surface of indoor scenes by splatting the 3D planes into 2.5D depth and normal maps.","As our PlanarSplatting operates directly on the 3D plane primitives, it eliminates the dependencies on 2D/3D plane detection and plane matching and tracking for planar surface reconstruction.","Furthermore, the essential merits of plane-based representation plus CUDA-based implementation of planar splatting functions, PlanarSplatting reconstructs an indoor scene in 3 minutes while having significantly better geometric accuracy.","Thanks to our ultra-fast reconstruction speed, the largest quantitative evaluation on the ScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated the advantages of our method.","We believe that our accurate and ultrafast planar surface reconstruction method will be applied in the structured data curation for surface reconstruction in the future.","The code of our CUDA implementation will be publicly available.","Project page: https://icetttb.github.io/PlanarSplatting/"],"url":"http://arxiv.org/abs/2412.03451v1"}
{"created":"2024-12-04 16:30:35","title":"State Frequency Estimation for Anomaly Detection","abstract":"Many works have studied the efficacy of state machines for detecting anomalies within NetFlows. These works typically learn a model from unlabeled data and compute anomaly scores for arbitrary traces based on their likelihood of occurrence or how well they fit within the model. However, these methods do not dynamically adapt their scores based on the traces seen at test time. This becomes a problem when an adversary produces seemingly common traces in their attack, causing the model to miss the detection by assigning low anomaly scores. We propose SEQUENT, a new approach that uses the state visit frequency to adapt its scoring for anomaly detection dynamically. SEQUENT subsequently uses the scores to generate root causes for anomalies. These allow the grouping of alarms and simplify the analysis of anomalies. Our evaluation of SEQUENT on three NetFlow datasets indicates that our approach outperforms existing methods, demonstrating its effectiveness in detecting anomalies.","sentences":["Many works have studied the efficacy of state machines for detecting anomalies within NetFlows.","These works typically learn a model from unlabeled data and compute anomaly scores for arbitrary traces based on their likelihood of occurrence or how well they fit within the model.","However, these methods do not dynamically adapt their scores based on the traces seen at test time.","This becomes a problem when an adversary produces seemingly common traces in their attack, causing the model to miss the detection by assigning low anomaly scores.","We propose SEQUENT, a new approach that uses the state visit frequency to adapt its scoring for anomaly detection dynamically.","SEQUENT subsequently uses the scores to generate root causes for anomalies.","These allow the grouping of alarms and simplify the analysis of anomalies.","Our evaluation of SEQUENT on three NetFlow datasets indicates that our approach outperforms existing methods, demonstrating its effectiveness in detecting anomalies."],"url":"http://arxiv.org/abs/2412.03442v1"}
{"created":"2024-12-04 16:30:03","title":"PBP: Post-training Backdoor Purification for Malware Classifiers","abstract":"In recent years, the rise of machine learning (ML) in cybersecurity has brought new challenges, including the increasing threat of backdoor poisoning attacks on ML malware classifiers. For instance, adversaries could inject malicious samples into public malware repositories, contaminating the training data and potentially misclassifying malware by the ML model. Current countermeasures predominantly focus on detecting poisoned samples by leveraging disagreements within the outputs of a diverse set of ensemble models on training data points. However, these methods are not suitable for scenarios where Machine Learning-as-a-Service (MLaaS) is used or when users aim to remove backdoors from a model after it has been trained. Addressing this scenario, we introduce PBP, a post-training defense for malware classifiers that mitigates various types of backdoor embeddings without assuming any specific backdoor embedding mechanism. Our method exploits the influence of backdoor attacks on the activation distribution of neural networks, independent of the trigger-embedding method. In the presence of a backdoor attack, the activation distribution of each layer is distorted into a mixture of distributions. By regulating the statistics of the batch normalization layers, we can guide a backdoored model to perform similarly to a clean one. Our method demonstrates substantial advantages over several state-of-the-art methods, as evidenced by experiments on two datasets, two types of backdoor methods, and various attack configurations. Notably, our approach requires only a small portion of the training data -- only 1\\% -- to purify the backdoor and reduce the attack success rate from 100\\% to almost 0\\%, a 100-fold improvement over the baseline methods. Our code is available at \\url{https://github.com/judydnguyen/pbp-backdoor-purification-official}.","sentences":["In recent years, the rise of machine learning (ML) in cybersecurity has brought new challenges, including the increasing threat of backdoor poisoning attacks on ML malware classifiers.","For instance, adversaries could inject malicious samples into public malware repositories, contaminating the training data and potentially misclassifying malware by the ML model.","Current countermeasures predominantly focus on detecting poisoned samples by leveraging disagreements within the outputs of a diverse set of ensemble models on training data points.","However, these methods are not suitable for scenarios where Machine Learning-as-a-Service (MLaaS) is used or when users aim to remove backdoors from a model after it has been trained.","Addressing this scenario, we introduce PBP, a post-training defense for malware classifiers that mitigates various types of backdoor embeddings without assuming any specific backdoor embedding mechanism.","Our method exploits the influence of backdoor attacks on the activation distribution of neural networks, independent of the trigger-embedding method.","In the presence of a backdoor attack, the activation distribution of each layer is distorted into a mixture of distributions.","By regulating the statistics of the batch normalization layers, we can guide a backdoored model to perform similarly to a clean one.","Our method demonstrates substantial advantages over several state-of-the-art methods, as evidenced by experiments on two datasets, two types of backdoor methods, and various attack configurations.","Notably, our approach requires only a small portion of the training data -- only 1\\% -- to purify the backdoor and reduce the attack success rate from 100\\% to almost 0\\%, a 100-fold improvement over the baseline methods.","Our code is available at \\url{https://github.com/judydnguyen/pbp-backdoor-purification-official}."],"url":"http://arxiv.org/abs/2412.03441v1"}
{"created":"2024-12-04 16:26:17","title":"BIMCaP: BIM-based AI-supported LiDAR-Camera Pose Refinement","abstract":"This paper introduces BIMCaP, a novel method to integrate mobile 3D sparse LiDAR data and camera measurements with pre-existing building information models (BIMs), enhancing fast and accurate indoor mapping with affordable sensors. BIMCaP refines sensor poses by leveraging a 3D BIM and employing a bundle adjustment technique to align real-world measurements with the model. Experiments using real-world open-access data show that BIMCaP achieves superior accuracy, reducing translational error by over 4 cm compared to current state-of-the-art methods. This advancement enhances the accuracy and cost-effectiveness of 3D mapping methodologies like SLAM. BIMCaP's improvements benefit various fields, including construction site management and emergency response, by providing up-to-date, aligned digital maps for better decision-making and productivity. Link to the repository: https://github.com/MigVega/BIMCaP","sentences":["This paper introduces BIMCaP, a novel method to integrate mobile 3D sparse LiDAR data and camera measurements with pre-existing building information models (BIMs), enhancing fast and accurate indoor mapping with affordable sensors.","BIMCaP refines sensor poses by leveraging a 3D BIM and employing a bundle adjustment technique to align real-world measurements with the model.","Experiments using real-world open-access data show that BIMCaP achieves superior accuracy, reducing translational error by over 4 cm compared to current state-of-the-art methods.","This advancement enhances the accuracy and cost-effectiveness of 3D mapping methodologies like SLAM.","BIMCaP's improvements benefit various fields, including construction site management and emergency response, by providing up-to-date, aligned digital maps for better decision-making and productivity.","Link to the repository: https://github.com/MigVega/BIMCaP"],"url":"http://arxiv.org/abs/2412.03434v1"}
{"created":"2024-12-04 16:17:09","title":"Assessing Foundation Models' Transferability to Physiological Signals in Precision Medicine","abstract":"The success of precision medicine requires computational models that can effectively process and interpret diverse physiological signals across heterogeneous patient populations. While foundation models have demonstrated remarkable transfer capabilities across various domains, their effectiveness in handling individual-specific physiological signals - crucial for precision medicine - remains largely unexplored. This work introduces a systematic pipeline for rapidly and efficiently evaluating foundation models' transfer capabilities in medical contexts. Our pipeline employs a three-stage approach. First, it leverages physiological simulation software to generate diverse, clinically relevant scenarios, particularly focusing on data-scarce medical conditions. This simulation-based approach enables both targeted capability assessment and subsequent model fine-tuning. Second, the pipeline projects these simulated signals through the foundation model to obtain embeddings, which are then evaluated using linear methods. This evaluation quantifies the model's ability to capture three critical aspects: physiological feature independence, temporal dynamics preservation, and medical scenario differentiation. Finally, the pipeline validates these representations through specific downstream medical tasks. Initial testing of our pipeline on the Moirai time series foundation model revealed significant limitations in physiological signal processing, including feature entanglement, temporal dynamics distortion, and reduced scenario discrimination. These findings suggest that current foundation models may require substantial architectural modifications or targeted fine-tuning before deployment in clinical settings.","sentences":["The success of precision medicine requires computational models that can effectively process and interpret diverse physiological signals across heterogeneous patient populations.","While foundation models have demonstrated remarkable transfer capabilities across various domains, their effectiveness in handling individual-specific physiological signals - crucial for precision medicine - remains largely unexplored.","This work introduces a systematic pipeline for rapidly and efficiently evaluating foundation models' transfer capabilities in medical contexts.","Our pipeline employs a three-stage approach.","First, it leverages physiological simulation software to generate diverse, clinically relevant scenarios, particularly focusing on data-scarce medical conditions.","This simulation-based approach enables both targeted capability assessment and subsequent model fine-tuning.","Second, the pipeline projects these simulated signals through the foundation model to obtain embeddings, which are then evaluated using linear methods.","This evaluation quantifies the model's ability to capture three critical aspects: physiological feature independence, temporal dynamics preservation, and medical scenario differentiation.","Finally, the pipeline validates these representations through specific downstream medical tasks.","Initial testing of our pipeline on the Moirai time series foundation model revealed significant limitations in physiological signal processing, including feature entanglement, temporal dynamics distortion, and reduced scenario discrimination.","These findings suggest that current foundation models may require substantial architectural modifications or targeted fine-tuning before deployment in clinical settings."],"url":"http://arxiv.org/abs/2412.03427v1"}
{"created":"2024-12-04 15:53:45","title":"Learning Semantic Association Rules from Internet of Things Data","abstract":"Association Rule Mining (ARM) is the task of discovering commonalities in data in the form of logical implications. ARM is used in the Internet of Things (IoT) for different tasks including monitoring and decision-making. However, existing methods give limited consideration to IoT-specific requirements such as heterogeneity and volume. Furthermore, they do not utilize important static domain-specific description data about IoT systems, which is increasingly represented as knowledge graphs. In this paper, we propose a novel ARM pipeline for IoT data that utilizes both dynamic sensor data and static IoT system metadata. Furthermore, we propose an Autoencoder-based Neurosymbolic ARM method (Aerial) as part of the pipeline to address the high volume of IoT data and reduce the total number of rules that are resource-intensive to process. Aerial learns a neural representation of a given data and extracts association rules from this representation by exploiting the reconstruction (decoding) mechanism of an autoencoder. Extensive evaluations on 3 IoT datasets from 2 domains show that ARM on both static and dynamic IoT data results in more generically applicable rules while Aerial can learn a more concise set of high-quality association rules than the state-of-the-art with full coverage over the datasets.","sentences":["Association Rule Mining (ARM) is the task of discovering commonalities in data in the form of logical implications.","ARM is used in the Internet of Things (IoT) for different tasks including monitoring and decision-making.","However, existing methods give limited consideration to IoT-specific requirements such as heterogeneity and volume.","Furthermore, they do not utilize important static domain-specific description data about IoT systems, which is increasingly represented as knowledge graphs.","In this paper, we propose a novel ARM pipeline for IoT data that utilizes both dynamic sensor data and static IoT system metadata.","Furthermore, we propose an Autoencoder-based Neurosymbolic ARM method (Aerial) as part of the pipeline to address the high volume of IoT data and reduce the total number of rules that are resource-intensive to process.","Aerial learns a neural representation of a given data and extracts association rules from this representation by exploiting the reconstruction (decoding) mechanism of an autoencoder.","Extensive evaluations on 3 IoT datasets from 2 domains show that ARM on both static and dynamic IoT data results in more generically applicable rules while Aerial can learn a more concise set of high-quality association rules than the state-of-the-art with full coverage over the datasets."],"url":"http://arxiv.org/abs/2412.03417v1"}
{"created":"2024-12-04 15:49:49","title":"Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion","abstract":"Sea Surface Temperature (SST) is crucial for understanding Earth's oceans and climate, significantly influencing weather patterns, ocean currents, marine ecosystem health, and the global energy balance. Large-scale SST monitoring relies on satellite infrared radiation detection, but cloud cover presents a major challenge, creating extensive observational gaps and hampering our ability to fully capture large-scale ocean temperature patterns. Efforts to address these gaps in existing L4 datasets have been made, but they often exhibit notable local and seasonal biases, compromising data reliability and accuracy. To tackle this challenge, we employed deep neural networks to reconstruct cloud-covered portions of satellite imagery while preserving the integrity of observed values in cloud-free areas, using MODIS satellite derived observations of SST. Our best-performing architecture showed significant skill improvements over established methodologies, achieving substantial reductions in error metrics when benchmarked against widely used approaches and datasets. These results underscore the potential of advanced AI techniques to enhance the completeness of satellite observations in Earth-science remote sensing, providing more accurate and reliable datasets for environmental assessments, data-driven model training, climate research, and seamless integration into model data assimilation workflows.","sentences":["Sea Surface Temperature (SST) is crucial for understanding Earth's oceans and climate, significantly influencing weather patterns, ocean currents, marine ecosystem health, and the global energy balance.","Large-scale SST monitoring relies on satellite infrared radiation detection, but cloud cover presents a major challenge, creating extensive observational gaps and hampering our ability to fully capture large-scale ocean temperature patterns.","Efforts to address these gaps in existing L4 datasets have been made, but they often exhibit notable local and seasonal biases, compromising data reliability and accuracy.","To tackle this challenge, we employed deep neural networks to reconstruct cloud-covered portions of satellite imagery while preserving the integrity of observed values in cloud-free areas, using MODIS satellite derived observations of SST.","Our best-performing architecture showed significant skill improvements over established methodologies, achieving substantial reductions in error metrics when benchmarked against widely used approaches and datasets.","These results underscore the potential of advanced AI techniques to enhance the completeness of satellite observations in Earth-science remote sensing, providing more accurate and reliable datasets for environmental assessments, data-driven model training, climate research, and seamless integration into model data assimilation workflows."],"url":"http://arxiv.org/abs/2412.03413v1"}
{"created":"2024-12-04 15:31:30","title":"Implicit Priors Editing in Stable Diffusion via Targeted Token Adjustment","abstract":"Implicit assumptions and priors are often necessary in text-to-image generation tasks, especially when textual prompts lack sufficient context. However, these assumptions can sometimes reflect outdated concepts, inaccuracies, or societal bias embedded in the training data. We present Embedding-only Editing (Embedit), a method designed to efficiently adjust implict assumptions and priors in the model without affecting its interpretation of unrelated objects or overall performance. Given a \"source\" prompt (e.g., \"rose\") that elicits an implicit assumption (e.g., rose is red) and a \"destination\" prompt that specifies the desired attribute (e.g., \"blue rose\"), Embedit fine-tunes only the word token embedding (WTE) of the target object (\"rose\") to optimize the last hidden state of text encoder in Stable Diffusion, a SOTA text-to-image model. This targeted adjustment prevents unintended effects on other objects in the model's knowledge base, as the WTEs for unrelated objects and the model weights remain unchanged. Consequently, when a prompt does not contain the edited object, all representations, and the model outputs are identical to those of the original, unedited model. Our method is highly efficient, modifying only 768 parameters for Stable Diffusion 1.4 and 2048 for XL in a single edit, matching the WTE dimension of each respective model. This minimal scope, combined with rapid execution, makes Embedit highly practical for real-world applications. Additionally, changes are easily reversible by restoring the original WTE layers. Our experimental results demonstrate that Embedit consistently outperforms previous methods across various models, tasks, and editing scenarios (both single and sequential multiple edits), achieving at least a 6.01% improvement (from 87.17% to 93.18%).","sentences":["Implicit assumptions and priors are often necessary in text-to-image generation tasks, especially when textual prompts lack sufficient context.","However, these assumptions can sometimes reflect outdated concepts, inaccuracies, or societal bias embedded in the training data.","We present Embedding-only Editing (Embedit), a method designed to efficiently adjust implict assumptions and priors in the model without affecting its interpretation of unrelated objects or overall performance.","Given a \"source\" prompt (e.g., \"rose\") that elicits an implicit assumption (e.g., rose is red) and a \"destination\" prompt that specifies the desired attribute (e.g., \"blue rose\"), Embedit fine-tunes only the word token embedding (WTE) of the target object (\"rose\") to optimize the last hidden state of text encoder in Stable Diffusion, a SOTA text-to-image model.","This targeted adjustment prevents unintended effects on other objects in the model's knowledge base, as the WTEs for unrelated objects and the model weights remain unchanged.","Consequently, when a prompt does not contain the edited object, all representations, and the model outputs are identical to those of the original, unedited model.","Our method is highly efficient, modifying only 768 parameters for Stable Diffusion 1.4 and 2048 for XL in a single edit, matching the WTE dimension of each respective model.","This minimal scope, combined with rapid execution, makes Embedit highly practical for real-world applications.","Additionally, changes are easily reversible by restoring the original WTE layers.","Our experimental results demonstrate that Embedit consistently outperforms previous methods across various models, tasks, and editing scenarios (both single and sequential multiple edits), achieving at least a 6.01% improvement (from 87.17% to 93.18%)."],"url":"http://arxiv.org/abs/2412.03400v1"}
{"created":"2024-12-04 15:27:39","title":"RedStone: Curating General, Code, Math, and QA Data for Large Language Models","abstract":"Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common Crawl as a comprehensive and flexible resource for pre-training LLMs, addressing both general-purpose language understanding and specialized domain knowledge. We introduce RedStone, an innovative and scalable pipeline engineered to extract and process data from Common Crawl, facilitating the creation of extensive and varied pre-training datasets. Unlike traditional datasets, which often require expensive curation and domain-specific expertise, RedStone leverages the breadth of Common Crawl to deliver datasets tailored to a wide array of domains. In this work, we exemplify its capability by constructing pre-training datasets across multiple fields, including general language understanding, code, mathematics, and question-answering tasks. The flexibility of RedStone allows for easy adaptation to other specialized domains, significantly lowering the barrier to creating valuable domain-specific datasets. Our findings demonstrate that Common Crawl, when harnessed through effective pipelines like RedStone, can serve as a rich, renewable source of pre-training data, unlocking new avenues for domain adaptation and knowledge discovery in LLMs. This work also underscores the importance of innovative data acquisition strategies and highlights the role of web-scale data as a powerful resource in the continued evolution of LLMs. RedStone code and data samples will be publicly available at \\url{https://aka.ms/redstone}.","sentences":["Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities.","This study explores the untapped potential of Common Crawl as a comprehensive and flexible resource for pre-training LLMs, addressing both general-purpose language understanding and specialized domain knowledge.","We introduce RedStone, an innovative and scalable pipeline engineered to extract and process data from Common Crawl, facilitating the creation of extensive and varied pre-training datasets.","Unlike traditional datasets, which often require expensive curation and domain-specific expertise, RedStone leverages the breadth of Common Crawl to deliver datasets tailored to a wide array of domains.","In this work, we exemplify its capability by constructing pre-training datasets across multiple fields, including general language understanding, code, mathematics, and question-answering tasks.","The flexibility of RedStone allows for easy adaptation to other specialized domains, significantly lowering the barrier to creating valuable domain-specific datasets.","Our findings demonstrate that Common Crawl, when harnessed through effective pipelines like RedStone, can serve as a rich, renewable source of pre-training data, unlocking new avenues for domain adaptation and knowledge discovery in LLMs.","This work also underscores the importance of innovative data acquisition strategies and highlights the role of web-scale data as a powerful resource in the continued evolution of LLMs.","RedStone code and data samples will be publicly available at \\url{https://aka.ms/redstone}."],"url":"http://arxiv.org/abs/2412.03398v1"}
{"created":"2024-12-04 15:19:01","title":"Enhancing Supply Chain Visibility with Generative AI: An Exploratory Case Study on Relationship Prediction in Knowledge Graphs","abstract":"A key stumbling block in effective supply chain risk management for companies and policymakers is a lack of visibility on interdependent supply network relationships. Relationship prediction, also called link prediction is an emergent area of supply chain surveillance research that aims to increase the visibility of supply chains using data-driven techniques. Existing methods have been successful for predicting relationships but struggle to extract the context in which these relationships are embedded - such as the products being supplied or locations they are supplied from. Lack of context prevents practitioners from distinguishing transactional relations from established supply chain relations, hindering accurate estimations of risk. In this work, we develop a new Generative Artificial Intelligence (Gen AI) enhanced machine learning framework that leverages pre-trained language models as embedding models combined with machine learning models to predict supply chain relationships within knowledge graphs. By integrating Generative AI techniques, our approach captures the nuanced semantic relationships between entities, thereby improving supply chain visibility and facilitating more precise risk management. Using data from a real case study, we show that GenAI-enhanced link prediction surpasses all benchmarks, and demonstrate how GenAI models can be explored and effectively used in supply chain risk management.","sentences":["A key stumbling block in effective supply chain risk management for companies and policymakers is a lack of visibility on interdependent supply network relationships.","Relationship prediction, also called link prediction is an emergent area of supply chain surveillance research that aims to increase the visibility of supply chains using data-driven techniques.","Existing methods have been successful for predicting relationships but struggle to extract the context in which these relationships are embedded - such as the products being supplied or locations they are supplied from.","Lack of context prevents practitioners from distinguishing transactional relations from established supply chain relations, hindering accurate estimations of risk.","In this work, we develop a new Generative Artificial Intelligence (Gen AI) enhanced machine learning framework that leverages pre-trained language models as embedding models combined with machine learning models to predict supply chain relationships within knowledge graphs.","By integrating Generative AI techniques, our approach captures the nuanced semantic relationships between entities, thereby improving supply chain visibility and facilitating more precise risk management.","Using data from a real case study, we show that GenAI-enhanced link prediction surpasses all benchmarks, and demonstrate how GenAI models can be explored and effectively used in supply chain risk management."],"url":"http://arxiv.org/abs/2412.03390v1"}
{"created":"2024-12-04 15:12:00","title":"Reactive Orchestration for Hierarchical Federated Learning Under a Communication Cost Budget","abstract":"Deploying a Hierarchical Federated Learning (HFL) pipeline across the computing continuum (CC) requires careful organization of participants into a hierarchical structure with intermediate aggregation nodes between FL clients and the global FL server. This is challenging to achieve due to (i) cost constraints, (ii) varying data distributions, and (iii) the volatile operating environment of the CC. In response to these challenges, we present a framework for the adaptive orchestration of HFL pipelines, designed to be reactive to client churn and infrastructure-level events, while balancing communication cost and ML model accuracy. Our mechanisms identify and react to events that cause HFL reconfiguration actions at runtime, building on multi-level monitoring information (model accuracy, resource availability, resource cost). Moreover, our framework introduces a generic methodology for estimating reconfiguration costs to continuously re-evaluate the quality of adaptation actions, while being extensible to optimize for various HFL performance criteria. By extending the Kubernetes ecosystem, our framework demonstrates the ability to react promptly and effectively to changes in the operating environment, making the best of the available communication cost budget and effectively balancing costs and ML performance at runtime.","sentences":["Deploying a Hierarchical Federated Learning (HFL) pipeline across the computing continuum (CC) requires careful organization of participants into a hierarchical structure with intermediate aggregation nodes between FL clients and the global FL server.","This is challenging to achieve due to (i) cost constraints, (ii) varying data distributions, and (iii) the volatile operating environment of the CC.","In response to these challenges, we present a framework for the adaptive orchestration of HFL pipelines, designed to be reactive to client churn and infrastructure-level events, while balancing communication cost and ML model accuracy.","Our mechanisms identify and react to events that cause HFL reconfiguration actions at runtime, building on multi-level monitoring information (model accuracy, resource availability, resource cost).","Moreover, our framework introduces a generic methodology for estimating reconfiguration costs to continuously re-evaluate the quality of adaptation actions, while being extensible to optimize for various HFL performance criteria.","By extending the Kubernetes ecosystem, our framework demonstrates the ability to react promptly and effectively to changes in the operating environment, making the best of the available communication cost budget and effectively balancing costs and ML performance at runtime."],"url":"http://arxiv.org/abs/2412.03385v1"}
{"created":"2024-12-04 15:02:28","title":"Granular Ball Twin Support Vector Machine with Universum Data","abstract":"Classification with support vector machines (SVM) often suffers from limited performance when relying solely on labeled data from target classes and is sensitive to noise and outliers. Incorporating prior knowledge from Universum data and more robust data representations can enhance accuracy and efficiency. Motivated by these findings, we propose a novel Granular Ball Twin Support Vector Machine with Universum Data (GBU-TSVM) that extends the TSVM framework to leverage both Universum samples and granular ball computing during model training. Unlike existing TSVM methods, the proposed GBU-TSVM represents data instances as hyper-balls rather than points in the feature space. This innovative approach improves the model's robustness and efficiency, particularly in handling noisy and large datasets. By grouping data points into granular balls, the model achieves superior computational efficiency, increased noise resistance, and enhanced interpretability. Additionally, the inclusion of Universum data, which consists of samples that are not strictly from the target classes, further refines the classification boundaries. This integration enriches the model with contextual information, refining classification boundaries and boosting overall accuracy. Experimental results on UCI benchmark datasets demonstrate that the GBU-TSVM outperforms existing TSVM models in both accuracy and computational efficiency. These findings highlight the potential of the GBU-TSVM model in setting a new standard in data representation and classification.","sentences":["Classification with support vector machines (SVM) often suffers from limited performance when relying solely on labeled data from target classes and is sensitive to noise and outliers.","Incorporating prior knowledge from Universum data and more robust data representations can enhance accuracy and efficiency.","Motivated by these findings, we propose a novel Granular Ball Twin Support Vector Machine with Universum Data (GBU-TSVM) that extends the TSVM framework to leverage both Universum samples and granular ball computing during model training.","Unlike existing TSVM methods, the proposed GBU-TSVM represents data instances as hyper-balls rather than points in the feature space.","This innovative approach improves the model's robustness and efficiency, particularly in handling noisy and large datasets.","By grouping data points into granular balls, the model achieves superior computational efficiency, increased noise resistance, and enhanced interpretability.","Additionally, the inclusion of Universum data, which consists of samples that are not strictly from the target classes, further refines the classification boundaries.","This integration enriches the model with contextual information, refining classification boundaries and boosting overall accuracy.","Experimental results on UCI benchmark datasets demonstrate that the GBU-TSVM outperforms existing TSVM models in both accuracy and computational efficiency.","These findings highlight the potential of the GBU-TSVM model in setting a new standard in data representation and classification."],"url":"http://arxiv.org/abs/2412.03375v1"}
{"created":"2024-12-04 15:01:20","title":"Exploring trends in audio mixes and masters: Insights from a dataset analysis","abstract":"We present an analysis of a dataset of audio metrics and aesthetic considerations about mixes and masters provided by the web platform MixCheck studio. The platform is designed for educational purposes, primarily targeting amateur music producers, and aimed at analysing their recordings prior to them being released. The analysis focuses on the following data points: integrated loudness, mono compatibility, presence of clipping and phase issues, compression and tonal profile across 30 user-specified genres. Both mixed (mixes) and mastered audio (masters) are included in the analysis, where mixes refer to the initial combination and balance of individual tracks, and masters refer to the final refined version optimized for distribution. Results show that loudness-related issues along with dynamics issues are the most prevalent, particularly in mastered audio. However mastered audio presents better results in compression than just mixed audio. Additionally, results show that mastered audio has a lower percentage of stereo field and phase issues.","sentences":["We present an analysis of a dataset of audio metrics and aesthetic considerations about mixes and masters provided by the web platform MixCheck studio.","The platform is designed for educational purposes, primarily targeting amateur music producers, and aimed at analysing their recordings prior to them being released.","The analysis focuses on the following data points: integrated loudness, mono compatibility, presence of clipping and phase issues, compression and tonal profile across 30 user-specified genres.","Both mixed (mixes) and mastered audio (masters) are included in the analysis, where mixes refer to the initial combination and balance of individual tracks, and masters refer to the final refined version optimized for distribution.","Results show that loudness-related issues along with dynamics issues are the most prevalent, particularly in mastered audio.","However mastered audio presents better results in compression than just mixed audio.","Additionally, results show that mastered audio has a lower percentage of stereo field and phase issues."],"url":"http://arxiv.org/abs/2412.03373v1"}
{"created":"2024-12-04 14:35:06","title":"Intuitive Axial Augmentation Using Polar-Sine-Based Piecewise Distortion for Medical Slice-Wise Segmentation","abstract":"Most data-driven models for medical image analysis rely on universal augmentations to improve performance. Experimental evidence has confirmed their effectiveness, but the unclear mechanism underlying them poses a barrier to the widespread acceptance and trust in such methods within the medical community. We revisit and acknowledge the unique characteristics of medical images apart from traditional digital images, and consequently, proposed a medical-specific augmentation algorithm that is more elastic and aligns well with radiology scan procedure. The method performs piecewise affine with sinusoidal distorted ray according to radius on polar coordinates, thus simulating uncertain postures of human lying flat on the scanning table. Our method could generate human visceral distribution without affecting the fundamental relative position on axial plane. Two non-adaptive algorithms, namely Meta-based Scan Table Removal and Similarity-Guided Parameter Search, are introduced to bolster robustness of our augmentation method. Experiments show our method improves accuracy across multiple famous segmentation frameworks without requiring more data samples. Our preview code is available in: https://github.com/MGAMZ/PSBPD.","sentences":["Most data-driven models for medical image analysis rely on universal augmentations to improve performance.","Experimental evidence has confirmed their effectiveness, but the unclear mechanism underlying them poses a barrier to the widespread acceptance and trust in such methods within the medical community.","We revisit and acknowledge the unique characteristics of medical images apart from traditional digital images, and consequently, proposed a medical-specific augmentation algorithm that is more elastic and aligns well with radiology scan procedure.","The method performs piecewise affine with sinusoidal distorted ray according to radius on polar coordinates, thus simulating uncertain postures of human lying flat on the scanning table.","Our method could generate human visceral distribution without affecting the fundamental relative position on axial plane.","Two non-adaptive algorithms, namely Meta-based Scan Table Removal and Similarity-Guided Parameter Search, are introduced to bolster robustness of our augmentation method.","Experiments show our method improves accuracy across multiple famous segmentation frameworks without requiring more data samples.","Our preview code is available in: https://github.com/MGAMZ/PSBPD."],"url":"http://arxiv.org/abs/2412.03352v1"}
{"created":"2024-12-04 14:30:19","title":"Fairer Analysis and Demographically Balanced Face Generation for Fairer Face Verification","abstract":"Face recognition and verification are two computer vision tasks whose performances have advanced with the introduction of deep representations. However, ethical, legal, and technical challenges due to the sensitive nature of face data and biases in real-world training datasets hinder their development. Generative AI addresses privacy by creating fictitious identities, but fairness problems remain. Using the existing DCFace SOTA framework, we introduce a new controlled generation pipeline that improves fairness. Through classical fairness metrics and a proposed in-depth statistical analysis based on logit models and ANOVA, we show that our generation pipeline improves fairness more than other bias mitigation approaches while slightly improving raw performance.","sentences":["Face recognition and verification are two computer vision tasks whose performances have advanced with the introduction of deep representations.","However, ethical, legal, and technical challenges due to the sensitive nature of face data and biases in real-world training datasets hinder their development.","Generative AI addresses privacy by creating fictitious identities, but fairness problems remain.","Using the existing DCFace SOTA framework, we introduce a new controlled generation pipeline that improves fairness.","Through classical fairness metrics and a proposed in-depth statistical analysis based on logit models and ANOVA, we show that our generation pipeline improves fairness more than other bias mitigation approaches while slightly improving raw performance."],"url":"http://arxiv.org/abs/2412.03349v1"}
{"created":"2024-12-04 14:13:38","title":"AI-Driven Day-to-Day Route Choice","abstract":"Understanding travelers' route choices can help policymakers devise optimal operational and planning strategies for both normal and abnormal circumstances. However, existing choice modeling methods often rely on predefined assumptions and struggle to capture the dynamic and adaptive nature of travel behavior. Recently, Large Language Models (LLMs) have emerged as a promising alternative, demonstrating remarkable ability to replicate human-like behaviors across various fields. Despite this potential, their capacity to accurately simulate human route choice behavior in transportation contexts remains doubtful. To satisfy this curiosity, this paper investigates the potential of LLMs for route choice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This agent integrates an LLM as its core, equipped with a memory system that learns from past experiences and makes decisions by balancing retrieved data and personality traits. The study systematically evaluates the LLMTraveler's ability to replicate human-like decision-making through two stages: (1) analyzing its route-switching behavior in single origin-destination (OD) pair congestion game scenarios, where it demonstrates patterns align with laboratory data but are not fully explained by traditional models, and (2) testing its capacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar and Willumsen (OW) network, producing results comparable to Multinomial Logit (MNL) and Reinforcement Learning (RL) models. These experiments demonstrate that the framework can partially replicate human-like decision-making in route choice while providing natural language explanations for its decisions. This capability offers valuable insights for transportation policymaking, such as simulating traveler responses to new policies or changes in the network.","sentences":["Understanding travelers' route choices can help policymakers devise optimal operational and planning strategies for both normal and abnormal circumstances.","However, existing choice modeling methods often rely on predefined assumptions and struggle to capture the dynamic and adaptive nature of travel behavior.","Recently, Large Language Models (LLMs) have emerged as a promising alternative, demonstrating remarkable ability to replicate human-like behaviors across various fields.","Despite this potential, their capacity to accurately simulate human route choice behavior in transportation contexts remains doubtful.","To satisfy this curiosity, this paper investigates the potential of LLMs for route choice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\"","This agent integrates an LLM as its core, equipped with a memory system that learns from past experiences and makes decisions by balancing retrieved data and personality traits.","The study systematically evaluates the LLMTraveler's ability to replicate human-like decision-making through two stages: (1) analyzing its route-switching behavior in single origin-destination (OD) pair congestion game scenarios, where it demonstrates patterns align with laboratory data but are not fully explained by traditional models, and (2) testing its capacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar and Willumsen (OW) network, producing results comparable to Multinomial Logit (MNL) and Reinforcement Learning (RL) models.","These experiments demonstrate that the framework can partially replicate human-like decision-making in route choice while providing natural language explanations for its decisions.","This capability offers valuable insights for transportation policymaking, such as simulating traveler responses to new policies or changes in the network."],"url":"http://arxiv.org/abs/2412.03338v1"}
{"created":"2024-12-04 14:05:18","title":"Yankari: A Monolingual Yoruba Dataset","abstract":"This paper presents Yankari, a large-scale monolingual dataset for the Yoruba language, aimed at addressing the critical gap in Natural Language Processing (NLP) resources for this important West African language. Despite being spoken by over 30 million people, Yoruba has been severely underrepresented in NLP research and applications. We detail our methodology for creating this dataset, which includes careful source selection, automated quality control, and rigorous data cleaning processes. The Yankari dataset comprises 51,407 documents from 13 diverse sources, totaling over 30 million tokens. Our approach focuses on ethical data collection practices, avoiding problematic sources and addressing issues prevalent in existing datasets. We provide thorough automated evaluations of the dataset, demonstrating its quality compared to existing resources. The Yankari dataset represents a significant advancement in Yoruba language resources, providing a foundation for developing more accurate NLP models, supporting comparative linguistic studies, and contributing to the digital accessibility of the Yoruba language.","sentences":["This paper presents Yankari, a large-scale monolingual dataset for the Yoruba language, aimed at addressing the critical gap in Natural Language Processing (NLP) resources for this important West African language.","Despite being spoken by over 30 million people, Yoruba has been severely underrepresented in NLP research and applications.","We detail our methodology for creating this dataset, which includes careful source selection, automated quality control, and rigorous data cleaning processes.","The Yankari dataset comprises 51,407 documents from 13 diverse sources, totaling over 30 million tokens.","Our approach focuses on ethical data collection practices, avoiding problematic sources and addressing issues prevalent in existing datasets.","We provide thorough automated evaluations of the dataset, demonstrating its quality compared to existing resources.","The Yankari dataset represents a significant advancement in Yoruba language resources, providing a foundation for developing more accurate NLP models, supporting comparative linguistic studies, and contributing to the digital accessibility of the Yoruba language."],"url":"http://arxiv.org/abs/2412.03334v1"}
{"created":"2024-12-04 14:03:27","title":"On Approximability of $\\ell_2^2$ Min-Sum Clustering","abstract":"The $\\ell_2^2$ min-sum $k$-clustering problem is to partition an input set into clusters $C_1,\\ldots,C_k$ to minimize $\\sum_{i=1}^k\\sum_{p,q\\in C_i}\\|p-q\\|_2^2$. Although $\\ell_2^2$ min-sum $k$-clustering is NP-hard, it is not known whether it is NP-hard to approximate $\\ell_2^2$ min-sum $k$-clustering beyond a certain factor.   In this paper, we give the first hardness-of-approximation result for the $\\ell_2^2$ min-sum $k$-clustering problem. We show that it is NP-hard to approximate the objective to a factor better than $1.056$ and moreover, assuming a balanced variant of the Johnson Coverage Hypothesis, it is NP-hard to approximate the objective to a factor better than 1.327.   We then complement our hardness result by giving the first $(1+\\varepsilon)$-coreset construction for $\\ell_2^2$ min-sum $k$-clustering. Our coreset uses $\\mathcal{O}\\left(k^{\\varepsilon^{-4}}\\right)$ space and can be leveraged to achieve a polynomial-time approximation scheme with runtime $nd\\cdot f(k,\\varepsilon^{-1})$, where $d$ is the underlying dimension of the input dataset and $f$ is a fixed function.   Finally, we consider a learning-augmented setting, where the algorithm has access to an oracle that outputs a label $i\\in[k]$ for input point, thereby implicitly partitioning the input dataset into $k$ clusters that induce an approximately optimal solution, up to some amount of adversarial error $\\alpha\\in\\left[0,\\frac{1}{2}\\right)$. We give a polynomial-time algorithm that outputs a $\\frac{1+\\gamma\\alpha}{(1-\\alpha)^2}$-approximation to $\\ell_2^2$ min-sum $k$-clustering, for a fixed constant $\\gamma>0$.","sentences":["The $\\ell_2^2$ min-sum $k$-clustering problem is to partition an input set into clusters $C_1,\\ldots,C_k$ to minimize $\\sum_{i=1}^k\\sum_{p,q\\in C_i}\\|p-q\\|_2^2$.","Although $\\ell_2^2$ min-sum $k$-clustering is NP-hard, it is not known whether it is NP-hard to approximate $\\ell_2^2$ min-sum $k$-clustering beyond a certain factor.   ","In this paper, we give the first hardness-of-approximation result for the $\\ell_2^2$ min-sum $k$-clustering problem.","We show that it is NP-hard to approximate the objective to a factor better than $1.056$ and moreover, assuming a balanced variant of the Johnson Coverage Hypothesis, it is NP-hard to approximate the objective to a factor better than 1.327.   ","We then complement our hardness result by giving the first $(1+\\varepsilon)$-coreset construction for $\\ell_2^2$ min-sum $k$-clustering.","Our coreset uses $\\mathcal{O}\\left(k^{\\varepsilon^{-4}}\\right)$ space and can be leveraged to achieve a polynomial-time approximation scheme with runtime $nd\\cdot f(k,\\varepsilon^{-1})$, where $d$ is the underlying dimension of the input dataset and $f$ is a fixed function.   ","Finally, we consider a learning-augmented setting, where the algorithm has access to an oracle that outputs a label $i\\in[k]$ for input point, thereby implicitly partitioning the input dataset into $k$ clusters that induce an approximately optimal solution, up to some amount of adversarial error $\\alpha\\in\\left[0,\\frac{1}{2}\\right)$. We give a polynomial-time algorithm that outputs a $\\frac{1+\\gamma\\alpha}{(1-\\alpha)^2}$-approximation to $\\ell_2^2$ min-sum $k$-clustering, for a fixed constant $\\gamma>0$."],"url":"http://arxiv.org/abs/2412.03332v1"}
{"created":"2024-12-04 14:02:12","title":"LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings","abstract":"Sentence embedding models play a key role in various Natural Language Processing tasks, such as in Topic Modeling, Document Clustering and Recommendation Systems. However, these models rely heavily on parallel data, which can be scarce for many low-resource languages, including Luxembourgish. This scarcity results in suboptimal performance of monolingual and cross-lingual sentence embedding models for these languages. To address this issue, we compile a relatively small but high-quality human-generated cross-lingual parallel dataset to train \\tool, an enhanced sentence embedding model for Luxembourgish with strong cross-lingual capabilities. Additionally, we present evidence suggesting that including low-resource languages in parallel training datasets can be more advantageous for other low-resource languages than relying solely on high-resource language pairs. Furthermore, recognizing the lack of sentence embedding benchmarks for low-resource languages, we create a paraphrase detection benchmark specifically for Luxembourgish, aiming to partially fill this gap and promote further research.","sentences":["Sentence embedding models play a key role in various Natural Language Processing tasks, such as in Topic Modeling, Document Clustering and Recommendation Systems.","However, these models rely heavily on parallel data, which can be scarce for many low-resource languages, including Luxembourgish.","This scarcity results in suboptimal performance of monolingual and cross-lingual sentence embedding models for these languages.","To address this issue, we compile a relatively small but high-quality human-generated cross-lingual parallel dataset to train \\tool, an enhanced sentence embedding model for Luxembourgish with strong cross-lingual capabilities.","Additionally, we present evidence suggesting that including low-resource languages in parallel training datasets can be more advantageous for other low-resource languages than relying solely on high-resource language pairs.","Furthermore, recognizing the lack of sentence embedding benchmarks for low-resource languages, we create a paraphrase detection benchmark specifically for Luxembourgish, aiming to partially fill this gap and promote further research."],"url":"http://arxiv.org/abs/2412.03331v1"}
{"created":"2024-12-04 13:55:14","title":"Scalable Bayesian Tensor Ring Factorization for Multiway Data Analysis","abstract":"Tensor decompositions play a crucial role in numerous applications related to multi-way data analysis. By employing a Bayesian framework with sparsity-inducing priors, Bayesian Tensor Ring (BTR) factorization offers probabilistic estimates and an effective approach for automatically adapting the tensor ring rank during the learning process. However, previous BTR method employs an Automatic Relevance Determination (ARD) prior, which can lead to sub-optimal solutions. Besides, it solely focuses on continuous data, whereas many applications involve discrete data. More importantly, it relies on the Coordinate-Ascent Variational Inference (CAVI) algorithm, which is inadequate for handling large tensors with extensive observations. These limitations greatly limit its application scales and scopes, making it suitable only for small-scale problems, such as image/video completion. To address these issues, we propose a novel BTR model that incorporates a nonparametric Multiplicative Gamma Process (MGP) prior, known for its superior accuracy in identifying latent structures. To handle discrete data, we introduce the P\\'olya-Gamma augmentation for closed-form updates. Furthermore, we develop an efficient Gibbs sampler for consistent posterior simulation, which reduces the computational complexity of previous VI algorithm by two orders, and an online EM algorithm that is scalable to extremely large tensors. To showcase the advantages of our model, we conduct extensive experiments on both simulation data and real-world applications.","sentences":["Tensor decompositions play a crucial role in numerous applications related to multi-way data analysis.","By employing a Bayesian framework with sparsity-inducing priors, Bayesian Tensor Ring (BTR) factorization offers probabilistic estimates and an effective approach for automatically adapting the tensor ring rank during the learning process.","However, previous BTR method employs an Automatic Relevance Determination (ARD) prior, which can lead to sub-optimal solutions.","Besides, it solely focuses on continuous data, whereas many applications involve discrete data.","More importantly, it relies on the Coordinate-Ascent Variational Inference (CAVI) algorithm, which is inadequate for handling large tensors with extensive observations.","These limitations greatly limit its application scales and scopes, making it suitable only for small-scale problems, such as image/video completion.","To address these issues, we propose a novel BTR model that incorporates a nonparametric Multiplicative Gamma Process (MGP) prior, known for its superior accuracy in identifying latent structures.","To handle discrete data, we introduce the P\\'olya-Gamma augmentation for closed-form updates.","Furthermore, we develop an efficient Gibbs sampler for consistent posterior simulation, which reduces the computational complexity of previous VI algorithm by two orders, and an online EM algorithm that is scalable to extremely large tensors.","To showcase the advantages of our model, we conduct extensive experiments on both simulation data and real-world applications."],"url":"http://arxiv.org/abs/2412.03321v1"}
{"created":"2024-12-04 13:52:04","title":"FlashAttention on a Napkin: A Diagrammatic Approach to Deep Learning IO-Awareness","abstract":"Optimizing deep learning algorithms currently requires slow, manual derivation, potentially leaving much performance untapped. Methods like FlashAttention have achieved a x6 performance improvement over native PyTorch by avoiding unnecessary data transfers, but required three iterations over three years. Automated compiled methods have consistently lagged behind. GPUs are limited by both transfers to processors and available compute, with transfer bandwidth having improved at a far slower pace. Already, transfer bandwidth accounts for 46% of GPU energy costs. This indicates the future of energy and capital-efficient algorithms relies on improved consideration of transfer costs (IO-awareness) and a systematic method for deriving optimized algorithms. In this paper, we present a diagrammatic approach to deep learning models which, with simple relabelings, derive optimal implementations and performance models that consider low-level memory. Diagrams generalize down the GPU hierarchy, providing a universal performance model for comparing hardware and quantization choices. Diagrams generate pseudocode, which reveals the application of hardware-specific features such as coalesced memory access, tensor core operations, and overlapped computation. We present attention algorithms for Ampere, which fits 13 warps per SM (FlashAttention fits 8), and for Hopper, which has improved overlapping and may achieve 1.32 PFLOPs.","sentences":["Optimizing deep learning algorithms currently requires slow, manual derivation, potentially leaving much performance untapped.","Methods like FlashAttention have achieved a x6 performance improvement over native PyTorch by avoiding unnecessary data transfers, but required three iterations over three years.","Automated compiled methods have consistently lagged behind.","GPUs are limited by both transfers to processors and available compute, with transfer bandwidth having improved at a far slower pace.","Already, transfer bandwidth accounts for 46% of GPU energy costs.","This indicates the future of energy and capital-efficient algorithms relies on improved consideration of transfer costs (IO-awareness) and a systematic method for deriving optimized algorithms.","In this paper, we present a diagrammatic approach to deep learning models which, with simple relabelings, derive optimal implementations and performance models that consider low-level memory.","Diagrams generalize down the GPU hierarchy, providing a universal performance model for comparing hardware and quantization choices.","Diagrams generate pseudocode, which reveals the application of hardware-specific features such as coalesced memory access, tensor core operations, and overlapped computation.","We present attention algorithms for Ampere, which fits 13 warps per SM (FlashAttention fits 8), and for Hopper, which has improved overlapping and may achieve 1.32 PFLOPs."],"url":"http://arxiv.org/abs/2412.03317v1"}
{"created":"2024-12-04 13:47:51","title":"Geometry-guided Cross-view Diffusion for One-to-many Cross-view Image Synthesis","abstract":"This paper presents a novel approach for cross-view synthesis aimed at generating plausible ground-level images from corresponding satellite imagery or vice versa. We refer to these tasks as satellite-to-ground (Sat2Grd) and ground-to-satellite (Grd2Sat) synthesis, respectively. Unlike previous works that typically focus on one-to-one generation, producing a single output image from a single input image, our approach acknowledges the inherent one-to-many nature of the problem. This recognition stems from the challenges posed by differences in illumination, weather conditions, and occlusions between the two views. To effectively model this uncertainty, we leverage recent advancements in diffusion models. Specifically, we exploit random Gaussian noise to represent the diverse possibilities learnt from the target view data. We introduce a Geometry-guided Cross-view Condition (GCC) strategy to establish explicit geometric correspondences between satellite and street-view features. This enables us to resolve the geometry ambiguity introduced by camera pose between image pairs, boosting the performance of cross-view image synthesis. Through extensive quantitative and qualitative analyses on three benchmark cross-view datasets, we demonstrate the superiority of our proposed geometry-guided cross-view condition over baseline methods, including recent state-of-the-art approaches in cross-view image synthesis. Our method generates images of higher quality, fidelity, and diversity than other state-of-the-art approaches.","sentences":["This paper presents a novel approach for cross-view synthesis aimed at generating plausible ground-level images from corresponding satellite imagery or vice versa.","We refer to these tasks as satellite-to-ground (Sat2Grd) and ground-to-satellite (Grd2Sat) synthesis, respectively.","Unlike previous works that typically focus on one-to-one generation, producing a single output image from a single input image, our approach acknowledges the inherent one-to-many nature of the problem.","This recognition stems from the challenges posed by differences in illumination, weather conditions, and occlusions between the two views.","To effectively model this uncertainty, we leverage recent advancements in diffusion models.","Specifically, we exploit random Gaussian noise to represent the diverse possibilities learnt from the target view data.","We introduce a Geometry-guided Cross-view Condition (GCC) strategy to establish explicit geometric correspondences between satellite and street-view features.","This enables us to resolve the geometry ambiguity introduced by camera pose between image pairs, boosting the performance of cross-view image synthesis.","Through extensive quantitative and qualitative analyses on three benchmark cross-view datasets, we demonstrate the superiority of our proposed geometry-guided cross-view condition over baseline methods, including recent state-of-the-art approaches in cross-view image synthesis.","Our method generates images of higher quality, fidelity, and diversity than other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2412.03315v1"}
{"created":"2024-12-04 13:32:14","title":"Typologie des comportements utilisateurs : {\u00e9}tude exploratoire des sessions de recherche complexe sur le Web","abstract":"In this study, we propose an exploratory approach aiming at a typology of user behaviour during a Web search session. We describe a typology based on generic IR variables (e.g. number of queries), but also on the study of topic (propositions with distinct semantic content defined from the search statement). To this end, we gathered experimental data enabling us to study variations across users (N=70) for the same task. We performed a multidimensional analysis and propose a 5 classes typology based on the individual behaviours during the processing of a complex search task.","sentences":["In this study, we propose an exploratory approach aiming at a typology of user behaviour during a Web search session.","We describe a typology based on generic IR variables (e.g. number of queries), but also on the study of topic (propositions with distinct semantic content defined from the search statement).","To this end, we gathered experimental data enabling us to study variations across users (N=70) for the same task.","We performed a multidimensional analysis and propose a 5 classes typology based on the individual behaviours during the processing of a complex search task."],"url":"http://arxiv.org/abs/2412.03309v1"}
{"created":"2024-12-04 13:29:52","title":"Contextual Data Integration for Bike-sharing Demand Prediction with Graph Neural Networks in Degraded Weather Conditions","abstract":"Demand for bike sharing is impacted by various factors, such as weather conditions, events, and the availability of other transportation modes. This impact remains elusive due to the complex interdependence of these factors or locationrelated user behavior variations. It is also not clear which factor is additional information which are not already contained in the historical demand. Intermodal dependencies between bike-sharing and other modes are also underexplored, and the value of this information has not been studied in degraded situations. The proposed study analyzes the impact of adding contextual data, such as weather, time embedding, and road traffic flow, to predict bike-sharing Origin-Destination (OD) flows in atypical weather situations Our study highlights a mild relationship between prediction quality of bike-sharing demand and road traffic flow, while the introduced time embedding allows outperforming state-of-the-art results, particularly in the case of degraded weather conditions. Including weather data as an additional input further improves our model with respect to the basic ST-ED-RMGC prediction model by reducing of more than 20% the prediction error in degraded weather condition.","sentences":["Demand for bike sharing is impacted by various factors, such as weather conditions, events, and the availability of other transportation modes.","This impact remains elusive due to the complex interdependence of these factors or locationrelated user behavior variations.","It is also not clear which factor is additional information which are not already contained in the historical demand.","Intermodal dependencies between bike-sharing and other modes are also underexplored, and the value of this information has not been studied in degraded situations.","The proposed study analyzes the impact of adding contextual data, such as weather, time embedding, and road traffic flow, to predict bike-sharing Origin-Destination (OD) flows in atypical weather situations Our study highlights a mild relationship between prediction quality of bike-sharing demand and road traffic flow, while the introduced time embedding allows outperforming state-of-the-art results, particularly in the case of degraded weather conditions.","Including weather data as an additional input further improves our model with respect to the basic ST-ED-RMGC prediction model by reducing of more than 20% the prediction error in degraded weather condition."],"url":"http://arxiv.org/abs/2412.03307v1"}
{"created":"2024-12-04 13:14:35","title":"Digital twin inference from multi-physical simulation data of DED additive manufacturing processes with neural ODEs","abstract":"A digital twin is a virtual representation that accurately replicates its physical counterpart, fostering bi-directional real-time data exchange throughout the entire process lifecycle. For Laser Directed Energy Deposition of Wire (DED-LB/w) additive manufacturing processes, digital twins may help to control the residual stress design in build parts. This study focuses on providing faster-than-real-time and highly accurate surrogate models for the formation of residual stresses by employing neural ordinary differential equations. The approach enables accurate prediction of temperatures and altered structural properties like stress tensor components. The developed surrogates can ultimately facilitate on-the-fly re-optimization of the ongoing manufacturing process to achieve desired structural outcomes. Consequently, this building block contributes significantly to realizing digital twins and the first-time-right paradigm in additive manufacturing.","sentences":["A digital twin is a virtual representation that accurately replicates its physical counterpart, fostering bi-directional real-time data exchange throughout the entire process lifecycle.","For Laser Directed Energy Deposition of Wire (DED-LB/w) additive manufacturing processes, digital twins may help to control the residual stress design in build parts.","This study focuses on providing faster-than-real-time and highly accurate surrogate models for the formation of residual stresses by employing neural ordinary differential equations.","The approach enables accurate prediction of temperatures and altered structural properties like stress tensor components.","The developed surrogates can ultimately facilitate on-the-fly re-optimization of the ongoing manufacturing process to achieve desired structural outcomes.","Consequently, this building block contributes significantly to realizing digital twins and the first-time-right paradigm in additive manufacturing."],"url":"http://arxiv.org/abs/2412.03295v1"}
{"created":"2024-12-04 13:11:38","title":"Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression","abstract":"In this paper, we present DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy. Central to our approach is a next-token prediction objective, enabling the model to reason effectively over the user's query in the context of current observations. Subsequently, a diffusion model is attached to generate robust action outputs. To enhance policy learning through self-reasoning, we introduce a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process. The whole framework is simple and flexible, making it easy to deploy and upgrade. We conduct extensive experiments using multiple real robots to validate the effectiveness of DiffusionVLA. Our tests include a challenging factory sorting task, where DiffusionVLA successfully categorizes objects, including those not seen during training. We observe that the reasoning module makes the model interpretable. It allows observers to understand the model thought process and identify potential causes of policy failures. Additionally, we test DiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102 previously unseen objects. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiffusionVLA can follow novel instructions and retain conversational ability. Notably, DiffusionVLA is data-efficient and fast at inference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can train from scratch on less than 50 demonstrations for a complex task. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.","sentences":["In this paper, we present DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy.","Central to our approach is a next-token prediction objective, enabling the model to reason effectively over the user's query in the context of current observations.","Subsequently, a diffusion model is attached to generate robust action outputs.","To enhance policy learning through self-reasoning, we introduce a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process.","The whole framework is simple and flexible, making it easy to deploy and upgrade.","We conduct extensive experiments using multiple real robots to validate the effectiveness of DiffusionVLA.","Our tests include a challenging factory sorting task, where DiffusionVLA successfully categorizes objects, including those not seen during training.","We observe that the reasoning module makes the model interpretable.","It allows observers to understand the model thought process and identify potential causes of policy failures.","Additionally, we test DiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102 previously unseen objects.","Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments.","Furthermore, DiffusionVLA can follow novel instructions and retain conversational ability.","Notably, DiffusionVLA is data-efficient and fast at inference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can train from scratch on less than 50 demonstrations for a complex task.","Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size."],"url":"http://arxiv.org/abs/2412.03293v1"}
{"created":"2024-12-04 13:10:14","title":"DMP_AI: An AI-Aided K-12 System for Teaching and Learning in Diverse Schools","abstract":"The use of Artificial Intelligence (AI) has gained momentum in education. However, the use of AI in K-12 education is still in its nascent stages, and further research and development is needed to realize its potential. Moreover, the creation of a comprehensive and cohesive system that effectively harnesses AI to support teaching and learning across a diverse range of primary and secondary schools presents substantial challenges that need to be addressed. To fill these gaps, especially in countries like China, we designed and implemented the DMP_AI (Data Management Platform_Artificial Intelligence) system, an innovative AI-aided educational system specifically designed for K-12 education. The system utilizes data mining, natural language processing, and machine learning, along with learning analytics, to offer a wide range of features, including student academic performance and behavior prediction, early warning system, analytics of Individualized Education Plan, talented students prediction and identification, and cross-school personalized electives recommendation. The development of this system has been meticulously carried out while prioritizing user privacy and addressing the challenges posed by data heterogeneity. We successfully implemented the DMP_AI system in real-world primary and secondary schools, allowing us to gain valuable insights into the potential and challenges of integrating AI into K-12 education in the real world. This system will serve as a valuable resource for supporting educators in providing effective and inclusive K-12 education.","sentences":["The use of Artificial Intelligence (AI) has gained momentum in education.","However, the use of AI in K-12 education is still in its nascent stages, and further research and development is needed to realize its potential.","Moreover, the creation of a comprehensive and cohesive system that effectively harnesses AI to support teaching and learning across a diverse range of primary and secondary schools presents substantial challenges that need to be addressed.","To fill these gaps, especially in countries like China, we designed and implemented the DMP_AI (Data Management Platform_Artificial Intelligence) system, an innovative AI-aided educational system specifically designed for K-12 education.","The system utilizes data mining, natural language processing, and machine learning, along with learning analytics, to offer a wide range of features, including student academic performance and behavior prediction, early warning system, analytics of Individualized Education Plan, talented students prediction and identification, and cross-school personalized electives recommendation.","The development of this system has been meticulously carried out while prioritizing user privacy and addressing the challenges posed by data heterogeneity.","We successfully implemented the DMP_AI system in real-world primary and secondary schools, allowing us to gain valuable insights into the potential and challenges of integrating AI into K-12 education in the real world.","This system will serve as a valuable resource for supporting educators in providing effective and inclusive K-12 education."],"url":"http://arxiv.org/abs/2412.03292v1"}
{"created":"2024-12-04 12:37:59","title":"Generating Synthetic Genotypes using Diffusion Models","abstract":"In this paper, we introduce the first diffusion model designed to generate complete synthetic human genotypes, which, by standard protocols, one can straightforwardly expand into full-length, DNA-level genomes. The synthetic genotypes mimic real human genotypes without just reproducing known genotypes, in terms of approved metrics. When training biomedically relevant classifiers with synthetic genotypes, accuracy is near-identical to the accuracy achieved when training classifiers with real data. We further demonstrate that augmenting small amounts of real with synthetically generated genotypes drastically improves performance rates. This addresses a significant challenge in translational human genetics: real human genotypes, although emerging in large volumes from genome wide association studies, are sensitive private data, which limits their public availability. Therefore, the integration of additional, insensitive data when striving for rapid sharing of biomedical knowledge of public interest appears imperative.","sentences":["In this paper, we introduce the first diffusion model designed to generate complete synthetic human genotypes, which, by standard protocols, one can straightforwardly expand into full-length, DNA-level genomes.","The synthetic genotypes mimic real human genotypes without just reproducing known genotypes, in terms of approved metrics.","When training biomedically relevant classifiers with synthetic genotypes, accuracy is near-identical to the accuracy achieved when training classifiers with real data.","We further demonstrate that augmenting small amounts of real with synthetically generated genotypes drastically improves performance rates.","This addresses a significant challenge in translational human genetics: real human genotypes, although emerging in large volumes from genome wide association studies, are sensitive private data, which limits their public availability.","Therefore, the integration of additional, insensitive data when striving for rapid sharing of biomedical knowledge of public interest appears imperative."],"url":"http://arxiv.org/abs/2412.03278v1"}
{"created":"2024-12-04 12:25:41","title":"Intent-driven In-context Learning for Few-shot Dialogue State Tracking","abstract":"Dialogue state tracking (DST) plays an essential role in task-oriented dialogue systems. However, user's input may contain implicit information, posing significant challenges for DST tasks. Additionally, DST data includes complex information, which not only contains a large amount of noise unrelated to the current turn, but also makes constructing DST datasets expensive. To address these challenges, we introduce Intent-driven In-context Learning for Few-shot DST (IDIC-DST). By extracting user's intent, we propose an Intent-driven Dialogue Information Augmentation module to augment the dialogue information, which can track dialogue states more effectively. Moreover, we mask noisy information from DST data and rewrite user's input in the Intent-driven Examples Retrieval module, where we retrieve similar examples. We then utilize a pre-trained large language model to update the dialogue state using the augmented dialogue information and examples. Experimental results demonstrate that IDIC-DST achieves state-of-the-art performance in few-shot settings on MultiWOZ 2.1 and MultiWOZ 2.4 datasets.","sentences":["Dialogue state tracking (DST) plays an essential role in task-oriented dialogue systems.","However, user's input may contain implicit information, posing significant challenges for DST tasks.","Additionally, DST data includes complex information, which not only contains a large amount of noise unrelated to the current turn, but also makes constructing DST datasets expensive.","To address these challenges, we introduce Intent-driven In-context Learning for Few-shot DST (IDIC-DST).","By extracting user's intent, we propose an Intent-driven Dialogue Information Augmentation module to augment the dialogue information, which can track dialogue states more effectively.","Moreover, we mask noisy information from DST data and rewrite user's input in the Intent-driven Examples Retrieval module, where we retrieve similar examples.","We then utilize a pre-trained large language model to update the dialogue state using the augmented dialogue information and examples.","Experimental results demonstrate that IDIC-DST achieves state-of-the-art performance in few-shot settings on MultiWOZ 2.1 and MultiWOZ 2.4 datasets."],"url":"http://arxiv.org/abs/2412.03270v1"}
{"created":"2024-12-04 11:59:36","title":"GERD: Geometric event response data generation","abstract":"Event-based vision sensors are appealing because of their time resolution, higher dynamic range, and low-power consumption. They also provide data that is fundamentally different from conventional frame-based cameras: events are sparse, discrete, and require integration in time. Unlike conventional models grounded in established geometric and physical principles, event-based models lack comparable foundations. We introduce a method to generate event-based data under controlled transformations. Specifically, we subject a prototypical object to transformations that change over time to produce carefully curated event videos. We hope this work simplifies studies for geometric approaches in event-based vision. GERD is available at https://github.com/ncskth/gerd","sentences":["Event-based vision sensors are appealing because of their time resolution, higher dynamic range, and low-power consumption.","They also provide data that is fundamentally different from conventional frame-based cameras: events are sparse, discrete, and require integration in time.","Unlike conventional models grounded in established geometric and physical principles, event-based models lack comparable foundations.","We introduce a method to generate event-based data under controlled transformations.","Specifically, we subject a prototypical object to transformations that change over time to produce carefully curated event videos.","We hope this work simplifies studies for geometric approaches in event-based vision.","GERD is available at https://github.com/ncskth/gerd"],"url":"http://arxiv.org/abs/2412.03259v1"}
{"created":"2024-12-04 11:57:36","title":"Learning on One Mode: Addressing Multi-Modality in Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without interacting with the environment. A common challenge is handling multi-modal action distributions, where multiple behaviours are represented in the data. Existing methods often assume unimodal behaviour policies, leading to suboptimal performance when this assumption is violated. We propose Weighted Imitation Learning on One Mode (LOM), a novel approach that focuses on learning from a single, promising mode of the behaviour policy. By using a Gaussian mixture model to identify modes and selecting the best mode based on expected returns, LOM avoids the pitfalls of averaging over conflicting actions. Theoretically, we show that LOM improves performance while maintaining simplicity in policy learning. Empirically, LOM outperforms existing methods on standard D4RL benchmarks and demonstrates its effectiveness in complex, multi-modal scenarios.","sentences":["Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without interacting with the environment.","A common challenge is handling multi-modal action distributions, where multiple behaviours are represented in the data.","Existing methods often assume unimodal behaviour policies, leading to suboptimal performance when this assumption is violated.","We propose Weighted Imitation Learning on One Mode (LOM), a novel approach that focuses on learning from a single, promising mode of the behaviour policy.","By using a Gaussian mixture model to identify modes and selecting the best mode based on expected returns, LOM avoids the pitfalls of averaging over conflicting actions.","Theoretically, we show that LOM improves performance while maintaining simplicity in policy learning.","Empirically, LOM outperforms existing methods on standard D4RL benchmarks and demonstrates its effectiveness in complex, multi-modal scenarios."],"url":"http://arxiv.org/abs/2412.03258v1"}
{"created":"2024-12-04 11:52:03","title":"Alignment at Pre-training! Towards Native Alignment for Arabic LLMs","abstract":"The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `post alignment'. We argue that alignment during the pre-training phase, which we term `native alignment', warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community.","sentences":["The alignment of large language models (LLMs) is critical for developing effective and safe language models.","Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `post alignment'.","We argue that alignment during the pre-training phase, which we term `native alignment', warrants investigation.","Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing.","This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models.","Our study specifically explores the application of native alignment in the context of Arabic LLMs.","We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability.","Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community."],"url":"http://arxiv.org/abs/2412.03253v1"}
{"created":"2024-12-04 11:51:50","title":"Variable-Speed Teaching-Playback as Real-World Data Augmentation for Imitation Learning","abstract":"Because imitation learning relies on human demonstrations in hard-to-simulate settings, the inclusion of force control in this method has resulted in a shortage of training data, even with a simple change in speed. Although the field of data augmentation has addressed the lack of data, conventional methods of data augmentation for robot manipulation are limited to simulation-based methods or downsampling for position control. This paper proposes a novel method of data augmentation that is applicable to force control and preserves the advantages of real-world datasets. We applied teaching-playback at variable speeds as real-world data augmentation to increase both the quantity and quality of environmental reactions at variable speeds. An experiment was conducted on bilateral control-based imitation learning using a method of imitation learning equipped with position-force control. We evaluated the effect of real-world data augmentation on two tasks, pick-and-place and wiping, at variable speeds, each from two human demonstrations at fixed speed. The results showed a maximum 55% increase in success rate from a simple change in speed of real-world reactions and improved accuracy along the duration/frequency command by gathering environmental reactions at variable speeds.","sentences":["Because imitation learning relies on human demonstrations in hard-to-simulate settings, the inclusion of force control in this method has resulted in a shortage of training data, even with a simple change in speed.","Although the field of data augmentation has addressed the lack of data, conventional methods of data augmentation for robot manipulation are limited to simulation-based methods or downsampling for position control.","This paper proposes a novel method of data augmentation that is applicable to force control and preserves the advantages of real-world datasets.","We applied teaching-playback at variable speeds as real-world data augmentation to increase both the quantity and quality of environmental reactions at variable speeds.","An experiment was conducted on bilateral control-based imitation learning using a method of imitation learning equipped with position-force control.","We evaluated the effect of real-world data augmentation on two tasks, pick-and-place and wiping, at variable speeds, each from two human demonstrations at fixed speed.","The results showed a maximum 55% increase in success rate from a simple change in speed of real-world reactions and improved accuracy along the duration/frequency command by gathering environmental reactions at variable speeds."],"url":"http://arxiv.org/abs/2412.03252v1"}
{"created":"2024-12-04 11:47:57","title":"AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning","abstract":"Large language models (LLMs) have enabled the creation of multi-modal LLMs that exhibit strong comprehension of visual data such as images and videos. However, these models usually rely on extensive visual tokens from visual encoders, leading to high computational demands, which limits their applicability in resource-constrained environments and for long-context tasks. In this work, we propose a training-free adaptive inference method for multi-modal LLMs that can accommodate a broad range of efficiency requirements with a minimum performance drop. Our method consists of a) iterative token merging based on embedding similarity before LLMs, and b) progressive token pruning within LLM layers based on multi-modal importance. With a minimalist design, our method can be applied to both video and image LLMs. Extensive experiments on diverse video and image benchmarks demonstrate that, our method substantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in FLOPs) while preserving the performance of video and image LLMs. Further, under a similar computational cost, our method outperforms the state-of-the-art methods in long video understanding (e.g., $\\textbf{+4.6}$ on MLVU). Additionally, our in-depth analysis provides insights into token redundancy and LLM layer behaviors, offering guidance for future research in designing efficient multi-modal LLMs. Our code will be available at https://github.com/LaVi-Lab/AIM.","sentences":["Large language models (LLMs) have enabled the creation of multi-modal LLMs that exhibit strong comprehension of visual data such as images and videos.","However, these models usually rely on extensive visual tokens from visual encoders, leading to high computational demands, which limits their applicability in resource-constrained environments and for long-context tasks.","In this work, we propose a training-free adaptive inference method for multi-modal LLMs that can accommodate a broad range of efficiency requirements with a minimum performance drop.","Our method consists of a) iterative token merging based on embedding similarity before LLMs, and b) progressive token pruning within LLM layers based on multi-modal importance.","With a minimalist design, our method can be applied to both video and image LLMs.","Extensive experiments on diverse video and image benchmarks demonstrate that, our method substantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in FLOPs) while preserving the performance of video and image LLMs.","Further, under a similar computational cost, our method outperforms the state-of-the-art methods in long video understanding (e.g., $\\textbf{+4.6}$ on MLVU).","Additionally, our in-depth analysis provides insights into token redundancy and LLM layer behaviors, offering guidance for future research in designing efficient multi-modal LLMs.","Our code will be available at https://github.com/LaVi-Lab/AIM."],"url":"http://arxiv.org/abs/2412.03248v1"}
{"created":"2024-12-04 11:39:03","title":"Dynamic Consistent $k$-Center Clustering with Optimal Recourse","abstract":"Given points from an arbitrary metric space and a sequence of point updates sent by an adversary, what is the minimum recourse per update (i.e., the minimum number of changes needed to the set of centers after an update), in order to maintain a constant-factor approximation to a $k$-clustering problem? This question has received attention in recent years under the name consistent clustering.   Previous works by Lattanzi and Vassilvitskii [ICLM '17] and Fichtenberger, Lattanzi, Norouzi-Fard, and Svensson [SODA '21] studied $k$-clustering objectives, including the $k$-center and the $k$-median objectives, under only point insertions. In this paper we study the $k$-center objective in the fully dynamic setting, where the update is either a point insertion or a point deletion. Before our work, {\\L}\\k{a}cki, Haeupler, Grunau, Rozho\\v{n}, and Jayaram [SODA '24] gave a deterministic fully dynamic constant-factor approximation algorithm for the $k$-center objective with worst-case recourse of $2$ per update.   In this work, we prove that the $k$-center clustering problem admits optimal recourse bounds by developing a deterministic fully dynamic constant-factor approximation algorithm with worst-case recourse of $1$ per update. Moreover our algorithm performs simple choices based on light data structures, and thus is arguably more direct and faster than the previous one which uses a sophisticated combinatorial structure. Additionally, we develop a new deterministic decremental algorithm and a new deterministic incremental algorithm, both of which maintain a $6$-approximate $k$-center solution with worst-case recourse of $1$ per update. Our incremental algorithm improves over the $8$-approximation algorithm by Charikar, Chekuri, Feder, and Motwani [STOC '97]. Finally, we remark that since all three of our algorithms are deterministic, they work against an adaptive adversary.","sentences":["Given points from an arbitrary metric space and a sequence of point updates sent by an adversary, what is the minimum recourse per update (i.e., the minimum number of changes needed to the set of centers after an update), in order to maintain a constant-factor approximation to a $k$-clustering problem?","This question has received attention in recent years under the name consistent clustering.   ","Previous works by Lattanzi and Vassilvitskii [ICLM '17] and Fichtenberger, Lattanzi, Norouzi-Fard, and Svensson","[SODA '21] studied $k$-clustering objectives, including the $k$-center and the $k$-median objectives, under only point insertions.","In this paper we study the $k$-center objective in the fully dynamic setting, where the update is either a point insertion or a point deletion.","Before our work, {\\L}\\k{a}cki, Haeupler, Grunau, Rozho\\v{n}, and Jayaram","[SODA '24] gave a deterministic fully dynamic constant-factor approximation algorithm for the $k$-center objective with worst-case recourse of $2$ per update.   ","In this work, we prove that the $k$-center clustering problem admits optimal recourse bounds by developing a deterministic fully dynamic constant-factor approximation algorithm with worst-case recourse of $1$ per update.","Moreover our algorithm performs simple choices based on light data structures, and thus is arguably more direct and faster than the previous one which uses a sophisticated combinatorial structure.","Additionally, we develop a new deterministic decremental algorithm and a new deterministic incremental algorithm, both of which maintain a $6$-approximate $k$-center solution with worst-case recourse of $1$ per update.","Our incremental algorithm improves over the $8$-approximation algorithm by Charikar, Chekuri, Feder, and Motwani","[STOC '97].","Finally, we remark that since all three of our algorithms are deterministic, they work against an adaptive adversary."],"url":"http://arxiv.org/abs/2412.03238v1"}
{"created":"2024-12-04 11:21:30","title":"Channel Reflection: Knowledge-Driven Data Augmentation for EEG-Based Brain-Computer Interfaces","abstract":"A brain-computer interface (BCI) enables direct communication between the human brain and external devices. Electroencephalography (EEG) based BCIs are currently the most popular for able-bodied users. To increase user-friendliness, usually a small amount of user-specific EEG data are used for calibration, which may not be enough to develop a pure data-driven decoding model. To cope with this typical calibration data shortage challenge in EEG-based BCIs, this paper proposes a parameter-free channel reflection (CR) data augmentation approach that incorporates prior knowledge on the channel distributions of different BCI paradigms in data augmentation. Experiments on eight public EEG datasets across four different BCI paradigms (motor imagery, steady-state visual evoked potential, P300, and seizure classifications) using different decoding algorithms demonstrated that: 1) CR is effective, i.e., it can noticeably improve the classification accuracy; 2) CR is robust, i.e., it consistently outperforms existing data augmentation approaches in the literature; and, 3) CR is flexible, i.e., it can be combined with other data augmentation approaches to further increase the performance. We suggest that data augmentation approaches like CR should be an essential step in EEG-based BCIs. Our code is available online.","sentences":["A brain-computer interface (BCI) enables direct communication between the human brain and external devices.","Electroencephalography (EEG) based BCIs are currently the most popular for able-bodied users.","To increase user-friendliness, usually a small amount of user-specific EEG data are used for calibration, which may not be enough to develop a pure data-driven decoding model.","To cope with this typical calibration data shortage challenge in EEG-based BCIs, this paper proposes a parameter-free channel reflection (CR) data augmentation approach that incorporates prior knowledge on the channel distributions of different BCI paradigms in data augmentation.","Experiments on eight public EEG datasets across four different BCI paradigms (motor imagery, steady-state visual evoked potential, P300, and seizure classifications) using different decoding algorithms demonstrated that: 1) CR is effective, i.e., it can noticeably improve the classification accuracy; 2) CR is robust, i.e., it consistently outperforms existing data augmentation approaches in the literature; and, 3) CR is flexible, i.e., it can be combined with other data augmentation approaches to further increase the performance.","We suggest that data augmentation approaches like CR should be an essential step in EEG-based BCIs.","Our code is available online."],"url":"http://arxiv.org/abs/2412.03224v1"}
{"created":"2024-12-04 11:18:32","title":"Linq-Embed-Mistral Technical Report","abstract":"This report explores the enhancement of text retrieval performance using advanced data refinement techniques. We develop Linq-Embed-Mistral\\footnote{\\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}} by building on the E5-mistral and Mistral-7B-v0.1 models, focusing on sophisticated data crafting, data filtering, and negative mining methods, which are highly tailored to each task, applied to both existing benchmark dataset and highly tailored synthetic dataset generated via large language models (LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024), achieving an average score of 68.2 across 56 datasets, and ranks 1st among all models for retrieval tasks on the MTEB leaderboard with a performance score of 60.2. This performance underscores its superior capability in enhancing search precision and reliability. Our contributions include advanced data refinement methods that significantly improve model performance on benchmark and synthetic datasets, techniques for homogeneous task ordering and mixed task fine-tuning to enhance model generalization and stability, and a streamlined evaluation process using 4-bit precision and a light retrieval evaluation set, which accelerates validation without sacrificing accuracy.","sentences":["This report explores the enhancement of text retrieval performance using advanced data refinement techniques.","We develop Linq-Embed-Mistral\\footnote{\\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}} by building on the E5-mistral and Mistral-7B-v0.1 models, focusing on sophisticated data crafting, data filtering, and negative mining methods, which are highly tailored to each task, applied to both existing benchmark dataset and highly tailored synthetic dataset generated via large language models (LLMs).","Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024), achieving an average score of 68.2 across 56 datasets, and ranks 1st among all models for retrieval tasks on the MTEB leaderboard with a performance score of 60.2.","This performance underscores its superior capability in enhancing search precision and reliability.","Our contributions include advanced data refinement methods that significantly improve model performance on benchmark and synthetic datasets, techniques for homogeneous task ordering and mixed task fine-tuning to enhance model generalization and stability, and a streamlined evaluation process using 4-bit precision and a light retrieval evaluation set, which accelerates validation without sacrificing accuracy."],"url":"http://arxiv.org/abs/2412.03223v1"}
{"created":"2024-12-04 11:14:06","title":"Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges","abstract":"Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.","sentences":["Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries.","These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters.","They are typically trained on vast datasets, utilizing architectures based on transformer blocks.","Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis.","An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video.","This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content.","This survey provides a comprehensive overview of the recent advancements in LLMs.","We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs.","We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations.","Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development."],"url":"http://arxiv.org/abs/2412.03220v1"}
{"created":"2024-12-04 11:05:01","title":"Continual Low-Rank Scaled Dot-product Attention","abstract":"Transformers are widely used for their ability to capture data relations in sequence processing, with great success for a wide range of static tasks. However, the computational and memory footprint of their main component, i.e., the Scaled Dot-product Attention, is commonly overlooked. This makes their adoption in applications involving stream data processing with constraints in response latency, computational and memory resources infeasible. Some works have proposed methods to lower the computational cost of transformers, i.e. low-rank approximations, sparsity in attention, and efficient formulations for Continual Inference. In this paper, we introduce a new formulation of the Scaled Dot-product Attention based on the Nystr\\\"om approximation that is suitable for Continual Inference. In experiments on Online Audio Classification and Online Action Detection tasks, the proposed Continual Scaled Dot-product Attention can lower the number of operations by up to three orders of magnitude compared to the original Transformers while retaining the predictive performance of competing models.","sentences":["Transformers are widely used for their ability to capture data relations in sequence processing, with great success for a wide range of static tasks.","However, the computational and memory footprint of their main component, i.e., the Scaled Dot-product Attention, is commonly overlooked.","This makes their adoption in applications involving stream data processing with constraints in response latency, computational and memory resources infeasible.","Some works have proposed methods to lower the computational cost of transformers, i.e. low-rank approximations, sparsity in attention, and efficient formulations for Continual Inference.","In this paper, we introduce a new formulation of the Scaled Dot-product Attention based on the Nystr\\\"om approximation that is suitable for Continual Inference.","In experiments on Online Audio Classification and Online Action Detection tasks, the proposed Continual Scaled Dot-product Attention can lower the number of operations by up to three orders of magnitude compared to the original Transformers while retaining the predictive performance of competing models."],"url":"http://arxiv.org/abs/2412.03214v1"}
{"created":"2024-12-04 10:57:55","title":"Semi-Supervised Transfer Boosting (SS-TrBoosting)","abstract":"Semi-supervised domain adaptation (SSDA) aims at training a high-performance model for a target domain using few labeled target data, many unlabeled target data, and plenty of auxiliary data from a source domain. Previous works in SSDA mainly focused on learning transferable representations across domains. However, it is difficult to find a feature space where the source and target domains share the same conditional probability distribution. Additionally, there is no flexible and effective strategy extending existing unsupervised domain adaptation (UDA) approaches to SSDA settings. In order to solve the above two challenges, we propose a novel fine-tuning framework, semi-supervised transfer boosting (SS-TrBoosting). Given a well-trained deep learning-based UDA or SSDA model, we use it as the initial model, generate additional base learners by boosting, and then use all of them as an ensemble. More specifically, half of the base learners are generated by supervised domain adaptation, and half by semi-supervised learning. Furthermore, for more efficient data transmission and better data privacy protection, we propose a source data generation approach to extend SS-TrBoosting to semi-supervised source-free domain adaptation (SS-SFDA). Extensive experiments showed that SS-TrBoosting can be applied to a variety of existing UDA, SSDA and SFDA approaches to further improve their performance.","sentences":["Semi-supervised domain adaptation (SSDA) aims at training a high-performance model for a target domain using few labeled target data, many unlabeled target data, and plenty of auxiliary data from a source domain.","Previous works in SSDA mainly focused on learning transferable representations across domains.","However, it is difficult to find a feature space where the source and target domains share the same conditional probability distribution.","Additionally, there is no flexible and effective strategy extending existing unsupervised domain adaptation (UDA) approaches to SSDA settings.","In order to solve the above two challenges, we propose a novel fine-tuning framework, semi-supervised transfer boosting (SS-TrBoosting).","Given a well-trained deep learning-based UDA or SSDA model, we use it as the initial model, generate additional base learners by boosting, and then use all of them as an ensemble.","More specifically, half of the base learners are generated by supervised domain adaptation, and half by semi-supervised learning.","Furthermore, for more efficient data transmission and better data privacy protection, we propose a source data generation approach to extend SS-TrBoosting to semi-supervised source-free domain adaptation (SS-SFDA).","Extensive experiments showed that SS-TrBoosting can be applied to a variety of existing UDA, SSDA and SFDA approaches to further improve their performance."],"url":"http://arxiv.org/abs/2412.03212v1"}
{"created":"2024-12-04 10:55:44","title":"Parametric Enhancement of PerceptNet: A Human-Inspired Approach for Image Quality Assessment","abstract":"While deep learning models can learn human-like features at earlier levels, which suggests their utility in modeling human vision, few attempts exist to incorporate these features by design. Current approaches mostly optimize all parameters blindly, only constraining minor architectural aspects. This paper demonstrates how parametrizing neural network layers enables more biologically-plausible operations while reducing trainable parameters and improving interpretability. We constrain operations to functional forms present in human vision, optimizing only these functions' parameters rather than all convolutional tensor elements independently. We present two parametric model versions: one with hand-chosen biologically plausible parameters, and another fitted to human perception experimental data. We compare these with a non-parametric version. All models achieve comparable state-of-the-art results, with parametric versions showing orders of magnitude parameter reduction for minimal performance loss. The parametric models demonstrate improved interpretability and training behavior. Notably, the model fitted to human perception, despite biological initialization, converges to biologically incorrect results. This raises scientific questions and highlights the need for diverse evaluation methods to measure models' humanness, rather than assuming task performance correlates with human-like behavior.","sentences":["While deep learning models can learn human-like features at earlier levels, which suggests their utility in modeling human vision, few attempts exist to incorporate these features by design.","Current approaches mostly optimize all parameters blindly, only constraining minor architectural aspects.","This paper demonstrates how parametrizing neural network layers enables more biologically-plausible operations while reducing trainable parameters and improving interpretability.","We constrain operations to functional forms present in human vision, optimizing only these functions' parameters rather than all convolutional tensor elements independently.","We present two parametric model versions: one with hand-chosen biologically plausible parameters, and another fitted to human perception experimental data.","We compare these with a non-parametric version.","All models achieve comparable state-of-the-art results, with parametric versions showing orders of magnitude parameter reduction for minimal performance loss.","The parametric models demonstrate improved interpretability and training behavior.","Notably, the model fitted to human perception, despite biological initialization, converges to biologically incorrect results.","This raises scientific questions and highlights the need for diverse evaluation methods to measure models' humanness, rather than assuming task performance correlates with human-like behavior."],"url":"http://arxiv.org/abs/2412.03210v1"}
{"created":"2024-12-04 10:41:01","title":"TrustOps: Continuously Building Trustworthy Software","abstract":"Software services play a crucial role in daily life, with automated actions determining access to resources and information. Trusting service providers to perform these actions fairly and accurately is essential, yet challenging for users to verify. Even with publicly available codebases, the rapid pace of development and the complexity of modern deployments hinder the understanding and evaluation of service actions, including for experts. Hence, current trust models rely heavily on the assumption that service providers follow best practices and adhere to laws and regulations, which is increasingly impractical and risky, leading to undetected flaws and data leaks.   In this paper, we argue that gathering verifiable evidence during software development and operations is needed for creating a new trust model. Therefore, we present TrustOps, an approach for continuously collecting verifiable evidence in all phases of the software life cycle, relying on and combining already existing tools and trust-enhancing technologies to do so. For this, we introduce the adaptable core principles of TrustOps and provide a roadmap for future research and development.","sentences":["Software services play a crucial role in daily life, with automated actions determining access to resources and information.","Trusting service providers to perform these actions fairly and accurately is essential, yet challenging for users to verify.","Even with publicly available codebases, the rapid pace of development and the complexity of modern deployments hinder the understanding and evaluation of service actions, including for experts.","Hence, current trust models rely heavily on the assumption that service providers follow best practices and adhere to laws and regulations, which is increasingly impractical and risky, leading to undetected flaws and data leaks.   ","In this paper, we argue that gathering verifiable evidence during software development and operations is needed for creating a new trust model.","Therefore, we present TrustOps, an approach for continuously collecting verifiable evidence in all phases of the software life cycle, relying on and combining already existing tools and trust-enhancing technologies to do so.","For this, we introduce the adaptable core principles of TrustOps and provide a roadmap for future research and development."],"url":"http://arxiv.org/abs/2412.03201v1"}
{"created":"2024-12-04 10:25:53","title":"Biologically-inspired Semi-supervised Semantic Segmentation for Biomedical Imaging","abstract":"We propose a novel two-stage semi-supervised learning approach for training downsampling-upsampling semantic segmentation architectures. The first stage does not use backpropagation. Rather, it exploits the bio-inspired Hebbian principle \"fire together, wire together\" as a local learning rule for updating the weights of both convolutional and transpose-convolutional layers, allowing unsupervised discovery of data features. In the second stage, the model is fine-tuned with standard backpropagation on a small subset of labeled data. We evaluate our methodology through experiments conducted on several widely used biomedical datasets, deeming that this domain is paramount in computer vision and is notably impacted by data scarcity. Results show that our proposed method outperforms SOTA approaches across different levels of label availability. Furthermore, we show that using our unsupervised stage to initialize the SOTA approaches leads to performance improvements. The code to replicate our experiments can be found at: https://github.com/ciampluca/hebbian-medical-image-segmentation","sentences":["We propose a novel two-stage semi-supervised learning approach for training downsampling-upsampling semantic segmentation architectures.","The first stage does not use backpropagation.","Rather, it exploits the bio-inspired Hebbian principle \"fire together, wire together\" as a local learning rule for updating the weights of both convolutional and transpose-convolutional layers, allowing unsupervised discovery of data features.","In the second stage, the model is fine-tuned with standard backpropagation on a small subset of labeled data.","We evaluate our methodology through experiments conducted on several widely used biomedical datasets, deeming that this domain is paramount in computer vision and is notably impacted by data scarcity.","Results show that our proposed method outperforms SOTA approaches across different levels of label availability.","Furthermore, we show that using our unsupervised stage to initialize the SOTA approaches leads to performance improvements.","The code to replicate our experiments can be found at: https://github.com/ciampluca/hebbian-medical-image-segmentation"],"url":"http://arxiv.org/abs/2412.03192v1"}
{"created":"2024-12-04 10:20:21","title":"Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for Traffic Prediction","abstract":"In smart mobility, large networks of geographically distributed sensors produce vast amounts of high-frequency spatio-temporal data that must be processed in real time to avoid major disruptions. Traditional centralized approaches are increasingly unsuitable to this task, as they struggle to scale with expanding sensor networks, and reliability issues in central components can easily affect the whole deployment. To address these challenges, we explore and adapt semi-decentralized training techniques for Spatio-Temporal Graph Neural Networks (ST-GNNs) in smart mobility domain. We implement a simulation framework where sensors are grouped by proximity into multiple cloudlets, each handling a subgraph of the traffic graph, fetching node features from other cloudlets to train its own local ST-GNN model, and exchanging model updates with other cloudlets to ensure consistency, enhancing scalability and removing reliance on a centralized aggregator. We perform extensive comparative evaluation of four different ST-GNN training setups -- centralized, traditional FL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the METR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed predictions. Experimental results show that semi-decentralized setups are comparable to centralized approaches in performance metrics, while offering advantages in terms of scalability and fault tolerance. In addition, we highlight often overlooked issues in existing literature for distributed ST-GNNs, such as the variation in model performance across different geographical areas due to region-specific traffic patterns, and the significant communication overhead and computational costs that arise from the large receptive field of GNNs, leading to substantial data transfers and increased computation of partial embeddings.","sentences":["In smart mobility, large networks of geographically distributed sensors produce vast amounts of high-frequency spatio-temporal data that must be processed in real time to avoid major disruptions.","Traditional centralized approaches are increasingly unsuitable to this task, as they struggle to scale with expanding sensor networks, and reliability issues in central components can easily affect the whole deployment.","To address these challenges, we explore and adapt semi-decentralized training techniques for Spatio-Temporal Graph Neural Networks (ST-GNNs) in smart mobility domain.","We implement a simulation framework where sensors are grouped by proximity into multiple cloudlets, each handling a subgraph of the traffic graph, fetching node features from other cloudlets to train its own local ST-GNN model, and exchanging model updates with other cloudlets to ensure consistency, enhancing scalability and removing reliance on a centralized aggregator.","We perform extensive comparative evaluation of four different ST-GNN training setups -- centralized, traditional FL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the METR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed predictions.","Experimental results show that semi-decentralized setups are comparable to centralized approaches in performance metrics, while offering advantages in terms of scalability and fault tolerance.","In addition, we highlight often overlooked issues in existing literature for distributed ST-GNNs, such as the variation in model performance across different geographical areas due to region-specific traffic patterns, and the significant communication overhead and computational costs that arise from the large receptive field of GNNs, leading to substantial data transfers and increased computation of partial embeddings."],"url":"http://arxiv.org/abs/2412.03188v1"}
{"created":"2024-12-04 09:57:57","title":"Automatic detection of diseases in Spanish clinical notes combining medical language models and ontologies","abstract":"In this paper we present a hybrid method for the automatic detection of dermatological pathologies in medical reports. We use a large language model combined with medical ontologies to predict, given a first appointment or follow-up medical report, the pathology a person may suffer from. The results show that teaching the model to learn the type, severity and location on the body of a dermatological pathology, as well as in which order it has to learn these three features, significantly increases its accuracy. The article presents the demonstration of state-of-the-art results for classification of medical texts with a precision of 0.84, micro and macro F1-score of 0.82 and 0.75, and makes both the method and the data set used available to the community.","sentences":["In this paper we present a hybrid method for the automatic detection of dermatological pathologies in medical reports.","We use a large language model combined with medical ontologies to predict, given a first appointment or follow-up medical report, the pathology a person may suffer from.","The results show that teaching the model to learn the type, severity and location on the body of a dermatological pathology, as well as in which order it has to learn these three features, significantly increases its accuracy.","The article presents the demonstration of state-of-the-art results for classification of medical texts with a precision of 0.84, micro and macro F1-score of 0.82 and 0.75, and makes both the method and the data set used available to the community."],"url":"http://arxiv.org/abs/2412.03176v1"}
{"created":"2024-12-04 09:46:59","title":"Numin: Weighted-Majority Ensembles for Intraday Trading","abstract":"We consider the application of machine learning models for short-term intra-day trading in equities. We envisage a scenario wherein machine learning models are submitted by independent data scientists to predict discretised ten-candle returns every five minutes, in response to five-minute candlestick data provided to them in near real-time. An ensemble model combines these multiple models via a weighted-majority algorithm. The weights of each model are dynamically updated based on the performance of each model, and can also be used to reward model owners. Each model's performance is evaluated according to two different metrics over a recent time window: In addition to accuracy, we also consider a `utility' metric that is a proxy for a model's potential profitability under a particular trading strategy. We present experimental results on real intra-day data that show that our weighted-majority ensemble techniques show improved accuracy as well as utility over any of the individual models, especially using the utility metric to dynamically re-weight models over shorter time-windows.","sentences":["We consider the application of machine learning models for short-term intra-day trading in equities.","We envisage a scenario wherein machine learning models are submitted by independent data scientists to predict discretised ten-candle returns every five minutes, in response to five-minute candlestick data provided to them in near real-time.","An ensemble model combines these multiple models via a weighted-majority algorithm.","The weights of each model are dynamically updated based on the performance of each model, and can also be used to reward model owners.","Each model's performance is evaluated according to two different metrics over a recent time window: In addition to accuracy, we also consider a `utility' metric that is a proxy for a model's potential profitability under a particular trading strategy.","We present experimental results on real intra-day data that show that our weighted-majority ensemble techniques show improved accuracy as well as utility over any of the individual models, especially using the utility metric to dynamically re-weight models over shorter time-windows."],"url":"http://arxiv.org/abs/2412.03167v1"}
{"created":"2024-12-04 09:13:03","title":"MCVO: A Generic Visual Odometry for Arbitrarily Arranged Multi-Cameras","abstract":"Making multi-camera visual SLAM systems easier to set up and more robust to the environment is always one of the focuses of vision robots. Existing monocular and binocular vision SLAM systems have narrow FoV and are fragile in textureless environments with degenerated accuracy and limited robustness. Thus multi-camera SLAM systems are gaining attention because they can provide redundancy for texture degeneration with wide FoV. However, current multi-camera SLAM systems face massive data processing pressure and elaborately designed camera configurations, leading to estimation failures for arbitrarily arranged multi-camera systems. To address these problems, we propose a generic visual odometry for arbitrarily arranged multi-cameras, which can achieve metric-scale state estimation with high flexibility in the cameras' arrangement. Specifically, we first design a learning-based feature extraction and tracking framework to shift the pressure of CPU processing of multiple video streams. Then we use the rigid constraints between cameras to estimate the metric scale poses for robust SLAM system initialization. Finally, we fuse the features of the multi-cameras in the SLAM back-end to achieve robust pose estimation and online scale optimization. Additionally, multi-camera features help improve the loop detection for pose graph optimization. Experiments on KITTI-360 and MultiCamData datasets validate the robustness of our method over arbitrarily placed cameras. Compared with other stereo and multi-camera visual SLAM systems, our method obtains higher pose estimation accuracy with better generalization ability. Our codes and online demos are available at \\url{https://github.com/JunhaoWang615/MCVO}","sentences":["Making multi-camera visual SLAM systems easier to set up and more robust to the environment is always one of the focuses of vision robots.","Existing monocular and binocular vision SLAM systems have narrow FoV and are fragile in textureless environments with degenerated accuracy and limited robustness.","Thus multi-camera SLAM systems are gaining attention because they can provide redundancy for texture degeneration with wide FoV.","However, current multi-camera SLAM systems face massive data processing pressure and elaborately designed camera configurations, leading to estimation failures for arbitrarily arranged multi-camera systems.","To address these problems, we propose a generic visual odometry for arbitrarily arranged multi-cameras, which can achieve metric-scale state estimation with high flexibility in the cameras' arrangement.","Specifically, we first design a learning-based feature extraction and tracking framework to shift the pressure of CPU processing of multiple video streams.","Then we use the rigid constraints between cameras to estimate the metric scale poses for robust SLAM system initialization.","Finally, we fuse the features of the multi-cameras in the SLAM back-end to achieve robust pose estimation and online scale optimization.","Additionally, multi-camera features help improve the loop detection for pose graph optimization.","Experiments on KITTI-360 and MultiCamData datasets validate the robustness of our method over arbitrarily placed cameras.","Compared with other stereo and multi-camera visual SLAM systems, our method obtains higher pose estimation accuracy with better generalization ability.","Our codes and online demos are available at \\url{https://github.com/JunhaoWang615/MCVO}"],"url":"http://arxiv.org/abs/2412.03146v1"}
{"created":"2024-12-04 09:08:07","title":"AffordDP: Generalizable Diffusion Policy with Transferable Affordance","abstract":"Diffusion-based policies have shown impressive performance in robotic manipulation tasks while struggling with out-of-domain distributions. Recent efforts attempted to enhance generalization by improving the visual feature encoding for diffusion policy. However, their generalization is typically limited to the same category with similar appearances. Our key insight is that leveraging affordances--manipulation priors that define \"where\" and \"how\" an agent interacts with an object--can substantially enhance generalization to entirely unseen object instances and categories. We introduce the Diffusion Policy with transferable Affordance (AffordDP), designed for generalizable manipulation across novel categories. AffordDP models affordances through 3D contact points and post-contact trajectories, capturing the essential static and dynamic information for complex tasks. The transferable affordance from in-domain data to unseen objects is achieved by estimating a 6D transformation matrix using foundational vision models and point cloud registration techniques. More importantly, we incorporate affordance guidance during diffusion sampling that can refine action sequence generation. This guidance directs the generated action to gradually move towards the desired manipulation for unseen objects while keeping the generated action within the manifold of action space. Experimental results from both simulated and real-world environments demonstrate that AffordDP consistently outperforms previous diffusion-based methods, successfully generalizing to unseen instances and categories where others fail.","sentences":["Diffusion-based policies have shown impressive performance in robotic manipulation tasks while struggling with out-of-domain distributions.","Recent efforts attempted to enhance generalization by improving the visual feature encoding for diffusion policy.","However, their generalization is typically limited to the same category with similar appearances.","Our key insight is that leveraging affordances--manipulation priors that define \"where\" and \"how\" an agent interacts with an object--can substantially enhance generalization to entirely unseen object instances and categories.","We introduce the Diffusion Policy with transferable Affordance (AffordDP), designed for generalizable manipulation across novel categories.","AffordDP models affordances through 3D contact points and post-contact trajectories, capturing the essential static and dynamic information for complex tasks.","The transferable affordance from in-domain data to unseen objects is achieved by estimating a 6D transformation matrix using foundational vision models and point cloud registration techniques.","More importantly, we incorporate affordance guidance during diffusion sampling that can refine action sequence generation.","This guidance directs the generated action to gradually move towards the desired manipulation for unseen objects while keeping the generated action within the manifold of action space.","Experimental results from both simulated and real-world environments demonstrate that AffordDP consistently outperforms previous diffusion-based methods, successfully generalizing to unseen instances and categories where others fail."],"url":"http://arxiv.org/abs/2412.03142v1"}
{"created":"2024-12-04 09:02:46","title":"Optimal bounds on a tree inference algorithm","abstract":"This paper tightens the best known analysis of Hein's 1989 algorithm to infer the topology of a weighted tree based on the lengths of paths between its leaves. It shows that the number of length queries required for a degree-$k$ tree of $n$ leaves is $O(n k \\log_k n)$, which is the lower bound. It also presents a family of trees for which the performance is asymptotically better, and shows that no such family exists for a competing $O(n k \\log_k n)$ algorithm.","sentences":["This paper tightens the best known analysis of Hein's 1989 algorithm to infer the topology of a weighted tree based on the lengths of paths between its leaves.","It shows that the number of length queries required for a degree-$k$ tree of $n$ leaves is $O(n k \\log_k n)$, which is the lower bound.","It also presents a family of trees for which the performance is asymptotically better, and shows that no such family exists for a competing $O(n k \\log_k n)$ algorithm."],"url":"http://arxiv.org/abs/2412.03138v1"}
{"created":"2024-12-04 08:58:56","title":"Asynchronous Event-Inertial Odometry using a Unified Gaussian Process Regression Framework","abstract":"Recent works have combined monocular event camera and inertial measurement unit to estimate the $SE(3)$ trajectory. However, the asynchronicity of event cameras brings a great challenge to conventional fusion algorithms. In this paper, we present an asynchronous event-inertial odometry under a unified Gaussian Process (GP) regression framework to naturally fuse asynchronous data associations and inertial measurements. A GP latent variable model is leveraged to build data-driven motion prior and acquire the analytical integration capacity. Then, asynchronous event-based feature associations and integral pseudo measurements are tightly coupled using the same GP framework. Subsequently, this fusion estimation problem is solved by underlying factor graph in a sliding-window manner. With consideration of sparsity, those historical states are marginalized orderly. A twin system is also designed for comparison, where the traditional inertial preintegration scheme is embedded in the GP-based framework to replace the GP latent variable model. Evaluations on public event-inertial datasets demonstrate the validity of both systems. Comparison experiments show competitive precision compared to the state-of-the-art synchronous scheme.","sentences":["Recent works have combined monocular event camera and inertial measurement unit to estimate the $SE(3)$ trajectory.","However, the asynchronicity of event cameras brings a great challenge to conventional fusion algorithms.","In this paper, we present an asynchronous event-inertial odometry under a unified Gaussian Process (GP) regression framework to naturally fuse asynchronous data associations and inertial measurements.","A GP latent variable model is leveraged to build data-driven motion prior and acquire the analytical integration capacity.","Then, asynchronous event-based feature associations and integral pseudo measurements are tightly coupled using the same GP framework.","Subsequently, this fusion estimation problem is solved by underlying factor graph in a sliding-window manner.","With consideration of sparsity, those historical states are marginalized orderly.","A twin system is also designed for comparison, where the traditional inertial preintegration scheme is embedded in the GP-based framework to replace the GP latent variable model.","Evaluations on public event-inertial datasets demonstrate the validity of both systems.","Comparison experiments show competitive precision compared to the state-of-the-art synchronous scheme."],"url":"http://arxiv.org/abs/2412.03136v1"}
{"created":"2024-12-04 08:46:55","title":"Short-reach Optical Communications: A Real-world Task for Neuromorphic Hardware","abstract":"Spiking neural networks (SNNs) emulated on dedicated neuromorphic accelerators promise to offer energy-efficient signal processing. However, the neuromorphic advantage over traditional algorithms still remains to be demonstrated in real-world applications. Here, we describe an intensity-modulation, direct-detection (IM/DD) task that is relevant to high-speed optical communication systems used in data centers. Compared to other machine learning-inspired benchmarks, the task offers several advantages. First, the dataset is inherently time-dependent, i.e., there is a time dimension that can be natively mapped to the dynamic evolution of SNNs. Second, small-scale SNNs can achieve the target accuracy required by technical communication standards. Third, due to the small scale and the defined target accuracy, the task facilitates the optimization for real-world aspects, such as energy efficiency, resource requirements, and system complexity.","sentences":["Spiking neural networks (SNNs) emulated on dedicated neuromorphic accelerators promise to offer energy-efficient signal processing.","However, the neuromorphic advantage over traditional algorithms still remains to be demonstrated in real-world applications.","Here, we describe an intensity-modulation, direct-detection (IM/DD) task that is relevant to high-speed optical communication systems used in data centers.","Compared to other machine learning-inspired benchmarks, the task offers several advantages.","First, the dataset is inherently time-dependent, i.e., there is a time dimension that can be natively mapped to the dynamic evolution of SNNs.","Second, small-scale SNNs can achieve the target accuracy required by technical communication standards.","Third, due to the small scale and the defined target accuracy, the task facilitates the optimization for real-world aspects, such as energy efficiency, resource requirements, and system complexity."],"url":"http://arxiv.org/abs/2412.03129v1"}
{"created":"2024-12-04 08:43:12","title":"Robust Multi-bit Text Watermark with LLM-based Paraphrasers","abstract":"We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder. To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level. Then we use a text classifier as the decoder to decode each bit of the watermark. Through extensive experiments, we show that our watermarks can achieve over 99.99\\% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence. More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data. We also show the stealthiness of our watermark with LLM-based evaluation. We open-source the code: https://github.com/xiaojunxu/multi-bit-text-watermark.","sentences":["We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs.","We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder.","To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level.","Then we use a text classifier as the decoder to decode each bit of the watermark.","Through extensive experiments, we show that our watermarks can achieve over 99.99\\% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence.","More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data.","We also show the stealthiness of our watermark with LLM-based evaluation.","We open-source the code: https://github.com/xiaojunxu/multi-bit-text-watermark."],"url":"http://arxiv.org/abs/2412.03123v1"}
{"created":"2024-12-04 08:40:11","title":"Splats in Splats: Embedding Invisible 3D Watermark within Gaussian Splatting","abstract":"3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations. Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets. However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment. Here we describe WaterGS, the first 3DGS watermarking framework that embeds 3D content in 3DGS itself without modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients. Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity. Extensive experiments indicate that WaterGS significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3X faster rendering speed, while ensuring security, robustness, and user experience. Codes and data will be released at https://water-gs.github.io.","sentences":["3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations.","Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets.","However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment.","Here we describe WaterGS, the first 3DGS watermarking framework that embeds 3D content in 3DGS itself without modifying any attributes of the vanilla 3DGS.","To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients.","Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity.","Extensive experiments indicate that WaterGS significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3X faster rendering speed, while ensuring security, robustness, and user experience.","Codes and data will be released at https://water-gs.github.io."],"url":"http://arxiv.org/abs/2412.03121v1"}
{"created":"2024-12-04 08:39:45","title":"Sinkhorn Algorithm for Sequentially Composed Optimal Transports","abstract":"Sinkhorn algorithm is the de-facto standard approximation algorithm for optimal transport, which has been applied to a variety of applications, including image processing and natural language processing. In theory, the proof of its convergence follows from the convergence of the Sinkhorn--Knopp algorithm for the matrix scaling problem, and Altschuler et al. show that its worst-case time complexity is in near-linear time. Very recently, sequentially composed optimal transports were proposed by Watanabe and Isobe as a hierarchical extension of optimal transports. In this paper, we present an efficient approximation algorithm, namely Sinkhorn algorithm for sequentially composed optimal transports, for its entropic regularization. Furthermore, we present a theoretical analysis of the Sinkhorn algorithm, namely (i) its exponential convergence to the optimal solution with respect to the Hilbert pseudometric, and (ii) a worst-case complexity analysis for the case of one sequential composition.","sentences":["Sinkhorn algorithm is the de-facto standard approximation algorithm for optimal transport, which has been applied to a variety of applications, including image processing and natural language processing.","In theory, the proof of its convergence follows from the convergence of the Sinkhorn--Knopp algorithm for the matrix scaling problem, and Altschuler et al. show that its worst-case time complexity is in near-linear time.","Very recently, sequentially composed optimal transports were proposed by Watanabe and Isobe as a hierarchical extension of optimal transports.","In this paper, we present an efficient approximation algorithm, namely Sinkhorn algorithm for sequentially composed optimal transports, for its entropic regularization.","Furthermore, we present a theoretical analysis of the Sinkhorn algorithm, namely (i) its exponential convergence to the optimal solution with respect to the Hilbert pseudometric, and (ii) a worst-case complexity analysis for the case of one sequential composition."],"url":"http://arxiv.org/abs/2412.03120v1"}
{"created":"2024-12-04 08:20:03","title":"Experience-driven discovery of planning strategies","abstract":"One explanation for how people can plan efficiently despite limited cognitive resources is that we possess a set of adaptive planning strategies and know when and how to use them. But how are these strategies acquired? While previous research has studied how individuals learn to choose among existing strategies, little is known about the process of forming new planning strategies. In this work, we propose that new planning strategies are discovered through metacognitive reinforcement learning. To test this, we designed a novel experiment to investigate the discovery of new planning strategies. We then present metacognitive reinforcement learning models and demonstrate their capability for strategy discovery as well as show that they provide a better explanation of human strategy discovery than alternative learning mechanisms. However, when fitted to human data, these models exhibit a slower discovery rate than humans, leaving room for improvement.","sentences":["One explanation for how people can plan efficiently despite limited cognitive resources is that we possess a set of adaptive planning strategies and know when and how to use them.","But how are these strategies acquired?","While previous research has studied how individuals learn to choose among existing strategies, little is known about the process of forming new planning strategies.","In this work, we propose that new planning strategies are discovered through metacognitive reinforcement learning.","To test this, we designed a novel experiment to investigate the discovery of new planning strategies.","We then present metacognitive reinforcement learning models and demonstrate their capability for strategy discovery as well as show that they provide a better explanation of human strategy discovery than alternative learning mechanisms.","However, when fitted to human data, these models exhibit a slower discovery rate than humans, leaving room for improvement."],"url":"http://arxiv.org/abs/2412.03111v1"}
{"created":"2024-12-04 08:10:48","title":"Few-Shot Learning with Adaptive Weight Masking in Conditional GANs","abstract":"Deep learning has revolutionized various fields, yet its efficacy is hindered by overfitting and the requirement of extensive annotated data, particularly in few-shot learning scenarios where limited samples are available. This paper introduces a novel approach to few-shot learning by employing a Residual Weight Masking Conditional Generative Adversarial Network (RWM-CGAN) for data augmentation. The proposed model integrates residual units within the generator to enhance network depth and sample quality, coupled with a weight mask regularization technique in the discriminator to improve feature learning from small-sample categories. This method addresses the core issues of robustness and generalization in few-shot learning by providing a controlled and clear augmentation of the sample space. Extensive experiments demonstrate that RWM-CGAN not only expands the sample space effectively but also enriches the diversity and quality of generated samples, leading to significant improvements in detection and classification accuracy on public datasets. The paper contributes to the advancement of few-shot learning by offering a practical solution to the challenges posed by data scarcity and the need for rapid generalization to new tasks or categories.","sentences":["Deep learning has revolutionized various fields, yet its efficacy is hindered by overfitting and the requirement of extensive annotated data, particularly in few-shot learning scenarios where limited samples are available.","This paper introduces a novel approach to few-shot learning by employing a Residual Weight Masking Conditional Generative Adversarial Network (RWM-CGAN) for data augmentation.","The proposed model integrates residual units within the generator to enhance network depth and sample quality, coupled with a weight mask regularization technique in the discriminator to improve feature learning from small-sample categories.","This method addresses the core issues of robustness and generalization in few-shot learning by providing a controlled and clear augmentation of the sample space.","Extensive experiments demonstrate that RWM-CGAN not only expands the sample space effectively but also enriches the diversity and quality of generated samples, leading to significant improvements in detection and classification accuracy on public datasets.","The paper contributes to the advancement of few-shot learning by offering a practical solution to the challenges posed by data scarcity and the need for rapid generalization to new tasks or categories."],"url":"http://arxiv.org/abs/2412.03105v1"}
{"created":"2024-12-04 08:06:15","title":"ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning","abstract":"Understanding time series is crucial for its application in real-world scenarios. Recently, large language models (LLMs) have been increasingly applied to time series tasks, leveraging their strong language capabilities to enhance various applications. However, research on multimodal LLMs (MLLMs) for time series understanding and reasoning remains limited, primarily due to the scarcity of high-quality datasets that align time series with textual information. This paper introduces ChatTS, a novel MLLM designed for time series analysis. ChatTS treats time series as a modality, similar to how vision MLLMs process images, enabling it to perform both understanding and reasoning with time series. To address the scarcity of training data, we propose an attribute-based method for generating synthetic time series with detailed attribute descriptions. We further introduce Time Series Evol-Instruct, a novel approach that generates diverse time series Q&As, enhancing the model's reasoning capabilities. To the best of our knowledge, ChatTS is the first MLLM that takes multivariate time series as input, which is fine-tuned exclusively on synthetic datasets. We evaluate its performance using benchmark datasets with real-world data, including six alignment tasks and four reasoning tasks. Our results show that ChatTS significantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and text/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a 25.8% improvement in reasoning tasks.","sentences":["Understanding time series is crucial for its application in real-world scenarios.","Recently, large language models (LLMs) have been increasingly applied to time series tasks, leveraging their strong language capabilities to enhance various applications.","However, research on multimodal LLMs (MLLMs) for time series understanding and reasoning remains limited, primarily due to the scarcity of high-quality datasets that align time series with textual information.","This paper introduces ChatTS, a novel MLLM designed for time series analysis.","ChatTS treats time series as a modality, similar to how vision MLLMs process images, enabling it to perform both understanding and reasoning with time series.","To address the scarcity of training data, we propose an attribute-based method for generating synthetic time series with detailed attribute descriptions.","We further introduce Time Series Evol-Instruct, a novel approach that generates diverse time series Q&As, enhancing the model's reasoning capabilities.","To the best of our knowledge, ChatTS is the first MLLM that takes multivariate time series as input, which is fine-tuned exclusively on synthetic datasets.","We evaluate its performance using benchmark datasets with real-world data, including six alignment tasks and four reasoning tasks.","Our results show that ChatTS significantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and text/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a 25.8% improvement in reasoning tasks."],"url":"http://arxiv.org/abs/2412.03104v1"}
{"created":"2024-12-04 07:48:50","title":"Decentralized Mobile Target Tracking Using Consensus-Based Estimation with Nearly-Constant-Velocity Modeling","abstract":"Mobile target tracking is crucial in various applications such as surveillance and autonomous navigation. This study presents a decentralized tracking framework utilizing a Consensus-Based Estimation Filter (CBEF) integrated with the Nearly-Constant-Velocity (NCV) model to predict a moving target's state. The framework facilitates agents in a network to collaboratively estimate the target's position by sharing local observations and achieving consensus despite communication constraints and measurement noise. A saturation-based filtering technique is employed to enhance robustness by mitigating the impact of noisy sensor data. Simulation results demonstrate that the proposed method effectively reduces the Mean Squared Estimation Error (MSEE) over time, indicating improved estimation accuracy and reliability. The findings underscore the effectiveness of the CBEF in decentralized environments, highlighting its scalability and resilience in the presence of uncertainties.","sentences":["Mobile target tracking is crucial in various applications such as surveillance and autonomous navigation.","This study presents a decentralized tracking framework utilizing a Consensus-Based Estimation Filter (CBEF) integrated with the Nearly-Constant-Velocity (NCV) model to predict a moving target's state.","The framework facilitates agents in a network to collaboratively estimate the target's position by sharing local observations and achieving consensus despite communication constraints and measurement noise.","A saturation-based filtering technique is employed to enhance robustness by mitigating the impact of noisy sensor data.","Simulation results demonstrate that the proposed method effectively reduces the Mean Squared Estimation Error (MSEE) over time, indicating improved estimation accuracy and reliability.","The findings underscore the effectiveness of the CBEF in decentralized environments, highlighting its scalability and resilience in the presence of uncertainties."],"url":"http://arxiv.org/abs/2412.03095v1"}
{"created":"2024-12-04 07:44:58","title":"Expanding Event Modality Applications through a Robust CLIP-Based Encoder","abstract":"This paper introduces a powerful encoder that transfers CLIP`s capabilities to event-based data, enhancing its utility and expanding its applicability across diverse domains. While large-scale datasets have significantly advanced image-based models, the scarcity of comprehensive event datasets has limited performance potential in event modality. To address this challenge, we adapt CLIP`s architecture to align event embeddings with image embeddings, supporting zero-shot learning and preserving text alignment while mitigating catastrophic forgetting. Our encoder achieves strong performance in object recognition, with competitive results in zero-shot and few-shot learning tasks. Notably, it generalizes effectively to events extracted from video data without requiring additional training, highlighting its versatility. Additionally, we integrate this encoder within a cross-modality framework that facilitates interaction across five modalities-Image, Event, Text, Sound, and Depth-expanding the possibilities for cross-modal applications. Overall, this work underscores the transformative potential of a robust event encoder, broadening the scope and utility of event-based data across various fields.","sentences":["This paper introduces a powerful encoder that transfers CLIP`s capabilities to event-based data, enhancing its utility and expanding its applicability across diverse domains.","While large-scale datasets have significantly advanced image-based models, the scarcity of comprehensive event datasets has limited performance potential in event modality.","To address this challenge, we adapt CLIP`s architecture to align event embeddings with image embeddings, supporting zero-shot learning and preserving text alignment while mitigating catastrophic forgetting.","Our encoder achieves strong performance in object recognition, with competitive results in zero-shot and few-shot learning tasks.","Notably, it generalizes effectively to events extracted from video data without requiring additional training, highlighting its versatility.","Additionally, we integrate this encoder within a cross-modality framework that facilitates interaction across five modalities-Image, Event, Text, Sound, and Depth-expanding the possibilities for cross-modal applications.","Overall, this work underscores the transformative potential of a robust event encoder, broadening the scope and utility of event-based data across various fields."],"url":"http://arxiv.org/abs/2412.03093v1"}
{"created":"2024-12-04 07:02:49","title":"RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos","abstract":"Dynamic view synthesis (DVS) has advanced remarkably in recent years, achieving high-fidelity rendering while reducing computational costs. Despite the progress, optimizing dynamic neural fields from casual videos remains challenging, as these videos do not provide direct 3D information, such as camera trajectories or the underlying scene geometry. In this work, we present RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual videos. It effectively learns motion and underlying geometry of scenes by separating dynamic and static primitives, and ensures that the learned motion and geometry are physically plausible by incorporating motion and geometric regularization terms. We also introduce a comprehensive benchmark, Kubric-MRig, that provides extensive camera and object motion along with simultaneous multi-view captures, features that are absent in previous benchmarks. Experimental results demonstrate that the proposed method significantly outperforms previous pose-free dynamic neural fields and achieves competitive rendering quality compared to existing pose-free static neural fields. The code and data are publicly available at https://rodygs.github.io/.","sentences":["Dynamic view synthesis (DVS) has advanced remarkably in recent years, achieving high-fidelity rendering while reducing computational costs.","Despite the progress, optimizing dynamic neural fields from casual videos remains challenging, as these videos do not provide direct 3D information, such as camera trajectories or the underlying scene geometry.","In this work, we present RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual videos.","It effectively learns motion and underlying geometry of scenes by separating dynamic and static primitives, and ensures that the learned motion and geometry are physically plausible by incorporating motion and geometric regularization terms.","We also introduce a comprehensive benchmark, Kubric-MRig, that provides extensive camera and object motion along with simultaneous multi-view captures, features that are absent in previous benchmarks.","Experimental results demonstrate that the proposed method significantly outperforms previous pose-free dynamic neural fields and achieves competitive rendering quality compared to existing pose-free static neural fields.","The code and data are publicly available at https://rodygs.github.io/."],"url":"http://arxiv.org/abs/2412.03077v1"}
{"created":"2024-12-04 06:52:10","title":"ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error Correction","abstract":"Automatic speech Recognition (ASR) is a fundamental and important task in the field of speech and natural language processing. It is an inherent building block in many applications such as voice assistant, speech translation, etc. Despite the advancement of ASR technologies in recent years, it is still inevitable for modern ASR systems to have a substantial number of erroneous recognition due to environmental noise, ambiguity, etc. Therefore, the error correction in ASR is crucial.   Motivated by this, this paper studies ASR error correction in the Chinese language, which is one of the most popular languages and enjoys a large number of users in the world. We first create a benchmark dataset named \\emph{ASR-EC} that contains a wide spectrum of ASR errors generated by industry-grade ASR systems. To the best of our knowledge, it is the first Chinese ASR error correction benchmark. Then, inspired by the recent advances in \\emph{large language models (LLMs)}, we investigate how to harness the power of LLMs to correct ASR errors. We apply LLMs to ASR error correction in three paradigms. The first paradigm is prompting, which is further categorized as zero-shot, few-shot, and multi-step. The second paradigm is finetuning, which finetunes LLMs with ASR error correction data. The third paradigm is multi-modal augmentation, which collectively utilizes the audio and ASR transcripts for error correction. Extensive experiments reveal that prompting is not effective for ASR error correction. Finetuning is effective only for a portion of LLMs. Multi-modal augmentation is the most effective method for error correction and achieves state-of-the-art performance.","sentences":["Automatic speech Recognition (ASR) is a fundamental and important task in the field of speech and natural language processing.","It is an inherent building block in many applications such as voice assistant, speech translation, etc.","Despite the advancement of ASR technologies in recent years, it is still inevitable for modern ASR systems to have a substantial number of erroneous recognition due to environmental noise, ambiguity, etc.","Therefore, the error correction in ASR is crucial.   ","Motivated by this, this paper studies ASR error correction in the Chinese language, which is one of the most popular languages and enjoys a large number of users in the world.","We first create a benchmark dataset named \\emph{ASR-EC} that contains a wide spectrum of ASR errors generated by industry-grade ASR systems.","To the best of our knowledge, it is the first Chinese ASR error correction benchmark.","Then, inspired by the recent advances in \\emph{large language models (LLMs)}, we investigate how to harness the power of LLMs to correct ASR errors.","We apply LLMs to ASR error correction in three paradigms.","The first paradigm is prompting, which is further categorized as zero-shot, few-shot, and multi-step.","The second paradigm is finetuning, which finetunes LLMs with ASR error correction data.","The third paradigm is multi-modal augmentation, which collectively utilizes the audio and ASR transcripts for error correction.","Extensive experiments reveal that prompting is not effective for ASR error correction.","Finetuning is effective only for a portion of LLMs.","Multi-modal augmentation is the most effective method for error correction and achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2412.03075v1"}
{"created":"2024-12-04 06:49:56","title":"Deep Learning based Computer-vision for Enhanced Beamforming","abstract":"Meeting the high data rate demands of modern applications necessitates the utilization of high-frequency spectrum bands, including millimeter-wave and sub-terahertz bands. However, these frequencies require precise alignment of narrow communication beams between transmitters and receivers, typically resulting in significant beam training overhead. This paper introduces a novel end-to-end vision-aided beamforming framework that utilizes images to predict optimal beams while considering geometric adjustments to reduce overhead. Our model demonstrates robust adaptability to dynamic environments without relying on additional training data where the experimental results indicate a top-5 beam prediction accuracy of 98.96%, significantly surpassing current state-of-the-art solutions in vision-aided beamforming.","sentences":["Meeting the high data rate demands of modern applications necessitates the utilization of high-frequency spectrum bands, including millimeter-wave and sub-terahertz bands.","However, these frequencies require precise alignment of narrow communication beams between transmitters and receivers, typically resulting in significant beam training overhead.","This paper introduces a novel end-to-end vision-aided beamforming framework that utilizes images to predict optimal beams while considering geometric adjustments to reduce overhead.","Our model demonstrates robust adaptability to dynamic environments without relying on additional training data where the experimental results indicate a top-5 beam prediction accuracy of 98.96%, significantly surpassing current state-of-the-art solutions in vision-aided beamforming."],"url":"http://arxiv.org/abs/2412.03073v1"}
{"created":"2024-12-04 06:42:55","title":"UTSD: Unified Time Series Diffusion Model","abstract":"Transformer-based architectures have achieved unprecedented success in time series analysis. However, facing the challenge of across-domain modeling, existing studies utilize statistical prior as prompt engineering fails under the huge distribution shift among various domains. In this paper, a Unified Time Series Diffusion (UTSD) model is established for the first time to model the multi-domain probability distribution, utilizing the powerful probability distribution modeling ability of Diffusion. Unlike the autoregressive models that capture the conditional probabilities of the prediction horizon to the historical sequence, we use a diffusion denoising process to model the mixture distribution of the cross-domain data and generate the prediction sequence for the target domain directly utilizing conditional sampling. The proposed UTSD contains three pivotal designs: (1) The condition network captures the multi-scale fluctuation patterns from the observation sequence, which are utilized as context representations to guide the denoising network to generate the prediction sequence; (2) Adapter-based fine-tuning strategy, the multi-domain universal representation learned in the pretraining stage is utilized for downstream tasks in target domains; (3) The diffusion and denoising process on the actual sequence space, combined with the improved classifier free guidance as the conditional generation strategy, greatly improves the stability and accuracy of the downstream task. We conduct extensive experiments on mainstream benchmarks, and the pre-trained UTSD outperforms existing foundation models on all data domains, exhibiting superior zero-shot generalization ability. After training from scratch, UTSD achieves comparable performance against domain-specific proprietary models. The empirical results validate the potential of UTSD as a time series foundational model.","sentences":["Transformer-based architectures have achieved unprecedented success in time series analysis.","However, facing the challenge of across-domain modeling, existing studies utilize statistical prior as prompt engineering fails under the huge distribution shift among various domains.","In this paper, a Unified Time Series Diffusion (UTSD) model is established for the first time to model the multi-domain probability distribution, utilizing the powerful probability distribution modeling ability of Diffusion.","Unlike the autoregressive models that capture the conditional probabilities of the prediction horizon to the historical sequence, we use a diffusion denoising process to model the mixture distribution of the cross-domain data and generate the prediction sequence for the target domain directly utilizing conditional sampling.","The proposed UTSD contains three pivotal designs: (1) The condition network captures the multi-scale fluctuation patterns from the observation sequence, which are utilized as context representations to guide the denoising network to generate the prediction sequence; (2) Adapter-based fine-tuning strategy, the multi-domain universal representation learned in the pretraining stage is utilized for downstream tasks in target domains; (3) The diffusion and denoising process on the actual sequence space, combined with the improved classifier free guidance as the conditional generation strategy, greatly improves the stability and accuracy of the downstream task.","We conduct extensive experiments on mainstream benchmarks, and the pre-trained UTSD outperforms existing foundation models on all data domains, exhibiting superior zero-shot generalization ability.","After training from scratch, UTSD achieves comparable performance against domain-specific proprietary models.","The empirical results validate the potential of UTSD as a time series foundational model."],"url":"http://arxiv.org/abs/2412.03068v1"}
{"created":"2024-12-04 06:25:26","title":"Revisiting Energy-Based Model for Out-of-Distribution Detection","abstract":"Out-of-distribution (OOD) detection is an essential approach to robustifying deep learning models, enabling them to identify inputs that fall outside of their trained distribution. Existing OOD detection methods usually depend on crafted data, such as specific outlier datasets or elaborate data augmentations. While this is reasonable, the frequent mismatch between crafted data and OOD data limits model robustness and generalizability. In response to this issue, we introduce Outlier Exposure by Simple Transformations (OEST), a framework that enhances OOD detection by leveraging \"peripheral-distribution\" (PD) data. Specifically, PD data are samples generated through simple data transformations, thus providing an efficient alternative to manually curated outliers.   We adopt energy-based models (EBMs) to study PD data. We recognize the \"energy barrier\" in OOD detection, which characterizes the energy difference between in-distribution (ID) and OOD samples and eases detection. PD data are introduced to establish the energy barrier during training. Furthermore, this energy barrier concept motivates a theoretically grounded energy-barrier loss to replace the classical energy-bounded loss, leading to an improved paradigm, OEST*, which achieves a more effective and theoretically sound separation between ID and OOD samples. We perform empirical validation of our proposal, and extensive experiments across various benchmarks demonstrate that OEST* achieves better or similar accuracy compared with state-of-the-art methods.","sentences":["Out-of-distribution (OOD) detection is an essential approach to robustifying deep learning models, enabling them to identify inputs that fall outside of their trained distribution.","Existing OOD detection methods usually depend on crafted data, such as specific outlier datasets or elaborate data augmentations.","While this is reasonable, the frequent mismatch between crafted data and OOD data limits model robustness and generalizability.","In response to this issue, we introduce Outlier Exposure by Simple Transformations (OEST), a framework that enhances OOD detection by leveraging \"peripheral-distribution\" (PD) data.","Specifically, PD data are samples generated through simple data transformations, thus providing an efficient alternative to manually curated outliers.   ","We adopt energy-based models (EBMs) to study PD data.","We recognize the \"energy barrier\" in OOD detection, which characterizes the energy difference between in-distribution (ID) and OOD samples and eases detection.","PD data are introduced to establish the energy barrier during training.","Furthermore, this energy barrier concept motivates a theoretically grounded energy-barrier loss to replace the classical energy-bounded loss, leading to an improved paradigm, OEST*, which achieves a more effective and theoretically sound separation between ID and OOD samples.","We perform empirical validation of our proposal, and extensive experiments across various benchmarks demonstrate that OEST* achieves better or similar accuracy compared with state-of-the-art methods."],"url":"http://arxiv.org/abs/2412.03058v1"}
{"created":"2024-12-04 06:12:19","title":"Point-GR: Graph Residual Point Cloud Network for 3D Object Classification and Segmentation","abstract":"In recent years, the challenge of 3D shape analysis within point cloud data has gathered significant attention in computer vision. Addressing the complexities of effective 3D information representation and meaningful feature extraction for classification tasks remains crucial. This paper presents Point-GR, a novel deep learning architecture designed explicitly to transform unordered raw point clouds into higher dimensions while preserving local geometric features. It introduces residual-based learning within the network to mitigate the point permutation issues in point cloud data. The proposed Point-GR network significantly reduced the number of network parameters in Classification and Part-Segmentation compared to baseline graph-based networks. Notably, the Point-GR model achieves a state-of-the-art scene segmentation mean IoU of 73.47% on the S3DIS benchmark dataset, showcasing its effectiveness. Furthermore, the model shows competitive results in Classification and Part-Segmentation tasks.","sentences":["In recent years, the challenge of 3D shape analysis within point cloud data has gathered significant attention in computer vision.","Addressing the complexities of effective 3D information representation and meaningful feature extraction for classification tasks remains crucial.","This paper presents Point-GR, a novel deep learning architecture designed explicitly to transform unordered raw point clouds into higher dimensions while preserving local geometric features.","It introduces residual-based learning within the network to mitigate the point permutation issues in point cloud data.","The proposed Point-GR network significantly reduced the number of network parameters in Classification and Part-Segmentation compared to baseline graph-based networks.","Notably, the Point-GR model achieves a state-of-the-art scene segmentation mean IoU of 73.47% on the S3DIS benchmark dataset, showcasing its effectiveness.","Furthermore, the model shows competitive results in Classification and Part-Segmentation tasks."],"url":"http://arxiv.org/abs/2412.03052v1"}
{"created":"2024-12-04 05:05:30","title":"Edge System Design Using Containers and Unikernels for IoT Applications","abstract":"Edge computing is emerging as a key enabler of low-latency, high-efficiency processing for the Internet of Things (IoT) and other real-time applications. To support these demands, containerization has gained traction in edge computing due to its lightweight virtualization and efficient resource management. However, there is currently no established framework to leverage both containers and unikernels on edge devices for optimized IoT deployments. This paper proposes a hybrid edge system design that leverages container and unikernel technologies to optimize resource utilization based on application complexity. Containers are employed for resource-intensive applications, e.g., computer vision, providing faster processing, flexibility, and ease of deployment. In contrast, unikernels are used for lightweight applications, offering enhanced resource performance with minimal overhead. Our system design also incorporates container orchestration to efficiently manage multiple instances across the edge efficiently, ensuring scalability and reliability. We demonstrate our hybrid approach's performance and efficiency advantages through real-world computer vision and data science applications on ARM-powered edge device. Our results demonstrate that this hybrid approach improves resource utilization and reduces latency compared to traditional virtualized solutions. This work provides insights into optimizing edge infrastructures, enabling more efficient and specialized deployment strategies for diverse application workloads.","sentences":["Edge computing is emerging as a key enabler of low-latency, high-efficiency processing for the Internet of Things (IoT) and other real-time applications.","To support these demands, containerization has gained traction in edge computing due to its lightweight virtualization and efficient resource management.","However, there is currently no established framework to leverage both containers and unikernels on edge devices for optimized IoT deployments.","This paper proposes a hybrid edge system design that leverages container and unikernel technologies to optimize resource utilization based on application complexity.","Containers are employed for resource-intensive applications, e.g., computer vision, providing faster processing, flexibility, and ease of deployment.","In contrast, unikernels are used for lightweight applications, offering enhanced resource performance with minimal overhead.","Our system design also incorporates container orchestration to efficiently manage multiple instances across the edge efficiently, ensuring scalability and reliability.","We demonstrate our hybrid approach's performance and efficiency advantages through real-world computer vision and data science applications on ARM-powered edge device.","Our results demonstrate that this hybrid approach improves resource utilization and reduces latency compared to traditional virtualized solutions.","This work provides insights into optimizing edge infrastructures, enabling more efficient and specialized deployment strategies for diverse application workloads."],"url":"http://arxiv.org/abs/2412.03032v1"}
{"created":"2024-12-04 04:38:45","title":"ASIGN: An Anatomy-aware Spatial Imputation Graphic Network for 3D Spatial Transcriptomics","abstract":"Spatial transcriptomics (ST) is an emerging technology that enables medical computer vision scientists to automatically interpret the molecular profiles underlying morphological features. Currently, however, most deep learning-based ST analyses are limited to two-dimensional (2D) sections, which can introduce diagnostic errors due to the heterogeneity of pathological tissues across 3D sections. Expanding ST to three-dimensional (3D) volumes is challenging due to the prohibitive costs; a 2D ST acquisition already costs over 50 times more than whole slide imaging (WSI), and a full 3D volume with 10 sections can be an order of magnitude more expensive. To reduce costs, scientists have attempted to predict ST data directly from WSI without performing actual ST acquisition. However, these methods typically yield unsatisfying results. To address this, we introduce a novel problem setting: 3D ST imputation using 3D WSI histology sections combined with a single 2D ST slide. To do so, we present the Anatomy-aware Spatial Imputation Graph Network (ASIGN) for more precise, yet affordable, 3D ST modeling. The ASIGN architecture extends existing 2D spatial relationships into 3D by leveraging cross-layer overlap and similarity-based expansion. Moreover, a multi-level spatial attention graph network integrates features comprehensively across different data sources. We evaluated ASIGN on three public spatial transcriptomics datasets, with experimental results demonstrating that ASIGN achieves state-of-the-art performance on both 2D and 3D scenarios. Code is available at https://github.com/hrlblab/ASIGN.","sentences":["Spatial transcriptomics (ST) is an emerging technology that enables medical computer vision scientists to automatically interpret the molecular profiles underlying morphological features.","Currently, however, most deep learning-based ST analyses are limited to two-dimensional (2D) sections, which can introduce diagnostic errors due to the heterogeneity of pathological tissues across 3D sections.","Expanding ST to three-dimensional (3D) volumes is challenging due to the prohibitive costs; a 2D ST acquisition already costs over 50 times more than whole slide imaging (WSI), and a full 3D volume with 10 sections can be an order of magnitude more expensive.","To reduce costs, scientists have attempted to predict ST data directly from WSI without performing actual ST acquisition.","However, these methods typically yield unsatisfying results.","To address this, we introduce a novel problem setting: 3D ST imputation using 3D WSI histology sections combined with a single 2D ST slide.","To do so, we present the Anatomy-aware Spatial Imputation Graph Network (ASIGN) for more precise, yet affordable, 3D ST modeling.","The ASIGN architecture extends existing 2D spatial relationships into 3D by leveraging cross-layer overlap and similarity-based expansion.","Moreover, a multi-level spatial attention graph network integrates features comprehensively across different data sources.","We evaluated ASIGN on three public spatial transcriptomics datasets, with experimental results demonstrating that ASIGN achieves state-of-the-art performance on both 2D and 3D scenarios.","Code is available at https://github.com/hrlblab/ASIGN."],"url":"http://arxiv.org/abs/2412.03026v1"}
{"created":"2024-12-04 04:24:15","title":"PEMF-VVTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm","abstract":"Video Virtual Try-on aims to fluently transfer the garment image to a semantically aligned try-on area in the source person video. Previous methods leveraged the inpainting mask to remove the original garment in the source video, thus achieving accurate garment transfer on simple model videos. However, when these methods are applied to realistic video data with more complex scene changes and posture movements, the overly large and incoherent agnostic masks will destroy the essential spatial-temporal information of the original video, thereby inhibiting the fidelity and coherence of the try-on video. To alleviate this problem, %avoid the inherent deficiencies of mask-based try-on paradigm, we propose a novel point-enhanced mask-free video virtual try-on framework (PEMF-VVTO). Specifically, we first leverage the pre-trained mask-based try-on model to construct large-scale paired training data (pseudo-person samples). Training on these mask-free data enables our model to perceive the original spatial-temporal information while realizing accurate garment transfer. Then, based on the pre-acquired sparse frame-cloth and frame-frame point alignments, we design the point-enhanced spatial attention (PSA) and point-enhanced temporal attention (PTA) to further improve the try-on accuracy and video coherence of the mask-free model. Concretely, PSA explicitly guides the garment transfer to desirable locations through the sparse semantic alignments of video frames and cloth. PTA exploits the temporal attention on sparse point correspondences to enhance the smoothness of generated videos. Extensive qualitative and quantitative experiments clearly illustrate that our PEMF-VVTO can generate more natural and coherent try-on videos than existing state-of-the-art methods.","sentences":["Video Virtual Try-on aims to fluently transfer the garment image to a semantically aligned try-on area in the source person video.","Previous methods leveraged the inpainting mask to remove the original garment in the source video, thus achieving accurate garment transfer on simple model videos.","However, when these methods are applied to realistic video data with more complex scene changes and posture movements, the overly large and incoherent agnostic masks will destroy the essential spatial-temporal information of the original video, thereby inhibiting the fidelity and coherence of the try-on video.","To alleviate this problem, %avoid the inherent deficiencies of mask-based try-on paradigm, we propose a novel point-enhanced mask-free video virtual try-on framework (PEMF-VVTO).","Specifically, we first leverage the pre-trained mask-based try-on model to construct large-scale paired training data (pseudo-person samples).","Training on these mask-free data enables our model to perceive the original spatial-temporal information while realizing accurate garment transfer.","Then, based on the pre-acquired sparse frame-cloth and frame-frame point alignments, we design the point-enhanced spatial attention (PSA) and point-enhanced temporal attention (PTA) to further improve the try-on accuracy and video coherence of the mask-free model.","Concretely, PSA explicitly guides the garment transfer to desirable locations through the sparse semantic alignments of video frames and cloth.","PTA exploits the temporal attention on sparse point correspondences to enhance the smoothness of generated videos.","Extensive qualitative and quantitative experiments clearly illustrate that our PEMF-VVTO can generate more natural and coherent try-on videos than existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2412.03021v1"}
{"created":"2024-12-04 04:03:12","title":"Benchmarking Attention Mechanisms and Consistency Regularization Semi-Supervised Learning for Post-Flood Building Damage Assessment in Satellite Images","abstract":"Post-flood building damage assessment is critical for rapid response and post-disaster reconstruction planning. Current research fails to consider the distinct requirements of disaster assessment (DA) from change detection (CD) in neural network design. This paper focuses on two key differences: 1) building change features in DA satellite images are more subtle than in CD; 2) DA datasets face more severe data scarcity and label imbalance. To address these issues, in terms of model architecture, the research explores the benchmark performance of attention mechanisms in post-flood DA tasks and introduces Simple Prior Attention UNet (SPAUNet) to enhance the model's ability to recognize subtle changes, in terms of semi-supervised learning (SSL) strategies, the paper constructs four different combinations of image-level label category reference distributions for consistent training. Experimental results on flood events of xBD dataset show that SPAUNet performs exceptionally well in supervised learning experiments, achieving a recall of 79.10\\% and an F1 score of 71.32\\% for damaged classification, outperforming CD methods. The results indicate the necessity of DA task-oriented model design. SSL experiments demonstrate the positive impact of image-level consistency regularization on the model. Using pseudo-labels to form the reference distribution for consistency training yields the best results, proving the potential of using the category distribution of a large amount of unlabeled data for SSL. This paper clarifies the differences between DA and CD tasks. It preliminarily explores model design strategies utilizing prior attention mechanisms and image-level consistency regularization, establishing new post-flood DA task benchmark methods.","sentences":["Post-flood building damage assessment is critical for rapid response and post-disaster reconstruction planning.","Current research fails to consider the distinct requirements of disaster assessment (DA) from change detection (CD) in neural network design.","This paper focuses on two key differences: 1) building change features in DA satellite images are more subtle than in CD; 2) DA datasets face more severe data scarcity and label imbalance.","To address these issues, in terms of model architecture, the research explores the benchmark performance of attention mechanisms in post-flood DA tasks and introduces Simple Prior Attention UNet (SPAUNet) to enhance the model's ability to recognize subtle changes, in terms of semi-supervised learning (SSL) strategies, the paper constructs four different combinations of image-level label category reference distributions for consistent training.","Experimental results on flood events of xBD dataset show that SPAUNet performs exceptionally well in supervised learning experiments, achieving a recall of 79.10\\% and an F1 score of 71.32\\% for damaged classification, outperforming CD methods.","The results indicate the necessity of DA task-oriented model design.","SSL experiments demonstrate the positive impact of image-level consistency regularization on the model.","Using pseudo-labels to form the reference distribution for consistency training yields the best results, proving the potential of using the category distribution of a large amount of unlabeled data for SSL.","This paper clarifies the differences between DA and CD tasks.","It preliminarily explores model design strategies utilizing prior attention mechanisms and image-level consistency regularization, establishing new post-flood DA task benchmark methods."],"url":"http://arxiv.org/abs/2412.03015v1"}
{"created":"2024-12-04 03:56:54","title":"Data Acquisition for Improving Model Fairness using Reinforcement Learning","abstract":"Machine learning systems are increasingly being used in critical decision making such as healthcare, finance, and criminal justice. Concerns around their fairness have resulted in several bias mitigation techniques that emphasize the need for high-quality data to ensure fairer decisions. However, the role of earlier stages of machine learning pipelines in mitigating model bias has not been explored well. In this paper, we focus on the task of acquiring additional labeled data points for training the downstream machine learning model to rapidly improve its fairness. Since not all data points in a data pool are equally beneficial to the task of fairness, we generate an ordering in which data points should be acquired. We present DataSift, a data acquisition framework based on the idea of data valuation that relies on partitioning and multi-armed bandits to determine the most valuable data points to acquire. Over several iterations, DataSift selects a partition and randomly samples a batch of data points from the selected partition, evaluates the benefit of acquiring the batch on model fairness, and updates the utility of partitions depending on the benefit. To further improve the effectiveness and efficiency of evaluating batches, we leverage influence functions that estimate the effect of acquiring a batch without retraining the model. We empirically evaluate DataSift on several real-world and synthetic datasets and show that the fairness of a machine learning model can be significantly improved even while acquiring a few data points.","sentences":["Machine learning systems are increasingly being used in critical decision making such as healthcare, finance, and criminal justice.","Concerns around their fairness have resulted in several bias mitigation techniques that emphasize the need for high-quality data to ensure fairer decisions.","However, the role of earlier stages of machine learning pipelines in mitigating model bias has not been explored well.","In this paper, we focus on the task of acquiring additional labeled data points for training the downstream machine learning model to rapidly improve its fairness.","Since not all data points in a data pool are equally beneficial to the task of fairness, we generate an ordering in which data points should be acquired.","We present DataSift, a data acquisition framework based on the idea of data valuation that relies on partitioning and multi-armed bandits to determine the most valuable data points to acquire.","Over several iterations, DataSift selects a partition and randomly samples a batch of data points from the selected partition, evaluates the benefit of acquiring the batch on model fairness, and updates the utility of partitions depending on the benefit.","To further improve the effectiveness and efficiency of evaluating batches, we leverage influence functions that estimate the effect of acquiring a batch without retraining the model.","We empirically evaluate DataSift on several real-world and synthetic datasets and show that the fairness of a machine learning model can be significantly improved even while acquiring a few data points."],"url":"http://arxiv.org/abs/2412.03009v1"}
{"created":"2024-12-04 03:56:14","title":"Provably Extending PageRank-based Local Clustering Algorithm to Weighted Directed Graphs with Self-Loops and to Hypergraphs","abstract":"Local clustering aims to find a compact cluster near the given starting instances. This work focuses on graph local clustering, which has broad applications beyond graphs because of the internal connectivities within various modalities. While most existing studies on local graph clustering adopt the discrete graph setting (i.e., unweighted graphs without self-loops), real-world graphs can be more complex. In this paper, we extend the non-approximating Andersen-Chung-Lang (\"ACL\") algorithm beyond discrete graphs and generalize its quadratic optimality to a wider range of graphs, including weighted, directed, and self-looped graphs and hypergraphs. Specifically, leveraging PageRank, we propose two algorithms: GeneralACL for graphs and HyperACL for hypergraphs. We theoretically prove that, under two mild conditions, both algorithms can identify a quadratically optimal local cluster in terms of conductance with at least 1/2 probability. On the property of hypergraphs, we address a fundamental gap in the literature by defining conductance for hypergraphs from the perspective of hypergraph random walks. Additionally, we provide experiments to validate our theoretical findings.","sentences":["Local clustering aims to find a compact cluster near the given starting instances.","This work focuses on graph local clustering, which has broad applications beyond graphs because of the internal connectivities within various modalities.","While most existing studies on local graph clustering adopt the discrete graph setting (i.e., unweighted graphs without self-loops), real-world graphs can be more complex.","In this paper, we extend the non-approximating Andersen-Chung-Lang (\"ACL\") algorithm beyond discrete graphs and generalize its quadratic optimality to a wider range of graphs, including weighted, directed, and self-looped graphs and hypergraphs.","Specifically, leveraging PageRank, we propose two algorithms: GeneralACL for graphs and HyperACL for hypergraphs.","We theoretically prove that, under two mild conditions, both algorithms can identify a quadratically optimal local cluster in terms of conductance with at least 1/2 probability.","On the property of hypergraphs, we address a fundamental gap in the literature by defining conductance for hypergraphs from the perspective of hypergraph random walks.","Additionally, we provide experiments to validate our theoretical findings."],"url":"http://arxiv.org/abs/2412.03008v1"}
{"created":"2024-12-04 03:35:08","title":"QuadricsReg: Large-Scale Point Cloud Registration using Quadric Primitives","abstract":"In the realm of large-scale point cloud registration, designing a compact symbolic representation is crucial for efficiently processing vast amounts of data, ensuring registration robustness against significant viewpoint variations and occlusions. This paper introduces a novel point cloud registration method, i.e., QuadricsReg, which leverages concise quadrics primitives to represent scenes and utilizes their geometric characteristics to establish correspondences for 6-DoF transformation estimation. As a symbolic feature, the quadric representation fully captures the primary geometric characteristics of scenes, which can efficiently handle the complexity of large-scale point clouds. The intrinsic characteristics of quadrics, such as types and scales, are employed to initialize correspondences. Then we build a multi-level compatibility graph set to find the correspondences using the maximum clique on the geometric consistency between quadrics. Finally, we estimate the 6-DoF transformation using the quadric correspondences, which is further optimized based on the quadric degeneracy-aware distance in a factor graph, ensuring high registration accuracy and robustness against degenerate structures. We test on 5 public datasets and the self-collected heterogeneous dataset across different LiDAR sensors and robot platforms. The exceptional registration success rates and minimal registration errors demonstrate the effectiveness of QuadricsReg in large-scale point cloud registration scenarios. Furthermore, the real-world registration testing on our self-collected heterogeneous dataset shows the robustness and generalization ability of QuadricsReg on different LiDAR sensors and robot platforms. The codes and demos will be released at \\url{https://levenberg.github.io/QuadricsReg}.","sentences":["In the realm of large-scale point cloud registration, designing a compact symbolic representation is crucial for efficiently processing vast amounts of data, ensuring registration robustness against significant viewpoint variations and occlusions.","This paper introduces a novel point cloud registration method, i.e., QuadricsReg, which leverages concise quadrics primitives to represent scenes and utilizes their geometric characteristics to establish correspondences for 6-DoF transformation estimation.","As a symbolic feature, the quadric representation fully captures the primary geometric characteristics of scenes, which can efficiently handle the complexity of large-scale point clouds.","The intrinsic characteristics of quadrics, such as types and scales, are employed to initialize correspondences.","Then we build a multi-level compatibility graph set to find the correspondences using the maximum clique on the geometric consistency between quadrics.","Finally, we estimate the 6-DoF transformation using the quadric correspondences, which is further optimized based on the quadric degeneracy-aware distance in a factor graph, ensuring high registration accuracy and robustness against degenerate structures.","We test on 5 public datasets and the self-collected heterogeneous dataset across different LiDAR sensors and robot platforms.","The exceptional registration success rates and minimal registration errors demonstrate the effectiveness of QuadricsReg in large-scale point cloud registration scenarios.","Furthermore, the real-world registration testing on our self-collected heterogeneous dataset shows the robustness and generalization ability of QuadricsReg on different LiDAR sensors and robot platforms.","The codes and demos will be released at \\url{https://levenberg.github.io/QuadricsReg}."],"url":"http://arxiv.org/abs/2412.02998v1"}
{"created":"2024-12-04 03:19:43","title":"EchoONE: Segmenting Multiple echocardiography Planes in One Model","abstract":"In clinical practice of echocardiography examinations, multiple planes containing the heart structures of different view are usually required in screening, diagnosis and treatment of cardiac disease. AI models for echocardiography have to be tailored for each specific plane due to the dramatic structure differences, thus resulting in repetition development and extra complexity. Effective solution for such a multi-plane segmentation (MPS) problem is highly demanded for medical images, yet has not been well investigated. In this paper, we propose a novel solution, EchoONE, for this problem with a SAM-based segmentation architecture, a prior-composable mask learning (PC-Mask) module for semantic-aware dense prompt generation, and a learnable CNN-branch with a simple yet effective local feature fusion and adaption (LFFA) module for SAM adapting. We extensively evaluated our method on multiple internal and external echocardiography datasets, and achieved consistently state-of-the-art performance for multi-source datasets with different heart planes. This is the first time that the MPS problem is solved in one model for echocardiography data. The code will be available at https://github.com/a2502503/EchoONE.","sentences":["In clinical practice of echocardiography examinations, multiple planes containing the heart structures of different view are usually required in screening, diagnosis and treatment of cardiac disease.","AI models for echocardiography have to be tailored for each specific plane due to the dramatic structure differences, thus resulting in repetition development and extra complexity.","Effective solution for such a multi-plane segmentation (MPS) problem is highly demanded for medical images, yet has not been well investigated.","In this paper, we propose a novel solution, EchoONE, for this problem with a SAM-based segmentation architecture, a prior-composable mask learning (PC-Mask) module for semantic-aware dense prompt generation, and a learnable CNN-branch with a simple yet effective local feature fusion and adaption (LFFA) module for SAM adapting.","We extensively evaluated our method on multiple internal and external echocardiography datasets, and achieved consistently state-of-the-art performance for multi-source datasets with different heart planes.","This is the first time that the MPS problem is solved in one model for echocardiography data.","The code will be available at https://github.com/a2502503/EchoONE."],"url":"http://arxiv.org/abs/2412.02993v1"}
{"created":"2024-12-04 02:47:45","title":"Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models","abstract":"Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.","sentences":["Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks.","Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist.","We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity.","We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models.","We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both.","Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance.","We then examine the effect of various components in the synthetic data pipeline on each data characteristic.","This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition.","This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms.","Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data.","We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement.","We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction."],"url":"http://arxiv.org/abs/2412.02980v1"}
{"created":"2024-12-04 02:37:53","title":"Stain-aware Domain Alignment for Imbalance Blood Cell Classification","abstract":"Blood cell identification is critical for hematological analysis as it aids physicians in diagnosing various blood-related diseases. In real-world scenarios, blood cell image datasets often present the issues of domain shift and data imbalance, posing challenges for accurate blood cell identification. To address these issues, we propose a novel blood cell classification method termed SADA via stain-aware domain alignment. The primary objective of this work is to mine domain-invariant features in the presence of domain shifts and data imbalances. To accomplish this objective, we propose a stain-based augmentation approach and a local alignment constraint to learn domain-invariant features. Furthermore, we propose a domain-invariant supervised contrastive learning strategy to capture discriminative features. We decouple the training process into two stages of domain-invariant feature learning and classification training, alleviating the problem of data imbalance. Experiment results on four public blood cell datasets and a private real dataset collected from the Third Affiliated Hospital of Sun Yat-sen University demonstrate that SADA can achieve a new state-of-the-art baseline, which is superior to the existing cutting-edge methods with a big margin. The source code can be available at the URL (\\url{https://github.com/AnoK3111/SADA}).","sentences":["Blood cell identification is critical for hematological analysis as it aids physicians in diagnosing various blood-related diseases.","In real-world scenarios, blood cell image datasets often present the issues of domain shift and data imbalance, posing challenges for accurate blood cell identification.","To address these issues, we propose a novel blood cell classification method termed SADA via stain-aware domain alignment.","The primary objective of this work is to mine domain-invariant features in the presence of domain shifts and data imbalances.","To accomplish this objective, we propose a stain-based augmentation approach and a local alignment constraint to learn domain-invariant features.","Furthermore, we propose a domain-invariant supervised contrastive learning strategy to capture discriminative features.","We decouple the training process into two stages of domain-invariant feature learning and classification training, alleviating the problem of data imbalance.","Experiment results on four public blood cell datasets and a private real dataset collected from the Third Affiliated Hospital of Sun Yat-sen University demonstrate that SADA can achieve a new state-of-the-art baseline, which is superior to the existing cutting-edge methods with a big margin.","The source code can be available at the URL (\\url{https://github.com/AnoK3111/SADA})."],"url":"http://arxiv.org/abs/2412.02976v1"}
{"created":"2024-12-04 02:37:31","title":"Theoretical limitations of multi-layer Transformer","abstract":"Transformers, especially the decoder-only variants, are the backbone of most modern large language models; yet we do not have much understanding of their expressive power except for the simple $1$-layer case.   Due to the difficulty of analyzing multi-layer models, all previous work relies on unproven complexity conjectures to show limitations for multi-layer Transformers. In this work, we prove the first $\\textit{unconditional}$ lower bound against multi-layer decoder-only transformers. For any constant $L$, we prove that any $L$-layer decoder-only transformer needs a polynomial model dimension ($n^{\\Omega(1)}$) to perform sequential composition of $L$ functions over an input of $n$ tokens.   As a consequence, our results give: (1) the first depth-width trade-off for multi-layer transformers, exhibiting that the $L$-step composition task is exponentially harder for $L$-layer models compared to $(L+1)$-layer ones; (2) an unconditional separation between encoder and decoder, exhibiting a hard task for decoders that can be solved by an exponentially shallower and smaller encoder; (3) a provable advantage of chain-of-thought, exhibiting a task that becomes exponentially easier with chain-of-thought.   On the technical side, we propose the multi-party $\\textit{autoregressive}$ $\\textit{communication}$ $\\textit{model}$ that captures the computation of a decoder-only Transformer. We also introduce a new proof technique that finds a certain $\\textit{indistinguishable}$ $\\textit{decomposition}$ of all possible inputs iteratively for proving lower bounds in this model. We believe our new communication model and proof technique will be helpful to further understand the computational power of transformers.","sentences":["Transformers, especially the decoder-only variants, are the backbone of most modern large language models; yet we do not have much understanding of their expressive power except for the simple $1$-layer case.   ","Due to the difficulty of analyzing multi-layer models, all previous work relies on unproven complexity conjectures to show limitations for multi-layer Transformers.","In this work, we prove the first $\\textit{unconditional}$ lower bound against multi-layer decoder-only transformers.","For any constant $L$, we prove that any $L$-layer decoder-only transformer needs a polynomial model dimension ($n^{\\Omega(1)}$) to perform sequential composition of $L$ functions over an input of $n$ tokens.   ","As a consequence, our results give: (1) the first depth-width trade-off for multi-layer transformers, exhibiting that the $L$-step composition task is exponentially harder for $L$-layer models compared to $(L+1)$-layer ones; (2) an unconditional separation between encoder and decoder, exhibiting a hard task for decoders that can be solved by an exponentially shallower and smaller encoder; (3) a provable advantage of chain-of-thought, exhibiting a task that becomes exponentially easier with chain-of-thought.   ","On the technical side, we propose the multi-party $\\textit{autoregressive}$ $\\textit{communication}$ $\\textit{model}$ that captures the computation of a decoder-only Transformer.","We also introduce a new proof technique that finds a certain $\\textit{indistinguishable}$ $\\textit{decomposition}$ of all possible inputs iteratively for proving lower bounds in this model.","We believe our new communication model and proof technique will be helpful to further understand the computational power of transformers."],"url":"http://arxiv.org/abs/2412.02975v1"}
{"created":"2024-12-04 02:34:38","title":"Supporting Gig Worker Needs and Advancing Policy Through Worker-Centered Data-Sharing","abstract":"The proliferating adoption of platform-based gig work increasingly raises concerns for worker conditions. Past studies documented how platforms leveraged design to exploit labor, withheld information to generate power asymmetries, and left workers alone to manage logistical overheads as well as social isolation. However, researchers also called attention to the potential of helping workers overcome such costs via worker-led datasharing, which can enable collective actions and mutual aid among workers, while offering advocates, lawmakers and regulatory bodies insights for improving work conditions. To understand stakeholders' desiderata for a data-sharing system (i.e. functionality and policy initiatives that it can serve), we interviewed 11 policy domain experts in the U.S. and conducted co-design workshops with 14 active gig workers across four domains. Our results outline policymakers' prioritized initiatives, information needs, and (mis)alignments with workers' concerns and desires around data collectives. We offer design recommendations for data-sharing systems that support worker needs while bringing us closer to legislation that promote more thriving and equitable gig work futures.","sentences":["The proliferating adoption of platform-based gig work increasingly raises concerns for worker conditions.","Past studies documented how platforms leveraged design to exploit labor, withheld information to generate power asymmetries, and left workers alone to manage logistical overheads as well as social isolation.","However, researchers also called attention to the potential of helping workers overcome such costs via worker-led datasharing, which can enable collective actions and mutual aid among workers, while offering advocates, lawmakers and regulatory bodies insights for improving work conditions.","To understand stakeholders' desiderata for a data-sharing system (i.e. functionality and policy initiatives that it can serve), we interviewed 11 policy domain experts in the U.S. and conducted co-design workshops with 14 active gig workers across four domains.","Our results outline policymakers' prioritized initiatives, information needs, and (mis)alignments with workers' concerns and desires around data collectives.","We offer design recommendations for data-sharing systems that support worker needs while bringing us closer to legislation that promote more thriving and equitable gig work futures."],"url":"http://arxiv.org/abs/2412.02973v1"}
{"created":"2024-12-04 02:31:28","title":"How Many Ratings per Item are Necessary for Reliable Significance Testing?","abstract":"Most approaches to machine learning evaluation assume that machine and human responses are repeatable enough to be measured against data with unitary, authoritative, \"gold standard\" responses, via simple metrics such as accuracy, precision, and recall that assume scores are independent given the test item. However, AI models have multiple sources of stochasticity and the human raters who create gold standards tend to disagree with each other, often in meaningful ways, hence a single output response per input item may not provide enough information. We introduce methods for determining whether an (existing or planned) evaluation dataset has enough responses per item to reliably compare the performance of one model to another. We apply our methods to several of very few extant gold standard test sets with multiple disaggregated responses per item and show that there are usually not enough responses per item to reliably compare the performance of one model against another. Our methods also allow us to estimate the number of responses per item for hypothetical datasets with similar response distributions to the existing datasets we study. When two models are very far apart in their predictive performance, fewer raters are needed to confidently compare them, as expected. However, as the models draw closer, we find that a larger number of raters than are currently typical in annotation collection are needed to ensure that the power analysis correctly reflects the difference in performance.","sentences":["Most approaches to machine learning evaluation assume that machine and human responses are repeatable enough to be measured against data with unitary, authoritative, \"gold standard\" responses, via simple metrics such as accuracy, precision, and recall that assume scores are independent given the test item.","However, AI models have multiple sources of stochasticity and the human raters who create gold standards tend to disagree with each other, often in meaningful ways, hence a single output response per input item may not provide enough information.","We introduce methods for determining whether an (existing or planned) evaluation dataset has enough responses per item to reliably compare the performance of one model to another.","We apply our methods to several of very few extant gold standard test sets with multiple disaggregated responses per item and show that there are usually not enough responses per item to reliably compare the performance of one model against another.","Our methods also allow us to estimate the number of responses per item for hypothetical datasets with similar response distributions to the existing datasets we study.","When two models are very far apart in their predictive performance, fewer raters are needed to confidently compare them, as expected.","However, as the models draw closer, we find that a larger number of raters than are currently typical in annotation collection are needed to ensure that the power analysis correctly reflects the difference in performance."],"url":"http://arxiv.org/abs/2412.02968v1"}
{"created":"2024-12-04 02:05:21","title":"Curriculum-style Data Augmentation for LLM-based Metaphor Detection","abstract":"Recently, utilizing large language models (LLMs) for metaphor detection has achieved promising results. However, these methods heavily rely on the capabilities of closed-source LLMs, which come with relatively high inference costs and latency. To address this, we propose a method for metaphor detection by fine-tuning open-source LLMs, effectively reducing inference costs and latency with a single inference step. Furthermore, metaphor detection suffers from a severe data scarcity problem, which hinders effective fine-tuning of LLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA). Specifically, before fine-tuning, we evaluate the training data to identify correctly predicted instances for fine-tuning, while incorrectly predicted instances are used as seed data for data augmentation. This approach enables the model to quickly learn simpler knowledge and progressively acquire more complex knowledge, thereby improving performance incrementally. Experimental results demonstrate that our method achieves state-of-the-art performance across all baselines. Additionally, we provide detailed ablation studies to validate the effectiveness of CDA.","sentences":["Recently, utilizing large language models (LLMs) for metaphor detection has achieved promising results.","However, these methods heavily rely on the capabilities of closed-source LLMs, which come with relatively high inference costs and latency.","To address this, we propose a method for metaphor detection by fine-tuning open-source LLMs, effectively reducing inference costs and latency with a single inference step.","Furthermore, metaphor detection suffers from a severe data scarcity problem, which hinders effective fine-tuning of LLMs.","To tackle this, we introduce Curriculum-style Data Augmentation (CDA).","Specifically, before fine-tuning, we evaluate the training data to identify correctly predicted instances for fine-tuning, while incorrectly predicted instances are used as seed data for data augmentation.","This approach enables the model to quickly learn simpler knowledge and progressively acquire more complex knowledge, thereby improving performance incrementally.","Experimental results demonstrate that our method achieves state-of-the-art performance across all baselines.","Additionally, we provide detailed ablation studies to validate the effectiveness of CDA."],"url":"http://arxiv.org/abs/2412.02956v1"}
{"created":"2024-12-04 01:20:43","title":"STDCformer: A Transformer-Based Model with a Spatial-Temporal Causal De-Confounding Strategy for Crowd Flow Prediction","abstract":"Existing works typically treat spatial-temporal prediction as the task of learning a function $F$ to transform historical observations to future observations. We further decompose this cross-time transformation into three processes: (1) Encoding ($E$): learning the intrinsic representation of observations, (2) Cross-Time Mapping ($M$): transforming past representations into future representations, and (3) Decoding ($D$): reconstructing future observations from the future representations. From this perspective, spatial-temporal prediction can be viewed as learning $F = E \\cdot M \\cdot D$, which includes learning the space transformations $\\left\\{{E},{D}\\right\\}$ between the observation space and the hidden representation space, as well as the spatial-temporal mapping $M$ from future states to past states within the representation space. This leads to two key questions: \\textbf{Q1: What kind of representation space allows for mapping the past to the future? Q2: How to achieve map the past to the future within the representation space?} To address Q1, we propose a Spatial-Temporal Backdoor Adjustment strategy, which learns a Spatial-Temporal De-Confounded (STDC) representation space and estimates the de-confounding causal effect of historical data on future data. This causal relationship we captured serves as the foundation for subsequent spatial-temporal mapping. To address Q2, we design a Spatial-Temporal Embedding (STE) that fuses the information of temporal and spatial confounders, capturing the intrinsic spatial-temporal characteristics of the representations. Additionally, we introduce a Cross-Time Attention mechanism, which queries the attention between the future and the past to guide spatial-temporal mapping.","sentences":["Existing works typically treat spatial-temporal prediction as the task of learning a function $F$ to transform historical observations to future observations.","We further decompose this cross-time transformation into three processes: (1) Encoding ($E$): learning the intrinsic representation of observations, (2) Cross-Time Mapping ($M$): transforming past representations into future representations, and (3) Decoding ($D$): reconstructing future observations from the future representations.","From this perspective, spatial-temporal prediction can be viewed as learning $F = E \\cdot M \\cdot D$, which includes learning the space transformations $\\left\\{{E},{D}\\right\\}$ between the observation space and the hidden representation space, as well as the spatial-temporal mapping $M$ from future states to past states within the representation space.","This leads to two key questions: \\textbf{Q1: What kind of representation space allows for mapping the past to the future?","Q2:","How to achieve map the past to the future within the representation space?} To address Q1, we propose a Spatial-Temporal Backdoor Adjustment strategy, which learns a Spatial-Temporal De-Confounded (STDC) representation space and estimates the de-confounding causal effect of historical data on future data.","This causal relationship we captured serves as the foundation for subsequent spatial-temporal mapping.","To address Q2, we design a Spatial-Temporal Embedding (STE) that fuses the information of temporal and spatial confounders, capturing the intrinsic spatial-temporal characteristics of the representations.","Additionally, we introduce a Cross-Time Attention mechanism, which queries the attention between the future and the past to guide spatial-temporal mapping."],"url":"http://arxiv.org/abs/2412.02942v1"}
{"created":"2024-12-04 01:07:59","title":"Dynamic Graph Neural Ordinary Differential Equation Network for Multi-modal Emotion Recognition in Conversation","abstract":"Multimodal emotion recognition in conversation (MERC) refers to identifying and classifying human emotional states by combining data from multiple different modalities (e.g., audio, images, text, video, etc.). Most existing multimodal emotion recognition methods use GCN to improve performance, but existing GCN methods are prone to overfitting and cannot capture the temporal dependency of the speaker's emotions. To address the above problems, we propose a Dynamic Graph Neural Ordinary Differential Equation Network (DGODE) for MERC, which combines the dynamic changes of emotions to capture the temporal dependency of speakers' emotions, and effectively alleviates the overfitting problem of GCNs. Technically, the key idea of DGODE is to utilize an adaptive mixhop mechanism to improve the generalization ability of GCNs and use the graph ODE evolution network to characterize the continuous dynamics of node representations over time and capture temporal dependencies. Extensive experiments on two publicly available multimodal emotion recognition datasets demonstrate that the proposed DGODE model has superior performance compared to various baselines. Furthermore, the proposed DGODE can also alleviate the over-smoothing problem, thereby enabling the construction of a deep GCN network.","sentences":["Multimodal emotion recognition in conversation (MERC) refers to identifying and classifying human emotional states by combining data from multiple different modalities (e.g., audio, images, text, video, etc.).","Most existing multimodal emotion recognition methods use GCN to improve performance, but existing GCN methods are prone to overfitting and cannot capture the temporal dependency of the speaker's emotions.","To address the above problems, we propose a Dynamic Graph Neural Ordinary Differential Equation Network (DGODE) for MERC, which combines the dynamic changes of emotions to capture the temporal dependency of speakers' emotions, and effectively alleviates the overfitting problem of GCNs.","Technically, the key idea of DGODE is to utilize an adaptive mixhop mechanism to improve the generalization ability of GCNs and use the graph ODE evolution network to characterize the continuous dynamics of node representations over time and capture temporal dependencies.","Extensive experiments on two publicly available multimodal emotion recognition datasets demonstrate that the proposed DGODE model has superior performance compared to various baselines.","Furthermore, the proposed DGODE can also alleviate the over-smoothing problem, thereby enabling the construction of a deep GCN network."],"url":"http://arxiv.org/abs/2412.02935v1"}
{"created":"2024-12-04 01:07:04","title":"BGTplanner: Maximizing Training Accuracy for Differentially Private Federated Recommenders via Strategic Privacy Budget Allocation","abstract":"To mitigate the rising concern about privacy leakage, the federated recommender (FR) paradigm emerges, in which decentralized clients co-train the recommendation model without exposing their raw user-item rating data. The differentially private federated recommender (DPFR) further enhances FR by injecting differentially private (DP) noises into clients. Yet, current DPFRs, suffering from noise distortion, cannot achieve satisfactory accuracy. Various efforts have been dedicated to improving DPFRs by adaptively allocating the privacy budget over the learning process. However, due to the intricate relation between privacy budget allocation and model accuracy, existing works are still far from maximizing DPFR accuracy. To address this challenge, we develop BGTplanner (Budget Planner) to strategically allocate the privacy budget for each round of DPFR training, improving overall training performance. Specifically, we leverage the Gaussian process regression and historical information to predict the change in recommendation accuracy with a certain allocated privacy budget. Additionally, Contextual Multi-Armed Bandit (CMAB) is harnessed to make privacy budget allocation decisions by reconciling the current improvement and long-term privacy constraints. Our extensive experimental results on real datasets demonstrate that \\emph{BGTplanner} achieves an average improvement of 6.76\\% in training performance compared to state-of-the-art baselines.","sentences":["To mitigate the rising concern about privacy leakage, the federated recommender (FR) paradigm emerges, in which decentralized clients co-train the recommendation model without exposing their raw user-item rating data.","The differentially private federated recommender (DPFR) further enhances FR by injecting differentially private (DP) noises into clients.","Yet, current DPFRs, suffering from noise distortion, cannot achieve satisfactory accuracy.","Various efforts have been dedicated to improving DPFRs by adaptively allocating the privacy budget over the learning process.","However, due to the intricate relation between privacy budget allocation and model accuracy, existing works are still far from maximizing DPFR accuracy.","To address this challenge, we develop BGTplanner (Budget Planner) to strategically allocate the privacy budget for each round of DPFR training, improving overall training performance.","Specifically, we leverage the Gaussian process regression and historical information to predict the change in recommendation accuracy with a certain allocated privacy budget.","Additionally, Contextual Multi-Armed Bandit (CMAB) is harnessed to make privacy budget allocation decisions by reconciling the current improvement and long-term privacy constraints.","Our extensive experimental results on real datasets demonstrate that \\emph{BGTplanner} achieves an average improvement of 6.76\\% in training performance compared to state-of-the-art baselines."],"url":"http://arxiv.org/abs/2412.02934v1"}
{"created":"2024-12-04 00:10:47","title":"Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data","abstract":"Transformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due to the quadratic cost of the attention mechanism. In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher-order tensors. To address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis' dimension, rather than quadratic in the total size of the input tensor. To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear. This strategy maintains the model's expressiveness while enabling scalable attention computation. We validate the effectiveness of HOT on two high-dimensional tasks, including multivariate time series forecasting, and 3D medical image classification. Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data.","sentences":["Transformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due to the quadratic cost of the attention mechanism.","In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher-order tensors.","To address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis' dimension, rather than quadratic in the total size of the input tensor.","To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear.","This strategy maintains the model's expressiveness while enabling scalable attention computation.","We validate the effectiveness of HOT on two high-dimensional tasks, including multivariate time series forecasting, and 3D medical image classification.","Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data."],"url":"http://arxiv.org/abs/2412.02919v1"}
{"created":"2024-12-03 23:58:35","title":"Single-Cell Omics Arena: A Benchmark Study for Large Language Models on Cell Type Annotation Using Single-Cell Data","abstract":"Over the past decade, the revolution in single-cell sequencing has enabled the simultaneous molecular profiling of various modalities across thousands of individual cells, allowing scientists to investigate the diverse functions of complex tissues and uncover underlying disease mechanisms. Among all the analytical steps, assigning individual cells to specific types is fundamental for understanding cellular heterogeneity. However, this process is usually labor-intensive and requires extensive expert knowledge. Recent advances in large language models (LLMs) have demonstrated their ability to efficiently process and synthesize vast corpora of text to automatically extract essential biological knowledge, such as marker genes, potentially promoting more efficient and automated cell type annotations. To thoroughly evaluate the capability of modern instruction-tuned LLMs in automating the cell type identification process, we introduce SOAR, a comprehensive benchmarking study of LLMs for cell type annotation tasks in single-cell genomics. Specifically, we assess the performance of 8 instruction-tuned LLMs across 11 datasets, spanning multiple cell types and species. Our study explores the potential of LLMs to accurately classify and annotate cell types in single-cell RNA sequencing (scRNA-seq) data, while extending their application to multiomics data through cross-modality translation. Additionally, we evaluate the effectiveness of chain-of-thought (CoT) prompting techniques in generating detailed biological insights during the annotation process. The results demonstrate that LLMs can provide robust interpretations of single-cell data without requiring additional fine-tuning, advancing the automation of cell type annotation in genomics research.","sentences":["Over the past decade, the revolution in single-cell sequencing has enabled the simultaneous molecular profiling of various modalities across thousands of individual cells, allowing scientists to investigate the diverse functions of complex tissues and uncover underlying disease mechanisms.","Among all the analytical steps, assigning individual cells to specific types is fundamental for understanding cellular heterogeneity.","However, this process is usually labor-intensive and requires extensive expert knowledge.","Recent advances in large language models (LLMs) have demonstrated their ability to efficiently process and synthesize vast corpora of text to automatically extract essential biological knowledge, such as marker genes, potentially promoting more efficient and automated cell type annotations.","To thoroughly evaluate the capability of modern instruction-tuned LLMs in automating the cell type identification process, we introduce SOAR, a comprehensive benchmarking study of LLMs for cell type annotation tasks in single-cell genomics.","Specifically, we assess the performance of 8 instruction-tuned LLMs across 11 datasets, spanning multiple cell types and species.","Our study explores the potential of LLMs to accurately classify and annotate cell types in single-cell RNA sequencing (scRNA-seq) data, while extending their application to multiomics data through cross-modality translation.","Additionally, we evaluate the effectiveness of chain-of-thought (CoT) prompting techniques in generating detailed biological insights during the annotation process.","The results demonstrate that LLMs can provide robust interpretations of single-cell data without requiring additional fine-tuning, advancing the automation of cell type annotation in genomics research."],"url":"http://arxiv.org/abs/2412.02915v1"}
{"created":"2024-12-03 23:19:40","title":"Does Few-Shot Learning Help LLM Performance in Code Synthesis?","abstract":"Large language models (LLMs) have made significant strides at code generation through improved model design, training, and chain-of-thought. However, prompt-level optimizations remain an important yet under-explored aspect of LLMs for coding. This work focuses on the few-shot examples present in most code generation prompts, offering a systematic study on whether few-shot examples improve LLM's coding capabilities, which few-shot examples have the largest impact, and how to select impactful examples. Our work offers 2 approaches for selecting few-shot examples, a model-free method, CODEEXEMPLAR-FREE, and a model-based method, CODEEXEMPLAR-BASED. The 2 methods offer a trade-off between improved performance and reliance on training data and interpretability. Both methods significantly improve CodeLlama's coding ability across the popular HumanEval+ coding benchmark. In summary, our work provides valuable insights into how to pick few-shot examples in code generation prompts to improve LLM code generation capabilities.","sentences":["Large language models (LLMs) have made significant strides at code generation through improved model design, training, and chain-of-thought.","However, prompt-level optimizations remain an important yet under-explored aspect of LLMs for coding.","This work focuses on the few-shot examples present in most code generation prompts, offering a systematic study on whether few-shot examples improve LLM's coding capabilities, which few-shot examples have the largest impact, and how to select impactful examples.","Our work offers 2 approaches for selecting few-shot examples, a model-free method, CODEEXEMPLAR-FREE, and a model-based method, CODEEXEMPLAR-BASED.","The 2 methods offer a trade-off between improved performance and reliance on training data and interpretability.","Both methods significantly improve CodeLlama's coding ability across the popular HumanEval+ coding benchmark.","In summary, our work provides valuable insights into how to pick few-shot examples in code generation prompts to improve LLM code generation capabilities."],"url":"http://arxiv.org/abs/2412.02906v1"}
{"created":"2024-12-03 23:11:06","title":"EgoCast: Forecasting Egocentric Human Pose in the Wild","abstract":"Accurately estimating and forecasting human body pose is important for enhancing the user's sense of immersion in Augmented Reality. Addressing this need, our paper introduces EgoCast, a bimodal method for 3D human pose forecasting using egocentric videos and proprioceptive data. We study the task of human pose forecasting in a realistic setting, extending the boundaries of temporal forecasting in dynamic scenes and building on the current framework for current pose estimation in the wild. We introduce a current-frame estimation module that generates pseudo-groundtruth poses for inference, eliminating the need for past groundtruth poses typically required by current methods during forecasting. Our experimental results on the recent Ego-Exo4D and Aria Digital Twin datasets validate EgoCast for real-life motion estimation. On the Ego-Exo4D Body Pose 2024 Challenge, our method significantly outperforms the state-of-the-art approaches, laying the groundwork for future research in human pose estimation and forecasting in unscripted activities with egocentric inputs.","sentences":["Accurately estimating and forecasting human body pose is important for enhancing the user's sense of immersion in Augmented Reality.","Addressing this need, our paper introduces EgoCast, a bimodal method for 3D human pose forecasting using egocentric videos and proprioceptive data.","We study the task of human pose forecasting in a realistic setting, extending the boundaries of temporal forecasting in dynamic scenes and building on the current framework for current pose estimation in the wild.","We introduce a current-frame estimation module that generates pseudo-groundtruth poses for inference, eliminating the need for past groundtruth poses typically required by current methods during forecasting.","Our experimental results on the recent Ego-Exo4D and Aria Digital Twin datasets validate EgoCast for real-life motion estimation.","On the Ego-Exo4D Body Pose 2024 Challenge, our method significantly outperforms the state-of-the-art approaches, laying the groundwork for future research in human pose estimation and forecasting in unscripted activities with egocentric inputs."],"url":"http://arxiv.org/abs/2412.02903v1"}
{"created":"2024-12-03 22:59:23","title":"GUESS: Generative Uncertainty Ensemble for Self Supervision","abstract":"Self-supervised learning (SSL) frameworks consist of pretext task, and loss function aiming to learn useful general features from unlabeled data. The basic idea of most SSL baselines revolves around enforcing the invariance to a variety of data augmentations via the loss function. However, one main issue is that, inattentive or deterministic enforcement of the invariance to any kind of data augmentation is generally not only inefficient, but also potentially detrimental to performance on the downstream tasks. In this work, we investigate the issue from the viewpoint of uncertainty in invariance representation. Uncertainty representation is fairly under-explored in the design of SSL architectures as well as loss functions. We incorporate uncertainty representation in both loss function as well as architecture design aiming for more data-dependent invariance enforcement. The former is represented in the form of data-derived uncertainty in SSL loss function resulting in a generative-discriminative loss function. The latter is achieved by feeding slightly different distorted versions of samples to the ensemble aiming for learning better and more robust representation. Specifically, building upon the recent methods that use hard and soft whitening (a.k.a redundancy reduction), we introduce a new approach GUESS, a pseudo-whitening framework, composed of controlled uncertainty injection, a new architecture, and a new loss function. We include detailed results and ablation analysis establishing GUESS as a new baseline.","sentences":["Self-supervised learning (SSL) frameworks consist of pretext task, and loss function aiming to learn useful general features from unlabeled data.","The basic idea of most SSL baselines revolves around enforcing the invariance to a variety of data augmentations via the loss function.","However, one main issue is that, inattentive or deterministic enforcement of the invariance to any kind of data augmentation is generally not only inefficient, but also potentially detrimental to performance on the downstream tasks.","In this work, we investigate the issue from the viewpoint of uncertainty in invariance representation.","Uncertainty representation is fairly under-explored in the design of SSL architectures as well as loss functions.","We incorporate uncertainty representation in both loss function as well as architecture design aiming for more data-dependent invariance enforcement.","The former is represented in the form of data-derived uncertainty in SSL loss function resulting in a generative-discriminative loss function.","The latter is achieved by feeding slightly different distorted versions of samples to the ensemble aiming for learning better and more robust representation.","Specifically, building upon the recent methods that use hard and soft whitening (a.k.a redundancy reduction), we introduce a new approach GUESS, a pseudo-whitening framework, composed of controlled uncertainty injection, a new architecture, and a new loss function.","We include detailed results and ablation analysis establishing GUESS as a new baseline."],"url":"http://arxiv.org/abs/2412.02896v1"}
{"created":"2024-12-03 22:58:25","title":"Reconstruction of dynamic systems using genetic algorithms with dynamic search limits","abstract":"Mathematical modeling is a powerful tool for describing, predicting, and understanding complex phenomena exhibited by real-world systems. However, identifying the equations that govern a system's dynamics from experimental data remains a significant challenge without a definitive solution. In this study, evolutionary computing techniques are presented to estimate the governing equations of a dynamical system using time-series data. The main approach is to propose polynomial equations with unknown coefficients, and subsequently perform a parametric estimation using genetic algorithms. Some of the main contributions of the present study are an adequate modification of the genetic algorithm to remove terms with minimal contributions, and a mechanism to escape local optima during the search. To evaluate the proposed method, we applied it to three dynamical systems: a linear model, a nonlinear model, and the Lorenz system. Our results demonstrate a reconstruction with an Integral Square Error below 0.22 and a coefficient of determination R-squared of 0.99 for all systems, indicating successful reconstruction of the governing dynamic equations.","sentences":["Mathematical modeling is a powerful tool for describing, predicting, and understanding complex phenomena exhibited by real-world systems.","However, identifying the equations that govern a system's dynamics from experimental data remains a significant challenge without a definitive solution.","In this study, evolutionary computing techniques are presented to estimate the governing equations of a dynamical system using time-series data.","The main approach is to propose polynomial equations with unknown coefficients, and subsequently perform a parametric estimation using genetic algorithms.","Some of the main contributions of the present study are an adequate modification of the genetic algorithm to remove terms with minimal contributions, and a mechanism to escape local optima during the search.","To evaluate the proposed method, we applied it to three dynamical systems: a linear model, a nonlinear model, and the Lorenz system.","Our results demonstrate a reconstruction with an Integral Square Error below 0.22 and a coefficient of determination R-squared of 0.99 for all systems, indicating successful reconstruction of the governing dynamic equations."],"url":"http://arxiv.org/abs/2412.02894v1"}
{"created":"2024-12-03 22:49:01","title":"EvRT-DETR: The Surprising Effectiveness of DETR-based Detection for Event Cameras","abstract":"Event-based cameras (EBCs) have emerged as a bio-inspired alternative to traditional cameras, offering advantages in power efficiency, temporal resolution, and high dynamic range. However, the development of image analysis methods for EBCs is challenging due to the sparse and asynchronous nature of the data. This work addresses the problem of object detection for the EBC cameras. The current approaches to EBC object detection focus on constructing complex data representations and rely on specialized architectures. Here, we demonstrate that the combination of a Real-Time DEtection TRansformer, or RT-DETR, a state-of-the-art natural image detector, with a simple image-like representation of the EBC data achieves remarkable performance, surpassing current state-of-the-art results. Specifically, we show that a properly trained RT-DETR model on the EBC data achieves performance comparable to the most advanced EBC object detection methods. Next, we propose a low-rank adaptation (LoRA)-inspired way to augment the RT-DETR model to handle temporal dynamics of the data. The designed EvRT-DETR model outperforms the current, most advanced results on standard benchmark datasets Gen1 (mAP $+2.3$) and Gen4 (mAP $+1.4$) while only using standard modules from natural image and video analysis. These results demonstrate that effective EBC object detection can be achieved through careful adaptation of mainstream object detection architectures without requiring specialized architectural engineering. The code is available at: https://github.com/realtime-intelligence/evrt-detr","sentences":["Event-based cameras (EBCs) have emerged as a bio-inspired alternative to traditional cameras, offering advantages in power efficiency, temporal resolution, and high dynamic range.","However, the development of image analysis methods for EBCs is challenging due to the sparse and asynchronous nature of the data.","This work addresses the problem of object detection for the EBC cameras.","The current approaches to EBC object detection focus on constructing complex data representations and rely on specialized architectures.","Here, we demonstrate that the combination of a Real-Time DEtection TRansformer, or RT-DETR, a state-of-the-art natural image detector, with a simple image-like representation of the EBC data achieves remarkable performance, surpassing current state-of-the-art results.","Specifically, we show that a properly trained RT-DETR model on the EBC data achieves performance comparable to the most advanced EBC object detection methods.","Next, we propose a low-rank adaptation (LoRA)-inspired way to augment the RT-DETR model to handle temporal dynamics of the data.","The designed EvRT-DETR model outperforms the current, most advanced results on standard benchmark datasets Gen1 (mAP $+2.3$) and Gen4 (mAP $+1.4$) while only using standard modules from natural image and video analysis.","These results demonstrate that effective EBC object detection can be achieved through careful adaptation of mainstream object detection architectures without requiring specialized architectural engineering.","The code is available at: https://github.com/realtime-intelligence/evrt-detr"],"url":"http://arxiv.org/abs/2412.02890v1"}
{"created":"2024-12-03 22:38:58","title":"Were You Helpful -- Predicting Helpful Votes from Amazon Reviews","abstract":"This project investigates factors that influence the perceived helpfulness of Amazon product reviews through machine learning techniques. After extensive feature analysis and correlation testing, we identified key metadata characteristics that serve as strong predictors of review helpfulness. While we initially explored natural language processing approaches using TextBlob for sentiment analysis, our final model focuses on metadata features that demonstrated more significant correlations, including the number of images per review, reviewer's historical helpful votes, and temporal aspects of the review.   The data pipeline encompasses careful preprocessing and feature standardization steps to prepare the input for model training. Through systematic evaluation of different feature combinations, we discovered that metadata elements we choose using a threshold provide reliable signals when combined for predicting how helpful other Amazon users will find a review. This insight suggests that contextual and user-behavioral factors may be more indicative of review helpfulness than the linguistic content itself.","sentences":["This project investigates factors that influence the perceived helpfulness of Amazon product reviews through machine learning techniques.","After extensive feature analysis and correlation testing, we identified key metadata characteristics that serve as strong predictors of review helpfulness.","While we initially explored natural language processing approaches using TextBlob for sentiment analysis, our final model focuses on metadata features that demonstrated more significant correlations, including the number of images per review, reviewer's historical helpful votes, and temporal aspects of the review.   ","The data pipeline encompasses careful preprocessing and feature standardization steps to prepare the input for model training.","Through systematic evaluation of different feature combinations, we discovered that metadata elements we choose using a threshold provide reliable signals when combined for predicting how helpful other Amazon users will find a review.","This insight suggests that contextual and user-behavioral factors may be more indicative of review helpfulness than the linguistic content itself."],"url":"http://arxiv.org/abs/2412.02884v1"}
{"created":"2024-12-03 22:34:38","title":"iSEEtree: interactive explorer for hierarchical data","abstract":"$\\textbf{Motivation:}$ Hierarchical data structures are prevalent across several fields of research, as they represent an organised and efficient approach to study complex interconnected systems. Their significance is particularly evident in microbiome analysis, where microbial communities are classified at various taxonomic levels along the phylogenetic tree. In light of this trend, the R/Bioconductor community has established a reproducible analytical framework for hierarchical data, which relies on the highly generic and optimised TreeSummarizedExperiment data container. However, using this framework requires basic proficiency in programming.   $\\textbf{Results:}$ To reduce the entry requirements, we developed iSEEtree, an R shiny app which provides a visual interface for the analysis and exploration of TreeSummarizedExperiment objects, thereby expanding the interactive graphics capabilities of related work to hierarchical structures. This way, users can interactively explore several aspects of their data without the need for extensive knowledge of R programming. We describe how iSEEtree enables the exploration of hierarchical multi-table data and demonstrate its functionality with applications to microbiome analysis.   $\\textbf{Availability and Implementation:}$ iSEEtree was implemented in the R programming language and is available on Bioconductor at $\\href{https://bioconductor.org/packages/iSEEtree}{https\\text{:}//bioconductor\\text{.}org/packages/iSEEtree}$ under an Artistic 2.0 license.   $\\textbf{Contact:}$ $\\href{email}{giulio\\text{.}benedetti@utu\\text{.}fi}$ or $\\href{email}{leo\\text{.}lahti@utu\\text{.}fi}$.","sentences":["$\\textbf{Motivation:}$ Hierarchical data structures are prevalent across several fields of research, as they represent an organised and efficient approach to study complex interconnected systems.","Their significance is particularly evident in microbiome analysis, where microbial communities are classified at various taxonomic levels along the phylogenetic tree.","In light of this trend, the R/Bioconductor community has established a reproducible analytical framework for hierarchical data, which relies on the highly generic and optimised TreeSummarizedExperiment data container.","However, using this framework requires basic proficiency in programming.   ","$\\textbf{Results:}$ To reduce the entry requirements, we developed iSEEtree, an R shiny app which provides a visual interface for the analysis and exploration of TreeSummarizedExperiment objects, thereby expanding the interactive graphics capabilities of related work to hierarchical structures.","This way, users can interactively explore several aspects of their data without the need for extensive knowledge of R programming.","We describe how iSEEtree enables the exploration of hierarchical multi-table data and demonstrate its functionality with applications to microbiome analysis.   ","$\\textbf{Availability and Implementation:}$ iSEEtree was implemented in the R programming language and is available on Bioconductor at $\\href{https://bioconductor.org/packages/iSEEtree}{https\\text{:}//bioconductor\\text{.}org/packages/iSEEtree}$ under an Artistic 2.0 license.   ","$\\textbf{Contact:}$ $\\href{email}{giulio\\text{.}benedetti@utu\\text{.}fi}$ or $\\href{email}{leo\\text{.}lahti@utu\\text{.}fi}$."],"url":"http://arxiv.org/abs/2412.02882v1"}
{"created":"2024-12-03 22:25:44","title":"Pairwise Spatiotemporal Partial Trajectory Matching for Co-movement Analysis","abstract":"Spatiotemporal pairwise movement analysis involves identifying shared geographic-based behaviors between individuals within specific time frames. Traditionally, this task relies on sequence modeling and behavior analysis techniques applied to tabular or video-based data, but these methods often lack interpretability and struggle to capture partial matching. In this paper, we propose a novel method for pairwise spatiotemporal partial trajectory matching that transforms tabular spatiotemporal data into interpretable trajectory images based on specified time windows, allowing for partial trajectory analysis. This approach includes localization of trajectories, checking for spatial overlap, and pairwise matching using a Siamese Neural Network. We evaluate our method on a co-walking classification task, demonstrating its effectiveness in a novel co-behavior identification application. Our model surpasses established methods, achieving an F1-score up to 0.73. Additionally, we explore the method's utility for pair routine pattern analysis in real-world scenarios, providing insights into the frequency, timing, and duration of shared behaviors. This approach offers a powerful, interpretable framework for spatiotemporal behavior analysis, with potential applications in social behavior research, urban planning, and healthcare.","sentences":["Spatiotemporal pairwise movement analysis involves identifying shared geographic-based behaviors between individuals within specific time frames.","Traditionally, this task relies on sequence modeling and behavior analysis techniques applied to tabular or video-based data, but these methods often lack interpretability and struggle to capture partial matching.","In this paper, we propose a novel method for pairwise spatiotemporal partial trajectory matching that transforms tabular spatiotemporal data into interpretable trajectory images based on specified time windows, allowing for partial trajectory analysis.","This approach includes localization of trajectories, checking for spatial overlap, and pairwise matching using a Siamese Neural Network.","We evaluate our method on a co-walking classification task, demonstrating its effectiveness in a novel co-behavior identification application.","Our model surpasses established methods, achieving an F1-score up to 0.73.","Additionally, we explore the method's utility for pair routine pattern analysis in real-world scenarios, providing insights into the frequency, timing, and duration of shared behaviors.","This approach offers a powerful, interpretable framework for spatiotemporal behavior analysis, with potential applications in social behavior research, urban planning, and healthcare."],"url":"http://arxiv.org/abs/2412.02879v1"}
{"created":"2024-12-03 22:25:42","title":"Modeling and Discovering Direct Causes for Predictive Models","abstract":"We introduce a causal modeling framework that captures the input-output behavior of predictive models (e.g., machine learning models) by representing it using causal graphs. The framework enables us to define and identify features that directly cause the predictions, which has broad implications for data collection and model evaluation. We show two assumptions under which the direct causes can be discovered from data, one of which further simplifies the discovery process. In addition to providing sound and complete algorithms, we propose an optimization technique based on an independence rule that can be integrated with the algorithms to speed up the discovery process both theoretically and empirically.","sentences":["We introduce a causal modeling framework that captures the input-output behavior of predictive models (e.g., machine learning models) by representing it using causal graphs.","The framework enables us to define and identify features that directly cause the predictions, which has broad implications for data collection and model evaluation.","We show two assumptions under which the direct causes can be discovered from data, one of which further simplifies the discovery process.","In addition to providing sound and complete algorithms, we propose an optimization technique based on an independence rule that can be integrated with the algorithms to speed up the discovery process both theoretically and empirically."],"url":"http://arxiv.org/abs/2412.02878v1"}
{"created":"2024-12-03 22:06:55","title":"A Novel Compact LLM Framework for Local, High-Privacy EHR Data Applications","abstract":"Large Language Models (LLMs) have shown impressive capabilities in natural language processing, yet their use in sensitive domains like healthcare, particularly with Electronic Health Records (EHR), faces significant challenges due to privacy concerns and limited computational resources. This paper presents a compact LLM framework designed for local deployment in settings with strict privacy requirements and limited access to high-performance GPUs. We introduce a novel preprocessing technique that uses information extraction methods, e.g., regular expressions, to filter and emphasize critical information in clinical notes, enhancing the performance of smaller LLMs on EHR data. Our framework is evaluated using zero-shot and few-shot learning paradigms on both private and publicly available (MIMIC-IV) datasets, and we also compare its performance with fine-tuned LLMs on the MIMIC-IV dataset. The results demonstrate that our preprocessing approach significantly boosts the prediction accuracy of smaller LLMs, making them suitable for high-privacy, resource-constrained applications. This study offers valuable insights into optimizing LLM performance for sensitive, data-intensive tasks while addressing computational and privacy limitations.","sentences":["Large Language Models (LLMs) have shown impressive capabilities in natural language processing, yet their use in sensitive domains like healthcare, particularly with Electronic Health Records (EHR), faces significant challenges due to privacy concerns and limited computational resources.","This paper presents a compact LLM framework designed for local deployment in settings with strict privacy requirements and limited access to high-performance GPUs.","We introduce a novel preprocessing technique that uses information extraction methods, e.g., regular expressions, to filter and emphasize critical information in clinical notes, enhancing the performance of smaller LLMs on EHR data.","Our framework is evaluated using zero-shot and few-shot learning paradigms on both private and publicly available (MIMIC-IV) datasets, and we also compare its performance with fine-tuned LLMs on the MIMIC-IV dataset.","The results demonstrate that our preprocessing approach significantly boosts the prediction accuracy of smaller LLMs, making them suitable for high-privacy, resource-constrained applications.","This study offers valuable insights into optimizing LLM performance for sensitive, data-intensive tasks while addressing computational and privacy limitations."],"url":"http://arxiv.org/abs/2412.02868v1"}
{"created":"2024-12-03 22:02:42","title":"GoldFish: Serverless Actors with Short-Term Memory State for the Edge-Cloud Continuum","abstract":"Serverless Computing is a computing paradigm that provides efficient infrastructure management and elastic scalability. Serverless functions scale up or down based on demand, which means that functions are not directly addressable and rely on platform-managed invocation. Serverless stateless nature requires functions to leverage external services, such as object storage and KVS, to exchange data. Serverless actors have emerged as a solution to these issues. However, the state-of-the-art serverless lifecycle and event-trigger invocation force actors to leverage remote services to manage their state and exchange data, which impacts the performance and incurs additional costs and dependency on third-party services.   To address these issues, in this paper, we introduce a novel serverless lifecycle model that allows short-term stateful actors, enabling actors to maintain their state between executions. Additionally, we propose a novel serverless Invocation Model that enables serverless actors to influence the processing of future messages. We present GoldFish, a lightweight WebAssembly short-term stateful serverless actor platform that provides a novel serverless actor lifecycle and invocation model. GoldFish leverages WebAssembly to provide the actors with lightweight sandbox isolation, making them suitable for the Edge-Cloud Continuum, where computational resources are limited. Experimental results show that GoldFish optimizes the data exchange latency by up to 92% and increases the throughput by up to 10x compared to OpenFaaS and Spin.","sentences":["Serverless Computing is a computing paradigm that provides efficient infrastructure management and elastic scalability.","Serverless functions scale up or down based on demand, which means that functions are not directly addressable and rely on platform-managed invocation.","Serverless stateless nature requires functions to leverage external services, such as object storage and KVS, to exchange data.","Serverless actors have emerged as a solution to these issues.","However, the state-of-the-art serverless lifecycle and event-trigger invocation force actors to leverage remote services to manage their state and exchange data, which impacts the performance and incurs additional costs and dependency on third-party services.   ","To address these issues, in this paper, we introduce a novel serverless lifecycle model that allows short-term stateful actors, enabling actors to maintain their state between executions.","Additionally, we propose a novel serverless Invocation Model that enables serverless actors to influence the processing of future messages.","We present GoldFish, a lightweight WebAssembly short-term stateful serverless actor platform that provides a novel serverless actor lifecycle and invocation model.","GoldFish leverages WebAssembly to provide the actors with lightweight sandbox isolation, making them suitable for the Edge-Cloud Continuum, where computational resources are limited.","Experimental results show that GoldFish optimizes the data exchange latency by up to 92% and increases the throughput by up to 10x compared to OpenFaaS and Spin."],"url":"http://arxiv.org/abs/2412.02867v1"}
{"created":"2024-12-03 22:00:12","title":"Memory-efficient Continual Learning with Neural Collapse Contrastive","abstract":"Contrastive learning has significantly improved representation quality, enhancing knowledge transfer across tasks in continual learning (CL). However, catastrophic forgetting remains a key challenge, as contrastive based methods primarily focus on \"soft relationships\" or \"softness\" between samples, which shift with changing data distributions and lead to representation overlap across tasks. Recently, the newly identified Neural Collapse phenomenon has shown promise in CL by focusing on \"hard relationships\" or \"hardness\" between samples and fixed prototypes. However, this approach overlooks \"softness\", crucial for capturing intra-class variability, and this rigid focus can also pull old class representations toward current ones, increasing forgetting. Building on these insights, we propose Focal Neural Collapse Contrastive (FNC2), a novel representation learning loss that effectively balances both soft and hard relationships. Additionally, we introduce the Hardness-Softness Distillation (HSD) loss to progressively preserve the knowledge gained from these relationships across tasks. Our method outperforms state-of-the-art approaches, particularly in minimizing memory reliance. Remarkably, even without the use of memory, our approach rivals rehearsal-based methods, offering a compelling solution for data privacy concerns.","sentences":["Contrastive learning has significantly improved representation quality, enhancing knowledge transfer across tasks in continual learning (CL).","However, catastrophic forgetting remains a key challenge, as contrastive based methods primarily focus on \"soft relationships\" or \"softness\" between samples, which shift with changing data distributions and lead to representation overlap across tasks.","Recently, the newly identified Neural Collapse phenomenon has shown promise in CL by focusing on \"hard relationships\" or \"hardness\" between samples and fixed prototypes.","However, this approach overlooks \"softness\", crucial for capturing intra-class variability, and this rigid focus can also pull old class representations toward current ones, increasing forgetting.","Building on these insights, we propose Focal Neural Collapse Contrastive (FNC2), a novel representation learning loss that effectively balances both soft and hard relationships.","Additionally, we introduce the Hardness-Softness Distillation (HSD) loss to progressively preserve the knowledge gained from these relationships across tasks.","Our method outperforms state-of-the-art approaches, particularly in minimizing memory reliance.","Remarkably, even without the use of memory, our approach rivals rehearsal-based methods, offering a compelling solution for data privacy concerns."],"url":"http://arxiv.org/abs/2412.02865v1"}
{"created":"2024-12-03 21:57:04","title":"Proximal Control of UAVs with Federated Learning for Human-Robot Collaborative Domains","abstract":"The human-robot interaction (HRI) is a growing area of research. In HRI, complex command (action) classification is still an open problem that usually prevents the real applicability of such a technique. The literature presents some works that use neural networks to detect these actions. However, occlusion is still a major issue in HRI, especially when using uncrewed aerial vehicles (UAVs), since, during the robot's movement, the human operator is often out of the robot's field of view. Furthermore, in multi-robot scenarios, distributed training is also an open problem. In this sense, this work proposes an action recognition and control approach based on Long Short-Term Memory (LSTM) Deep Neural Networks with two layers in association with three densely connected layers and Federated Learning (FL) embedded in multiple drones. The FL enabled our approach to be trained in a distributed fashion, i.e., access to data without the need for cloud or other repositories, which facilitates the multi-robot system's learning. Furthermore, our multi-robot approach results also prevented occlusion situations, with experiments with real robots achieving an accuracy greater than 96%.","sentences":["The human-robot interaction (HRI) is a growing area of research.","In HRI, complex command (action) classification is still an open problem that usually prevents the real applicability of such a technique.","The literature presents some works that use neural networks to detect these actions.","However, occlusion is still a major issue in HRI, especially when using uncrewed aerial vehicles (UAVs), since, during the robot's movement, the human operator is often out of the robot's field of view.","Furthermore, in multi-robot scenarios, distributed training is also an open problem.","In this sense, this work proposes an action recognition and control approach based on Long Short-Term Memory (LSTM)","Deep Neural Networks with two layers in association with three densely connected layers and Federated Learning (FL) embedded in multiple drones.","The FL enabled our approach to be trained in a distributed fashion, i.e., access to data without the need for cloud or other repositories, which facilitates the multi-robot system's learning.","Furthermore, our multi-robot approach results also prevented occlusion situations, with experiments with real robots achieving an accuracy greater than 96%."],"url":"http://arxiv.org/abs/2412.02863v1"}
{"created":"2024-12-03 21:43:11","title":"Is Large-Scale Pretraining the Secret to Good Domain Generalization?","abstract":"Multi-Source Domain Generalization (DG) is the task of training on multiple source domains and achieving high classification performance on unseen target domains. Recent methods combine robust features from web-scale pretrained backbones with new features learned from source data, and this has dramatically improved benchmark results. However, it remains unclear if DG finetuning methods are becoming better over time, or if improved benchmark performance is simply an artifact of stronger pre-training. Prior studies have shown that perceptual similarity to pre-training data correlates with zero-shot performance, but we find the effect limited in the DG setting. Instead, we posit that having perceptually similar data in pretraining is not enough; and that it is how well these data were learned that determines performance. This leads us to introduce the Alignment Hypothesis, which states that the final DG performance will be high if and only if alignment of image and class label text embeddings is high. Our experiments confirm the Alignment Hypothesis is true, and we use it as an analysis tool of existing DG methods evaluated on DomainBed datasets by splitting evaluation data into In-pretraining (IP) and Out-of-pretraining (OOP). We show that all evaluated DG methods struggle on DomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, our findings highlight the need for DG methods which can generalize beyond pretraining alignment.","sentences":["Multi-Source Domain Generalization (DG) is the task of training on multiple source domains and achieving high classification performance on unseen target domains.","Recent methods combine robust features from web-scale pretrained backbones with new features learned from source data, and this has dramatically improved benchmark results.","However, it remains unclear if DG finetuning methods are becoming better over time, or if improved benchmark performance is simply an artifact of stronger pre-training.","Prior studies have shown that perceptual similarity to pre-training data correlates with zero-shot performance, but we find the effect limited in the DG setting.","Instead, we posit that having perceptually similar data in pretraining is not enough; and that it is how well these data were learned that determines performance.","This leads us to introduce the Alignment Hypothesis, which states that the final DG performance will be high if and only if alignment of image and class label text embeddings is high.","Our experiments confirm the Alignment Hypothesis is true, and we use it as an analysis tool of existing DG methods evaluated on DomainBed datasets by splitting evaluation data into In-pretraining (IP) and Out-of-pretraining (OOP).","We show that all evaluated DG methods struggle on DomainBed-OOP, while recent methods excel on DomainBed-IP.","Put together, our findings highlight the need for DG methods which can generalize beyond pretraining alignment."],"url":"http://arxiv.org/abs/2412.02856v1"}
{"created":"2024-12-03 21:42:30","title":"Optimized CNNs for Rapid 3D Point Cloud Object Recognition","abstract":"This study introduces a method for efficiently detecting objects within 3D point clouds using convolutional neural networks (CNNs). Our approach adopts a unique feature-centric voting mechanism to construct convolutional layers that capitalize on the typical sparsity observed in input data. We explore the trade-off between accuracy and speed across diverse network architectures and advocate for integrating an $\\mathcal{L}_1$ penalty on filter activations to augment sparsity within intermediate layers. This research pioneers the proposal of sparse convolutional layers combined with $\\mathcal{L}_1$ regularization to effectively handle large-scale 3D data processing. Our method's efficacy is demonstrated on the MVTec 3D-AD object detection benchmark. The Vote3Deep models, with just three layers, outperform the previous state-of-the-art in both laser-only approaches and combined laser-vision methods. Additionally, they maintain competitive processing speeds. This underscores our approach's capability to substantially enhance detection performance while ensuring computational efficiency suitable for real-time applications.","sentences":["This study introduces a method for efficiently detecting objects within 3D point clouds using convolutional neural networks (CNNs).","Our approach adopts a unique feature-centric voting mechanism to construct convolutional layers that capitalize on the typical sparsity observed in input data.","We explore the trade-off between accuracy and speed across diverse network architectures and advocate for integrating an $\\mathcal{L}_1$ penalty on filter activations to augment sparsity within intermediate layers.","This research pioneers the proposal of sparse convolutional layers combined with $\\mathcal{L}_1$ regularization to effectively handle large-scale 3D data processing.","Our method's efficacy is demonstrated on the MVTec 3D-AD object detection benchmark.","The Vote3Deep models, with just three layers, outperform the previous state-of-the-art in both laser-only approaches and combined laser-vision methods.","Additionally, they maintain competitive processing speeds.","This underscores our approach's capability to substantially enhance detection performance while ensuring computational efficiency suitable for real-time applications."],"url":"http://arxiv.org/abs/2412.02855v1"}
{"created":"2024-12-03 21:31:46","title":"Block MedCare: Advancing healthcare through blockchain integration with AI and IoT","abstract":"This research explores the integration of blockchain technology in healthcare, focusing on enhancing the security and efficiency of Electronic Health Record (EHR) management. We propose a novel Ethereum-based system that empowers patients with secure control over their medical data. Our approach addresses key challenges in healthcare blockchain implementation, including scalability, privacy, and regulatory compliance. The system incorporates digital signatures, Role-Based Access Control, and a multi-layered architecture to ensure secure, controlled access. We developed a decentralized application (dApp) with user-friendly interfaces for patients, doctors, and administrators, demonstrating the practical application of our solution. A survey among healthcare professionals and IT experts revealed strong interest in blockchain adoption, while also highlighting concerns about integration costs. The study explores future enhancements, including integration with IoT devices and AI-driven analytics, contributing to the evolution of secure, efficient, and interoperable healthcare systems that leverage cutting-edge technologies for improved patient care.","sentences":["This research explores the integration of blockchain technology in healthcare, focusing on enhancing the security and efficiency of Electronic Health Record (EHR) management.","We propose a novel Ethereum-based system that empowers patients with secure control over their medical data.","Our approach addresses key challenges in healthcare blockchain implementation, including scalability, privacy, and regulatory compliance.","The system incorporates digital signatures, Role-Based Access Control, and a multi-layered architecture to ensure secure, controlled access.","We developed a decentralized application (dApp) with user-friendly interfaces for patients, doctors, and administrators, demonstrating the practical application of our solution.","A survey among healthcare professionals and IT experts revealed strong interest in blockchain adoption, while also highlighting concerns about integration costs.","The study explores future enhancements, including integration with IoT devices and AI-driven analytics, contributing to the evolution of secure, efficient, and interoperable healthcare systems that leverage cutting-edge technologies for improved patient care."],"url":"http://arxiv.org/abs/2412.02851v1"}
