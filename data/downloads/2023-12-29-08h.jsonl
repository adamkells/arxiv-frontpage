{"created":"2023-12-28 18:59:41","title":"Amodal Ground Truth and Completion in the Wild","abstract":"The problem we study in this paper is amodal image segmentation: predicting entire object segmentation masks including both visible and invisible (occluded) parts. In previous work, the amodal segmentation ground truth on real images is usually predicted by manual annotaton and thus is subjective. In contrast, we use 3D data to establish an automatic pipeline to determine authentic ground truth amodal masks for partially occluded objects in real images. This pipeline is used to construct an amodal completion evaluation benchmark, MP3D-Amodal, consisting of a variety of object categories and labels. To better handle the amodal completion task in the wild, we explore two architecture variants: a two-stage model that first infers the occluder, followed by amodal mask completion; and a one-stage model that exploits the representation power of Stable Diffusion for amodal segmentation across many categories. Without bells and whistles, our method achieves a new state-of-the-art performance on Amodal segmentation datasets that cover a large variety of objects, including COCOA and our new MP3D-Amodal dataset. The dataset, model, and code are available at https://www.robots.ox.ac.uk/~vgg/research/amodal/.","sentences":["The problem we study in this paper is amodal image segmentation: predicting entire object segmentation masks including both visible and invisible (occluded) parts.","In previous work, the amodal segmentation ground truth on real images is usually predicted by manual annotaton and thus is subjective.","In contrast, we use 3D data to establish an automatic pipeline to determine authentic ground truth amodal masks for partially occluded objects in real images.","This pipeline is used to construct an amodal completion evaluation benchmark, MP3D-Amodal, consisting of a variety of object categories and labels.","To better handle the amodal completion task in the wild, we explore two architecture variants: a two-stage model that first infers the occluder, followed by amodal mask completion; and a one-stage model that exploits the representation power of Stable Diffusion for amodal segmentation across many categories.","Without bells and whistles, our method achieves a new state-of-the-art performance on Amodal segmentation datasets that cover a large variety of objects, including COCOA and our new MP3D-Amodal dataset.","The dataset, model, and code are available at https://www.robots.ox.ac.uk/~vgg/research/amodal/."],"url":"http://arxiv.org/abs/2312.17247v1"}
{"created":"2023-12-28 18:59:09","title":"The LLM Surgeon","abstract":"State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimentally, our method can prune rows and columns from a range of OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance, and achieve state-of-the-art results in unstructured and semi-structured pruning of large language models.","sentences":["State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data.","However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints.","We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch.","To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models.","In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal.","We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient.","Experimentally, our method can prune rows and columns from a range of OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance, and achieve state-of-the-art results in unstructured and semi-structured pruning of large language models."],"url":"http://arxiv.org/abs/2312.17244v1"}
{"created":"2023-12-28 18:59:04","title":"Unsupervised Universal Image Segmentation","abstract":"Several unsupervised image segmentation approaches have been proposed which eliminate the need for dense manually-annotated segmentation masks; current models separately handle either semantic segmentation (e.g., STEGO) or class-agnostic instance segmentation (e.g., CutLER), but not both (i.e., panoptic segmentation). We propose an Unsupervised Universal Segmentation model (U2Seg) adept at performing various image segmentation tasks -- instance, semantic and panoptic -- using a novel unified framework. U2Seg generates pseudo semantic labels for these segmentation tasks via leveraging self-supervised models followed by clustering; each cluster represents different semantic and/or instance membership of pixels. We then self-train the model on these pseudo semantic labels, yielding substantial performance gains over specialized methods tailored to each task: a +2.6 AP$^{\\text{box}}$ boost vs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAcc increase (vs. STEGO) in unsupervised semantic segmentation on COCOStuff. Moreover, our method sets up a new baseline for unsupervised panoptic segmentation, which has not been previously explored. U2Seg is also a strong pretrained model for few-shot segmentation, surpassing CutLER by +5.0 AP$^{\\text{mask}}$ when trained on a low-data regime, e.g., only 1% COCO labels. We hope our simple yet effective method can inspire more research on unsupervised universal image segmentation.","sentences":["Several unsupervised image segmentation approaches have been proposed which eliminate the need for dense manually-annotated segmentation masks; current models separately handle either semantic segmentation (e.g., STEGO) or class-agnostic instance segmentation (e.g., CutLER), but not both (i.e., panoptic segmentation).","We propose an Unsupervised Universal Segmentation model (U2Seg) adept at performing various image segmentation tasks -- instance, semantic and panoptic -- using a novel unified framework.","U2Seg generates pseudo semantic labels for these segmentation tasks via leveraging self-supervised models followed by clustering; each cluster represents different semantic and/or instance membership of pixels.","We then self-train the model on these pseudo semantic labels, yielding substantial performance gains over specialized methods tailored to each task: a +2.6 AP$^{\\text{box}}$ boost vs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAcc increase (vs. STEGO) in unsupervised semantic segmentation on COCOStuff.","Moreover, our method sets up a new baseline for unsupervised panoptic segmentation, which has not been previously explored.","U2Seg is also a strong pretrained model for few-shot segmentation, surpassing CutLER by +5.0","AP$^{\\text{mask}}$ when trained on a low-data regime, e.g., only 1% COCO labels.","We hope our simple yet effective method can inspire more research on unsupervised universal image segmentation."],"url":"http://arxiv.org/abs/2312.17243v1"}
{"created":"2023-12-28 18:58:45","title":"Compact Neural Graphics Primitives with Learned Hash Probing","abstract":"Neural graphics primitives are faster and achieve higher quality when their neural networks are augmented by spatial data structures that hold trainable features arranged in a grid. However, existing feature grids either come with a large memory footprint (dense or factorized grids, trees, and hash tables) or slow performance (index learning and vector quantization). In this paper, we show that a hash table with learned probes has neither disadvantage, resulting in a favorable combination of size and speed. Inference is faster than unprobed hash tables at equal quality while training is only 1.2-2.6x slower, significantly outperforming prior index learning approaches. We arrive at this formulation by casting all feature grids into a common framework: they each correspond to a lookup function that indexes into a table of feature vectors. In this framework, the lookup functions of existing data structures can be combined by simple arithmetic combinations of their indices, resulting in Pareto optimal compression and speed.","sentences":["Neural graphics primitives are faster and achieve higher quality when their neural networks are augmented by spatial data structures that hold trainable features arranged in a grid.","However, existing feature grids either come with a large memory footprint (dense or factorized grids, trees, and hash tables) or slow performance (index learning and vector quantization).","In this paper, we show that a hash table with learned probes has neither disadvantage, resulting in a favorable combination of size and speed.","Inference is faster than unprobed hash tables at equal quality while training is only 1.2-2.6x slower, significantly outperforming prior index learning approaches.","We arrive at this formulation by casting all feature grids into a common framework: they each correspond to a lookup function that indexes into a table of feature vectors.","In this framework, the lookup functions of existing data structures can be combined by simple arithmetic combinations of their indices, resulting in Pareto optimal compression and speed."],"url":"http://arxiv.org/abs/2312.17241v1"}
{"created":"2023-12-28 18:58:33","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model","abstract":"While LISA effectively bridges the gap between segmentation and large language models to enable reasoning segmentation, it poses certain limitations: unable to distinguish different instances of the target region, and constrained by the pre-defined textual response formats. In this work, we introduce LISA++, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact. The main enhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}: The instance segmentation ability has been added, providing a more detailed scene analysis along with the existing multi-region semantic segmentation. \\textbf{2) More Natural Conversation}: Improved capability for multi-turn dialogue, with the ability to incorporate segmentation results directly into text responses, i.e., Segmentation in Dialogue (SiD). These improvements are achieved by curating the existing samples of generic segmentation datasets, aimed specifically at enhancing the segmentation and conversational skills without structural change and additional data sources. Comparative analysis with the original LISA model shows significant advancements in these areas, positioning LISA++ as a notable upgrade in visual understanding and interaction. LISA++'s adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA, and the potential as a foundational model for diverse applications.","sentences":["While LISA effectively bridges the gap between segmentation and large language models to enable reasoning segmentation, it poses certain limitations: unable to distinguish different instances of the target region, and constrained by the pre-defined textual response formats.","In this work, we introduce LISA++, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact.","The main enhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}:","The instance segmentation ability has been added, providing a more detailed scene analysis along with the existing multi-region semantic segmentation.","\\textbf{2) More Natural Conversation}: Improved capability for multi-turn dialogue, with the ability to incorporate segmentation results directly into text responses, i.e., Segmentation in Dialogue (SiD).","These improvements are achieved by curating the existing samples of generic segmentation datasets, aimed specifically at enhancing the segmentation and conversational skills without structural change and additional data sources.","Comparative analysis with the original LISA model shows significant advancements in these areas, positioning LISA++ as a notable upgrade in visual understanding and interaction.","LISA++'s adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA, and the potential as a foundational model for diverse applications."],"url":"http://arxiv.org/abs/2312.17240v1"}
{"created":"2023-12-28 18:58:06","title":"Factoring Expertise, Workload, and Turnover into Code Review Recommendation","abstract":"Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover while more evenly distributing review workload.   We conduct historical analyses to understand the natural concentration of review workload and the degree of knowledge spreading that is inherent in code review. Even though review workload is highly concentrated, we show that code review natural spreads knowledge thereby reducing the files at risk to turnover.   Using simulation, we evaluate existing code review recommenders and develop novel recommenders to understand their impact on the level of expertise during review, the workload of reviewers, and the files at risk to turnover. Our simulations use seeded random replacement of reviewers to allow us to compare the reviewer recommenders without the confounding variation of different reviewers being replaced for each recommender.   Combining recommenders, we develop the SofiaWL recommender that suggests experts with low active review workload when none of the files under review are known by only one developer. In contrast, when knowledge is concentrated on one developer, it sends the review to other reviewers to spread knowledge. For the projects we study, we are able to globally increase expertise during reviews, +3%, reduce workload concentration, -12%, and reduce the files at risk, -28%. We make our scripts and data available in our replication package. Developers can optimize for a particular outcome measure based on the needs of their project, or use our GitHub bot to automatically balance the outcomes.","sentences":["Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects.","Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers.","In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover while more evenly distributing review workload.   ","We conduct historical analyses to understand the natural concentration of review workload and the degree of knowledge spreading that is inherent in code review.","Even though review workload is highly concentrated, we show that code review natural spreads knowledge thereby reducing the files at risk to turnover.   ","Using simulation, we evaluate existing code review recommenders and develop novel recommenders to understand their impact on the level of expertise during review, the workload of reviewers, and the files at risk to turnover.","Our simulations use seeded random replacement of reviewers to allow us to compare the reviewer recommenders without the confounding variation of different reviewers being replaced for each recommender.   ","Combining recommenders, we develop the SofiaWL recommender that suggests experts with low active review workload when none of the files under review are known by only one developer.","In contrast, when knowledge is concentrated on one developer, it sends the review to other reviewers to spread knowledge.","For the projects we study, we are able to globally increase expertise during reviews, +3%, reduce workload concentration, -12%, and reduce the files at risk, -28%.","We make our scripts and data available in our replication package.","Developers can optimize for a particular outcome measure based on the needs of their project, or use our GitHub bot to automatically balance the outcomes."],"url":"http://arxiv.org/abs/2312.17236v1"}
{"created":"2023-12-28 18:57:11","title":"Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels","abstract":"Current 3D scene segmentation methods are heavily dependent on manually annotated 3D training datasets. Such manual annotations are labor-intensive, and often lack fine-grained details. Importantly, models trained on this data typically struggle to recognize object classes beyond the annotated classes, i.e., they do not generalize well to unseen domains and require additional domain-specific annotations. In contrast, 2D foundation models demonstrate strong generalization and impressive zero-shot abilities, inspiring us to incorporate these characteristics from 2D models into 3D models. Therefore, we explore the use of image segmentation foundation models to automatically generate training labels for 3D segmentation. We propose Segment3D, a method for class-agnostic 3D scene segmentation that produces high-quality 3D segmentation masks. It improves over existing 3D segmentation models (especially on fine-grained masks), and enables easily adding new training data to further boost the segmentation performance -- all without the need for manual training labels.","sentences":["Current 3D scene segmentation methods are heavily dependent on manually annotated 3D training datasets.","Such manual annotations are labor-intensive, and often lack fine-grained details.","Importantly, models trained on this data typically struggle to recognize object classes beyond the annotated classes, i.e., they do not generalize well to unseen domains and require additional domain-specific annotations.","In contrast, 2D foundation models demonstrate strong generalization and impressive zero-shot abilities, inspiring us to incorporate these characteristics from 2D models into 3D models.","Therefore, we explore the use of image segmentation foundation models to automatically generate training labels for 3D segmentation.","We propose Segment3D, a method for class-agnostic 3D scene segmentation that produces high-quality 3D segmentation masks.","It improves over existing 3D segmentation models (especially on fine-grained masks), and enables easily adding new training data to further boost the segmentation performance -- all without the need for manual training labels."],"url":"http://arxiv.org/abs/2312.17232v1"}
{"created":"2023-12-28 18:54:21","title":"Gradient-based Planning with World Models","abstract":"The enduring challenge in the field of artificial intelligence has been the control of systems to achieve desired behaviours. While for systems governed by straightforward dynamics equations, methods like Linear Quadratic Regulation (LQR) have historically proven highly effective, most real-world tasks, which require a general problem-solver, demand world models with dynamics that cannot be easily described by simple equations. Consequently, these models must be learned from data using neural networks. Most model predictive control (MPC) algorithms designed for visual world models have traditionally explored gradient-free population-based optimisation methods, such as Cross Entropy and Model Predictive Path Integral (MPPI) for planning. However, we present an exploration of a gradient-based alternative that fully leverages the differentiability of the world model. In our study, we conduct a comparative analysis between our method and other MPC-based alternatives, as well as policy-based algorithms. In a sample-efficient setting, our method achieves on par or superior performance compared to the alternative approaches in most tasks. Additionally, we introduce a hybrid model that combines policy networks and gradient-based MPC, which outperforms pure policy based methods thereby holding promise for Gradient-based planning with world models in complex real-world tasks.","sentences":["The enduring challenge in the field of artificial intelligence has been the control of systems to achieve desired behaviours.","While for systems governed by straightforward dynamics equations, methods like Linear Quadratic Regulation (LQR) have historically proven highly effective, most real-world tasks, which require a general problem-solver, demand world models with dynamics that cannot be easily described by simple equations.","Consequently, these models must be learned from data using neural networks.","Most model predictive control (MPC) algorithms designed for visual world models have traditionally explored gradient-free population-based optimisation methods, such as Cross Entropy and Model Predictive Path Integral (MPPI) for planning.","However, we present an exploration of a gradient-based alternative that fully leverages the differentiability of the world model.","In our study, we conduct a comparative analysis between our method and other MPC-based alternatives, as well as policy-based algorithms.","In a sample-efficient setting, our method achieves on par or superior performance compared to the alternative approaches in most tasks.","Additionally, we introduce a hybrid model that combines policy networks and gradient-based MPC, which outperforms pure policy based methods thereby holding promise for Gradient-based planning with world models in complex real-world tasks."],"url":"http://arxiv.org/abs/2312.17227v1"}
{"created":"2023-12-28 18:29:40","title":"Navigating the Research Landscape of Decentralized Autonomous Organizations: A Research Note and Agenda","abstract":"This note and agenda serve as a cause for thought for scholars interested in researching Decentralized Autonomous Organizations (DAOs), addressing both the opportunities and challenges posed by this phenomenon. It covers key aspects of data retrieval, data selection criteria, issues in data reliability and validity such as governance token pricing complexities, discrepancy in treasuries, Mainnet and Testnet data, understanding the variety of DAO types and proposal categories, airdrops affecting governance, and the Sybil problem. The agenda aims to equip scholars with the essential knowledge required to conduct nuanced and rigorous academic studies on DAOs by illuminating these various aspects and proposing directions for future research.","sentences":["This note and agenda serve as a cause for thought for scholars interested in researching Decentralized Autonomous Organizations (DAOs), addressing both the opportunities and challenges posed by this phenomenon.","It covers key aspects of data retrieval, data selection criteria, issues in data reliability and validity such as governance token pricing complexities, discrepancy in treasuries, Mainnet and Testnet data, understanding the variety of DAO types and proposal categories, airdrops affecting governance, and the Sybil problem.","The agenda aims to equip scholars with the essential knowledge required to conduct nuanced and rigorous academic studies on DAOs by illuminating these various aspects and proposing directions for future research."],"url":"http://arxiv.org/abs/2312.17197v1"}
{"created":"2023-12-28 18:12:42","title":"Virtual Scientific Companion for Synchrotron Beamlines: A Prototype","abstract":"The extraordinarily high X-ray flux and specialized instrumentation at synchrotron beamlines have enabled versatile in-situ and high throughput studies that are impossible elsewhere. Dexterous and efficient control of experiments are thus crucial for efficient beamline operation. Artificial intelligence and machine learning methods are constantly being developed to enhance facility performance, but the full potential of these developments can only be reached with efficient human-computer-interaction. Natural language is the most intuitive and efficient way for humans to communicate. However, the low credibility and reproducibility of existing large language models and tools demand extensive development to be made for robust and reliable performance for scientific purposes. In this work, we introduce the prototype of virtual scientific companion (VISION) and demonstrate that it is possible to control basic beamline operations through natural language with open-source language model and the limited computational resources at beamline. The human-AI nature of VISION leverages existing automation systems and data framework at synchrotron beamlines.","sentences":["The extraordinarily high X-ray flux and specialized instrumentation at synchrotron beamlines have enabled versatile in-situ and high throughput studies that are impossible elsewhere.","Dexterous and efficient control of experiments are thus crucial for efficient beamline operation.","Artificial intelligence and machine learning methods are constantly being developed to enhance facility performance, but the full potential of these developments can only be reached with efficient human-computer-interaction.","Natural language is the most intuitive and efficient way for humans to communicate.","However, the low credibility and reproducibility of existing large language models and tools demand extensive development to be made for robust and reliable performance for scientific purposes.","In this work, we introduce the prototype of virtual scientific companion (VISION) and demonstrate that it is possible to control basic beamline operations through natural language with open-source language model and the limited computational resources at beamline.","The human-AI nature of VISION leverages existing automation systems and data framework at synchrotron beamlines."],"url":"http://arxiv.org/abs/2312.17180v1"}
{"created":"2023-12-28 18:02:22","title":"Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution","abstract":"Vision-language pretrained models have seen remarkable success, but their application to safety-critical settings is limited by their lack of interpretability. To improve the interpretability of vision-language models such as CLIP, we propose a multi-modal information bottleneck (M2IB) approach that learns latent representations that compress irrelevant information while preserving relevant visual and textual features. We demonstrate how M2IB can be applied to attribution analysis of vision-language pretrained models, increasing attribution accuracy and improving the interpretability of such models when applied to safety-critical domains such as healthcare. Crucially, unlike commonly used unimodal attribution methods, M2IB does not require ground truth labels, making it possible to audit representations of vision-language pretrained models when multiple modalities but no ground-truth data is available. Using CLIP as an example, we demonstrate the effectiveness of M2IB attribution and show that it outperforms gradient-based, perturbation-based, and attention-based attribution methods both qualitatively and quantitatively.","sentences":["Vision-language pretrained models have seen remarkable success, but their application to safety-critical settings is limited by their lack of interpretability.","To improve the interpretability of vision-language models such as CLIP, we propose a multi-modal information bottleneck (M2IB) approach that learns latent representations that compress irrelevant information while preserving relevant visual and textual features.","We demonstrate how M2IB can be applied to attribution analysis of vision-language pretrained models, increasing attribution accuracy and improving the interpretability of such models when applied to safety-critical domains such as healthcare.","Crucially, unlike commonly used unimodal attribution methods, M2IB does not require ground truth labels, making it possible to audit representations of vision-language pretrained models when multiple modalities but no ground-truth data is available.","Using CLIP as an example, we demonstrate the effectiveness of M2IB attribution and show that it outperforms gradient-based, perturbation-based, and attention-based attribution methods both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2312.17174v1"}
{"created":"2023-12-28 17:54:56","title":"Can Active Sampling Reduce Causal Confusion in Offline Reinforcement Learning?","abstract":"Causal confusion is a phenomenon where an agent learns a policy that reflects imperfect spurious correlations in the data. Such a policy may falsely appear to be optimal during training if most of the training data contain such spurious correlations. This phenomenon is particularly pronounced in domains such as robotics, with potentially large gaps between the open- and closed-loop performance of an agent. In such settings, causally confused models may appear to perform well according to open-loop metrics during training but fail catastrophically when deployed in the real world. In this paper, we study causal confusion in offline reinforcement learning. We investigate whether selectively sampling appropriate points from a dataset of demonstrations may enable offline reinforcement learning agents to disambiguate the underlying causal mechanisms of the environment, alleviate causal confusion in offline reinforcement learning, and produce a safer model for deployment. To answer this question, we consider a set of tailored offline reinforcement learning datasets that exhibit causal ambiguity and assess the ability of active sampling techniques to reduce causal confusion at evaluation. We provide empirical evidence that uniform and active sampling techniques are able to consistently reduce causal confusion as training progresses and that active sampling is able to do so significantly more efficiently than uniform sampling.","sentences":["Causal confusion is a phenomenon where an agent learns a policy that reflects imperfect spurious correlations in the data.","Such a policy may falsely appear to be optimal during training if most of the training data contain such spurious correlations.","This phenomenon is particularly pronounced in domains such as robotics, with potentially large gaps between the open- and closed-loop performance of an agent.","In such settings, causally confused models may appear to perform well according to open-loop metrics during training but fail catastrophically when deployed in the real world.","In this paper, we study causal confusion in offline reinforcement learning.","We investigate whether selectively sampling appropriate points from a dataset of demonstrations may enable offline reinforcement learning agents to disambiguate the underlying causal mechanisms of the environment, alleviate causal confusion in offline reinforcement learning, and produce a safer model for deployment.","To answer this question, we consider a set of tailored offline reinforcement learning datasets that exhibit causal ambiguity and assess the ability of active sampling techniques to reduce causal confusion at evaluation.","We provide empirical evidence that uniform and active sampling techniques are able to consistently reduce causal confusion as training progresses and that active sampling is able to do so significantly more efficiently than uniform sampling."],"url":"http://arxiv.org/abs/2312.17168v1"}
{"created":"2023-12-28 17:52:21","title":"Securing NextG Systems against Poisoning Attacks on Federated Learning: A Game-Theoretic Solution","abstract":"This paper studies the poisoning attack and defense interactions in a federated learning (FL) system, specifically in the context of wireless signal classification using deep learning for next-generation (NextG) communications. FL collectively trains a global model without the need for clients to exchange their data samples. By leveraging geographically dispersed clients, the trained global model can be used for incumbent user identification, facilitating spectrum sharing. However, in this distributed learning system, the presence of malicious clients introduces the risk of poisoning the training data to manipulate the global model through falsified local model exchanges. To address this challenge, a proactive defense mechanism is employed in this paper to make informed decisions regarding the admission or rejection of clients participating in FL systems. Consequently, the attack-defense interactions are modeled as a game, centered around the underlying admission and poisoning decisions. First, performance bounds are established, encompassing the best and worst strategies for attackers and defenders. Subsequently, the attack and defense utilities are characterized within the Nash equilibrium, where no player can unilaterally improve its performance given the fixed strategies of others. The results offer insights into novel operational modes that safeguard FL systems against poisoning attacks by quantifying the performance of both attacks and defenses in the context of NextG communications.","sentences":["This paper studies the poisoning attack and defense interactions in a federated learning (FL) system, specifically in the context of wireless signal classification using deep learning for next-generation (NextG) communications.","FL collectively trains a global model without the need for clients to exchange their data samples.","By leveraging geographically dispersed clients, the trained global model can be used for incumbent user identification, facilitating spectrum sharing.","However, in this distributed learning system, the presence of malicious clients introduces the risk of poisoning the training data to manipulate the global model through falsified local model exchanges.","To address this challenge, a proactive defense mechanism is employed in this paper to make informed decisions regarding the admission or rejection of clients participating in FL systems.","Consequently, the attack-defense interactions are modeled as a game, centered around the underlying admission and poisoning decisions.","First, performance bounds are established, encompassing the best and worst strategies for attackers and defenders.","Subsequently, the attack and defense utilities are characterized within the Nash equilibrium, where no player can unilaterally improve its performance given the fixed strategies of others.","The results offer insights into novel operational modes that safeguard FL systems against poisoning attacks by quantifying the performance of both attacks and defenses in the context of NextG communications."],"url":"http://arxiv.org/abs/2312.17164v1"}
{"created":"2023-12-28 17:52:09","title":"FENet: Focusing Enhanced Network for Lane Detection","abstract":"Inspired by human driving focus, this research pioneers networks augmented with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss - targeted innovations addressing obstacles to precise lane detection for autonomous driving. Experiments demonstrate our Focusing Sampling strategy, emphasizing vital distant details unlike uniform approaches, significantly boosts both benchmark and practical curved/distant lane recognition accuracy essential for safety. While FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, FENetV2 proves most reliable on the proposed Partial Field analysis. Hence we specifically recommend V2 for practical lane navigation despite fractional degradation on standard entire-image measures. Future directions include collecting on-road data and integrating complementary dual frameworks to further breakthroughs guided by human perception principles. Code will be made available.","sentences":["Inspired by human driving focus, this research pioneers networks augmented with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss - targeted innovations addressing obstacles to precise lane detection for autonomous driving.","Experiments demonstrate our Focusing Sampling strategy, emphasizing vital distant details unlike uniform approaches, significantly boosts both benchmark and practical curved/distant lane recognition accuracy essential for safety.","While FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, FENetV2 proves most reliable on the proposed Partial Field analysis.","Hence we specifically recommend V2 for practical lane navigation despite fractional degradation on standard entire-image measures.","Future directions include collecting on-road data and integrating complementary dual frameworks to further breakthroughs guided by human perception principles.","Code will be made available."],"url":"http://arxiv.org/abs/2312.17163v1"}
{"created":"2023-12-28 17:47:25","title":"Replica Tree-based Federated Learning using Limited Data","abstract":"Learning from limited data has been extensively studied in machine learning, considering that deep neural networks achieve optimal performance when trained using a large amount of samples. Although various strategies have been proposed for centralized training, the topic of federated learning with small datasets remains largely unexplored. Moreover, in realistic scenarios, such as settings where medical institutions are involved, the number of participating clients is also constrained. In this work, we propose a novel federated learning framework, named RepTreeFL. At the core of the solution is the concept of a replica, where we replicate each participating client by copying its model architecture and perturbing its local data distribution. Our approach enables learning from limited data and a small number of clients by aggregating a larger number of models with diverse data distributions. Furthermore, we leverage the hierarchical structure of the client network (both original and virtual), alongside the model diversity across replicas, and introduce a diversity-based tree aggregation, where replicas are combined in a tree-like manner and the aggregation weights are dynamically updated based on the model discrepancy. We evaluated our method on two tasks and two types of data, graph generation and image classification (binary and multi-class), with both homogeneous and heterogeneous model architectures. Experimental results demonstrate the effectiveness and outperformance of RepTreeFL in settings where both data and clients are limited. Our code is available at https://github.com/basiralab/RepTreeFL.","sentences":["Learning from limited data has been extensively studied in machine learning, considering that deep neural networks achieve optimal performance when trained using a large amount of samples.","Although various strategies have been proposed for centralized training, the topic of federated learning with small datasets remains largely unexplored.","Moreover, in realistic scenarios, such as settings where medical institutions are involved, the number of participating clients is also constrained.","In this work, we propose a novel federated learning framework, named RepTreeFL.","At the core of the solution is the concept of a replica, where we replicate each participating client by copying its model architecture and perturbing its local data distribution.","Our approach enables learning from limited data and a small number of clients by aggregating a larger number of models with diverse data distributions.","Furthermore, we leverage the hierarchical structure of the client network (both original and virtual), alongside the model diversity across replicas, and introduce a diversity-based tree aggregation, where replicas are combined in a tree-like manner and the aggregation weights are dynamically updated based on the model discrepancy.","We evaluated our method on two tasks and two types of data, graph generation and image classification (binary and multi-class), with both homogeneous and heterogeneous model architectures.","Experimental results demonstrate the effectiveness and outperformance of RepTreeFL in settings where both data and clients are limited.","Our code is available at https://github.com/basiralab/RepTreeFL."],"url":"http://arxiv.org/abs/2312.17159v1"}
{"created":"2023-12-28 17:30:25","title":"On-Demand JSON: A Better Way to Parse Documents?","abstract":"JSON is a popular standard for data interchange on the Internet. Ingesting JSON documents can be a performance bottleneck. A popular parsing strategy consists in converting the input text into a tree-based data structure -- sometimes called a Document Object Model or DOM. We designed and implemented a novel JSON parsing interface -- called On-Demand -- that appears to the programmer like a conventional DOM-based approach. However, the underlying implementation is a pointer iterating through the content, only materializing the results (objects, arrays, strings, numbers) lazily.On recent commodity processors, an implementation of our approach provides superior performance in multiple benchmarks. To ensure reproducibility, our work is freely available as open source software. Several systems use On Demand: e.g., Apache Doris, the Node.js JavaScript runtime, Milvus, and Velox.","sentences":["JSON is a popular standard for data interchange on the Internet.","Ingesting JSON documents can be a performance bottleneck.","A popular parsing strategy consists in converting the input text into a tree-based data structure -- sometimes called a Document Object Model or DOM.","We designed and implemented a novel JSON parsing interface -- called On-Demand -- that appears to the programmer like a conventional DOM-based approach.","However, the underlying implementation is a pointer iterating through the content, only materializing the results (objects, arrays, strings, numbers) lazily.","On recent commodity processors, an implementation of our approach provides superior performance in multiple benchmarks.","To ensure reproducibility, our work is freely available as open source software.","Several systems use On Demand: e.g., Apache Doris, the Node.js JavaScript runtime, Milvus, and Velox."],"url":"http://arxiv.org/abs/2312.17149v1"}
{"created":"2023-12-28 17:14:28","title":"On Inapproximability of Reconfiguration Problems: PSPACE-Hardness and some Tight NP-Hardness Results","abstract":"The field of combinatorial reconfiguration studies search problems with a focus on transforming one feasible solution into another.   Recently, Ohsaka [STACS'23] put forth the Reconfiguration Inapproximability Hypothesis (RIH), which roughly asserts that there is some $\\varepsilon>0$ such that given as input a $k$-CSP instance (for some constant $k$) over some constant sized alphabet, and two satisfying assignments $\\psi_s$ and $\\psi_t$, it is PSPACE-hard to find a sequence of assignments starting from $\\psi_s$ and ending at $\\psi_t$ such that every assignment in the sequence satisfies at least $(1-\\varepsilon)$ fraction of the constraints and also that every assignment in the sequence is obtained by changing its immediately preceding assignment (in the sequence) on exactly one variable. Assuming RIH, many important reconfiguration problems have been shown to be PSPACE-hard to approximate by Ohsaka [STACS'23; SODA'24].   In this paper, we prove RIH, thus establishing the first (constant factor) PSPACE-hardness of approximation results for many reconfiguration problems, resolving an open question posed by Ito et al. [TCS'11]. Our proof uses known constructions of Probabilistically Checkable Proofs of Proximity (in a black-box manner) to create the gap.   We also prove that the aforementioned $k$-CSP Reconfiguration problem is NP-hard to approximate to within a factor of $1/2 + \\varepsilon$ (for any $\\varepsilon>0$) when $k=2$. We complement this with a $(1/2 - \\varepsilon)$-approximation polynomial time algorithm, which improves upon a $(1/4 - \\varepsilon)$-approximation algorithm of Ohsaka [2023] (again for any $\\varepsilon>0$).   Finally, we show that Set Cover Reconfiguration is NP-hard to approximate to within a factor of $2 - \\varepsilon$ for any constant $\\varepsilon > 0$, which matches the simple linear-time 2-approximation algorithm by Ito et al. [TCS'11].","sentences":["The field of combinatorial reconfiguration studies search problems with a focus on transforming one feasible solution into another.   ","Recently, Ohsaka [STACS'23] put forth the Reconfiguration Inapproximability Hypothesis (RIH), which roughly asserts that there is some $\\varepsilon>0$ such that given as input a $k$-CSP instance (for some constant $k$) over some constant sized alphabet, and two satisfying assignments $\\psi_s$ and $\\psi_t$, it is PSPACE-hard to find a sequence of assignments starting from $\\psi_s$ and ending at $\\psi_t$ such that every assignment in the sequence satisfies at least $(1-\\varepsilon)$ fraction of the constraints and also that every assignment in the sequence is obtained by changing its immediately preceding assignment (in the sequence) on exactly one variable.","Assuming RIH, many important reconfiguration problems have been shown to be PSPACE-hard to approximate by Ohsaka","[STACS'23; SODA'24].   ","In this paper, we prove RIH, thus establishing the first (constant factor) PSPACE-hardness of approximation results for many reconfiguration problems, resolving an open question posed by Ito et al.","[TCS'11].","Our proof uses known constructions of Probabilistically Checkable Proofs of Proximity (in a black-box manner) to create the gap.   ","We also prove that the aforementioned $k$-CSP Reconfiguration problem is NP-hard to approximate to within a factor of $1/2 + \\varepsilon$ (for any $\\varepsilon>0$) when $k=2$. We complement this with a $(1/2 - \\varepsilon)$-approximation polynomial time algorithm, which improves upon a $(1/4 - \\varepsilon)$-approximation algorithm of Ohsaka [2023] (again for any $\\varepsilon>0$).   ","Finally, we show that Set Cover Reconfiguration is NP-hard to approximate to within a factor of $2 - \\varepsilon$ for any constant $\\varepsilon > 0$, which matches the simple linear-time 2-approximation algorithm by Ito et al.","[TCS'11]."],"url":"http://arxiv.org/abs/2312.17140v1"}
{"created":"2023-12-28 16:59:06","title":"Large Language Model for Causal Decision Making","abstract":"Large Language Models (LLMs) have shown their success in language understanding and reasoning on general topics. However, their capability to inference based on user-specified structured data and knowledge in corpus-rare concepts like causal decision-making is still limited. In this work, we explore the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can identify the causal task, execute a corresponding function, and interpret its numerical results based on users' queries and the provided dataset. Meanwhile, we propose a data generation process for more controllable GPT prompting and present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal problem identification and input parameter extraction for causal function calling and (2) Causal-Interpret-Bench for in-context causal interpretation. With three case studies, we showed that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers. Numerical studies also reveal that it has a remarkable ability to identify the correct causal task given a query.","sentences":["Large Language Models (LLMs) have shown their success in language understanding and reasoning on general topics.","However, their capability to inference based on user-specified structured data and knowledge in corpus-rare concepts like causal decision-making is still limited.","In this work, we explore the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can identify the causal task, execute a corresponding function, and interpret its numerical results based on users' queries and the provided dataset.","Meanwhile, we propose a data generation process for more controllable GPT prompting and present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal problem identification and input parameter extraction for causal function calling and (2) Causal-Interpret-Bench for in-context causal interpretation.","With three case studies, we showed that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers.","Numerical studies also reveal that it has a remarkable ability to identify the correct causal task given a query."],"url":"http://arxiv.org/abs/2312.17122v1"}
{"created":"2023-12-28 16:55:40","title":"Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math","abstract":"High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce \\textsc{MathPile}, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of ``\\emph{less is more}'', firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates. We hope our \\textsc{MathPile} can help to enhance the mathematical reasoning abilities of language models. We plan to open-source different versions of \\mathpile with the scripts used for processing, to facilitate future developments in this field.","sentences":["High-quality, large-scale corpora are the cornerstone of building foundation models.","In this work, we introduce \\textsc{MathPile}, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens.","Throughout its creation, we adhered to the principle of ``\\emph{less is more}'', firmly believing in the supremacy of data quality over quantity, even in the pre-training phase.","Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus.","Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates.","We hope our \\textsc{MathPile} can help to enhance the mathematical reasoning abilities of language models.","We plan to open-source different versions of \\mathpile with the scripts used for processing, to facilitate future developments in this field."],"url":"http://arxiv.org/abs/2312.17120v1"}
{"created":"2023-12-28 16:53:23","title":"Generalizable Visual Reinforcement Learning with Segment Anything Model","abstract":"Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL). While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged. In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents. We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly. Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations. Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods. Video and code: https://yanjieze.com/SAM-G/","sentences":["Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL).","While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged.","In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents.","We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly.","Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations.","Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods.","Video and code:","https://yanjieze.com/SAM-G/"],"url":"http://arxiv.org/abs/2312.17116v1"}
{"created":"2023-12-28 16:23:58","title":"TSPP: A Unified Benchmarking Tool for Time-series Forecasting","abstract":"Recently there has been increasing interest in developing and deploying deep graph learning algorithms for many tasks, such as fraud detection and recommender systems. Albeit, there is a limited number of publicly available graph-structured datasets, most of which are tiny compared to production-sized applications or are limited in their application domain. This work tackles this shortcoming by proposing a scalable synthetic graph generation tool to scale the datasets to production-size graphs with trillions of edges and billions of nodes. The tool learns a series of parametric models from proprietary datasets that can be released to researchers to study various graph methods on the synthetic data increasing prototype development and novel applications. We demonstrate the generalizability of the framework across a series of datasets, mimicking structural and feature distributions as well as the ability to scale them across varying sizes demonstrating their usefulness for benchmarking and model development.","sentences":["Recently there has been increasing interest in developing and deploying deep graph learning algorithms for many tasks, such as fraud detection and recommender systems.","Albeit, there is a limited number of publicly available graph-structured datasets, most of which are tiny compared to production-sized applications or are limited in their application domain.","This work tackles this shortcoming by proposing a scalable synthetic graph generation tool to scale the datasets to production-size graphs with trillions of edges and billions of nodes.","The tool learns a series of parametric models from proprietary datasets that can be released to researchers to study various graph methods on the synthetic data increasing prototype development and novel applications.","We demonstrate the generalizability of the framework across a series of datasets, mimicking structural and feature distributions as well as the ability to scale them across varying sizes demonstrating their usefulness for benchmarking and model development."],"url":"http://arxiv.org/abs/2312.17100v1"}
{"created":"2023-12-28 15:34:54","title":"An Adaptive Framework of Geographical Group-Specific Network on O2O Recommendation","abstract":"Online to offline recommendation strongly correlates with the user and service's spatiotemporal information, therefore calling for a higher degree of model personalization. The traditional methodology is based on a uniform model structure trained by collected centralized data, which is unlikely to capture all user patterns over different geographical areas or time periods. To tackle this challenge, we propose a geographical group-specific modeling method called GeoGrouse, which simultaneously studies the common knowledge as well as group-specific knowledge of user preferences. An automatic grouping paradigm is employed and verified based on users' geographical grouping indicators. Offline and online experiments are conducted to verify the effectiveness of our approach, and substantial business improvement is achieved.","sentences":["Online to offline recommendation strongly correlates with the user and service's spatiotemporal information, therefore calling for a higher degree of model personalization.","The traditional methodology is based on a uniform model structure trained by collected centralized data, which is unlikely to capture all user patterns over different geographical areas or time periods.","To tackle this challenge, we propose a geographical group-specific modeling method called GeoGrouse, which simultaneously studies the common knowledge as well as group-specific knowledge of user preferences.","An automatic grouping paradigm is employed and verified based on users' geographical grouping indicators.","Offline and online experiments are conducted to verify the effectiveness of our approach, and substantial business improvement is achieved."],"url":"http://arxiv.org/abs/2312.17072v1"}
{"created":"2023-12-28 14:53:32","title":"Multi-Attention Fusion Drowsy Driving Detection Model","abstract":"Drowsy driving represents a major contributor to traffic accidents, and the implementation of driver drowsy driving detection systems has been proven to significantly reduce the occurrence of such accidents. Despite the development of numerous drowsy driving detection algorithms, many of them impose specific prerequisites such as the availability of complete facial images, optimal lighting conditions, and the use of RGB images. In our study, we introduce a novel approach called the Multi-Attention Fusion Drowsy Driving Detection Model (MAF). MAF is aimed at significantly enhancing classification performance, especially in scenarios involving partial facial occlusion and low lighting conditions. It accomplishes this by capitalizing on the local feature extraction capabilities provided by multi-attention fusion, thereby enhancing the algorithm's overall robustness. To enhance our dataset, we collected real-world data that includes both occluded and unoccluded faces captured under nighttime and daytime lighting conditions. We conducted a comprehensive series of experiments using both publicly available datasets and our self-built data. The results of these experiments demonstrate that our proposed model achieves an impressive driver drowsiness detection accuracy of 96.8%.","sentences":["Drowsy driving represents a major contributor to traffic accidents, and the implementation of driver drowsy driving detection systems has been proven to significantly reduce the occurrence of such accidents.","Despite the development of numerous drowsy driving detection algorithms, many of them impose specific prerequisites such as the availability of complete facial images, optimal lighting conditions, and the use of RGB images.","In our study, we introduce a novel approach called the Multi-Attention Fusion Drowsy Driving Detection Model (MAF).","MAF is aimed at significantly enhancing classification performance, especially in scenarios involving partial facial occlusion and low lighting conditions.","It accomplishes this by capitalizing on the local feature extraction capabilities provided by multi-attention fusion, thereby enhancing the algorithm's overall robustness.","To enhance our dataset, we collected real-world data that includes both occluded and unoccluded faces captured under nighttime and daytime lighting conditions.","We conducted a comprehensive series of experiments using both publicly available datasets and our self-built data.","The results of these experiments demonstrate that our proposed model achieves an impressive driver drowsiness detection accuracy of 96.8%."],"url":"http://arxiv.org/abs/2312.17052v1"}
{"created":"2023-12-28 14:52:07","title":"FILP-3D: Enhancing 3D Few-shot Class-incremental Learning with Pre-trained Vision-Language Models","abstract":"Few-shot class-incremental learning (FSCIL) aims to mitigate the catastrophic forgetting issue when a model is incrementally trained on limited data. While the Contrastive Vision-Language Pre-Training (CLIP) model has been effective in addressing 2D few/zero-shot learning tasks, its direct application to 3D FSCIL faces limitations. These limitations arise from feature space misalignment and significant noise in real-world scanned 3D data. To address these challenges, we introduce two novel components: the Redundant Feature Eliminator (RFE) and the Spatial Noise Compensator (SNC). RFE aligns the feature spaces of input point clouds and their embeddings by performing a unique dimensionality reduction on the feature space of pre-trained models (PTMs), effectively eliminating redundant information without compromising semantic integrity. On the other hand, SNC is a graph-based 3D model designed to capture robust geometric information within point clouds, thereby augmenting the knowledge lost due to projection, particularly when processing real-world scanned data. Considering the imbalance in existing 3D datasets, we also propose new evaluation metrics that offer a more nuanced assessment of a 3D FSCIL model. Traditional accuracy metrics are proved to be biased; thus, our metrics focus on the model's proficiency in learning new classes while maintaining the balance between old and new classes. Experimental results on both established 3D FSCIL benchmarks and our dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods.","sentences":["Few-shot class-incremental learning (FSCIL) aims to mitigate the catastrophic forgetting issue when a model is incrementally trained on limited data.","While the Contrastive Vision-Language Pre-Training (CLIP) model has been effective in addressing 2D few/zero-shot learning tasks, its direct application to 3D FSCIL faces limitations.","These limitations arise from feature space misalignment and significant noise in real-world scanned 3D data.","To address these challenges, we introduce two novel components: the Redundant Feature Eliminator (RFE) and the Spatial Noise Compensator (SNC).","RFE aligns the feature spaces of input point clouds and their embeddings by performing a unique dimensionality reduction on the feature space of pre-trained models (PTMs), effectively eliminating redundant information without compromising semantic integrity.","On the other hand, SNC is a graph-based 3D model designed to capture robust geometric information within point clouds, thereby augmenting the knowledge lost due to projection, particularly when processing real-world scanned data.","Considering the imbalance in existing 3D datasets, we also propose new evaluation metrics that offer a more nuanced assessment of a 3D FSCIL model.","Traditional accuracy metrics are proved to be biased; thus, our metrics focus on the model's proficiency in learning new classes while maintaining the balance between old and new classes.","Experimental results on both established 3D FSCIL benchmarks and our dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2312.17051v1"}
{"created":"2023-12-28 14:42:24","title":"Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding","abstract":"Transformer has taken the natural language processing (NLP) field by storm since birth, owing to its superior ability to model complex dependencies in sequences. Despite the great success of pretrained language models (PLMs) based on Transformer across almost all NLP tasks, they all suffer from a preset length limit and thus can hardly extend this success to longer sequences beyond seen data, namely the length extrapolation problem. Length extrapolation has aroused great interest among researchers, as it is the core feature of human language capacity. To enhance length extrapolation of Transformers, a plethora of methods have been proposed, mostly focusing on extrapolatable position encodings. In this article, we provide an organized and systematical review of these research efforts in a unified notation from a position encoding perspective, aiming to enable the reader to gain a deep understanding of existing methods and provide stimuli for future research.","sentences":["Transformer has taken the natural language processing (NLP) field by storm since birth, owing to its superior ability to model complex dependencies in sequences.","Despite the great success of pretrained language models (PLMs) based on Transformer across almost all NLP tasks, they all suffer from a preset length limit and thus can hardly extend this success to longer sequences beyond seen data, namely the length extrapolation problem.","Length extrapolation has aroused great interest among researchers, as it is the core feature of human language capacity.","To enhance length extrapolation of Transformers, a plethora of methods have been proposed, mostly focusing on extrapolatable position encodings.","In this article, we provide an organized and systematical review of these research efforts in a unified notation from a position encoding perspective, aiming to enable the reader to gain a deep understanding of existing methods and provide stimuli for future research."],"url":"http://arxiv.org/abs/2312.17044v1"}
{"created":"2023-12-28 14:31:07","title":"AI Powered Road Network Prediction with Multi-Modal Data","abstract":"This study presents an innovative approach for automatic road detection with deep learning, by employing fusion strategies for utilizing both lower-resolution satellite imagery and GPS trajectory data, a concept never explored before. We rigorously investigate both early and late fusion strategies, and assess deep learning based road detection performance using different fusion settings. Our extensive ablation studies assess the efficacy of our framework under diverse model architectures, loss functions, and geographic domains (Istanbul and Montreal). For an unbiased and complete evaluation of road detection results, we use both region-based and boundary-based evaluation metrics for road segmentation. The outcomes reveal that the ResUnet model outperforms U-Net and D-Linknet in road extraction tasks, achieving superior results over the benchmark study using low-resolution Sentinel-2 data. This research not only contributes to the field of automatic road detection but also offers novel insights into the utilization of data fusion methods in diverse applications.","sentences":["This study presents an innovative approach for automatic road detection with deep learning, by employing fusion strategies for utilizing both lower-resolution satellite imagery and GPS trajectory data, a concept never explored before.","We rigorously investigate both early and late fusion strategies, and assess deep learning based road detection performance using different fusion settings.","Our extensive ablation studies assess the efficacy of our framework under diverse model architectures, loss functions, and geographic domains (Istanbul and Montreal).","For an unbiased and complete evaluation of road detection results, we use both region-based and boundary-based evaluation metrics for road segmentation.","The outcomes reveal that the ResUnet model outperforms U-Net and D-Linknet in road extraction tasks, achieving superior results over the benchmark study using low-resolution Sentinel-2 data.","This research not only contributes to the field of automatic road detection but also offers novel insights into the utilization of data fusion methods in diverse applications."],"url":"http://arxiv.org/abs/2312.17040v1"}
{"created":"2023-12-28 13:50:38","title":"Selective Run-Length Encoding","abstract":"Run-Length Encoding (RLE) is one of the most fundamental tools in data compression. However, its compression power drops significantly if there lacks consecutive elements in the sequence. In extreme cases, the output of the encoder may require more space than the input (aka size inflation). To alleviate this issue, using combinatorics, we quantify RLE's space savings for a given input distribution. With this insight, we develop the first algorithm that automatically identifies suitable symbols, then selectively encodes these symbols with RLE while directly storing the others without RLE. Through experiments on real-world datasets of various modalities, we empirically validate that our method, which maintains RLE's efficiency advantage, can effectively mitigate the size inflation dilemma.","sentences":["Run-Length Encoding (RLE) is one of the most fundamental tools in data compression.","However, its compression power drops significantly if there lacks consecutive elements in the sequence.","In extreme cases, the output of the encoder may require more space than the input (aka size inflation).","To alleviate this issue, using combinatorics, we quantify RLE's space savings for a given input distribution.","With this insight, we develop the first algorithm that automatically identifies suitable symbols, then selectively encodes these symbols with RLE while directly storing the others without RLE.","Through experiments on real-world datasets of various modalities, we empirically validate that our method, which maintains RLE's efficiency advantage, can effectively mitigate the size inflation dilemma."],"url":"http://arxiv.org/abs/2312.17024v1"}
{"created":"2023-12-28 13:27:06","title":"Perspectives of Global and Hong Kong's Media on China's Belt and Road Initiative","abstract":"This study delves into the media analysis of China's ambitious Belt and Road Initiative (BRI), which, in a polarized world, and furthermore, owing to the very polarizing nature of the initiative itself, has received both strong criticisms and conversely positive coverage in media from across the world. In that context, Hong Kong's dynamic media environment, with a particular focus on its drastically changing press freedom before and after the implementation of the National Security Law is of further interest.   Leveraging data science techniques, this study employs Global Database of Events, Language, and Tone (GDELT) to comprehensively collect and analyse (English) news articles on the BRI. Through sentiment analysis, we uncover patterns in media coverage over different periods from several countries across the globe, and delve further to investigate the the media situation in the Hong Kong region. This work thus provides valuable insights into how the Belt and Road Initiative has been portrayed in the media and its evolving reception on the global stage, with a specific emphasis on the unique media landscape of Hong Kong.   In an era characterised by increasing globalisation and inter-connectivity, but also competition for influence, animosity and trade-wars, understanding the perceptions and coverage of such significant international projects is crucial. This work stands as an interdisciplinary endeavour merging geopolitical science and data science to uncover the intricate dynamics of media coverage in general, and with an added emphasis on Hong Kong.","sentences":["This study delves into the media analysis of China's ambitious Belt and Road Initiative (BRI), which, in a polarized world, and furthermore, owing to the very polarizing nature of the initiative itself, has received both strong criticisms and conversely positive coverage in media from across the world.","In that context, Hong Kong's dynamic media environment, with a particular focus on its drastically changing press freedom before and after the implementation of the National Security Law is of further interest.   ","Leveraging data science techniques, this study employs Global Database of Events, Language, and Tone (GDELT) to comprehensively collect and analyse (English) news articles on the BRI.","Through sentiment analysis, we uncover patterns in media coverage over different periods from several countries across the globe, and delve further to investigate the the media situation in the Hong Kong region.","This work thus provides valuable insights into how the Belt and Road Initiative has been portrayed in the media and its evolving reception on the global stage, with a specific emphasis on the unique media landscape of Hong Kong.   ","In an era characterised by increasing globalisation and inter-connectivity, but also competition for influence, animosity and trade-wars, understanding the perceptions and coverage of such significant international projects is crucial.","This work stands as an interdisciplinary endeavour merging geopolitical science and data science to uncover the intricate dynamics of media coverage in general, and with an added emphasis on Hong Kong."],"url":"http://arxiv.org/abs/2312.17013v1"}
{"created":"2023-12-28 13:26:44","title":"A Generalization of the Sugeno integral to aggregate Interval-valued data: an application to Brain Computer Interface and Social Network Analysis","abstract":"Intervals are a popular way to represent the uncertainty related to data, in which we express the vagueness of each observation as the width of the interval. However, when using intervals for this purpose, we need to use the appropriate set of mathematical tools to work with. This can be problematic due to the scarcity and complexity of interval-valued functions in comparison with the numerical ones. In this work, we propose to extend a generalization of the Sugeno integral to work with interval-valued data. Then, we use this integral to aggregate interval-valued data in two different settings: first, we study the use of intervals in a brain-computer interface; secondly, we study how to construct interval-valued relationships in a social network, and how to aggregate their information. Our results show that interval-valued data can effectively model some of the uncertainty and coalitions of the data in both cases. For the case of brain-computer interface, we found that our results surpassed the results of other interval-valued functions.","sentences":["Intervals are a popular way to represent the uncertainty related to data, in which we express the vagueness of each observation as the width of the interval.","However, when using intervals for this purpose, we need to use the appropriate set of mathematical tools to work with.","This can be problematic due to the scarcity and complexity of interval-valued functions in comparison with the numerical ones.","In this work, we propose to extend a generalization of the Sugeno integral to work with interval-valued data.","Then, we use this integral to aggregate interval-valued data in two different settings: first, we study the use of intervals in a brain-computer interface; secondly, we study how to construct interval-valued relationships in a social network, and how to aggregate their information.","Our results show that interval-valued data can effectively model some of the uncertainty and coalitions of the data in both cases.","For the case of brain-computer interface, we found that our results surpassed the results of other interval-valued functions."],"url":"http://arxiv.org/abs/2312.17012v1"}
{"created":"2023-12-28 13:20:36","title":"On the rate of convergence of an over-parametrized Transformer classifier learned by gradient descent","abstract":"One of the most recent and fascinating breakthroughs in artificial intelligence is ChatGPT, a chatbot which can simulate human conversation. ChatGPT is an instance of GPT4, which is a language model based on generative gredictive gransformers. So if one wants to study from a theoretical point of view, how powerful such artificial intelligence can be, one approach is to consider transformer networks and to study which problems one can solve with these networks theoretically. Here it is not only important what kind of models these network can approximate, or how they can generalize their knowledge learned by choosing the best possible approximation to a concrete data set, but also how well optimization of such transformer network based on concrete data set works. In this article we consider all these three different aspects simultaneously and show a theoretical upper bound on the missclassification probability of a transformer network fitted to the observed data. For simplicity we focus in this context on transformer encoder networks which can be applied to define an estimate in the context of a classification problem involving natural language.","sentences":["One of the most recent and fascinating breakthroughs in artificial intelligence is ChatGPT, a chatbot which can simulate human conversation.","ChatGPT is an instance of GPT4, which is a language model based on generative gredictive gransformers.","So if one wants to study from a theoretical point of view, how powerful such artificial intelligence can be, one approach is to consider transformer networks and to study which problems one can solve with these networks theoretically.","Here it is not only important what kind of models these network can approximate, or how they can generalize their knowledge learned by choosing the best possible approximation to a concrete data set, but also how well optimization of such transformer network based on concrete data set works.","In this article we consider all these three different aspects simultaneously and show a theoretical upper bound on the missclassification probability of a transformer network fitted to the observed data.","For simplicity we focus in this context on transformer encoder networks which can be applied to define an estimate in the context of a classification problem involving natural language."],"url":"http://arxiv.org/abs/2312.17007v1"}
{"created":"2023-12-28 13:02:53","title":"Multi-Tier Computing-Enabled Digital Twin in 6G Networks","abstract":"Digital twin (DT) is the recurrent and common feature in discussions about future technologies, bringing together advanced communication, computation, and artificial intelligence, to name a few. In the context of Industry 4.0, industries such as manufacturing, automotive, and healthcare are rapidly adopting DT-based development. The main challenges to date have been the high demands on communication and computing resources, as well as privacy and security concerns, arising from the large volumes of data exchanges. To achieve low latency and high security services in the emerging DT, multi-tier computing has been proposed by combining edge/fog computing and cloud computing. Specifically, low latency data transmission, efficient resource allocation, and validated security strategies of multi-tier computing systems are used to solve the operational problems of the DT system. In this paper, we introduce the architecture and applications of DT using examples from manufacturing, the Internet-of-Vehicles and healthcare. At the same time, the architecture and technology of multi-tier computing systems are studied to support DT. This paper will provide valuable reference and guidance for the theory, algorithms, and applications in collaborative multi-tier computing and DT.","sentences":["Digital twin (DT) is the recurrent and common feature in discussions about future technologies, bringing together advanced communication, computation, and artificial intelligence, to name a few.","In the context of Industry 4.0, industries such as manufacturing, automotive, and healthcare are rapidly adopting DT-based development.","The main challenges to date have been the high demands on communication and computing resources, as well as privacy and security concerns, arising from the large volumes of data exchanges.","To achieve low latency and high security services in the emerging DT, multi-tier computing has been proposed by combining edge/fog computing and cloud computing.","Specifically, low latency data transmission, efficient resource allocation, and validated security strategies of multi-tier computing systems are used to solve the operational problems of the DT system.","In this paper, we introduce the architecture and applications of DT using examples from manufacturing, the Internet-of-Vehicles and healthcare.","At the same time, the architecture and technology of multi-tier computing systems are studied to support DT.","This paper will provide valuable reference and guidance for the theory, algorithms, and applications in collaborative multi-tier computing and DT."],"url":"http://arxiv.org/abs/2312.16999v1"}
{"created":"2023-12-28 11:57:58","title":"PG-LBO: Enhancing High-Dimensional Bayesian Optimization with Pseudo-Label and Gaussian Process Guidance","abstract":"Variational Autoencoder based Bayesian Optimization (VAE-BO) has demonstrated its excellent performance in addressing high-dimensional structured optimization problems. However, current mainstream methods overlook the potential of utilizing a pool of unlabeled data to construct the latent space, while only concentrating on designing sophisticated models to leverage the labeled data. Despite their effective usage of labeled data, these methods often require extra network structures, additional procedure, resulting in computational inefficiency. To address this issue, we propose a novel method to effectively utilize unlabeled data with the guidance of labeled data. Specifically, we tailor the pseudo-labeling technique from semi-supervised learning to explicitly reveal the relative magnitudes of optimization objective values hidden within the unlabeled data. Based on this technique, we assign appropriate training weights to unlabeled data to enhance the construction of a discriminative latent space. Furthermore, we treat the VAE encoder and the Gaussian Process (GP) in Bayesian optimization as a unified deep kernel learning process, allowing the direct utilization of labeled data, which we term as Gaussian Process guidance. This directly and effectively integrates the goal of improving GP accuracy into the VAE training, thereby guiding the construction of the latent space. The extensive experiments demonstrate that our proposed method outperforms existing VAE-BO algorithms in various optimization scenarios. Our code will be published at https://github.com/TaicaiChen/PG-LBO.","sentences":["Variational Autoencoder based Bayesian Optimization (VAE-BO) has demonstrated its excellent performance in addressing high-dimensional structured optimization problems.","However, current mainstream methods overlook the potential of utilizing a pool of unlabeled data to construct the latent space, while only concentrating on designing sophisticated models to leverage the labeled data.","Despite their effective usage of labeled data, these methods often require extra network structures, additional procedure, resulting in computational inefficiency.","To address this issue, we propose a novel method to effectively utilize unlabeled data with the guidance of labeled data.","Specifically, we tailor the pseudo-labeling technique from semi-supervised learning to explicitly reveal the relative magnitudes of optimization objective values hidden within the unlabeled data.","Based on this technique, we assign appropriate training weights to unlabeled data to enhance the construction of a discriminative latent space.","Furthermore, we treat the VAE encoder and the Gaussian Process (GP) in Bayesian optimization as a unified deep kernel learning process, allowing the direct utilization of labeled data, which we term as Gaussian Process guidance.","This directly and effectively integrates the goal of improving GP accuracy into the VAE training, thereby guiding the construction of the latent space.","The extensive experiments demonstrate that our proposed method outperforms existing VAE-BO algorithms in various optimization scenarios.","Our code will be published at https://github.com/TaicaiChen/PG-LBO."],"url":"http://arxiv.org/abs/2312.16983v1"}
{"created":"2023-12-28 11:39:08","title":"Few-shot learning for automated content analysis: Efficient coding of arguments and claims in the debate on arms deliveries to Ukraine","abstract":"Pre-trained language models (PLM) based on transformer neural networks developed in the field of natural language processing (NLP) offer great opportunities to improve automatic content analysis in communication science, especially for the coding of complex semantic categories in large datasets via supervised machine learning. However, three characteristics so far impeded the widespread adoption of the methods in the applying disciplines: the dominance of English language models in NLP research, the necessary computing resources, and the effort required to produce training data to fine-tune PLMs. In this study, we address these challenges by using a multilingual transformer model in combination with the adapter extension to transformers, and few-shot learning methods. We test our approach on a realistic use case from communication science to automatically detect claims and arguments together with their stance in the German news debate on arms deliveries to Ukraine. In three experiments, we evaluate (1) data preprocessing strategies and model variants for this task, (2) the performance of different few-shot learning methods, and (3) how well the best setup performs on varying training set sizes in terms of validity, reliability, replicability and reproducibility of the results. We find that our proposed combination of transformer adapters with pattern exploiting training provides a parameter-efficient and easily shareable alternative to fully fine-tuning PLMs. It performs on par in terms of validity, while overall, provides better properties for application in communication studies. The results also show that pre-fine-tuning for a task on a near-domain dataset leads to substantial improvement, in particular in the few-shot setting. Further, the results indicate that it is useful to bias the dataset away from the viewpoints of specific prominent individuals.","sentences":["Pre-trained language models (PLM) based on transformer neural networks developed in the field of natural language processing (NLP) offer great opportunities to improve automatic content analysis in communication science, especially for the coding of complex semantic categories in large datasets via supervised machine learning.","However, three characteristics so far impeded the widespread adoption of the methods in the applying disciplines: the dominance of English language models in NLP research, the necessary computing resources, and the effort required to produce training data to fine-tune PLMs.","In this study, we address these challenges by using a multilingual transformer model in combination with the adapter extension to transformers, and few-shot learning methods.","We test our approach on a realistic use case from communication science to automatically detect claims and arguments together with their stance in the German news debate on arms deliveries to Ukraine.","In three experiments, we evaluate (1) data preprocessing strategies and model variants for this task, (2) the performance of different few-shot learning methods, and (3) how well the best setup performs on varying training set sizes in terms of validity, reliability, replicability and reproducibility of the results.","We find that our proposed combination of transformer adapters with pattern exploiting training provides a parameter-efficient and easily shareable alternative to fully fine-tuning PLMs.","It performs on par in terms of validity, while overall, provides better properties for application in communication studies.","The results also show that pre-fine-tuning for a task on a near-domain dataset leads to substantial improvement, in particular in the few-shot setting.","Further, the results indicate that it is useful to bias the dataset away from the viewpoints of specific prominent individuals."],"url":"http://arxiv.org/abs/2312.16975v1"}
{"created":"2023-12-28 11:14:43","title":"Reinforcement-based Display-size Selection for Frugal Satellite Image Change Detection","abstract":"We introduce a novel interactive satellite image change detection algorithm based on active learning. The proposed method is iterative and consists in frugally probing the user (oracle) about the labels of the most critical images, and according to the oracle's annotations, it updates change detection results. First, we consider a probabilistic framework which assigns to each unlabeled sample a relevance measure modeling how critical is that sample when training change detection functions. We obtain these relevance measures by minimizing an objective function mixing diversity, representativity and uncertainty. These criteria when combined allow exploring different data modes and also refining change detections. Then, we further explore the potential of this objective function, by considering a reinforcement learning approach that finds the best combination of diversity, representativity and uncertainty as well as display-sizes through active learning iterations, leading to better generalization as shown through experiments in interactive satellite image change detection.","sentences":["We introduce a novel interactive satellite image change detection algorithm based on active learning.","The proposed method is iterative and consists in frugally probing the user (oracle) about the labels of the most critical images, and according to the oracle's annotations, it updates change detection results.","First, we consider a probabilistic framework which assigns to each unlabeled sample a relevance measure modeling how critical is that sample when training change detection functions.","We obtain these relevance measures by minimizing an objective function mixing diversity, representativity and uncertainty.","These criteria when combined allow exploring different data modes and also refining change detections.","Then, we further explore the potential of this objective function, by considering a reinforcement learning approach that finds the best combination of diversity, representativity and uncertainty as well as display-sizes through active learning iterations, leading to better generalization as shown through experiments in interactive satellite image change detection."],"url":"http://arxiv.org/abs/2312.16965v1"}
{"created":"2023-12-28 11:13:40","title":"Algorithms for Optimally Shifting Intervals under Intersection Graph Models","abstract":"We propose a new model for graph editing problems on intersection graphs. In well-studied graph editing problems, adding and deleting vertices and edges are used as graph editing operations. As a graph editing operation on intersection graphs, we propose moving objects corresponding to vertices. In this paper, we focus on interval graphs as an intersection graph. We give a linear-time algorithm to find the total moving distance for transforming an interval graph into a complete graph. The concept of this algorithm can be applied for (i) transforming a unit square graph into a complete graph over $L_\\infty$ distance and (ii) attaining the existence of a $k$-clique on unit interval graphs. In addition, we provide LP-formulations to achieve several properties in the associated graph of unit intervals.","sentences":["We propose a new model for graph editing problems on intersection graphs.","In well-studied graph editing problems, adding and deleting vertices and edges are used as graph editing operations.","As a graph editing operation on intersection graphs, we propose moving objects corresponding to vertices.","In this paper, we focus on interval graphs as an intersection graph.","We give a linear-time algorithm to find the total moving distance for transforming an interval graph into a complete graph.","The concept of this algorithm can be applied for (i) transforming a unit square graph into a complete graph over $L_\\infty$ distance and (ii) attaining the existence of a $k$-clique on unit interval graphs.","In addition, we provide LP-formulations to achieve several properties in the associated graph of unit intervals."],"url":"http://arxiv.org/abs/2312.16964v1"}
{"created":"2023-12-28 11:05:55","title":"Adaptive Flip Graph Algorithm for Matrix Multiplication","abstract":"This study proposes the \"adaptive flip graph algorithm\", which combines adaptive searches with the flip graph algorithm for finding fast and efficient methods for matrix multiplication. The adaptive flip graph algorithm addresses the inherent limitations of exploration and inefficient search encountered in the original flip graph algorithm, particularly when dealing with large matrix multiplication. For the limitation of exploration, the proposed algorithm adaptively transitions over the flip graph, introducing a flexibility that does not strictly reduce the number of multiplications. Concerning the issue of inefficient search in large instances, the proposed algorithm adaptively constraints the search range instead of relying on a completely random search, facilitating more effective exploration. Numerical experimental results demonstrate the effectiveness of the adaptive flip graph algorithm, showing a reduction in the number of multiplications for a $4\\times 5$ matrix multiplied by a $5\\times 5$ matrix from $76$ to $73$, and that from $95$ to $94$ for a $5 \\times 5$ matrix multiplied by another $5\\times 5$ matrix. These results are obtained in characteristic two.","sentences":["This study proposes the \"adaptive flip graph algorithm\", which combines adaptive searches with the flip graph algorithm for finding fast and efficient methods for matrix multiplication.","The adaptive flip graph algorithm addresses the inherent limitations of exploration and inefficient search encountered in the original flip graph algorithm, particularly when dealing with large matrix multiplication.","For the limitation of exploration, the proposed algorithm adaptively transitions over the flip graph, introducing a flexibility that does not strictly reduce the number of multiplications.","Concerning the issue of inefficient search in large instances, the proposed algorithm adaptively constraints the search range instead of relying on a completely random search, facilitating more effective exploration.","Numerical experimental results demonstrate the effectiveness of the adaptive flip graph algorithm, showing a reduction in the number of multiplications for a $4\\times 5$ matrix multiplied by a $5\\times 5$ matrix from $76$ to $73$, and that from $95$ to $94$ for a $5 \\times 5$ matrix multiplied by another $5\\times 5$ matrix.","These results are obtained in characteristic two."],"url":"http://arxiv.org/abs/2312.16960v1"}
{"created":"2023-12-28 10:58:14","title":"Blockchain-based Privacy-Preserving Public Key Searchable Encryption with Strong Traceability","abstract":"Public key searchable encryption (PKSE) scheme allows data users to search over encrypted data. To identify illegal users, many traceable PKSE schemes have been proposed. However, existing schemes cannot trace the keywords which illegal users searched and protect users' privacy simultaneously. In some practical applications, tracing both illegal users' identities and the keywords which they searched is quite important to against the abuse of data. It is a challenge to bind users' identities and keywords while protecting their privacy. Moreover, existing traceable PKSE schemes do not consider the unforgeability and immutability of trapdoor query records, which can lead to the occurrence of frame-up and denying. In this paper, to solve these problems, we propose a blockchain-based privacy-preserving PKSE with strong traceability (BP3KSEST) scheme. Our scheme provides the following features: (1) authorized users can authenticate to trapdoor generation center and obtain trapdoors without releasing their identities and keywords; (2) when data users misbehave in the system, the trusted third party (TTP) can trace both their identities and the keywords which they searched; (3) trapdoor query records are unforgeable; (4) trapdoor query records are immutable because records are stored in blockchain. Notably, this scheme is suitable to the scenarios where privacy must be considered, e.g., electronic health record (EHR). We formalize both the definition and security model of our BP3KSEST scheme, and present a concrete construction. Furthermore, the security of the proposed scheme is formally proven. Finally, the implementation and evaluation are conducted to analyze its efficiency.","sentences":["Public key searchable encryption (PKSE) scheme allows data users to search over encrypted data.","To identify illegal users, many traceable PKSE schemes have been proposed.","However, existing schemes cannot trace the keywords which illegal users searched and protect users' privacy simultaneously.","In some practical applications, tracing both illegal users' identities and the keywords which they searched is quite important to against the abuse of data.","It is a challenge to bind users' identities and keywords while protecting their privacy.","Moreover, existing traceable PKSE schemes do not consider the unforgeability and immutability of trapdoor query records, which can lead to the occurrence of frame-up and denying.","In this paper, to solve these problems, we propose a blockchain-based privacy-preserving PKSE with strong traceability (BP3KSEST) scheme.","Our scheme provides the following features: (1) authorized users can authenticate to trapdoor generation center and obtain trapdoors without releasing their identities and keywords; (2) when data users misbehave in the system, the trusted third party (TTP) can trace both their identities and the keywords which they searched; (3) trapdoor query records are unforgeable; (4) trapdoor query records are immutable because records are stored in blockchain.","Notably, this scheme is suitable to the scenarios where privacy must be considered, e.g., electronic health record (EHR).","We formalize both the definition and security model of our BP3KSEST scheme, and present a concrete construction.","Furthermore, the security of the proposed scheme is formally proven.","Finally, the implementation and evaluation are conducted to analyze its efficiency."],"url":"http://arxiv.org/abs/2312.16954v1"}
{"created":"2023-12-28 10:27:04","title":"Joint Signal Recovery and Graph Learning from Incomplete Time-Series","abstract":"Learning a graph from data is the key to taking advantage of graph signal processing tools. Most of the conventional algorithms for graph learning require complete data statistics, which might not be available in some scenarios. In this work, we aim to learn a graph from incomplete time-series observations. From another viewpoint, we consider the problem of semi-blind recovery of time-varying graph signals where the underlying graph model is unknown. We propose an algorithm based on the method of block successive upperbound minimization (BSUM), for simultaneous inference of the signal and the graph from incomplete data.   Simulation results on synthetic and real time-series demonstrate the performance of the proposed method for graph learning and signal recovery.","sentences":["Learning a graph from data is the key to taking advantage of graph signal processing tools.","Most of the conventional algorithms for graph learning require complete data statistics, which might not be available in some scenarios.","In this work, we aim to learn a graph from incomplete time-series observations.","From another viewpoint, we consider the problem of semi-blind recovery of time-varying graph signals where the underlying graph model is unknown.","We propose an algorithm based on the method of block successive upperbound minimization (BSUM), for simultaneous inference of the signal and the graph from incomplete data.   ","Simulation results on synthetic and real time-series demonstrate the performance of the proposed method for graph learning and signal recovery."],"url":"http://arxiv.org/abs/2312.16940v1"}
{"created":"2023-12-28 10:05:13","title":"EvPlug: Learn a Plug-and-Play Module for Event and Image Fusion","abstract":"Event cameras and RGB cameras exhibit complementary characteristics in imaging: the former possesses high dynamic range (HDR) and high temporal resolution, while the latter provides rich texture and color information. This makes the integration of event cameras into middle- and high-level RGB-based vision tasks highly promising. However, challenges arise in multi-modal fusion, data annotation, and model architecture design. In this paper, we propose EvPlug, which learns a plug-and-play event and image fusion module from the supervision of the existing RGB-based model. The learned fusion module integrates event streams with image features in the form of a plug-in, endowing the RGB-based model to be robust to HDR and fast motion scenes while enabling high temporal resolution inference. Our method only requires unlabeled event-image pairs (no pixel-wise alignment required) and does not alter the structure or weights of the RGB-based model. We demonstrate the superiority of EvPlug in several vision tasks such as object detection, semantic segmentation, and 3D hand pose estimation","sentences":["Event cameras and RGB cameras exhibit complementary characteristics in imaging: the former possesses high dynamic range (HDR) and high temporal resolution, while the latter provides rich texture and color information.","This makes the integration of event cameras into middle- and high-level RGB-based vision tasks highly promising.","However, challenges arise in multi-modal fusion, data annotation, and model architecture design.","In this paper, we propose EvPlug, which learns a plug-and-play event and image fusion module from the supervision of the existing RGB-based model.","The learned fusion module integrates event streams with image features in the form of a plug-in, endowing the RGB-based model to be robust to HDR and fast motion scenes while enabling high temporal resolution inference.","Our method only requires unlabeled event-image pairs (no pixel-wise alignment required) and does not alter the structure or weights of the RGB-based model.","We demonstrate the superiority of EvPlug in several vision tasks such as object detection, semantic segmentation, and 3D hand pose estimation"],"url":"http://arxiv.org/abs/2312.16933v1"}
{"created":"2023-12-28 09:56:44","title":"Encoding categorical data: Is there yet anything 'hotter' than one-hot encoding?","abstract":"Categorical features are present in about 40% of real world problems, highlighting the crucial role of encoding as a preprocessing component. Some recent studies have reported benefits of the various target-based encoders over classical target-agnostic approaches. However, these claims are not supported by any statistical analysis, and are based on a single dataset or a very small and heterogeneous sample of datasets. The present study explores the encoding effects in an exhaustive sample of classification problems from OpenML repository. We fitted linear mixed-effects models to the experimental data, treating task ID as a random effect, and the encoding scheme and the various characteristics of categorical features as fixed effects. We found that in multiclass tasks, one-hot encoding and Helmert contrast coding outperform target-based encoders. In binary tasks, there were no significant differences across the encoding schemes; however, one-hot encoding demonstrated a marginally positive effect on the outcome. Importantly, we found no significant interactions between the encoding schemes and the characteristics of categorical features. This suggests that our findings are generalizable to a wide variety of problems across domains.","sentences":["Categorical features are present in about 40% of real world problems, highlighting the crucial role of encoding as a preprocessing component.","Some recent studies have reported benefits of the various target-based encoders over classical target-agnostic approaches.","However, these claims are not supported by any statistical analysis, and are based on a single dataset or a very small and heterogeneous sample of datasets.","The present study explores the encoding effects in an exhaustive sample of classification problems from OpenML repository.","We fitted linear mixed-effects models to the experimental data, treating task ID as a random effect, and the encoding scheme and the various characteristics of categorical features as fixed effects.","We found that in multiclass tasks, one-hot encoding and Helmert contrast coding outperform target-based encoders.","In binary tasks, there were no significant differences across the encoding schemes; however, one-hot encoding demonstrated a marginally positive effect on the outcome.","Importantly, we found no significant interactions between the encoding schemes and the characteristics of categorical features.","This suggests that our findings are generalizable to a wide variety of problems across domains."],"url":"http://arxiv.org/abs/2312.16930v1"}
{"created":"2023-12-28 09:50:56","title":"Efficient High-Quality Clustering for Large Bipartite Graphs","abstract":"A bipartite graph contains inter-set edges between two disjoint vertex sets, and is widely used to model real-world data, such as user-item purchase records, author-article publications, and biological interactions between drugs and proteins. k-Bipartite Graph Clustering (k-BGC) is to partition the target vertex set in a bipartite graph into k disjoint clusters. The clustering quality is important to the utility of k-BGC in various applications like social network analysis, recommendation systems, text mining, and bioinformatics, to name a few. Existing approaches to k-BGC either output clustering results with compromised quality due to inadequate exploitation of high-order information between vertices, or fail to handle sizable bipartite graphs with billions of edges.   Motivated by this, this paper presents two efficient k-BGC solutions, HOPE and HOPE+, which achieve state-of-the-art performance on large-scale bipartite graphs. HOPE obtains high scalability and effectiveness through a new k-BGC problem formulation based on the novel notion of high-order perspective (HOP) vectors and an efficient technique for low-rank approximation of HOP vectors. HOPE+ further elevates the k-BGC performance to another level with a judicious problem transformation and a highly efficient two-stage optimization framework. Two variants, HOPE+ (FNEM) and HOPE+ (SNEM) are designed when either the Frobenius norm or spectral norm is applied in the transformation. Extensive experiments, comparing HOPE and HOPE+ against 13 competitors on 10 real-world datasets, exhibit that our solutions, especially HOPE+, are superior to existing methods in terms of result quality, while being up to orders of magnitude faster. On the largest dataset MAG with 1.1 billion edges, HOPE+ is able to produce clusters with the highest clustering accuracy within 31 minutes, which is unmatched by any existing solution for k-BGC.","sentences":["A bipartite graph contains inter-set edges between two disjoint vertex sets, and is widely used to model real-world data, such as user-item purchase records, author-article publications, and biological interactions between drugs and proteins.","k-Bipartite Graph Clustering (k-BGC) is to partition the target vertex set in a bipartite graph into k disjoint clusters.","The clustering quality is important to the utility of k-BGC in various applications like social network analysis, recommendation systems, text mining, and bioinformatics, to name a few.","Existing approaches to k-BGC either output clustering results with compromised quality due to inadequate exploitation of high-order information between vertices, or fail to handle sizable bipartite graphs with billions of edges.   ","Motivated by this, this paper presents two efficient k-BGC solutions, HOPE and HOPE+, which achieve state-of-the-art performance on large-scale bipartite graphs.","HOPE obtains high scalability and effectiveness through a new k-BGC problem formulation based on the novel notion of high-order perspective (HOP) vectors and an efficient technique for low-rank approximation of HOP vectors.","HOPE+ further elevates the k-BGC performance to another level with a judicious problem transformation and a highly efficient two-stage optimization framework.","Two variants, HOPE+ (FNEM) and HOPE+ (SNEM) are designed when either the Frobenius norm or spectral norm is applied in the transformation.","Extensive experiments, comparing HOPE and HOPE+ against 13 competitors on 10 real-world datasets, exhibit that our solutions, especially HOPE+, are superior to existing methods in terms of result quality, while being up to orders of magnitude faster.","On the largest dataset MAG with 1.1 billion edges, HOPE+ is able to produce clusters with the highest clustering accuracy within 31 minutes, which is unmatched by any existing solution for k-BGC."],"url":"http://arxiv.org/abs/2312.16926v1"}
{"created":"2023-12-28 08:59:51","title":"A GAN-based Semantic Communication for Text without CSI","abstract":"Recently, semantic communication (SC) has been regarded as one of the potential paradigms of 6G. Current SC frameworks require channel state information (CSI) to handle severe signal distortion induced by channel fading. Since the channel estimation overhead for obtaining CSI cannot be neglected, we therefore propose a generative adversarial network (GAN) based SC framework (Ti-GSC) that doesn't require CSI. In Ti-GSC, two main modules, i.e., an autoencoder-based encoder-decoder module (AEDM) and a GAN-based signal distortion suppression module (GSDSM) are included where AEDM first encodes the data at the source before transmission, and then GSDSM suppresses the distortion of the received signals in both syntactic and semantic dimensions at the destination. At last, AEDM decodes the distortion-suppressed signal at the destination. To measure signal distortion, syntactic distortion and semantic distortion terms are newly added to the total loss function. To achieve better training results, joint optimization-based training (JOT) and alternating optimization-based training (AOT) are designed for the proposed Ti-GSC. Experimental results show that JOT is more efficient for Ti-GSC. Moreover, without CSI, bilingual evaluation understudy (BLEU) score achieved by Ti-GSC is about 40% and 62% higher than that achieved by existing SC frameworks in Rician and Rayleigh fading, respectively. (*Due to the notification of arXiv \"The Abstract field cannot be longer than 1,920 characters\", the appeared Abstract is shortened. For the full Abstract, please download the Article.)","sentences":["Recently, semantic communication (SC) has been regarded as one of the potential paradigms of 6G. Current SC frameworks require channel state information (CSI) to handle severe signal distortion induced by channel fading.","Since the channel estimation overhead for obtaining CSI cannot be neglected, we therefore propose a generative adversarial network (GAN) based SC framework (Ti-GSC) that doesn't require CSI.","In Ti-GSC, two main modules, i.e., an autoencoder-based encoder-decoder module (AEDM) and a GAN-based signal distortion suppression module (GSDSM) are included where AEDM first encodes the data at the source before transmission, and then GSDSM suppresses the distortion of the received signals in both syntactic and semantic dimensions at the destination.","At last, AEDM decodes the distortion-suppressed signal at the destination.","To measure signal distortion, syntactic distortion and semantic distortion terms are newly added to the total loss function.","To achieve better training results, joint optimization-based training (JOT) and alternating optimization-based training (AOT) are designed for the proposed Ti-GSC.","Experimental results show that JOT is more efficient for Ti-GSC.","Moreover, without CSI, bilingual evaluation understudy (BLEU) score achieved by Ti-GSC is about 40% and 62% higher than that achieved by existing SC frameworks in Rician and Rayleigh fading, respectively.","(*Due to the notification of arXiv \"The Abstract field cannot be longer than 1,920 characters\", the appeared Abstract is shortened.","For the full Abstract, please download the Article.)"],"url":"http://arxiv.org/abs/2312.16909v1"}
{"created":"2023-12-28 08:36:35","title":"Replication-proof Bandit Mechanism Design","abstract":"We study a problem of designing replication-proof bandit mechanisms when agents strategically register or replicate their own arms to maximize their payoff. We consider Bayesian agents who are unaware of ex-post realization of their own arms' mean rewards, which is the first to study Bayesian extension of Shin et al. (2022). This extension presents significant challenges in analyzing equilibrium, in contrast to the fully-informed setting by Shin et al. (2022) under which the problem simply reduces to a case where each agent only has a single arm. With Bayesian agents, even in a single-agent setting, analyzing the replication-proofness of an algorithm becomes complicated. Remarkably, we first show that the algorithm proposed by Shin et al. (2022), defined H-UCB, is no longer replication-proof for any exploration parameters. Then, we provide sufficient and necessary conditions for an algorithm to be replication-proof in the single-agent setting. These results centers around several analytical results in comparing the expected regret of multiple bandit instances, which might be of independent interest. We further prove that exploration-then-commit (ETC) algorithm satisfies these properties, whereas UCB does not, which in fact leads to the failure of being replication-proof. We expand this result to multi-agent setting, and provide a replication-proof algorithm for any problem instance. The proof mainly relies on the single-agent result, as well as some structural properties of ETC and the novel introduction of a restarting round, which largely simplifies the analysis while maintaining the regret unchanged (up to polylogarithmic factor). We finalize our result by proving its sublinear regret upper bound, which matches that of H-UCB.","sentences":["We study a problem of designing replication-proof bandit mechanisms when agents strategically register or replicate their own arms to maximize their payoff.","We consider Bayesian agents who are unaware of ex-post realization of their own arms' mean rewards, which is the first to study Bayesian extension of Shin et al. (2022).","This extension presents significant challenges in analyzing equilibrium, in contrast to the fully-informed setting by Shin et al. (2022) under which the problem simply reduces to a case where each agent only has a single arm.","With Bayesian agents, even in a single-agent setting, analyzing the replication-proofness of an algorithm becomes complicated.","Remarkably, we first show that the algorithm proposed by Shin et al. (2022), defined H-UCB, is no longer replication-proof for any exploration parameters.","Then, we provide sufficient and necessary conditions for an algorithm to be replication-proof in the single-agent setting.","These results centers around several analytical results in comparing the expected regret of multiple bandit instances, which might be of independent interest.","We further prove that exploration-then-commit (ETC) algorithm satisfies these properties, whereas UCB does not, which in fact leads to the failure of being replication-proof.","We expand this result to multi-agent setting, and provide a replication-proof algorithm for any problem instance.","The proof mainly relies on the single-agent result, as well as some structural properties of ETC and the novel introduction of a restarting round, which largely simplifies the analysis while maintaining the regret unchanged (up to polylogarithmic factor).","We finalize our result by proving its sublinear regret upper bound, which matches that of H-UCB."],"url":"http://arxiv.org/abs/2312.16896v1"}
{"created":"2023-12-28 08:34:45","title":"Chaurah: A Smart Raspberry Pi based Parking System","abstract":"The widespread usage of cars and other large, heavy vehicles necessitates the development of an effective parking infrastructure. Additionally, algorithms for detection and recognition of number plates are often used to identify automobiles all around the world where standardized plate sizes and fonts are enforced, making recognition an effortless task. As a result, both kinds of data can be combined to develop an intelligent parking system focuses on the technology of Automatic Number Plate Recognition (ANPR). Retrieving characters from an inputted number plate image is the sole purpose of ANPR which is a costly procedure. In this article, we propose Chaurah, a minimal cost ANPR system that relies on a Raspberry Pi 3 that was specifically created for parking facilities. The system employs a dual-stage methodology, with the first stage being an ANPR system which makes use of two convolutional neural networks (CNNs). The primary locates and recognises license plates from a vehicle image, while the secondary performs Optical Character Recognition (OCR) to identify individualized numbers from the number plate. An application built with Flutter and Firebase for database administration and license plate record comparison makes up the second component of the overall solution. The application also acts as an user-interface for the billing mechanism based on parking time duration resulting in an all-encompassing software deployment of the study.","sentences":["The widespread usage of cars and other large, heavy vehicles necessitates the development of an effective parking infrastructure.","Additionally, algorithms for detection and recognition of number plates are often used to identify automobiles all around the world where standardized plate sizes and fonts are enforced, making recognition an effortless task.","As a result, both kinds of data can be combined to develop an intelligent parking system focuses on the technology of Automatic Number Plate Recognition (ANPR).","Retrieving characters from an inputted number plate image is the sole purpose of ANPR which is a costly procedure.","In this article, we propose Chaurah, a minimal cost ANPR system that relies on a Raspberry Pi 3 that was specifically created for parking facilities.","The system employs a dual-stage methodology, with the first stage being an ANPR system which makes use of two convolutional neural networks (CNNs).","The primary locates and recognises license plates from a vehicle image, while the secondary performs Optical Character Recognition (OCR) to identify individualized numbers from the number plate.","An application built with Flutter and Firebase for database administration and license plate record comparison makes up the second component of the overall solution.","The application also acts as an user-interface for the billing mechanism based on parking time duration resulting in an all-encompassing software deployment of the study."],"url":"http://arxiv.org/abs/2312.16894v1"}
{"created":"2023-12-28 08:31:56","title":"FlexSSL : A Generic and Efficient Framework for Semi-Supervised Learning","abstract":"Semi-supervised learning holds great promise for many real-world applications, due to its ability to leverage both unlabeled and expensive labeled data. However, most semi-supervised learning algorithms still heavily rely on the limited labeled data to infer and utilize the hidden information from unlabeled data. We note that any semi-supervised learning task under the self-training paradigm also hides an auxiliary task of discriminating label observability. Jointly solving these two tasks allows full utilization of information from both labeled and unlabeled data, thus alleviating the problem of over-reliance on labeled data. This naturally leads to a new generic and efficient learning framework without the reliance on any domain-specific information, which we call FlexSSL. The key idea of FlexSSL is to construct a semi-cooperative \"game\", which forges cooperation between a main self-interested semi-supervised learning task and a companion task that infers label observability to facilitate main task training. We show with theoretical derivation of its connection to loss re-weighting on noisy labels. Through evaluations on a diverse range of tasks, we demonstrate that FlexSSL can consistently enhance the performance of semi-supervised learning algorithms.","sentences":["Semi-supervised learning holds great promise for many real-world applications, due to its ability to leverage both unlabeled and expensive labeled data.","However, most semi-supervised learning algorithms still heavily rely on the limited labeled data to infer and utilize the hidden information from unlabeled data.","We note that any semi-supervised learning task under the self-training paradigm also hides an auxiliary task of discriminating label observability.","Jointly solving these two tasks allows full utilization of information from both labeled and unlabeled data, thus alleviating the problem of over-reliance on labeled data.","This naturally leads to a new generic and efficient learning framework without the reliance on any domain-specific information, which we call FlexSSL.","The key idea of FlexSSL is to construct a semi-cooperative \"game\", which forges cooperation between a main self-interested semi-supervised learning task and a companion task that infers label observability to facilitate main task training.","We show with theoretical derivation of its connection to loss re-weighting on noisy labels.","Through evaluations on a diverse range of tasks, we demonstrate that FlexSSL can consistently enhance the performance of semi-supervised learning algorithms."],"url":"http://arxiv.org/abs/2312.16892v1"}
{"created":"2023-12-28 08:31:33","title":"DiffKG: Knowledge Graph Diffusion Model for Recommendation","abstract":"Knowledge Graphs (KGs) have emerged as invaluable resources for enriching recommendation systems by providing a wealth of factual information and capturing semantic relationships among items. Leveraging KGs can significantly enhance recommendation performance. However, not all relations within a KG are equally relevant or beneficial for the target recommendation task. In fact, certain item-entity connections may introduce noise or lack informative value, thus potentially misleading our understanding of user preferences. To bridge this research gap, we propose a novel knowledge graph diffusion model for recommendation, referred to as DiffKG. Our framework integrates a generative diffusion model with a data augmentation paradigm, enabling robust knowledge graph representation learning. This integration facilitates a better alignment between knowledge-aware item semantics and collaborative relation modeling. Moreover, we introduce a collaborative knowledge graph convolution mechanism that incorporates collaborative signals reflecting user-item interaction patterns, guiding the knowledge graph diffusion process. We conduct extensive experiments on three publicly available datasets, consistently demonstrating the superiority of our DiffKG compared to various competitive baselines. We provide the source code repository of our proposed DiffKG model at the following link: https://github.com/HKUDS/DiffKG.","sentences":["Knowledge Graphs (KGs) have emerged as invaluable resources for enriching recommendation systems by providing a wealth of factual information and capturing semantic relationships among items.","Leveraging KGs can significantly enhance recommendation performance.","However, not all relations within a KG are equally relevant or beneficial for the target recommendation task.","In fact, certain item-entity connections may introduce noise or lack informative value, thus potentially misleading our understanding of user preferences.","To bridge this research gap, we propose a novel knowledge graph diffusion model for recommendation, referred to as DiffKG.","Our framework integrates a generative diffusion model with a data augmentation paradigm, enabling robust knowledge graph representation learning.","This integration facilitates a better alignment between knowledge-aware item semantics and collaborative relation modeling.","Moreover, we introduce a collaborative knowledge graph convolution mechanism that incorporates collaborative signals reflecting user-item interaction patterns, guiding the knowledge graph diffusion process.","We conduct extensive experiments on three publicly available datasets, consistently demonstrating the superiority of our DiffKG compared to various competitive baselines.","We provide the source code repository of our proposed DiffKG model at the following link: https://github.com/HKUDS/DiffKG."],"url":"http://arxiv.org/abs/2312.16890v1"}
{"created":"2023-12-28 08:18:48","title":"Jeffreys divergence-based regularization of neural network output distribution applied to speaker recognition","abstract":"A new loss function for speaker recognition with deep neural network is proposed, based on Jeffreys Divergence. Adding this divergence to the cross-entropy loss function allows to maximize the target value of the output distribution while smoothing the non-target values. This objective function provides highly discriminative features. Beyond this effect, we propose a theoretical justification of its effectiveness and try to understand how this loss function affects the model, in particular the impact on dataset types (i.e. in-domain or out-of-domain w.r.t the training corpus). Our experiments show that Jeffreys loss consistently outperforms the state-of-the-art for speaker recognition, especially on out-of-domain data, and helps limit false alarms.","sentences":["A new loss function for speaker recognition with deep neural network is proposed, based on Jeffreys Divergence.","Adding this divergence to the cross-entropy loss function allows to maximize the target value of the output distribution while smoothing the non-target values.","This objective function provides highly discriminative features.","Beyond this effect, we propose a theoretical justification of its effectiveness and try to understand how this loss function affects the model, in particular the impact on dataset types (i.e. in-domain or out-of-domain w.r.t the training corpus).","Our experiments show that Jeffreys loss consistently outperforms the state-of-the-art for speaker recognition, especially on out-of-domain data, and helps limit false alarms."],"url":"http://arxiv.org/abs/2312.16885v1"}
{"created":"2023-12-28 06:38:52","title":"Sensor Data Simulation for Anomaly Detection of the Elderly Living Alone","abstract":"With the increase of the number of elderly people living alone around the world, there is a growing demand for sensor-based detection of anomalous behaviors. Although smart homes with ambient sensors could be useful for detecting such anomalies, there is a problem of lack of sufficient real data for developing detection algorithms. For coping with this problem, several sensor data simulators have been proposed, but they have not been able to model appropriately the long-term transitions and correlations between anomalies that exist in reality. In this paper, therefore, we propose a novel sensor data simulator that can model these factors in generation of sensor data. Anomalies considered in this study were classified into three types of \\textit{state anomalies}, \\textit{activity anomalies}, and \\textit{moving anomalies}. The simulator produces 10 years data in 100 min. including six anomalies, two for each type. Numerical evaluations show that this simulator is superior to the past simulators in the sense that it simulates well day-to-day variations of real data.","sentences":["With the increase of the number of elderly people living alone around the world, there is a growing demand for sensor-based detection of anomalous behaviors.","Although smart homes with ambient sensors could be useful for detecting such anomalies, there is a problem of lack of sufficient real data for developing detection algorithms.","For coping with this problem, several sensor data simulators have been proposed, but they have not been able to model appropriately the long-term transitions and correlations between anomalies that exist in reality.","In this paper, therefore, we propose a novel sensor data simulator that can model these factors in generation of sensor data.","Anomalies considered in this study were classified into three types of \\textit{state anomalies}, \\textit{activity anomalies}, and \\textit{moving anomalies}.","The simulator produces 10 years data in 100 min. including six anomalies, two for each type.","Numerical evaluations show that this simulator is superior to the past simulators in the sense that it simulates well day-to-day variations of real data."],"url":"http://arxiv.org/abs/2312.16852v1"}
{"created":"2023-12-28 06:04:39","title":"Dynamic Appearance Modeling of Clothed 3D Human Avatars using a Single Camera","abstract":"The appearance of a human in clothing is driven not only by the pose but also by its temporal context, i.e., motion. However, such context has been largely neglected by existing monocular human modeling methods whose neural networks often struggle to learn a video of a person with large dynamics due to the motion ambiguity, i.e., there exist numerous geometric configurations of clothes that are dependent on the context of motion even for the same pose. In this paper, we introduce a method for high-quality modeling of clothed 3D human avatars using a video of a person with dynamic movements. The main challenge comes from the lack of 3D ground truth data of geometry and its temporal correspondences. We address this challenge by introducing a novel compositional human modeling framework that takes advantage of both explicit and implicit human modeling. For explicit modeling, a neural network learns to generate point-wise shape residuals and appearance features of a 3D body model by comparing its 2D rendering results and the original images. This explicit model allows for the reconstruction of discriminative 3D motion features from UV space by encoding their temporal correspondences. For implicit modeling, an implicit network combines the appearance and 3D motion features to decode high-fidelity clothed 3D human avatars with motion-dependent geometry and texture. The experiments show that our method can generate a large variation of secondary motion in a physically plausible way.","sentences":["The appearance of a human in clothing is driven not only by the pose but also by its temporal context, i.e., motion.","However, such context has been largely neglected by existing monocular human modeling methods whose neural networks often struggle to learn a video of a person with large dynamics due to the motion ambiguity, i.e., there exist numerous geometric configurations of clothes that are dependent on the context of motion even for the same pose.","In this paper, we introduce a method for high-quality modeling of clothed 3D human avatars using a video of a person with dynamic movements.","The main challenge comes from the lack of 3D ground truth data of geometry and its temporal correspondences.","We address this challenge by introducing a novel compositional human modeling framework that takes advantage of both explicit and implicit human modeling.","For explicit modeling, a neural network learns to generate point-wise shape residuals and appearance features of a 3D body model by comparing its 2D rendering results and the original images.","This explicit model allows for the reconstruction of discriminative 3D motion features from UV space by encoding their temporal correspondences.","For implicit modeling, an implicit network combines the appearance and 3D motion features to decode high-fidelity clothed 3D human avatars with motion-dependent geometry and texture.","The experiments show that our method can generate a large variation of secondary motion in a physically plausible way."],"url":"http://arxiv.org/abs/2312.16842v1"}
{"created":"2023-12-28 06:00:55","title":"Hiding in Plain Sight: Towards the Science of Linguistic Steganography","abstract":"Covert communication (also known as steganography) is the practice of concealing a secret inside an innocuous-looking public object (cover) so that the modified public object (covert code) makes sense to everyone but only someone who knows the code can extract the secret (message). Linguistic steganography is the practice of encoding a secret message in natural language text such as spoken conversation or short public communications such as tweets.. While ad hoc methods for covert communications in specific domains exist ( JPEG images, Chinese poetry, etc), there is no general model for linguistic steganography specifically. We present a novel mathematical formalism for creating linguistic steganographic codes, with three parameters: Decodability (probability that the receiver of the coded message will decode the cover correctly), density (frequency of code words in a cover code), and detectability (probability that an attacker can tell the difference between an untampered cover compared to its steganized version). Verbal or linguistic steganography is most challenging because of its lack of artifacts to hide the secret message in. We detail a practical construction in Python of a steganographic code for Tweets using inserted words to encode hidden digits while using n-gram frequency distortion as the measure of detectability of the insertions. Using the publicly accessible Stanford Sentiment Analysis dataset we implemented the tweet steganization scheme -- a codeword (an existing word in the data set) inserted in random positions in random existing tweets to find the tweet that has the least possible n-gram distortion. We argue that this approximates KL distance in a localized manner at low cost and thus we get a linguistic steganography scheme that is both formal and practical and permits a tradeoff between codeword density and detectability of the covert message.","sentences":["Covert communication (also known as steganography) is the practice of concealing a secret inside an innocuous-looking public object (cover) so that the modified public object (covert code) makes sense to everyone but only someone who knows the code can extract the secret (message).","Linguistic steganography is the practice of encoding a secret message in natural language text such as spoken conversation or short public communications such as tweets..","While ad hoc methods for covert communications in specific domains exist ( JPEG images, Chinese poetry, etc), there is no general model for linguistic steganography specifically.","We present a novel mathematical formalism for creating linguistic steganographic codes, with three parameters: Decodability (probability that the receiver of the coded message will decode the cover correctly), density (frequency of code words in a cover code), and detectability (probability that an attacker can tell the difference between an untampered cover compared to its steganized version).","Verbal or linguistic steganography is most challenging because of its lack of artifacts to hide the secret message in.","We detail a practical construction in Python of a steganographic code for Tweets using inserted words to encode hidden digits while using n-gram frequency distortion as the measure of detectability of the insertions.","Using the publicly accessible Stanford Sentiment Analysis dataset we implemented the tweet steganization scheme -- a codeword (an existing word in the data set) inserted in random positions in random existing tweets to find the tweet that has the least possible n-gram distortion.","We argue that this approximates KL distance in a localized manner at low cost and thus we get a linguistic steganography scheme that is both formal and practical and permits a tradeoff between codeword density and detectability of the covert message."],"url":"http://arxiv.org/abs/2312.16840v1"}
{"created":"2023-12-28 05:57:51","title":"Similar but Different: A Survey of Ground Segmentation and Traversability Estimation for Terrestrial Robots","abstract":"With the increasing demand for mobile robots and autonomous vehicles, several approaches for long-term robot navigation have been proposed. Among these techniques, ground segmentation and traversability estimation play important roles in perception and path planning, respectively. Even though these two techniques appear similar, their objectives are different. Ground segmentation divides data into ground and non-ground elements; thus, it is used as a preprocessing stage to extract objects of interest by rejecting ground points. In contrast, traversability estimation identifies and comprehends areas in which robots can move safely. Nevertheless, some researchers use these terms without clear distinction, leading to misunderstanding the two concepts. Therefore, in this study, we survey related literature and clearly distinguish ground and traversable regions considering four aspects: a) maneuverability of robot platforms, b) position of a robot in the surroundings, c) subset relation of negative obstacles, and d) subset relation of deformable objects.","sentences":["With the increasing demand for mobile robots and autonomous vehicles, several approaches for long-term robot navigation have been proposed.","Among these techniques, ground segmentation and traversability estimation play important roles in perception and path planning, respectively.","Even though these two techniques appear similar, their objectives are different.","Ground segmentation divides data into ground and non-ground elements; thus, it is used as a preprocessing stage to extract objects of interest by rejecting ground points.","In contrast, traversability estimation identifies and comprehends areas in which robots can move safely.","Nevertheless, some researchers use these terms without clear distinction, leading to misunderstanding the two concepts.","Therefore, in this study, we survey related literature and clearly distinguish ground and traversable regions considering four aspects: a) maneuverability of robot platforms, b) position of a robot in the surroundings, c) subset relation of negative obstacles, and d) subset relation of deformable objects."],"url":"http://arxiv.org/abs/2312.16839v1"}
{"created":"2023-12-28 05:46:26","title":"DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaption by Combining 3D GANs and Diffusion Priors","abstract":"Text-guided domain adaption and generation of 3D-aware portraits find many applications in various fields. However, due to the lack of training data and the challenges in handling the high variety of geometry and appearance, the existing methods for these tasks suffer from issues like inflexibility, instability, and low fidelity. In this paper, we propose a novel framework DiffusionGAN3D, which boosts text-guided 3D domain adaption and generation by combining 3D GANs and diffusion priors. Specifically, we integrate the pre-trained 3D generative models (e.g., EG3D) and text-to-image diffusion models. The former provides a strong foundation for stable and high-quality avatar generation from text. And the diffusion models in turn offer powerful priors and guide the 3D generator finetuning with informative direction to achieve flexible and efficient text-guided domain adaption. To enhance the diversity in domain adaption and the generation capability in text-to-avatar, we introduce the relative distance loss and case-specific learnable triplane respectively. Besides, we design a progressive texture refinement module to improve the texture quality for both tasks above. Extensive experiments demonstrate that the proposed framework achieves excellent results in both domain adaption and text-to-avatar tasks, outperforming existing methods in terms of generation quality and efficiency. The project homepage is at https://younglbw.github.io/DiffusionGAN3D-homepage/.","sentences":["Text-guided domain adaption and generation of 3D-aware portraits find many applications in various fields.","However, due to the lack of training data and the challenges in handling the high variety of geometry and appearance, the existing methods for these tasks suffer from issues like inflexibility, instability, and low fidelity.","In this paper, we propose a novel framework DiffusionGAN3D, which boosts text-guided 3D domain adaption and generation by combining 3D GANs and diffusion priors.","Specifically, we integrate the pre-trained 3D generative models (e.g., EG3D) and text-to-image diffusion models.","The former provides a strong foundation for stable and high-quality avatar generation from text.","And the diffusion models in turn offer powerful priors and guide the 3D generator finetuning with informative direction to achieve flexible and efficient text-guided domain adaption.","To enhance the diversity in domain adaption and the generation capability in text-to-avatar, we introduce the relative distance loss and case-specific learnable triplane respectively.","Besides, we design a progressive texture refinement module to improve the texture quality for both tasks above.","Extensive experiments demonstrate that the proposed framework achieves excellent results in both domain adaption and text-to-avatar tasks, outperforming existing methods in terms of generation quality and efficiency.","The project homepage is at https://younglbw.github.io/DiffusionGAN3D-homepage/."],"url":"http://arxiv.org/abs/2312.16837v1"}
{"created":"2023-12-28 05:45:03","title":"Remixed2Remixed: Domain adaptation for speech enhancement by Noise2Noise learning with Remixing","abstract":"This paper proposes Remixed2Remixed, a domain adaptation method for speech enhancement, which adopts Noise2Noise (N2N) learning to adapt models trained on artificially generated (out-of-domain: OOD) noisy-clean pair data to better separate real-world recorded (in-domain) noisy data. The proposed method uses a teacher model trained on OOD data to acquire pseudo-in-domain speech and noise signals, which are shuffled and remixed twice in each batch to generate two bootstrapped mixtures. The student model is then trained by optimizing an N2N-based cost function computed using these two bootstrapped mixtures. As the training strategy is similar to the recently proposed RemixIT, we also investigate the effectiveness of N2N-based loss as a regularization of RemixIT. Experimental results on the CHiME-7 unsupervised domain adaptation for conversational speech enhancement (UDASE) task revealed that the proposed method outperformed the challenge baseline system, RemixIT, and reduced the blurring of performance caused by teacher models.","sentences":["This paper proposes Remixed2Remixed, a domain adaptation method for speech enhancement, which adopts Noise2Noise (N2N) learning to adapt models trained on artificially generated (out-of-domain: OOD) noisy-clean pair data to better separate real-world recorded (in-domain) noisy data.","The proposed method uses a teacher model trained on OOD data to acquire pseudo-in-domain speech and noise signals, which are shuffled and remixed twice in each batch to generate two bootstrapped mixtures.","The student model is then trained by optimizing an N2N-based cost function computed using these two bootstrapped mixtures.","As the training strategy is similar to the recently proposed RemixIT, we also investigate the effectiveness of N2N-based loss as a regularization of RemixIT.","Experimental results on the CHiME-7 unsupervised domain adaptation for conversational speech enhancement (UDASE) task revealed that the proposed method outperformed the challenge baseline system, RemixIT, and reduced the blurring of performance caused by teacher models."],"url":"http://arxiv.org/abs/2312.16836v1"}
{"created":"2023-12-28 05:39:33","title":"Hierarchical Aggregations for High-Dimensional Multiplex Graph Embedding","abstract":"We investigate the problem of multiplex graph embedding, that is, graphs in which nodes interact through multiple types of relations (dimensions). In recent years, several methods have been developed to address this problem. However, the need for more effective and specialized approaches grows with the production of graph data with diverse characteristics. In particular, real-world multiplex graphs may exhibit a high number of dimensions, making it difficult to construct a single consensus representation. Furthermore, important information can be hidden in complex latent structures scattered in multiple dimensions. To address these issues, we propose HMGE, a novel embedding method based on hierarchical aggregation for high-dimensional multiplex graphs. Hierarchical aggregation consists of learning a hierarchical combination of the graph dimensions and refining the embeddings at each hierarchy level. Non-linear combinations are computed from previous ones, thus uncovering complex information and latent structures hidden in the multiplex graph dimensions. Moreover, we leverage mutual information maximization between local patches and global summaries to train the model without supervision. This allows to capture of globally relevant information present in diverse locations of the graph. Detailed experiments on synthetic and real-world data illustrate the suitability of our approach to downstream supervised tasks, including link prediction and node classification.","sentences":["We investigate the problem of multiplex graph embedding, that is, graphs in which nodes interact through multiple types of relations (dimensions).","In recent years, several methods have been developed to address this problem.","However, the need for more effective and specialized approaches grows with the production of graph data with diverse characteristics.","In particular, real-world multiplex graphs may exhibit a high number of dimensions, making it difficult to construct a single consensus representation.","Furthermore, important information can be hidden in complex latent structures scattered in multiple dimensions.","To address these issues, we propose HMGE, a novel embedding method based on hierarchical aggregation for high-dimensional multiplex graphs.","Hierarchical aggregation consists of learning a hierarchical combination of the graph dimensions and refining the embeddings at each hierarchy level.","Non-linear combinations are computed from previous ones, thus uncovering complex information and latent structures hidden in the multiplex graph dimensions.","Moreover, we leverage mutual information maximization between local patches and global summaries to train the model without supervision.","This allows to capture of globally relevant information present in diverse locations of the graph.","Detailed experiments on synthetic and real-world data illustrate the suitability of our approach to downstream supervised tasks, including link prediction and node classification."],"url":"http://arxiv.org/abs/2312.16834v1"}
{"created":"2023-12-28 05:09:31","title":"METER: A Dynamic Concept Adaptation Framework for Online Anomaly Detection","abstract":"Real-time analytics and decision-making require online anomaly detection (OAD) to handle drifts in data streams efficiently and effectively. Unfortunately, existing approaches are often constrained by their limited detection capacity and slow adaptation to evolving data streams, inhibiting their efficacy and efficiency in handling concept drift, which is a major challenge in evolving data streams. In this paper, we introduce METER, a novel dynamic concept adaptation framework that introduces a new paradigm for OAD. METER addresses concept drift by first training a base detection model on historical data to capture recurring central concepts, and then learning to dynamically adapt to new concepts in data streams upon detecting concept drift. Particularly, METER employs a novel dynamic concept adaptation technique that leverages a hypernetwork to dynamically generate the parameter shift of the base detection model, providing a more effective and efficient solution than conventional retraining or fine-tuning approaches. Further, METER incorporates a lightweight drift detection controller, underpinned by evidential deep learning, to support robust and interpretable concept drift detection. We conduct an extensive experimental evaluation, and the results show that METER significantly outperforms existing OAD approaches in various application scenarios.","sentences":["Real-time analytics and decision-making require online anomaly detection (OAD) to handle drifts in data streams efficiently and effectively.","Unfortunately, existing approaches are often constrained by their limited detection capacity and slow adaptation to evolving data streams, inhibiting their efficacy and efficiency in handling concept drift, which is a major challenge in evolving data streams.","In this paper, we introduce METER, a novel dynamic concept adaptation framework that introduces a new paradigm for OAD.","METER addresses concept drift by first training a base detection model on historical data to capture recurring central concepts, and then learning to dynamically adapt to new concepts in data streams upon detecting concept drift.","Particularly, METER employs a novel dynamic concept adaptation technique that leverages a hypernetwork to dynamically generate the parameter shift of the base detection model, providing a more effective and efficient solution than conventional retraining or fine-tuning approaches.","Further, METER incorporates a lightweight drift detection controller, underpinned by evidential deep learning, to support robust and interpretable concept drift detection.","We conduct an extensive experimental evaluation, and the results show that METER significantly outperforms existing OAD approaches in various application scenarios."],"url":"http://arxiv.org/abs/2312.16831v1"}
{"created":"2023-12-28 05:00:19","title":"GUITAR: Gradient Pruning toward Fast Neural Ranking","abstract":"With the continuous popularity of deep learning and representation learning, fast vector search becomes a vital task in various ranking/retrieval based applications, say recommendation, ads ranking and question answering. Neural network based ranking is widely adopted due to its powerful capacity in modeling complex relationships, such as between users and items, questions and answers. However, it is usually exploited in offline or re-ranking manners for it is time-consuming in computations. Online neural network ranking--so called fast neural ranking--is considered challenging because neural network measures are usually non-convex and asymmetric. Traditional Approximate Nearest Neighbor (ANN) search which usually focuses on metric ranking measures, is not applicable to these advanced measures.   In this paper, we introduce a novel graph searching framework to accelerate the searching in the fast neural ranking problem. The proposed graph searching algorithm is bi-level: we first construct a probable candidate set; then we only evaluate the neural network measure over the probable candidate set instead of evaluating the neural network over all neighbors. Specifically, we propose a gradient-based algorithm that approximates the rank of the neural network matching score to construct the probable candidate set; and we present an angle-based heuristic procedure to adaptively identify the proper size of the probable candidate set. Empirical results on public data confirm the effectiveness of our proposed algorithms.","sentences":["With the continuous popularity of deep learning and representation learning, fast vector search becomes a vital task in various ranking/retrieval based applications, say recommendation, ads ranking and question answering.","Neural network based ranking is widely adopted due to its powerful capacity in modeling complex relationships, such as between users and items, questions and answers.","However, it is usually exploited in offline or re-ranking manners for it is time-consuming in computations.","Online neural network ranking--so called fast neural ranking--is considered challenging because neural network measures are usually non-convex and asymmetric.","Traditional Approximate Nearest Neighbor (ANN) search which usually focuses on metric ranking measures, is not applicable to these advanced measures.   ","In this paper, we introduce a novel graph searching framework to accelerate the searching in the fast neural ranking problem.","The proposed graph searching algorithm is bi-level: we first construct a probable candidate set; then we only evaluate the neural network measure over the probable candidate set instead of evaluating the neural network over all neighbors.","Specifically, we propose a gradient-based algorithm that approximates the rank of the neural network matching score to construct the probable candidate set; and we present an angle-based heuristic procedure to adaptively identify the proper size of the probable candidate set.","Empirical results on public data confirm the effectiveness of our proposed algorithms."],"url":"http://arxiv.org/abs/2312.16828v1"}
{"created":"2023-12-28 04:38:06","title":"Layer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer Level Attack and Knowledge Distillation","abstract":"Recently, serious concerns have been raised about the privacy issues related to training datasets in machine learning algorithms when including personal data. Various regulations in different countries, including the GDPR grant individuals to have personal data erased, known as 'the right to be forgotten' or 'the right to erasure'. However, there has been less research on effectively and practically deleting the requested personal data from the training set while not jeopardizing the overall machine learning performance. In this work, we propose a fast and novel machine unlearning paradigm at the layer level called layer attack unlearning, which is highly accurate and fast compared to existing machine unlearning algorithms. We introduce the Partial-PGD algorithm to locate the samples to forget efficiently. In addition, we only use the last layer of the model inspired by the Forward-Forward algorithm for unlearning process. Lastly, we use Knowledge Distillation (KD) to reliably learn the decision boundaries from the teacher using soft label information to improve accuracy performance. We conducted extensive experiments with SOTA machine unlearning models and demonstrated the effectiveness of our approach for accuracy and end-to-end unlearning performance.","sentences":["Recently, serious concerns have been raised about the privacy issues related to training datasets in machine learning algorithms when including personal data.","Various regulations in different countries, including the GDPR grant individuals to have personal data erased, known as 'the right to be forgotten' or 'the right to erasure'.","However, there has been less research on effectively and practically deleting the requested personal data from the training set while not jeopardizing the overall machine learning performance.","In this work, we propose a fast and novel machine unlearning paradigm at the layer level called layer attack unlearning, which is highly accurate and fast compared to existing machine unlearning algorithms.","We introduce the Partial-PGD algorithm to locate the samples to forget efficiently.","In addition, we only use the last layer of the model inspired by the Forward-Forward algorithm for unlearning process.","Lastly, we use Knowledge Distillation (KD) to reliably learn the decision boundaries from the teacher using soft label information to improve accuracy performance.","We conducted extensive experiments with SOTA machine unlearning models and demonstrated the effectiveness of our approach for accuracy and end-to-end unlearning performance."],"url":"http://arxiv.org/abs/2312.16823v1"}
{"created":"2023-12-28 04:31:08","title":"Catch Me if You Can: Effective Honeypot Placement in Dynamic AD Attack Graphs","abstract":"We study a Stackelberg game between an attacker and a defender on large Active Directory (AD) attack graphs where the defender employs a set of honeypots to stop the attacker from reaching high-value targets. Contrary to existing works that focus on small and static attack graphs, AD graphs typically contain hundreds of thousands of nodes and edges and constantly change over time. We consider two types of attackers: a simple attacker who cannot observe honeypots and a competent attacker who can. To jointly solve the game, we propose a mixed-integer programming (MIP) formulation. We observed that the optimal blocking plan for static graphs performs poorly in dynamic graphs. To solve the dynamic graph problem, we re-design the mixed-integer programming formulation by combining m MIP (dyMIP(m)) instances to produce a near-optimal blocking plan. Furthermore, to handle a large number of dynamic graph instances, we use a clustering algorithm to efficiently find the m-most representative graph instances for a constant m (dyMIP(m)). We prove a lower bound on the optimal blocking strategy for dynamic graphs and show that our dyMIP(m) algorithms produce close to optimal results for a range of AD graphs under realistic conditions.","sentences":["We study a Stackelberg game between an attacker and a defender on large Active Directory (AD) attack graphs where the defender employs a set of honeypots to stop the attacker from reaching high-value targets.","Contrary to existing works that focus on small and static attack graphs, AD graphs typically contain hundreds of thousands of nodes and edges and constantly change over time.","We consider two types of attackers: a simple attacker who cannot observe honeypots and a competent attacker who can.","To jointly solve the game, we propose a mixed-integer programming (MIP) formulation.","We observed that the optimal blocking plan for static graphs performs poorly in dynamic graphs.","To solve the dynamic graph problem, we re-design the mixed-integer programming formulation by combining m MIP (dyMIP(m))","instances to produce a near-optimal blocking plan.","Furthermore, to handle a large number of dynamic graph instances, we use a clustering algorithm to efficiently find the m-most representative graph instances for a constant m (dyMIP(m)).","We prove a lower bound on the optimal blocking strategy for dynamic graphs and show that our dyMIP(m) algorithms produce close to optimal results for a range of AD graphs under realistic conditions."],"url":"http://arxiv.org/abs/2312.16820v1"}
{"created":"2023-12-28 04:00:25","title":"Review of Machine Learning Approaches for Diagnostics and Prognostics of Industrial Systems Using Industrial Open Source Data","abstract":"In the field of Prognostics and Health Management (PHM), recent years have witnessed a significant surge in the application of machine learning (ML). Despite this growth, the field grapples with a lack of unified guidelines and systematic approaches for effectively implementing these ML techniques and comprehensive analysis regarding industrial open-source data across varied scenarios. To address these gaps, this paper provides a comprehensive review of machine learning approaches for diagnostics and prognostics of industrial systems using open-source datasets from PHM Data Challenge Competitions held between 2018 and 2023 by PHM Society and IEEE Reliability Society and summarizes a unified ML framework. This review systematically categorizes and scrutinizes the problems, challenges, methodologies, and advancements demonstrated in these competitions, highlighting the evolving role of both conventional machine learning and deep learning in tackling complex industrial tasks related to detection, diagnosis, assessment, and prognosis. Moreover, this paper delves into the common challenges in PHM data challenge competitions by emphasizing both data-related and model-related issues and summarizes the solutions that have been employed to address these challenges. Finally, we identify key themes and potential directions for future research, providing opportunities and prospects for ML further development in PHM.","sentences":["In the field of Prognostics and Health Management (PHM), recent years have witnessed a significant surge in the application of machine learning (ML).","Despite this growth, the field grapples with a lack of unified guidelines and systematic approaches for effectively implementing these ML techniques and comprehensive analysis regarding industrial open-source data across varied scenarios.","To address these gaps, this paper provides a comprehensive review of machine learning approaches for diagnostics and prognostics of industrial systems using open-source datasets from PHM Data Challenge Competitions held between 2018 and 2023 by PHM Society and IEEE Reliability Society and summarizes a unified ML framework.","This review systematically categorizes and scrutinizes the problems, challenges, methodologies, and advancements demonstrated in these competitions, highlighting the evolving role of both conventional machine learning and deep learning in tackling complex industrial tasks related to detection, diagnosis, assessment, and prognosis.","Moreover, this paper delves into the common challenges in PHM data challenge competitions by emphasizing both data-related and model-related issues and summarizes the solutions that have been employed to address these challenges.","Finally, we identify key themes and potential directions for future research, providing opportunities and prospects for ML further development in PHM."],"url":"http://arxiv.org/abs/2312.16810v1"}
{"created":"2023-12-28 03:35:13","title":"Efficient Interference Graph Estimation via Concurrent Flooding","abstract":"Traditional wisdom for network management allocates network resources separately for the measurement and data transmission tasks. Heavy measurement tasks may take up resources for data transmission and significantly reduce network performance. It is therefore challenging for interference graphs, deemed as incurring heavy measurement overhead, to be used in practice in wireless networks. To address this challenge in wireless sensor networks, we propose to use power as a new dimension for interference graph estimation (IGE) and integrate IGE with concurrent flooding such that IGE can be done simultaneously with flooding using the same frequency-time resources. With controlled and real-world experiments, we show that it is feasible to efficiently achieve IGE via concurrent flooding on the commercial off-the-shelf (COTS) devices by controlling the transmit powers of nodes. We believe that efficient IGE would be a key enabler for the practical use of the existing scheduling algorithms assuming known interference graphs.","sentences":["Traditional wisdom for network management allocates network resources separately for the measurement and data transmission tasks.","Heavy measurement tasks may take up resources for data transmission and significantly reduce network performance.","It is therefore challenging for interference graphs, deemed as incurring heavy measurement overhead, to be used in practice in wireless networks.","To address this challenge in wireless sensor networks, we propose to use power as a new dimension for interference graph estimation (IGE) and integrate IGE with concurrent flooding such that IGE can be done simultaneously with flooding using the same frequency-time resources.","With controlled and real-world experiments, we show that it is feasible to efficiently achieve IGE via concurrent flooding on the commercial off-the-shelf (COTS) devices by controlling the transmit powers of nodes.","We believe that efficient IGE would be a key enabler for the practical use of the existing scheduling algorithms assuming known interference graphs."],"url":"http://arxiv.org/abs/2312.16807v1"}
{"created":"2023-12-28 03:04:30","title":"Temporal Knowledge Distillation for Time-Sensitive Financial Services Applications","abstract":"Detecting anomalies has become an increasingly critical function in the financial service industry. Anomaly detection is frequently used in key compliance and risk functions such as financial crime detection fraud and cybersecurity. The dynamic nature of the underlying data patterns especially in adversarial environments like fraud detection poses serious challenges to the machine learning models. Keeping up with the rapid changes by retraining the models with the latest data patterns introduces pressures in balancing the historical and current patterns while managing the training data size. Furthermore the model retraining times raise problems in time-sensitive and high-volume deployment systems where the retraining period directly impacts the models ability to respond to ongoing attacks in a timely manner. In this study we propose a temporal knowledge distillation-based label augmentation approach (TKD) which utilizes the learning from older models to rapidly boost the latest model and effectively reduces the model retraining times to achieve improved agility. Experimental results show that the proposed approach provides advantages in retraining times while improving the model performance.","sentences":["Detecting anomalies has become an increasingly critical function in the financial service industry.","Anomaly detection is frequently used in key compliance and risk functions such as financial crime detection fraud and cybersecurity.","The dynamic nature of the underlying data patterns especially in adversarial environments like fraud detection poses serious challenges to the machine learning models.","Keeping up with the rapid changes by retraining the models with the latest data patterns introduces pressures in balancing the historical and current patterns while managing the training data size.","Furthermore the model retraining times raise problems in time-sensitive and high-volume deployment systems where the retraining period directly impacts the models ability to respond to ongoing attacks in a timely manner.","In this study we propose a temporal knowledge distillation-based label augmentation approach (TKD) which utilizes the learning from older models to rapidly boost the latest model and effectively reduces the model retraining times to achieve improved agility.","Experimental results show that the proposed approach provides advantages in retraining times while improving the model performance."],"url":"http://arxiv.org/abs/2312.16799v1"}
{"created":"2023-12-28 02:26:45","title":"L-LO: Enhancing Pose Estimation Precision via a Landmark-Based LiDAR Odometry","abstract":"The majority of existing LiDAR odometry solutions are based on simple geometric features such as points, lines or planes which cannot fully reflect the characteristics of surrounding environments. In this study, we propose a novel LiDAR odometry which effectively utilizes the overall exterior characteristics of environmental landmarks. The vehicle pose estimation is accomplished by means of two sequential pose estimation stages, namely, horizontal pose estimation and vertical pose estimation. To achieve effective landmark registration, a comprehensive index is proposed to evaluate the level of similarity between landmarks. This index takes into account two crucial aspects of landmarks, namely, dimension and shape in evaluating their similarity. To assess the performance of the proposed algorithm, we utilize the widely recognized KITTI dataset as well as experimental data collected by an unmanned ground vehicle platform. Both graphical and numerical results indicate that our algorithm outperforms leading LiDAR odometry solutions in terms of positioning accuracy.","sentences":["The majority of existing LiDAR odometry solutions are based on simple geometric features such as points, lines or planes which cannot fully reflect the characteristics of surrounding environments.","In this study, we propose a novel LiDAR odometry which effectively utilizes the overall exterior characteristics of environmental landmarks.","The vehicle pose estimation is accomplished by means of two sequential pose estimation stages, namely, horizontal pose estimation and vertical pose estimation.","To achieve effective landmark registration, a comprehensive index is proposed to evaluate the level of similarity between landmarks.","This index takes into account two crucial aspects of landmarks, namely, dimension and shape in evaluating their similarity.","To assess the performance of the proposed algorithm, we utilize the widely recognized KITTI dataset as well as experimental data collected by an unmanned ground vehicle platform.","Both graphical and numerical results indicate that our algorithm outperforms leading LiDAR odometry solutions in terms of positioning accuracy."],"url":"http://arxiv.org/abs/2312.16787v1"}
{"created":"2023-12-28 02:11:55","title":"Study of Adaptive LLR-based AP selection for Grant-Free Random Access in Cell-Free Networks","abstract":"This paper presents an iterative detection and decoding scheme along with an adaptive strategy to improve the selection of access points (APs) in a grant-free uplink cell-free scenario. With the requirement for the APs to have low-computational power in mind, we introduce a low-complexity scheme for local activity and data detection. At the central processing unit (CPU) level, we propose an adaptive technique based on local log-likelihood ratios (LLRs) to select the list of APs that should be considered for each device. Simulation results show that the proposed LLRs-based APs selection scheme outperforms the existing techniques in the literature in terms of bit error rate (BER) while requiring comparable fronthaul load.","sentences":["This paper presents an iterative detection and decoding scheme along with an adaptive strategy to improve the selection of access points (APs) in a grant-free uplink cell-free scenario.","With the requirement for the APs to have low-computational power in mind, we introduce a low-complexity scheme for local activity and data detection.","At the central processing unit (CPU) level, we propose an adaptive technique based on local log-likelihood ratios (LLRs) to select the list of APs that should be considered for each device.","Simulation results show that the proposed LLRs-based APs selection scheme outperforms the existing techniques in the literature in terms of bit error rate (BER) while requiring comparable fronthaul load."],"url":"http://arxiv.org/abs/2312.16781v1"}
{"created":"2023-12-28 00:25:12","title":"Graph Neural Networks for Antisocial Behavior Detection on Twitter","abstract":"Social media resurgence of antisocial behavior has exerted a downward spiral on stereotypical beliefs, and hateful comments towards individuals and social groups, as well as false or distorted news. The advances in graph neural networks employed on massive quantities of graph-structured data raise high hopes for the future of mediating communication on social media platforms. An approach based on graph convolutional data was employed to better capture the dependencies between the heterogeneous types of data.   Utilizing past and present experiences on the topic, we proposed and evaluated a graph-based approach for antisocial behavior detection, with general applicability that is both language- and context-independent. In this research, we carried out an experimental validation of our graph-based approach on several PAN datasets provided as part of their shared tasks, that enable the discussion of the results obtained by the proposed solution.","sentences":["Social media resurgence of antisocial behavior has exerted a downward spiral on stereotypical beliefs, and hateful comments towards individuals and social groups, as well as false or distorted news.","The advances in graph neural networks employed on massive quantities of graph-structured data raise high hopes for the future of mediating communication on social media platforms.","An approach based on graph convolutional data was employed to better capture the dependencies between the heterogeneous types of data.   ","Utilizing past and present experiences on the topic, we proposed and evaluated a graph-based approach for antisocial behavior detection, with general applicability that is both language- and context-independent.","In this research, we carried out an experimental validation of our graph-based approach on several PAN datasets provided as part of their shared tasks, that enable the discussion of the results obtained by the proposed solution."],"url":"http://arxiv.org/abs/2312.16755v1"}
{"created":"2023-12-27 22:34:55","title":"Flock: A Low-Cost Streaming Query Engine on FaaS Platforms","abstract":"In this paper, we present Flock, a cloud-native streaming query engine that leverages the on-demand elasticity of Function-as-a-Service (FaaS) platforms to perform real-time data analytics. Traditional server-centric deployments often suffer from resource under- or over-provisioning, leading to resource wastage or performance degradation. Flock addresses these issues by providing more fine-grained elasticity that can dynamically match the per-query basis with continuous scaling, and its billing methods are more fine-grained with millisecond granularity, making it a low-cost solution for stream processing. Our approach, payload invocation, eliminates the need for external storage services and eliminates the requirement for a query coordinator in the data architecture. Our evaluation shows that Flock significantly outperforms state-of-the-art systems in terms of cost, especially on ARM processors, making it a promising solution for real-time data analytics on FaaS platforms.","sentences":["In this paper, we present Flock, a cloud-native streaming query engine that leverages the on-demand elasticity of Function-as-a-Service (FaaS) platforms to perform real-time data analytics.","Traditional server-centric deployments often suffer from resource under- or over-provisioning, leading to resource wastage or performance degradation.","Flock addresses these issues by providing more fine-grained elasticity that can dynamically match the per-query basis with continuous scaling, and its billing methods are more fine-grained with millisecond granularity, making it a low-cost solution for stream processing.","Our approach, payload invocation, eliminates the need for external storage services and eliminates the requirement for a query coordinator in the data architecture.","Our evaluation shows that Flock significantly outperforms state-of-the-art systems in terms of cost, especially on ARM processors, making it a promising solution for real-time data analytics on FaaS platforms."],"url":"http://arxiv.org/abs/2312.16735v1"}
{"created":"2023-12-27 21:22:43","title":"A pipeline for multiple orange detection and tracking with 3-D fruit relocalization and neural-net based yield regression in commercial citrus orchards","abstract":"Traditionally, sweet orange crop forecasting has involved manually counting fruits from numerous trees, which is a labor-intensive process. Automatic systems for fruit counting, based on proximal imaging, computer vision, and machine learning, have been considered a promising alternative or complement to manual counting. These systems require data association components that prevent multiple counting of the same fruit observed in different images. However, there is a lack of work evaluating the accuracy of multiple fruit counting, especially considering (i) occluded and re-entering green fruits on leafy trees, and (ii) counting ground-truth data measured in the crop field. We propose a non-invasive alternative that utilizes fruit counting from videos, implemented as a pipeline. Firstly, we employ CNNs for the detection of visible fruits. Inter-frame association techniques are then applied to track the fruits across frames. To handle occluded and re-appeared fruit, we introduce a relocalization component that employs 3-D estimation of fruit locations. Finally, a neural network regressor is utilized to estimate the total number of fruit, integrating image-based fruit counting with other tree data such as crop variety and tree size. The results demonstrate that the performance of our approach is closely tied to the quality of the field-collected videos. By ensuring that at least 30% of the fruit is accurately detected, tracked, and counted, our yield regressor achieves an impressive coefficient of determination of 0.85. To the best of our knowledge, this study represents one of the few endeavors in fruit estimation that incorporates manual fruit counting as a reference point for evaluation. We also introduce annotated datasets for multiple orange tracking (MOrangeT) and detection (OranDet), publicly available to foster the development of novel methods for image-based fruit counting.","sentences":["Traditionally, sweet orange crop forecasting has involved manually counting fruits from numerous trees, which is a labor-intensive process.","Automatic systems for fruit counting, based on proximal imaging, computer vision, and machine learning, have been considered a promising alternative or complement to manual counting.","These systems require data association components that prevent multiple counting of the same fruit observed in different images.","However, there is a lack of work evaluating the accuracy of multiple fruit counting, especially considering (i) occluded and re-entering green fruits on leafy trees, and (ii) counting ground-truth data measured in the crop field.","We propose a non-invasive alternative that utilizes fruit counting from videos, implemented as a pipeline.","Firstly, we employ CNNs for the detection of visible fruits.","Inter-frame association techniques are then applied to track the fruits across frames.","To handle occluded and re-appeared fruit, we introduce a relocalization component that employs 3-D estimation of fruit locations.","Finally, a neural network regressor is utilized to estimate the total number of fruit, integrating image-based fruit counting with other tree data such as crop variety and tree size.","The results demonstrate that the performance of our approach is closely tied to the quality of the field-collected videos.","By ensuring that at least 30% of the fruit is accurately detected, tracked, and counted, our yield regressor achieves an impressive coefficient of determination of 0.85.","To the best of our knowledge, this study represents one of the few endeavors in fruit estimation that incorporates manual fruit counting as a reference point for evaluation.","We also introduce annotated datasets for multiple orange tracking (MOrangeT) and detection (OranDet), publicly available to foster the development of novel methods for image-based fruit counting."],"url":"http://arxiv.org/abs/2312.16724v1"}
{"created":"2023-12-27 20:56:55","title":"Landslide Detection and Segmentation Using Remote Sensing Images and Deep Neural Network","abstract":"Knowledge about historic landslide event occurrence is important for supporting disaster risk reduction strategies. Building upon findings from 2022 Landslide4Sense Competition, we propose a deep neural network based system for landslide detection and segmentation from multisource remote sensing image input. We use a U-Net trained with Cross Entropy loss as baseline model. We then improve the U-Net baseline model by leveraging a wide range of deep learning techniques. In particular, we conduct feature engineering by generating new band data from the original bands, which helps to enhance the quality of remote sensing image input. Regarding the network architecture, we replace traditional convolutional layers in the U-Net baseline by a residual-convolutional layer. We also propose an attention layer which leverages the multi-head attention scheme. Additionally, we generate multiple output masks with three different resolutions, which creates an ensemble of three outputs in the inference process to enhance the performance. Finally, we propose a combined loss function which leverages Focal loss and IoU loss to train the network. Our experiments on the development set of the Landslide4Sense challenge achieve an F1 score and an mIoU score of 84.07 and 76.07, respectively. Our best model setup outperforms the challenge baseline and the proposed U-Net baseline, improving the F1 score/mIoU score by 6.8/7.4 and 10.5/8.8, respectively.","sentences":["Knowledge about historic landslide event occurrence is important for supporting disaster risk reduction strategies.","Building upon findings from 2022 Landslide4Sense Competition, we propose a deep neural network based system for landslide detection and segmentation from multisource remote sensing image input.","We use a U-Net trained with Cross Entropy loss as baseline model.","We then improve the U-Net baseline model by leveraging a wide range of deep learning techniques.","In particular, we conduct feature engineering by generating new band data from the original bands, which helps to enhance the quality of remote sensing image input.","Regarding the network architecture, we replace traditional convolutional layers in the U-Net baseline by a residual-convolutional layer.","We also propose an attention layer which leverages the multi-head attention scheme.","Additionally, we generate multiple output masks with three different resolutions, which creates an ensemble of three outputs in the inference process to enhance the performance.","Finally, we propose a combined loss function which leverages Focal loss and IoU loss to train the network.","Our experiments on the development set of the Landslide4Sense challenge achieve an F1 score and an mIoU score of 84.07 and 76.07, respectively.","Our best model setup outperforms the challenge baseline and the proposed U-Net baseline, improving the F1 score/mIoU score by 6.8/7.4 and 10.5/8.8, respectively."],"url":"http://arxiv.org/abs/2312.16717v1"}
{"created":"2023-12-27 20:49:28","title":"Adversarial Attacks on LoRa Device Identification and Rogue Signal Detection with Deep Learning","abstract":"Low-Power Wide-Area Network (LPWAN) technologies, such as LoRa, have gained significant attention for their ability to enable long-range, low-power communication for Internet of Things (IoT) applications. However, the security of LoRa networks remains a major concern, particularly in scenarios where device identification and classification of legitimate and spoofed signals are crucial. This paper studies a deep learning framework to address these challenges, considering LoRa device identification and legitimate vs. rogue LoRa device classification tasks. A deep neural network (DNN), either a convolutional neural network (CNN) or feedforward neural network (FNN), is trained for each task by utilizing real experimental I/Q data for LoRa signals, while rogue signals are generated by using kernel density estimation (KDE) of received signals by rogue devices. Fast Gradient Sign Method (FGSM)-based adversarial attacks are considered for LoRa signal classification tasks using deep learning models. The impact of these attacks is assessed on the performance of two tasks, namely device identification and legitimate vs. rogue device classification, by utilizing separate or common perturbations against these signal classification tasks. Results presented in this paper quantify the level of transferability of adversarial attacks on different LoRa signal classification tasks as a major vulnerability and highlight the need to make IoT applications robust to adversarial attacks.","sentences":["Low-Power Wide-Area Network (LPWAN) technologies, such as LoRa, have gained significant attention for their ability to enable long-range, low-power communication for Internet of Things (IoT) applications.","However, the security of LoRa networks remains a major concern, particularly in scenarios where device identification and classification of legitimate and spoofed signals are crucial.","This paper studies a deep learning framework to address these challenges, considering LoRa device identification and legitimate vs. rogue LoRa device classification tasks.","A deep neural network (DNN), either a convolutional neural network (CNN) or feedforward neural network (FNN), is trained for each task by utilizing real experimental I/Q data for LoRa signals, while rogue signals are generated by using kernel density estimation (KDE) of received signals by rogue devices.","Fast Gradient Sign Method (FGSM)-based adversarial attacks are considered for LoRa signal classification tasks using deep learning models.","The impact of these attacks is assessed on the performance of two tasks, namely device identification and legitimate vs. rogue device classification, by utilizing separate or common perturbations against these signal classification tasks.","Results presented in this paper quantify the level of transferability of adversarial attacks on different LoRa signal classification tasks as a major vulnerability and highlight the need to make IoT applications robust to adversarial attacks."],"url":"http://arxiv.org/abs/2312.16715v1"}
{"created":"2023-12-27 20:42:40","title":"Knowledge Enhanced Conditional Imputation for Healthcare Time-series","abstract":"This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data. This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets. By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs).","sentences":["This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data.","Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data.","This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets.","By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs)."],"url":"http://arxiv.org/abs/2312.16713v1"}
{"created":"2023-12-27 20:02:40","title":"On the Granular Representation of Fuzzy Quantifier-Based Fuzzy Rough Sets","abstract":"Rough set theory is a well-known mathematical framework that can deal with inconsistent data by providing lower and upper approximations of concepts. A prominent property of these approximations is their granular representation: that is, they can be written as unions of simple sets, called granules. The latter can be identified with \"if. . . , then. . . \" rules, which form the backbone of rough set rule induction. It has been shown previously that this property can be maintained for various fuzzy rough set models, including those based on ordered weighted average (OWA) operators. In this paper, we will focus on some instances of the general class of fuzzy quantifier-based fuzzy rough sets (FQFRS). In these models, the lower and upper approximations are evaluated using binary and unary fuzzy quantifiers, respectively. One of the main targets of this study is to examine the granular representation of different models of FQFRS. The main findings reveal that Choquet-based fuzzy rough sets can be represented granularly under the same conditions as OWA-based fuzzy rough sets, whereas Sugeno-based FRS can always be represented granularly. This observation highlights the potential of these models for resolving data inconsistencies and managing noise.","sentences":["Rough set theory is a well-known mathematical framework that can deal with inconsistent data by providing lower and upper approximations of concepts.","A prominent property of these approximations is their granular representation: that is, they can be written as unions of simple sets, called granules.","The latter can be identified with \"if. . .",", then. . .","\" rules, which form the backbone of rough set rule induction.","It has been shown previously that this property can be maintained for various fuzzy rough set models, including those based on ordered weighted average (OWA) operators.","In this paper, we will focus on some instances of the general class of fuzzy quantifier-based fuzzy rough sets (FQFRS).","In these models, the lower and upper approximations are evaluated using binary and unary fuzzy quantifiers, respectively.","One of the main targets of this study is to examine the granular representation of different models of FQFRS.","The main findings reveal that Choquet-based fuzzy rough sets can be represented granularly under the same conditions as OWA-based fuzzy rough sets, whereas Sugeno-based FRS can always be represented granularly.","This observation highlights the potential of these models for resolving data inconsistencies and managing noise."],"url":"http://arxiv.org/abs/2312.16704v1"}
{"created":"2023-12-27 19:58:52","title":"Rethinking Tabular Data Understanding with Large Language Models","abstract":"Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WIKITABLEQUESTIONS, representing a substantial advancement over previous existing table processing paradigms of LLMs.","sentences":["Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area.","In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways.","We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks.","This prompts the proposal of a method for table structure normalization.","Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks.","Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WIKITABLEQUESTIONS, representing a substantial advancement over previous existing table processing paradigms of LLMs."],"url":"http://arxiv.org/abs/2312.16702v1"}
{"created":"2023-12-27 19:30:43","title":"Multi-channel Sensor Network Construction, Data Fusion and Challenges for Smart Home","abstract":"Both sensor networks and data fusion are essential foundations for developing the smart home Internet of Things (IoT) and related fields. We proposed a multi-channel sensor network construction method involving hardware, acquisition, and synchronization in the smart home environment and a smart home data fusion method (SHDFM) for multi-modal data (position, gait, voice, pose, facial expression, temperature, and humidity) generated in the smart home environment to address the configuration of a multi-channel sensor network, improve the quality and efficiency of various human activities and environmental data collection, and reduce the difficulty of multi-modal data fusion in the smart home. SHDFM contains 5 levels, with inputs and outputs as criteria to provide recommendations for multi-modal data fusion strategies in the smart home. We built a real experimental environment using the proposed method in this paper. To validate our method, we created a real experimental environment - a physical setup in a home-like scenario where the multi-channel sensor network and data fusion techniques were deployed and evaluated. The acceptance and testing results show that the proposed construction and data fusion methods can be applied to the examples with high robustness, replicability, and scalability. Besides, we discuss how smart homes with multi-channel sensor networks can support digital twins.","sentences":["Both sensor networks and data fusion are essential foundations for developing the smart home Internet of Things (IoT) and related fields.","We proposed a multi-channel sensor network construction method involving hardware, acquisition, and synchronization in the smart home environment and a smart home data fusion method (SHDFM) for multi-modal data (position, gait, voice, pose, facial expression, temperature, and humidity) generated in the smart home environment to address the configuration of a multi-channel sensor network, improve the quality and efficiency of various human activities and environmental data collection, and reduce the difficulty of multi-modal data fusion in the smart home.","SHDFM contains 5 levels, with inputs and outputs as criteria to provide recommendations for multi-modal data fusion strategies in the smart home.","We built a real experimental environment using the proposed method in this paper.","To validate our method, we created a real experimental environment - a physical setup in a home-like scenario where the multi-channel sensor network and data fusion techniques were deployed and evaluated.","The acceptance and testing results show that the proposed construction and data fusion methods can be applied to the examples with high robustness, replicability, and scalability.","Besides, we discuss how smart homes with multi-channel sensor networks can support digital twins."],"url":"http://arxiv.org/abs/2312.16697v1"}
{"created":"2023-12-27 17:58:00","title":"Computing Balanced Solutions for Large International Kidney Exchange Schemes When Cycle Length Is Unbounded","abstract":"In kidney exchange programmes (KEP) patients may swap their incompatible donors leading to cycles of kidney transplants. Nowadays, countries try to merge their national patient-donor pools leading to international KEPs (IKEPs). As shown in the literature, long-term stability of an IKEP can be achieved through a credit-based system. In each round, every country is prescribed a \"fair\" initial allocation of kidney transplants. The initial allocation, which we obtain by using solution concepts from cooperative game theory, is adjusted by incorporating credits from the previous round, yielding the target allocation. The goal is to find, in each round, an optimal solution that closely approximates this target allocation. There is a known polynomial-time algorithm for finding an optimal solution that lexicographically minimizes the country deviations from the target allocation if only $2$-cycles (matchings) are permitted. In practice, kidney swaps along longer cycles may be performed. However, the problem of computing optimal solutions for maximum cycle length $\\ell$ is NP-hard for every $\\ell\\geq 3$. This situation changes back to polynomial time once we allow unbounded cycle length. However, in contrast to the case where $\\ell=2$, we show that for $\\ell=\\infty$, lexicographical minimization is only polynomial-time solvable under additional conditions (assuming P $\\neq$ NP). Nevertheless, the fact that the optimal solutions themselves can be computed in polynomial time if $\\ell=\\infty$ still enables us to perform a large scale experimental study for showing how stability and total social welfare are affected when we set $\\ell=\\infty$ instead of $\\ell=2$.","sentences":["In kidney exchange programmes (KEP) patients may swap their incompatible donors leading to cycles of kidney transplants.","Nowadays, countries try to merge their national patient-donor pools leading to international KEPs (IKEPs).","As shown in the literature, long-term stability of an IKEP can be achieved through a credit-based system.","In each round, every country is prescribed a \"fair\" initial allocation of kidney transplants.","The initial allocation, which we obtain by using solution concepts from cooperative game theory, is adjusted by incorporating credits from the previous round, yielding the target allocation.","The goal is to find, in each round, an optimal solution that closely approximates this target allocation.","There is a known polynomial-time algorithm for finding an optimal solution that lexicographically minimizes the country deviations from the target allocation if only $2$-cycles (matchings) are permitted.","In practice, kidney swaps along longer cycles may be performed.","However, the problem of computing optimal solutions for maximum cycle length $\\ell$ is NP-hard for every $\\ell\\geq 3$.","This situation changes back to polynomial time once we allow unbounded cycle length.","However, in contrast to the case where $\\ell=2$, we show that for $\\ell=\\infty$, lexicographical minimization is only polynomial-time solvable under additional conditions (assuming P $\\neq$ NP).","Nevertheless, the fact that the optimal solutions themselves can be computed in polynomial time if $\\ell=\\infty$ still enables us to perform a large scale experimental study for showing how stability and total social welfare are affected when we set $\\ell=\\infty$ instead of $\\ell=2$."],"url":"http://arxiv.org/abs/2312.16653v1"}
{"created":"2023-12-27 17:36:32","title":"Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection","abstract":"In this paper, we study the problem of generalizable synthetic image detection, aiming to detect forgery images from diverse generative methods, e.g., GANs and diffusion models. Cutting-edge solutions start to explore the benefits of pre-trained models, and mainly follow the fixed paradigm of solely training an attached classifier, e.g., combining frozen CLIP-ViT with a learnable linear layer in UniFD. However, our analysis shows that such a fixed paradigm is prone to yield detectors with insufficient learning regarding forgery representations. We attribute the key challenge to the lack of forgery adaptation, and present a novel forgery-aware adaptive transformer approach, namely FatFormer. Based on the pre-trained vision-language spaces of CLIP, FatFormer introduces two core designs for the adaption to build generalized forgery representations. First, motivated by the fact that both image and frequency analysis are essential for synthetic image detection, we develop a forgery-aware adapter to adapt image features to discern and integrate local forgery traces within image and frequency domains. Second, we find that considering the contrastive objectives between adapted image features and text prompt embeddings, a previously overlooked aspect, results in a nontrivial generalization improvement. Accordingly, we introduce language-guided alignment to supervise the forgery adaptation with image and text prompts in FatFormer. Experiments show that, by coupling these two designs, our approach tuned on 4-class ProGAN data attains a remarkable detection performance, achieving an average of 98% accuracy to unseen GANs, and surprisingly generalizes to unseen diffusion models with 95% accuracy.","sentences":["In this paper, we study the problem of generalizable synthetic image detection, aiming to detect forgery images from diverse generative methods, e.g., GANs and diffusion models.","Cutting-edge solutions start to explore the benefits of pre-trained models, and mainly follow the fixed paradigm of solely training an attached classifier, e.g., combining frozen CLIP-ViT with a learnable linear layer in UniFD.","However, our analysis shows that such a fixed paradigm is prone to yield detectors with insufficient learning regarding forgery representations.","We attribute the key challenge to the lack of forgery adaptation, and present a novel forgery-aware adaptive transformer approach, namely FatFormer.","Based on the pre-trained vision-language spaces of CLIP, FatFormer introduces two core designs for the adaption to build generalized forgery representations.","First, motivated by the fact that both image and frequency analysis are essential for synthetic image detection, we develop a forgery-aware adapter to adapt image features to discern and integrate local forgery traces within image and frequency domains.","Second, we find that considering the contrastive objectives between adapted image features and text prompt embeddings, a previously overlooked aspect, results in a nontrivial generalization improvement.","Accordingly, we introduce language-guided alignment to supervise the forgery adaptation with image and text prompts in FatFormer.","Experiments show that, by coupling these two designs, our approach tuned on 4-class ProGAN data attains a remarkable detection performance, achieving an average of 98% accuracy to unseen GANs, and surprisingly generalizes to unseen diffusion models with 95% accuracy."],"url":"http://arxiv.org/abs/2312.16649v1"}
{"created":"2023-12-27 17:23:57","title":"LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization","abstract":"Global visual localization in LiDAR-maps, crucial for autonomous driving applications, remains largely unexplored due to the challenging issue of bridging the cross-modal heterogeneity gap. Popular multi-modal learning approach Contrastive Language-Image Pre-Training (CLIP) has popularized contrastive symmetric loss using batch construction technique by applying it to multi-modal domains of text and image. We apply this approach to the domains of 2D image and 3D LiDAR points on the task of cross-modal localization. Our method is explained as follows: A batch of N (image, LiDAR) pairs is constructed so as to predict what is the right match between N X N possible pairings across the batch by jointly training an image encoder and LiDAR encoder to learn a multi-modal embedding space. In this way, the cosine similarity between N positive pairings is maximized, whereas that between the remaining negative pairings is minimized. Finally, over the obtained similarity scores, a symmetric cross-entropy loss is optimized. To the best of our knowledge, this is the first work to apply batched loss approach to a cross-modal setting of image & LiDAR data and also to show Zero-shot transfer in a visual localization setting. We conduct extensive analyses on standard autonomous driving datasets such as KITTI and KITTI-360 datasets. Our method outperforms state-of-the-art recall@1 accuracy on the KITTI-360 dataset by 22.4%, using only perspective images, in contrast to the state-of-the-art approach, which utilizes the more informative fisheye images. Additionally, this superior performance is achieved without resorting to complex architectures. Moreover, we demonstrate the zero-shot capabilities of our model and we beat SOTA by 8% without even training on it. Furthermore, we establish the first benchmark for cross-modal localization on the KITTI dataset.","sentences":["Global visual localization in LiDAR-maps, crucial for autonomous driving applications, remains largely unexplored due to the challenging issue of bridging the cross-modal heterogeneity gap.","Popular multi-modal learning approach Contrastive Language-Image Pre-Training (CLIP) has popularized contrastive symmetric loss using batch construction technique by applying it to multi-modal domains of text and image.","We apply this approach to the domains of 2D image and 3D LiDAR points on the task of cross-modal localization.","Our method is explained as follows: A batch of N (image, LiDAR) pairs is constructed so as to predict what is the right match between N X N possible pairings across the batch by jointly training an image encoder and LiDAR encoder to learn a multi-modal embedding space.","In this way, the cosine similarity between N positive pairings is maximized, whereas that between the remaining negative pairings is minimized.","Finally, over the obtained similarity scores, a symmetric cross-entropy loss is optimized.","To the best of our knowledge, this is the first work to apply batched loss approach to a cross-modal setting of image & LiDAR data and also to show Zero-shot transfer in a visual localization setting.","We conduct extensive analyses on standard autonomous driving datasets such as KITTI and KITTI-360 datasets.","Our method outperforms state-of-the-art recall@1 accuracy on the KITTI-360 dataset by 22.4%, using only perspective images, in contrast to the state-of-the-art approach, which utilizes the more informative fisheye images.","Additionally, this superior performance is achieved without resorting to complex architectures.","Moreover, we demonstrate the zero-shot capabilities of our model and we beat SOTA by 8% without even training on it.","Furthermore, we establish the first benchmark for cross-modal localization on the KITTI dataset."],"url":"http://arxiv.org/abs/2312.16648v1"}
{"created":"2023-12-27 17:00:09","title":"Fault-Tolerant Vertical Federated Learning on Dynamic Networks","abstract":"Vertical Federated learning (VFL) is a class of FL where each client shares the same sample space but only holds a subset of the features. While VFL tackles key privacy challenges of distributed learning, it often assumes perfect hardware and communication capabilities. This assumption hinders the broad deployment of VFL, particularly on edge devices, which are heterogeneous in their in-situ capabilities and will connect/disconnect from the network over time. To address this gap, we define Internet Learning (IL) including its data splitting and network context and which puts good performance under extreme dynamic condition of clients as the primary goal. We propose VFL as a naive baseline and develop several extensions to handle the IL paradigm of learning. Furthermore, we implement new methods, propose metrics, and extensively analyze results based on simulating a sensor network. The results show that the developed methods are more robust to changes in the network than VFL baseline.","sentences":["Vertical Federated learning (VFL) is a class of FL where each client shares the same sample space but only holds a subset of the features.","While VFL tackles key privacy challenges of distributed learning, it often assumes perfect hardware and communication capabilities.","This assumption hinders the broad deployment of VFL, particularly on edge devices, which are heterogeneous in their in-situ capabilities and will connect/disconnect from the network over time.","To address this gap, we define Internet Learning (IL) including its data splitting and network context and which puts good performance under extreme dynamic condition of clients as the primary goal.","We propose VFL as a naive baseline and develop several extensions to handle the IL paradigm of learning.","Furthermore, we implement new methods, propose metrics, and extensively analyze results based on simulating a sensor network.","The results show that the developed methods are more robust to changes in the network than VFL baseline."],"url":"http://arxiv.org/abs/2312.16638v1"}
{"created":"2023-12-27 16:37:22","title":"Participatory prompting: a user-centric research method for eliciting AI assistance opportunities in knowledge workflows","abstract":"Generative AI, such as image generation models and large language models, stands to provide tremendous value to end-user programmers in creative and knowledge workflows. Current research methods struggle to engage end-users in a realistic conversation that balances the actually existing capabilities of generative AI with the open-ended nature of user workflows and the many opportunities for the application of this technology. In this work-in-progress paper, we introduce participatory prompting, a method for eliciting opportunities for generative AI in end-user workflows. The participatory prompting method combines a contextual inquiry and a researcher-mediated interaction with a generative model, which helps study participants interact with a generative model without having to develop prompting strategies of their own. We discuss the ongoing development of a study whose aim will be to identify end-user programming opportunities for generative AI in data analysis workflows.","sentences":["Generative AI, such as image generation models and large language models, stands to provide tremendous value to end-user programmers in creative and knowledge workflows.","Current research methods struggle to engage end-users in a realistic conversation that balances the actually existing capabilities of generative AI with the open-ended nature of user workflows and the many opportunities for the application of this technology.","In this work-in-progress paper, we introduce participatory prompting, a method for eliciting opportunities for generative AI in end-user workflows.","The participatory prompting method combines a contextual inquiry and a researcher-mediated interaction with a generative model, which helps study participants interact with a generative model without having to develop prompting strategies of their own.","We discuss the ongoing development of a study whose aim will be to identify end-user programming opportunities for generative AI in data analysis workflows."],"url":"http://arxiv.org/abs/2312.16633v1"}
{"created":"2023-12-27 16:22:50","title":"MIM4DD: Mutual Information Maximization for Dataset Distillation","abstract":"Dataset distillation (DD) aims to synthesize a small dataset whose test performance is comparable to a full dataset using the same model. State-of-the-art (SoTA) methods optimize synthetic datasets primarily by matching heuristic indicators extracted from two networks: one from real data and one from synthetic data (see Fig.1, Left), such as gradients and training trajectories. DD is essentially a compression problem that emphasizes maximizing the preservation of information contained in the data. We argue that well-defined metrics which measure the amount of shared information between variables in information theory are necessary for success measurement but are never considered by previous works. Thus, we introduce mutual information (MI) as the metric to quantify the shared information between the synthetic and the real datasets, and devise MIM4DD numerically maximizing the MI via a newly designed optimizable objective within a contrastive learning framework to update the synthetic dataset. Specifically, we designate the samples in different datasets that share the same labels as positive pairs and vice versa negative pairs. Then we respectively pull and push those samples in positive and negative pairs into contrastive space via minimizing NCE loss. As a result, the targeted MI can be transformed into a lower bound represented by feature maps of samples, which is numerically feasible. Experiment results show that MIM4DD can be implemented as an add-on module to existing SoTA DD methods.","sentences":["Dataset distillation (DD) aims to synthesize a small dataset whose test performance is comparable to a full dataset using the same model.","State-of-the-art (SoTA) methods optimize synthetic datasets primarily by matching heuristic indicators extracted from two networks: one from real data and one from synthetic data (see Fig.1, Left), such as gradients and training trajectories.","DD is essentially a compression problem that emphasizes maximizing the preservation of information contained in the data.","We argue that well-defined metrics which measure the amount of shared information between variables in information theory are necessary for success measurement but are never considered by previous works.","Thus, we introduce mutual information (MI) as the metric to quantify the shared information between the synthetic and the real datasets, and devise MIM4DD numerically maximizing the MI via a newly designed optimizable objective within a contrastive learning framework to update the synthetic dataset.","Specifically, we designate the samples in different datasets that share the same labels as positive pairs and vice versa negative pairs.","Then we respectively pull and push those samples in positive and negative pairs into contrastive space via minimizing NCE loss.","As a result, the targeted MI can be transformed into a lower bound represented by feature maps of samples, which is numerically feasible.","Experiment results show that MIM4DD can be implemented as an add-on module to existing SoTA DD methods."],"url":"http://arxiv.org/abs/2312.16627v1"}
{"created":"2023-12-27 16:11:07","title":"Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model","abstract":"BERT-based models have shown a remarkable ability in the Chinese Spelling Check (CSC) task recently. However, traditional BERT-based methods still suffer from two limitations. First, although previous works have identified that explicit prior knowledge like Part-Of-Speech (POS) tagging can benefit in the CSC task, they neglected the fact that spelling errors inherent in CSC data can lead to incorrect tags and therefore mislead models. Additionally, they ignored the correlation between the implicit hierarchical information encoded by BERT's intermediate layers and different linguistic phenomena. This results in sub-optimal accuracy. To alleviate the above two issues, we design a heterogeneous knowledge-infused framework to strengthen BERT-based CSC models. To incorporate explicit POS knowledge, we utilize an auxiliary task strategy driven by Gaussian mixture model. Meanwhile, to incorporate implicit hierarchical linguistic knowledge within the encoder, we propose a novel form of n-gram-based layerwise self-attention to generate a multilayer representation. Experimental results show that our proposed framework yields a stable performance boost over four strong baseline models and outperforms the previous state-of-the-art methods on two datasets.","sentences":["BERT-based models have shown a remarkable ability in the Chinese Spelling Check (CSC) task recently.","However, traditional BERT-based methods still suffer from two limitations.","First, although previous works have identified that explicit prior knowledge like Part-Of-Speech (POS) tagging can benefit in the CSC task, they neglected the fact that spelling errors inherent in CSC data can lead to incorrect tags and therefore mislead models.","Additionally, they ignored the correlation between the implicit hierarchical information encoded by BERT's intermediate layers and different linguistic phenomena.","This results in sub-optimal accuracy.","To alleviate the above two issues, we design a heterogeneous knowledge-infused framework to strengthen BERT-based CSC models.","To incorporate explicit POS knowledge, we utilize an auxiliary task strategy driven by Gaussian mixture model.","Meanwhile, to incorporate implicit hierarchical linguistic knowledge within the encoder, we propose a novel form of n-gram-based layerwise self-attention to generate a multilayer representation.","Experimental results show that our proposed framework yields a stable performance boost over four strong baseline models and outperforms the previous state-of-the-art methods on two datasets."],"url":"http://arxiv.org/abs/2312.16623v1"}
{"created":"2023-12-27 15:50:47","title":"Agnostically Learning Multi-index Models with Queries","abstract":"We study the power of query access for the task of agnostic learning under the Gaussian distribution. In the agnostic model, no assumptions are made on the labels and the goal is to compute a hypothesis that is competitive with the {\\em best-fit} function in a known class, i.e., it achieves error $\\mathrm{opt}+\\epsilon$, where $\\mathrm{opt}$ is the error of the best function in the class. We focus on a general family of Multi-Index Models (MIMs), which are $d$-variate functions that depend only on few relevant directions, i.e., have the form $g(\\mathbf{W} \\mathbf{x})$ for an unknown link function $g$ and a $k \\times d$ matrix $\\mathbf{W}$. Multi-index models cover a wide range of commonly studied function classes, including constant-depth neural networks with ReLU activations, and intersections of halfspaces.   Our main result shows that query access gives significant runtime improvements over random examples for agnostically learning MIMs. Under standard regularity assumptions for the link function (namely, bounded variation or surface area), we give an agnostic query learner for MIMs with complexity $O(k)^{\\mathrm{poly}(1/\\epsilon)} \\; \\mathrm{poly}(d) $. In contrast, algorithms that rely only on random examples inherently require $d^{\\mathrm{poly}(1/\\epsilon)}$ samples and runtime, even for the basic problem of agnostically learning a single ReLU or a halfspace.   Our algorithmic result establishes a strong computational separation between the agnostic PAC and the agnostic PAC+Query models under the Gaussian distribution. Prior to our work, no such separation was known -- even for the special case of agnostically learning a single halfspace, for which it was an open problem first posed by Feldman. Our results are enabled by a general dimension-reduction technique that leverages query access to estimate gradients of (a smoothed version of) the underlying label function.","sentences":["We study the power of query access for the task of agnostic learning under the Gaussian distribution.","In the agnostic model, no assumptions are made on the labels and the goal is to compute a hypothesis that is competitive with the {\\em best-fit} function in a known class, i.e., it achieves error $\\mathrm{opt}+\\epsilon$, where $\\mathrm{opt}$ is the error of the best function in the class.","We focus on a general family of Multi-Index Models (MIMs), which are $d$-variate functions that depend only on few relevant directions, i.e., have the form $g(\\mathbf{W} \\mathbf{x})$ for an unknown link function $g$ and a $k \\times d$ matrix $\\mathbf{W}$. Multi-index models cover a wide range of commonly studied function classes, including constant-depth neural networks with ReLU activations, and intersections of halfspaces.   ","Our main result shows that query access gives significant runtime improvements over random examples for agnostically learning MIMs.","Under standard regularity assumptions for the link function (namely, bounded variation or surface area), we give an agnostic query learner for MIMs with complexity $O(k)^{\\mathrm{poly}(1/\\epsilon)} \\; \\mathrm{poly}(d) $.","In contrast, algorithms that rely only on random examples inherently require $d^{\\mathrm{poly}(1/\\epsilon)}$ samples and runtime, even for the basic problem of agnostically learning a single ReLU or a halfspace.   ","Our algorithmic result establishes a strong computational separation between the agnostic PAC and the agnostic PAC+Query models under the Gaussian distribution.","Prior to our work, no such separation was known -- even for the special case of agnostically learning a single halfspace, for which it was an open problem first posed by Feldman.","Our results are enabled by a general dimension-reduction technique that leverages query access to estimate gradients of (a smoothed version of) the underlying label function."],"url":"http://arxiv.org/abs/2312.16616v1"}
{"created":"2023-12-27 15:36:17","title":"Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions","abstract":"In this paper, we propose the use of self-supervised pretraining on a large unlabelled data set to improve the performance of a personalized voice activity detection (VAD) model in adverse conditions. We pretrain a long short-term memory (LSTM)-encoder using the autoregressive predictive coding (APC) framework and fine-tune it for personalized VAD. We also propose a denoising variant of APC, with the goal of improving the robustness of personalized VAD. The trained models are systematically evaluated on both clean speech and speech contaminated by various types of noise at different SNR-levels and compared to a purely supervised model. Our experiments show that self-supervised pretraining not only improves performance in clean conditions, but also yields models which are more robust to adverse conditions compared to purely supervised learning.","sentences":["In this paper, we propose the use of self-supervised pretraining on a large unlabelled data set to improve the performance of a personalized voice activity detection (VAD) model in adverse conditions.","We pretrain a long short-term memory (LSTM)-encoder using the autoregressive predictive coding (APC) framework and fine-tune it for personalized VAD.","We also propose a denoising variant of APC, with the goal of improving the robustness of personalized VAD.","The trained models are systematically evaluated on both clean speech and speech contaminated by various types of noise at different SNR-levels and compared to a purely supervised model.","Our experiments show that self-supervised pretraining not only improves performance in clean conditions, but also yields models which are more robust to adverse conditions compared to purely supervised learning."],"url":"http://arxiv.org/abs/2312.16613v1"}
{"created":"2023-12-27 15:33:52","title":"Exploring intra-task relations to improve meta-learning algorithms","abstract":"Meta-learning has emerged as an effective methodology to model several real-world tasks and problems due to its extraordinary effectiveness in the low-data regime. There are many scenarios ranging from the classification of rare diseases to language modelling of uncommon languages where the availability of large datasets is rare. Similarly, for more broader scenarios like self-driving, an autonomous vehicle needs to be trained to handle every situation well. This requires training the ML model on a variety of tasks with good quality data. But often times, we find that the data distribution across various tasks is skewed, i.e.the data follows a long-tail distribution. This leads to the model performing well on some tasks and not performing so well on others leading to model robustness issues. Meta-learning has recently emerged as a potential learning paradigm which can effectively learn from one task and generalize that learning to unseen tasks. In this study, we aim to exploit external knowledge of task relations to improve training stability via effective mini-batching of tasks. We hypothesize that selecting a diverse set of tasks in a mini-batch will lead to a better estimate of the full gradient and hence will lead to a reduction of noise in training.","sentences":["Meta-learning has emerged as an effective methodology to model several real-world tasks and problems due to its extraordinary effectiveness in the low-data regime.","There are many scenarios ranging from the classification of rare diseases to language modelling of uncommon languages where the availability of large datasets is rare.","Similarly, for more broader scenarios like self-driving, an autonomous vehicle needs to be trained to handle every situation well.","This requires training the ML model on a variety of tasks with good quality data.","But often times, we find that the data distribution across various tasks is skewed, i.e.the data follows a long-tail distribution.","This leads to the model performing well on some tasks and not performing so well on others leading to model robustness issues.","Meta-learning has recently emerged as a potential learning paradigm which can effectively learn from one task and generalize that learning to unseen tasks.","In this study, we aim to exploit external knowledge of task relations to improve training stability via effective mini-batching of tasks.","We hypothesize that selecting a diverse set of tasks in a mini-batch will lead to a better estimate of the full gradient and hence will lead to a reduction of noise in training."],"url":"http://arxiv.org/abs/2312.16612v1"}
{"created":"2023-12-27 15:30:05","title":"Learning from small data sets: Patch-based regularizers in inverse problems for image reconstruction","abstract":"The solution of inverse problems is of fundamental interest in medical and astronomical imaging, geophysics as well as engineering and life sciences. Recent advances were made by using methods from machine learning, in particular deep neural networks. Most of these methods require a huge amount of (paired) data and computer capacity to train the networks, which often may not be available. Our paper addresses the issue of learning from small data sets by taking patches of very few images into account. We focus on the combination of model-based and data-driven methods by approximating just the image prior, also known as regularizer in the variational model. We review two methodically different approaches, namely optimizing the maximum log-likelihood of the patch distribution, and penalizing Wasserstein-like discrepancies of whole empirical patch distributions. From the point of view of Bayesian inverse problems, we show how we can achieve uncertainty quantification by approximating the posterior using Langevin Monte Carlo methods. We demonstrate the power of the methods in computed tomography, image super-resolution, and inpainting. Indeed, the approach provides also high-quality results in zero-shot super-resolution, where only a low-resolution image is available. The paper is accompanied by a GitHub repository containing implementations of all methods as well as data examples so that the reader can get their own insight into the performance.","sentences":["The solution of inverse problems is of fundamental interest in medical and astronomical imaging, geophysics as well as engineering and life sciences.","Recent advances were made by using methods from machine learning, in particular deep neural networks.","Most of these methods require a huge amount of (paired) data and computer capacity to train the networks, which often may not be available.","Our paper addresses the issue of learning from small data sets by taking patches of very few images into account.","We focus on the combination of model-based and data-driven methods by approximating just the image prior, also known as regularizer in the variational model.","We review two methodically different approaches, namely optimizing the maximum log-likelihood of the patch distribution, and penalizing Wasserstein-like discrepancies of whole empirical patch distributions.","From the point of view of Bayesian inverse problems, we show how we can achieve uncertainty quantification by approximating the posterior using Langevin Monte Carlo methods.","We demonstrate the power of the methods in computed tomography, image super-resolution, and inpainting.","Indeed, the approach provides also high-quality results in zero-shot super-resolution, where only a low-resolution image is available.","The paper is accompanied by a GitHub repository containing implementations of all methods as well as data examples so that the reader can get their own insight into the performance."],"url":"http://arxiv.org/abs/2312.16611v1"}
{"created":"2023-12-27 14:49:28","title":"EasyView: Bringing Performance Profiles into Integrated Development Environments","abstract":"Dynamic program analysis (also known as profiling) is well-known for its powerful capabilities of identifying performance inefficiencies in software packages. Although a large number of dynamic program analysis techniques are developed in academia and industry, very few of them are widely used by software developers in their regular software developing activities. There are three major reasons. First, the dynamic analysis tools (also known as profilers) are disjoint from the coding environments such as IDEs and editors; frequently switching focus between them significantly complicates the entire cycle of software development. Second, mastering various tools to interpret their analysis results requires substantial efforts; even worse, many tools have their own design of graphical user interfaces (GUI) for data presentation, which steepens the learning curves. Third, most existing tools expose few interfaces to support user-defined analysis, which makes the tools less customizable to fulfill diverse user demands. We develop EasyView, a general solution to integrate the interpretation and visualization of various profiling results in the coding environments, which bridges software developers with profilers to provide easy and intuitive dynamic analysis during the code development cycle. The novelty of EasyView is three-fold. First, we develop a generic data format, which enables EasyView to support mainstream profilers for different languages. Second, we develop a set of customizable schemes to analyze and visualize the profiles in intuitive ways. Third, we tightly integrate EasyView with popular coding environments, such as Microsoft Visual Studio Code, with easy code exploration and user interaction. Our evaluation shows that EasyView is able to support various profilers for different languages and provide unique insights into performance inefficiencies in different domains.","sentences":["Dynamic program analysis (also known as profiling) is well-known for its powerful capabilities of identifying performance inefficiencies in software packages.","Although a large number of dynamic program analysis techniques are developed in academia and industry, very few of them are widely used by software developers in their regular software developing activities.","There are three major reasons.","First, the dynamic analysis tools (also known as profilers) are disjoint from the coding environments such as IDEs and editors; frequently switching focus between them significantly complicates the entire cycle of software development.","Second, mastering various tools to interpret their analysis results requires substantial efforts; even worse, many tools have their own design of graphical user interfaces (GUI) for data presentation, which steepens the learning curves.","Third, most existing tools expose few interfaces to support user-defined analysis, which makes the tools less customizable to fulfill diverse user demands.","We develop EasyView, a general solution to integrate the interpretation and visualization of various profiling results in the coding environments, which bridges software developers with profilers to provide easy and intuitive dynamic analysis during the code development cycle.","The novelty of EasyView is three-fold.","First, we develop a generic data format, which enables EasyView to support mainstream profilers for different languages.","Second, we develop a set of customizable schemes to analyze and visualize the profiles in intuitive ways.","Third, we tightly integrate EasyView with popular coding environments, such as Microsoft Visual Studio Code, with easy code exploration and user interaction.","Our evaluation shows that EasyView is able to support various profilers for different languages and provide unique insights into performance inefficiencies in different domains."],"url":"http://arxiv.org/abs/2312.16598v1"}
{"created":"2023-12-27 14:01:35","title":"Multi-modality Affinity Inference for Weakly Supervised 3D Semantic Segmentation","abstract":"3D point cloud semantic segmentation has a wide range of applications. Recently, weakly supervised point cloud segmentation methods have been proposed, aiming to alleviate the expensive and laborious manual annotation process by leveraging scene-level labels. However, these methods have not effectively exploited the rich geometric information (such as shape and scale) and appearance information (such as color and texture) present in RGB-D scans. Furthermore, current approaches fail to fully leverage the point affinity that can be inferred from the feature extraction network, which is crucial for learning from weak scene-level labels. Additionally, previous work overlooks the detrimental effects of the long-tailed distribution of point cloud data in weakly supervised 3D semantic segmentation. To this end, this paper proposes a simple yet effective scene-level weakly supervised point cloud segmentation method with a newly introduced multi-modality point affinity inference module. The point affinity proposed in this paper is characterized by features from multiple modalities (e.g., point cloud and RGB), and is further refined by normalizing the classifier weights to alleviate the detrimental effects of long-tailed distribution without the need of the prior of category distribution. Extensive experiments on the ScanNet and S3DIS benchmarks verify the effectiveness of our proposed method, which outperforms the state-of-the-art by ~4% to ~6% mIoU. Codes are released at https://github.com/Sunny599/AAAI24-3DWSSG-MMA.","sentences":["3D point cloud semantic segmentation has a wide range of applications.","Recently, weakly supervised point cloud segmentation methods have been proposed, aiming to alleviate the expensive and laborious manual annotation process by leveraging scene-level labels.","However, these methods have not effectively exploited the rich geometric information (such as shape and scale) and appearance information (such as color and texture) present in RGB-D scans.","Furthermore, current approaches fail to fully leverage the point affinity that can be inferred from the feature extraction network, which is crucial for learning from weak scene-level labels.","Additionally, previous work overlooks the detrimental effects of the long-tailed distribution of point cloud data in weakly supervised 3D semantic segmentation.","To this end, this paper proposes a simple yet effective scene-level weakly supervised point cloud segmentation method with a newly introduced multi-modality point affinity inference module.","The point affinity proposed in this paper is characterized by features from multiple modalities (e.g., point cloud and RGB), and is further refined by normalizing the classifier weights to alleviate the detrimental effects of long-tailed distribution without the need of the prior of category distribution.","Extensive experiments on the ScanNet and S3DIS benchmarks verify the effectiveness of our proposed method, which outperforms the state-of-the-art by ~4% to ~6% mIoU. Codes are released at https://github.com/Sunny599/AAAI24-3DWSSG-MMA."],"url":"http://arxiv.org/abs/2312.16578v1"}
{"created":"2023-12-27 13:36:29","title":"GRSDet: Learning to Generate Local Reverse Samples for Few-shot Object Detection","abstract":"Few-shot object detection (FSOD) aims to achieve object detection only using a few novel class training data. Most of the existing methods usually adopt a transfer-learning strategy to construct the novel class distribution by transferring the base class knowledge. However, this direct way easily results in confusion between the novel class and other similar categories in the decision space. To address the problem, we propose generating local reverse samples (LRSamples) in Prototype Reference Frames to adaptively adjust the center position and boundary range of the novel class distribution to learn more discriminative novel class samples for FSOD. Firstly, we propose a Center Calibration Variance Augmentation (CCVA) module, which contains the selection rule of LRSamples, the generator of LRSamples, and augmentation on the calibrated distribution centers. Specifically, we design an intra-class feature converter (IFC) as the generator of CCVA to learn the selecting rule. By transferring the knowledge of IFC from the base training to fine-tuning, the IFC generates plentiful novel samples to calibrate the novel class distribution. Moreover, we propose a Feature Density Boundary Optimization (FDBO) module to adaptively adjust the importance of samples depending on their distance from the decision boundary. It can emphasize the importance of the high-density area of the similar class (closer decision boundary area) and reduce the weight of the low-density area of the similar class (farther decision boundary area), thus optimizing a clearer decision boundary for each category. We conduct extensive experiments to demonstrate the effectiveness of our proposed method. Our method achieves consistent improvement on the Pascal VOC and MS COCO datasets based on DeFRCN and MFDC baselines.","sentences":["Few-shot object detection (FSOD) aims to achieve object detection only using a few novel class training data.","Most of the existing methods usually adopt a transfer-learning strategy to construct the novel class distribution by transferring the base class knowledge.","However, this direct way easily results in confusion between the novel class and other similar categories in the decision space.","To address the problem, we propose generating local reverse samples (LRSamples) in Prototype Reference Frames to adaptively adjust the center position and boundary range of the novel class distribution to learn more discriminative novel class samples for FSOD.","Firstly, we propose a Center Calibration Variance Augmentation (CCVA) module, which contains the selection rule of LRSamples, the generator of LRSamples, and augmentation on the calibrated distribution centers.","Specifically, we design an intra-class feature converter (IFC) as the generator of CCVA to learn the selecting rule.","By transferring the knowledge of IFC from the base training to fine-tuning, the IFC generates plentiful novel samples to calibrate the novel class distribution.","Moreover, we propose a Feature Density Boundary Optimization (FDBO) module to adaptively adjust the importance of samples depending on their distance from the decision boundary.","It can emphasize the importance of the high-density area of the similar class (closer decision boundary area) and reduce the weight of the low-density area of the similar class (farther decision boundary area), thus optimizing a clearer decision boundary for each category.","We conduct extensive experiments to demonstrate the effectiveness of our proposed method.","Our method achieves consistent improvement on the Pascal VOC and MS COCO datasets based on DeFRCN and MFDC baselines."],"url":"http://arxiv.org/abs/2312.16571v1"}
{"created":"2023-12-27 13:04:46","title":"RDGCL: Reaction-Diffusion Graph Contrastive Learning for Recommendation","abstract":"Contrastive learning (CL) has emerged as a promising technique for improving recommender systems, addressing the challenge of data sparsity by leveraging self-supervised signals from raw data. Integration of CL with graph convolutional network (GCN)-based collaborative filterings (CFs) has been explored in recommender systems. However, current CL-based recommendation models heavily rely on low-pass filters and graph augmentations. In this paper, we propose a novel CL method for recommender systems called the reaction-diffusion graph contrastive learning model (RDGCL). We design our own GCN for CF based on both the diffusion, i.e., low-pass filter, and the reaction, i.e., high-pass filter, equations. Our proposed CL-based training occurs between reaction and diffusion-based embeddings, so there is no need for graph augmentations. Experimental evaluation on 6 benchmark datasets demonstrates that our proposed method outperforms state-of-the-art CL-based recommendation models. By enhancing recommendation accuracy and diversity, our method brings an advancement in CL for recommender systems.","sentences":["Contrastive learning (CL) has emerged as a promising technique for improving recommender systems, addressing the challenge of data sparsity by leveraging self-supervised signals from raw data.","Integration of CL with graph convolutional network (GCN)-based collaborative filterings (CFs) has been explored in recommender systems.","However, current CL-based recommendation models heavily rely on low-pass filters and graph augmentations.","In this paper, we propose a novel CL method for recommender systems called the reaction-diffusion graph contrastive learning model (RDGCL).","We design our own GCN for CF based on both the diffusion, i.e., low-pass filter, and the reaction, i.e., high-pass filter, equations.","Our proposed CL-based training occurs between reaction and diffusion-based embeddings, so there is no need for graph augmentations.","Experimental evaluation on 6 benchmark datasets demonstrates that our proposed method outperforms state-of-the-art CL-based recommendation models.","By enhancing recommendation accuracy and diversity, our method brings an advancement in CL for recommender systems."],"url":"http://arxiv.org/abs/2312.16563v1"}
{"created":"2023-12-27 12:49:27","title":"Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching","abstract":"Long-range interactions are essential for the correct description of complex systems in many scientific fields. The price to pay for including them in the calculations, however, is a dramatic increase in the overall computational costs. Recently, deep graph networks have been employed as efficient, data-driven surrogate models for predicting properties of complex systems represented as graphs. These models rely on a local and iterative message passing strategy that should, in principle, capture long-range information without explicitly modeling the corresponding interactions. In practice, most deep graph networks cannot really model long-range dependencies due to the intrinsic limitations of (synchronous) message passing, namely oversmoothing, oversquashing, and underreaching. This work proposes a general framework that learns to mitigate these limitations: within a variational inference framework, we endow message passing architectures with the ability to freely adapt their depth and filter messages along the way. With theoretical and empirical arguments, we show that this simple strategy better captures long-range interactions, by surpassing the state of the art on five node and graph prediction datasets suited for this problem. Our approach consistently improves the performances of the baselines tested on these tasks. We complement the exposition with qualitative analyses and ablations to get a deeper understanding of the framework's inner workings.","sentences":["Long-range interactions are essential for the correct description of complex systems in many scientific fields.","The price to pay for including them in the calculations, however, is a dramatic increase in the overall computational costs.","Recently, deep graph networks have been employed as efficient, data-driven surrogate models for predicting properties of complex systems represented as graphs.","These models rely on a local and iterative message passing strategy that should, in principle, capture long-range information without explicitly modeling the corresponding interactions.","In practice, most deep graph networks cannot really model long-range dependencies due to the intrinsic limitations of (synchronous) message passing, namely oversmoothing, oversquashing, and underreaching.","This work proposes a general framework that learns to mitigate these limitations: within a variational inference framework, we endow message passing architectures with the ability to freely adapt their depth and filter messages along the way.","With theoretical and empirical arguments, we show that this simple strategy better captures long-range interactions, by surpassing the state of the art on five node and graph prediction datasets suited for this problem.","Our approach consistently improves the performances of the baselines tested on these tasks.","We complement the exposition with qualitative analyses and ablations to get a deeper understanding of the framework's inner workings."],"url":"http://arxiv.org/abs/2312.16560v1"}
{"created":"2023-12-27 12:37:55","title":"A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning","abstract":"Federated learning (FL) enables multiple clients to collaboratively learn a shared model without sharing their individual data. Concerns about utility, privacy, and training efficiency in FL have garnered significant research attention. Differential privacy has emerged as a prevalent technique in FL, safeguarding the privacy of individual user data while impacting utility and training efficiency. Within Differential Privacy Federated Learning (DPFL), previous studies have primarily focused on the utility-privacy trade-off, neglecting training efficiency, which is crucial for timely completion. Moreover, differential privacy achieves privacy by introducing controlled randomness (noise) on selected clients in each communication round. Previous work has mainly examined the impact of noise level ($\\sigma$) and communication rounds ($T$) on the privacy-utility dynamic, overlooking other influential factors like the sample ratio ($q$, the proportion of selected clients). This paper systematically formulates an efficiency-constrained utility-privacy bi-objective optimization problem in DPFL, focusing on $\\sigma$, $T$, and $q$. We provide a comprehensive theoretical analysis, yielding analytical solutions for the Pareto front. Extensive empirical experiments verify the validity and efficacy of our analysis, offering valuable guidance for low-cost parameter design in DPFL.","sentences":["Federated learning (FL) enables multiple clients to collaboratively learn a shared model without sharing their individual data.","Concerns about utility, privacy, and training efficiency in FL have garnered significant research attention.","Differential privacy has emerged as a prevalent technique in FL, safeguarding the privacy of individual user data while impacting utility and training efficiency.","Within Differential Privacy Federated Learning (DPFL), previous studies have primarily focused on the utility-privacy trade-off, neglecting training efficiency, which is crucial for timely completion.","Moreover, differential privacy achieves privacy by introducing controlled randomness (noise) on selected clients in each communication round.","Previous work has mainly examined the impact of noise level ($\\sigma$) and communication rounds ($T$) on the privacy-utility dynamic, overlooking other influential factors like the sample ratio ($q$, the proportion of selected clients).","This paper systematically formulates an efficiency-constrained utility-privacy bi-objective optimization problem in DPFL, focusing on $\\sigma$, $T$, and $q$. We provide a comprehensive theoretical analysis, yielding analytical solutions for the Pareto front.","Extensive empirical experiments verify the validity and efficacy of our analysis, offering valuable guidance for low-cost parameter design in DPFL."],"url":"http://arxiv.org/abs/2312.16554v1"}
{"created":"2023-12-27 12:29:21","title":"AE-Flow: AutoEncoder Normalizing Flow","abstract":"Recently normalizing flows have been gaining traction in text-to-speech (TTS) and voice conversion (VC) due to their state-of-the-art (SOTA) performance. Normalizing flows are unsupervised generative models. In this paper, we introduce supervision to the training process of normalizing flows, without the need for parallel data. We call this training paradigm AutoEncoder Normalizing Flow (AE-Flow). It adds a reconstruction loss forcing the model to use information from the conditioning to reconstruct an audio sample. Our goal is to understand the impact of each component and find the right combination of the negative log-likelihood (NLL) and the reconstruction loss in training normalizing flows with coupling blocks. For that reason we will compare flow-based mapping model trained with: (i) NLL loss, (ii) NLL and reconstruction losses, as well as (iii) reconstruction loss only. Additionally, we compare our model with SOTA VC baseline. The models are evaluated in terms of naturalness, speaker similarity, intelligibility in many-to-many and many-to-any VC settings. The results show that the proposed training paradigm systematically improves speaker similarity and naturalness when compared to regular training methods of normalizing flows. Furthermore, we show that our method improves speaker similarity and intelligibility over the state-of-the-art.","sentences":["Recently normalizing flows have been gaining traction in text-to-speech (TTS) and voice conversion (VC) due to their state-of-the-art (SOTA) performance.","Normalizing flows are unsupervised generative models.","In this paper, we introduce supervision to the training process of normalizing flows, without the need for parallel data.","We call this training paradigm AutoEncoder Normalizing Flow (AE-Flow).","It adds a reconstruction loss forcing the model to use information from the conditioning to reconstruct an audio sample.","Our goal is to understand the impact of each component and find the right combination of the negative log-likelihood (NLL) and the reconstruction loss in training normalizing flows with coupling blocks.","For that reason we will compare flow-based mapping model trained with: (i) NLL loss, (ii) NLL and reconstruction losses, as well as (iii) reconstruction loss only.","Additionally, we compare our model with SOTA VC baseline.","The models are evaluated in terms of naturalness, speaker similarity, intelligibility in many-to-many and many-to-any VC settings.","The results show that the proposed training paradigm systematically improves speaker similarity and naturalness when compared to regular training methods of normalizing flows.","Furthermore, we show that our method improves speaker similarity and intelligibility over the state-of-the-art."],"url":"http://arxiv.org/abs/2312.16552v1"}
{"created":"2023-12-27 12:20:12","title":"How Robust are LLMs to In-Context Majority Label Bias?","abstract":"In the In-Context Learning (ICL) setup, various forms of label biases can manifest. One such manifestation is majority label bias, which arises when the distribution of labeled examples in the in-context samples is skewed towards one or more specific classes making Large Language Models (LLMs) more prone to predict those labels. Such discrepancies can arise from various factors, including logistical constraints, inherent biases in data collection methods, limited access to diverse data sources, etc. which are unavoidable in a real-world industry setup. In this work, we study the robustness of in-context learning in LLMs to shifts that occur due to majority label bias within the purview of text classification tasks. Prior works have shown that in-context learning with LLMs is susceptible to such biases. In our study, we go one level deeper and show that the robustness boundary varies widely for different models and tasks, with certain LLMs being highly robust (~90%) to majority label bias. Additionally, our findings also highlight the impact of model size and the richness of instructional prompts contributing towards model robustness. We restrict our study to only publicly available open-source models to ensure transparency and reproducibility.","sentences":["In the In-Context Learning (ICL) setup, various forms of label biases can manifest.","One such manifestation is majority label bias, which arises when the distribution of labeled examples in the in-context samples is skewed towards one or more specific classes making Large Language Models (LLMs) more prone to predict those labels.","Such discrepancies can arise from various factors, including logistical constraints, inherent biases in data collection methods, limited access to diverse data sources, etc. which are unavoidable in a real-world industry setup.","In this work, we study the robustness of in-context learning in LLMs to shifts that occur due to majority label bias within the purview of text classification tasks.","Prior works have shown that in-context learning with LLMs is susceptible to such biases.","In our study, we go one level deeper and show that the robustness boundary varies widely for different models and tasks, with certain LLMs being highly robust (~90%) to majority label bias.","Additionally, our findings also highlight the impact of model size and the richness of instructional prompts contributing towards model robustness.","We restrict our study to only publicly available open-source models to ensure transparency and reproducibility."],"url":"http://arxiv.org/abs/2312.16549v1"}
{"created":"2023-12-27 12:17:59","title":"FreqyWM: Frequency Watermarking for the New Data Economy","abstract":"We present a novel technique for modulating the appearance frequency of a few tokens within a dataset for encoding an invisible watermark that can be used to protect ownership rights upon data. We develop optimal as well as fast heuristic algorithms for creating and verifying such watermarks. We also demonstrate the robustness of our technique against various attacks and derive analytical bounds for the false positive probability of erroneously detecting a watermark on a dataset that does not carry it. Our technique is applicable to both single dimensional and multidimensional datasets, is independent of token type, allows for a fine control of the introduced distortion, and can be used in a variety of use cases that involve buying and selling data in contemporary data marketplaces.","sentences":["We present a novel technique for modulating the appearance frequency of a few tokens within a dataset for encoding an invisible watermark that can be used to protect ownership rights upon data.","We develop optimal as well as fast heuristic algorithms for creating and verifying such watermarks.","We also demonstrate the robustness of our technique against various attacks and derive analytical bounds for the false positive probability of erroneously detecting a watermark on a dataset that does not carry it.","Our technique is applicable to both single dimensional and multidimensional datasets, is independent of token type, allows for a fine control of the introduced distortion, and can be used in a variety of use cases that involve buying and selling data in contemporary data marketplaces."],"url":"http://arxiv.org/abs/2312.16547v1"}
{"created":"2023-12-27 11:04:13","title":"Mapping bibliographic metadata collections: the case of OpenCitations Meta and OpenAlex","abstract":"This study describes the methodology and analyses the results of the process of mapping entities between two large open bibliographic metadata collections, OpenCitations Meta and OpenAlex. The primary objective of this mapping is to integrate OpenAlex internal identifiers into the existing metadata of bibliographic resources in OpenCitations Meta, thereby interlinking and aligning these collections. Furthermore, analysing the output of the mapping provides a unique perspective on the consistency and accuracy of bibliographic metadata, offering a valuable tool for identifying potential inconsistencies in the processed data.","sentences":["This study describes the methodology and analyses the results of the process of mapping entities between two large open bibliographic metadata collections, OpenCitations Meta and OpenAlex.","The primary objective of this mapping is to integrate OpenAlex internal identifiers into the existing metadata of bibliographic resources in OpenCitations Meta, thereby interlinking and aligning these collections.","Furthermore, analysing the output of the mapping provides a unique perspective on the consistency and accuracy of bibliographic metadata, offering a valuable tool for identifying potential inconsistencies in the processed data."],"url":"http://arxiv.org/abs/2312.16523v1"}
{"created":"2023-12-27 10:41:18","title":"S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational Question Answering","abstract":"Supplying data augmentation to conversational question answering (CQA) can effectively improve model performance. However, there is less improvement from single-turn datasets in CQA due to the distribution gap between single-turn and multi-turn datasets. On the other hand, while numerous single-turn datasets are available, we have not utilized them effectively. To solve this problem, we propose a novel method to convert single-turn datasets to multi-turn datasets. The proposed method consists of three parts, namely, a QA pair Generator, a QA pair Reassembler, and a question Rewriter. Given a sample consisting of context and single-turn QA pairs, the Generator obtains candidate QA pairs and a knowledge graph based on the context. The Reassembler utilizes the knowledge graph to get sequential QA pairs, and the Rewriter rewrites questions from a conversational perspective to obtain a multi-turn dataset S2M. Our experiments show that our method can synthesize effective training resources for CQA. Notably, S2M ranks 1st place on the QuAC leaderboard at the time of submission (Aug 24th, 2022).","sentences":["Supplying data augmentation to conversational question answering (CQA) can effectively improve model performance.","However, there is less improvement from single-turn datasets in CQA due to the distribution gap between single-turn and multi-turn datasets.","On the other hand, while numerous single-turn datasets are available, we have not utilized them effectively.","To solve this problem, we propose a novel method to convert single-turn datasets to multi-turn datasets.","The proposed method consists of three parts, namely, a QA pair Generator, a QA pair Reassembler, and a question Rewriter.","Given a sample consisting of context and single-turn QA pairs, the Generator obtains candidate QA pairs and a knowledge graph based on the context.","The Reassembler utilizes the knowledge graph to get sequential QA pairs, and the Rewriter rewrites questions from a conversational perspective to obtain a multi-turn dataset","S2M. Our experiments show that our method can synthesize effective training resources for CQA.","Notably, S2M ranks 1st place on the QuAC leaderboard at the time of submission (Aug 24th, 2022)."],"url":"http://arxiv.org/abs/2312.16511v1"}
{"created":"2023-12-27 10:07:11","title":"A Non-Uniform Low-Light Image Enhancement Method with Multi-Scale Attention Transformer and Luminance Consistency Loss","abstract":"Low-light image enhancement aims to improve the perception of images collected in dim environments and provide high-quality data support for image recognition tasks. When dealing with photos captured under non-uniform illumination, existing methods cannot adaptively extract the differentiated luminance information, which will easily cause over-exposure and under-exposure. From the perspective of unsupervised learning, we propose a multi-scale attention Transformer named MSATr, which sufficiently extracts local and global features for light balance to improve the visual quality. Specifically, we present a multi-scale window division scheme, which uses exponential sequences to adjust the window size of each layer. Within different-sized windows, the self-attention computation can be refined, ensuring the pixel-level feature processing capability of the model. For feature interaction across windows, a global transformer branch is constructed to provide comprehensive brightness perception and alleviate exposure problems. Furthermore, we propose a loop training strategy, using the diverse images generated by weighted mixing and a luminance consistency loss to improve the model's generalization ability effectively. Extensive experiments on several benchmark datasets quantitatively and qualitatively prove that our MSATr is superior to state-of-the-art low-light image enhancement methods, and the enhanced images have more natural brightness and outstanding details. The code is released at https://github.com/fang001021/MSATr.","sentences":["Low-light image enhancement aims to improve the perception of images collected in dim environments and provide high-quality data support for image recognition tasks.","When dealing with photos captured under non-uniform illumination, existing methods cannot adaptively extract the differentiated luminance information, which will easily cause over-exposure and under-exposure.","From the perspective of unsupervised learning, we propose a multi-scale attention Transformer named MSATr, which sufficiently extracts local and global features for light balance to improve the visual quality.","Specifically, we present a multi-scale window division scheme, which uses exponential sequences to adjust the window size of each layer.","Within different-sized windows, the self-attention computation can be refined, ensuring the pixel-level feature processing capability of the model.","For feature interaction across windows, a global transformer branch is constructed to provide comprehensive brightness perception and alleviate exposure problems.","Furthermore, we propose a loop training strategy, using the diverse images generated by weighted mixing and a luminance consistency loss to improve the model's generalization ability effectively.","Extensive experiments on several benchmark datasets quantitatively and qualitatively prove that our MSATr is superior to state-of-the-art low-light image enhancement methods, and the enhanced images have more natural brightness and outstanding details.","The code is released at https://github.com/fang001021/MSATr."],"url":"http://arxiv.org/abs/2312.16498v1"}
{"created":"2023-12-27 09:30:31","title":"Source Code is a Graph, Not a Sequence: A Cross-Lingual Perspective on Code Clone Detection","abstract":"Source code clone detection is the task of finding code fragments that have the same or similar functionality, but may differ in syntax or structure. This task is important for software maintenance, reuse, and quality assurance (Roy et al. 2009). However, code clone detection is challenging, as source code can be written in different languages, domains, and styles. In this paper, we argue that source code is inherently a graph, not a sequence, and that graph-based methods are more suitable for code clone detection than sequence-based methods. We compare the performance of two state-of-the-art models: CodeBERT (Feng et al. 2020), a sequence-based model, and CodeGraph (Yu et al. 2023), a graph-based model, on two benchmark data-sets: BCB (Svajlenko et al. 2014) and PoolC (PoolC no date). We show that CodeGraph outperforms CodeBERT on both data-sets, especially on cross-lingual code clones. To the best of our knowledge, this is the first work to demonstrate the superiority of graph-based methods over sequence-based methods on cross-lingual code clone detection.","sentences":["Source code clone detection is the task of finding code fragments that have the same or similar functionality, but may differ in syntax or structure.","This task is important for software maintenance, reuse, and quality assurance (Roy et al. 2009).","However, code clone detection is challenging, as source code can be written in different languages, domains, and styles.","In this paper, we argue that source code is inherently a graph, not a sequence, and that graph-based methods are more suitable for code clone detection than sequence-based methods.","We compare the performance of two state-of-the-art models: CodeBERT (Feng et al. 2020), a sequence-based model, and CodeGraph (Yu et al. 2023), a graph-based model, on two benchmark data-sets: BCB (Svajlenko et al. 2014) and PoolC (PoolC no date).","We show that CodeGraph outperforms CodeBERT on both data-sets, especially on cross-lingual code clones.","To the best of our knowledge, this is the first work to demonstrate the superiority of graph-based methods over sequence-based methods on cross-lingual code clone detection."],"url":"http://arxiv.org/abs/2312.16488v1"}
{"created":"2023-12-27 09:21:45","title":"PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion","abstract":"Current large-scale diffusion models represent a giant leap forward in conditional image synthesis, capable of interpreting diverse cues like text, human poses, and edges. However, their reliance on substantial computational resources and extensive data collection remains a bottleneck. On the other hand, the integration of existing diffusion models, each specialized for different controls and operating in unique latent spaces, poses a challenge due to incompatible image resolutions and latent space embedding structures, hindering their joint use. Addressing these constraints, we present \"PanGu-Draw\", a novel latent diffusion model designed for resource-efficient text-to-image synthesis that adeptly accommodates multiple control signals. We first propose a resource-efficient Time-Decoupling Training Strategy, which splits the monolithic text-to-image model into structure and texture generators. Each generator is trained using a regimen that maximizes data utilization and computational efficiency, cutting data preparation by 48% and reducing training resources by 51%. Secondly, we introduce \"Coop-Diffusion\", an algorithm that enables the cooperative use of various pre-trained diffusion models with different latent spaces and predefined resolutions within a unified denoising process. This allows for multi-control image synthesis at arbitrary resolutions without the necessity for additional data or retraining. Empirical validations of Pangu-Draw show its exceptional prowess in text-to-image and multi-control image generation, suggesting a promising direction for future model training efficiencies and generation versatility. The largest 5B T2I PanGu-Draw model is released on the Ascend platform. Project page: https://pangu-draw.github.io","sentences":["Current large-scale diffusion models represent a giant leap forward in conditional image synthesis, capable of interpreting diverse cues like text, human poses, and edges.","However, their reliance on substantial computational resources and extensive data collection remains a bottleneck.","On the other hand, the integration of existing diffusion models, each specialized for different controls and operating in unique latent spaces, poses a challenge due to incompatible image resolutions and latent space embedding structures, hindering their joint use.","Addressing these constraints, we present \"PanGu-Draw\", a novel latent diffusion model designed for resource-efficient text-to-image synthesis that adeptly accommodates multiple control signals.","We first propose a resource-efficient Time-Decoupling Training Strategy, which splits the monolithic text-to-image model into structure and texture generators.","Each generator is trained using a regimen that maximizes data utilization and computational efficiency, cutting data preparation by 48% and reducing training resources by 51%.","Secondly, we introduce \"Coop-Diffusion\", an algorithm that enables the cooperative use of various pre-trained diffusion models with different latent spaces and predefined resolutions within a unified denoising process.","This allows for multi-control image synthesis at arbitrary resolutions without the necessity for additional data or retraining.","Empirical validations of Pangu-Draw show its exceptional prowess in text-to-image and multi-control image generation, suggesting a promising direction for future model training efficiencies and generation versatility.","The largest 5B T2I PanGu-Draw model is released on the Ascend platform.","Project page: https://pangu-draw.github.io"],"url":"http://arxiv.org/abs/2312.16486v1"}
{"created":"2023-12-27 09:03:43","title":"Noisy Correspondence Learning with Self-Reinforcing Errors Mitigation","abstract":"Cross-modal retrieval relies on well-matched large-scale datasets that are laborious in practice. Recently, to alleviate expensive data collection, co-occurring pairs from the Internet are automatically harvested for training. However, it inevitably includes mismatched pairs, \\ie, noisy correspondences, undermining supervision reliability and degrading performance. Current methods leverage deep neural networks' memorization effect to address noisy correspondences, which overconfidently focus on \\emph{similarity-guided training with hard negatives} and suffer from self-reinforcing errors. In light of above, we introduce a novel noisy correspondence learning framework, namely \\textbf{S}elf-\\textbf{R}einforcing \\textbf{E}rrors \\textbf{M}itigation (SREM). Specifically, by viewing sample matching as classification tasks within the batch, we generate classification logits for the given sample. Instead of a single similarity score, we refine sample filtration through energy uncertainty and estimate model's sensitivity of selected clean samples using swapped classification entropy, in view of the overall prediction distribution. Additionally, we propose cross-modal biased complementary learning to leverage negative matches overlooked in hard-negative training, further improving model optimization stability and curbing self-reinforcing errors. Extensive experiments on challenging benchmarks affirm the efficacy and efficiency of SREM.","sentences":["Cross-modal retrieval relies on well-matched large-scale datasets that are laborious in practice.","Recently, to alleviate expensive data collection, co-occurring pairs from the Internet are automatically harvested for training.","However, it inevitably includes mismatched pairs, \\ie, noisy correspondences, undermining supervision reliability and degrading performance.","Current methods leverage deep neural networks' memorization effect to address noisy correspondences, which overconfidently focus on \\emph{similarity-guided training with hard negatives} and suffer from self-reinforcing errors.","In light of above, we introduce a novel noisy correspondence learning framework, namely \\textbf{S}elf-\\textbf{R}einforcing \\textbf{E}rrors \\textbf{M}itigation (SREM).","Specifically, by viewing sample matching as classification tasks within the batch, we generate classification logits for the given sample.","Instead of a single similarity score, we refine sample filtration through energy uncertainty and estimate model's sensitivity of selected clean samples using swapped classification entropy, in view of the overall prediction distribution.","Additionally, we propose cross-modal biased complementary learning to leverage negative matches overlooked in hard-negative training, further improving model optimization stability and curbing self-reinforcing errors.","Extensive experiments on challenging benchmarks affirm the efficacy and efficiency of SREM."],"url":"http://arxiv.org/abs/2312.16478v1"}
{"created":"2023-12-27 08:47:39","title":"Federated Continual Learning via Knowledge Fusion: A Survey","abstract":"Data privacy and silos are nontrivial and greatly challenging in many real-world applications. Federated learning is a decentralized approach to training models across multiple local clients without the exchange of raw data from client devices to global servers. However, existing works focus on a static data environment and ignore continual learning from streaming data with incremental tasks. Federated Continual Learning (FCL) is an emerging paradigm to address model learning in both federated and continual learning environments. The key objective of FCL is to fuse heterogeneous knowledge from different clients and retain knowledge of previous tasks while learning on new ones. In this work, we delineate federated learning and continual learning first and then discuss their integration, i.e., FCL, and particular FCL via knowledge fusion. In summary, our motivations are four-fold: we (1) raise a fundamental problem called ''spatial-temporal catastrophic forgetting'' and evaluate its impact on the performance using a well-known method called federated averaging (FedAvg), (2) integrate most of the existing FCL methods into two generic frameworks, namely synchronous FCL and asynchronous FCL, (3) categorize a large number of methods according to the mechanism involved in knowledge fusion, and finally (4) showcase an outlook on the future work of FCL.","sentences":["Data privacy and silos are nontrivial and greatly challenging in many real-world applications.","Federated learning is a decentralized approach to training models across multiple local clients without the exchange of raw data from client devices to global servers.","However, existing works focus on a static data environment and ignore continual learning from streaming data with incremental tasks.","Federated Continual Learning (FCL) is an emerging paradigm to address model learning in both federated and continual learning environments.","The key objective of FCL is to fuse heterogeneous knowledge from different clients and retain knowledge of previous tasks while learning on new ones.","In this work, we delineate federated learning and continual learning first and then discuss their integration, i.e., FCL, and particular FCL via knowledge fusion.","In summary, our motivations are four-fold: we (1) raise a fundamental problem called ''spatial-temporal catastrophic forgetting'' and evaluate its impact on the performance using a well-known method called federated averaging (FedAvg), (2) integrate most of the existing FCL methods into two generic frameworks, namely synchronous FCL and asynchronous FCL, (3) categorize a large number of methods according to the mechanism involved in knowledge fusion, and finally (4) showcase an outlook on the future work of FCL."],"url":"http://arxiv.org/abs/2312.16475v1"}
{"created":"2023-12-27 08:35:47","title":"Transfer and Alignment Network for Generalized Category Discovery","abstract":"Generalized Category Discovery is a crucial real-world task. Despite the improved performance on known categories, current methods perform poorly on novel categories. We attribute the poor performance to two reasons: biased knowledge transfer between labeled and unlabeled data and noisy representation learning on the unlabeled data. To mitigate these two issues, we propose a Transfer and Alignment Network (TAN), which incorporates two knowledge transfer mechanisms to calibrate the biased knowledge and two feature alignment mechanisms to learn discriminative features. Specifically, we model different categories with prototypes and transfer the prototypes in labeled data to correct model bias towards known categories. On the one hand, we pull instances with known categories in unlabeled data closer to these prototypes to form more compact clusters and avoid boundary overlap between known and novel categories. On the other hand, we use these prototypes to calibrate noisy prototypes estimated from unlabeled data based on category similarities, which allows for more accurate estimation of prototypes for novel categories that can be used as reliable learning targets later. After knowledge transfer, we further propose two feature alignment mechanisms to acquire both instance- and category-level knowledge from unlabeled data by aligning instance features with both augmented features and the calibrated prototypes, which can boost model performance on both known and novel categories with less noise. Experiments on three benchmark datasets show that our model outperforms SOTA methods, especially on novel categories. Theoretical analysis is provided for an in-depth understanding of our model in general. Our code and data are available at https://github.com/Lackel/TAN.","sentences":["Generalized Category Discovery is a crucial real-world task.","Despite the improved performance on known categories, current methods perform poorly on novel categories.","We attribute the poor performance to two reasons: biased knowledge transfer between labeled and unlabeled data and noisy representation learning on the unlabeled data.","To mitigate these two issues, we propose a Transfer and Alignment Network (TAN), which incorporates two knowledge transfer mechanisms to calibrate the biased knowledge and two feature alignment mechanisms to learn discriminative features.","Specifically, we model different categories with prototypes and transfer the prototypes in labeled data to correct model bias towards known categories.","On the one hand, we pull instances with known categories in unlabeled data closer to these prototypes to form more compact clusters and avoid boundary overlap between known and novel categories.","On the other hand, we use these prototypes to calibrate noisy prototypes estimated from unlabeled data based on category similarities, which allows for more accurate estimation of prototypes for novel categories that can be used as reliable learning targets later.","After knowledge transfer, we further propose two feature alignment mechanisms to acquire both instance- and category-level knowledge from unlabeled data by aligning instance features with both augmented features and the calibrated prototypes, which can boost model performance on both known and novel categories with less noise.","Experiments on three benchmark datasets show that our model outperforms SOTA methods, especially on novel categories.","Theoretical analysis is provided for an in-depth understanding of our model in general.","Our code and data are available at https://github.com/Lackel/TAN."],"url":"http://arxiv.org/abs/2312.16467v1"}
{"created":"2023-12-27 08:15:47","title":"Near-Optimal Fault Tolerance for Efficient Batch Matrix Multiplication via an Additive Combinatorics Lens","abstract":"Fault tolerance is a major concern in distributed computational settings. In the classic master-worker setting, a server (the master) needs to perform some heavy computation which it may distribute to $m$ other machines (workers) in order to speed up the time complexity. In this setting, it is crucial that the computation is made robust to failed workers, in order for the master to be able to retrieve the result of the joint computation despite failures. A prime complexity measure is thus the \\emph{recovery threshold}, which is the number of workers that the master needs to wait for in order to derive the output. This is the counterpart to the number of failed workers that it can tolerate.   In this paper, we address the fundamental and well-studied task of matrix multiplication. Specifically, our focus is on when the master needs to multiply a batch of $n$ pairs of matrices. Several coding techniques have been proven successful in reducing the recovery threshold for this task, and one approach that is also very efficient in terms of computation time is called \\emph{Rook Codes}. The previously best known recovery threshold for batch matrix multiplication using Rook Codes is $O(n^{\\log_2{3}})=O(n^{1.585})$.   Our main contribution is a lower bound proof that says that any Rook Code for batch matrix multiplication must have a recovery threshold that is at least $\\omega(n)$. Notably, we employ techniques from Additive Combinatorics in order to prove this, which may be of further interest. Moreover, we show a Rook Code that achieves a recovery threshold of $n^{1+o(1)}$, establishing a near-optimal answer to the fault tolerance of this coding scheme.","sentences":["Fault tolerance is a major concern in distributed computational settings.","In the classic master-worker setting, a server (the master) needs to perform some heavy computation which it may distribute to $m$ other machines (workers) in order to speed up the time complexity.","In this setting, it is crucial that the computation is made robust to failed workers, in order for the master to be able to retrieve the result of the joint computation despite failures.","A prime complexity measure is thus the \\emph{recovery threshold}, which is the number of workers that the master needs to wait for in order to derive the output.","This is the counterpart to the number of failed workers that it can tolerate.   ","In this paper, we address the fundamental and well-studied task of matrix multiplication.","Specifically, our focus is on when the master needs to multiply a batch of $n$ pairs of matrices.","Several coding techniques have been proven successful in reducing the recovery threshold for this task, and one approach that is also very efficient in terms of computation time is called \\emph{Rook Codes}.","The previously best known recovery threshold for batch matrix multiplication using Rook Codes is $O(n^{\\log_2{3}})=O(n^{1.585})$.   Our main contribution is a lower bound proof that says that any Rook Code for batch matrix multiplication must have a recovery threshold that is at least $\\omega(n)$. Notably, we employ techniques from Additive Combinatorics in order to prove this, which may be of further interest.","Moreover, we show a Rook Code that achieves a recovery threshold of $n^{1+o(1)}$, establishing a near-optimal answer to the fault tolerance of this coding scheme."],"url":"http://arxiv.org/abs/2312.16460v1"}
{"created":"2023-12-27 07:35:17","title":"Domain Generalization with Vital Phase Augmentation","abstract":"Deep neural networks have shown remarkable performance in image classification. However, their performance significantly deteriorates with corrupted input data. Domain generalization methods have been proposed to train robust models against out-of-distribution data. Data augmentation in the frequency domain is one of such approaches that enable a model to learn phase features to establish domain-invariant representations. This approach changes the amplitudes of the input data while preserving the phases. However, using fixed phases leads to susceptibility to phase fluctuations because amplitudes and phase fluctuations commonly occur in out-of-distribution. In this study, to address this problem, we introduce an approach using finite variation of the phases of input data rather than maintaining fixed phases. Based on the assumption that the degree of domain-invariant features varies for each phase, we propose a method to distinguish phases based on this degree. In addition, we propose a method called vital phase augmentation (VIPAug) that applies the variation to the phases differently according to the degree of domain-invariant features of given phases. The model depends more on the vital phases that contain more domain-invariant features for attaining robustness to amplitude and phase fluctuations. We present experimental evaluations of our proposed approach, which exhibited improved performance for both clean and corrupted data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100 datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet datasets. Our code is available at https://github.com/excitedkid/vipaug.","sentences":["Deep neural networks have shown remarkable performance in image classification.","However, their performance significantly deteriorates with corrupted input data.","Domain generalization methods have been proposed to train robust models against out-of-distribution data.","Data augmentation in the frequency domain is one of such approaches that enable a model to learn phase features to establish domain-invariant representations.","This approach changes the amplitudes of the input data while preserving the phases.","However, using fixed phases leads to susceptibility to phase fluctuations because amplitudes and phase fluctuations commonly occur in out-of-distribution.","In this study, to address this problem, we introduce an approach using finite variation of the phases of input data rather than maintaining fixed phases.","Based on the assumption that the degree of domain-invariant features varies for each phase, we propose a method to distinguish phases based on this degree.","In addition, we propose a method called vital phase augmentation (VIPAug) that applies the variation to the phases differently according to the degree of domain-invariant features of given phases.","The model depends more on the vital phases that contain more domain-invariant features for attaining robustness to amplitude and phase fluctuations.","We present experimental evaluations of our proposed approach, which exhibited improved performance for both clean and corrupted data.","VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100 datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet datasets.","Our code is available at https://github.com/excitedkid/vipaug."],"url":"http://arxiv.org/abs/2312.16451v1"}
{"created":"2023-12-27 07:29:52","title":"FCDNet: Frequency-Guided Complementary Dependency Modeling for Multivariate Time-Series Forecasting","abstract":"Multivariate time-series (MTS) forecasting is a challenging task in many real-world non-stationary dynamic scenarios. In addition to intra-series temporal signals, the inter-series dependency also plays a crucial role in shaping future trends. How to enable the model's awareness of dependency information has raised substantial research attention. Previous approaches have either presupposed dependency constraints based on domain knowledge or imposed them using real-time feature similarity. However, MTS data often exhibit both enduring long-term static relationships and transient short-term interactions, which mutually influence their evolving states. It is necessary to recognize and incorporate the complementary dependencies for more accurate MTS prediction. The frequency information in time series reflects the evolutionary rules behind complex temporal dynamics, and different frequency components can be used to well construct long-term and short-term interactive dependency structures between variables. To this end, we propose FCDNet, a concise yet effective framework for multivariate time-series forecasting. Specifically, FCDNet overcomes the above limitations by applying two light-weight dependency constructors to help extract long- and short-term dependency information adaptively from multi-level frequency patterns. With the growth of input variables, the number of trainable parameters in FCDNet only increases linearly, which is conducive to the model's scalability and avoids over-fitting. Additionally, adopting a frequency-based perspective can effectively mitigate the influence of noise within MTS data, which helps capture more genuine dependencies. The experimental results on six real-world datasets from multiple fields show that FCDNet significantly exceeds strong baselines, with an average improvement of 6.82% on MAE, 4.98% on RMSE, and 4.91% on MAPE.","sentences":["Multivariate time-series (MTS) forecasting is a challenging task in many real-world non-stationary dynamic scenarios.","In addition to intra-series temporal signals, the inter-series dependency also plays a crucial role in shaping future trends.","How to enable the model's awareness of dependency information has raised substantial research attention.","Previous approaches have either presupposed dependency constraints based on domain knowledge or imposed them using real-time feature similarity.","However, MTS data often exhibit both enduring long-term static relationships and transient short-term interactions, which mutually influence their evolving states.","It is necessary to recognize and incorporate the complementary dependencies for more accurate MTS prediction.","The frequency information in time series reflects the evolutionary rules behind complex temporal dynamics, and different frequency components can be used to well construct long-term and short-term interactive dependency structures between variables.","To this end, we propose FCDNet, a concise yet effective framework for multivariate time-series forecasting.","Specifically, FCDNet overcomes the above limitations by applying two light-weight dependency constructors to help extract long- and short-term dependency information adaptively from multi-level frequency patterns.","With the growth of input variables, the number of trainable parameters in FCDNet only increases linearly, which is conducive to the model's scalability and avoids over-fitting.","Additionally, adopting a frequency-based perspective can effectively mitigate the influence of noise within MTS data, which helps capture more genuine dependencies.","The experimental results on six real-world datasets from multiple fields show that FCDNet significantly exceeds strong baselines, with an average improvement of 6.82% on MAE, 4.98% on RMSE, and 4.91% on MAPE."],"url":"http://arxiv.org/abs/2312.16450v1"}
{"created":"2023-12-27 06:57:23","title":"Visual Spatial Attention and Proprioceptive Data-Driven Reinforcement Learning for Robust Peg-in-Hole Task Under Variable Conditions","abstract":"Anchor-bolt insertion is a peg-in-hole task performed in the construction field for holes in concrete. Efforts have been made to automate this task, but the variable lighting and hole surface conditions, as well as the requirements for short setup and task execution time make the automation challenging. In this study, we introduce a vision and proprioceptive data-driven robot control model for this task that is robust to challenging lighting and hole surface conditions. This model consists of a spatial attention point network (SAP) and a deep reinforcement learning (DRL) policy that are trained jointly end-to-end to control the robot. The model is trained in an offline manner, with a sample-efficient framework designed to reduce training time and minimize the reality gap when transferring the model to the physical world. Through evaluations with an industrial robot performing the task in 12 unknown holes, starting from 16 different initial positions, and under three different lighting conditions (two with misleading shadows), we demonstrate that SAP can generate relevant attention points of the image even in challenging lighting conditions. We also show that the proposed model enables task execution with higher success rate and shorter task completion time than various baselines. Due to the proposed model's high effectiveness even in severe lighting, initial positions, and hole conditions, and the offline training framework's high sample-efficiency and short training time, this approach can be easily applied to construction.","sentences":["Anchor-bolt insertion is a peg-in-hole task performed in the construction field for holes in concrete.","Efforts have been made to automate this task, but the variable lighting and hole surface conditions, as well as the requirements for short setup and task execution time make the automation challenging.","In this study, we introduce a vision and proprioceptive data-driven robot control model for this task that is robust to challenging lighting and hole surface conditions.","This model consists of a spatial attention point network (SAP) and a deep reinforcement learning (DRL) policy that are trained jointly end-to-end to control the robot.","The model is trained in an offline manner, with a sample-efficient framework designed to reduce training time and minimize the reality gap when transferring the model to the physical world.","Through evaluations with an industrial robot performing the task in 12 unknown holes, starting from 16 different initial positions, and under three different lighting conditions (two with misleading shadows), we demonstrate that SAP can generate relevant attention points of the image even in challenging lighting conditions.","We also show that the proposed model enables task execution with higher success rate and shorter task completion time than various baselines.","Due to the proposed model's high effectiveness even in severe lighting, initial positions, and hole conditions, and the offline training framework's high sample-efficiency and short training time, this approach can be easily applied to construction."],"url":"http://arxiv.org/abs/2312.16438v1"}
{"created":"2023-12-27 06:34:54","title":"Preference as Reward, Maximum Preference Optimization with Importance Sampling","abstract":"Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design a simple and intuitive off-policy preferences optimization algorithm from an importance sampling view, and add an off-policy KL-regularization term which makes KL-regularization truly effective. To simplify the learning process and save memory usage, we can generate regularization data in advance, which eliminate the needs for both reward model and reference policy in the stage of optimization.","sentences":["Preference learning is a key technology for aligning language models with human values.","Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward.","The processing of RLHF is complex, time-consuming and unstable.","Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable.","DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic.","IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy.","But IPO's pairwise loss still can't s make the KL-regularization to work.","In this paper, we design a simple and intuitive off-policy preferences optimization algorithm from an importance sampling view, and add an off-policy KL-regularization term which makes KL-regularization truly effective.","To simplify the learning process and save memory usage, we can generate regularization data in advance, which eliminate the needs for both reward model and reference policy in the stage of optimization."],"url":"http://arxiv.org/abs/2312.16430v1"}
{"created":"2023-12-27 06:31:06","title":"GAD-PVI: A General Accelerated Dynamic-Weight Particle-Based Variational Inference Framework","abstract":"Particle-based Variational Inference (ParVI) methods approximate the target distribution by iteratively evolving finite weighted particle systems. Recent advances of ParVI methods reveal the benefits of accelerated position update strategies and dynamic weight adjustment approaches. In this paper, we propose the first ParVI framework that possesses both accelerated position update and dynamical weight adjustment simultaneously, named the General Accelerated Dynamic-Weight Particle-based Variational Inference (GAD-PVI) framework. Generally, GAD-PVI simulates the semi-Hamiltonian gradient flow on a novel Information-Fisher-Rao space, which yields an additional decrease on the local functional dissipation. GAD-PVI is compatible with different dissimilarity functionals and associated smoothing approaches under three information metrics. Experiments on both synthetic and real-world data demonstrate the faster convergence and reduced approximation error of GAD-PVI methods over the state-of-the-art.","sentences":["Particle-based Variational Inference (ParVI) methods approximate the target distribution by iteratively evolving finite weighted particle systems.","Recent advances of ParVI methods reveal the benefits of accelerated position update strategies and dynamic weight adjustment approaches.","In this paper, we propose the first ParVI framework that possesses both accelerated position update and dynamical weight adjustment simultaneously, named the General Accelerated Dynamic-Weight Particle-based Variational Inference (GAD-PVI) framework.","Generally, GAD-PVI simulates the semi-Hamiltonian gradient flow on a novel Information-Fisher-Rao space, which yields an additional decrease on the local functional dissipation.","GAD-PVI is compatible with different dissimilarity functionals and associated smoothing approaches under three information metrics.","Experiments on both synthetic and real-world data demonstrate the faster convergence and reduced approximation error of GAD-PVI methods over the state-of-the-art."],"url":"http://arxiv.org/abs/2312.16429v1"}
{"created":"2023-12-27 06:15:00","title":"Soft Contrastive Learning for Time Series","abstract":"Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experiments, we demonstrate that SoftCLT consistently improves the performance in various downstream tasks including classification, semi-supervised learning, transfer learning, and anomaly detection, showing state-of-the-art performance. Code is available at this repository: https://github.com/seunghan96/softclt.","sentences":["Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way.","However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations.","To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series.","This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one.","Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps.","SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles.","In experiments, we demonstrate that SoftCLT consistently improves the performance in various downstream tasks including classification, semi-supervised learning, transfer learning, and anomaly detection, showing state-of-the-art performance.","Code is available at this repository: https://github.com/seunghan96/softclt."],"url":"http://arxiv.org/abs/2312.16424v1"}
{"created":"2023-12-27 05:35:14","title":"Refining Latent Homophilic Structures over Heterophilic Graphs for Robust Graph Convolution Networks","abstract":"Graph convolution networks (GCNs) are extensively utilized in various graph tasks to mine knowledge from spatial data. Our study marks the pioneering attempt to quantitatively investigate the GCN robustness over omnipresent heterophilic graphs for node classification. We uncover that the predominant vulnerability is caused by the structural out-of-distribution (OOD) issue. This finding motivates us to present a novel method that aims to harden GCNs by automatically learning Latent Homophilic Structures over heterophilic graphs. We term such a methodology as LHS. To elaborate, our initial step involves learning a latent structure by employing a novel self-expressive technique based on multi-node interactions. Subsequently, the structure is refined using a pairwisely constrained dual-view contrastive learning approach. We iteratively perform the above procedure, enabling a GCN model to aggregate information in a homophilic way on heterophilic graphs. Armed with such an adaptable structure, we can properly mitigate the structural OOD threats over heterophilic graphs. Experiments on various benchmarks show the effectiveness of the proposed LHS approach for robust GCNs.","sentences":["Graph convolution networks (GCNs) are extensively utilized in various graph tasks to mine knowledge from spatial data.","Our study marks the pioneering attempt to quantitatively investigate the GCN robustness over omnipresent heterophilic graphs for node classification.","We uncover that the predominant vulnerability is caused by the structural out-of-distribution (OOD) issue.","This finding motivates us to present a novel method that aims to harden GCNs by automatically learning Latent Homophilic Structures over heterophilic graphs.","We term such a methodology as LHS.","To elaborate, our initial step involves learning a latent structure by employing a novel self-expressive technique based on multi-node interactions.","Subsequently, the structure is refined using a pairwisely constrained dual-view contrastive learning approach.","We iteratively perform the above procedure, enabling a GCN model to aggregate information in a homophilic way on heterophilic graphs.","Armed with such an adaptable structure, we can properly mitigate the structural OOD threats over heterophilic graphs.","Experiments on various benchmarks show the effectiveness of the proposed LHS approach for robust GCNs."],"url":"http://arxiv.org/abs/2312.16418v1"}
{"created":"2023-12-27 05:27:22","title":"Deterministic Minimum Steiner Cut in Maximum Flow Time","abstract":"We devise a deterministic algorithm for minimum Steiner cut which uses polylogarithmic maximum flow calls and near-linear time outside of these maximum flow calls. This improves on Li and Panigrahi's (FOCS 2020) algorithm which takes $O(m^{1+\\epsilon})$ time outside of maximum flow calls. Our algorithm thus shows that deterministic minimum Steiner cut can be solved in maximum flow time up to polylogarithmic factors, given any black-box deterministic maximum flow algorithm. Our main technical contribution is a novel deterministic graph decomposition method for terminal vertices which generalizes all existing $s$-strong partitioning methods and may have future applications.","sentences":["We devise a deterministic algorithm for minimum Steiner cut which uses polylogarithmic maximum flow calls and near-linear time outside of these maximum flow calls.","This improves on Li and Panigrahi's (FOCS 2020) algorithm which takes $O(m^{1+\\epsilon})$ time outside of maximum flow calls.","Our algorithm thus shows that deterministic minimum Steiner cut can be solved in maximum flow time up to polylogarithmic factors, given any black-box deterministic maximum flow algorithm.","Our main technical contribution is a novel deterministic graph decomposition method for terminal vertices which generalizes all existing $s$-strong partitioning methods and may have future applications."],"url":"http://arxiv.org/abs/2312.16415v1"}
{"created":"2023-12-27 05:11:02","title":"Improved Approximation Coflows Scheduling Algorithms for Minimizing the Total Weighted Completion Time and Makespan in Heterogeneous Parallel Networks","abstract":"Coflow is a network abstraction used to represent communication patterns in data centers. The coflow scheduling problem encountered in large data centers is a challenging $\\mathcal{NP}$-hard problem. This paper tackles the scheduling problem of coflows with release times in heterogeneous parallel networks, which feature an architecture consisting of multiple network cores running in parallel. Two polynomial-time approximation algorithms are presented in this paper, designed to minimize the total weighted completion time and makespan in heterogeneous parallel networks, respectively. For any given $\\epsilon>0$, our proposed approximation algorithm for minimizing the total weighted completion time achieves approximation ratios of $3 + \\epsilon$ and $2 + \\epsilon$ in the cases of arbitrary and zero release times, respectively. Additionally, we introduce an approximation algorithm for minimizing the makespan, achieving an approximation ratio of $2 + \\epsilon$ for $\\epsilon>0$. Notably, these advancements surpass the previously best-known approximation ratio of $O(\\log m/ \\log \\log m)$ for both minimizing the total weighted completion time and makespan. This result also improves upon the previous approximation ratios of $6-\\frac{2}{m}$ and $5-\\frac{2}{m}$ for arbitrary and zero release times, respectively, in identical parallel networks.","sentences":["Coflow is a network abstraction used to represent communication patterns in data centers.","The coflow scheduling problem encountered in large data centers is a challenging $\\mathcal{NP}$-hard problem.","This paper tackles the scheduling problem of coflows with release times in heterogeneous parallel networks, which feature an architecture consisting of multiple network cores running in parallel.","Two polynomial-time approximation algorithms are presented in this paper, designed to minimize the total weighted completion time and makespan in heterogeneous parallel networks, respectively.","For any given $\\epsilon>0$, our proposed approximation algorithm for minimizing the total weighted completion time achieves approximation ratios of $3 + \\epsilon$ and $2 + \\epsilon$ in the cases of arbitrary and zero release times, respectively.","Additionally, we introduce an approximation algorithm for minimizing the makespan, achieving an approximation ratio of $2 + \\epsilon$ for $\\epsilon>0$. Notably, these advancements surpass the previously best-known approximation ratio of $O(\\log m/ \\log \\log m)$ for both minimizing the total weighted completion time and makespan.","This result also improves upon the previous approximation ratios of $6-\\frac{2}{m}$ and $5-\\frac{2}{m}$ for arbitrary and zero release times, respectively, in identical parallel networks."],"url":"http://arxiv.org/abs/2312.16413v1"}
{"created":"2023-12-27 04:40:12","title":"Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning","abstract":"Continual learning (CL) has shown promising results and comparable performance to learning at once in a fully supervised manner. However, CL strategies typically require a large number of labeled samples, making their real-life deployment challenging. In this work, we focus on semi-supervised continual learning (SSCL), where the model progressively learns from partially labeled data with unknown categories. We provide a comprehensive analysis of SSCL and demonstrate that unreliable distributions of unlabeled data lead to unstable training and refinement of the progressing stages. This problem severely impacts the performance of SSCL. To address the limitations, we propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for semi-supervised continual learning, which leverages both semantic and structural information to achieve more stable knowledge distillation on unlabeled data and exhibit robustness against distribution bias. Firstly, we formalize a general model of structural distillation and design a dynamic graph construction for the continual learning progress. Next, we define a structure distillation vector and design a dynamic sub-graph distillation algorithm, which enables end-to-end training and adaptability to scale up tasks. The entire proposed method is adaptable to various CL methods and supervision settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100, and ImageNet-100, with varying supervision ratios, demonstrate the effectiveness of our proposed approach in mitigating the catastrophic forgetting problem in semi-supervised continual learning scenarios.","sentences":["Continual learning (CL) has shown promising results and comparable performance to learning at once in a fully supervised manner.","However, CL strategies typically require a large number of labeled samples, making their real-life deployment challenging.","In this work, we focus on semi-supervised continual learning (SSCL), where the model progressively learns from partially labeled data with unknown categories.","We provide a comprehensive analysis of SSCL and demonstrate that unreliable distributions of unlabeled data lead to unstable training and refinement of the progressing stages.","This problem severely impacts the performance of SSCL.","To address the limitations, we propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for semi-supervised continual learning, which leverages both semantic and structural information to achieve more stable knowledge distillation on unlabeled data and exhibit robustness against distribution bias.","Firstly, we formalize a general model of structural distillation and design a dynamic graph construction for the continual learning progress.","Next, we define a structure distillation vector and design a dynamic sub-graph distillation algorithm, which enables end-to-end training and adaptability to scale up tasks.","The entire proposed method is adaptable to various CL methods and supervision settings.","Finally, experiments conducted on three datasets CIFAR10, CIFAR100, and ImageNet-100, with varying supervision ratios, demonstrate the effectiveness of our proposed approach in mitigating the catastrophic forgetting problem in semi-supervised continual learning scenarios."],"url":"http://arxiv.org/abs/2312.16409v1"}
