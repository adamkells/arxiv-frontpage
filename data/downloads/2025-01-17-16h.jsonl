{"created":"2025-01-15 18:59:39","title":"Octopus: Scalable Low-Cost CXL Memory Pooling","abstract":"Compute Express Link (CXL) is widely-supported interconnect standard that promises to enable memory disaggregation in data centers. CXL allows for memory pooling, which can be used to create a shared memory space across multiple servers. However, CXL does not specify how to actually build a memory pool. Existing proposals for CXL memory pools are expensive, as they require CXL switches or large multi-headed devices. In this paper, we propose a new design for CXL memory pools that is cost-effective. We call these designs Octopus topologies. Our design uses small CXL devices that can be made cheaply and offer fast access latencies. Specifically, we propose asymmetric CXL topologies where hosts connect to different sets of CXL devices. This enables pooling and sharing memory across multiple hosts even as each individual CXL device is only connected to a small number of hosts. Importantly, this uses hardware that is readily available today. We also show the trade-off in terms of CXL pod size and cost overhead per host. Octopus improves the Pareto frontier defined by prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost per host.","sentences":["Compute Express Link (CXL) is widely-supported interconnect standard that promises to enable memory disaggregation in data centers.","CXL allows for memory pooling, which can be used to create a shared memory space across multiple servers.","However, CXL does not specify how to actually build a memory pool.","Existing proposals for CXL memory pools are expensive, as they require CXL switches or large multi-headed devices.","In this paper, we propose a new design for CXL memory pools that is cost-effective.","We call these designs Octopus topologies.","Our design uses small CXL devices that can be made cheaply and offer fast access latencies.","Specifically, we propose asymmetric CXL topologies where hosts connect to different sets of CXL devices.","This enables pooling and sharing memory across multiple hosts even as each individual CXL device is only connected to a small number of hosts.","Importantly, this uses hardware that is readily available today.","We also show the trade-off in terms of CXL pod size and cost overhead per host.","Octopus improves the Pareto frontier defined by prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost per host."],"url":"http://arxiv.org/abs/2501.09020v1"}
{"created":"2025-01-15 18:48:38","title":"SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and Segmentation Mask Generation","abstract":"Acquiring and annotating surgical data is often resource-intensive, ethical constraining, and requiring significant expert involvement. While generative AI models like text-to-image can alleviate data scarcity, incorporating spatial annotations, such as segmentation masks, is crucial for precision-driven surgical applications, simulation, and education. This study introduces both a novel task and method, SimGen, for Simultaneous Image and Mask Generation. SimGen is a diffusion model based on the DDPM framework and Residual U-Net, designed to jointly generate high-fidelity surgical images and their corresponding segmentation masks. The model leverages cross-correlation priors to capture dependencies between continuous image and discrete mask distributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to enhance class separability and uniformity in the RGB space of the masks. SimGen delivers high-fidelity images and accurate segmentation masks, outperforming baselines across six public datasets assessed on image and semantic inception distance metrics. Ablation study shows that the CFL improves mask quality and spatial separation. Downstream experiments suggest generated image-mask pairs are usable if regulations limit human data release for research. This work offers a cost-effective solution for generating paired surgical images and complex labels, advancing surgical AI development by reducing the need for expensive manual annotations.","sentences":["Acquiring and annotating surgical data is often resource-intensive, ethical constraining, and requiring significant expert involvement.","While generative AI models like text-to-image can alleviate data scarcity, incorporating spatial annotations, such as segmentation masks, is crucial for precision-driven surgical applications, simulation, and education.","This study introduces both a novel task and method, SimGen, for Simultaneous Image and Mask Generation.","SimGen is a diffusion model based on the DDPM framework and Residual U-Net, designed to jointly generate high-fidelity surgical images and their corresponding segmentation masks.","The model leverages cross-correlation priors to capture dependencies between continuous image and discrete mask distributions.","Additionally, a Canonical Fibonacci Lattice (CFL) is employed to enhance class separability and uniformity in the RGB space of the masks.","SimGen delivers high-fidelity images and accurate segmentation masks, outperforming baselines across six public datasets assessed on image and semantic inception distance metrics.","Ablation study shows that the CFL improves mask quality and spatial separation.","Downstream experiments suggest generated image-mask pairs are usable if regulations limit human data release for research.","This work offers a cost-effective solution for generating paired surgical images and complex labels, advancing surgical AI development by reducing the need for expensive manual annotations."],"url":"http://arxiv.org/abs/2501.09008v1"}
{"created":"2025-01-15 18:43:50","title":"Lightweight Security for Ambient-Powered Programmable Reflections with Reconfigurable Intelligent Surfaces","abstract":"Ambient Internet-of-Things (AIoT) form a new class of emerging technology that promises to deliver pervasive wireless connectivity to previously disconnected devices and products, assisting dependent industries (for example, supply chain, clothing, remote surveillance, climate monitoring, and sensors) to obtain granular real-time service visibility. Such ultra-low complexity and power consumption devices, that are either battery-less or have the capability for limited energy storage, can provide data feeds about the condition of any aspect (e.g., an environment or an item) that is being monitored, enabling proactive or reactive control by any application server. Although the security of data involving AIoT devices is critical for key decisions of any dependent operational system, the implementation of resource intensive cryptographic algorithms and other security mechanisms becomes nearly infeasible, or very challenging, due to the device energy and computational limitations. In this article, we present a lightweight security solution that enables confidentiality, integrity, and privacy protection in wireless links including AIoT. We consider, as a case study, an ambient-powered Reconfigurable Intelligent Surface (RIS) that harvests energy from its incident radio waves to realize programmable reflective beamforming, enabling the communication between a Base Station (BS) and end-user terminals. The proposed lightweight security solution is applied to the control channel between the BS and the RIS controller which is responsible for the metasurface's dynamic management and phase configuration optimization.","sentences":["Ambient Internet-of-Things (AIoT) form a new class of emerging technology that promises to deliver pervasive wireless connectivity to previously disconnected devices and products, assisting dependent industries (for example, supply chain, clothing, remote surveillance, climate monitoring, and sensors) to obtain granular real-time service visibility.","Such ultra-low complexity and power consumption devices, that are either battery-less or have the capability for limited energy storage, can provide data feeds about the condition of any aspect (e.g., an environment or an item) that is being monitored, enabling proactive or reactive control by any application server.","Although the security of data involving AIoT devices is critical for key decisions of any dependent operational system, the implementation of resource intensive cryptographic algorithms and other security mechanisms becomes nearly infeasible, or very challenging, due to the device energy and computational limitations.","In this article, we present a lightweight security solution that enables confidentiality, integrity, and privacy protection in wireless links including AIoT.","We consider, as a case study, an ambient-powered Reconfigurable Intelligent Surface (RIS) that harvests energy from its incident radio waves to realize programmable reflective beamforming, enabling the communication between a Base Station (BS) and end-user terminals.","The proposed lightweight security solution is applied to the control channel between the BS and the RIS controller which is responsible for the metasurface's dynamic management and phase configuration optimization."],"url":"http://arxiv.org/abs/2501.09005v1"}
{"created":"2025-01-15 18:37:08","title":"Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails","abstract":"As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM \"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets. In addition, we introduce a novel training blend that combines safety with topic following data.This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs.","sentences":["As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel.","Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications.","To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories.","This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types.","Using a hybrid data generation pipeline that combines human annotations with a multi-LLM \"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy.","To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets.","In addition, we introduce a novel training blend that combines safety with topic following data.","This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference.","We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs."],"url":"http://arxiv.org/abs/2501.09004v1"}
{"created":"2025-01-15 18:23:33","title":"VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science","abstract":"Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial and error approaches for development rather than data driven methods. While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility. To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT GAN), a novel generative model specifically designed for augmenting small, noisy datasets. We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state of the art tabular generative models. We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised. Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, which is an extremely common pharmaceutical task. We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT GAN pretrained on ChEMBL available as a pip package.","sentences":["Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial and error approaches for development rather than data driven methods.","While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility.","To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT GAN), a novel generative model specifically designed for augmenting small, noisy datasets.","We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state of the art tabular generative models.","We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised.","Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, which is an extremely common pharmaceutical task.","We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT GAN pretrained on ChEMBL available as a pip package."],"url":"http://arxiv.org/abs/2501.08995v1"}
{"created":"2025-01-15 17:47:57","title":"Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models","abstract":"As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation. Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data. The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries. Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson correlation for substantive validity, factor analysis and Cronbach's alpha for structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity. Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated strong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting structural validity and generalizability. Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility. Substantive validity was supported by correlations between note length and scores for Succinct (rho = -0.200, p = 0.029) and Organized (rho = -0.190, p = 0.037). Discriminant validity distinguished high- from low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer integration of LLMs into healthcare workflows.","sentences":["As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation.","Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data.","The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries.","Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b).","Validation included Pearson correlation for substantive validity, factor analysis and Cronbach's alpha for structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity.","Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability.","The PDSQI-9 demonstrated strong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting structural validity and generalizability.","Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility.","Substantive validity was supported by correlations between note length and scores for Succinct (rho = -0.200, p = 0.029) and Organized (rho = -0.190, p = 0.037).","Discriminant validity distinguished high- from low-quality summaries (p < 0.001).","The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer integration of LLMs into healthcare workflows."],"url":"http://arxiv.org/abs/2501.08977v1"}
{"created":"2025-01-15 17:36:56","title":"Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models","abstract":"Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment analysis that aims to extract and classify sentiments based on specific aspects or features of a product, service, or entity. Unlike traditional sentiment analysis, which assigns a general sentiment score to entire reviews or texts, ABSA focuses on breaking down the text into individual components or aspects (e.g., quality, price, service) and evaluating the sentiment towards each. This allows for a more granular level of understanding of customer opinions, enabling businesses to pinpoint specific areas of strength and improvement. The process involves several key steps, including aspect extraction, sentiment classification, and aspect-level sentiment aggregation for a review paragraph or any other form that the users have provided. ABSA has significant applications in areas such as product reviews, social media monitoring, customer feedback analysis, and market research. By leveraging techniques from natural language processing (NLP) and machine learning, ABSA facilitates the extraction of valuable insights, enabling companies to make data-driven decisions that enhance customer satisfaction and optimize offerings. As ABSA evolves, it holds the potential to greatly improve personalized customer experiences by providing a deeper understanding of sentiment across various product aspects. In this work, we have analyzed the strength of LLMs for a complete cross-domain aspect-based sentiment analysis with the aim of defining the framework for certain products and using it for other similar situations. We argue that it is possible to that at an effectiveness of 92\\% accuracy for the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.","sentences":["Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment analysis that aims to extract and classify sentiments based on specific aspects or features of a product, service, or entity.","Unlike traditional sentiment analysis, which assigns a general sentiment score to entire reviews or texts, ABSA focuses on breaking down the text into individual components or aspects (e.g., quality, price, service) and evaluating the sentiment towards each.","This allows for a more granular level of understanding of customer opinions, enabling businesses to pinpoint specific areas of strength and improvement.","The process involves several key steps, including aspect extraction, sentiment classification, and aspect-level sentiment aggregation for a review paragraph or any other form that the users have provided.","ABSA has significant applications in areas such as product reviews, social media monitoring, customer feedback analysis, and market research.","By leveraging techniques from natural language processing (NLP) and machine learning, ABSA facilitates the extraction of valuable insights, enabling companies to make data-driven decisions that enhance customer satisfaction and optimize offerings.","As ABSA evolves, it holds the potential to greatly improve personalized customer experiences by providing a deeper understanding of sentiment across various product aspects.","In this work, we have analyzed the strength of LLMs for a complete cross-domain aspect-based sentiment analysis with the aim of defining the framework for certain products and using it for other similar situations.","We argue that it is possible to that at an effectiveness of 92\\% accuracy for the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12."],"url":"http://arxiv.org/abs/2501.08974v1"}
{"created":"2025-01-15 17:28:53","title":"Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography","abstract":"We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them.","sentences":["We often interact with untrusted parties.","Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data.","Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs.","While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for.","In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible.","In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness.","This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible.","We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME.","Finally, we outline current limitations and discuss the path forward in implementing them."],"url":"http://arxiv.org/abs/2501.08970v1"}
{"created":"2025-01-15 17:18:46","title":"An analysis of data variation and bias in image-based dermatological datasets for machine learning classification","abstract":"AI algorithms have become valuable in aiding professionals in healthcare. The increasing confidence obtained by these models is helpful in critical decision demands. In clinical dermatology, classification models can detect malignant lesions on patients' skin using only RGB images as input. However, most learning-based methods employ data acquired from dermoscopic datasets on training, which are large and validated by a gold standard. Clinical models aim to deal with classification on users' smartphone cameras that do not contain the corresponding resolution provided by dermoscopy. Also, clinical applications bring new challenges. It can contain captures from uncontrolled environments, skin tone variations, viewpoint changes, noises in data and labels, and unbalanced classes. A possible alternative would be to use transfer learning to deal with the clinical images. However, as the number of samples is low, it can cause degradations on the model's performance; the source distribution used in training differs from the test set. This work aims to evaluate the gap between dermoscopic and clinical samples and understand how the dataset variations impact training. It assesses the main differences between distributions that disturb the model's prediction. Finally, from experiments on different architectures, we argue how to combine the data from divergent distributions, decreasing the impact on the model's final accuracy.","sentences":["AI algorithms have become valuable in aiding professionals in healthcare.","The increasing confidence obtained by these models is helpful in critical decision demands.","In clinical dermatology, classification models can detect malignant lesions on patients' skin using only RGB images as input.","However, most learning-based methods employ data acquired from dermoscopic datasets on training, which are large and validated by a gold standard.","Clinical models aim to deal with classification on users' smartphone cameras that do not contain the corresponding resolution provided by dermoscopy.","Also, clinical applications bring new challenges.","It can contain captures from uncontrolled environments, skin tone variations, viewpoint changes, noises in data and labels, and unbalanced classes.","A possible alternative would be to use transfer learning to deal with the clinical images.","However, as the number of samples is low, it can cause degradations on the model's performance; the source distribution used in training differs from the test set.","This work aims to evaluate the gap between dermoscopic and clinical samples and understand how the dataset variations impact training.","It assesses the main differences between distributions that disturb the model's prediction.","Finally, from experiments on different architectures, we argue how to combine the data from divergent distributions, decreasing the impact on the model's final accuracy."],"url":"http://arxiv.org/abs/2501.08962v1"}
