{"created":"2024-10-24 17:59:21","title":"Deep Insights into Cognitive Decline: A Survey of Leveraging Non-Intrusive Modalities with Deep Learning Techniques","abstract":"Cognitive decline is a natural part of aging, often resulting in reduced cognitive abilities. In some cases, however, this decline is more pronounced, typically due to disorders such as Alzheimer's disease. Early detection of anomalous cognitive decline is crucial, as it can facilitate timely professional intervention. While medical data can help in this detection, it often involves invasive procedures. An alternative approach is to employ non-intrusive techniques such as speech or handwriting analysis, which do not necessarily affect daily activities. This survey reviews the most relevant methodologies that use deep learning techniques to automate the cognitive decline estimation task, including audio, text, and visual processing. We discuss the key features and advantages of each modality and methodology, including state-of-the-art approaches like Transformer architecture and foundation models. In addition, we present works that integrate different modalities to develop multimodal models. We also highlight the most significant datasets and the quantitative results from studies using these resources. From this review, several conclusions emerge. In most cases, the textual modality achieves the best results and is the most relevant for detecting cognitive decline. Moreover, combining various approaches from individual modalities into a multimodal model consistently enhances performance across nearly all scenarios.","sentences":["Cognitive decline is a natural part of aging, often resulting in reduced cognitive abilities.","In some cases, however, this decline is more pronounced, typically due to disorders such as Alzheimer's disease.","Early detection of anomalous cognitive decline is crucial, as it can facilitate timely professional intervention.","While medical data can help in this detection, it often involves invasive procedures.","An alternative approach is to employ non-intrusive techniques such as speech or handwriting analysis, which do not necessarily affect daily activities.","This survey reviews the most relevant methodologies that use deep learning techniques to automate the cognitive decline estimation task, including audio, text, and visual processing.","We discuss the key features and advantages of each modality and methodology, including state-of-the-art approaches like Transformer architecture and foundation models.","In addition, we present works that integrate different modalities to develop multimodal models.","We also highlight the most significant datasets and the quantitative results from studies using these resources.","From this review, several conclusions emerge.","In most cases, the textual modality achieves the best results and is the most relevant for detecting cognitive decline.","Moreover, combining various approaches from individual modalities into a multimodal model consistently enhances performance across nearly all scenarios."],"url":"http://arxiv.org/abs/2410.18972v1"}
{"created":"2024-10-24 17:58:31","title":"Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms","abstract":"Building a generalist model for user interface (UI) understanding is challenging due to various foundational issues, such as platform diversity, resolution variation, and data limitation. In this paper, we introduce Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI understanding across a wide range of platforms, including iPhone, Android, iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI 2 introduces three key innovations: support for multiple platform types, high-resolution perception through adaptive scaling, and advanced task training data generation powered by GPT-4o with set-of-mark visual prompting. These advancements enable Ferret-UI 2 to perform complex, user-centered interactions, making it highly versatile and adaptable for the expanding diversity of platform ecosystems. Extensive empirical experiments on referring, grounding, user-centric advanced tasks (comprising 9 subtasks $\\times$ 5 platforms), GUIDE next-action prediction dataset, and GUI-World multi-platform benchmark demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also shows strong cross-platform transfer capabilities.","sentences":["Building a generalist model for user interface (UI) understanding is challenging due to various foundational issues, such as platform diversity, resolution variation, and data limitation.","In this paper, we introduce Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI understanding across a wide range of platforms, including iPhone, Android, iPad, Webpage, and AppleTV.","Building on the foundation of Ferret-UI, Ferret-UI 2 introduces three key innovations: support for multiple platform types, high-resolution perception through adaptive scaling, and advanced task training data generation powered by GPT-4o with set-of-mark visual prompting.","These advancements enable Ferret-UI 2 to perform complex, user-centered interactions, making it highly versatile and adaptable for the expanding diversity of platform ecosystems.","Extensive empirical experiments on referring, grounding, user-centric advanced tasks (comprising 9 subtasks $\\times$ 5 platforms), GUIDE next-action prediction dataset, and GUI-World multi-platform benchmark demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also shows strong cross-platform transfer capabilities."],"url":"http://arxiv.org/abs/2410.18967v1"}
{"created":"2024-10-24 17:58:22","title":"Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions","abstract":"Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. While multiple approaches have been developed to identify data contamination, these approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 47 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our analysis reveals that when classifying instances used for pretraining LLMs, detection approaches based on these three assumptions perform close to random guessing, suggesting that current LLMs learn data distributions rather than memorizing individual instances. Overall, this work underscores the importance of approaches clearly stating their underlying assumptions and testing their validity across various scenarios.","sentences":["Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers.","However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments.","While multiple approaches have been developed to identify data contamination, these approaches rely on specific assumptions that may not hold universally across different settings.","To bridge this gap, we systematically review 47 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated.","We identify and analyze eight categories of assumptions and test three of them as case studies.","Our analysis reveals that when classifying instances used for pretraining LLMs, detection approaches based on these three assumptions perform close to random guessing, suggesting that current LLMs learn data distributions rather than memorizing individual instances.","Overall, this work underscores the importance of approaches clearly stating their underlying assumptions and testing their validity across various scenarios."],"url":"http://arxiv.org/abs/2410.18966v1"}
{"created":"2024-10-24 17:56:08","title":"Context is Key: A Benchmark for Forecasting with Essential Textual Information","abstract":"Forecasting is a critical task in decision making across various domains. While numerical data provides a foundation, it often lacks crucial context necessary for accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge or constraints, which can be efficiently communicated through natural language. However, the ability of existing forecasting models to effectively integrate this textual information remains an open question. To address this, we introduce \"Context is Key\" (CiK), a time series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. By presenting this benchmark, we aim to advance multimodal forecasting, promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/ .","sentences":["Forecasting is a critical task in decision making across various domains.","While numerical data provides a foundation, it often lacks crucial context necessary for accurate predictions.","Human forecasters frequently rely on additional information, such as background knowledge or constraints, which can be efficiently communicated through natural language.","However, the ability of existing forecasting models to effectively integrate this textual information remains an open question.","To address this, we introduce \"Context is Key\" (CiK), a time series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities.","We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark.","Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings.","By presenting this benchmark, we aim to advance multimodal forecasting, promoting models that are both accurate and accessible to decision-makers with varied technical expertise.","The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/ ."],"url":"http://arxiv.org/abs/2410.18959v1"}
{"created":"2024-10-24 17:55:52","title":"Stable Consistency Tuning: Understanding and Improving Consistency Models","abstract":"Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling. These models are trained either through consistency distillation, which leverages pretrained diffusion models, or consistency training/tuning directly from raw data. In this work, we propose a novel framework for understanding consistency models by modeling the denoising process of the diffusion model as a Markov Decision Process (MDP) and framing consistency model training as the value estimation through Temporal Difference~(TD) Learning. More importantly, this framework allows us to analyze the limitations of current consistency training/tuning strategies. Built upon Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT), which incorporates variance-reduced learning using the score identity. SCT leads to significant performance improvements on benchmarks such as CIFAR-10 and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID 1.55, a new SoTA for consistency models.","sentences":["Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising.","In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling.","These models are trained either through consistency distillation, which leverages pretrained diffusion models, or consistency training/tuning directly from raw data.","In this work, we propose a novel framework for understanding consistency models by modeling the denoising process of the diffusion model as a Markov Decision Process (MDP) and framing consistency model training as the value estimation through Temporal Difference~(TD) Learning.","More importantly, this framework allows us to analyze the limitations of current consistency training/tuning strategies.","Built upon Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT), which incorporates variance-reduced learning using the score identity.","SCT leads to significant performance improvements on benchmarks such as CIFAR-10 and ImageNet-64.","On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID 1.55, a new SoTA for consistency models."],"url":"http://arxiv.org/abs/2410.18958v1"}
{"created":"2024-10-24 17:55:03","title":"Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in Low-Resource Code","abstract":"Large Language Models (LLMs) demonstrate strong proficiency in generating code for high-resource programming languages (HRPLs) like Python but struggle significantly with low-resource programming languages (LRPLs) such as Racket or D. This performance gap deepens the digital divide, preventing developers using LRPLs from benefiting equally from LLM advancements and reinforcing disparities in innovation within underrepresented programming communities. While generating additional training data for LRPLs is promising, it faces two key challenges: manual annotation is labor-intensive and costly, and LLM-generated LRPL code is often of subpar quality. The underlying cause of this issue is the gap between natural language to programming language gap (NL-PL Gap), which is especially pronounced in LRPLs due to limited aligned data. In this work, we introduce a novel approach called Bridge-Coder, which leverages LLMs' intrinsic capabilities to enhance the performance on LRPLs. Our method consists of two key stages. Bridge Generation, where we create high-quality dataset by utilizing LLMs' general knowledge understanding, proficiency in HRPLs, and in-context learning abilities. Then, we apply the Bridged Alignment, which progressively improves the alignment between NL instructions and LRPLs. Experimental results across multiple LRPLs show that Bridge-Coder significantly enhances model performance, demonstrating the effectiveness and generalization of our approach. Furthermore, we offer a detailed analysis of the key components of our method, providing valuable insights for future work aimed at addressing the challenges associated with LRPLs.","sentences":["Large Language Models (LLMs) demonstrate strong proficiency in generating code for high-resource programming languages (HRPLs) like Python but struggle significantly with low-resource programming languages (LRPLs) such as Racket or D. This performance gap deepens the digital divide, preventing developers using LRPLs from benefiting equally from LLM advancements and reinforcing disparities in innovation within underrepresented programming communities.","While generating additional training data for LRPLs is promising, it faces two key challenges: manual annotation is labor-intensive and costly, and LLM-generated LRPL code is often of subpar quality.","The underlying cause of this issue is the gap between natural language to programming language gap (NL-PL Gap), which is especially pronounced in LRPLs due to limited aligned data.","In this work, we introduce a novel approach called Bridge-Coder, which leverages LLMs' intrinsic capabilities to enhance the performance on LRPLs.","Our method consists of two key stages.","Bridge Generation, where we create high-quality dataset by utilizing LLMs' general knowledge understanding, proficiency in HRPLs, and in-context learning abilities.","Then, we apply the Bridged Alignment, which progressively improves the alignment between NL instructions and LRPLs.","Experimental results across multiple LRPLs show that Bridge-Coder significantly enhances model performance, demonstrating the effectiveness and generalization of our approach.","Furthermore, we offer a detailed analysis of the key components of our method, providing valuable insights for future work aimed at addressing the challenges associated with LRPLs."],"url":"http://arxiv.org/abs/2410.18957v1"}
{"created":"2024-10-24 17:54:42","title":"Large Spatial Model: End-to-end Unposed Images to Semantic 3D","abstract":"Reconstructing and understanding 3D structures from a limited number of images is a well-established problem in computer vision. Traditional methods usually break this task into multiple subtasks, each requiring complex transformations between different data representations. For instance, dense reconstruction through Structure-from-Motion (SfM) involves converting images into key points, optimizing camera parameters, and estimating structures. Afterward, accurate sparse reconstructions are required for further dense modeling, which is subsequently fed into task-specific neural networks. This multi-step process results in considerable processing time and increased engineering complexity.   In this work, we present the Large Spatial Model (LSM), which processes unposed RGB images directly into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward operation, and it can generate versatile label maps by interacting with language at novel viewpoints. Leveraging a Transformer-based architecture, LSM integrates global geometry through pixel-aligned point maps. To enhance spatial attribute regression, we incorporate local context aggregation with multi-scale fusion, improving the accuracy of fine local details. To tackle the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field. An efficient decoder then parameterizes a set of semantic anisotropic Gaussians, facilitating supervised end-to-end learning. Extensive experiments across various tasks show that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time.","sentences":["Reconstructing and understanding 3D structures from a limited number of images is a well-established problem in computer vision.","Traditional methods usually break this task into multiple subtasks, each requiring complex transformations between different data representations.","For instance, dense reconstruction through Structure-from-Motion (SfM) involves converting images into key points, optimizing camera parameters, and estimating structures.","Afterward, accurate sparse reconstructions are required for further dense modeling, which is subsequently fed into task-specific neural networks.","This multi-step process results in considerable processing time and increased engineering complexity.   ","In this work, we present the Large Spatial Model (LSM), which processes unposed RGB images directly into semantic radiance fields.","LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward operation, and it can generate versatile label maps by interacting with language at novel viewpoints.","Leveraging a Transformer-based architecture, LSM integrates global geometry through pixel-aligned point maps.","To enhance spatial attribute regression, we incorporate local context aggregation with multi-scale fusion, improving the accuracy of fine local details.","To tackle the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field.","An efficient decoder then parameterizes a set of semantic anisotropic Gaussians, facilitating supervised end-to-end learning.","Extensive experiments across various tasks show that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time."],"url":"http://arxiv.org/abs/2410.18956v1"}
{"created":"2024-10-24 17:53:53","title":"BioMistral-NLU: Towards More Generalizable Medical Language Understanding through Instruction Tuning","abstract":"Large language models (LLMs) such as ChatGPT are fine-tuned on large and diverse instruction-following corpora, and can generalize to new tasks. However, those instruction-tuned LLMs often perform poorly in specialized medical natural language understanding (NLU) tasks that require domain knowledge, granular text comprehension, and structured data extraction. To bridge the gap, we: (1) propose a unified prompting format for 7 important NLU tasks, % through span extraction and multi-choice question-answering (QA), (2) curate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing open-source medical NLU corpora, and (3) develop BioMistral-NLU, a generalizable medical NLU model, through fine-tuning BioMistral on MNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6 important NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical Language Understanding Evaluation (BLUE) and Biomedical Language Understanding and Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLU outperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT and GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step over diverse NLU tasks enhance LLMs' generalizability across diverse medical NLU tasks. Our ablation experiments show that instruction-tuning on a wider variety of tasks, even when the total number of training instances remains constant, enhances downstream zero-shot generalization.","sentences":["Large language models (LLMs) such as ChatGPT are fine-tuned on large and diverse instruction-following corpora, and can generalize to new tasks.","However, those instruction-tuned LLMs often perform poorly in specialized medical natural language understanding (NLU) tasks that require domain knowledge, granular text comprehension, and structured data extraction.","To bridge the gap, we: (1) propose a unified prompting format for 7 important NLU tasks, % through span extraction and multi-choice question-answering (QA), (2) curate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing open-source medical NLU corpora, and (3) develop BioMistral-NLU, a generalizable medical NLU model, through fine-tuning BioMistral on MNLU-Instruct.","We evaluate BioMistral-NLU in a zero-shot setting, across 6 important NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical Language Understanding Evaluation (BLUE) and Biomedical Language Understanding and Reasoning Benchmark (BLURB).","Our experiments show that our BioMistral-NLU outperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT and GPT-4.","Our dataset-agnostic prompting strategy and instruction tuning step over diverse NLU tasks enhance LLMs' generalizability across diverse medical NLU tasks.","Our ablation experiments show that instruction-tuning on a wider variety of tasks, even when the total number of training instances remains constant, enhances downstream zero-shot generalization."],"url":"http://arxiv.org/abs/2410.18955v1"}
{"created":"2024-10-24 17:53:33","title":"Learning Structured Compressed Sensing with Automatic Resource Allocation","abstract":"Multidimensional data acquisition often requires extensive time and poses significant challenges for hardware and software regarding data storage and processing. Rather than designing a single compression matrix as in conventional compressed sensing, structured compressed sensing yields dimension-specific compression matrices, reducing the number of optimizable parameters. Recent advances in machine learning (ML) have enabled task-based supervised learning of subsampling matrices, albeit at the expense of complex downstream models. Additionally, the sampling resource allocation across dimensions is often determined in advance through heuristics. To address these challenges, we introduce Structured COmpressed Sensing with Automatic Resource Allocation (SCOSARA) with an information theory-based unsupervised learning strategy. SCOSARA adaptively distributes samples across sampling dimensions while maximizing Fisher information content. Using ultrasound localization as a case study, we compare SCOSARA to state-of-the-art ML-based and greedy search algorithms. Simulation results demonstrate that SCOSARA can produce high-quality subsampling matrices that achieve lower Cram\\'er-Rao Bound values than the baselines. In addition, SCOSARA outperforms other ML-based algorithms in terms of the number of trainable parameters, computational complexity, and memory requirements while automatically choosing the number of samples per axis.","sentences":["Multidimensional data acquisition often requires extensive time and poses significant challenges for hardware and software regarding data storage and processing.","Rather than designing a single compression matrix as in conventional compressed sensing, structured compressed sensing yields dimension-specific compression matrices, reducing the number of optimizable parameters.","Recent advances in machine learning (ML) have enabled task-based supervised learning of subsampling matrices, albeit at the expense of complex downstream models.","Additionally, the sampling resource allocation across dimensions is often determined in advance through heuristics.","To address these challenges, we introduce Structured COmpressed Sensing with Automatic Resource Allocation (SCOSARA) with an information theory-based unsupervised learning strategy.","SCOSARA adaptively distributes samples across sampling dimensions while maximizing Fisher information content.","Using ultrasound localization as a case study, we compare SCOSARA to state-of-the-art ML-based and greedy search algorithms.","Simulation results demonstrate that SCOSARA can produce high-quality subsampling matrices that achieve lower Cram\\'er-Rao Bound values than the baselines.","In addition, SCOSARA outperforms other ML-based algorithms in terms of the number of trainable parameters, computational complexity, and memory requirements while automatically choosing the number of samples per axis."],"url":"http://arxiv.org/abs/2410.18954v1"}
{"created":"2024-10-24 17:50:08","title":"Adjusted Overfitting Regression","abstract":"In this paper, I will introduce a new form of regression, that can adjust overfitting and underfitting through, \"distance-based regression.\" Overfitting often results in finding false patterns causing inaccurate results, so by having a new approach that minimizes overfitting, more accurate predictions can be derived. Then I will proceed with a test of my regression form and show additional ways to optimize the regression. Finally, I will apply my new technique to a specific data set to demonstrate its practical value.","sentences":["In this paper, I will introduce a new form of regression, that can adjust overfitting and underfitting through, \"distance-based regression.\"","Overfitting often results in finding false patterns causing inaccurate results, so by having a new approach that minimizes overfitting, more accurate predictions can be derived.","Then I will proceed with a test of my regression form and show additional ways to optimize the regression.","Finally, I will apply my new technique to a specific data set to demonstrate its practical value."],"url":"http://arxiv.org/abs/2410.18950v1"}
{"created":"2024-10-24 17:22:24","title":"Matching Composition and Efficient Weight Reduction in Dynamic Matching","abstract":"We consider the foundational problem of maintaining a $(1-\\varepsilon)$-approximate maximum weight matching (MWM) in an $n$-node dynamic graph undergoing edge insertions and deletions. We provide a general reduction that reduces the problem on graphs with a weight range of $\\mathrm{poly}(n)$ to $\\mathrm{poly}(1/\\varepsilon)$ at the cost of just an additive $\\mathrm{poly}(1/\\varepsilon)$ in update time. This improves upon the prior reduction of Gupta-Peng (FOCS 2013) which reduces the problem to a weight range of $\\varepsilon^{-O(1/\\varepsilon)}$ with a multiplicative cost of $O(\\log n)$.   When combined with a reduction of Bernstein-Dudeja-Langley (STOC 2021) this yields a reduction from dynamic $(1-\\varepsilon)$-approximate MWM in bipartite graphs with a weight range of $\\mathrm{poly}(n)$ to dynamic $(1-\\varepsilon)$-approximate maximum cardinality matching in bipartite graphs at the cost of a multiplicative $\\mathrm{poly}(1/\\varepsilon)$ in update time, thereby resolving an open problem in [GP'13; BDL'21]. Additionally, we show that our approach is amenable to MWM problems in streaming, shared-memory work-depth, and massively parallel computation models. We also apply our techniques to obtain an efficient dynamic algorithm for rounding weighted fractional matchings in general graphs. Underlying our framework is a new structural result about MWM that we call the \"matching composition lemma\" and new dynamic matching subroutines that may be of independent interest.","sentences":["We consider the foundational problem of maintaining a $(1-\\varepsilon)$-approximate maximum weight matching (MWM) in an $n$-node dynamic graph undergoing edge insertions and deletions.","We provide a general reduction that reduces the problem on graphs with a weight range of $\\mathrm{poly}(n)$ to $\\mathrm{poly}(1/\\varepsilon)$ at the cost of just an additive $\\mathrm{poly}(1/\\varepsilon)$ in update time.","This improves upon the prior reduction of Gupta-Peng (FOCS 2013) which reduces the problem to a weight range of $\\varepsilon^{-O(1/\\varepsilon)}$ with a multiplicative cost of $O(\\log n)$.   When combined with a reduction of Bernstein-Dudeja-Langley (STOC 2021) this yields a reduction from dynamic $(1-\\varepsilon)$-approximate MWM in bipartite graphs with a weight range of $\\mathrm{poly}(n)$ to dynamic $(1-\\varepsilon)$-approximate maximum cardinality matching in bipartite graphs at the cost of a multiplicative $\\mathrm{poly}(1/\\varepsilon)$ in update time, thereby resolving an open problem in [GP'13; BDL'21].","Additionally, we show that our approach is amenable to MWM problems in streaming, shared-memory work-depth, and massively parallel computation models.","We also apply our techniques to obtain an efficient dynamic algorithm for rounding weighted fractional matchings in general graphs.","Underlying our framework is a new structural result about MWM that we call the \"matching composition lemma\" and new dynamic matching subroutines that may be of independent interest."],"url":"http://arxiv.org/abs/2410.18936v1"}
{"created":"2024-10-24 17:19:53","title":"ANAVI: Audio Noise Awareness using Visuals of Indoor environments for NAVIgation","abstract":"We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for quieter robot path planning. While humans are naturally aware of the noise they make and its impact on those around them, robots currently lack this awareness. A key challenge in achieving audio awareness for robots is estimating how loud will the robot's actions be at a listener's location? Since sound depends upon the geometry and material composition of rooms, we train the robot to passively perceive loudness using visual observations of indoor environments. To this end, we generate data on how loud an 'impulse' sounds at different listener locations in simulated homes, and train our Acoustic Noise Predictor (ANP). Next, we collect acoustic profiles corresponding to different actions for navigation. Unifying ANP with action acoustics, we demonstrate experiments with wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these robots adhere to the noise constraints of the environment. See code and data at https://anavi-corl24.github.io/","sentences":["We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for quieter robot path planning.","While humans are naturally aware of the noise they make and its impact on those around them, robots currently lack this awareness.","A key challenge in achieving audio awareness for robots is estimating how loud will the robot's actions be at a listener's location?","Since sound depends upon the geometry and material composition of rooms, we train the robot to passively perceive loudness using visual observations of indoor environments.","To this end, we generate data on how loud an 'impulse' sounds at different listener locations in simulated homes, and train our Acoustic Noise Predictor (ANP).","Next, we collect acoustic profiles corresponding to different actions for navigation.","Unifying ANP with action acoustics, we demonstrate experiments with wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these robots adhere to the noise constraints of the environment.","See code and data at https://anavi-corl24.github.io/"],"url":"http://arxiv.org/abs/2410.18932v1"}
{"created":"2024-10-24 17:13:39","title":"LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor Search","abstract":"Approximate nearest neighbor (ANN) search is a key component in many modern machine learning pipelines; recent use cases include retrieval-augmented generation (RAG) and vector databases. Clustering-based ANN algorithms, that use score computation methods based on product quantization (PQ), are often used in industrial-scale applications due to their scalability and suitability for distributed and disk-based implementations. However, they have slower query times than the leading graph-based ANN algorithms. In this work, we propose a new supervised score computation method based on the observation that inner product approximation is a multivariate (multi-output) regression problem that can be solved efficiently by reduced-rank regression. Our experiments show that on modern high-dimensional data sets, the proposed reduced-rank regression (RRR) method is superior to PQ in both query latency and memory usage. We also introduce LoRANN, a clustering-based ANN library that leverages the proposed score computation method. LoRANN is competitive with the leading graph-based algorithms and outperforms the state-of-the-art GPU ANN methods on high-dimensional data sets.","sentences":["Approximate nearest neighbor (ANN) search is a key component in many modern machine learning pipelines; recent use cases include retrieval-augmented generation (RAG) and vector databases.","Clustering-based ANN algorithms, that use score computation methods based on product quantization (PQ), are often used in industrial-scale applications due to their scalability and suitability for distributed and disk-based implementations.","However, they have slower query times than the leading graph-based ANN algorithms.","In this work, we propose a new supervised score computation method based on the observation that inner product approximation is a multivariate (multi-output) regression problem that can be solved efficiently by reduced-rank regression.","Our experiments show that on modern high-dimensional data sets, the proposed reduced-rank regression (RRR) method is superior to PQ in both query latency and memory usage.","We also introduce LoRANN, a clustering-based ANN library that leverages the proposed score computation method.","LoRANN is competitive with the leading graph-based algorithms and outperforms the state-of-the-art GPU ANN methods on high-dimensional data sets."],"url":"http://arxiv.org/abs/2410.18926v1"}
{"created":"2024-10-24 17:11:52","title":"SegLLM: Multi-round Reasoning Segmentation","abstract":"We present SegLLM, a novel multi-round interactive reasoning segmentation model that enhances LLM-based segmentation by exploiting conversational memory of both visual and textual outputs. By leveraging a mask-aware multimodal LLM, SegLLM re-integrates previous segmentation results into its input stream, enabling it to reason about complex user intentions and segment objects in relation to previously identified entities, including positional, interactional, and hierarchical relationships, across multiple interactions. This capability allows SegLLM to respond to visual and text queries in a chat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM outperforms existing methods in multi-round interactive reasoning segmentation by over 20%. Additionally, we observed that training on multi-round reasoning segmentation data enhances performance on standard single-round referring segmentation and localization tasks, resulting in a 5.5% increase in cIoU for referring expression segmentation and a 4.5% improvement in Acc@0.5 for referring expression localization.","sentences":["We present SegLLM, a novel multi-round interactive reasoning segmentation model that enhances LLM-based segmentation by exploiting conversational memory of both visual and textual outputs.","By leveraging a mask-aware multimodal LLM, SegLLM re-integrates previous segmentation results into its input stream, enabling it to reason about complex user intentions and segment objects in relation to previously identified entities, including positional, interactional, and hierarchical relationships, across multiple interactions.","This capability allows SegLLM to respond to visual and text queries in a chat-like manner.","Evaluated on the newly curated MRSeg benchmark, SegLLM outperforms existing methods in multi-round interactive reasoning segmentation by over 20%.","Additionally, we observed that training on multi-round reasoning segmentation data enhances performance on standard single-round referring segmentation and localization tasks, resulting in a 5.5% increase in cIoU for referring expression segmentation and a 4.5% improvement in Acc@0.5 for referring expression localization."],"url":"http://arxiv.org/abs/2410.18923v1"}
{"created":"2024-10-24 17:08:20","title":"Using Parametric PINNs for Predicting Internal and External Turbulent Flows","abstract":"Computational fluid dynamics (CFD) solvers employing two-equation eddy viscosity models are the industry standard for simulating turbulent flows using the Reynolds-averaged Navier-Stokes (RANS) formulation. While these methods are computationally less expensive than direct numerical simulations, they can still incur significant computational costs to achieve the desired accuracy. In this context, physics-informed neural networks (PINNs) offer a promising approach for developing parametric surrogate models that leverage both existing, but limited CFD solutions and the governing differential equations to predict simulation outcomes in a computationally efficient, differentiable, and near real-time manner. In this work, we build upon the previously proposed RANS-PINN framework, which only focused on predicting flow over a cylinder. To investigate the efficacy of RANS-PINN as a viable approach to building parametric surrogate models, we investigate its accuracy in predicting relevant turbulent flow variables for both internal and external flows. To ensure training convergence with a more complex loss function, we adopt a novel sampling approach that exploits the domain geometry to ensure a proper balance among the contributions from various regions within the solution domain. The effectiveness of this framework is then demonstrated for two scenarios that represent a broad class of internal and external flow problems.","sentences":["Computational fluid dynamics (CFD) solvers employing two-equation eddy viscosity models are the industry standard for simulating turbulent flows using the Reynolds-averaged Navier-Stokes (RANS) formulation.","While these methods are computationally less expensive than direct numerical simulations, they can still incur significant computational costs to achieve the desired accuracy.","In this context, physics-informed neural networks (PINNs) offer a promising approach for developing parametric surrogate models that leverage both existing, but limited CFD solutions and the governing differential equations to predict simulation outcomes in a computationally efficient, differentiable, and near real-time manner.","In this work, we build upon the previously proposed RANS-PINN framework, which only focused on predicting flow over a cylinder.","To investigate the efficacy of RANS-PINN as a viable approach to building parametric surrogate models, we investigate its accuracy in predicting relevant turbulent flow variables for both internal and external flows.","To ensure training convergence with a more complex loss function, we adopt a novel sampling approach that exploits the domain geometry to ensure a proper balance among the contributions from various regions within the solution domain.","The effectiveness of this framework is then demonstrated for two scenarios that represent a broad class of internal and external flow problems."],"url":"http://arxiv.org/abs/2410.18917v1"}
{"created":"2024-10-24 17:05:34","title":"Testing Support Size More Efficiently Than Learning Histograms","abstract":"Consider two problems about an unknown probability distribution $p$:   1. How many samples from $p$ are required to test if $p$ is supported on $n$ elements or not? Specifically, given samples from $p$, determine whether it is supported on at most $n$ elements, or it is \"$\\epsilon$-far\" (in total variation distance) from being supported on $n$ elements.   2. Given $m$ samples from $p$, what is the largest lower bound on its support size that we can produce?   The best known upper bound for problem (1) uses a general algorithm for learning the histogram of the distribution $p$, which requires $\\Theta(\\tfrac{n}{\\epsilon^2 \\log n})$ samples. We show that testing can be done more efficiently than learning the histogram, using only $O(\\tfrac{n}{\\epsilon \\log n} \\log(1/\\epsilon))$ samples, nearly matching the best known lower bound of $\\Omega(\\tfrac{n}{\\epsilon \\log n})$. This algorithm also provides a better solution to problem (2), producing larger lower bounds on support size than what follows from previous work. The proof relies on an analysis of Chebyshev polynomial approximations outside the range where they are designed to be good approximations, and the paper is intended as an accessible self-contained exposition of the Chebyshev polynomial method.","sentences":["Consider two problems about an unknown probability distribution $p$:   1.","How many samples from $p$ are required to test if $p$ is supported on $n$ elements or not?","Specifically, given samples from $p$, determine whether it is supported on at most $n$ elements, or it is \"$\\epsilon$-far\" (in total variation distance) from being supported on $n$ elements.   ","2.","Given $m$ samples from $p$, what is the largest lower bound on its support size that we can produce?   ","The best known upper bound for problem (1) uses a general algorithm for learning the histogram of the distribution $p$, which requires $\\Theta(\\tfrac{n}{\\epsilon^2 \\log n})$ samples.","We show that testing can be done more efficiently than learning the histogram, using only $O(\\tfrac{n}{\\epsilon \\log n} \\log(1/\\epsilon))$ samples, nearly matching the best known lower bound of $\\Omega(\\tfrac{n}{\\epsilon \\log n})$.","This algorithm also provides a better solution to problem (2), producing larger lower bounds on support size than what follows from previous work.","The proof relies on an analysis of Chebyshev polynomial approximations outside the range where they are designed to be good approximations, and the paper is intended as an accessible self-contained exposition of the Chebyshev polynomial method."],"url":"http://arxiv.org/abs/2410.18915v1"}
{"created":"2024-10-24 17:02:52","title":"Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling","abstract":"Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at https://gs-dynamics.github.io.","sentences":["Videos of robots interacting with objects encode rich information about the objects' dynamics.","However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications.","In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics.","We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks.","This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions.","By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions.","The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction.","The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks.","We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics.","Our project page is available at https://gs-dynamics.github.io."],"url":"http://arxiv.org/abs/2410.18912v1"}
{"created":"2024-10-24 16:59:26","title":"SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment","abstract":"Imitation learning from human demonstrations is an effective paradigm for robot manipulation, but acquiring large datasets is costly and resource-intensive, especially for long-horizon tasks. To address this issue, we propose SkillMimicGen (SkillGen), an automated system for generating demonstration datasets from a few human demos. SkillGen segments human demos into manipulation skills, adapts these skills to new contexts, and stitches them together through free-space transit and transfer motion. We also propose a Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and termination components from SkillGen datasets, enabling skills to be sequenced using motion planning at test-time. We demonstrate that SkillGen greatly improves data generation and policy learning performance over a state-of-the-art data generation framework, resulting in the capability to produce data for large scene variations, including clutter, and agents that are on average 24% more successful. We demonstrate the efficacy of SkillGen by generating over 24K demonstrations across 18 task variants in simulation from just 60 human demonstrations, and training proficient, often near-perfect, HSP agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task. Videos, and more at https://skillgen.github.io.","sentences":["Imitation learning from human demonstrations is an effective paradigm for robot manipulation, but acquiring large datasets is costly and resource-intensive, especially for long-horizon tasks.","To address this issue, we propose SkillMimicGen (SkillGen), an automated system for generating demonstration datasets from a few human demos.","SkillGen segments human demos into manipulation skills, adapts these skills to new contexts, and stitches them together through free-space transit and transfer motion.","We also propose a Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and termination components from SkillGen datasets, enabling skills to be sequenced using motion planning at test-time.","We demonstrate that SkillGen greatly improves data generation and policy learning performance over a state-of-the-art data generation framework, resulting in the capability to produce data for large scene variations, including clutter, and agents that are on average 24% more successful.","We demonstrate the efficacy of SkillGen by generating over 24K demonstrations across 18 task variants in simulation from just 60 human demonstrations, and training proficient, often near-perfect, HSP agents.","Finally, we apply SkillGen to 3 real-world manipulation tasks and also demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.","Videos, and more at https://skillgen.github.io."],"url":"http://arxiv.org/abs/2410.18907v1"}
{"created":"2024-10-24 16:48:12","title":"LLMs for Extremely Low-Resource Finno-Ugric Languages","abstract":"The advancement of large language models (LLMs) has predominantly focused on high-resource languages, leaving low-resource languages, such as those in the Finno-Ugric family, significantly underrepresented. This paper addresses this gap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle of LLM creation, from data collection to instruction tuning and evaluation. Our contributions include developing multilingual base and instruction-tuned models; creating evaluation benchmarks, including the smugri-MT-bench multi-turn conversational benchmark; and conducting human evaluation. We intend for this work to promote linguistic diversity, ensuring that lesser-resourced languages can benefit from advancements in NLP.","sentences":["The advancement of large language models (LLMs) has predominantly focused on high-resource languages, leaving low-resource languages, such as those in the Finno-Ugric family, significantly underrepresented.","This paper addresses this gap by focusing on V\\~oro, Livonian, and Komi.","We cover almost the entire cycle of LLM creation, from data collection to instruction tuning and evaluation.","Our contributions include developing multilingual base and instruction-tuned models; creating evaluation benchmarks, including the smugri-MT-bench multi-turn conversational benchmark; and conducting human evaluation.","We intend for this work to promote linguistic diversity, ensuring that lesser-resourced languages can benefit from advancements in NLP."],"url":"http://arxiv.org/abs/2410.18902v1"}
{"created":"2024-10-24 16:35:23","title":"ArterialNet: Reconstructing Arterial Blood Pressure Waveform with Wearable Pulsatile Signals, a Cohort-Aware Approach","abstract":"Continuous arterial blood pressure (ABP) monitoring is invasive but essential for hemodynamic monitoring. Recent techniques have reconstructed ABP non-invasively using pulsatile signals but produced inaccurate systolic and diastolic blood pressure (SBP and DBP) values and were sensitive to individual variability. ArterialNet integrates generalized pulsatile-to-ABP signal translation and personalized feature extraction using hybrid loss functions and regularization. We validated ArterialNet using the MIMIC-III dataset and achieved a root mean square error (RMSE) of 5.41 mmHg, with at least a 58% lower standard deviation. ArterialNet reconstructed ABP with an RMSE of 7.99 mmHg in remote health scenarios. ArterialNet achieved superior performance in ABP reconstruction and SBP and DBP estimations, with significantly reduced subject variance, demonstrating its potential in remote health settings. We also ablated ArterialNet architecture to investigate the contributions of each component and evaluated its translational impact and robustness by conducting a series of ablations on data quality and availability.","sentences":["Continuous arterial blood pressure (ABP) monitoring is invasive but essential for hemodynamic monitoring.","Recent techniques have reconstructed ABP non-invasively using pulsatile signals but produced inaccurate systolic and diastolic blood pressure (SBP and DBP) values and were sensitive to individual variability.","ArterialNet integrates generalized pulsatile-to-ABP signal translation and personalized feature extraction using hybrid loss functions and regularization.","We validated ArterialNet using the MIMIC-III dataset and achieved a root mean square error (RMSE) of 5.41 mmHg, with at least a 58% lower standard deviation.","ArterialNet reconstructed ABP with an RMSE of 7.99 mmHg in remote health scenarios.","ArterialNet achieved superior performance in ABP reconstruction and SBP and DBP estimations, with significantly reduced subject variance, demonstrating its potential in remote health settings.","We also ablated ArterialNet architecture to investigate the contributions of each component and evaluated its translational impact and robustness by conducting a series of ablations on data quality and availability."],"url":"http://arxiv.org/abs/2410.18895v1"}
{"created":"2024-10-24 16:27:03","title":"Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance","abstract":"NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models. While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. In this work, we consider the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples. Through a case study of four datasets from the TRUE benchmark, covering different tasks and domains, we empirically analyze the labeling quality of existing datasets, and compare expert, crowd-sourced, and our LLM-based annotations in terms of agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. Our findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs so-called mistakes are due to label errors rather than genuine model failures. Additionally, we discuss the implications of mislabeled data and propose methods to mitigate them in training to improve model performance.","sentences":["NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field.","Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models.","While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency.","Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets.","In this work, we consider the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples.","Through a case study of four datasets from the TRUE benchmark, covering different tasks and domains, we empirically analyze the labeling quality of existing datasets, and compare expert, crowd-sourced, and our LLM-based annotations in terms of agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method.","Our findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance.","This suggests that many of the LLMs so-called mistakes are due to label errors rather than genuine model failures.","Additionally, we discuss the implications of mislabeled data and propose methods to mitigate them in training to improve model performance."],"url":"http://arxiv.org/abs/2410.18889v1"}
{"created":"2024-10-24 16:20:57","title":"Connectivity Labeling Schemes for Edge and Vertex Faults via Expander Hierarchies","abstract":"We consider the problem of assigning short labels to the vertices and edges of a graph $G$ so that given any query $\\langle s,t,F\\rangle$ with $|F|\\leq f$, we can determine whether $s$ and $t$ are still connected in $G-F$, given only the labels of $F\\cup\\{s,t\\}$. This problem has been considered when $F\\subset E$ (edge faults), where correctness is guaranteed with high probability (w.h.p.) or deterministically, and when $F\\subset V$ (vertex faults), both w.h.p.~and deterministically. Our main results are as follows.   [Deterministic Edge Faults.] We give a new deterministic labeling scheme for edge faults that uses $\\tilde{O}(\\sqrt{f})$-bit labels, which can be constructed in polynomial time. This improves on Dory and Parter's [PODC 2021] existential bound of $O(f\\log n)$ (requiring exponential time to compute) and the efficient $\\tilde{O}(f^2)$-bit scheme of Izumi, Emek, Wadayama, and Masuzawa [PODC 2023]. Our construction uses an improved edge-expander hierarchy and a distributed coding technique based on Reed-Solomon codes.   [Deterministic Vertex Faults.] We improve Parter, Petruschka, and Pettie's [STOC 2024] deterministic $O(f^7\\log^{13} n)$-bit labeling scheme for vertex faults to $O(f^4\\log^{7.5} n)$ bits, using an improved vertex-expander hierarchy and better sparsification of shortcut graphs.   [Randomized Edge/Verex Faults.] We improve the size of Dory and Parter's [PODC 2021] randomized edge fault labeling scheme from $O(\\min\\{f+\\log n, \\log^3 n\\})$ bits to $O(\\min\\{f+\\log n, \\log^2 n\\log f\\})$ bits, shaving a $\\log n/\\log f$ factor. We also improve the size of Parter, Petruschka, and Pettie's [STOC 2024] randomized vertex fault labeling scheme from $O(f^3\\log^5 n)$ bits to $O(f^2\\log^6 n)$ bits, which comes closer to their $\\Omega(f)$-bit lower bound.","sentences":["We consider the problem of assigning short labels to the vertices and edges of a graph $G$ so that given any query $\\langle s,t,F\\rangle$ with $|F|\\leq f$, we can determine whether $s$ and $t$ are still connected in $G-F$, given only the labels of $F\\cup\\{s,t\\}$. This problem has been considered when $F\\subset E$ (edge faults), where correctness is guaranteed with high probability (w.h.p.) or deterministically, and when $F\\subset V$ (vertex faults), both w.h.p.~and deterministically.","Our main results are as follows.   ","[Deterministic Edge Faults.]","We give a new deterministic labeling scheme for edge faults that uses $\\tilde{O}(\\sqrt{f})$-bit labels, which can be constructed in polynomial time.","This improves on Dory and Parter's [PODC 2021] existential bound of $O(f\\log n)$ (requiring exponential time to compute) and the efficient $\\tilde{O}(f^2)$-bit scheme of Izumi, Emek, Wadayama, and Masuzawa","[PODC 2023].","Our construction uses an improved edge-expander hierarchy and a distributed coding technique based on Reed-Solomon codes.   ","[Deterministic Vertex Faults.]","We improve Parter, Petruschka, and","Pettie's [STOC 2024] deterministic $O(f^7\\log^{13} n)$-bit labeling scheme for vertex faults to $O(f^4\\log^{7.5} n)$ bits, using an improved vertex-expander hierarchy and better sparsification of shortcut graphs.   ","[Randomized Edge/Verex Faults.]","We improve the size of Dory and Parter's [PODC 2021] randomized edge fault labeling scheme from $O(\\min\\{f+\\log n, \\log^3 n\\})$ bits to $O(\\min\\{f+\\log n, \\log^2 n\\log f\\})$ bits, shaving a $\\log n/\\log f$ factor.","We also improve the size of Parter, Petruschka, and Pettie's","[STOC 2024] randomized vertex fault labeling scheme from $O(f^3\\log^5 n)$ bits to $O(f^2\\log^6 n)$ bits, which comes closer to their $\\Omega(f)$-bit lower bound."],"url":"http://arxiv.org/abs/2410.18885v1"}
{"created":"2024-10-24 16:17:18","title":"Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences","abstract":"One-step text-to-image generator models offer advantages such as swift inference efficiency, flexible architectures, and state-of-the-art generation performance. In this paper, we study the problem of aligning one-step generator models with human preferences for the first time. Inspired by the success of reinforcement learning using human feedback (RLHF), we formulate the alignment problem as maximizing expected human reward functions while adding an Integral Kullback-Leibler divergence term to prevent the generator from diverging. By overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the first, fast-converging and image data-free human preference alignment method for one-step text-to-image generators. We also introduce novel theoretical insights, showing that using CFG for diffusion distillation is secretly doing RLHF with DI++. Such an interesting finding brings understanding and potential contributions to future research involving CFG. In the experiment sections, we align both UNet-based and DiT-based one-step generators using DI++, which use the Stable Diffusion 1.5 and the PixelArt-$\\alpha$ as the reference diffusion processes. The resulting DiT-based one-step text-to-image model achieves a strong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO validation prompt dataset. It also achieves a leading Human preference Score (HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\\alpha$. Both theoretical contributions and empirical evidence indicate that DI++ is a strong human-preference alignment approach for one-step text-to-image models.","sentences":["One-step text-to-image generator models offer advantages such as swift inference efficiency, flexible architectures, and state-of-the-art generation performance.","In this paper, we study the problem of aligning one-step generator models with human preferences for the first time.","Inspired by the success of reinforcement learning using human feedback (RLHF), we formulate the alignment problem as maximizing expected human reward functions while adding an Integral Kullback-Leibler divergence term to prevent the generator from diverging.","By overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the first, fast-converging and image data-free human preference alignment method for one-step text-to-image generators.","We also introduce novel theoretical insights, showing that using CFG for diffusion distillation is secretly doing RLHF with DI++.","Such an interesting finding brings understanding and potential contributions to future research involving CFG.","In the experiment sections, we align both UNet-based and DiT-based one-step generators using DI++, which use the Stable Diffusion 1.5 and the PixelArt-$\\alpha$ as the reference diffusion processes.","The resulting DiT-based one-step text-to-image model achieves a strong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO validation prompt dataset.","It also achieves a leading Human preference Score (HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\\alpha$. Both theoretical contributions and empirical evidence indicate that DI++ is a strong human-preference alignment approach for one-step text-to-image models."],"url":"http://arxiv.org/abs/2410.18881v1"}
{"created":"2024-10-24 16:08:41","title":"Packing Short Cycles","abstract":"Cycle packing is a fundamental problem in optimization, graph theory, and algorithms. Motivated by recent advancements in finding vertex-disjoint paths between a specified set of vertices that either minimize the total length of the paths [Bj\\\"orklund, Husfeldt, ICALP 2014; Mari, Mukherjee, Pilipczuk, and Sankowski, SODA 2024] or request the paths to be shortest [Lochet, SODA 2021], we consider the following cycle packing problems: Min-Sum Cycle Packing and Shortest Cycle Packing.   In Min-Sum Cycle Packing, we try to find, in a weighted undirected graph, $k$ vertex-disjoint cycles of minimum total weight. Our first main result is an algorithm that, for any fixed $k$, solves the problem in polynomial time. We complement this result by establishing the W[1]-hardness of Min-Sum Cycle Packing parameterized by $k$. The same results hold for the version of the problem where the task is to find $k$ edge-disjoint cycles.   Our second main result concerns Shortest Cycle Packing, which is a special case of Min-Sum Cycle Packing that asks to find a packing of $k$ shortest cycles in a graph. We prove this problem to be fixed-parameter tractable (FPT) when parameterized by $k$ on weighted planar graphs. We also obtain a polynomial kernel for the edge-disjoint variant of the problem on planar graphs. Deciding whether Min-Sum Cycle Packing is FPT on planar graphs and whether Shortest Cycle Packing is FPT on general graphs remain challenging open questions.","sentences":["Cycle packing is a fundamental problem in optimization, graph theory, and algorithms.","Motivated by recent advancements in finding vertex-disjoint paths between a specified set of vertices that either minimize the total length of the paths [Bj\\\"orklund, Husfeldt, ICALP 2014; Mari, Mukherjee, Pilipczuk, and Sankowski, SODA 2024] or request the paths to be shortest [Lochet, SODA 2021], we consider the following cycle packing problems: Min-Sum Cycle Packing and Shortest Cycle Packing.   ","In Min-Sum Cycle Packing, we try to find, in a weighted undirected graph, $k$ vertex-disjoint cycles of minimum total weight.","Our first main result is an algorithm that, for any fixed $k$, solves the problem in polynomial time.","We complement this result by establishing the W[1]-hardness of Min-Sum Cycle Packing parameterized by $k$. The same results hold for the version of the problem where the task is to find $k$ edge-disjoint cycles.   ","Our second main result concerns Shortest Cycle Packing, which is a special case of Min-Sum Cycle Packing that asks to find a packing of $k$ shortest cycles in a graph.","We prove this problem to be fixed-parameter tractable (FPT) when parameterized by $k$ on weighted planar graphs.","We also obtain a polynomial kernel for the edge-disjoint variant of the problem on planar graphs.","Deciding whether Min-Sum Cycle Packing is FPT on planar graphs and whether Shortest Cycle Packing is FPT on general graphs remain challenging open questions."],"url":"http://arxiv.org/abs/2410.18878v1"}
{"created":"2024-10-24 15:53:21","title":"A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics","abstract":"By incorporating physical consistency as inductive bias, deep neural networks display increased generalization capabilities and data efficiency in learning nonlinear dynamic models. However, the complexity of these models generally increases with the system dimensionality, requiring larger datasets, more complex deep networks, and significant computational effort. We propose a novel geometric network architecture to learn physically-consistent reduced-order dynamic parameters that accurately describe the original high-dimensional system behavior. This is achieved by building on recent advances in model-order reduction and by adopting a Riemannian perspective to jointly learn a structure-preserving latent space and the associated low-dimensional dynamics. Our approach enables accurate long-term predictions of the high-dimensional dynamics of rigid and deformable systems with increased data efficiency by inferring interpretable and physically plausible reduced Lagrangian models.","sentences":["By incorporating physical consistency as inductive bias, deep neural networks display increased generalization capabilities and data efficiency in learning nonlinear dynamic models.","However, the complexity of these models generally increases with the system dimensionality, requiring larger datasets, more complex deep networks, and significant computational effort.","We propose a novel geometric network architecture to learn physically-consistent reduced-order dynamic parameters that accurately describe the original high-dimensional system behavior.","This is achieved by building on recent advances in model-order reduction and by adopting a Riemannian perspective to jointly learn a structure-preserving latent space and the associated low-dimensional dynamics.","Our approach enables accurate long-term predictions of the high-dimensional dynamics of rigid and deformable systems with increased data efficiency by inferring interpretable and physically plausible reduced Lagrangian models."],"url":"http://arxiv.org/abs/2410.18868v1"}
{"created":"2024-10-24 15:48:34","title":"FedSPD: A Soft-clustering Approach for Personalized Decentralized Federated Learning","abstract":"Federated learning has recently gained popularity as a framework for distributed clients to collaboratively train a machine learning model using local data. While traditional federated learning relies on a central server for model aggregation, recent advancements adopt a decentralized framework, enabling direct model exchange between clients and eliminating the single point of failure. However, existing decentralized frameworks often assume all clients train a shared model. Personalizing each client's model can enhance performance, especially with heterogeneous client data distributions. We propose FedSPD, an efficient personalized federated learning algorithm for the decentralized setting, and show that it learns accurate models even in low-connectivity networks. To provide theoretical guarantees on convergence, we introduce a clustering-based framework that enables consensus on models for distinct data clusters while personalizing to unique mixtures of these clusters at different clients. This flexibility, allowing selective model updates based on data distribution, substantially reduces communication costs compared to prior work on personalized federated learning in decentralized settings. Experimental results on real-world datasets show that FedSPD outperforms multiple decentralized variants of personalized federated learning algorithms, especially in scenarios with low-connectivity networks.","sentences":["Federated learning has recently gained popularity as a framework for distributed clients to collaboratively train a machine learning model using local data.","While traditional federated learning relies on a central server for model aggregation, recent advancements adopt a decentralized framework, enabling direct model exchange between clients and eliminating the single point of failure.","However, existing decentralized frameworks often assume all clients train a shared model.","Personalizing each client's model can enhance performance, especially with heterogeneous client data distributions.","We propose FedSPD, an efficient personalized federated learning algorithm for the decentralized setting, and show that it learns accurate models even in low-connectivity networks.","To provide theoretical guarantees on convergence, we introduce a clustering-based framework that enables consensus on models for distinct data clusters while personalizing to unique mixtures of these clusters at different clients.","This flexibility, allowing selective model updates based on data distribution, substantially reduces communication costs compared to prior work on personalized federated learning in decentralized settings.","Experimental results on real-world datasets show that FedSPD outperforms multiple decentralized variants of personalized federated learning algorithms, especially in scenarios with low-connectivity networks."],"url":"http://arxiv.org/abs/2410.18862v1"}
{"created":"2024-10-24 15:41:56","title":"Demystifying Large Language Models for Medicine: A Primer","abstract":"Large language models (LLMs) represent a transformative class of AI tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions. Their potential application spans a broad range of medical tasks, such as clinical documentation, matching patients to clinical trials, and answering medical questions. In this primer paper, we propose an actionable guideline to help healthcare professionals more efficiently utilize LLMs in their work, along with a set of best practices. This approach consists of several main phases, including formulating the task, choosing LLMs, prompt engineering, fine-tuning, and deployment. We start with the discussion of critical considerations in identifying healthcare tasks that align with the core capabilities of LLMs and selecting models based on the selected task and data, performance requirements, and model interface. We then review the strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs to specialized medical tasks. Deployment considerations, including regulatory compliance, ethical guidelines, and continuous monitoring for fairness and bias, are also discussed. By providing a structured step-by-step methodology, this tutorial aims to equip healthcare professionals with the tools necessary to effectively integrate LLMs into clinical practice, ensuring that these powerful technologies are applied in a safe, reliable, and impactful manner.","sentences":["Large language models (LLMs) represent a transformative class of AI tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions.","Their potential application spans a broad range of medical tasks, such as clinical documentation, matching patients to clinical trials, and answering medical questions.","In this primer paper, we propose an actionable guideline to help healthcare professionals more efficiently utilize LLMs in their work, along with a set of best practices.","This approach consists of several main phases, including formulating the task, choosing LLMs, prompt engineering, fine-tuning, and deployment.","We start with the discussion of critical considerations in identifying healthcare tasks that align with the core capabilities of LLMs and selecting models based on the selected task and data, performance requirements, and model interface.","We then review the strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs to specialized medical tasks.","Deployment considerations, including regulatory compliance, ethical guidelines, and continuous monitoring for fairness and bias, are also discussed.","By providing a structured step-by-step methodology, this tutorial aims to equip healthcare professionals with the tools necessary to effectively integrate LLMs into clinical practice, ensuring that these powerful technologies are applied in a safe, reliable, and impactful manner."],"url":"http://arxiv.org/abs/2410.18856v1"}
{"created":"2024-10-24 15:20:16","title":"Diffusion for Multi-Embodiment Grasping","abstract":"Grasping is a fundamental skill in robotics with diverse applications across medical, industrial, and domestic domains. However, current approaches for predicting valid grasps are often tailored to specific grippers, limiting their applicability when gripper designs change. To address this limitation, we explore the transfer of grasping strategies between various gripper designs, enabling the use of data from diverse sources. In this work, we present an approach based on equivariant diffusion that facilitates gripper-agnostic encoding of scenes containing graspable objects and gripper-aware decoding of grasp poses by integrating gripper geometry into the model. We also develop a dataset generation framework that produces cluttered scenes with variable-sized object heaps, improving the training of grasp synthesis methods. Experimental evaluation on diverse object datasets demonstrates the generalizability of our approach across gripper architectures, ranging from simple parallel-jaw grippers to humanoid hands, outperforming both single-gripper and multi-gripper state-of-the-art methods.","sentences":["Grasping is a fundamental skill in robotics with diverse applications across medical, industrial, and domestic domains.","However, current approaches for predicting valid grasps are often tailored to specific grippers, limiting their applicability when gripper designs change.","To address this limitation, we explore the transfer of grasping strategies between various gripper designs, enabling the use of data from diverse sources.","In this work, we present an approach based on equivariant diffusion that facilitates gripper-agnostic encoding of scenes containing graspable objects and gripper-aware decoding of grasp poses by integrating gripper geometry into the model.","We also develop a dataset generation framework that produces cluttered scenes with variable-sized object heaps, improving the training of grasp synthesis methods.","Experimental evaluation on diverse object datasets demonstrates the generalizability of our approach across gripper architectures, ranging from simple parallel-jaw grippers to humanoid hands, outperforming both single-gripper and multi-gripper state-of-the-art methods."],"url":"http://arxiv.org/abs/2410.18835v1"}
{"created":"2024-10-24 15:19:48","title":"MazeNet: An Accurate, Fast, and Scalable Deep Learning Solution for Steiner Minimum Trees","abstract":"The Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem, which seeks the shortest interconnection of a given number of terminals in a rectilinear plane while avoiding obstacles, is a critical task in integrated circuit design, network optimization, and robot path planning. Since OARSMT is NP-hard, exact algorithms scale poorly with the number of terminals, leading practical solvers to sacrifice accuracy for large problems. We propose MazeNet, a deep learning-based method that learns to solve the OARSMT from data. MazeNet reframes OARSMT as a maze-solving task that can be addressed with a recurrent convolutional neural network (RCNN). A key hallmark of MazeNet is its scalability: we only need to train the RCNN blocks on mazes with a small number of terminals; larger mazes can be solved by replicating the same pre-trained blocks to create a larger network. Across a wide range of experiments, MazeNet achieves perfect OARSMT-solving accuracy, significantly reduces runtime compared to classical exact algorithms, and can handle more terminals than state-of-the-art approximate algorithms.","sentences":["The Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem, which seeks the shortest interconnection of a given number of terminals in a rectilinear plane while avoiding obstacles, is a critical task in integrated circuit design, network optimization, and robot path planning.","Since OARSMT is NP-hard, exact algorithms scale poorly with the number of terminals, leading practical solvers to sacrifice accuracy for large problems.","We propose MazeNet, a deep learning-based method that learns to solve the OARSMT from data.","MazeNet reframes OARSMT as a maze-solving task that can be addressed with a recurrent convolutional neural network (RCNN).","A key hallmark of MazeNet is its scalability: we only need to train the RCNN blocks on mazes with a small number of terminals; larger mazes can be solved by replicating the same pre-trained blocks to create a larger network.","Across a wide range of experiments, MazeNet achieves perfect OARSMT-solving accuracy, significantly reduces runtime compared to classical exact algorithms, and can handle more terminals than state-of-the-art approximate algorithms."],"url":"http://arxiv.org/abs/2410.18832v1"}
{"created":"2024-10-24 15:15:42","title":"PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models","abstract":"Privacy vulnerabilities in LLMs, such as leakage from memorization, have been constantly identified, and various mitigation proposals have been proposed. LoRA is usually used in fine-tuning LLMs and a good entry point to insert privacy-enhancing modules. In this ongoing research, we introduce PSY, a Posterior Sampling based PrivacY enhancer that can be used in LoRA. We propose a simple yet effective realization of PSY using posterior sampling, which effectively prevents privacy leakage from intermediate information and, in turn, preserves the privacy of data owners. We evaluate LoRA extended with PSY against state-of-the-art membership inference and data extraction attacks. The experiments are executed on three different LLM architectures fine-tuned on three datasets with LoRA. In contrast to the commonly used differential privacy method, we find that our proposed modification consistently reduces the attack success rate. Meanwhile, our method has almost no negative impact on model fine-tuning or final performance. Most importantly, PSY reveals a promising path toward privacy enhancement with latent space extensions.","sentences":["Privacy vulnerabilities in LLMs, such as leakage from memorization, have been constantly identified, and various mitigation proposals have been proposed.","LoRA is usually used in fine-tuning LLMs and a good entry point to insert privacy-enhancing modules.","In this ongoing research, we introduce PSY, a Posterior Sampling based PrivacY enhancer that can be used in LoRA.","We propose a simple yet effective realization of PSY using posterior sampling, which effectively prevents privacy leakage from intermediate information and, in turn, preserves the privacy of data owners.","We evaluate LoRA extended with PSY against state-of-the-art membership inference and data extraction attacks.","The experiments are executed on three different LLM architectures fine-tuned on three datasets with LoRA.","In contrast to the commonly used differential privacy method, we find that our proposed modification consistently reduces the attack success rate.","Meanwhile, our method has almost no negative impact on model fine-tuning or final performance.","Most importantly, PSY reveals a promising path toward privacy enhancement with latent space extensions."],"url":"http://arxiv.org/abs/2410.18824v1"}
{"created":"2024-10-24 15:08:38","title":"Deterministic $(2/3-\\varepsilon)$-Approximation of Matroid Intersection using Nearly-Linear Independence-Oracle Queries","abstract":"In the matroid intersection problem, we are given two matroids $\\mathcal{M}_1 = (V, \\mathcal{I}_1)$ and $\\mathcal{M}_2 = (V, \\mathcal{I}_2)$ defined on the same ground set $V$ of $n$ elements, and the objective is to find a common independent set $S \\in \\mathcal{I}_1 \\cap \\mathcal{I}_2$ of largest possible cardinality, denoted by $r$. In this paper, we consider a deterministic matroid intersection algorithm with only a nearly linear number of independence oracle queries. Our contribution is to present a deterministic $O(\\frac{n}{\\varepsilon} + r \\log r)$-independence-query $(2/3-\\varepsilon)$-approximation algorithm for any $\\varepsilon > 0$. Our idea is very simple: we apply a recent $\\tilde{O}(n \\sqrt{r}/\\varepsilon)$-independence-query $(1 - \\varepsilon)$-approximation algorithm of Blikstad [ICALP 2021], but terminate it before completion. Moreover, we also present a semi-streaming algorithm for $(2/3 -\\varepsilon)$-approximation of matroid intersection in $O(1/\\varepsilon)$ passes.","sentences":["In the matroid intersection problem, we are given two matroids $\\mathcal{M}_1 = (V, \\mathcal{I}_1)$ and $\\mathcal{M}_2 = (V, \\mathcal{I}_2)$ defined on the same ground set $V$ of $n$ elements, and the objective is to find a common independent set $S \\in \\mathcal{I}_1 \\cap \\mathcal{I}_2$ of largest possible cardinality, denoted by $r$. In this paper, we consider a deterministic matroid intersection algorithm with only a nearly linear number of independence oracle queries.","Our contribution is to present a deterministic $O(\\frac{n}{\\varepsilon} + r \\log r)$-independence-query $(2/3-\\varepsilon)$-approximation algorithm for any $\\varepsilon > 0$.","Our idea is very simple: we apply a recent $\\tilde{O}(n \\sqrt{r}/\\varepsilon)$-independence-query $(1 - \\varepsilon)$-approximation algorithm of Blikstad [ICALP 2021], but terminate it before completion.","Moreover, we also present a semi-streaming algorithm for $(2/3 -\\varepsilon)$-approximation of matroid intersection in $O(1/\\varepsilon)$ passes."],"url":"http://arxiv.org/abs/2410.18820v1"}
{"created":"2024-10-24 14:57:46","title":"TangibleChannel: An Innovative Data Physicalization System for Visual Channel Education","abstract":"In this paper, we provide an overview of our attempts to harness data physicalizations as pedagogical tools for enhancing the understanding of visual channels. We first elaborate the research goals that we have crafted for the physicalization prototype, shedding light on the key principles that guided our design choices. Then we detail the materials and datasets we employed for nine channels on our physicalization prototype. A preliminary pilot study is followed to validate its effectiveness. In the end, we present our upcoming research initiatives, including a comparative study for assessing the usability of the physicalization system. In general, the main purpose of our work is to stimulate a wider engagement among visualization educators and researchers, encouraging them to delve into the potentialities of data physicalization as an innovative addition to contemporary teaching methodologies.","sentences":["In this paper, we provide an overview of our attempts to harness data physicalizations as pedagogical tools for enhancing the understanding of visual channels.","We first elaborate the research goals that we have crafted for the physicalization prototype, shedding light on the key principles that guided our design choices.","Then we detail the materials and datasets we employed for nine channels on our physicalization prototype.","A preliminary pilot study is followed to validate its effectiveness.","In the end, we present our upcoming research initiatives, including a comparative study for assessing the usability of the physicalization system.","In general, the main purpose of our work is to stimulate a wider engagement among visualization educators and researchers, encouraging them to delve into the potentialities of data physicalization as an innovative addition to contemporary teaching methodologies."],"url":"http://arxiv.org/abs/2410.18810v1"}
{"created":"2024-10-24 14:54:09","title":"A Combinatorial Approach to Neural Emergent Communication","abstract":"Substantial research on deep learning-based emergent communication uses the referential game framework, specifically the Lewis signaling game, however we argue that successful communication in this game typically only need one or two effective symbols (i.e. message length) because of a sampling pitfall in the training data. To address this issue, we provide a theoretical analysis and introduce a combinatorial algorithm SolveMinSym (SMS) to determine the minimum number of symbols for successful communication min(|M|) in the Lewis signaling game. We use SMS algorithm to create datasets with different min(|M|) to empirically show that higher min(|M|) for the training data increases the number of effective symbols in the emergent language.","sentences":["Substantial research on deep learning-based emergent communication uses the referential game framework, specifically the Lewis signaling game, however we argue that successful communication in this game typically only need one or two effective symbols (i.e. message length) because of a sampling pitfall in the training data.","To address this issue, we provide a theoretical analysis and introduce a combinatorial algorithm SolveMinSym (SMS) to determine the minimum number of symbols for successful communication min(|M|) in the Lewis signaling game.","We use SMS algorithm to create datasets with different min(|M|) to empirically show that higher min(|M|) for the training data increases the number of effective symbols in the emergent language."],"url":"http://arxiv.org/abs/2410.18806v1"}
{"created":"2024-10-24 14:52:21","title":"Language-Agnostic Modeling of Source Reliability on Wikipedia","abstract":"Over the last few years, content verification through reliable sources has become a fundamental need to combat disinformation. Here, we present a language-agnostic model designed to assess the reliability of sources across multiple language editions of Wikipedia. Utilizing editorial activity data, the model evaluates source reliability within different articles of varying controversiality such as Climate Change, COVID-19, History, Media, and Biology topics. Crafting features that express domain usage across articles, the model effectively predicts source reliability, achieving an F1 Macro score of approximately 0.80 for English and other high-resource languages. For mid-resource languages, we achieve 0.65 while the performance of low-resource languages varies; in all cases, the time the domain remains present in the articles (which we dub as permanence) is one of the most predictive features. We highlight the challenge of maintaining consistent model performance across languages of varying resource levels and demonstrate that adapting models from higher-resource languages can improve performance. This work contributes not only to Wikipedia's efforts in ensuring content verifiability but in ensuring reliability across diverse user-generated content in various language communities.","sentences":["Over the last few years, content verification through reliable sources has become a fundamental need to combat disinformation.","Here, we present a language-agnostic model designed to assess the reliability of sources across multiple language editions of Wikipedia.","Utilizing editorial activity data, the model evaluates source reliability within different articles of varying controversiality such as Climate Change, COVID-19, History, Media, and Biology topics.","Crafting features that express domain usage across articles, the model effectively predicts source reliability, achieving an F1 Macro score of approximately 0.80 for English and other high-resource languages.","For mid-resource languages, we achieve 0.65 while the performance of low-resource languages varies; in all cases, the time the domain remains present in the articles (which we dub as permanence) is one of the most predictive features.","We highlight the challenge of maintaining consistent model performance across languages of varying resource levels and demonstrate that adapting models from higher-resource languages can improve performance.","This work contributes not only to Wikipedia's efforts in ensuring content verifiability but in ensuring reliability across diverse user-generated content in various language communities."],"url":"http://arxiv.org/abs/2410.18803v1"}
{"created":"2024-10-24 14:51:09","title":"PointPatchRL -- Masked Reconstruction Improves Reinforcement Learning on Point Clouds","abstract":"Perceiving the environment via cameras is crucial for Reinforcement Learning (RL) in robotics. While images are a convenient form of representation, they often complicate extracting important geometric details, especially with varying geometries or deformable objects. In contrast, point clouds naturally represent this geometry and easily integrate color and positional data from multiple camera views. However, while deep learning on point clouds has seen many recent successes, RL on point clouds is under-researched, with only the simplest encoder architecture considered in the literature. We introduce PointPatchRL (PPRL), a method for RL on point clouds that builds on the common paradigm of dividing point clouds into overlapping patches, tokenizing them, and processing the tokens with transformers. PPRL provides significant improvements compared with other point-cloud processing architectures previously used for RL. We then complement PPRL with masked reconstruction for representation learning and show that our method outperforms strong model-free and model-based baselines on image observations in complex manipulation tasks containing deformable objects and variations in target object geometry. Videos and code are available at https://alrhub.github.io/pprl-website","sentences":["Perceiving the environment via cameras is crucial for Reinforcement Learning (RL) in robotics.","While images are a convenient form of representation, they often complicate extracting important geometric details, especially with varying geometries or deformable objects.","In contrast, point clouds naturally represent this geometry and easily integrate color and positional data from multiple camera views.","However, while deep learning on point clouds has seen many recent successes, RL on point clouds is under-researched, with only the simplest encoder architecture considered in the literature.","We introduce PointPatchRL (PPRL), a method for RL on point clouds that builds on the common paradigm of dividing point clouds into overlapping patches, tokenizing them, and processing the tokens with transformers.","PPRL provides significant improvements compared with other point-cloud processing architectures previously used for RL.","We then complement PPRL with masked reconstruction for representation learning and show that our method outperforms strong model-free and model-based baselines on image observations in complex manipulation tasks containing deformable objects and variations in target object geometry.","Videos and code are available at https://alrhub.github.io/pprl-website"],"url":"http://arxiv.org/abs/2410.18800v1"}
{"created":"2024-10-24 14:50:42","title":"Distill Visual Chart Reasoning Ability from LLMs to MLLMs","abstract":"Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approach to enhance MLLMs is to construct relevant training data focusing on the two aspects. However, collecting and annotating complex charts and questions is costly and time-consuming, and ensuring the quality of annotated answers remains a challenge. In this paper, we propose Code-as-Intermediary Translation (CIT), a cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs. The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information. Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce ReachQA, a dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities. Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks like MathVista. The code and dataset are publicly available at https://github.com/hewei2001/ReachQA.","sentences":["Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs).","Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it.","Thus, a promising approach to enhance MLLMs is to construct relevant training data focusing on the two aspects.","However, collecting and annotating complex charts and questions is costly and time-consuming, and ensuring the quality of annotated answers remains a challenge.","In this paper, we propose Code-as-Intermediary Translation (CIT), a cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs.","The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information.","Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce ReachQA, a dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities.","Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks like MathVista.","The code and dataset are publicly available at https://github.com/hewei2001/ReachQA."],"url":"http://arxiv.org/abs/2410.18798v1"}
{"created":"2024-10-24 14:49:59","title":"Learning Geodesics of Geometric Shape Deformations From Images","abstract":"This paper presents a novel method, named geodesic deformable networks (GDN), that for the first time enables the learning of geodesic flows of deformation fields derived from images. In particular, the capability of our proposed GDN being able to predict geodesics is important for quantifying and comparing deformable shape presented in images. The geodesic deformations, also known as optimal transformations that align pairwise images, are often parameterized by a time sequence of smooth vector fields governed by nonlinear differential equations. A bountiful literature has been focusing on learning the initial conditions (e.g., initial velocity fields) based on registration networks. However, the definition of geodesics central to deformation-based shape analysis is blind to the networks. To address this problem, we carefully develop an efficient neural operator to treat the geodesics as unknown mapping functions learned from the latent deformation spaces. A composition of integral operators and smooth activation functions is then formulated to effectively approximate such mappings. In contrast to previous works, our GDN jointly optimizes a newly defined geodesic loss, which adds additional benefits to promote the network regularizability and generalizability. We demonstrate the effectiveness of GDN on both 2D synthetic data and 3D real brain magnetic resonance imaging (MRI).","sentences":["This paper presents a novel method, named geodesic deformable networks (GDN), that for the first time enables the learning of geodesic flows of deformation fields derived from images.","In particular, the capability of our proposed GDN being able to predict geodesics is important for quantifying and comparing deformable shape presented in images.","The geodesic deformations, also known as optimal transformations that align pairwise images, are often parameterized by a time sequence of smooth vector fields governed by nonlinear differential equations.","A bountiful literature has been focusing on learning the initial conditions (e.g., initial velocity fields) based on registration networks.","However, the definition of geodesics central to deformation-based shape analysis is blind to the networks.","To address this problem, we carefully develop an efficient neural operator to treat the geodesics as unknown mapping functions learned from the latent deformation spaces.","A composition of integral operators and smooth activation functions is then formulated to effectively approximate such mappings.","In contrast to previous works, our GDN jointly optimizes a newly defined geodesic loss, which adds additional benefits to promote the network regularizability and generalizability.","We demonstrate the effectiveness of GDN on both 2D synthetic data and 3D real brain magnetic resonance imaging (MRI)."],"url":"http://arxiv.org/abs/2410.18797v1"}
{"created":"2024-10-24 14:47:25","title":"An LLM Agent for Automatic Geospatial Data Analysis","abstract":"Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors. Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries. To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively. GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing. In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks. This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization. By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks. Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls. Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion. In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming.","sentences":["Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors.","Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries.","To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively.","GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing.","In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks.","This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization.","By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks.","Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls.","Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion.","In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming."],"url":"http://arxiv.org/abs/2410.18792v1"}
{"created":"2024-10-24 14:37:55","title":"Applying Neural Monte Carlo Tree Search to Unsignalized Multi-intersection Scheduling for Autonomous Vehicles","abstract":"Dynamic scheduling of access to shared resources by autonomous systems is a challenging problem, characterized as being NP-hard. The complexity of this task leads to a combinatorial explosion of possibilities in highly dynamic systems where arriving requests must be continuously scheduled subject to strong safety and time constraints. An example of such a system is an unsignalized intersection, where automated vehicles' access to potential conflict zones must be dynamically scheduled. In this paper, we apply Neural Monte Carlo Tree Search (NMCTS) to the challenging task of scheduling platoons of vehicles crossing unsignalized intersections. Crucially, we introduce a transformation model that maps successive sequences of potentially conflicting road-space reservation requests from platoons of vehicles into a series of board-game-like problems and use NMCTS to search for solutions representing optimal road-space allocation schedules in the context of past allocations. To optimize search, we incorporate a prioritized re-sampling method with parallel NMCTS (PNMCTS) to improve the quality of training data. To optimize training, a curriculum learning strategy is used to train the agent to schedule progressively more complex boards culminating in overlapping boards that represent busy intersections. In a busy single four-way unsignalized intersection simulation, PNMCTS solved 95\\% of unseen scenarios, reducing crossing time by 43\\% in light and 52\\% in heavy traffic versus first-in, first-out control. In a 3x3 multi-intersection network, the proposed method maintained free-flow in light traffic when all intersections are under control of PNMCTS and outperformed state-of-the-art RL-based traffic-light controllers in average travel time by 74.5\\% and total throughput by 16\\% in heavy traffic.","sentences":["Dynamic scheduling of access to shared resources by autonomous systems is a challenging problem, characterized as being NP-hard.","The complexity of this task leads to a combinatorial explosion of possibilities in highly dynamic systems where arriving requests must be continuously scheduled subject to strong safety and time constraints.","An example of such a system is an unsignalized intersection, where automated vehicles' access to potential conflict zones must be dynamically scheduled.","In this paper, we apply Neural Monte Carlo Tree Search (NMCTS) to the challenging task of scheduling platoons of vehicles crossing unsignalized intersections.","Crucially, we introduce a transformation model that maps successive sequences of potentially conflicting road-space reservation requests from platoons of vehicles into a series of board-game-like problems and use NMCTS to search for solutions representing optimal road-space allocation schedules in the context of past allocations.","To optimize search, we incorporate a prioritized re-sampling method with parallel NMCTS (PNMCTS) to improve the quality of training data.","To optimize training, a curriculum learning strategy is used to train the agent to schedule progressively more complex boards culminating in overlapping boards that represent busy intersections.","In a busy single four-way unsignalized intersection simulation, PNMCTS solved 95\\% of unseen scenarios, reducing crossing time by 43\\% in light and 52\\% in heavy traffic versus first-in, first-out control.","In a 3x3 multi-intersection network, the proposed method maintained free-flow in light traffic when all intersections are under control of PNMCTS and outperformed state-of-the-art RL-based traffic-light controllers in average travel time by 74.5\\% and total throughput by 16\\% in heavy traffic."],"url":"http://arxiv.org/abs/2410.18786v1"}
{"created":"2024-10-24 14:36:12","title":"Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality","abstract":"The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this prior work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Our theory is established based on a key observation: the DDPM update rule is equivalent to running a suitably parameterized SDE upon discretization, where the nonlinear component of the drift term is intrinsically low-dimensional.","sentences":["The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI.","While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency.","This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data.","We strengthen this prior work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality.","For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy.","Our theory is established based on a key observation: the DDPM update rule is equivalent to running a suitably parameterized SDE upon discretization, where the nonlinear component of the drift term is intrinsically low-dimensional."],"url":"http://arxiv.org/abs/2410.18784v1"}
{"created":"2024-10-24 14:31:52","title":"A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs","abstract":"A primary challenge in large language model (LLM) development is their onerous pre-training cost. Typically, such pre-training involves optimizing a self-supervised objective (such as next-token prediction) over a large corpus. This paper explores a promising paradigm to improve LLM pre-training efficiency and quality by suitably leveraging a small language model (SLM). In particular, this paradigm relies on an SLM to both (1) provide soft labels as additional training supervision, and (2) select a small subset of valuable (\"informative\" and \"hard\") training examples. Put together, this enables an effective transfer of the SLM's predictive distribution to the LLM, while prioritizing specific regions of the training data distribution. Empirically, this leads to reduced LLM training time compared to standard training, while improving the overall quality. Theoretically, we develop a statistical framework to systematically study the utility of SLMs in enabling efficient training of high-quality LLMs. In particular, our framework characterizes how the SLM's seemingly low-quality supervision can enhance the training of a much more capable LLM. Furthermore, it also highlights the need for an adaptive utilization of such supervision, by striking a balance between the bias and variance introduced by the SLM-provided soft labels. We corroborate our theoretical framework by improving the pre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B parameters on the Pile dataset.","sentences":["A primary challenge in large language model (LLM) development is their onerous pre-training cost.","Typically, such pre-training involves optimizing a self-supervised objective (such as next-token prediction) over a large corpus.","This paper explores a promising paradigm to improve LLM pre-training efficiency and quality by suitably leveraging a small language model (SLM).","In particular, this paradigm relies on an SLM to both (1) provide soft labels as additional training supervision, and (2) select a small subset of valuable (\"informative\" and \"hard\") training examples.","Put together, this enables an effective transfer of the SLM's predictive distribution to the LLM, while prioritizing specific regions of the training data distribution.","Empirically, this leads to reduced LLM training time compared to standard training, while improving the overall quality.","Theoretically, we develop a statistical framework to systematically study the utility of SLMs in enabling efficient training of high-quality LLMs.","In particular, our framework characterizes how the SLM's seemingly low-quality supervision can enhance the training of a much more capable LLM.","Furthermore, it also highlights the need for an adaptive utilization of such supervision, by striking a balance between the bias and variance introduced by the SLM-provided soft labels.","We corroborate our theoretical framework by improving the pre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B parameters on the Pile dataset."],"url":"http://arxiv.org/abs/2410.18779v1"}
{"created":"2024-10-24 14:19:38","title":"Attention-based Citywide Electric Vehicle Charging Demand Prediction Approach Considering Urban Region and Dynamic Influences","abstract":"Electric vehicle charging demand prediction is important for vacant charging pile recommendation and charging infrastructure planning, thus facilitating vehicle electrification and green energy development. The performance of previous spatio-temporal studies is still far from satisfactory because the traditional graphs are difficult to model non-pairwise spatial relationships and multivariate temporal features are not adequately taken into account. To tackle these issues, we propose an attention-based heterogeneous multivariate data fusion approach (AHMDF) for citywide electric vehicle charging demand prediction, which incorporates geo-based clustered hypergraph and multivariate gated Transformer to considers both static and dynamic influences. To learn non-pairwise relationships, we cluster service areas by the types and numbers of points of interest in the areas and develop attentive hypergraph networks accordingly. Graph attention mechanisms are used for information propagation between neighboring areas. Additionally, we improve the Transformer encoder utilizing gated mechanisms so that it can selectively learn dynamic auxiliary information and temporal features. Experiments on an electric vehicle charging benchmark dataset demonstrate the effectiveness of our proposed approach compared with a broad range of competing baselines. Furthermore, we demonstrate the impact of dynamic influences on prediction results in different areas of the city and the effectiveness of our clustering method.","sentences":["Electric vehicle charging demand prediction is important for vacant charging pile recommendation and charging infrastructure planning, thus facilitating vehicle electrification and green energy development.","The performance of previous spatio-temporal studies is still far from satisfactory because the traditional graphs are difficult to model non-pairwise spatial relationships and multivariate temporal features are not adequately taken into account.","To tackle these issues, we propose an attention-based heterogeneous multivariate data fusion approach (AHMDF) for citywide electric vehicle charging demand prediction, which incorporates geo-based clustered hypergraph and multivariate gated Transformer to considers both static and dynamic influences.","To learn non-pairwise relationships, we cluster service areas by the types and numbers of points of interest in the areas and develop attentive hypergraph networks accordingly.","Graph attention mechanisms are used for information propagation between neighboring areas.","Additionally, we improve the Transformer encoder utilizing gated mechanisms so that it can selectively learn dynamic auxiliary information and temporal features.","Experiments on an electric vehicle charging benchmark dataset demonstrate the effectiveness of our proposed approach compared with a broad range of competing baselines.","Furthermore, we demonstrate the impact of dynamic influences on prediction results in different areas of the city and the effectiveness of our clustering method."],"url":"http://arxiv.org/abs/2410.18766v1"}
{"created":"2024-10-24 14:00:28","title":"Double Auctions: Formalization and Automated Checkers","abstract":"Double auctions are widely used in financial markets, such as those for stocks, derivatives, currencies, and commodities, to match demand and supply. Once all buyers and sellers have placed their trade requests, the exchange determines how these requests are to be matched. The two most common objectives for determining the matching are maximizing trade volume at a uniform price and maximizing trade volume through dynamic pricing. Prior research has primarily focused on single-quantity trade requests. In this work, we extend the framework to handle multiple-quantity trade requests and present fully formalized matching algorithms for double auctions, along with their correctness proofs. We establish new uniqueness theorems, enabling automatic detection of violations in exchange systems by comparing their output to that of a verified program. All proofs are formalized in the Coq Proof Assistant, and we extract verified OCaml and Haskell programs that could serve as a resource for exchanges and market regulators. We demonstrate the practical applicability of our work by running the verified program on real market data from an exchange to automatically check for violations in the exchange algorithm.","sentences":["Double auctions are widely used in financial markets, such as those for stocks, derivatives, currencies, and commodities, to match demand and supply.","Once all buyers and sellers have placed their trade requests, the exchange determines how these requests are to be matched.","The two most common objectives for determining the matching are maximizing trade volume at a uniform price and maximizing trade volume through dynamic pricing.","Prior research has primarily focused on single-quantity trade requests.","In this work, we extend the framework to handle multiple-quantity trade requests and present fully formalized matching algorithms for double auctions, along with their correctness proofs.","We establish new uniqueness theorems, enabling automatic detection of violations in exchange systems by comparing their output to that of a verified program.","All proofs are formalized in the Coq Proof Assistant, and we extract verified OCaml and Haskell programs that could serve as a resource for exchanges and market regulators.","We demonstrate the practical applicability of our work by running the verified program on real market data from an exchange to automatically check for violations in the exchange algorithm."],"url":"http://arxiv.org/abs/2410.18751v1"}
{"created":"2024-10-24 13:42:05","title":"5G Replicates TSN: Extending IEEE 802.1CB Capabilities to Integrated 5G/TSN Systems","abstract":"The IEEE 802.1 time-sensitive networking (TSN) standards improve real-time capabilities of the standard Ethernet. TSN and local/private 5G systems are envisaged to co-exist in industrial environments. The IEEE 802.1CB standard provides fault tolerance to TSN systems via frame replication and elimination for reliability (FRER) capabilities. This paper presents X-FRER, a novel framework for extending FRER capabilities to the 3GPP-defined bridge model for 5G and TSN integration. The different embodiments of X-FRER realize FRER-like functionality through multi-path transmissions in a 5G system based on a single or multiple protocol data unit (PDU) sessions. X-FRER also provides enhanced replication and elimination functionality for integrated deployments. Performance evaluation shows that X-FRER empowers a vanilla 5G system with TSN-like capabilities for end-to-end reliability in integrated TSN and 5G deployments.","sentences":["The IEEE 802.1 time-sensitive networking (TSN) standards improve real-time capabilities of the standard Ethernet.","TSN and local/private 5G systems are envisaged to co-exist in industrial environments.","The IEEE 802.1CB standard provides fault tolerance to TSN systems via frame replication and elimination for reliability (FRER) capabilities.","This paper presents X-FRER, a novel framework for extending FRER capabilities to the 3GPP-defined bridge model for 5G and TSN integration.","The different embodiments of X-FRER realize FRER-like functionality through multi-path transmissions in a 5G system based on a single or multiple protocol data unit (PDU) sessions.","X-FRER also provides enhanced replication and elimination functionality for integrated deployments.","Performance evaluation shows that X-FRER empowers a vanilla 5G system with TSN-like capabilities for end-to-end reliability in integrated TSN and 5G deployments."],"url":"http://arxiv.org/abs/2410.18739v1"}
{"created":"2024-10-24 13:41:40","title":"Cellpose+, a morphological analysis tool for feature extraction of stained cell images","abstract":"Advanced image segmentation and processing tools present an opportunity to study cell processes and their dynamics. However, image analysis is often routine and time-consuming. Nowadays, alternative data-driven approaches using deep learning are potentially offering automatized, accurate, and fast image analysis. In this paper, we extend the applications of Cellpose, a state-of-the-art cell segmentation framework, with feature extraction capabilities to assess morphological characteristics. We also introduce a dataset of DAPI and FITC stained cells to which our new method is applied.","sentences":["Advanced image segmentation and processing tools present an opportunity to study cell processes and their dynamics.","However, image analysis is often routine and time-consuming.","Nowadays, alternative data-driven approaches using deep learning are potentially offering automatized, accurate, and fast image analysis.","In this paper, we extend the applications of Cellpose, a state-of-the-art cell segmentation framework, with feature extraction capabilities to assess morphological characteristics.","We also introduce a dataset of DAPI and FITC stained cells to which our new method is applied."],"url":"http://arxiv.org/abs/2410.18738v1"}
{"created":"2024-10-24 13:41:32","title":"Rectified Diffusion Guidance for Conditional Generation","abstract":"Classifier-Free Guidance (CFG), which combines the conditional and unconditional score functions with two coefficients summing to one, serves as a practical technique for diffusion model sampling. Theoretically, however, denoising with CFG cannot be expressed as a reciprocal diffusion process, which may consequently leave some hidden risks during use. In this work, we revisit the theory behind CFG and rigorously confirm that the improper configuration of the combination coefficients (i.e., the widely used summing-to-one version) brings about expectation shift of the generative distribution. To rectify this issue, we propose ReCFG with a relaxation on the guidance coefficients such that denoising with ReCFG strictly aligns with the diffusion theory. We further show that our approach enjoys a closed-form solution given the guidance strength. That way, the rectified coefficients can be readily pre-computed via traversing the observed data, leaving the sampling speed barely affected. Empirical evidence on real-world data demonstrate the compatibility of our post-hoc design with existing state-of-the-art diffusion models, including both class-conditioned ones (e.g., EDM2 on ImageNet) and text-conditioned ones (e.g., SD3 on CC12M), without any retraining. We will open-source the code to facilitate further research.","sentences":["Classifier-Free Guidance (CFG), which combines the conditional and unconditional score functions with two coefficients summing to one, serves as a practical technique for diffusion model sampling.","Theoretically, however, denoising with CFG cannot be expressed as a reciprocal diffusion process, which may consequently leave some hidden risks during use.","In this work, we revisit the theory behind CFG and rigorously confirm that the improper configuration of the combination coefficients (i.e., the widely used summing-to-one version) brings about expectation shift of the generative distribution.","To rectify this issue, we propose ReCFG with a relaxation on the guidance coefficients such that denoising with ReCFG strictly aligns with the diffusion theory.","We further show that our approach enjoys a closed-form solution given the guidance strength.","That way, the rectified coefficients can be readily pre-computed via traversing the observed data, leaving the sampling speed barely affected.","Empirical evidence on real-world data demonstrate the compatibility of our post-hoc design with existing state-of-the-art diffusion models, including both class-conditioned ones (e.g., EDM2 on ImageNet) and text-conditioned ones (e.g., SD3 on CC12M), without any retraining.","We will open-source the code to facilitate further research."],"url":"http://arxiv.org/abs/2410.18737v1"}
{"created":"2024-10-24 13:22:33","title":"Low-Latency Video Anonymization for Crowd Anomaly Detection: Privacy vs. Performance","abstract":"Recent advancements in artificial intelligence promise ample potential in monitoring applications with surveillance cameras. However, concerns about privacy and model bias have made it challenging to utilize them in public. Although de-identification approaches have been proposed in the literature, aiming to achieve a certain level of anonymization, most of them employ deep learning models that are computationally demanding for real-time edge deployment. In this study, we revisit conventional anonymization solutions for privacy protection and real-time video anomaly detection (VAD) applications. We propose a novel lightweight adaptive anonymization for VAD (LA3D) that employs dynamic adjustment to enhance privacy protection. We evaluated the approaches on publicly available privacy and VAD data sets to examine the strengths and weaknesses of the different anonymization techniques and highlight the promising efficacy of our approach. Our experiment demonstrates that LA3D enables substantial improvement in the privacy anonymization capability without majorly degrading VAD efficacy.","sentences":["Recent advancements in artificial intelligence promise ample potential in monitoring applications with surveillance cameras.","However, concerns about privacy and model bias have made it challenging to utilize them in public.","Although de-identification approaches have been proposed in the literature, aiming to achieve a certain level of anonymization, most of them employ deep learning models that are computationally demanding for real-time edge deployment.","In this study, we revisit conventional anonymization solutions for privacy protection and real-time video anomaly detection (VAD) applications.","We propose a novel lightweight adaptive anonymization for VAD (LA3D) that employs dynamic adjustment to enhance privacy protection.","We evaluated the approaches on publicly available privacy and VAD data sets to examine the strengths and weaknesses of the different anonymization techniques and highlight the promising efficacy of our approach.","Our experiment demonstrates that LA3D enables substantial improvement in the privacy anonymization capability without majorly degrading VAD efficacy."],"url":"http://arxiv.org/abs/2410.18717v1"}
{"created":"2024-10-24 13:02:46","title":"Deterministic Edge Connectivity and Max Flow using Subquadratic Cut Queries","abstract":"We give the first deterministic algorithm that makes sub-quadratic queries to find the global min-cut of a simple graph in the cut query model. Given an $n$-vertex graph $G$, our algorithm makes $\\widetilde{O}(n^{5/3})$ queries to compute the global min-cut in $G$. As a key ingredient, we also show an algorithm for finding $s$-$t$ max-flows of size $\\widetilde{O}(n)$ in $\\widetilde{O}(n^{5/3})$ queries. We also show efficient cut-query implementations of versions of expander decomposition and isolating cuts, which may be of independent interest.","sentences":["We give the first deterministic algorithm that makes sub-quadratic queries to find the global min-cut of a simple graph in the cut query model.","Given an $n$-vertex graph $G$, our algorithm makes $\\widetilde{O}(n^{5/3})$ queries to compute the global min-cut in $G$. As a key ingredient, we also show an algorithm for finding $s$-$t$ max-flows of size $\\widetilde{O}(n)$ in $\\widetilde{O}(n^{5/3})$ queries.","We also show efficient cut-query implementations of versions of expander decomposition and isolating cuts, which may be of independent interest."],"url":"http://arxiv.org/abs/2410.18704v1"}
{"created":"2024-10-24 12:56:01","title":"GrammaMT: Improving Machine Translation with Grammar-Informed In-Context Learning","abstract":"We introduce GrammaMT, a grammatically-aware prompting approach for machine translation that uses Interlinear Glossed Text (IGT), a common form of linguistic description providing morphological and lexical annotations for source sentences. GrammaMT proposes three prompting strategies: gloss-shot, chain-gloss and model-gloss. All are training-free, requiring only a few examples that involve minimal effort to collect, and making them well-suited for low-resource setups. Experiments show that GrammaMT enhances translation performance on open-source instruction-tuned LLMs for various low- to high-resource languages across three benchmarks: (1) the largest IGT corpus, (2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages, and (3) even in an out-of-domain setting with FLORES. Moreover, ablation studies reveal that leveraging gloss resources could substantially boost MT performance (by over 17 BLEU points) if LLMs accurately generate or access input sentence glosses.","sentences":["We introduce GrammaMT, a grammatically-aware prompting approach for machine translation that uses Interlinear Glossed Text (IGT), a common form of linguistic description providing morphological and lexical annotations for source sentences.","GrammaMT proposes three prompting strategies: gloss-shot, chain-gloss and model-gloss.","All are training-free, requiring only a few examples that involve minimal effort to collect, and making them well-suited for low-resource setups.","Experiments show that GrammaMT enhances translation performance on open-source instruction-tuned LLMs for various low- to high-resource languages across three benchmarks: (1) the largest IGT corpus, (2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages, and (3) even in an out-of-domain setting with FLORES.","Moreover, ablation studies reveal that leveraging gloss resources could substantially boost MT performance (by over 17 BLEU points) if LLMs accurately generate or access input sentence glosses."],"url":"http://arxiv.org/abs/2410.18702v1"}
{"created":"2024-10-24 12:42:04","title":"Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch","abstract":"The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs. Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases. Recent research indicates that continually scaling up data synthesis from strong models (e.g., GPT-4) can further elicit reasoning performance. Though promising, the open-sourced community still lacks high-quality data at scale and scalable data synthesis methods with affordable costs. To address this, we introduce ScaleQuest, a scalable and novel data synthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to generate questions from scratch without the need for seed data with complex augmentation constraints. With the efficient ScaleQuest, we automatically constructed a mathematical reasoning dataset consisting of 1 million problem-solution pairs, which are more effective than existing open-sourced datasets. It can universally increase the performance of mainstream open-source models (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2% to 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base model with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and well-aligned model on closed-source data, and proprietary models such as GPT-4-Turbo and Claude-3.5 Sonnet.","sentences":["The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs.","Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases.","Recent research indicates that continually scaling up data synthesis from strong models (e.g., GPT-4) can further elicit reasoning performance.","Though promising, the open-sourced community still lacks high-quality data at scale and scalable data synthesis methods with affordable costs.","To address this, we introduce ScaleQuest, a scalable and novel data synthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to generate questions from scratch without the need for seed data with complex augmentation constraints.","With the efficient ScaleQuest, we automatically constructed a mathematical reasoning dataset consisting of 1 million problem-solution pairs, which are more effective than existing open-sourced datasets.","It can universally increase the performance of mainstream open-source models (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2% to 46.4% gains on MATH.","Notably, simply fine-tuning the Qwen2-Math-7B-Base model with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and well-aligned model on closed-source data, and proprietary models such as GPT-4-Turbo and Claude-3.5 Sonnet."],"url":"http://arxiv.org/abs/2410.18693v1"}
{"created":"2024-10-24 12:32:22","title":"ODDN: Addressing Unpaired Data Challenges in Open-World Deepfake Detection on Online Social Networks","abstract":"Despite significant advances in deepfake detection, handling varying image quality, especially due to different compressions on online social networks (OSNs), remains challenging. Current methods succeed by leveraging correlations between paired images, whether raw or compressed. However, in open-world scenarios, paired data is scarce, with compressed images readily available but corresponding raw versions difficult to obtain. This imbalance, where unpaired data vastly outnumbers paired data, often leads to reduced detection performance, as existing methods struggle without corresponding raw images. To overcome this issue, we propose a novel approach named the open-world deepfake detection network (ODDN), which comprises two core modules: open-world data aggregation (ODA) and compression-discard gradient correction (CGC). ODA effectively aggregates correlations between compressed and raw samples through both fine-grained and coarse-grained analyses for paired and unpaired data, respectively. CGC incorporates a compression-discard gradient correction to further enhance performance across diverse compression methods in OSN. This technique optimizes the training gradient to ensure the model remains insensitive to compression variations. Extensive experiments conducted on 17 popular deepfake datasets demonstrate the superiority of the ODDN over SOTA baselines.","sentences":["Despite significant advances in deepfake detection, handling varying image quality, especially due to different compressions on online social networks (OSNs), remains challenging.","Current methods succeed by leveraging correlations between paired images, whether raw or compressed.","However, in open-world scenarios, paired data is scarce, with compressed images readily available but corresponding raw versions difficult to obtain.","This imbalance, where unpaired data vastly outnumbers paired data, often leads to reduced detection performance, as existing methods struggle without corresponding raw images.","To overcome this issue, we propose a novel approach named the open-world deepfake detection network (ODDN), which comprises two core modules: open-world data aggregation (ODA) and compression-discard gradient correction (CGC).","ODA effectively aggregates correlations between compressed and raw samples through both fine-grained and coarse-grained analyses for paired and unpaired data, respectively.","CGC incorporates a compression-discard gradient correction to further enhance performance across diverse compression methods in OSN.","This technique optimizes the training gradient to ensure the model remains insensitive to compression variations.","Extensive experiments conducted on 17 popular deepfake datasets demonstrate the superiority of the ODDN over SOTA baselines."],"url":"http://arxiv.org/abs/2410.18687v1"}
{"created":"2024-10-24 12:32:19","title":"Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification","abstract":"Leveraging large language models (LLMs) has garnered increasing attention and introduced novel perspectives in time series classification. However, existing approaches often overlook the crucial dynamic temporal information inherent in time series data and face challenges in aligning this data with textual semantics. To address these limitations, we propose HiTime, a hierarchical multi-modal model that seamlessly integrates temporal information into LLMs for multivariate time series classification (MTSC). Our model employs a hierarchical feature encoder to capture diverse aspects of time series data through both data-specific and task-specific embeddings. To facilitate semantic space alignment between time series and text, we introduce a dual-view contrastive alignment module that bridges the gap between modalities. Additionally, we adopt a hybrid prompting strategy to fine-tune the pre-trained LLM in a parameter-efficient manner. By effectively incorporating dynamic temporal features and ensuring semantic alignment, HiTime enables LLMs to process continuous time series data and achieves state-of-the-art classification performance through text generation. Extensive experiments on benchmark datasets demonstrate that HiTime significantly enhances time series classification accuracy compared to most competitive baseline methods. Our findings highlight the potential of integrating temporal features into LLMs, paving the way for advanced time series analysis. The code is publicly available for further research and validation. Our codes are publicly available1.","sentences":["Leveraging large language models (LLMs) has garnered increasing attention and introduced novel perspectives in time series classification.","However, existing approaches often overlook the crucial dynamic temporal information inherent in time series data and face challenges in aligning this data with textual semantics.","To address these limitations, we propose HiTime, a hierarchical multi-modal model that seamlessly integrates temporal information into LLMs for multivariate time series classification (MTSC).","Our model employs a hierarchical feature encoder to capture diverse aspects of time series data through both data-specific and task-specific embeddings.","To facilitate semantic space alignment between time series and text, we introduce a dual-view contrastive alignment module that bridges the gap between modalities.","Additionally, we adopt a hybrid prompting strategy to fine-tune the pre-trained LLM in a parameter-efficient manner.","By effectively incorporating dynamic temporal features and ensuring semantic alignment, HiTime enables LLMs to process continuous time series data and achieves state-of-the-art classification performance through text generation.","Extensive experiments on benchmark datasets demonstrate that HiTime significantly enhances time series classification accuracy compared to most competitive baseline methods.","Our findings highlight the potential of integrating temporal features into LLMs, paving the way for advanced time series analysis.","The code is publicly available for further research and validation.","Our codes are publicly available1."],"url":"http://arxiv.org/abs/2410.18686v1"}
{"created":"2024-10-24 12:24:27","title":"Rigid Single-Slice-in-Volume registration via rotation-equivariant 2D/3D feature matching","abstract":"2D to 3D registration is essential in tasks such as diagnosis, surgical navigation, environmental understanding, navigation in robotics, autonomous systems, or augmented reality. In medical imaging, the aim is often to place a 2D image in a 3D volumetric observation to w. Current approaches for rigid single slice in volume registration are limited by requirements such as pose initialization, stacks of adjacent slices, or reliable anatomical landmarks. Here, we propose a self-supervised 2D/3D registration approach to match a single 2D slice to the corresponding 3D volume. The method works in data without anatomical priors such as images of tumors. It addresses the dimensionality disparity and establishes correspondences between 2D in-plane and 3D out-of-plane rotation-equivariant features by using group equivariant CNNs. These rotation-equivariant features are extracted from the 2D query slice and aligned with their 3D counterparts. Results demonstrate the robustness of the proposed slice-in-volume registration on the NSCLC-Radiomics CT and KIRBY21 MRI datasets, attaining an absolute median angle error of less than 2 degrees and a mean-matching feature accuracy of 89% at a tolerance of 3 pixels.","sentences":["2D to 3D registration is essential in tasks such as diagnosis, surgical navigation, environmental understanding, navigation in robotics, autonomous systems, or augmented reality.","In medical imaging, the aim is often to place a 2D image in a 3D volumetric observation to w. Current approaches for rigid single slice in volume registration are limited by requirements such as pose initialization, stacks of adjacent slices, or reliable anatomical landmarks.","Here, we propose a self-supervised 2D/3D registration approach to match a single 2D slice to the corresponding 3D volume.","The method works in data without anatomical priors such as images of tumors.","It addresses the dimensionality disparity and establishes correspondences between 2D in-plane and 3D out-of-plane rotation-equivariant features by using group equivariant CNNs.","These rotation-equivariant features are extracted from the 2D query slice and aligned with their 3D counterparts.","Results demonstrate the robustness of the proposed slice-in-volume registration on the NSCLC-Radiomics CT and KIRBY21 MRI datasets, attaining an absolute median angle error of less than 2 degrees and a mean-matching feature accuracy of 89% at a tolerance of 3 pixels."],"url":"http://arxiv.org/abs/2410.18683v1"}
{"created":"2024-10-24 12:12:46","title":"Ali-AUG: Innovative Approaches to Labeled Data Augmentation using One-Step Diffusion Model","abstract":"This paper introduces Ali-AUG, a novel single-step diffusion model for efficient labeled data augmentation in industrial applications. Our method addresses the challenge of limited labeled data by generating synthetic, labeled images with precise feature insertion. Ali-AUG utilizes a stable diffusion architecture enhanced with skip connections and LoRA modules to efficiently integrate masks and images, ensuring accurate feature placement without affecting unrelated image content. Experimental validation across various industrial datasets demonstrates Ali-AUG's superiority in generating high-quality, defect-enhanced images while maintaining rapid single-step inference. By offering precise control over feature insertion and minimizing required training steps, our technique significantly enhances data augmentation capabilities, providing a powerful tool for improving the performance of deep learning models in scenarios with limited labeled data. Ali-AUG is especially useful for use cases like defective product image generation to train AI-based models to improve their ability to detect defects in manufacturing processes. Using different data preparation strategies, including Classification Accuracy Score (CAS) and Naive Augmentation Score (NAS), we show that Ali-AUG improves model performance by 31% compared to other augmentation methods and by 45% compared to models without data augmentation. Notably, Ali-AUG reduces training time by 32% and supports both paired and unpaired datasets, enhancing flexibility in data preparation.","sentences":["This paper introduces Ali-AUG, a novel single-step diffusion model for efficient labeled data augmentation in industrial applications.","Our method addresses the challenge of limited labeled data by generating synthetic, labeled images with precise feature insertion.","Ali-AUG utilizes a stable diffusion architecture enhanced with skip connections and LoRA modules to efficiently integrate masks and images, ensuring accurate feature placement without affecting unrelated image content.","Experimental validation across various industrial datasets demonstrates Ali-AUG's superiority in generating high-quality, defect-enhanced images while maintaining rapid single-step inference.","By offering precise control over feature insertion and minimizing required training steps, our technique significantly enhances data augmentation capabilities, providing a powerful tool for improving the performance of deep learning models in scenarios with limited labeled data.","Ali-AUG is especially useful for use cases like defective product image generation to train AI-based models to improve their ability to detect defects in manufacturing processes.","Using different data preparation strategies, including Classification Accuracy Score (CAS) and Naive Augmentation Score (NAS), we show that Ali-AUG improves model performance by 31% compared to other augmentation methods and by 45% compared to models without data augmentation.","Notably, Ali-AUG reduces training time by 32% and supports both paired and unpaired datasets, enhancing flexibility in data preparation."],"url":"http://arxiv.org/abs/2410.18678v1"}
{"created":"2024-10-24 12:11:52","title":"Enhancing pretraining efficiency for medical image segmentation via transferability metrics","abstract":"In medical image segmentation tasks, the scarcity of labeled training data poses a significant challenge when training deep neural networks. When using U-Net-style architectures, it is common practice to address this problem by pretraining the encoder part on a large general-purpose dataset like ImageNet. However, these methods are resource-intensive and do not guarantee improved performance on the downstream task. In this paper we investigate a variety of training setups on medical image segmentation datasets, using ImageNet-pretrained models. By examining over 300 combinations of models, datasets, and training methods, we find that shorter pretraining often leads to better results on the downstream task, providing additional proof to the well-known fact that the accuracy of the model on ImageNet is a poor indicator for downstream performance. As our main contribution, we introduce a novel transferability metric, based on contrastive learning, that measures how robustly a pretrained model is able to represent the target data. In contrast to other transferability scores, our method is applicable to the case of transferring from ImageNet classification to medical image segmentation. We apply our robustness score by measuring it throughout the pretraining phase to indicate when the model weights are optimal for downstream transfer. This reduces pretraining time and improves results on the target task.","sentences":["In medical image segmentation tasks, the scarcity of labeled training data poses a significant challenge when training deep neural networks.","When using U-Net-style architectures, it is common practice to address this problem by pretraining the encoder part on a large general-purpose dataset like ImageNet.","However, these methods are resource-intensive and do not guarantee improved performance on the downstream task.","In this paper we investigate a variety of training setups on medical image segmentation datasets, using ImageNet-pretrained models.","By examining over 300 combinations of models, datasets, and training methods, we find that shorter pretraining often leads to better results on the downstream task, providing additional proof to the well-known fact that the accuracy of the model on ImageNet is a poor indicator for downstream performance.","As our main contribution, we introduce a novel transferability metric, based on contrastive learning, that measures how robustly a pretrained model is able to represent the target data.","In contrast to other transferability scores, our method is applicable to the case of transferring from ImageNet classification to medical image segmentation.","We apply our robustness score by measuring it throughout the pretraining phase to indicate when the model weights are optimal for downstream transfer.","This reduces pretraining time and improves results on the target task."],"url":"http://arxiv.org/abs/2410.18677v1"}
{"created":"2024-10-24 11:59:32","title":"3D Shape Completion with Test-Time Training","abstract":"This work addresses the problem of \\textit{shape completion}, i.e., the task of restoring incomplete shapes by predicting their missing parts. While previous works have often predicted the fractured and restored shape in one step, we approach the task by separately predicting the fractured and newly restored parts, but ensuring these predictions are interconnected. We use a decoder network motivated by related work on the prediction of signed distance functions (DeepSDF). In particular, our representation allows us to consider test-time-training, i.e., finetuning network parameters to match the given incomplete shape more accurately during inference. While previous works often have difficulties with artifacts around the fracture boundary, we demonstrate that our overfitting to the fractured parts leads to significant improvements in the restoration of eight different shape categories of the ShapeNet data set in terms of their chamfer distances.","sentences":["This work addresses the problem of \\textit{shape completion}, i.e., the task of restoring incomplete shapes by predicting their missing parts.","While previous works have often predicted the fractured and restored shape in one step, we approach the task by separately predicting the fractured and newly restored parts, but ensuring these predictions are interconnected.","We use a decoder network motivated by related work on the prediction of signed distance functions (DeepSDF).","In particular, our representation allows us to consider test-time-training, i.e., finetuning network parameters to match the given incomplete shape more accurately during inference.","While previous works often have difficulties with artifacts around the fracture boundary, we demonstrate that our overfitting to the fractured parts leads to significant improvements in the restoration of eight different shape categories of the ShapeNet data set in terms of their chamfer distances."],"url":"http://arxiv.org/abs/2410.18668v1"}
{"created":"2024-10-24 11:57:20","title":"DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation","abstract":"Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models will be available at: https://github.com/shallowdream204/DreamClear.","sentences":["Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets.","To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model.","GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models.","GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering.","This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction.","The result is a large-scale dataset of one million high-quality images.","Our second contribution, DreamClear, is a DiT-based image restoration model.","It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration.","To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM).","It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address.","Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration.","Code and pre-trained models will be available at: https://github.com/shallowdream204/DreamClear."],"url":"http://arxiv.org/abs/2410.18666v1"}
{"created":"2024-10-24 11:46:23","title":"Recognizing Sumsets is NP-Complete","abstract":"Sumsets are central objects in additive combinatorics. In 2007, Granville asked whether one can efficiently recognize whether a given set $S$ is a sumset, i.e. whether there is a set $A$ such that $A+A=S$. Granville suggested an algorithm that takes exponential time in the size of the given set, but can we do polynomial or even linear time? This basic computational question is indirectly asking a fundamental structural question: do the special characteristics of sumsets allow them to be efficiently recognizable? In this paper, we answer this question negatively by proving that the problem is NP-complete. Specifically, our results hold for integer sets and over any finite field. Assuming the Exponential Time Hypothesis, our lower bound becomes $2^{\\Omega(n^{1/4})}$.","sentences":["Sumsets are central objects in additive combinatorics.","In 2007, Granville asked whether one can efficiently recognize whether a given set $S$ is a sumset, i.e. whether there is a set $A$ such that $A+A=S$. Granville suggested an algorithm that takes exponential time in the size of the given set, but can we do polynomial or even linear time?","This basic computational question is indirectly asking a fundamental structural question: do the special characteristics of sumsets allow them to be efficiently recognizable?","In this paper, we answer this question negatively by proving that the problem is NP-complete.","Specifically, our results hold for integer sets and over any finite field.","Assuming the Exponential Time Hypothesis, our lower bound becomes $2^{\\Omega(n^{1/4})}$."],"url":"http://arxiv.org/abs/2410.18661v1"}
{"created":"2024-10-24 11:36:19","title":"NIDS Neural Networks Using Sliding Time Window Data Processing with Trainable Activations and its Generalization Capability","abstract":"This paper presents neural networks for network intrusion detection systems (NIDS), that operate on flow data preprocessed with a time window. It requires only eleven features which do not rely on deep packet inspection and can be found in most NIDS datasets and easily obtained from conventional flow collectors. The time window aggregates information with respect to hosts facilitating the identification of flow signatures that are missed by other aggregation methods. Several network architectures are studied and the use of Kalmogorov-Arnold Network (KAN)-inspired trainable activation functions that help to achieve higher accuracy with simpler network structure is proposed. The reported training accuracy exceeds 99% for the proposed method with as little as twenty neural network input features. This work also studies the generalization capability of NIDS, a crucial aspect that has not been adequately addressed in the previous studies. The generalization experiments are conducted using CICIDS2017 dataset and a custom dataset collected as part of this study. It is shown that the performance metrics decline significantly when changing datasets, and the reduction in performance metrics can be attributed to the difference in signatures of the same type flows in different datasets, which in turn can be attributed to the differences between the underlying networks. It is shown that the generalization accuracy of some neural networks can be very unstable and sensitive to random initialization parameters, and neural networks with fewer parameters and well-tuned activations are more stable and achieve higher accuracy.","sentences":["This paper presents neural networks for network intrusion detection systems (NIDS), that operate on flow data preprocessed with a time window.","It requires only eleven features which do not rely on deep packet inspection and can be found in most NIDS datasets and easily obtained from conventional flow collectors.","The time window aggregates information with respect to hosts facilitating the identification of flow signatures that are missed by other aggregation methods.","Several network architectures are studied and the use of Kalmogorov-Arnold Network (KAN)-inspired trainable activation functions that help to achieve higher accuracy with simpler network structure is proposed.","The reported training accuracy exceeds 99% for the proposed method with as little as twenty neural network input features.","This work also studies the generalization capability of NIDS, a crucial aspect that has not been adequately addressed in the previous studies.","The generalization experiments are conducted using CICIDS2017 dataset and a custom dataset collected as part of this study.","It is shown that the performance metrics decline significantly when changing datasets, and the reduction in performance metrics can be attributed to the difference in signatures of the same type flows in different datasets, which in turn can be attributed to the differences between the underlying networks.","It is shown that the generalization accuracy of some neural networks can be very unstable and sensitive to random initialization parameters, and neural networks with fewer parameters and well-tuned activations are more stable and achieve higher accuracy."],"url":"http://arxiv.org/abs/2410.18658v1"}
{"created":"2024-10-24 11:32:00","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","abstract":"Generating high-quality charts with Large Language Models presents significant challenges due to limited data and the high cost of scaling through human curation. Instruction, data, and code triplets are scarce and expensive to manually curate as their creation demands technical expertise. To address this scalability issue, we introduce a reference-free automatic feedback generator, which eliminates the need for costly human intervention. Our novel framework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are compelling: in our first experiment, 74% of respondents strongly preferred, and 10% preferred, the results after feedback. The second post-feedback experiment demonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K significantly improves data diversity by increasing queries, datasets, and chart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally, an LLM user study revealed that 94% of participants preferred ChartUIE-8K's queries, with 93% deeming them aligned with real-world use cases. Core contributions are available as open-source at an anonymized project site, with ample qualitative examples.","sentences":["Generating high-quality charts with Large Language Models presents significant challenges due to limited data and the high cost of scaling through human curation.","Instruction, data, and code triplets are scarce and expensive to manually curate as their creation demands technical expertise.","To address this scalability issue, we introduce a reference-free automatic feedback generator, which eliminates the need for costly human intervention.","Our novel framework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset (ChartUIE-8K).","Quantitative results are compelling: in our first experiment, 74% of respondents strongly preferred, and 10% preferred, the results after feedback.","The second post-feedback experiment demonstrates that ChartAF outperforms nine baselines.","Moreover, ChartUIE-8K significantly improves data diversity by increasing queries, datasets, and chart types by 5982%, 1936%, and 91%, respectively, over benchmarks.","Finally, an LLM user study revealed that 94% of participants preferred ChartUIE-8K's queries, with 93% deeming them aligned with real-world use cases.","Core contributions are available as open-source at an anonymized project site, with ample qualitative examples."],"url":"http://arxiv.org/abs/2410.18652v1"}
{"created":"2024-10-24 11:25:50","title":"Counting Locally Optimal Tours in the TSP","abstract":"We show that the problem of counting the number of 2-optimal tours in instances of the Travelling Salesperson Problem (TSP) on complete graphs is #P-complete. In addition, we show that the expected number of 2-optimal tours in random instances of the TSP on complete graphs is $O(1.2098^n \\sqrt{n!})$. Based on numerical experiments, we conjecture that the true bound is at most $O(\\sqrt{n!})$, which is approximately the square root of the total number of tours.","sentences":["We show that the problem of counting the number of 2-optimal tours in instances of the Travelling Salesperson Problem (TSP) on complete graphs is #P-complete.","In addition, we show that the expected number of 2-optimal tours in random instances of the TSP on complete graphs is $O(1.2098^n \\sqrt{n!})$. Based on numerical experiments, we conjecture that the true bound is at most $O(\\sqrt{n!})$, which is approximately the square root of the total number of tours."],"url":"http://arxiv.org/abs/2410.18650v1"}
{"created":"2024-10-24 11:21:49","title":"GADT: Enhancing Transferable Adversarial Attacks through Gradient-guided Adversarial Data Transformation","abstract":"Current Transferable Adversarial Examples (TAE) are primarily generated by adding Adversarial Noise (AN). Recent studies emphasize the importance of optimizing Data Augmentation (DA) parameters along with AN, which poses a greater threat to real-world AI applications. However, existing DA-based strategies often struggle to find optimal solutions due to the challenging DA search procedure without proper guidance. In this work, we propose a novel DA-based attack algorithm, GADT. GADT identifies suitable DA parameters through iterative antagonism and uses posterior estimates to update AN based on these parameters. We uniquely employ a differentiable DA operation library to identify adversarial DA parameters and introduce a new loss function as a metric during DA optimization. This loss term enhances adversarial effects while preserving the original image content, maintaining attack crypticity. Extensive experiments on public datasets with various networks demonstrate that GADT can be integrated with existing transferable attack methods, updating their DA parameters effectively while retaining their AN formulation strategies. Furthermore, GADT can be utilized in other black-box attack scenarios, e.g., query-based attacks, offering a new avenue to enhance attacks on real-world AI applications in both research and industrial contexts.","sentences":["Current Transferable Adversarial Examples (TAE) are primarily generated by adding Adversarial Noise (AN).","Recent studies emphasize the importance of optimizing Data Augmentation (DA) parameters along with AN, which poses a greater threat to real-world AI applications.","However, existing DA-based strategies often struggle to find optimal solutions due to the challenging DA search procedure without proper guidance.","In this work, we propose a novel DA-based attack algorithm, GADT.","GADT identifies suitable DA parameters through iterative antagonism and uses posterior estimates to update AN based on these parameters.","We uniquely employ a differentiable DA operation library to identify adversarial DA parameters and introduce a new loss function as a metric during DA optimization.","This loss term enhances adversarial effects while preserving the original image content, maintaining attack crypticity.","Extensive experiments on public datasets with various networks demonstrate that GADT can be integrated with existing transferable attack methods, updating their DA parameters effectively while retaining their AN formulation strategies.","Furthermore, GADT can be utilized in other black-box attack scenarios, e.g., query-based attacks, offering a new avenue to enhance attacks on real-world AI applications in both research and industrial contexts."],"url":"http://arxiv.org/abs/2410.18648v1"}
{"created":"2024-10-24 11:19:30","title":"Data Scaling Laws in Imitation Learning for Robotic Manipulation","abstract":"Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy's generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90% success rates in novel environments with unseen objects.","sentences":["Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities.","In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment.","To this end, we conduct a comprehensive empirical study on data scaling in imitation learning.","By collecting data across numerous environments and objects, we study how a policy's generalization performance changes with the number of training environments, objects, and demonstrations.","Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol.","Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects.","The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect.","Based on these insights, we propose an efficient data collection strategy.","With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90% success rates in novel environments with unseen objects."],"url":"http://arxiv.org/abs/2410.18647v1"}
{"created":"2024-10-24 11:10:54","title":"Smart ETL and LLM-based contents classification: the European Smart Tourism Tools Observatory experience","abstract":"Purpose: Our research project focuses on improving the content update of the online European Smart Tourism Tools (STTs) Observatory by incorporating and categorizing STTs. The categorization is based on their taxonomy, and it facilitates the end user's search process. The use of a Smart ETL (Extract, Transform, and Load) process, where \\emph{Smart} indicates the use of Artificial Intelligence (AI), is central to this endeavor.   Methods: The contents describing STTs are derived from PDF catalogs, where PDF-scraping techniques extract QR codes, images, links, and text information. Duplicate STTs between the catalogs are removed, and the remaining ones are classified based on their text information using Large Language Models (LLMs). Finally, the data is transformed to comply with the Dublin Core metadata structure (the observatory's metadata structure), chosen for its wide acceptance and flexibility.   Results: The Smart ETL process to import STTs to the observatory combines PDF-scraping techniques with LLMs for text content-based classification. Our preliminary results have demonstrated the potential of LLMs for text content-based classification.   Conclusion: The proposed approach's feasibility is a step towards efficient content-based classification, not only in Smart Tourism but also adaptable to other fields. Future work will mainly focus on refining this classification process.","sentences":["Purpose: Our research project focuses on improving the content update of the online European Smart Tourism Tools (STTs) Observatory by incorporating and categorizing STTs.","The categorization is based on their taxonomy, and it facilitates the end user's search process.","The use of a Smart ETL (Extract, Transform, and Load) process, where \\emph{Smart} indicates the use of Artificial Intelligence (AI), is central to this endeavor.   ","Methods: The contents describing STTs are derived from PDF catalogs, where PDF-scraping techniques extract QR codes, images, links, and text information.","Duplicate STTs between the catalogs are removed, and the remaining ones are classified based on their text information using Large Language Models (LLMs).","Finally, the data is transformed to comply with the Dublin Core metadata structure (the observatory's metadata structure), chosen for its wide acceptance and flexibility.   ","Results:","The Smart ETL process to import STTs to the observatory combines PDF-scraping techniques with LLMs for text content-based classification.","Our preliminary results have demonstrated the potential of LLMs for text content-based classification.   ","Conclusion: The proposed approach's feasibility is a step towards efficient content-based classification, not only in Smart Tourism but also adaptable to other fields.","Future work will mainly focus on refining this classification process."],"url":"http://arxiv.org/abs/2410.18641v1"}
{"created":"2024-10-24 10:58:17","title":"Diffusion Attribution Score: Evaluating Training Data Influence in Diffusion Model","abstract":"As diffusion models become increasingly popular, the misuse of copyrighted and private images has emerged as a major concern. One promising solution to mitigate this issue is identifying the contribution of specific training samples in generative models, a process known as data attribution. Existing data attribution methods for diffusion models typically quantify the contribution of a training sample by evaluating the change in diffusion loss when the sample is included or excluded from the training process. However, we argue that the direct usage of diffusion loss cannot represent such a contribution accurately due to the calculation of diffusion loss. Specifically, these approaches measure the divergence between predicted and ground truth distributions, which leads to an indirect comparison between the predicted distributions and cannot represent the variances between model behaviors. To address these issues, we aim to measure the direct comparison between predicted distributions with an attribution score to analyse the training sample importance, which is achieved by Diffusion Attribution Score (DAS). Underpinned by rigorous theoretical analysis, we elucidate the effectiveness of DAS. Additionally, we explore strategies to accelerate DAS calculations, facilitating its application to large-scale diffusion models. Our extensive experiments across various datasets and diffusion models demonstrate that DAS significantly surpasses previous benchmarks in terms of the linear data-modelling score, establishing new state-of-the-art performance.","sentences":["As diffusion models become increasingly popular, the misuse of copyrighted and private images has emerged as a major concern.","One promising solution to mitigate this issue is identifying the contribution of specific training samples in generative models, a process known as data attribution.","Existing data attribution methods for diffusion models typically quantify the contribution of a training sample by evaluating the change in diffusion loss when the sample is included or excluded from the training process.","However, we argue that the direct usage of diffusion loss cannot represent such a contribution accurately due to the calculation of diffusion loss.","Specifically, these approaches measure the divergence between predicted and ground truth distributions, which leads to an indirect comparison between the predicted distributions and cannot represent the variances between model behaviors.","To address these issues, we aim to measure the direct comparison between predicted distributions with an attribution score to analyse the training sample importance, which is achieved by Diffusion Attribution Score (DAS).","Underpinned by rigorous theoretical analysis, we elucidate the effectiveness of DAS.","Additionally, we explore strategies to accelerate DAS calculations, facilitating its application to large-scale diffusion models.","Our extensive experiments across various datasets and diffusion models demonstrate that DAS significantly surpasses previous benchmarks in terms of the linear data-modelling score, establishing new state-of-the-art performance."],"url":"http://arxiv.org/abs/2410.18639v1"}
{"created":"2024-10-24 10:56:02","title":"Moving Object Segmentation in Point Cloud Data using Hidden Markov Models","abstract":"Autonomous agents require the capability to identify dynamic objects in their environment for safe planning and navigation. Incomplete and erroneous dynamic detections jeopardize the agent's ability to accomplish its task. Dynamic detection is a challenging problem due to the numerous sources of uncertainty inherent in the problem's inputs and the wide variety of applications, which often lead to use-case-tailored solutions. We propose a robust learning-free approach to segment moving objects in point cloud data. The foundation of the approach lies in modelling each voxel using a hidden Markov model (HMM), and probabilistically integrating beliefs into a map using an HMM filter. The proposed approach is tested on benchmark datasets and consistently performs better than or as well as state-of-the-art methods with strong generalized performance across sensor characteristics and environments. The approach is open-sourced at https://github.com/vb44/HMM-MOS.","sentences":["Autonomous agents require the capability to identify dynamic objects in their environment for safe planning and navigation.","Incomplete and erroneous dynamic detections jeopardize the agent's ability to accomplish its task.","Dynamic detection is a challenging problem due to the numerous sources of uncertainty inherent in the problem's inputs and the wide variety of applications, which often lead to use-case-tailored solutions.","We propose a robust learning-free approach to segment moving objects in point cloud data.","The foundation of the approach lies in modelling each voxel using a hidden Markov model (HMM), and probabilistically integrating beliefs into a map using an HMM filter.","The proposed approach is tested on benchmark datasets and consistently performs better than or as well as state-of-the-art methods with strong generalized performance across sensor characteristics and environments.","The approach is open-sourced at https://github.com/vb44/HMM-MOS."],"url":"http://arxiv.org/abs/2410.18638v1"}
{"created":"2024-10-24 10:47:30","title":"Little Giants: Synthesizing High-Quality Embedding Data at Scale","abstract":"Synthetic data generation has become an increasingly popular way of training models without the need for large, manually labeled datasets. For tasks like text embedding, synthetic data offers diverse and scalable training examples, significantly reducing the cost of human annotation. However, most current approaches rely heavily on proprietary models like GPT-4, which are expensive and inefficient for generating large-scale embedding data. In this paper, we introduce SPEED, a framework that aligns open-source small models (8B) to efficiently generate large-scale synthetic embedding data. Through supervised fine-tuning, preference optimization, and self-improvement, SPEED enables small open-source models to produce high-quality data. Remarkably, SPEED uses only less than 1/10 of the GPT API calls, outperforming the state-of-the-art embedding model E5_mistral when both are trained solely on their synthetic data. Using this efficient generator, we conduct a comprehensive study on how various factors within the alignment pipeline impact data quality and reveal the scaling law for synthetic embedding data.","sentences":["Synthetic data generation has become an increasingly popular way of training models without the need for large, manually labeled datasets.","For tasks like text embedding, synthetic data offers diverse and scalable training examples, significantly reducing the cost of human annotation.","However, most current approaches rely heavily on proprietary models like GPT-4, which are expensive and inefficient for generating large-scale embedding data.","In this paper, we introduce SPEED, a framework that aligns open-source small models (8B) to efficiently generate large-scale synthetic embedding data.","Through supervised fine-tuning, preference optimization, and self-improvement, SPEED enables small open-source models to produce high-quality data.","Remarkably, SPEED uses only less than 1/10 of the GPT API calls, outperforming the state-of-the-art embedding model E5_mistral when both are trained solely on their synthetic data.","Using this efficient generator, we conduct a comprehensive study on how various factors within the alignment pipeline impact data quality and reveal the scaling law for synthetic embedding data."],"url":"http://arxiv.org/abs/2410.18634v1"}
{"created":"2024-10-24 10:37:54","title":"Wavetable Synthesis Using CVAE for Timbre Control Based on Semantic Label","abstract":"Synthesizers are essential in modern music production. However, their complex timbre parameters, often filled with technical terms, require expertise. This research introduces a method of timbre control in wavetable synthesis that is intuitive and sensible and utilizes semantic labels. Using a conditional variational autoencoder (CVAE), users can select a wavetable and define the timbre with labels such as bright, warm, and rich. The CVAE model, featuring convolutional and upsampling layers, effectively captures the wavetable nuances, ensuring real-time performance owing to their processing in the time domain. Experiments demonstrate that this approach allows for real-time, effective control of the timbre of the wavetable using semantic inputs and aims for intuitive timbre control through data-based semantic control.","sentences":["Synthesizers are essential in modern music production.","However, their complex timbre parameters, often filled with technical terms, require expertise.","This research introduces a method of timbre control in wavetable synthesis that is intuitive and sensible and utilizes semantic labels.","Using a conditional variational autoencoder (CVAE), users can select a wavetable and define the timbre with labels such as bright, warm, and rich.","The CVAE model, featuring convolutional and upsampling layers, effectively captures the wavetable nuances, ensuring real-time performance owing to their processing in the time domain.","Experiments demonstrate that this approach allows for real-time, effective control of the timbre of the wavetable using semantic inputs and aims for intuitive timbre control through data-based semantic control."],"url":"http://arxiv.org/abs/2410.18628v1"}
{"created":"2024-10-24 10:35:02","title":"SAMG: State-Action-Aware Offline-to-Online Reinforcement Learning with Offline Model Guidance","abstract":"The offline-to-online (O2O) paradigm in reinforcement learning (RL) utilizes pre-trained models on offline datasets for subsequent online fine-tuning. However, conventional O2O RL algorithms typically require maintaining and retraining the large offline datasets to mitigate the effects of out-of-distribution (OOD) data, which limits their efficiency in exploiting online samples. To address this challenge, we introduce a new paradigm called SAMG: State-Action-Conditional Offline-to-Online Reinforcement Learning with Offline Model Guidance. In particular, rather than directly training on offline data, SAMG freezes the pre-trained offline critic to provide offline values for each state-action pair to deliver compact offline information. This framework eliminates the need for retraining with offline data by freezing and leveraging these values of the offline model. These are then incorporated with the online target critic using a Bellman equation weighted by a policy state-action-aware coefficient. This coefficient, derived from a conditional variational auto-encoder (C-VAE), aims to capture the reliability of the offline data on a state-action level. SAMG could be easily integrated with existing Q-function based O2O RL algorithms. Theoretical analysis shows good optimality and lower estimation error of SAMG. Empirical evaluations demonstrate that SAMG outperforms four state-of-the-art O2O RL algorithms in the D4RL benchmark.","sentences":["The offline-to-online (O2O) paradigm in reinforcement learning (RL) utilizes pre-trained models on offline datasets for subsequent online fine-tuning.","However, conventional O2O RL algorithms typically require maintaining and retraining the large offline datasets to mitigate the effects of out-of-distribution (OOD) data, which limits their efficiency in exploiting online samples.","To address this challenge, we introduce a new paradigm called SAMG: State-Action-Conditional Offline-to-Online Reinforcement Learning with Offline Model Guidance.","In particular, rather than directly training on offline data, SAMG freezes the pre-trained offline critic to provide offline values for each state-action pair to deliver compact offline information.","This framework eliminates the need for retraining with offline data by freezing and leveraging these values of the offline model.","These are then incorporated with the online target critic using a Bellman equation weighted by a policy state-action-aware coefficient.","This coefficient, derived from a conditional variational auto-encoder (C-VAE), aims to capture the reliability of the offline data on a state-action level.","SAMG could be easily integrated with existing Q-function based O2O RL algorithms.","Theoretical analysis shows good optimality and lower estimation error of SAMG.","Empirical evaluations demonstrate that SAMG outperforms four state-of-the-art O2O RL algorithms in the D4RL benchmark."],"url":"http://arxiv.org/abs/2410.18626v1"}
{"created":"2024-10-24 10:32:10","title":"Prompting and Fine-Tuning of Small LLMs for Length-Controllable Telephone Call Summarization","abstract":"This paper explores the rapid development of a telephone call summarization system utilizing large language models (LLMs). Our approach involves initial experiments with prompting existing LLMs to generate summaries of telephone conversations, followed by the creation of a tailored synthetic training dataset utilizing stronger frontier models. We place special focus on the diversity of the generated data and on the ability to control the length of the generated summaries to meet various use-case specific requirements. The effectiveness of our method is evaluated using two state-of-the-art LLM-as-a-judge-based evaluation techniques to ensure the quality and relevance of the summaries. Our results show that fine-tuned Llama-2-7B-based summarization model performs on-par with GPT-4 in terms of factual accuracy, completeness and conciseness. Our findings demonstrate the potential for quickly bootstrapping a practical and efficient call summarization system.","sentences":["This paper explores the rapid development of a telephone call summarization system utilizing large language models (LLMs).","Our approach involves initial experiments with prompting existing LLMs to generate summaries of telephone conversations, followed by the creation of a tailored synthetic training dataset utilizing stronger frontier models.","We place special focus on the diversity of the generated data and on the ability to control the length of the generated summaries to meet various use-case specific requirements.","The effectiveness of our method is evaluated using two state-of-the-art LLM-as-a-judge-based evaluation techniques to ensure the quality and relevance of the summaries.","Our results show that fine-tuned Llama-2-7B-based summarization model performs on-par with GPT-4 in terms of factual accuracy, completeness and conciseness.","Our findings demonstrate the potential for quickly bootstrapping a practical and efficient call summarization system."],"url":"http://arxiv.org/abs/2410.18624v1"}
{"created":"2024-10-24 10:08:05","title":"TripCast: Pre-training of Masked 2D Transformers for Trip Time Series Forecasting","abstract":"Deep learning and pre-trained models have shown great success in time series forecasting. However, in the tourism industry, time series data often exhibit a leading time property, presenting a 2D structure. This introduces unique challenges for forecasting in this sector. In this study, we propose a novel modelling paradigm, TripCast, which treats trip time series as 2D data and learns representations through masking and reconstruction processes. Pre-trained on large-scale real-world data, TripCast notably outperforms other state-of-the-art baselines in in-domain forecasting scenarios and demonstrates strong scalability and transferability in out-domain forecasting scenarios.","sentences":["Deep learning and pre-trained models have shown great success in time series forecasting.","However, in the tourism industry, time series data often exhibit a leading time property, presenting a 2D structure.","This introduces unique challenges for forecasting in this sector.","In this study, we propose a novel modelling paradigm, TripCast, which treats trip time series as 2D data and learns representations through masking and reconstruction processes.","Pre-trained on large-scale real-world data, TripCast notably outperforms other state-of-the-art baselines in in-domain forecasting scenarios and demonstrates strong scalability and transferability in out-domain forecasting scenarios."],"url":"http://arxiv.org/abs/2410.18612v1"}
{"created":"2024-10-24 10:05:41","title":"Learning Transparent Reward Models via Unsupervised Feature Selection","abstract":"In complex real-world tasks such as robotic manipulation and autonomous driving, collecting expert demonstrations is often more straightforward than specifying precise learning objectives and task descriptions. Learning from expert data can be achieved through behavioral cloning or by learning a reward function, i.e., inverse reinforcement learning. The latter allows for training with additional data outside the training distribution, guided by the inferred reward function. We propose a novel approach to construct compact and transparent reward models from automatically selected state features. These inferred rewards have an explicit form and enable the learning of policies that closely match expert behavior by training standard reinforcement learning algorithms from scratch. We validate our method's performance in various robotic environments with continuous and high-dimensional state spaces. Webpage: \\url{https://sites.google.com/view/transparent-reward}.","sentences":["In complex real-world tasks such as robotic manipulation and autonomous driving, collecting expert demonstrations is often more straightforward than specifying precise learning objectives and task descriptions.","Learning from expert data can be achieved through behavioral cloning or by learning a reward function, i.e., inverse reinforcement learning.","The latter allows for training with additional data outside the training distribution, guided by the inferred reward function.","We propose a novel approach to construct compact and transparent reward models from automatically selected state features.","These inferred rewards have an explicit form and enable the learning of policies that closely match expert behavior by training standard reinforcement learning algorithms from scratch.","We validate our method's performance in various robotic environments with continuous and high-dimensional state spaces.","Webpage: \\url{https://sites.google.com/view/transparent-reward}."],"url":"http://arxiv.org/abs/2410.18608v1"}
{"created":"2024-10-24 10:04:24","title":"STTATTS: Unified Speech-To-Text And Text-To-Speech Model","abstract":"Speech recognition and speech synthesis models are typically trained separately, each with its own set of learning objectives, training data, and model parameters, resulting in two distinct large networks. We propose a parameter-efficient approach to learning ASR and TTS jointly via a multi-task learning objective and shared parameters. Our evaluation demonstrates that the performance of our multi-task model is comparable to that of individually trained models while significantly saving computational and memory costs ($\\sim$50\\% reduction in the total number of parameters required for the two tasks combined). We experiment with English as a resource-rich language, and Arabic as a relatively low-resource language due to shortage of TTS data. Our models are trained with publicly available data, and both the training code and model checkpoints are openly available for further research.","sentences":["Speech recognition and speech synthesis models are typically trained separately, each with its own set of learning objectives, training data, and model parameters, resulting in two distinct large networks.","We propose a parameter-efficient approach to learning ASR and TTS jointly via a multi-task learning objective and shared parameters.","Our evaluation demonstrates that the performance of our multi-task model is comparable to that of individually trained models while significantly saving computational and memory costs ($\\sim$50\\% reduction in the total number of parameters required for the two tasks combined).","We experiment with English as a resource-rich language, and Arabic as a relatively low-resource language due to shortage of TTS data.","Our models are trained with publicly available data, and both the training code and model checkpoints are openly available for further research."],"url":"http://arxiv.org/abs/2410.18607v1"}
{"created":"2024-10-24 09:59:10","title":"Understanding Players as if They Are Talking to the Game in a Customized Language: A Pilot Study","abstract":"This pilot study explores the application of language models (LMs) to model game event sequences, treating them as a customized natural language. We investigate a popular mobile game, transforming raw event data into textual sequences and pretraining a Longformer model on this data. Our approach captures the rich and nuanced interactions within game sessions, effectively identifying meaningful player segments. The results demonstrate the potential of self-supervised LMs in enhancing game design and personalization without relying on ground-truth labels.","sentences":["This pilot study explores the application of language models (LMs) to model game event sequences, treating them as a customized natural language.","We investigate a popular mobile game, transforming raw event data into textual sequences and pretraining a Longformer model on this data.","Our approach captures the rich and nuanced interactions within game sessions, effectively identifying meaningful player segments.","The results demonstrate the potential of self-supervised LMs in enhancing game design and personalization without relying on ground-truth labels."],"url":"http://arxiv.org/abs/2410.18605v1"}
{"created":"2024-10-24 09:42:52","title":"Differential Informed Auto-Encoder","abstract":"In this article, an encoder was trained to obtain the inner structure of the original data by obtain a differential equations. A decoder was trained to resample the original data domain, to generate new data that obey the differential structure of the original data using the physics-informed neural network.","sentences":["In this article, an encoder was trained to obtain the inner structure of the original data by obtain a differential equations.","A decoder was trained to resample the original data domain, to generate new data that obey the differential structure of the original data using the physics-informed neural network."],"url":"http://arxiv.org/abs/2410.18593v1"}
{"created":"2024-10-24 09:37:23","title":"Knowledge Distillation Using Frontier Open-source LLMs: Generalizability and the Role of Synthetic Data","abstract":"Leading open-source large language models (LLMs) such as Llama-3.1-Instruct-405B are extremely capable at generating text, answering questions, and solving a variety of natural language understanding tasks. However, they incur higher inference cost and latency compared to smaller LLMs. Knowledge distillation provides a way to use outputs from these large, capable teacher models to train smaller student models which can be used for inference at lower cost and latency, while retaining comparable accuracy. We investigate the efficacy of distillation using the Llama-3.1-405B-Instruct teacher and the smaller Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct student models. Contributions of this work include (a) We evaluate the generalizability of distillation with the above Llama-3.1 teacher-student pairs across different tasks and datasets (b) We show that using synthetic data during distillation significantly improves the accuracy of 8B and 70B models, and when used with reasoning chains, even matches or surpasses the zero-shot accuracy of 405B model on some datasets (c) We empirically show that distillation enables 8B and 70B models to internalize 405B's reasoning ability by using only standard fine-tuning (without customizing any loss function). This allows cost and latency-efficient student model inference. (d) We show pitfalls in evaluation of distillation, and present task-specific evaluation, including both human and LLM-grading, and ground-truth based traditional accuracy benchmarks. This methodical study brings out the fundamental importance of synthetic data quality in knowledge distillation, and of combining multiple, task-specific ways of accuracy and quality evaluation in assessing the effectiveness of distillation.","sentences":["Leading open-source large language models (LLMs) such as Llama-3.1-Instruct-405B are extremely capable at generating text, answering questions, and solving a variety of natural language understanding tasks.","However, they incur higher inference cost and latency compared to smaller LLMs.","Knowledge distillation provides a way to use outputs from these large, capable teacher models to train smaller student models which can be used for inference at lower cost and latency, while retaining comparable accuracy.","We investigate the efficacy of distillation using the Llama-3.1-405B-Instruct teacher and the smaller Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct student models.","Contributions of this work include (a) We evaluate the generalizability of distillation with the above Llama-3.1 teacher-student pairs across different tasks and datasets (b) We show that using synthetic data during distillation significantly improves the accuracy of 8B and 70B models, and when used with reasoning chains, even matches or surpasses the zero-shot accuracy of 405B model on some datasets (c)","We empirically show that distillation enables 8B and 70B models to internalize 405B's reasoning ability by using only standard fine-tuning (without customizing any loss function).","This allows cost and latency-efficient student model inference.","(d) We show pitfalls in evaluation of distillation, and present task-specific evaluation, including both human and LLM-grading, and ground-truth based traditional accuracy benchmarks.","This methodical study brings out the fundamental importance of synthetic data quality in knowledge distillation, and of combining multiple, task-specific ways of accuracy and quality evaluation in assessing the effectiveness of distillation."],"url":"http://arxiv.org/abs/2410.18588v1"}
{"created":"2024-10-24 09:36:13","title":"Aligning CodeLLMs with Direct Preference Optimization","abstract":"The last year has witnessed the rapid progress of large language models (LLMs) across diverse domains. Among them, CodeLLMs have garnered particular attention because they can not only assist in completing various programming tasks but also represent the decision-making and logical reasoning capabilities of LLMs. However, current CodeLLMs mainly focus on pre-training and supervised fine-tuning scenarios, leaving the alignment stage, which is important for post-training LLMs, under-explored. This work first identifies that the commonly used PPO algorithm may be suboptimal for the alignment of CodeLLM because the involved reward rules are routinely coarse-grained and potentially flawed. We then advocate addressing this using the DPO algorithm. Based on only preference data pairs, DPO can render the model rank data automatically, giving rise to a fine-grained rewarding pattern more robust than human intervention. We also contribute a pipeline for collecting preference pairs for DPO on CodeLLMs. Studies show that our method significantly improves the performance of existing CodeLLMs on benchmarks such as MBPP and HumanEval.","sentences":["The last year has witnessed the rapid progress of large language models (LLMs) across diverse domains.","Among them, CodeLLMs have garnered particular attention because they can not only assist in completing various programming tasks but also represent the decision-making and logical reasoning capabilities of LLMs.","However, current CodeLLMs mainly focus on pre-training and supervised fine-tuning scenarios, leaving the alignment stage, which is important for post-training LLMs, under-explored.","This work first identifies that the commonly used PPO algorithm may be suboptimal for the alignment of CodeLLM because the involved reward rules are routinely coarse-grained and potentially flawed.","We then advocate addressing this using the DPO algorithm.","Based on only preference data pairs, DPO can render the model rank data automatically, giving rise to a fine-grained rewarding pattern more robust than human intervention.","We also contribute a pipeline for collecting preference pairs for DPO on CodeLLMs.","Studies show that our method significantly improves the performance of existing CodeLLMs on benchmarks such as MBPP and HumanEval."],"url":"http://arxiv.org/abs/2410.18585v1"}
{"created":"2024-10-24 09:35:34","title":"Benchmarking Graph Learning for Drug-Drug Interaction Prediction","abstract":"Predicting drug-drug interaction (DDI) plays an important role in pharmacology and healthcare for identifying potential adverse interactions and beneficial combination therapies between drug pairs. Recently, a flurry of graph learning methods have been introduced to predict drug-drug interactions. However, evaluating existing methods has several limitations, such as the absence of a unified comparison framework for DDI prediction methods, lack of assessments in meaningful real-world scenarios, and insufficient exploration of side information usage. In order to address these unresolved limitations in the literature, we propose a DDI prediction benchmark on graph learning. We first conduct unified evaluation comparison among existing methods. To meet realistic scenarios, we further evaluate the performance of different methods in settings with new drugs involved and examine the performance across different DDI types. Component analysis is conducted on the biomedical network to better utilize side information. Through this work, we hope to provide more insights for the problem of DDI prediction. Our implementation and data is open-sourced at https://anonymous.4open.science/r/DDI-Benchmark-ACD9/.","sentences":["Predicting drug-drug interaction (DDI) plays an important role in pharmacology and healthcare for identifying potential adverse interactions and beneficial combination therapies between drug pairs.","Recently, a flurry of graph learning methods have been introduced to predict drug-drug interactions.","However, evaluating existing methods has several limitations, such as the absence of a unified comparison framework for DDI prediction methods, lack of assessments in meaningful real-world scenarios, and insufficient exploration of side information usage.","In order to address these unresolved limitations in the literature, we propose a DDI prediction benchmark on graph learning.","We first conduct unified evaluation comparison among existing methods.","To meet realistic scenarios, we further evaluate the performance of different methods in settings with new drugs involved and examine the performance across different DDI types.","Component analysis is conducted on the biomedical network to better utilize side information.","Through this work, we hope to provide more insights for the problem of DDI prediction.","Our implementation and data is open-sourced at https://anonymous.4open.science/r/DDI-Benchmark-ACD9/."],"url":"http://arxiv.org/abs/2410.18583v1"}
{"created":"2024-10-24 09:31:48","title":"Resilience-based post disaster recovery optimization for infrastructure system via Deep Reinforcement Learning","abstract":"Infrastructure systems are critical in modern communities but are highly susceptible to various natural and man-made disasters. Efficient post-disaster recovery requires repair-scheduling approaches under the limitation of capped resources that need to be shared across the system. Existing approaches, including component ranking methods, greedy evolutionary algorithms, and data-driven machine learning models, face various limitations when tested within such a context. To tackle these issues, we propose a novel approach to optimize post-disaster recovery of infrastructure systems by leveraging Deep Reinforcement Learning (DRL) methods and incorporating a specialized resilience metric to lead the optimization. The system topology is represented adopting a graph-based structure, where the system's recovery process is formulated as a sequential decision-making problem. Deep Q-learning algorithms are employed to learn optimal recovery strategies by mapping system states to specific actions, as for instance which component ought to be repaired next, with the goal of maximizing long-term recovery from a resilience-oriented perspective. To demonstrate the efficacy of our proposed approach, we implement this scheme on the example of post-earthquake recovery optimization for an electrical substation system. We assess different deep Q-learning algorithms to this end, namely vanilla Deep Q-Networks (DQN), Double DQN(DDQN), Duel DQN, and duel DDQN, demonstrating superiority of the DDQN for the considered problem. A further comparative analysis against baseline methods during testing reveals the superior performance of the proposed method in terms of both optimization effect and computational cost, rendering this an attractive approach in the context of resilience enhancement and rapid response and recovery.","sentences":["Infrastructure systems are critical in modern communities but are highly susceptible to various natural and man-made disasters.","Efficient post-disaster recovery requires repair-scheduling approaches under the limitation of capped resources that need to be shared across the system.","Existing approaches, including component ranking methods, greedy evolutionary algorithms, and data-driven machine learning models, face various limitations when tested within such a context.","To tackle these issues, we propose a novel approach to optimize post-disaster recovery of infrastructure systems by leveraging Deep Reinforcement Learning (DRL) methods and incorporating a specialized resilience metric to lead the optimization.","The system topology is represented adopting a graph-based structure, where the system's recovery process is formulated as a sequential decision-making problem.","Deep Q-learning algorithms are employed to learn optimal recovery strategies by mapping system states to specific actions, as for instance which component ought to be repaired next, with the goal of maximizing long-term recovery from a resilience-oriented perspective.","To demonstrate the efficacy of our proposed approach, we implement this scheme on the example of post-earthquake recovery optimization for an electrical substation system.","We assess different deep Q-learning algorithms to this end, namely vanilla Deep Q-Networks (DQN), Double DQN(DDQN), Duel DQN, and duel DDQN, demonstrating superiority of the DDQN for the considered problem.","A further comparative analysis against baseline methods during testing reveals the superior performance of the proposed method in terms of both optimization effect and computational cost, rendering this an attractive approach in the context of resilience enhancement and rapid response and recovery."],"url":"http://arxiv.org/abs/2410.18577v1"}
{"created":"2024-10-24 09:29:18","title":"SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning","abstract":"Large Language Models (LLMs) can transfer their reasoning skills to smaller models by teaching them to generate the intermediate reasoning process required to solve multistep reasoning tasks. While LLMs can accurately solve reasoning tasks through a variety of strategies, even without fine-tuning, smaller models are not expressive enough to fit the LLMs distribution on all strategies when distilled and tend to prioritize one strategy over the others. This reliance on one strategy poses a challenge for smaller models when attempting to solve reasoning tasks that may be difficult with their preferred strategy. To address this, we propose a distillation method SIKeD (Self-guided Iterative Knowledge Distillation for mathematical reasoning), where the LLM teaches the smaller model to approach a task using different strategies and the smaller model uses its self-generated on-policy outputs to choose the most suitable strategy for the given task. The training continues in a self-guided iterative manner, where for each training iteration, a decision is made on how to combine the LLM data with the self-generated outputs. Unlike traditional distillation methods, SIKeD allows the smaller model to learn which strategy is suitable for a given task while continuously learning to solve a task using different strategies. Our experiments on various mathematical reasoning datasets show that SIKeD significantly outperforms traditional distillation techniques across smaller models of different sizes. Our code is available at: https://github.com/kumar-shridhar/SIKeD","sentences":["Large Language Models (LLMs) can transfer their reasoning skills to smaller models by teaching them to generate the intermediate reasoning process required to solve multistep reasoning tasks.","While LLMs can accurately solve reasoning tasks through a variety of strategies, even without fine-tuning, smaller models are not expressive enough to fit the LLMs distribution on all strategies when distilled and tend to prioritize one strategy over the others.","This reliance on one strategy poses a challenge for smaller models when attempting to solve reasoning tasks that may be difficult with their preferred strategy.","To address this, we propose a distillation method SIKeD (Self-guided Iterative Knowledge Distillation for mathematical reasoning), where the LLM teaches the smaller model to approach a task using different strategies and the smaller model uses its self-generated on-policy outputs to choose the most suitable strategy for the given task.","The training continues in a self-guided iterative manner, where for each training iteration, a decision is made on how to combine the LLM data with the self-generated outputs.","Unlike traditional distillation methods, SIKeD allows the smaller model to learn which strategy is suitable for a given task while continuously learning to solve a task using different strategies.","Our experiments on various mathematical reasoning datasets show that SIKeD significantly outperforms traditional distillation techniques across smaller models of different sizes.","Our code is available at: https://github.com/kumar-shridhar/SIKeD"],"url":"http://arxiv.org/abs/2410.18574v1"}
{"created":"2024-10-24 09:24:07","title":"Zero-shot Object Navigation with Vision-Language Models Reasoning","abstract":"Object navigation is crucial for robots, but traditional methods require substantial training data and cannot be generalized to unknown environments. Zero-shot object navigation (ZSON) aims to address this challenge, allowing robots to interact with unknown objects without specific training data. Language-driven zero-shot object navigation (L-ZSON) is an extension of ZSON that incorporates natural language instructions to guide robot navigation and interaction with objects. In this paper, we propose a novel Vision Language model with a Tree-of-thought Network (VLTNet) for L-ZSON. VLTNet comprises four main modules: vision language model understanding, semantic mapping, tree-of-thought reasoning and exploration, and goal identification. Among these modules, Tree-of-Thought (ToT) reasoning and exploration module serves as a core component, innovatively using the ToT reasoning framework for navigation frontier selection during robot exploration. Compared to conventional frontier selection without reasoning, navigation using ToT reasoning involves multi-path reasoning processes and backtracking when necessary, enabling globally informed decision-making with higher accuracy. Experimental results on PASTURE and RoboTHOR benchmarks demonstrate the outstanding performance of our model in LZSON, particularly in scenarios involving complex natural language as target instructions.","sentences":["Object navigation is crucial for robots, but traditional methods require substantial training data and cannot be generalized to unknown environments.","Zero-shot object navigation (ZSON) aims to address this challenge, allowing robots to interact with unknown objects without specific training data.","Language-driven zero-shot object navigation (L-ZSON) is an extension of ZSON that incorporates natural language instructions to guide robot navigation and interaction with objects.","In this paper, we propose a novel Vision Language model with a Tree-of-thought Network (VLTNet) for L-ZSON.","VLTNet comprises four main modules: vision language model understanding, semantic mapping, tree-of-thought reasoning and exploration, and goal identification.","Among these modules, Tree-of-Thought (ToT) reasoning and exploration module serves as a core component, innovatively using the ToT reasoning framework for navigation frontier selection during robot exploration.","Compared to conventional frontier selection without reasoning, navigation using ToT reasoning involves multi-path reasoning processes and backtracking when necessary, enabling globally informed decision-making with higher accuracy.","Experimental results on PASTURE and RoboTHOR benchmarks demonstrate the outstanding performance of our model in LZSON, particularly in scenarios involving complex natural language as target instructions."],"url":"http://arxiv.org/abs/2410.18570v1"}
{"created":"2024-10-24 09:03:48","title":"Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data","abstract":"Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication. We also propose a synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation. Using this data, we trained a 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale. This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models.","sentences":["Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models.","In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication.","We also propose a synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation.","Using this data, we trained a 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale.","This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models."],"url":"http://arxiv.org/abs/2410.18558v1"}
{"created":"2024-10-24 09:02:56","title":"Research on gesture recognition method based on SEDCNN-SVM","abstract":"Gesture recognition based on surface electromyographic signal (sEMG) is one of the most used methods. The traditional manual feature extraction can only extract some low-level signal features, this causes poor classifier performance and low recognition accuracy when dealing with some complex signals. A recognition method, namely SEDCNN-SVM, is proposed to recognize sEMG of different gestures. SEDCNN-SVM consists of an improved deep convolutional neural network (DCNN) and a support vector machine (SVM). The DCNN can automatically extract and learn the feature information of sEMG through the convolution operation of the convolutional layer, so that it can capture the complex and high-level features in the data. The Squeeze and Excitation Networks (SE-Net) and the residual module were added to the model, so that the feature representation of each channel could be improved, the loss of feature information in convolutional operations was reduced, useful feature information was captured, and the problem of network gradient vanishing was eased. The SVM can improve the generalization ability and classification accuracy of the model by constructing an optimal hyperplane of the feature space. Hence, the SVM was used to replace the full connection layer and the Softmax function layer of the DCNN, the use of a suitable kernel function in SVM can improve the model's generalization ability and classification accuracy. To verify the effectiveness of the proposed classification algorithm, this method is analyzed and compared with other comparative classification methods. The recognition accuracy of SEDCNN-SVM can reach 0.955, it is significantly improved compared with other classification methods, the SEDCNN-SVM model is recognized online in real time.","sentences":["Gesture recognition based on surface electromyographic signal (sEMG) is one of the most used methods.","The traditional manual feature extraction can only extract some low-level signal features, this causes poor classifier performance and low recognition accuracy when dealing with some complex signals.","A recognition method, namely SEDCNN-SVM, is proposed to recognize sEMG of different gestures.","SEDCNN-SVM consists of an improved deep convolutional neural network (DCNN) and a support vector machine (SVM).","The DCNN can automatically extract and learn the feature information of sEMG through the convolution operation of the convolutional layer, so that it can capture the complex and high-level features in the data.","The Squeeze and Excitation Networks (SE-Net) and the residual module were added to the model, so that the feature representation of each channel could be improved, the loss of feature information in convolutional operations was reduced, useful feature information was captured, and the problem of network gradient vanishing was eased.","The SVM can improve the generalization ability and classification accuracy of the model by constructing an optimal hyperplane of the feature space.","Hence, the SVM was used to replace the full connection layer and the Softmax function layer of the DCNN, the use of a suitable kernel function in SVM can improve the model's generalization ability and classification accuracy.","To verify the effectiveness of the proposed classification algorithm, this method is analyzed and compared with other comparative classification methods.","The recognition accuracy of SEDCNN-SVM can reach 0.955, it is significantly improved compared with other classification methods, the SEDCNN-SVM model is recognized online in real time."],"url":"http://arxiv.org/abs/2410.18557v1"}
{"created":"2024-10-24 08:54:08","title":"IMAN: An Adaptive Network for Robust NPC Mortality Prediction with Missing Modalities","abstract":"Accurate prediction of mortality in nasopharyngeal carcinoma (NPC), a complex malignancy particularly challenging in advanced stages, is crucial for optimizing treatment strategies and improving patient outcomes. However, this predictive process is often compromised by the high-dimensional and heterogeneous nature of NPC-related data, coupled with the pervasive issue of incomplete multi-modal data, manifesting as missing radiological images or incomplete diagnostic reports. Traditional machine learning approaches suffer significant performance degradation when faced with such incomplete data, as they fail to effectively handle the high-dimensionality and intricate correlations across modalities. Even advanced multi-modal learning techniques like Transformers struggle to maintain robust performance in the presence of missing modalities, as they lack specialized mechanisms to adaptively integrate and align the diverse data types, while also capturing nuanced patterns and contextual relationships within the complex NPC data. To address these problem, we introduce IMAN: an adaptive network for robust NPC mortality prediction with missing modalities.","sentences":["Accurate prediction of mortality in nasopharyngeal carcinoma (NPC), a complex malignancy particularly challenging in advanced stages, is crucial for optimizing treatment strategies and improving patient outcomes.","However, this predictive process is often compromised by the high-dimensional and heterogeneous nature of NPC-related data, coupled with the pervasive issue of incomplete multi-modal data, manifesting as missing radiological images or incomplete diagnostic reports.","Traditional machine learning approaches suffer significant performance degradation when faced with such incomplete data, as they fail to effectively handle the high-dimensionality and intricate correlations across modalities.","Even advanced multi-modal learning techniques like Transformers struggle to maintain robust performance in the presence of missing modalities, as they lack specialized mechanisms to adaptively integrate and align the diverse data types, while also capturing nuanced patterns and contextual relationships within the complex NPC data.","To address these problem, we introduce IMAN: an adaptive network for robust NPC mortality prediction with missing modalities."],"url":"http://arxiv.org/abs/2410.18551v1"}
{"created":"2024-10-24 08:45:46","title":"Stronger adversaries grow cheaper forests: online node-weighted Steiner problems","abstract":"We propose a $O(\\log k \\log n)$-competitive randomized algorithm for online node-weighted Steiner forest. This is essentially optimal and significantly improves over the previous bound of $O(\\log^2 k \\log n)$ by Hajiaghayi et al. [2017]. In fact, our result extends to the more general prize-collecting setting, improving over previous works by a poly-logarithmic factor. Our key technical contribution is a randomized online algorithm for set cover and non-metric facility location in a new adversarial model which we call semi-adaptive adversaries. As a by-product of our techniques, we obtain the first deterministic $O(\\log |C| \\log |F|)$-competitive algorithm for non-metric facility location.","sentences":["We propose a $O(\\log k \\log n)$-competitive randomized algorithm for online node-weighted Steiner forest.","This is essentially optimal and significantly improves over the previous bound of $O(\\log^2 k \\log n)$ by Hajiaghayi et al.","[2017].","In fact, our result extends to the more general prize-collecting setting, improving over previous works by a poly-logarithmic factor.","Our key technical contribution is a randomized online algorithm for set cover and non-metric facility location in a new adversarial model which we call semi-adaptive adversaries.","As a by-product of our techniques, we obtain the first deterministic $O(\\log |C| \\log |F|)$-competitive algorithm for non-metric facility location."],"url":"http://arxiv.org/abs/2410.18542v1"}
{"created":"2024-10-24 08:39:24","title":"Interpretable Representation Learning from Videos using Nonlinear Priors","abstract":"Learning interpretable representations of visual data is an important challenge, to make machines' decisions understandable to humans and to improve generalisation outside of the training distribution. To this end, we propose a deep learning framework where one can specify nonlinear priors for videos (e.g. of Newtonian physics) that allow the model to learn interpretable latent variables and use these to generate videos of hypothetical scenarios not observed at training time. We do this by extending the Variational Auto-Encoder (VAE) prior from a simple isotropic Gaussian to an arbitrary nonlinear temporal Additive Noise Model (ANM), which can describe a large number of processes (e.g. Newtonian physics). We propose a novel linearization method that constructs a Gaussian Mixture Model (GMM) approximating the prior, and derive a numerically stable Monte Carlo estimate of the KL divergence between the posterior and prior GMMs. We validate the method on different real-world physics videos including a pendulum, a mass on a spring, a falling object and a pulsar (rotating neutron star). We specify a physical prior for each experiment and show that the correct variables are learned. Once a model is trained, we intervene on it to change different physical variables (such as oscillation amplitude or adding air drag) to generate physically correct videos of hypothetical scenarios that were not observed previously.","sentences":["Learning interpretable representations of visual data is an important challenge, to make machines' decisions understandable to humans and to improve generalisation outside of the training distribution.","To this end, we propose a deep learning framework where one can specify nonlinear priors for videos (e.g. of Newtonian physics) that allow the model to learn interpretable latent variables and use these to generate videos of hypothetical scenarios not observed at training time.","We do this by extending the Variational Auto-Encoder (VAE) prior from a simple isotropic Gaussian to an arbitrary nonlinear temporal Additive Noise Model (ANM), which can describe a large number of processes (e.g. Newtonian physics).","We propose a novel linearization method that constructs a Gaussian Mixture Model (GMM) approximating the prior, and derive a numerically stable Monte Carlo estimate of the KL divergence between the posterior and prior GMMs.","We validate the method on different real-world physics videos including a pendulum, a mass on a spring, a falling object and a pulsar (rotating neutron star).","We specify a physical prior for each experiment and show that the correct variables are learned.","Once a model is trained, we intervene on it to change different physical variables (such as oscillation amplitude or adding air drag) to generate physically correct videos of hypothetical scenarios that were not observed previously."],"url":"http://arxiv.org/abs/2410.18539v1"}
{"created":"2024-10-24 08:33:15","title":"Putting Off the Catching Up: Online Joint Replenishment Problem with Holding and Backlog Costs","abstract":"We study an online generalization of the classic Joint Replenishment Problem (JRP) that models the trade-off between ordering costs, holding costs, and backlog costs in supply chain planning systems. A retailer places orders to a supplier for multiple items over time: each request is for some item that the retailer needs in the future, and has an arrival time and a soft deadline. If a request is served before its deadline, the retailer pays a holding cost per unit of the item until the deadline. However, if a request is served after its deadline, the retailer pays a backlog cost per unit. Each service incurs a fixed joint service cost and a fixed item-dependent cost for every item included in a service. These fixed costs are the same irrespective of the units of each item ordered. The goal is to schedule services to satisfy all the online requests while minimizing the sum of the service costs, the holding costs, and the backlog costs.   Constant competitive online algorithms have been developed for two special cases: the make-to-order version when the deadlines are equal to arrival times (Buchbinder et al., 2013), and the make-to-stock version with hard deadlines with zero holding costs (Bienkowski et al., 2014). Our general model with holding and backlog costs has not been investigated earlier, and no online algorithms are known even in the make-to-stock version with hard deadlines and non-zero holding costs. We develop a new online algorithm for the general version of online JRP with both holding and backlog costs and establish that it is 30-competitive. Along the way, we develop a 3-competitive algorithm for the single-item case that we build on to get our final result. Our algorithm uses a greedy strategy and its competitiveness is shown using a dual fitting analysis.","sentences":["We study an online generalization of the classic Joint Replenishment Problem (JRP) that models the trade-off between ordering costs, holding costs, and backlog costs in supply chain planning systems.","A retailer places orders to a supplier for multiple items over time: each request is for some item that the retailer needs in the future, and has an arrival time and a soft deadline.","If a request is served before its deadline, the retailer pays a holding cost per unit of the item until the deadline.","However, if a request is served after its deadline, the retailer pays a backlog cost per unit.","Each service incurs a fixed joint service cost and a fixed item-dependent cost for every item included in a service.","These fixed costs are the same irrespective of the units of each item ordered.","The goal is to schedule services to satisfy all the online requests while minimizing the sum of the service costs, the holding costs, and the backlog costs.   ","Constant competitive online algorithms have been developed for two special cases: the make-to-order version when the deadlines are equal to arrival times (Buchbinder et al., 2013), and the make-to-stock version with hard deadlines with zero holding costs (Bienkowski et al., 2014).","Our general model with holding and backlog costs has not been investigated earlier, and no online algorithms are known even in the make-to-stock version with hard deadlines and non-zero holding costs.","We develop a new online algorithm for the general version of online JRP with both holding and backlog costs and establish that it is 30-competitive.","Along the way, we develop a 3-competitive algorithm for the single-item case that we build on to get our final result.","Our algorithm uses a greedy strategy and its competitiveness is shown using a dual fitting analysis."],"url":"http://arxiv.org/abs/2410.18535v1"}
{"created":"2024-10-24 08:27:26","title":"LOGO -- Long cOntext aliGnment via efficient preference Optimization","abstract":"Long-context models(LCMs) have shown great potential in processing long input sequences(even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the generation performance of these LCMs is far from satisfactory and might result in misaligned responses, such as hallucinations. To enhance the generation capability of LCMs, existing works have investigated the effects of data size and quality for both pre-training and instruction tuning. Though achieving meaningful improvement, previous methods fall short in either effectiveness or efficiency. In this paper, we introduce LOGO(Long cOntext aliGnment via efficient preference Optimization), a training strategy that first introduces preference optimization for long-context alignment. To overcome the GPU memory-bound issue caused by the long sequence, LOGO employs a reference-free preference optimization strategy and adopts a position synthesis method to construct the training data. By training with only 0.3B data on a single 8$\\times$A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while preserving the model's original capabilities on other tasks, e.g., language modeling and MMLU. Moreover, LOGO can extend the model's context window size while enhancing its generation performance.","sentences":["Long-context models(LCMs) have shown great potential in processing long input sequences(even more than 100M tokens) conveniently and effectively.","With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context.","Yet, the generation performance of these LCMs is far from satisfactory and might result in misaligned responses, such as hallucinations.","To enhance the generation capability of LCMs, existing works have investigated the effects of data size and quality for both pre-training and instruction tuning.","Though achieving meaningful improvement, previous methods fall short in either effectiveness or efficiency.","In this paper, we introduce LOGO(Long cOntext aliGnment via efficient preference Optimization), a training strategy that first introduces preference optimization for long-context alignment.","To overcome the GPU memory-bound issue caused by the long sequence, LOGO employs a reference-free preference optimization strategy and adopts a position synthesis method to construct the training data.","By training with only 0.3B data on a single 8$\\times$A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while preserving the model's original capabilities on other tasks, e.g., language modeling and MMLU.","Moreover, LOGO can extend the model's context window size while enhancing its generation performance."],"url":"http://arxiv.org/abs/2410.18533v1"}
{"created":"2024-10-24 08:21:51","title":"PRACT: Optimizing Principled Reasoning and Acting of LLM Agent","abstract":"We introduce the Principled Reasoning and Acting (PRAct) framework, a novel method for learning and enforcing action principles from trajectory data. Central to our approach is the use of text gradients from a reflection and optimization engine to derive these action principles. To adapt action principles to specific task requirements, we propose a new optimization framework, Reflective Principle Optimization (RPO). After execution, RPO employs a reflector to critique current action principles and an optimizer to update them accordingly. We develop the RPO framework under two scenarios: Reward-RPO, which uses environmental rewards for reflection, and Self-RPO, which conducts self-reflection without external rewards. Additionally, two RPO methods, RPO-Traj and RPO-Batch, is introduced to adapt to different settings. Experimental results across four environments demonstrate that the PRAct agent, leveraging the RPO framework, effectively learns and applies action principles to enhance performance.","sentences":["We introduce the Principled Reasoning and Acting (PRAct) framework, a novel method for learning and enforcing action principles from trajectory data.","Central to our approach is the use of text gradients from a reflection and optimization engine to derive these action principles.","To adapt action principles to specific task requirements, we propose a new optimization framework, Reflective Principle Optimization (RPO).","After execution, RPO employs a reflector to critique current action principles and an optimizer to update them accordingly.","We develop the RPO framework under two scenarios: Reward-RPO, which uses environmental rewards for reflection, and Self-RPO, which conducts self-reflection without external rewards.","Additionally, two RPO methods, RPO-Traj and RPO-Batch, is introduced to adapt to different settings.","Experimental results across four environments demonstrate that the PRAct agent, leveraging the RPO framework, effectively learns and applies action principles to enhance performance."],"url":"http://arxiv.org/abs/2410.18528v1"}
{"created":"2024-10-24 08:11:34","title":"Towards Reinforcement Learning Controllers for Soft Robots using Learned Environments","abstract":"Soft robotic manipulators offer operational advantage due to their compliant and deformable structures. However, their inherently nonlinear dynamics presents substantial challenges. Traditional analytical methods often depend on simplifying assumptions, while learning-based techniques can be computationally demanding and limit the control policies to existing data. This paper introduces a novel approach to soft robotic control, leveraging state-of-the-art policy gradient methods within parallelizable synthetic environments learned from data. We also propose a safety oriented actuation space exploration protocol via cascaded updates and weighted randomness. Specifically, our recurrent forward dynamics model is learned by generating a training dataset from a physically safe \\textit{mean reverting} random walk in actuation space to explore the partially-observed state-space. We demonstrate a reinforcement learning approach towards closed-loop control through state-of-the-art actor-critic methods, which efficiently learn high-performance behaviour over long horizons. This approach removes the need for any knowledge regarding the robot's operation or capabilities and sets the stage for a comprehensive benchmarking tool in soft robotics control.","sentences":["Soft robotic manipulators offer operational advantage due to their compliant and deformable structures.","However, their inherently nonlinear dynamics presents substantial challenges.","Traditional analytical methods often depend on simplifying assumptions, while learning-based techniques can be computationally demanding and limit the control policies to existing data.","This paper introduces a novel approach to soft robotic control, leveraging state-of-the-art policy gradient methods within parallelizable synthetic environments learned from data.","We also propose a safety oriented actuation space exploration protocol via cascaded updates and weighted randomness.","Specifically, our recurrent forward dynamics model is learned by generating a training dataset from a physically safe \\textit{mean reverting} random walk in actuation space to explore the partially-observed state-space.","We demonstrate a reinforcement learning approach towards closed-loop control through state-of-the-art actor-critic methods, which efficiently learn high-performance behaviour over long horizons.","This approach removes the need for any knowledge regarding the robot's operation or capabilities and sets the stage for a comprehensive benchmarking tool in soft robotics control."],"url":"http://arxiv.org/abs/2410.18519v1"}
{"created":"2024-10-24 08:11:34","title":"Unsupervised semantic segmentation of urban high-density multispectral point clouds","abstract":"The availability of highly accurate urban airborne laser scanning (ALS) data will increase rapidly in the future, especially as acquisition costs decrease, for example through the use of drones. Current challenges in data processing are related to the limited spectral information and low point density of most ALS datasets. Another challenge will be the growing need for annotated training data, frequently produced by manual processes, to enable semantic interpretation of point clouds. This study proposes to semantically segment new high-density (1200 points per square metre on average) multispectral ALS data with an unsupervised ground-aware deep clustering method GroupSP inspired by the unsupervised GrowSP algorithm. GroupSP divides the scene into superpoints as a preprocessing step. The neural network is trained iteratively by grouping the superpoints and using the grouping assignments as pseudo-labels. The predictions for the unseen data are given by over-segmenting the test set and mapping the predicted classes into ground truth classes manually or with automated majority voting. GroupSP obtained an overall accuracy (oAcc) of 97% and a mean intersection over union (mIoU) of 80%. When compared to other unsupervised semantic segmentation methods, GroupSP outperformed GrowSP and non-deep K-means. However, a supervised random forest classifier outperformed GroupSP. The labelling efforts in GroupSP can be minimal; it was shown, that the GroupSP can semantically segment seven urban classes (building, high vegetation, low vegetation, asphalt, rock, football field, and gravel) with oAcc of 95% and mIoU of 75% using only 0.004% of the available annotated points in the mapping assignment. Finally, the multispectral information was examined; adding each new spectral channel improved the mIoU. Additionally, echo deviation was valuable, especially when distinguishing ground-level classes.","sentences":["The availability of highly accurate urban airborne laser scanning (ALS) data will increase rapidly in the future, especially as acquisition costs decrease, for example through the use of drones.","Current challenges in data processing are related to the limited spectral information and low point density of most ALS datasets.","Another challenge will be the growing need for annotated training data, frequently produced by manual processes, to enable semantic interpretation of point clouds.","This study proposes to semantically segment new high-density (1200 points per square metre on average) multispectral ALS data with an unsupervised ground-aware deep clustering method GroupSP inspired by the unsupervised GrowSP algorithm.","GroupSP","divides the scene into superpoints as a preprocessing step.","The neural network is trained iteratively by grouping the superpoints and using the grouping assignments as pseudo-labels.","The predictions for the unseen data are given by over-segmenting the test set and mapping the predicted classes into ground truth classes manually or with automated majority voting.","GroupSP obtained an overall accuracy (oAcc) of 97% and a mean intersection over union (mIoU) of 80%.","When compared to other unsupervised semantic segmentation methods, GroupSP outperformed GrowSP and non-deep K-means.","However, a supervised random forest classifier outperformed GroupSP.","The labelling efforts in GroupSP can be minimal; it was shown, that the GroupSP can semantically segment seven urban classes (building, high vegetation, low vegetation, asphalt, rock, football field, and gravel) with oAcc of 95% and mIoU of 75% using only 0.004% of the available annotated points in the mapping assignment.","Finally, the multispectral information was examined; adding each new spectral channel improved the mIoU. Additionally, echo deviation was valuable, especially when distinguishing ground-level classes."],"url":"http://arxiv.org/abs/2410.18520v1"}
{"created":"2024-10-24 08:01:22","title":"Scaling up Masked Diffusion Models on Text","abstract":"Masked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored. This paper establishes the first scaling law for MDMs, demonstrating a scaling rate comparable to autoregressive models (ARMs) and a relatively small compute gap. Motivated by their scalability, we train a family of MDMs with up to 1.1 billion (B) parameters to systematically evaluate their performance against ARMs of comparable or larger sizes. Fully leveraging the probabilistic formulation of MDMs, we propose a simple yet effective \\emph{unsupervised classifier-free guidance} that effectively exploits large-scale unpaired data, boosting performance for conditional inference. In language understanding, a 1.1B MDM shows competitive results, outperforming the larger 1.5B GPT-2 model on four out of eight zero-shot benchmarks. In text generation, MDMs provide a flexible trade-off compared to ARMs utilizing KV-cache: MDMs match the performance of ARMs while being 1.4 times faster, or achieve higher quality than ARMs at a higher computational cost. Moreover, MDMs address challenging tasks for ARMs by effectively handling bidirectional reasoning and adapting to temporal shifts in data. Notably, a 1.1B MDM breaks the \\emph{reverse curse} encountered by much larger ARMs with significantly more data and computation, such as Llama-2 (13B) and GPT-3 (175B). Our code is available at \\url{https://github.com/ML-GSAI/SMDM}.","sentences":["Masked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored.","This paper establishes the first scaling law for MDMs, demonstrating a scaling rate comparable to autoregressive models (ARMs) and a relatively small compute gap.","Motivated by their scalability, we train a family of MDMs with up to 1.1 billion (B) parameters to systematically evaluate their performance against ARMs of comparable or larger sizes.","Fully leveraging the probabilistic formulation of MDMs, we propose a simple yet effective \\emph{unsupervised classifier-free guidance} that effectively exploits large-scale unpaired data, boosting performance for conditional inference.","In language understanding, a 1.1B MDM shows competitive results, outperforming the larger 1.5B GPT-2 model on four out of eight zero-shot benchmarks.","In text generation, MDMs provide a flexible trade-off compared to ARMs utilizing KV-cache: MDMs match the performance of ARMs while being 1.4 times faster, or achieve higher quality than ARMs at a higher computational cost.","Moreover, MDMs address challenging tasks for ARMs by effectively handling bidirectional reasoning and adapting to temporal shifts in data.","Notably, a 1.1B MDM breaks the \\emph{reverse curse} encountered by much larger ARMs with significantly more data and computation, such as Llama-2 (13B) and GPT-3 (175B).","Our code is available at \\url{https://github.com/ML-GSAI/SMDM}."],"url":"http://arxiv.org/abs/2410.18514v1"}
{"created":"2024-10-24 07:50:40","title":"Ubiquitous Field Transportation Robots with Robust Wheel-Leg Transformable Modules","abstract":"This paper introduces two field transportation robots. Both robots are equipped with transformable wheel-leg modules, which can smoothly switch between operation modes and can work in various challenging terrains. SWhegPro, with six S-shaped legs, enables transporting loads in challenging uneven outdoor terrains. SWhegPro3, featuring four three-impeller wheels, has surprising stair-climbing performance in indoor scenarios. Different from ordinary gear-driven transformable mechanisms, the modular wheels we designed driven by self-locking electric push rods can switch modes accurately and stably with high loads, significantly improving the load capacity of the robot in leg mode. This study analyzes the robot's wheel-leg module operation when the terrain parameters change. Through the derivation of mathematical models and calculations based on simplified kinematic models, a method for optimizing the robot parameters and wheel-leg structure parameters is finally proposed.The design and control strategy are then verified through simulations and field experiments in various complex terrains, and the working performance of the two field transportation robots is calculated and analyzed by recording sensor data and proposing evaluation methods.","sentences":["This paper introduces two field transportation robots.","Both robots are equipped with transformable wheel-leg modules, which can smoothly switch between operation modes and can work in various challenging terrains.","SWhegPro, with six S-shaped legs, enables transporting loads in challenging uneven outdoor terrains.","SWhegPro3, featuring four three-impeller wheels, has surprising stair-climbing performance in indoor scenarios.","Different from ordinary gear-driven transformable mechanisms, the modular wheels we designed driven by self-locking electric push rods can switch modes accurately and stably with high loads, significantly improving the load capacity of the robot in leg mode.","This study analyzes the robot's wheel-leg module operation when the terrain parameters change.","Through the derivation of mathematical models and calculations based on simplified kinematic models, a method for optimizing the robot parameters and wheel-leg structure parameters is finally proposed.","The design and control strategy are then verified through simulations and field experiments in various complex terrains, and the working performance of the two field transportation robots is calculated and analyzed by recording sensor data and proposing evaluation methods."],"url":"http://arxiv.org/abs/2410.18507v1"}
{"created":"2024-10-24 07:50:07","title":"CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models","abstract":"We present CCI3.0-HQ (https://huggingface.co/datasets/BAAI/CCI3-HQ), a high-quality 500GB subset of the Chinese Corpora Internet 3.0 (CCI3.0)(https://huggingface.co/datasets/BAAI/CCI3-Data), developed using a novel two-stage hybrid filtering pipeline that significantly enhances data quality. To evaluate its effectiveness, we trained a 0.5B parameter model from scratch on 100B tokens across various datasets, achieving superior performance on 10 benchmarks in a zero-shot setting compared to CCI3.0, SkyPile, and WanjuanV1. The high-quality filtering process effectively distills the capabilities of the Qwen2-72B-instruct model into a compact 0.5B model, attaining optimal F1 scores for Chinese web data classification. We believe this open-access dataset will facilitate broader access to high-quality language models.","sentences":["We present CCI3.0-HQ (https://huggingface.co/datasets/BAAI/CCI3-HQ), a high-quality 500GB subset of the Chinese Corpora Internet 3.0 (CCI3.0)(https://huggingface.co/datasets/BAAI/CCI3-Data), developed using a novel two-stage hybrid filtering pipeline that significantly enhances data quality.","To evaluate its effectiveness, we trained a 0.5B parameter model from scratch on 100B tokens across various datasets, achieving superior performance on 10 benchmarks in a zero-shot setting compared to CCI3.0, SkyPile, and WanjuanV1.","The high-quality filtering process effectively distills the capabilities of the Qwen2-72B-instruct model into a compact 0.5B model, attaining optimal F1 scores for Chinese web data classification.","We believe this open-access dataset will facilitate broader access to high-quality language models."],"url":"http://arxiv.org/abs/2410.18505v1"}
{"created":"2024-10-24 07:48:13","title":"SFB-net for cardiac segmentation: Bridging the semantic gap with attention","abstract":"In the past few years, deep learning algorithms have been widely used for cardiac image segmentation. However, most of these architectures rely on convolutions that hardly model long-range dependencies, limiting their ability to extract contextual information. In order to tackle this issue, this article introduces the Swin Filtering Block network (SFB-net) which takes advantage of both conventional and swin transformer layers. The former are used to introduce spatial attention at the bottom of the network, while the latter are applied to focus on high level semantically rich features between the encoder and decoder. An average Dice score of 92.4 was achieved on the ACDC dataset. To the best of our knowledge, this result outperforms any other work on this dataset. The average Dice score of 87.99 obtained on the M\\&amp;M's dataset demonstrates that the proposed method generalizes well to data from different vendors and centres.","sentences":["In the past few years, deep learning algorithms have been widely used for cardiac image segmentation.","However, most of these architectures rely on convolutions that hardly model long-range dependencies, limiting their ability to extract contextual information.","In order to tackle this issue, this article introduces the Swin Filtering Block network (SFB-net) which takes advantage of both conventional and swin transformer layers.","The former are used to introduce spatial attention at the bottom of the network, while the latter are applied to focus on high level semantically rich features between the encoder and decoder.","An average Dice score of 92.4 was achieved on the ACDC dataset.","To the best of our knowledge, this result outperforms any other work on this dataset.","The average Dice score of 87.99 obtained on the M\\&amp;M's dataset demonstrates that the proposed method generalizes well to data from different vendors and centres."],"url":"http://arxiv.org/abs/2410.18503v1"}
{"created":"2024-10-24 07:27:00","title":"Enhancing Information Diffusion Prediction by Addressing Noise in Social Connection Data","abstract":"With the increasing use of online social media platforms, information diffusion has become a prevalent phenomenon, making Information Diffusion Prediction (IDP) critical for various applications. Many studies utilize social connection data to improve prediction performance. However, previous research has largely overlooked the issue of noise within social connection data, which often contains irrelevant or misleading connections due to the diversity of social relationships and the presence of weak ties. Such noise can lead to inaccurate learning of user preferences and negatively affect prediction outcomes. To address this issue, we propose DIDP, a novel social connection denoising framework for information diffusion prediction. First, we introduce a graph learning encoder module that encodes the information diffusion hypergraph and social graph to obtain latent diffusion embeddings and social embeddings. Next, DIDP integrates a denoising diffusion module to adaptively remove different types of noise from the learned social embeddings in the latent space. Through multi-step noise injection and removal, the framework enhances the ability to learn robust embeddings. Additionally, we introduce a cross-domain contrastive learning module that maximizes the mutual information between the diffusion embeddings and the denoised social embeddings of the same user. This module guides the denoising process and facilitates cross-domain knowledge transfer. Experimental results show that DIDP significantly outperforms state-of-the-art methods in the information diffusion prediction task.","sentences":["With the increasing use of online social media platforms, information diffusion has become a prevalent phenomenon, making Information Diffusion Prediction (IDP) critical for various applications.","Many studies utilize social connection data to improve prediction performance.","However, previous research has largely overlooked the issue of noise within social connection data, which often contains irrelevant or misleading connections due to the diversity of social relationships and the presence of weak ties.","Such noise can lead to inaccurate learning of user preferences and negatively affect prediction outcomes.","To address this issue, we propose DIDP, a novel social connection denoising framework for information diffusion prediction.","First, we introduce a graph learning encoder module that encodes the information diffusion hypergraph and social graph to obtain latent diffusion embeddings and social embeddings.","Next, DIDP integrates a denoising diffusion module to adaptively remove different types of noise from the learned social embeddings in the latent space.","Through multi-step noise injection and removal, the framework enhances the ability to learn robust embeddings.","Additionally, we introduce a cross-domain contrastive learning module that maximizes the mutual information between the diffusion embeddings and the denoised social embeddings of the same user.","This module guides the denoising process and facilitates cross-domain knowledge transfer.","Experimental results show that DIDP significantly outperforms state-of-the-art methods in the information diffusion prediction task."],"url":"http://arxiv.org/abs/2410.18492v1"}
{"created":"2024-10-24 07:25:12","title":"Synth4Seg -- Learning Defect Data Synthesis for Defect Segmentation using Bi-level Optimization","abstract":"Defect segmentation is crucial for quality control in advanced manufacturing, yet data scarcity poses challenges for state-of-the-art supervised deep learning. Synthetic defect data generation is a popular approach for mitigating data challenges. However, many current methods simply generate defects following a fixed set of rules, which may not directly relate to downstream task performance. This can lead to suboptimal performance and may even hinder the downstream task. To solve this problem, we leverage a novel bi-level optimization-based synthetic defect data generation framework. We use an online synthetic defect generation module grounded in the commonly-used Cut\\&Paste framework, and adopt an efficient gradient-based optimization algorithm to solve the bi-level optimization problem. We achieve simultaneous training of the defect segmentation network, and learn various parameters of the data synthesis module by maximizing the validation performance of the trained defect segmentation network. Our experimental results on benchmark datasets under limited data settings show that the proposed bi-level optimization method can be used for learning the most effective locations for pasting synthetic defects thereby improving the segmentation performance by up to 18.3\\% when compared to pasting defects at random locations. We also demonstrate up to 2.6\\% performance gain by learning the importance weights for different augmentation-specific defect data sources when compared to giving equal importance to all the data sources.","sentences":["Defect segmentation is crucial for quality control in advanced manufacturing, yet data scarcity poses challenges for state-of-the-art supervised deep learning.","Synthetic defect data generation is a popular approach for mitigating data challenges.","However, many current methods simply generate defects following a fixed set of rules, which may not directly relate to downstream task performance.","This can lead to suboptimal performance and may even hinder the downstream task.","To solve this problem, we leverage a novel bi-level optimization-based synthetic defect data generation framework.","We use an online synthetic defect generation module grounded in the commonly-used Cut\\&Paste framework, and adopt an efficient gradient-based optimization algorithm to solve the bi-level optimization problem.","We achieve simultaneous training of the defect segmentation network, and learn various parameters of the data synthesis module by maximizing the validation performance of the trained defect segmentation network.","Our experimental results on benchmark datasets under limited data settings show that the proposed bi-level optimization method can be used for learning the most effective locations for pasting synthetic defects thereby improving the segmentation performance by up to 18.3\\% when compared to pasting defects at random locations.","We also demonstrate up to 2.6\\% performance gain by learning the importance weights for different augmentation-specific defect data sources when compared to giving equal importance to all the data sources."],"url":"http://arxiv.org/abs/2410.18490v1"}
{"created":"2024-10-24 07:22:18","title":"Graph Pre-Training Models Are Strong Anomaly Detectors","abstract":"Graph Anomaly Detection (GAD) is a challenging and practical research topic where Graph Neural Networks (GNNs) have recently shown promising results. The effectiveness of existing GNNs in GAD has been mainly attributed to the simultaneous learning of node representations and the classifier in an end-to-end manner. Meanwhile, graph pre-training, the two-stage learning paradigm such as DGI and GraphMAE, has shown potential in leveraging unlabeled graph data to enhance downstream tasks, yet its impact on GAD remains under-explored. In this work, we show that graph pre-training models are strong graph anomaly detectors. Specifically, we demonstrate that pre-training is highly competitive, markedly outperforming the state-of-the-art end-to-end training models when faced with limited supervision. To understand this phenomenon, we further uncover pre-training enhances the detection of distant, under-represented, unlabeled anomalies that go beyond 2-hop neighborhoods of known anomalies, shedding light on its superior performance against end-to-end models. Moreover, we extend our examination to the potential of pre-training in graph-level anomaly detection. We envision this work to stimulate a re-evaluation of pre-training's role in GAD and offer valuable insights for future research.","sentences":["Graph Anomaly Detection (GAD) is a challenging and practical research topic where Graph Neural Networks (GNNs) have recently shown promising results.","The effectiveness of existing GNNs in GAD has been mainly attributed to the simultaneous learning of node representations and the classifier in an end-to-end manner.","Meanwhile, graph pre-training, the two-stage learning paradigm such as DGI and GraphMAE, has shown potential in leveraging unlabeled graph data to enhance downstream tasks, yet its impact on GAD remains under-explored.","In this work, we show that graph pre-training models are strong graph anomaly detectors.","Specifically, we demonstrate that pre-training is highly competitive, markedly outperforming the state-of-the-art end-to-end training models when faced with limited supervision.","To understand this phenomenon, we further uncover pre-training enhances the detection of distant, under-represented, unlabeled anomalies that go beyond 2-hop neighborhoods of known anomalies, shedding light on its superior performance against end-to-end models.","Moreover, we extend our examination to the potential of pre-training in graph-level anomaly detection.","We envision this work to stimulate a re-evaluation of pre-training's role in GAD and offer valuable insights for future research."],"url":"http://arxiv.org/abs/2410.18487v1"}
{"created":"2024-10-24 07:12:08","title":"FirmRCA: Towards Post-Fuzzing Analysis on ARM Embedded Firmware with Efficient Event-based Fault Localization","abstract":"While fuzzing has demonstrated its effectiveness in exposing vulnerabilities within embedded firmware, the discovery of crashing test cases is only the first step in improving the security of these critical systems. The subsequent fault localization process, which aims to precisely identify the root causes of observed crashes, is a crucial yet time-consuming post-fuzzing work. Unfortunately, the automated root cause analysis on embedded firmware crashes remains an underexplored area, which is challenging from several perspectives: (1) the fuzzing campaign towards the embedded firmware lacks adequate debugging mechanisms, making it hard to automatically extract essential runtime information for analysis; (2) the inherent raw binary nature of embedded firmware often leads to over-tainted and noisy suspicious instructions, which provides limited guidance for analysts in manually investigating the root cause and remediating the underlying vulnerability. To address these challenges, we design and implement FirmRCA, a practical fault localization framework tailored specifically for embedded firmware. FirmRCA introduces an event-based footprint collection approach to aid and significantly expedite reverse execution. Next, to solve the complicated memory alias problem, FirmRCA proposes a history-driven method by tracking data propagation through the execution trace, enabling precise identification of deep crash origins. Finally, FirmRCA proposes a novel strategy to highlight key instructions related to the root cause, providing practical guidance in the final investigation. We evaluate FirmRCA with both synthetic and real-world targets, including 41 crashing test cases across 17 firmware images. The results show that FirmRCA can effectively (92.7% success rate) identify the root cause of crashing test cases within the top 10 instructions.","sentences":["While fuzzing has demonstrated its effectiveness in exposing vulnerabilities within embedded firmware, the discovery of crashing test cases is only the first step in improving the security of these critical systems.","The subsequent fault localization process, which aims to precisely identify the root causes of observed crashes, is a crucial yet time-consuming post-fuzzing work.","Unfortunately, the automated root cause analysis on embedded firmware crashes remains an underexplored area, which is challenging from several perspectives: (1) the fuzzing campaign towards the embedded firmware lacks adequate debugging mechanisms, making it hard to automatically extract essential runtime information for analysis; (2) the inherent raw binary nature of embedded firmware often leads to over-tainted and noisy suspicious instructions, which provides limited guidance for analysts in manually investigating the root cause and remediating the underlying vulnerability.","To address these challenges, we design and implement FirmRCA, a practical fault localization framework tailored specifically for embedded firmware.","FirmRCA introduces an event-based footprint collection approach to aid and significantly expedite reverse execution.","Next, to solve the complicated memory alias problem, FirmRCA proposes a history-driven method by tracking data propagation through the execution trace, enabling precise identification of deep crash origins.","Finally, FirmRCA proposes a novel strategy to highlight key instructions related to the root cause, providing practical guidance in the final investigation.","We evaluate FirmRCA with both synthetic and real-world targets, including 41 crashing test cases across 17 firmware images.","The results show that FirmRCA can effectively (92.7% success rate) identify the root cause of crashing test cases within the top 10 instructions."],"url":"http://arxiv.org/abs/2410.18483v1"}
{"created":"2024-10-24 07:05:07","title":"DFEPT: Data Flow Embedding for Enhancing Pre-Trained Model Based Vulnerability Detection","abstract":"Software vulnerabilities represent one of the most pressing threats to computing systems. Identifying vulnerabilities in source code is crucial for protecting user privacy and reducing economic losses. Traditional static analysis tools rely on experts with knowledge in security to manually build rules for operation, a process that requires substantial time and manpower costs and also faces challenges in adapting to new vulnerabilities. The emergence of pre-trained code language models has provided a new solution for automated vulnerability detection. However, code pre-training models are typically based on token-level large-scale pre-training, which hampers their ability to effectively capture the structural and dependency relationships among code segments. In the context of software vulnerabilities, certain types of vulnerabilities are related to the dependency relationships within the code. Consequently, identifying and analyzing these vulnerability samples presents a significant challenge for pre-trained models. In this paper, we propose a data flow embedding technique to enhance the performance of pre-trained models in vulnerability detection tasks, named DFEPT, which provides effective vulnerability data flow information to pre-trained models. Specifically, we parse data flow graphs from function-level source code, and use the data type of the variable as the node characteristics of the DFG. By applying graph learning techniques, we embed the data flow graph and incorporate relative positional information into the graph embedding using sine positional encoding to ensure the completeness of vulnerability data flow information. Our research shows that DFEPT can provide effective vulnerability semantic information to pre-trained models, achieving an accuracy of 64.97% on the Devign dataset and an F1-Score of 47.9% on the Reveal dataset.","sentences":["Software vulnerabilities represent one of the most pressing threats to computing systems.","Identifying vulnerabilities in source code is crucial for protecting user privacy and reducing economic losses.","Traditional static analysis tools rely on experts with knowledge in security to manually build rules for operation, a process that requires substantial time and manpower costs and also faces challenges in adapting to new vulnerabilities.","The emergence of pre-trained code language models has provided a new solution for automated vulnerability detection.","However, code pre-training models are typically based on token-level large-scale pre-training, which hampers their ability to effectively capture the structural and dependency relationships among code segments.","In the context of software vulnerabilities, certain types of vulnerabilities are related to the dependency relationships within the code.","Consequently, identifying and analyzing these vulnerability samples presents a significant challenge for pre-trained models.","In this paper, we propose a data flow embedding technique to enhance the performance of pre-trained models in vulnerability detection tasks, named DFEPT, which provides effective vulnerability data flow information to pre-trained models.","Specifically, we parse data flow graphs from function-level source code, and use the data type of the variable as the node characteristics of the DFG.","By applying graph learning techniques, we embed the data flow graph and incorporate relative positional information into the graph embedding using sine positional encoding to ensure the completeness of vulnerability data flow information.","Our research shows that DFEPT can provide effective vulnerability semantic information to pre-trained models, achieving an accuracy of 64.97% on the Devign dataset and an F1-Score of 47.9% on the Reveal dataset."],"url":"http://arxiv.org/abs/2410.18479v1"}
{"created":"2024-10-24 07:04:52","title":"Classifier Clustering and Feature Alignment for Federated Learning under Distributed Concept Drift","abstract":"Data heterogeneity is one of the key challenges in federated learning, and many efforts have been devoted to tackling this problem. However, distributed concept drift with data heterogeneity, where clients may additionally experience different concept drifts, is a largely unexplored area. In this work, we focus on real drift, where the conditional distribution $P(Y|X)$ changes. We first study how distributed concept drift affects the model training and find that local classifier plays a critical role in drift adaptation. Moreover, to address data heterogeneity, we study the feature alignment under distributed concept drift, and find two factors that are crucial for feature alignment: the conditional distribution $P(Y|X)$ and the degree of data heterogeneity. Motivated by the above findings, we propose FedCCFA, a federated learning framework with classifier clustering and feature alignment. To enhance collaboration under distributed concept drift, FedCCFA clusters local classifiers at class-level and generates clustered feature anchors according to the clustering results. Assisted by these anchors, FedCCFA adaptively aligns clients' feature spaces based on the entropy of label distribution $P(Y)$, alleviating the inconsistency in feature space. Our results demonstrate that FedCCFA significantly outperforms existing methods under various concept drift settings. Code is available at https://github.com/Chen-Junbao/FedCCFA.","sentences":["Data heterogeneity is one of the key challenges in federated learning, and many efforts have been devoted to tackling this problem.","However, distributed concept drift with data heterogeneity, where clients may additionally experience different concept drifts, is a largely unexplored area.","In this work, we focus on real drift, where the conditional distribution $P(Y|X)$ changes.","We first study how distributed concept drift affects the model training and find that local classifier plays a critical role in drift adaptation.","Moreover, to address data heterogeneity, we study the feature alignment under distributed concept drift, and find two factors that are crucial for feature alignment: the conditional distribution $P(Y|X)$ and the degree of data heterogeneity.","Motivated by the above findings, we propose FedCCFA, a federated learning framework with classifier clustering and feature alignment.","To enhance collaboration under distributed concept drift, FedCCFA clusters local classifiers at class-level and generates clustered feature anchors according to the clustering results.","Assisted by these anchors, FedCCFA adaptively aligns clients' feature spaces based on the entropy of label distribution $P(Y)$, alleviating the inconsistency in feature space.","Our results demonstrate that FedCCFA significantly outperforms existing methods under various concept drift settings.","Code is available at https://github.com/Chen-Junbao/FedCCFA."],"url":"http://arxiv.org/abs/2410.18478v1"}
{"created":"2024-10-24 06:47:28","title":"What If the Input is Expanded in OOD Detection?","abstract":"Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world. Various scoring functions are proposed to distinguish it from in-distribution (ID) data. However, existing methods generally focus on excavating the discriminative information from a single input, which implicitly limits its representation dimension. In this work, we introduce a novel perspective, i.e., employing different common corruptions on the input space, to expand that. We reveal an interesting phenomenon termed confidence mutation, where the confidence of OOD data can decrease significantly under the corruptions, while the ID data shows a higher confidence expectation considering the resistance of semantic features. Based on that, we formalize a new scoring method, namely, Confidence aVerage (CoVer), which can capture the dynamic differences by simply averaging the scores obtained from different corrupted inputs and the original ones, making the OOD and ID distributions more separable in detection tasks. Extensive experiments and analyses have been conducted to understand and verify the effectiveness of CoVer. The code is publicly available at: https://github.com/tmlr-group/CoVer.","sentences":["Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world.","Various scoring functions are proposed to distinguish it from in-distribution (ID) data.","However, existing methods generally focus on excavating the discriminative information from a single input, which implicitly limits its representation dimension.","In this work, we introduce a novel perspective, i.e., employing different common corruptions on the input space, to expand that.","We reveal an interesting phenomenon termed confidence mutation, where the confidence of OOD data can decrease significantly under the corruptions, while the ID data shows a higher confidence expectation considering the resistance of semantic features.","Based on that, we formalize a new scoring method, namely, Confidence aVerage (CoVer), which can capture the dynamic differences by simply averaging the scores obtained from different corrupted inputs and the original ones, making the OOD and ID distributions more separable in detection tasks.","Extensive experiments and analyses have been conducted to understand and verify the effectiveness of CoVer.","The code is publicly available at: https://github.com/tmlr-group/CoVer."],"url":"http://arxiv.org/abs/2410.18472v1"}
{"created":"2024-10-24 06:10:31","title":"Integrating Deep Feature Extraction and Hybrid ResNet-DenseNet Model for Multi-Class Abnormality Detection in Endoscopic Images","abstract":"This paper presents a deep learning framework for the multi-class classification of gastrointestinal abnormalities in Video Capsule Endoscopy (VCE) frames. The aim is to automate the identification of ten GI abnormality classes, including angioectasia, bleeding, and ulcers, thereby reducing the diagnostic burden on gastroenterologists. Utilizing an ensemble of DenseNet and ResNet architectures, the proposed model achieves an overall accuracy of 94\\% across a well-structured dataset. Precision scores range from 0.56 for erythema to 1.00 for worms, with recall rates peaking at 98% for normal findings. This study emphasizes the importance of robust data preprocessing techniques, including normalization and augmentation, in enhancing model performance. The contributions of this work lie in developing an effective AI-driven tool that streamlines the diagnostic process in gastroenterology, ultimately improving patient care and clinical outcomes.","sentences":["This paper presents a deep learning framework for the multi-class classification of gastrointestinal abnormalities in Video Capsule Endoscopy (VCE) frames.","The aim is to automate the identification of ten GI abnormality classes, including angioectasia, bleeding, and ulcers, thereby reducing the diagnostic burden on gastroenterologists.","Utilizing an ensemble of DenseNet and ResNet architectures, the proposed model achieves an overall accuracy of 94\\% across a well-structured dataset.","Precision scores range from 0.56 for erythema to 1.00 for worms, with recall rates peaking at 98% for normal findings.","This study emphasizes the importance of robust data preprocessing techniques, including normalization and augmentation, in enhancing model performance.","The contributions of this work lie in developing an effective AI-driven tool that streamlines the diagnostic process in gastroenterology, ultimately improving patient care and clinical outcomes."],"url":"http://arxiv.org/abs/2410.18457v1"}
{"created":"2024-10-24 06:06:26","title":"Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs","abstract":"In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairs -- significantly smaller than existing datasets. Using this curated dataset, we developed the Skywork-Reward model series -- Skywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8B -- with the former currently holding the top position on the RewardBench leaderboard. Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications.","sentences":["In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques.","We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairs -- significantly smaller than existing datasets.","Using this curated dataset, we developed the Skywork-Reward model series -- Skywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8B -- with the former currently holding the top position on the RewardBench leaderboard.","Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications."],"url":"http://arxiv.org/abs/2410.18451v1"}
{"created":"2024-10-24 05:49:41","title":"GPT-Signal: Generative AI for Semi-automated Feature Engineering in the Alpha Research Process","abstract":"In the trading process, financial signals often imply the time to buy and sell assets to generate excess returns compared to a benchmark (e.g., an index). Alpha is the portion of an asset's return that is not explained by exposure to this benchmark, and the alpha research process is a popular technique aiming at developing strategies to generate alphas and gain excess returns. Feature Engineering, a significant pre-processing procedure in machine learning and data analysis that helps extract and create transformed features from raw data, plays an important role in algorithmic trading strategies and the alpha research process. With the recent development of Generative Artificial Intelligence(Gen AI) and Large Language Models (LLMs), we present a novel way of leveraging GPT-4 to generate new return-predictive formulaic alphas, making alpha mining a semi-automated process, and saving time and energy for investors and traders.","sentences":["In the trading process, financial signals often imply the time to buy and sell assets to generate excess returns compared to a benchmark (e.g., an index).","Alpha is the portion of an asset's return that is not explained by exposure to this benchmark, and the alpha research process is a popular technique aiming at developing strategies to generate alphas and gain excess returns.","Feature Engineering, a significant pre-processing procedure in machine learning and data analysis that helps extract and create transformed features from raw data, plays an important role in algorithmic trading strategies and the alpha research process.","With the recent development of Generative Artificial Intelligence(Gen AI) and Large Language Models (LLMs), we present a novel way of leveraging GPT-4 to generate new return-predictive formulaic alphas, making alpha mining a semi-automated process, and saving time and energy for investors and traders."],"url":"http://arxiv.org/abs/2410.18448v1"}
{"created":"2024-10-24 05:45:04","title":"ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis","abstract":"Supervised fine-tuning (SFT) is a common method to enhance the tool calling capabilities of Large Language Models (LLMs), with the training data often being synthesized. The current data synthesis process generally involves sampling a set of tools, formulating a requirement based on these tools, and generating the call statements. However, tools sampled randomly lack relevance, making them difficult to combine and thus reducing the diversity of the data. Additionally, current work overlooks the coherence between turns of dialogues, leading to a gap between the synthesized data and real-world scenarios. To address these issues, we propose a Graph-based Sampling strategy to sample more relevant tool combinations, and a Planned-generation strategy to create plans that guide the synthesis of coherent dialogues. We integrate these two strategies and enable multiple agents to synthesize the dialogue data interactively, resulting in our tool-calling data synthesis pipeline ToolFlow. Data quality assessments demonstrate improvements in the naturalness and coherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B using 8,000 synthetic dialogues generated with ToolFlow. Results show that the model achieves tool-calling performance comparable to or even surpassing GPT-4, while maintaining strong general capabilities.","sentences":["Supervised fine-tuning (SFT) is a common method to enhance the tool calling capabilities of Large Language Models (LLMs), with the training data often being synthesized.","The current data synthesis process generally involves sampling a set of tools, formulating a requirement based on these tools, and generating the call statements.","However, tools sampled randomly lack relevance, making them difficult to combine and thus reducing the diversity of the data.","Additionally, current work overlooks the coherence between turns of dialogues, leading to a gap between the synthesized data and real-world scenarios.","To address these issues, we propose a Graph-based Sampling strategy to sample more relevant tool combinations, and a Planned-generation strategy to create plans that guide the synthesis of coherent dialogues.","We integrate these two strategies and enable multiple agents to synthesize the dialogue data interactively, resulting in our tool-calling data synthesis pipeline ToolFlow.","Data quality assessments demonstrate improvements in the naturalness and coherence of our synthesized dialogues.","Finally, we apply SFT on LLaMA-3.1-8B using 8,000 synthetic dialogues generated with ToolFlow.","Results show that the model achieves tool-calling performance comparable to or even surpassing GPT-4, while maintaining strong general capabilities."],"url":"http://arxiv.org/abs/2410.18447v1"}
{"created":"2024-10-24 05:40:07","title":"Evaluating and Improving Automatic Speech Recognition Systems for Korean Meteorological Experts","abstract":"This paper explores integrating Automatic Speech Recognition (ASR) into natural language query systems to improve weather forecasting efficiency for Korean meteorologists. We address challenges in developing ASR systems for the Korean weather domain, specifically specialized vocabulary and Korean linguistic intricacies. To tackle these issues, we constructed an evaluation dataset of spoken queries recorded by native Korean speakers. Using this dataset, we assessed various configurations of a multilingual ASR model family, identifying performance limitations related to domain-specific terminology. We then implemented a simple text-to-speech-based data augmentation method, which improved the recognition of specialized terms while maintaining general-domain performance. Our contributions include creating a domain-specific dataset, comprehensive ASR model evaluations, and an effective augmentation technique. We believe our work provides a foundation for future advancements in ASR for the Korean weather forecasting domain.","sentences":["This paper explores integrating Automatic Speech Recognition (ASR) into natural language query systems to improve weather forecasting efficiency for Korean meteorologists.","We address challenges in developing ASR systems for the Korean weather domain, specifically specialized vocabulary and Korean linguistic intricacies.","To tackle these issues, we constructed an evaluation dataset of spoken queries recorded by native Korean speakers.","Using this dataset, we assessed various configurations of a multilingual ASR model family, identifying performance limitations related to domain-specific terminology.","We then implemented a simple text-to-speech-based data augmentation method, which improved the recognition of specialized terms while maintaining general-domain performance.","Our contributions include creating a domain-specific dataset, comprehensive ASR model evaluations, and an effective augmentation technique.","We believe our work provides a foundation for future advancements in ASR for the Korean weather forecasting domain."],"url":"http://arxiv.org/abs/2410.18444v1"}
{"created":"2024-10-24 05:29:20","title":"The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI","abstract":"In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model [17]. In addition, we propose a factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with a harmonic series. We also present a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a probability distribution over block distances in the matrix to decide which block is likely to participate in a given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings.","sentences":["In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI.","We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization perspective.","In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data.","We also present cross entropy optimization method to optimize hyperparameters for word2vec model [17].","In addition, we propose a factored combination of rotary positional encoding (RoPE)","[32] and attention with linear biases (ALiBi)","[23] with a harmonic series.","We also present a probabilistic FlashAttention","[6, 7] (PrFlashAttention) method with a probability distribution over block distances in the matrix to decide which block is likely to participate in a given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors.","Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings."],"url":"http://arxiv.org/abs/2410.18441v1"}
{"created":"2024-10-24 04:33:14","title":"Building Dialogue Understanding Models for Low-resource Language Indonesian from Scratch","abstract":"Making use of off-the-shelf resources of resource-rich languages to transfer knowledge for low-resource languages raises much attention recently. The requirements of enabling the model to reach the reliable performance lack well guided, such as the scale of required annotated data or the effective framework. To investigate the first question, we empirically investigate the cost-effectiveness of several methods to train the intent classification and slot-filling models for Indonesia (ID) from scratch by utilizing the English data. Confronting the second challenge, we propose a Bi-Confidence-Frequency Cross-Lingual transfer framework (BiCF), composed by ``BiCF Mixing'', ``Latent Space Refinement'' and ``Joint Decoder'', respectively, to tackle the obstacle of lacking low-resource language dialogue data. Extensive experiments demonstrate our framework performs reliably and cost-efficiently on different scales of manually annotated Indonesian data. We release a large-scale fine-labeled dialogue dataset (ID-WOZ) and ID-BERT of Indonesian for further research.","sentences":["Making use of off-the-shelf resources of resource-rich languages to transfer knowledge for low-resource languages raises much attention recently.","The requirements of enabling the model to reach the reliable performance lack well guided, such as the scale of required annotated data or the effective framework.","To investigate the first question, we empirically investigate the cost-effectiveness of several methods to train the intent classification and slot-filling models for Indonesia (ID) from scratch by utilizing the English data.","Confronting the second challenge, we propose a Bi-Confidence-Frequency Cross-Lingual transfer framework (BiCF), composed by ``BiCF Mixing'', ``Latent Space Refinement'' and ``Joint Decoder'', respectively, to tackle the obstacle of lacking low-resource language dialogue data.","Extensive experiments demonstrate our framework performs reliably and cost-efficiently on different scales of manually annotated Indonesian data.","We release a large-scale fine-labeled dialogue dataset (ID-WOZ) and ID-BERT of Indonesian for further research."],"url":"http://arxiv.org/abs/2410.18430v1"}
{"created":"2024-10-24 04:24:47","title":"Doubly Non-Central Beta Matrix Factorization for Stable Dimensionality Reduction of Bounded Support Matrix Data","abstract":"We consider the problem of developing interpretable and computationally efficient matrix decomposition methods for matrices whose entries have bounded support. Such matrices are found in large-scale DNA methylation studies and many other settings. Our approach decomposes the data matrix into a Tucker representation wherein the number of columns in the constituent factor matrices is not constrained. We derive a computationally efficient sampling algorithm to solve for the Tucker decomposition. We evaluate the performance of our method using three criteria: predictability, computability, and stability. Empirical results show that our method has similar performance as other state-of-the-art approaches in terms of held-out prediction and computational complexity, but has significantly better performance in terms of stability to changes in hyper-parameters. The improved stability results in higher confidence in the results in applications where the constituent factors are used to generate and test scientific hypotheses such as DNA methylation analysis of cancer samples.","sentences":["We consider the problem of developing interpretable and computationally efficient matrix decomposition methods for matrices whose entries have bounded support.","Such matrices are found in large-scale DNA methylation studies and many other settings.","Our approach decomposes the data matrix into a Tucker representation wherein the number of columns in the constituent factor matrices is not constrained.","We derive a computationally efficient sampling algorithm to solve for the Tucker decomposition.","We evaluate the performance of our method using three criteria: predictability, computability, and stability.","Empirical results show that our method has similar performance as other state-of-the-art approaches in terms of held-out prediction and computational complexity, but has significantly better performance in terms of stability to changes in hyper-parameters.","The improved stability results in higher confidence in the results in applications where the constituent factors are used to generate and test scientific hypotheses such as DNA methylation analysis of cancer samples."],"url":"http://arxiv.org/abs/2410.18425v1"}
{"created":"2024-10-24 04:23:57","title":"A Causal Graph-Enhanced Gaussian Process Regression for Modeling Engine-out NOx","abstract":"The stringent regulatory requirements on nitrogen oxides (NOx) emissions from diesel compression ignition engines require accurate and reliable models for real-time monitoring and diagnostics. Although traditional methods such as physical sensors and virtual engine control module (ECM) sensors provide essential data, they are only used for estimation. Ubiquitous literature primarily focuses on deterministic models with little emphasis on capturing the uncertainties due to sensors. The lack of probabilistic frameworks restricts the applicability of these models for robust diagnostics. The objective of this paper is to develop and validate a probabilistic model to predict engine-out NOx emissions using Gaussian process regression. Our approach is as follows. We employ three variants of Gaussian process models: the first with a standard radial basis function kernel with input window, the second incorporating a deep kernel using convolutional neural networks to capture temporal dependencies, and the third enriching the deep kernel with a causal graph derived via graph convolutional networks. The causal graph embeds physics knowledge into the learning process. All models are compared against a virtual ECM sensor using both quantitative and qualitative metrics. We conclude that our model provides an improvement in predictive performance when using an input window and a deep kernel structure. Even more compelling is the further enhancement achieved by the incorporation of a causal graph into the deep kernel. These findings are corroborated across different validation datasets.","sentences":["The stringent regulatory requirements on nitrogen oxides (NOx) emissions from diesel compression ignition engines require accurate and reliable models for real-time monitoring and diagnostics.","Although traditional methods such as physical sensors and virtual engine control module (ECM) sensors provide essential data, they are only used for estimation.","Ubiquitous literature primarily focuses on deterministic models with little emphasis on capturing the uncertainties due to sensors.","The lack of probabilistic frameworks restricts the applicability of these models for robust diagnostics.","The objective of this paper is to develop and validate a probabilistic model to predict engine-out NOx emissions using Gaussian process regression.","Our approach is as follows.","We employ three variants of Gaussian process models: the first with a standard radial basis function kernel with input window, the second incorporating a deep kernel using convolutional neural networks to capture temporal dependencies, and the third enriching the deep kernel with a causal graph derived via graph convolutional networks.","The causal graph embeds physics knowledge into the learning process.","All models are compared against a virtual ECM sensor using both quantitative and qualitative metrics.","We conclude that our model provides an improvement in predictive performance when using an input window and a deep kernel structure.","Even more compelling is the further enhancement achieved by the incorporation of a causal graph into the deep kernel.","These findings are corroborated across different validation datasets."],"url":"http://arxiv.org/abs/2410.18424v1"}
{"created":"2024-10-24 04:05:20","title":"Knowledge-Assisted Privacy Preserving in Semantic Communication","abstract":"Semantic communication (SC) offers promising advancements in data transmission efficiency and reliability by focusing on delivering true meaning rather than solely binary bits of messages. However, privacy concerns in SC might become outstanding. Eavesdroppers equipped with advanced semantic coding models and extensive knowledge could be capable of correctly decoding and reasoning sensitive semantics from just a few stolen bits. To this end, this article explores utilizing knowledge to enhance data privacy in SC networks. Specifically, we first identify the potential attacks in SC based on the analysis of knowledge. Then, we propose a knowledge-assisted privacy preserving SC framework, which consists of a data transmission layer for precisely encoding and decoding source messages, and a knowledge management layer responsible for injecting appropriate knowledge into the transmission pair. Moreover, we elaborate on the transceiver design in the proposed SC framework to explain how knowledge should be utilized properly. Finally, some challenges of the proposed SC framework are discussed to expedite the practical implementation.","sentences":["Semantic communication (SC) offers promising advancements in data transmission efficiency and reliability by focusing on delivering true meaning rather than solely binary bits of messages.","However, privacy concerns in SC might become outstanding.","Eavesdroppers equipped with advanced semantic coding models and extensive knowledge could be capable of correctly decoding and reasoning sensitive semantics from just a few stolen bits.","To this end, this article explores utilizing knowledge to enhance data privacy in SC networks.","Specifically, we first identify the potential attacks in SC based on the analysis of knowledge.","Then, we propose a knowledge-assisted privacy preserving SC framework, which consists of a data transmission layer for precisely encoding and decoding source messages, and a knowledge management layer responsible for injecting appropriate knowledge into the transmission pair.","Moreover, we elaborate on the transceiver design in the proposed SC framework to explain how knowledge should be utilized properly.","Finally, some challenges of the proposed SC framework are discussed to expedite the practical implementation."],"url":"http://arxiv.org/abs/2410.18418v1"}
{"created":"2024-10-24 04:02:30","title":"Large Language Models Reflect the Ideology of their Creators","abstract":"Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use.   In this paper, we uncover notable diversity in the ideological stance exhibited across different LLMs and languages in which they are accessed. We do this by prompting a diverse panel of popular LLMs to describe a large number of prominent and controversial personalities from recent world history, both in English and in Chinese. By identifying and analyzing moral assessments reflected in the generated descriptions, we find consistent normative differences between how the same LLM responds in Chinese compared to English. Similarly, we identify normative disagreements between Western and non-Western LLMs about prominent actors in geopolitical conflicts. Furthermore, popularly hypothesized disparities in political goals among Western models are reflected in significant normative differences related to inclusion, social inequality, and political scandals.   Our results show that the ideological stance of an LLM often reflects the worldview of its creators. This raises important concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically `unbiased', and it poses risks for political instrumentalization.","sentences":["Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering.","These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information.","However, the behavior of LLMs varies depending on their design, training, and use.   ","In this paper, we uncover notable diversity in the ideological stance exhibited across different LLMs and languages in which they are accessed.","We do this by prompting a diverse panel of popular LLMs to describe a large number of prominent and controversial personalities from recent world history, both in English and in Chinese.","By identifying and analyzing moral assessments reflected in the generated descriptions, we find consistent normative differences between how the same LLM responds in Chinese compared to English.","Similarly, we identify normative disagreements between Western and non-Western LLMs about prominent actors in geopolitical conflicts.","Furthermore, popularly hypothesized disparities in political goals among Western models are reflected in significant normative differences related to inclusion, social inequality, and political scandals.   ","Our results show that the ideological stance of an LLM often reflects the worldview of its creators.","This raises important concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically `unbiased', and it poses risks for political instrumentalization."],"url":"http://arxiv.org/abs/2410.18417v1"}
{"created":"2024-10-24 03:59:02","title":"HardRace: A Dynamic Data Race Monitor for Production Use","abstract":"Data races are critical issues in multithreaded program, leading to unpredictable, catastrophic and difficult-to-diagnose problems. Despite the extensive in-house testing, data races often escape to deployed software and manifest in production runs. Existing approaches suffer from either prohibitively high runtime overhead or incomplete detection capability. In this paper, we introduce HardRace, a data race monitor to detect races on-the-fly while with sufficiently low runtime overhead and high detection capability. HardRace firstly employs sound static analysis to determine a minimal set of essential memory accesses relevant to data races. It then leverages hardware trace instruction, i.e., Intel PTWRITE, to selectively record only these memory accesses and thread synchronization events during execution with negligible runtime overhead. Given the tracing data, HardRace performs standard data race detection algorithms to timely report potential races occurred in production runs. The experimental evaluations show that HardRace outperforms state-of-the-art tools like ProRace and Kard in terms of both runtime overhead and detection capability -- HardRace can detect all kinds of data races in read-world applications while maintaining a negligible overhead, less than 2% on average.","sentences":["Data races are critical issues in multithreaded program, leading to unpredictable, catastrophic and difficult-to-diagnose problems.","Despite the extensive in-house testing, data races often escape to deployed software and manifest in production runs.","Existing approaches suffer from either prohibitively high runtime overhead or incomplete detection capability.","In this paper, we introduce HardRace, a data race monitor to detect races on-the-fly while with sufficiently low runtime overhead and high detection capability.","HardRace firstly employs sound static analysis to determine a minimal set of essential memory accesses relevant to data races.","It then leverages hardware trace instruction, i.e., Intel PTWRITE, to selectively record only these memory accesses and thread synchronization events during execution with negligible runtime overhead.","Given the tracing data, HardRace performs standard data race detection algorithms to timely report potential races occurred in production runs.","The experimental evaluations show that HardRace outperforms state-of-the-art tools like ProRace and Kard in terms of both runtime overhead and detection capability -- HardRace can detect all kinds of data races in read-world applications while maintaining a negligible overhead, less than 2% on average."],"url":"http://arxiv.org/abs/2410.18412v1"}
{"created":"2024-10-24 03:57:45","title":"Federated Single Sign-On and Zero Trust Co-design for AI and HPC Digital Research Infrastructures","abstract":"Scientific workflows have become highly heterogenous, leveraging distributed facilities such as High Performance Computing (HPC), Artificial Intelligence (AI), Machine Learning (ML), scientific instruments (data-driven pipelines) and edge computing. As a result, Identity and Access Management (IAM) and Cybersecurity challenges across the diverse hardware and software stacks are growing. Nevertheless, scientific productivity relies on lowering access barriers via seamless, single sign-on (SSO) and federated login while ensuring access controls and compliance. We present an implementation of a federated IAM solution, which is coupled with multiple layers of security controls, multi-factor authentication, cloud-native protocols, and time-limited role-based access controls (RBAC) that has been co-designed and deployed for the Isambard-AI and HPC supercomputing Digital Research Infrastructures (DRIs) in the UK. Isambard DRIs as a national research resource are expected to comply with regulatory frameworks. Implementation details for monitoring, alerting and controls are outlined in the paper alongside selected user stories for demonstrating IAM workflows for different roles.","sentences":["Scientific workflows have become highly heterogenous, leveraging distributed facilities such as High Performance Computing (HPC), Artificial Intelligence (AI), Machine Learning (ML), scientific instruments (data-driven pipelines) and edge computing.","As a result, Identity and Access Management (IAM) and Cybersecurity challenges across the diverse hardware and software stacks are growing.","Nevertheless, scientific productivity relies on lowering access barriers via seamless, single sign-on (SSO) and federated login while ensuring access controls and compliance.","We present an implementation of a federated IAM solution, which is coupled with multiple layers of security controls, multi-factor authentication, cloud-native protocols, and time-limited role-based access controls (RBAC) that has been co-designed and deployed for the Isambard-AI and HPC supercomputing Digital Research Infrastructures (DRIs) in the UK.","Isambard DRIs as a national research resource are expected to comply with regulatory frameworks.","Implementation details for monitoring, alerting and controls are outlined in the paper alongside selected user stories for demonstrating IAM workflows for different roles."],"url":"http://arxiv.org/abs/2410.18411v1"}
{"created":"2024-10-24 03:42:43","title":"MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases","abstract":"The improvement in translating natural language to structured query language (SQL) can be attributed to the advancements in large language models (LLMs). Open-source LLMs, tailored for specific database dialects such as MySQL, have shown great performance. However, cloud service providers are looking for a unified database manager service (e.g., Cosmos DB from Azure, Amazon Aurora from AWS, Lindorm from AlibabaCloud) that can support multiple dialects. This requirement has led to the concept of multi-dialect query generation, which presents challenges to LLMs. These challenges include syntactic differences among dialects and imbalanced data distribution across multiple dialects. To tackle these challenges, we propose MoMQ, a novel Mixture-of-Experts-based multi-dialect query generation framework across both relational and non-relational databases. MoMQ employs a dialect expert group for each dialect and a multi-level routing strategy to handle dialect-specific knowledge, reducing interference during query generation. Additionally, a shared expert group is introduced to address data imbalance, facilitating the transfer of common knowledge from high-resource dialects to low-resource ones. Furthermore, we have developed a high-quality multi-dialect query generation benchmark that covers relational and non-relational databases such as MySQL, PostgreSQL, Cypher for Neo4j, and nGQL for NebulaGraph. Extensive experiments have shown that MoMQ performs effectively and robustly even in resource-imbalanced scenarios.","sentences":["The improvement in translating natural language to structured query language (SQL) can be attributed to the advancements in large language models (LLMs).","Open-source LLMs, tailored for specific database dialects such as MySQL, have shown great performance.","However, cloud service providers are looking for a unified database manager service (e.g., Cosmos DB from Azure, Amazon Aurora from AWS, Lindorm from AlibabaCloud) that can support multiple dialects.","This requirement has led to the concept of multi-dialect query generation, which presents challenges to LLMs.","These challenges include syntactic differences among dialects and imbalanced data distribution across multiple dialects.","To tackle these challenges, we propose MoMQ, a novel Mixture-of-Experts-based multi-dialect query generation framework across both relational and non-relational databases.","MoMQ employs a dialect expert group for each dialect and a multi-level routing strategy to handle dialect-specific knowledge, reducing interference during query generation.","Additionally, a shared expert group is introduced to address data imbalance, facilitating the transfer of common knowledge from high-resource dialects to low-resource ones.","Furthermore, we have developed a high-quality multi-dialect query generation benchmark that covers relational and non-relational databases such as MySQL, PostgreSQL, Cypher for Neo4j, and nGQL for NebulaGraph.","Extensive experiments have shown that MoMQ performs effectively and robustly even in resource-imbalanced scenarios."],"url":"http://arxiv.org/abs/2410.18406v1"}
{"created":"2024-10-24 03:39:55","title":"Enhancing Feature-Specific Data Protection via Bayesian Coordinate Differential Privacy","abstract":"Local Differential Privacy (LDP) offers strong privacy guarantees without requiring users to trust external parties. However, LDP applies uniform protection to all data features, including less sensitive ones, which degrades performance of downstream tasks. To overcome this limitation, we propose a Bayesian framework, Bayesian Coordinate Differential Privacy (BCDP), that enables feature-specific privacy quantification. This more nuanced approach complements LDP by adjusting privacy protection according to the sensitivity of each feature, enabling improved performance of downstream tasks without compromising privacy. We characterize the properties of BCDP and articulate its connections with standard non-Bayesian privacy frameworks. We further apply our BCDP framework to the problems of private mean estimation and ordinary least-squares regression. The BCDP-based approach obtains improved accuracy compared to a purely LDP-based approach, without compromising on privacy.","sentences":["Local Differential Privacy (LDP) offers strong privacy guarantees without requiring users to trust external parties.","However, LDP applies uniform protection to all data features, including less sensitive ones, which degrades performance of downstream tasks.","To overcome this limitation, we propose a Bayesian framework, Bayesian Coordinate Differential Privacy (BCDP), that enables feature-specific privacy quantification.","This more nuanced approach complements LDP by adjusting privacy protection according to the sensitivity of each feature, enabling improved performance of downstream tasks without compromising privacy.","We characterize the properties of BCDP and articulate its connections with standard non-Bayesian privacy frameworks.","We further apply our BCDP framework to the problems of private mean estimation and ordinary least-squares regression.","The BCDP-based approach obtains improved accuracy compared to a purely LDP-based approach, without compromising on privacy."],"url":"http://arxiv.org/abs/2410.18404v1"}
{"created":"2024-10-24 03:29:57","title":"DMVC: Multi-Camera Video Compression Network aimed at Improving Deep Learning Accuracy","abstract":"We introduce a cutting-edge video compression framework tailored for the age of ubiquitous video data, uniquely designed to serve machine learning applications. Unlike traditional compression methods that prioritize human visual perception, our innovative approach focuses on preserving semantic information critical for deep learning accuracy, while efficiently reducing data size. The framework operates on a batch basis, capable of handling multiple video streams simultaneously, thereby enhancing scalability and processing efficiency. It features a dual reconstruction mode: lightweight for real-time applications requiring swift responses, and high-precision for scenarios where accuracy is crucial. Based on a designed deep learning algorithms, it adeptly segregates essential information from redundancy, ensuring machine learning tasks are fed with data of the highest relevance. Our experimental results, derived from diverse datasets including urban surveillance and autonomous vehicle navigation, showcase DMVC's superiority in maintaining or improving machine learning task accuracy, while achieving significant data compression. This breakthrough paves the way for smarter, scalable video analysis systems, promising immense potential across various applications from smart city infrastructure to autonomous systems, establishing a new benchmark for integrating video compression with machine learning.","sentences":["We introduce a cutting-edge video compression framework tailored for the age of ubiquitous video data, uniquely designed to serve machine learning applications.","Unlike traditional compression methods that prioritize human visual perception, our innovative approach focuses on preserving semantic information critical for deep learning accuracy, while efficiently reducing data size.","The framework operates on a batch basis, capable of handling multiple video streams simultaneously, thereby enhancing scalability and processing efficiency.","It features a dual reconstruction mode: lightweight for real-time applications requiring swift responses, and high-precision for scenarios where accuracy is crucial.","Based on a designed deep learning algorithms, it adeptly segregates essential information from redundancy, ensuring machine learning tasks are fed with data of the highest relevance.","Our experimental results, derived from diverse datasets including urban surveillance and autonomous vehicle navigation, showcase DMVC's superiority in maintaining or improving machine learning task accuracy, while achieving significant data compression.","This breakthrough paves the way for smarter, scalable video analysis systems, promising immense potential across various applications from smart city infrastructure to autonomous systems, establishing a new benchmark for integrating video compression with machine learning."],"url":"http://arxiv.org/abs/2410.18400v1"}
{"created":"2024-10-24 03:17:14","title":"Revisiting Differentiable Structure Learning: Inconsistency of $\\ell_1$ Penalty and Beyond","abstract":"Recent advances in differentiable structure learning have framed the combinatorial problem of learning directed acyclic graphs as a continuous optimization problem. Various aspects, including data standardization, have been studied to identify factors that influence the empirical performance of these methods. In this work, we investigate critical limitations in differentiable structure learning methods, focusing on settings where the true structure can be identified up to Markov equivalence classes, particularly in the linear Gaussian case. While Ng et al. (2024) highlighted potential non-convexity issues in this setting, we demonstrate and explain why the use of $\\ell_1$-penalized likelihood in such cases is fundamentally inconsistent, even if the global optimum of the optimization problem can be found. To resolve this limitation, we develop a hybrid differentiable structure learning method based on $\\ell_0$-penalized likelihood with hard acyclicity constraint, where the $\\ell_0$ penalty can be approximated by different techniques including Gumbel-Softmax. Specifically, we first estimate the underlying moral graph, and use it to restrict the search space of the optimization problem, which helps alleviate the non-convexity issue. Experimental results show that the proposed method enhances empirical performance both before and after data standardization, providing a more reliable path for future advancements in differentiable structure learning, especially for learning Markov equivalence classes.","sentences":["Recent advances in differentiable structure learning have framed the combinatorial problem of learning directed acyclic graphs as a continuous optimization problem.","Various aspects, including data standardization, have been studied to identify factors that influence the empirical performance of these methods.","In this work, we investigate critical limitations in differentiable structure learning methods, focusing on settings where the true structure can be identified up to Markov equivalence classes, particularly in the linear Gaussian case.","While Ng et al. (2024) highlighted potential non-convexity issues in this setting, we demonstrate and explain why the use of $\\ell_1$-penalized likelihood in such cases is fundamentally inconsistent, even if the global optimum of the optimization problem can be found.","To resolve this limitation, we develop a hybrid differentiable structure learning method based on $\\ell_0$-penalized likelihood with hard acyclicity constraint, where the $\\ell_0$ penalty can be approximated by different techniques including Gumbel-Softmax.","Specifically, we first estimate the underlying moral graph, and use it to restrict the search space of the optimization problem, which helps alleviate the non-convexity issue.","Experimental results show that the proposed method enhances empirical performance both before and after data standardization, providing a more reliable path for future advancements in differentiable structure learning, especially for learning Markov equivalence classes."],"url":"http://arxiv.org/abs/2410.18396v1"}
{"created":"2024-10-24 03:13:53","title":"A contrastive-learning approach for auditory attention detection","abstract":"Carrying conversations in multi-sound environments is one of the more challenging tasks, since the sounds overlap across time and frequency making it difficult to understand a single sound source. One proposed approach to help isolate an attended speech source is through decoding the electroencephalogram (EEG) and identifying the attended audio source using statistical or machine learning techniques. However, the limited amount of data in comparison to other machine learning problems and the distributional shift between different EEG recordings emphasizes the need for a self supervised approach that works with limited data to achieve a more robust solution. In this paper, we propose a method based on self supervised learning to minimize the difference between the latent representations of an attended speech signal and the corresponding EEG signal. This network is further finetuned for the auditory attention classification task. We compare our results with previously published methods and achieve state-of-the-art performance on the validation set.","sentences":["Carrying conversations in multi-sound environments is one of the more challenging tasks, since the sounds overlap across time and frequency making it difficult to understand a single sound source.","One proposed approach to help isolate an attended speech source is through decoding the electroencephalogram (EEG) and identifying the attended audio source using statistical or machine learning techniques.","However, the limited amount of data in comparison to other machine learning problems and the distributional shift between different EEG recordings emphasizes the need for a self supervised approach that works with limited data to achieve a more robust solution.","In this paper, we propose a method based on self supervised learning to minimize the difference between the latent representations of an attended speech signal and the corresponding EEG signal.","This network is further finetuned for the auditory attention classification task.","We compare our results with previously published methods and achieve state-of-the-art performance on the validation set."],"url":"http://arxiv.org/abs/2410.18395v1"}
