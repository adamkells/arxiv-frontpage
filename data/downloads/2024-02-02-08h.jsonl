{"created":"2024-02-01 18:59:56","title":"We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline","abstract":"There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA","sentences":["There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain.","While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames.","However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking.","In this work, we address this gap.","Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets.","To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark.","Code available at https://github.com/SimarKareer/UnifiedVideoDA"],"url":"http://arxiv.org/abs/2402.00868v1"}
{"created":"2024-02-01 18:59:22","title":"Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection","abstract":"Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples. However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability. To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods. We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem. Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data. Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures.","sentences":["Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection.","These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples.","However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability.","To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods.","We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem.","Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data.","Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures."],"url":"http://arxiv.org/abs/2402.00865v1"}
{"created":"2024-02-01 18:56:18","title":"Evaluating Large Language Models for Generalization and Robustness via Data Compression","abstract":"Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness. Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers. We also find the context size and tokenization implementation have a big impact of on the overall compression performance.","sentences":["Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation.","To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff.","Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff.","We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness.","Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data.","We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness.","Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers.","We also find the context size and tokenization implementation have a big impact of on the overall compression performance."],"url":"http://arxiv.org/abs/2402.00861v1"}
{"created":"2024-02-01 18:54:34","title":"Early Time Classification with Accumulated Accuracy Gap Control","abstract":"Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances. As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method. We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control.","sentences":["Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input.","In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule.","This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification.","We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances.","As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times.","Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method.","We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control."],"url":"http://arxiv.org/abs/2402.00857v1"}
{"created":"2024-02-01 18:51:54","title":"Towards Efficient and Exact Optimization of Language Model Alignment","abstract":"The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms. We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data.","sentences":["The alignment of language models with human preferences is vital for their application in real-world tasks.","The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy.","While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement.","Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data.","Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   ","In this paper, we propose efficient exact optimization (EXO) of the alignment objective.","We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms.","We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data."],"url":"http://arxiv.org/abs/2402.00856v1"}
{"created":"2024-02-01 18:50:50","title":"SymbolicAI: A framework for logic-based approaches combining generative models and solvers","abstract":"We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. In turn, the framework facilitates the creation and evaluation of explainable computational graphs. We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short. The framework codebase and benchmark are linked below.","sentences":["We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes.","SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI.","We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths.","The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives.","As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems.","In turn, the framework facilitates the creation and evaluation of explainable computational graphs.","We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows.","We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short.","The framework codebase and benchmark are linked below."],"url":"http://arxiv.org/abs/2402.00854v1"}
{"created":"2024-02-01 18:46:28","title":"Data Augmentation Scheme for Raman Spectra with Highly Correlated Annotations","abstract":"In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations. As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum. Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra. However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative. They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances. However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables. In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on such data exhibits low correlations between the model predictions. We show that training a CNN on these generated data points improves the performance on datasets where the annotations do not bear the same correlation as the dataset that was used for model training. This data augmentation technique enables us to reuse spectra as training data for new contexts that exhibit different correlations. The additional data allows for building a better and more robust model. This is of interest in scenarios where large amounts of historical data are available but are currently not used for model training. We demonstrate the capabilities of the proposed method using synthetic spectra of Ralstonia eutropha batch cultivations to monitor substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations during of the experiments.","sentences":["In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations.","As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum.","Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra.","However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative.","They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances.","However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables.","In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on such data exhibits low correlations between the model predictions.","We show that training a CNN on these generated data points improves the performance on datasets where the annotations do not bear the same correlation as the dataset that was used for model training.","This data augmentation technique enables us to reuse spectra as training data for new contexts that exhibit different correlations.","The additional data allows for building a better and more robust model.","This is of interest in scenarios where large amounts of historical data are available but are currently not used for model training.","We demonstrate the capabilities of the proposed method using synthetic spectra of Ralstonia eutropha batch cultivations to monitor substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations during of the experiments."],"url":"http://arxiv.org/abs/2402.00851v1"}
{"created":"2024-02-01 18:38:55","title":"BootsTAP: Bootstrapped Training for Tracking-Any-Point","abstract":"To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.","sentences":["To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes.","This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time.","Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion.","In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup.","We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%."],"url":"http://arxiv.org/abs/2402.00847v1"}
{"created":"2024-02-01 18:29:16","title":"X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System","abstract":"The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats. Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes. This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable.","sentences":["The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex.","Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks.","However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making.","This transparency gap in IDS research is significant, affecting confidence and accountability.","To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology.","Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats.","Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes.","This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable."],"url":"http://arxiv.org/abs/2402.00839v1"}
{"created":"2024-02-01 18:28:55","title":"OLMo: Accelerating the Science of Language Models","abstract":"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.","sentences":["Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings.","As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed.","Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs.","To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling.","Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code.","We hope this release will empower and strengthen the open research community and inspire a new wave of innovation."],"url":"http://arxiv.org/abs/2402.00838v1"}
{"created":"2024-02-01 18:22:32","title":"ALISON: Fast and Effective Stylometric Authorship Obfuscation","abstract":"Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.","sentences":["Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research.","Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier.","AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship.","To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours.","To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation.","We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics.","To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON."],"url":"http://arxiv.org/abs/2402.00835v1"}
{"created":"2024-02-01 18:20:55","title":"Approximating maximum-size properly colored forests","abstract":"In the Properly Colored Spanning Tree problem, we are given an edge-colored undirected graph and the goal is to find a properly colored spanning tree, i.e., a spanning tree in which any two adjacent edges have distinct colors. The problem is interesting not only from a graph coloring point of view, but is also closely related to the Degree Bounded Spanning Tree and (1,2)-Traveling Salesman problems, two classical questions that have attracted considerable interest in combinatorial optimization and approximation theory. Previous work on properly colored spanning trees has mainly focused on determining the existence of such a tree and hence has not considered the question from an algorithmic perspective. We propose an optimization version called Maximum-size Properly Colored Forest problem, which aims to find a properly colored forest with as many edges as possible. We consider the problem in different graph classes and for different numbers of colors, and present polynomial-time approximation algorithms as well as inapproximability results for these settings. Our proof technique relies on the sum of matching matroids defined by the color classes, a connection that might be of independent combinatorial interest.   We also consider the Maximum-size Properly Colored Tree problem, which asks for the maximum size of a properly colored tree not necessarily spanning all the vertices. We show that the optimum is significantly more difficult to approximate than in the forest case, and provide an approximation algorithm for complete multigraphs.","sentences":["In the Properly Colored Spanning Tree problem, we are given an edge-colored undirected graph and the goal is to find a properly colored spanning tree, i.e., a spanning tree in which any two adjacent edges have distinct colors.","The problem is interesting not only from a graph coloring point of view, but is also closely related to the Degree Bounded Spanning Tree and (1,2)-Traveling Salesman problems, two classical questions that have attracted considerable interest in combinatorial optimization and approximation theory.","Previous work on properly colored spanning trees has mainly focused on determining the existence of such a tree and hence has not considered the question from an algorithmic perspective.","We propose an optimization version called Maximum-size Properly Colored Forest problem, which aims to find a properly colored forest with as many edges as possible.","We consider the problem in different graph classes and for different numbers of colors, and present polynomial-time approximation algorithms as well as inapproximability results for these settings.","Our proof technique relies on the sum of matching matroids defined by the color classes, a connection that might be of independent combinatorial interest.   ","We also consider the Maximum-size Properly Colored Tree problem, which asks for the maximum size of a properly colored tree not necessarily spanning all the vertices.","We show that the optimum is significantly more difficult to approximate than in the forest case, and provide an approximation algorithm for complete multigraphs."],"url":"http://arxiv.org/abs/2402.00834v1"}
{"created":"2024-02-01 18:17:37","title":"A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks","abstract":"Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking. This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks. Furthermore, detecting Black Hole failures in backbone networks is particularly challenging. It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward. Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our method of selecting and analyzing four YANG models relevant to Black Hole detection in ISP networks, focusing on routing protocols and ISP-specific configurations. Our BHMM approach derived from these models demonstrates a 10% improvement in detection accuracy and a 13% increase in packet delivery rate, highlighting the efficiency of our approach. Additionally, we evaluate the Machine Learning approach leveraged with BHMM analysis in two different network settings, a commercial ISP network, and a scientific research-only network topology. This evaluation also demonstrates the practical applicability of our method, yielding significantly improved prediction outcomes in both environments.","sentences":["Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking.","This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks.","Furthermore, detecting Black Hole failures in backbone networks is particularly challenging.","It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward.","Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized","Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis.","This paper details our method of selecting and analyzing four YANG models relevant to Black Hole detection in ISP networks, focusing on routing protocols and ISP-specific configurations.","Our BHMM approach derived from these models demonstrates a 10% improvement in detection accuracy and a 13% increase in packet delivery rate, highlighting the efficiency of our approach.","Additionally, we evaluate the Machine Learning approach leveraged with BHMM analysis in two different network settings, a commercial ISP network, and a scientific research-only network topology.","This evaluation also demonstrates the practical applicability of our method, yielding significantly improved prediction outcomes in both environments."],"url":"http://arxiv.org/abs/2402.00831v1"}
{"created":"2024-02-01 18:16:53","title":"The En Route Truck-Drone Delivery Problem","abstract":"We study the truck-drone cooperative delivery problem in a setting where a single truck carrying a drone travels at constant speed on a straight-line trajectory/street. Delivery to clients located in the plane and not on the truck's trajectory is performed by the drone, which has limited carrying capacity and flying range, and whose battery can be recharged when on the truck. We show that the problem of maximizing the number of deliveries is strongly NP-hard even in this simple setting. We present a 2-approximation algorithm for the problem, and an optimal algorithm for a non-trivial family of instances.","sentences":["We study the truck-drone cooperative delivery problem in a setting where a single truck carrying a drone travels at constant speed on a straight-line trajectory/street.","Delivery to clients located in the plane and not on the truck's trajectory is performed by the drone, which has limited carrying capacity and flying range, and whose battery can be recharged when on the truck.","We show that the problem of maximizing the number of deliveries is strongly NP-hard even in this simple setting.","We present a 2-approximation algorithm for the problem, and an optimal algorithm for a non-trivial family of instances."],"url":"http://arxiv.org/abs/2402.00829v1"}
{"created":"2024-02-01 18:05:38","title":"WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework","abstract":"Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition. However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training. This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing. To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework. Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data. Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise. To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries. Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the CSI ratio. Next, it designs the OSGR network based on an uncertainty quantification method. Throughout the learning process, this network effectively mitigates uncertainty stemming from domains. Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR. Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness. Notably, WiOpen also demonstrates superiority in cross-domain tasks when compared to state-of-the-art approaches.","sentences":["Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition.","However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training.","This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing.","To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework.","Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing.","This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data.","Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise.","To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries.","Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the CSI ratio.","Next, it designs the OSGR network based on an uncertainty quantification method.","Throughout the learning process, this network effectively mitigates uncertainty stemming from domains.","Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR.","Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness.","Notably, WiOpen also demonstrates superiority in cross-domain tasks when compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.00822v1"}
{"created":"2024-02-01 17:45:26","title":"Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI","abstract":"In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.","sentences":["In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets.","However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention.","Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings.","This paper posits that BDL can elevate the capabilities of deep learning.","It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles.","Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential."],"url":"http://arxiv.org/abs/2402.00809v1"}
{"created":"2024-02-01 17:44:11","title":"Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching","abstract":"Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.","sentences":["Deep generative models have recently emerged as an effective approach to offline reinforcement learning.","However, their large model size poses challenges in computation.","We address this issue by proposing a knowledge distillation method based on data augmentation.","In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator.","Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks."],"url":"http://arxiv.org/abs/2402.00807v1"}
{"created":"2024-02-01 17:40:10","title":"Signal Quality Auditing for Time-series Data","abstract":"Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts. SQA is vital for addressing \"silent failures\" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences. We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data. We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment. We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data. To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility. The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses.","sentences":["Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts.","SQA is vital for addressing \"silent failures\" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences.","We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data.","We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment.","We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data.","To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility.","The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses."],"url":"http://arxiv.org/abs/2402.00803v1"}
{"created":"2024-02-01 17:23:54","title":"Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction","abstract":"We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.","sentences":["We introduce a novel framework for incorporating human expertise into algorithmic predictions.","Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm.","We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data.","We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor.","We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante).","In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population.","Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration."],"url":"http://arxiv.org/abs/2402.00793v1"}
{"created":"2024-02-01 17:21:53","title":"Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces","abstract":"Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance. Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption. The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.","sentences":["Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers.","Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs.","Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning.","State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data.","However, adapting SSMs to non-sequential graph data presents a notable challenge.","In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism.","Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance.","Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption.","The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba."],"url":"http://arxiv.org/abs/2402.00789v1"}
{"created":"2024-02-01 17:21:45","title":"Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning","abstract":"Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability.","sentences":["Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis.","However, a critical concern is the manual definition of behavioural rules in ABMs.","Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification.","This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents.","However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled.","To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework.","The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation.","Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods.","To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels.","Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours.","We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability."],"url":"http://arxiv.org/abs/2402.00787v1"}
{"created":"2024-02-01 17:17:55","title":"CroissantLLM: A Truly Bilingual French-English Language Model","abstract":"We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models. We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives. This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models.","sentences":["We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware.","To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets.","We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources.","To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language.","Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models.","We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives.","This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models."],"url":"http://arxiv.org/abs/2402.00786v1"}
{"created":"2024-02-01 16:59:59","title":"Mixed Static and Reconfigurable Metasurface Deployment in Indoor Dense Spaces: How Much Reconfigurability is Needed?","abstract":"In this paper, we investigate how metasurfaces can be deployed to deliver high data rates in a millimeter-wave (mmWave) indoor dense space with many blocking objects. These surfaces can either be static metasurfaces (SMSs) that reflect with fixed phase-shifts or reconfigurable intelligent surfaces (RISs) that can reconfigure their phase-shifts to the currently served user. The latter comes with an increased power, cabling, and signaling cost. To see how reconfigurability affects the network performance, we propose an iterative algorithm based on the feasible point pursuit successive convex approximation method. We jointly optimize the types and phase-shifts of the surfaces and the time portion allocated to each user equipment to maximize the minimum data rate achieved by the network. Our numerical results demonstrate that the minimum data rate improves as more RISs are introduced but the gain diminishes after some point. Therefore, introducing more reconfigurability is not always necessary. Another result shows that to reach the same data rate achieved by using 22 SMSs, at least 18 RISs are needed. This suggests that when it is costly to deploy many RISs, as an inexpensive alternative solution, one can reach the same data rate just by densely deploying more SMSs.","sentences":["In this paper, we investigate how metasurfaces can be deployed to deliver high data rates in a millimeter-wave (mmWave) indoor dense space with many blocking objects.","These surfaces can either be static metasurfaces (SMSs) that reflect with fixed phase-shifts or reconfigurable intelligent surfaces (RISs) that can reconfigure their phase-shifts to the currently served user.","The latter comes with an increased power, cabling, and signaling cost.","To see how reconfigurability affects the network performance, we propose an iterative algorithm based on the feasible point pursuit successive convex approximation method.","We jointly optimize the types and phase-shifts of the surfaces and the time portion allocated to each user equipment to maximize the minimum data rate achieved by the network.","Our numerical results demonstrate that the minimum data rate improves as more RISs are introduced but the gain diminishes after some point.","Therefore, introducing more reconfigurability is not always necessary.","Another result shows that to reach the same data rate achieved by using 22 SMSs, at least 18 RISs are needed.","This suggests that when it is costly to deploy many RISs, as an inexpensive alternative solution, one can reach the same data rate just by densely deploying more SMSs."],"url":"http://arxiv.org/abs/2402.00771v1"}
{"created":"2024-02-01 16:51:11","title":"Control-Theoretic Techniques for Online Adaptation of Deep Neural Networks in Dynamical Systems","abstract":"Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science. In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference. However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls. Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution. To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online. We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel last-layer update laws that guarantee desirable error convergence under various conditions on the time derivative of the DNN input vector. We further show that training the DNN under spectral normalization controls the upper bound of the error trajectories of the online DNN predictions, which is desirable when numerically differentiated quantities or noisy state measurements are input to the DNN. The proposed online DNN adaptation laws are validated in simulation to learn the dynamics of the Van der Pol system under domain shift, where parameters are varied in inference from the training dataset. The simulations demonstrate the effectiveness of using control-theoretic techniques to derive performance improvements and guarantees in DNN-based learning systems.","sentences":["Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science.","In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference.","However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls.","Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution.","To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online.","We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel last-layer update laws that guarantee desirable error convergence under various conditions on the time derivative of the DNN input vector.","We further show that training the DNN under spectral normalization controls the upper bound of the error trajectories of the online DNN predictions, which is desirable when numerically differentiated quantities or noisy state measurements are input to the DNN.","The proposed online DNN adaptation laws are validated in simulation to learn the dynamics of the Van der Pol system under domain shift, where parameters are varied in inference from the training dataset.","The simulations demonstrate the effectiveness of using control-theoretic techniques to derive performance improvements and guarantees in DNN-based learning systems."],"url":"http://arxiv.org/abs/2402.00761v1"}
{"created":"2024-02-01 16:43:04","title":"Unlearnable Algorithms for In-context Learning","abstract":"Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.","sentences":["Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance.","However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining.","In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM).","We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data.","We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets.","We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches.","This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests."],"url":"http://arxiv.org/abs/2402.00751v1"}
{"created":"2024-02-01 16:40:32","title":"Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model","abstract":"Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction. We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM. The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management. The code is available at https://github.com/jmyissb/HealthLLM.","sentences":["Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment.","However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges.","Hence, a more professional and detailed intelligent healthcare method is needed for development.","To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring.","Compared to traditional health management methods, our approach has three main advantages.","First, our method integrates health reports into a large model to provide detailed task information.","Second, professional medical expertise is used to adjust the weighted scores of health characteristics.","Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction.","We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM.","The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management.","The code is available at https://github.com/jmyissb/HealthLLM."],"url":"http://arxiv.org/abs/2402.00746v1"}
{"created":"2024-02-01 16:39:45","title":"Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data","abstract":"In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \\citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \\cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention.","sentences":["In practice, it is observed that transformer-based models can learn concepts in context in the inference stage.","While existing literature, e.g., \\citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data).","However, in reality, they are presented in two tokens (i.e., unstructured data \\cite{wibisono2023role}).","In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data.","We study the exact components in a transformer that facilitate the in-context learning.","In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention."],"url":"http://arxiv.org/abs/2402.00743v1"}
{"created":"2024-02-01 16:39:28","title":"Transforming and Combining Rewards for Aligning Large Language Models","abstract":"A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.","sentences":["A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model.","We study two closely related problems that arise in this approach.","First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others?","Second, we often wish to align language models to multiple properties: how should we combine multiple reward models?","Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models.","This derived transformation has two important properties.","First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well.","This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model).","Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise.","Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach."],"url":"http://arxiv.org/abs/2402.00742v1"}
{"created":"2024-02-01 16:37:21","title":"FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game","abstract":"Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players. A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs.","sentences":["Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team.","The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years.","However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability.","In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs.","Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs.","Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players.","A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs."],"url":"http://arxiv.org/abs/2402.00738v1"}
{"created":"2024-02-01 16:33:06","title":"BIOMERO: BioImage analysis in OMERO","abstract":"In the rapidly evolving field of bioimaging, the integration and orchestration of Findable, Accessible, Interoperable, and Reusable (FAIR) image analysis workflows remains a challenge. We introduce BIOMERO, a bridge connecting OMERO, a renowned bioimaging data management platform, FAIR workflows and high-performance computing (HPC) environments. BIOMERO, featuring our opensource Python library \"OMERO Slurm Client\", facilitates seamless execution of FAIR workflows, particularly for large datasets from High Content or High Throughput Screening. BIOMERO empowers researchers by eliminating the need for specialized knowledge, enabling scalable image processing directly from OMERO. BIOMERO notably supports the sharing and utilization of FAIR workflows between OMERO, Cytomine/BIAFLOWS, and other bioimaging communities. BIOMERO will promote the widespread adoption of FAIR workflows, emphasizing reusability, across the realm of bioimaging research. Its user-friendly interface will empower users, including those without technical expertise, to seamlessly apply these workflows to their datasets, democratizing the utilization of AI by the broader research community.","sentences":["In the rapidly evolving field of bioimaging, the integration and orchestration of Findable, Accessible, Interoperable, and Reusable (FAIR) image analysis workflows remains a challenge.","We introduce BIOMERO, a bridge connecting OMERO, a renowned bioimaging data management platform, FAIR workflows and high-performance computing (HPC) environments.","BIOMERO, featuring our opensource Python library \"OMERO Slurm Client\", facilitates seamless execution of FAIR workflows, particularly for large datasets from High Content or High Throughput Screening.","BIOMERO empowers researchers by eliminating the need for specialized knowledge, enabling scalable image processing directly from OMERO.","BIOMERO notably supports the sharing and utilization of FAIR workflows between OMERO, Cytomine/BIAFLOWS, and other bioimaging communities.","BIOMERO will promote the widespread adoption of FAIR workflows, emphasizing reusability, across the realm of bioimaging research.","Its user-friendly interface will empower users, including those without technical expertise, to seamlessly apply these workflows to their datasets, democratizing the utilization of AI by the broader research community."],"url":"http://arxiv.org/abs/2402.00734v1"}
{"created":"2024-02-01 16:30:00","title":"MobilityDL: A Review of Deep Learning From Trajectory Data","abstract":"Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior. As data availability and computing power have increased, so has the popularity of deep learning from trajectory data. This review paper provides the first comprehensive overview of deep learning approaches for trajectory data. We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used. Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information).","sentences":["Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior.","As data availability and computing power have increased, so has the popularity of deep learning from trajectory data.","This review paper provides the first comprehensive overview of deep learning approaches for trajectory data.","We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used.","Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information)."],"url":"http://arxiv.org/abs/2402.00732v1"}
{"created":"2024-02-01 16:07:12","title":"ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction","abstract":"Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change. Yet, S2S prediction remains challenging due to the chaotic nature of the system. At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability. Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction. ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years. We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model. Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart. We establish two tasks that vary in complexity: full and sparse dynamics prediction. Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task. We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench.","sentences":["Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change.","Yet, S2S prediction remains challenging due to the chaotic nature of the system.","At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability.","Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction.","ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years.","We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model.","Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart.","We establish two tasks that vary in complexity: full and sparse dynamics prediction.","Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task.","We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench."],"url":"http://arxiv.org/abs/2402.00712v1"}
{"created":"2024-02-01 16:05:15","title":"Towards an autonomous industry 4.0 warehouse: A UAV and blockchain-based system for inventory and traceability applications in big data-driven supply chain management","abstract":"In this paper we present the design and evaluation of a UAV-based system aimed at automating inventory tasks and keeping the traceability of industrial items attached to Radio-Frequency IDentification (RFID) tags. To confront current shortcomings, such a system is developed under a versatile, modular and scalable architecture aimed to reinforce cyber security and decentralization while fostering external audits and big data analytics. Therefore, the system uses a blockchain and a distributed ledger to store certain inventory data collected by UAVs, validate them, ensure their trustworthiness and make them available to the interested parties. In order to show the performance of the proposed system, different tests were performed in a real industrial warehouse, concluding that the system is able to obtain the inventory data really fast in comparison to traditional manual tasks, while being also able to estimate the position of the items when hovering over them thanks to their tag's signal strength. In addition, the performance of the proposed blockchain-based architecture was evaluated in different scenarios.","sentences":["In this paper we present the design and evaluation of a UAV-based system aimed at automating inventory tasks and keeping the traceability of industrial items attached to Radio-Frequency IDentification (RFID) tags.","To confront current shortcomings, such a system is developed under a versatile, modular and scalable architecture aimed to reinforce cyber security and decentralization while fostering external audits and big data analytics.","Therefore, the system uses a blockchain and a distributed ledger to store certain inventory data collected by UAVs, validate them, ensure their trustworthiness and make them available to the interested parties.","In order to show the performance of the proposed system, different tests were performed in a real industrial warehouse, concluding that the system is able to obtain the inventory data really fast in comparison to traditional manual tasks, while being also able to estimate the position of the items when hovering over them thanks to their tag's signal strength.","In addition, the performance of the proposed blockchain-based architecture was evaluated in different scenarios."],"url":"http://arxiv.org/abs/2402.00709v1"}
{"created":"2024-02-01 16:00:21","title":"Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)","abstract":"The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children. However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data. This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge. In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands. One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values. The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents. We provide information about the datasets and the samples, and describe the fertility outcome of interest. We also introduce the fertility prediction data challenge PreFer which is based on these datasets and will start in Spring 2024. We outline the ways in which measuring the predictability of fertility outcomes using these datasets and combining their strengths in the data challenge can advance our understanding of fertility behaviour and computational social science. We further provide details for participants on how to take part in the data challenge.","sentences":["The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children.","However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data.","This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge.","In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands.","One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values.","The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents.","We provide information about the datasets and the samples, and describe the fertility outcome of interest.","We also introduce the fertility prediction data challenge PreFer which is based on these datasets and will start in Spring 2024.","We outline the ways in which measuring the predictability of fertility outcomes using these datasets and combining their strengths in the data challenge can advance our understanding of fertility behaviour and computational social science.","We further provide details for participants on how to take part in the data challenge."],"url":"http://arxiv.org/abs/2402.00705v1"}
{"created":"2024-02-01 15:55:50","title":"PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software","abstract":"The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics. Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation. Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects. PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain. We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions.","sentences":["The development and training of deep learning models have become increasingly costly and complex.","Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications.","The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models.","Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse.","This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models.","Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use.","To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics.","Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation.","Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects.","PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain.","We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions."],"url":"http://arxiv.org/abs/2402.00699v1"}
{"created":"2024-02-01 15:55:25","title":"Time-Series Analysis Approach for Improving Energy Efficiency of a Fixed-Route Vessel in Short-Sea Shipping","abstract":"Several approaches have been developed for improving the ship energy efficiency, thereby reducing operating costs and ensuring compliance with climate change mitigation regulations. Many of these approaches will heavily depend on measured data from onboard IoT devices, including operational and environmental information, as well as external data sources for additional navigational data. In this paper, we develop a framework that implements time-series analysis techniques to optimize the vessel's speed profile for improving the vessel's energy efficiency. We present a case study involving a real-world data from a passenger vessel that was collected over a span of 15 months in the south of Sweden. The results indicate that the implemented models exhibit a range of outcomes and adaptability across different scenarios. The findings highlight the effectiveness of time-series analysis approach for optimizing vessel voyages within the context of constrained landscapes, as often seen in short-sea shipping.","sentences":["Several approaches have been developed for improving the ship energy efficiency, thereby reducing operating costs and ensuring compliance with climate change mitigation regulations.","Many of these approaches will heavily depend on measured data from onboard IoT devices, including operational and environmental information, as well as external data sources for additional navigational data.","In this paper, we develop a framework that implements time-series analysis techniques to optimize the vessel's speed profile for improving the vessel's energy efficiency.","We present a case study involving a real-world data from a passenger vessel that was collected over a span of 15 months in the south of Sweden.","The results indicate that the implemented models exhibit a range of outcomes and adaptability across different scenarios.","The findings highlight the effectiveness of time-series analysis approach for optimizing vessel voyages within the context of constrained landscapes, as often seen in short-sea shipping."],"url":"http://arxiv.org/abs/2402.00698v1"}
{"created":"2024-02-01 15:50:40","title":"A Framework for Building Point Cloud Cleaning, Plane Detection and Semantic Segmentation","abstract":"This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling. We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure. Following the cleaning process, we perform plane detection using the robust RANSAC paradigm. The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls. The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements. Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects. Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings. The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization.","sentences":["This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling.","We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure.","Following the cleaning process, we perform plane detection using the robust RANSAC paradigm.","The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls.","The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements.","Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects.","Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings.","The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization."],"url":"http://arxiv.org/abs/2402.00692v1"}
{"created":"2024-02-01 15:50:37","title":"Comparative Study of Large Language Model Architectures on Frontier","abstract":"Large language models (LLMs) have garnered significant attention in both the AI community and beyond. Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants. However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies. Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world's first Exascale supercomputer. Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance. Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark. Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design. To our knowledge, these pre-trained models represent the largest available for materials science. Our findings provide practical guidance for building LLMs on HPC platforms.","sentences":["Large language models (LLMs) have garnered significant attention in both the AI community and beyond.","Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants.","However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies.","Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world's first Exascale supercomputer.","Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance.","Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark.","Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design.","To our knowledge, these pre-trained models represent the largest available for materials science.","Our findings provide practical guidance for building LLMs on HPC platforms."],"url":"http://arxiv.org/abs/2402.00691v1"}
{"created":"2024-02-01 15:48:45","title":"A Review on the Use of Blockchain for the Internet of Things","abstract":"The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks. Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance. Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin. In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented. After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications. Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application. Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications.","sentences":["The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks.","Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance.","Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin.","In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented.","After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications.","Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application.","Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications."],"url":"http://arxiv.org/abs/2402.00687v1"}
{"created":"2024-02-01 15:46:04","title":"WayFASTER: a Self-Supervised Traversability Prediction for Increased Navigation Awareness","abstract":"Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors. Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible, but were visible at a different time. To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations. Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics. Our experiments demonstrate that our method excels at avoiding geometric obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable. By using a sequence of images, WayFASTER significantly enhances the robot's awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible. This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential.","sentences":["Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors.","Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible, but were visible at a different time.","To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations.","Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics.","Our experiments demonstrate that our method excels at avoiding geometric obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable.","By using a sequence of images, WayFASTER significantly enhances the robot's awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible.","This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential."],"url":"http://arxiv.org/abs/2402.00683v1"}
{"created":"2024-02-01 15:26:09","title":"Revising Apetrei's bounding volume hierarchy construction algorithm to allow stackless traversal","abstract":"Stackless traversal is a technique to speed up range queries by avoiding usage of a stack during the tree traversal. One way to achieve that is to transform a given binary tree to store a left child and a skip-connection (also called an escape index). In general, this operation requires an additional tree traversal during the tree construction. For some tree structures, however, it is possible achieve the same result at a reduced cost. We propose one such algorithm for a GPU hierarchy construction algorithm proposed by Karras in [Karras, 2012]. Furthermore, we show that our algorithm also works with the improved algorithm proposed by Apetrei in [Apetrei, 2014], despite a different ordering of the internal nodes. We achieve that by modifying the Apetrei's algorithm to restore the original Karras' ordering of the internal nodes. Using the modified algorithm, we show how to construct a hierarchy suitable for a stackless traversal in a single bottom-up pass.","sentences":["Stackless traversal is a technique to speed up range queries by avoiding usage of a stack during the tree traversal.","One way to achieve that is to transform a given binary tree to store a left child and a skip-connection (also called an escape index).","In general, this operation requires an additional tree traversal during the tree construction.","For some tree structures, however, it is possible achieve the same result at a reduced cost.","We propose one such algorithm for a GPU hierarchy construction algorithm proposed by Karras in [Karras, 2012].","Furthermore, we show that our algorithm also works with the improved algorithm proposed by Apetrei in [Apetrei, 2014], despite a different ordering of the internal nodes.","We achieve that by modifying the Apetrei's algorithm to restore the original Karras' ordering of the internal nodes.","Using the modified algorithm, we show how to construct a hierarchy suitable for a stackless traversal in a single bottom-up pass."],"url":"http://arxiv.org/abs/2402.00665v1"}
{"created":"2024-02-01 15:18:48","title":"Modeling Freight Mode Choice Using Machine Learning Classifiers: A Comparative Study Using the Commodity Flow Survey (CFS) Data","abstract":"This study explores the usefulness of machine learning classifiers for modeling freight mode choice. We investigate eight commonly used machine learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random Forest, Boosting and Bagging, along with the classical Multinomial Logit model. US 2012 Commodity Flow Survey data are used as the primary data source; we augment it with spatial attributes from secondary data sources. The performance of the classifiers is compared based on prediction accuracy results. The current research also examines the role of sample size and training-testing data split ratios on the predictive ability of the various approaches. In addition, the importance of variables is estimated to determine how the variables influence freight mode choice. The results show that the tree-based ensemble classifiers perform the best. Specifically, Random Forest produces the most accurate predictions, closely followed by Boosting and Bagging. With regard to variable importance, shipment characteristics, such as shipment distance, industry classification of the shipper and shipment size, are the most significant factors for freight mode choice decisions.","sentences":["This study explores the usefulness of machine learning classifiers for modeling freight mode choice.","We investigate eight commonly used machine learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random Forest, Boosting and Bagging, along with the classical Multinomial Logit model.","US 2012 Commodity Flow Survey data are used as the primary data source; we augment it with spatial attributes from secondary data sources.","The performance of the classifiers is compared based on prediction accuracy results.","The current research also examines the role of sample size and training-testing data split ratios on the predictive ability of the various approaches.","In addition, the importance of variables is estimated to determine how the variables influence freight mode choice.","The results show that the tree-based ensemble classifiers perform the best.","Specifically, Random Forest produces the most accurate predictions, closely followed by Boosting and Bagging.","With regard to variable importance, shipment characteristics, such as shipment distance, industry classification of the shipper and shipment size, are the most significant factors for freight mode choice decisions."],"url":"http://arxiv.org/abs/2402.00659v1"}
{"created":"2024-02-01 15:18:19","title":"Pre-training by Predicting Program Dependencies for Vulnerability Analysis Tasks","abstract":"Vulnerability analysis is crucial for software security. This work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis. The code understanding ability of a pre-trained model is highly related to its pre-training objectives. The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis. However, existing pre-training objectives either ignore such structure or focus on learning to use it. The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated. To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code. During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code. After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions. To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis. Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks. Also, PDBERT achieves F1-scores of over 99% and 94% for predicting control and data dependencies, respectively, in partial and complete functions.","sentences":["Vulnerability analysis is crucial for software security.","This work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis.","The code understanding ability of a pre-trained model is highly related to its pre-training objectives.","The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis.","However, existing pre-training objectives either ignore such structure or focus on learning to use it.","The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated.","To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code.","During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code.","After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions.","To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis.","Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks.","Also, PDBERT achieves F1-scores of over 99% and 94% for predicting control and data dependencies, respectively, in partial and complete functions."],"url":"http://arxiv.org/abs/2402.00657v1"}
{"created":"2024-02-01 15:14:16","title":"Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques","abstract":"The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File). With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics. In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance. The proposed method achieved over 92% accuracy without incorporating external information, an over 19% increase compared to directly fitting Random Forests models over 10,000 samples. Furthermore, SHAP (Shapely Additive Explanations) values were computed to explain the outputs and major patterns obtained from the proposed model. The model framework could enhance the performance and interpretability of existing freight mode choice models.","sentences":["The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File).","With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics.","In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance.","The proposed method achieved over 92% accuracy without incorporating external information, an over 19% increase compared to directly fitting Random Forests models over 10,000 samples.","Furthermore, SHAP (Shapely Additive Explanations) values were computed to explain the outputs and major patterns obtained from the proposed model.","The model framework could enhance the performance and interpretability of existing freight mode choice models."],"url":"http://arxiv.org/abs/2402.00654v1"}
{"created":"2024-02-01 14:52:16","title":"Fisheye Camera and Ultrasonic Sensor Fusion For Near-Field Obstacle Perception in Bird's-Eye-View","abstract":"Accurate obstacle identification represents a fundamental challenge within the scope of near-field perception for autonomous driving. Conventionally, fisheye cameras are frequently employed for comprehensive surround-view perception, including rear-view obstacle localization. However, the performance of such cameras can significantly deteriorate in low-light conditions, during nighttime, or when subjected to intense sun glare. Conversely, cost-effective sensors like ultrasonic sensors remain largely unaffected under these conditions. Therefore, we present, to our knowledge, the first end-to-end multimodal fusion model tailored for efficient obstacle perception in a bird's-eye-view (BEV) perspective, utilizing fisheye cameras and ultrasonic sensors. Initially, ResNeXt-50 is employed as a set of unimodal encoders to extract features specific to each modality. Subsequently, the feature space associated with the visible spectrum undergoes transformation into BEV. The fusion of these two modalities is facilitated via concatenation. At the same time, the ultrasonic spectrum-based unimodal feature maps pass through content-aware dilated convolution, applied to mitigate the sensor misalignment between two sensors in the fused feature space. Finally, the fused features are utilized by a two-stage semantic occupancy decoder to generate grid-wise predictions for precise obstacle perception. We conduct a systematic investigation to determine the optimal strategy for multimodal fusion of both sensors. We provide insights into our dataset creation procedures, annotation guidelines, and perform a thorough data analysis to ensure adequate coverage of all scenarios. When applied to our dataset, the experimental results underscore the robustness and effectiveness of our proposed multimodal fusion approach.","sentences":["Accurate obstacle identification represents a fundamental challenge within the scope of near-field perception for autonomous driving.","Conventionally, fisheye cameras are frequently employed for comprehensive surround-view perception, including rear-view obstacle localization.","However, the performance of such cameras can significantly deteriorate in low-light conditions, during nighttime, or when subjected to intense sun glare.","Conversely, cost-effective sensors like ultrasonic sensors remain largely unaffected under these conditions.","Therefore, we present, to our knowledge, the first end-to-end multimodal fusion model tailored for efficient obstacle perception in a bird's-eye-view (BEV) perspective, utilizing fisheye cameras and ultrasonic sensors.","Initially, ResNeXt-50 is employed as a set of unimodal encoders to extract features specific to each modality.","Subsequently, the feature space associated with the visible spectrum undergoes transformation into BEV.","The fusion of these two modalities is facilitated via concatenation.","At the same time, the ultrasonic spectrum-based unimodal feature maps pass through content-aware dilated convolution, applied to mitigate the sensor misalignment between two sensors in the fused feature space.","Finally, the fused features are utilized by a two-stage semantic occupancy decoder to generate grid-wise predictions for precise obstacle perception.","We conduct a systematic investigation to determine the optimal strategy for multimodal fusion of both sensors.","We provide insights into our dataset creation procedures, annotation guidelines, and perform a thorough data analysis to ensure adequate coverage of all scenarios.","When applied to our dataset, the experimental results underscore the robustness and effectiveness of our proposed multimodal fusion approach."],"url":"http://arxiv.org/abs/2402.00637v1"}
{"created":"2024-02-01 14:44:57","title":"Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication Optimization","abstract":"Memory is a critical design consideration in current data-intensive DNN accelerators, as it profoundly determines energy consumption, bandwidth requirements, and area costs. As DNN structures become more complex, a larger on-chip memory capacity is required to reduce data movement overhead, but at the expense of silicon costs. Some previous works have proposed memory-oriented optimizations, such as different data reuse and layer fusion schemes. However, these methods are not general and potent enough to cope with various graph structures.   In this paper, we explore the intrinsic connection between network structures and memory features to optimize both hardware and mapping. First, we introduce a graph-level execution scheme with a corresponding dataflow and memory management method. This scheme enables the execution of arbitrary graph patterns with high data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping co-exploration framework leveraging graph-level features of networks. It aims to minimize communication overhead, such as energy consumption and bandwidth requirements, with a smaller memory capacity. We formulate the graph-partition scheduling and memory configuration search as an optimization problem and employ a genetic-based method to achieve efficient co-exploration for large and irregular networks. Experiments demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements, and more stable optimization for graph partition compared to the greedy algorithm and dynamic programming introduced in prior works. Cocco also reduces the costs by 1.89% to 50.33% using co-exploration compared to other typical methods.","sentences":["Memory is a critical design consideration in current data-intensive DNN accelerators, as it profoundly determines energy consumption, bandwidth requirements, and area costs.","As DNN structures become more complex, a larger on-chip memory capacity is required to reduce data movement overhead, but at the expense of silicon costs.","Some previous works have proposed memory-oriented optimizations, such as different data reuse and layer fusion schemes.","However, these methods are not general and potent enough to cope with various graph structures.   ","In this paper, we explore the intrinsic connection between network structures and memory features to optimize both hardware and mapping.","First, we introduce a graph-level execution scheme with a corresponding dataflow and memory management method.","This scheme enables the execution of arbitrary graph patterns with high data reuse and low hardware overhead.","Subsequently, we propose Cocco, a hardware-mapping co-exploration framework leveraging graph-level features of networks.","It aims to minimize communication overhead, such as energy consumption and bandwidth requirements, with a smaller memory capacity.","We formulate the graph-partition scheduling and memory configuration search as an optimization problem and employ a genetic-based method to achieve efficient co-exploration for large and irregular networks.","Experiments demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements, and more stable optimization for graph partition compared to the greedy algorithm and dynamic programming introduced in prior works.","Cocco also reduces the costs by 1.89% to 50.33% using co-exploration compared to other typical methods."],"url":"http://arxiv.org/abs/2402.00629v1"}
{"created":"2024-02-01 14:02:06","title":"Deep Clustering Using the Soft Silhouette Score: Towards Compact and Well-Separated Clusters","abstract":"Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets. Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance. The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset. In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient. Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient. When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters. In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function. The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results.","sentences":["Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets.","Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance.","The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset.","In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient.","Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient.","When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters.","In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function.","The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results."],"url":"http://arxiv.org/abs/2402.00608v1"}
{"created":"2024-02-01 13:59:04","title":"Are Synthetic Time-series Data Really not as Good as Real Data?","abstract":"Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem. Integrating universal data synthesis methods holds promise in improving generalization. However, current methods cannot guarantee that the generator's output covers all unseen real data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability. We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data. Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data. Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities. Through experiments, our non-deep-learning synthetic data enables models to achieve superior reconstruction performance and universal explicit representation extraction without the need for real data.","sentences":["Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem.","Integrating universal data synthesis methods holds promise in improving generalization.","However, current methods cannot guarantee that the generator's output covers all unseen real data.","In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability.","We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data.","Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data.","Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities.","Through experiments, our non-deep-learning synthetic data enables models to achieve superior reconstruction performance and universal explicit representation extraction without the need for real data."],"url":"http://arxiv.org/abs/2402.00607v1"}
{"created":"2024-02-01 13:51:35","title":"A Promise Theory Perspective on the Role of Intent in Group Dynamics","abstract":"We present a simple argument using Promise Theory and dimensional analysis for the Dunbar scaling hierarchy, supported by recent data from group formation in Wikipedia editing. We show how the assumption of a common priority seeds group alignment until the costs associated with attending to the group outweigh the benefits in a detailed balance scenario. Subject to partial efficiency of implementing promised intentions, we can reproduce a series of compatible rates that balance growth with entropy.","sentences":["We present a simple argument using Promise Theory and dimensional analysis for the Dunbar scaling hierarchy, supported by recent data from group formation in Wikipedia editing.","We show how the assumption of a common priority seeds group alignment until the costs associated with attending to the group outweigh the benefits in a detailed balance scenario.","Subject to partial efficiency of implementing promised intentions, we can reproduce a series of compatible rates that balance growth with entropy."],"url":"http://arxiv.org/abs/2402.00598v1"}
{"created":"2024-02-01 13:45:06","title":"Identifying relevant Factors of Requirements Quality: an industrial Case Study","abstract":"[Context and Motivation]: The quality of requirements specifications impacts subsequent, dependent software engineering activities. Requirements quality defects like ambiguous statements can result in incomplete or wrong features and even lead to budget overrun or project failure. [Problem]: Attempts at measuring the impact of requirements quality have been held back by the vast amount of interacting factors. Requirements quality research lacks an understanding of which factors are relevant in practice. [Principal Ideas and Results]: We conduct a case study considering data from both interview transcripts and issue reports to identify relevant factors of requirements quality. The results include 17 factors and 11 interaction effects relevant to the case company. [Contribution]: The results contribute empirical evidence that (1) strengthens existing requirements engineering theories and (2) advances industry-relevant requirements quality research.","sentences":["[Context and Motivation]: The quality of requirements specifications impacts subsequent, dependent software engineering activities.","Requirements quality defects like ambiguous statements can result in incomplete or wrong features and even lead to budget overrun or project failure.","[Problem]: Attempts at measuring the impact of requirements quality have been held back by the vast amount of interacting factors.","Requirements quality research lacks an understanding of which factors are relevant in practice.","[Principal Ideas and Results]: We conduct a case study considering data from both interview transcripts and issue reports to identify relevant factors of requirements quality.","The results include 17 factors and 11 interaction effects relevant to the case company.","[Contribution]: The results contribute empirical evidence that (1) strengthens existing requirements engineering theories and (2) advances industry-relevant requirements quality research."],"url":"http://arxiv.org/abs/2402.00594v1"}
{"created":"2024-02-01 13:41:44","title":"Uncertainty-Aware Partial-Label Learning","abstract":"In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.","sentences":["In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels.","Partial-label learning allows training classifiers in this weakly supervised setting.","While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates.","However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving.","In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory.","Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance.","Additionally, we prove that our algorithm is risk-consistent."],"url":"http://arxiv.org/abs/2402.00592v1"}
{"created":"2024-02-01 13:34:59","title":"BrainSLAM: SLAM on Neural Population Activity Data","abstract":"Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location. This is the first demonstration of inference of a spatial map from brain recordings. Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making.","sentences":["Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments.","Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data.","We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex.","This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze.","The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors).","Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location.","This is the first demonstration of inference of a spatial map from brain recordings.","Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making."],"url":"http://arxiv.org/abs/2402.00588v1"}
{"created":"2024-02-01 13:14:38","title":"Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks","abstract":"We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks. We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model. We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments.","sentences":["We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks.","We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model.","We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments."],"url":"http://arxiv.org/abs/2402.00576v1"}
{"created":"2024-02-01 13:13:16","title":"Diffusion-based Light Field Synthesis","abstract":"Light fields (LFs), conducive to comprehensive scene radiance recorded across angular dimensions, find wide applications in 3D reconstruction, virtual reality, and computational photography.However, the LF acquisition is inevitably time-consuming and resource-intensive due to the mainstream acquisition strategy involving manual capture or laborious software synthesis.Given such a challenge, we introduce LFdiff, a straightforward yet effective diffusion-based generative framework tailored for LF synthesis, which adopts only a single RGB image as input.LFdiff leverages disparity estimated by a monocular depth estimation network and incorporates two distinctive components: a novel condition scheme and a noise estimation network tailored for LF data.Specifically, we design a position-aware warping condition scheme, enhancing inter-view geometry learning via a robust conditional signal.We then propose DistgUnet, a disentanglement-based noise estimation network, to harness comprehensive LF representations.Extensive experiments demonstrate that LFdiff excels in synthesizing visually pleasing and disparity-controllable light fields with enhanced generalization capability.Additionally, comprehensive results affirm the broad applicability of the generated LF data, spanning applications like LF super-resolution and refocusing.","sentences":["Light fields (LFs), conducive to comprehensive scene radiance recorded across angular dimensions, find wide applications in 3D reconstruction, virtual reality, and computational photography.","However, the LF acquisition is inevitably time-consuming and resource-intensive due to the mainstream acquisition strategy involving manual capture or laborious software synthesis.","Given such a challenge, we introduce LFdiff, a straightforward yet effective diffusion-based generative framework tailored for LF synthesis, which adopts only a single RGB image as input.","LFdiff leverages disparity estimated by a monocular depth estimation network and incorporates two distinctive components: a novel condition scheme and a noise estimation network tailored for LF data.","Specifically, we design a position-aware warping condition scheme, enhancing inter-view geometry learning via a robust conditional signal.","We then propose DistgUnet, a disentanglement-based noise estimation network, to harness comprehensive LF representations.","Extensive experiments demonstrate that LFdiff excels in synthesizing visually pleasing and disparity-controllable light fields with enhanced generalization capability.","Additionally, comprehensive results affirm the broad applicability of the generated LF data, spanning applications like LF super-resolution and refocusing."],"url":"http://arxiv.org/abs/2402.00575v1"}
{"created":"2024-02-01 12:06:55","title":"Masked Conditional Diffusion Model for Enhancing Deepfake Detection","abstract":"Recent studies on deepfake detection have achieved promising results when training and testing faces are from the same dataset. However, their results severely degrade when confronted with forged samples that the model has not yet seen during training. In this paper, deepfake data to help detect deepfakes. this paper present we put a new insight into diffusion model-based data augmentation, and propose a Masked Conditional Diffusion Model (MCDM) for enhancing deepfake detection. It generates a variety of forged faces from a masked pristine one, encouraging the deepfake detection model to learn generic and robust representations without overfitting to special artifacts. Extensive experiments demonstrate that forgery images generated with our method are of high quality and helpful to improve the performance of deepfake detection models.","sentences":["Recent studies on deepfake detection have achieved promising results when training and testing faces are from the same dataset.","However, their results severely degrade when confronted with forged samples that the model has not yet seen during training.","In this paper, deepfake data to help detect deepfakes.","this paper present we put a new insight into diffusion model-based data augmentation, and propose a Masked Conditional Diffusion Model (MCDM) for enhancing deepfake detection.","It generates a variety of forged faces from a masked pristine one, encouraging the deepfake detection model to learn generic and robust representations without overfitting to special artifacts.","Extensive experiments demonstrate that forgery images generated with our method are of high quality and helpful to improve the performance of deepfake detection models."],"url":"http://arxiv.org/abs/2402.00541v1"}
{"created":"2024-02-01 12:06:41","title":"Experimental Evaluation of Interactive Edge/Cloud Virtual Reality Gaming over Wi-Fi using Unity Render Streaming","abstract":"Virtual Reality (VR) streaming enables end-users to seamlessly immerse themselves in interactive virtual environments using even low-end devices. However, the quality of the VR experience heavily relies on Wi-Fi performance, since it serves as the last hop in the network chain. Our study delves into the intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data and leveraging a simulator tailored to VR traffic patterns. In this work we further evaluate Wi-Fi's suitability for VR streaming in terms of the quality of service it provides. In particular, we employ Unity Render Streaming to remotely stream real-time VR gaming content over Wi-Fi 6 using WebRTC, leveraging a server physically located at the network's edge, near the end user. Our findings demonstrate the system's sustained network performance, showcasing minimal round-trip time and jitter at 60 and 90 fps. In addition, we uncover the characteristics and patterns of the generated traffic streams, unveiling a surprising video transmission approach inherent to WebRTC-based services. This approach involves the fragmentation of video frames into discrete batches of packets, transmitted at regular intervals regardless of the targeted frame rate.This segmentation mechanism maintains consistent video packet delays across video frame rates but leads to increased Wi-Fi airtime consumption at higher frame rates. The presented results demonstrate that shortening the interval between batches is advantageous as it improves Wi-Fi efficiency and reduces delays in delivering complete frames.","sentences":["Virtual Reality (VR) streaming enables end-users to seamlessly immerse themselves in interactive virtual environments using even low-end devices.","However, the quality of the VR experience heavily relies on Wi-Fi performance, since it serves as the last hop in the network chain.","Our study delves into the intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data and leveraging a simulator tailored to VR traffic patterns.","In this work we further evaluate Wi-Fi's suitability for VR streaming in terms of the quality of service it provides.","In particular, we employ Unity Render Streaming to remotely stream real-time VR gaming content over Wi-Fi 6 using WebRTC, leveraging a server physically located at the network's edge, near the end user.","Our findings demonstrate the system's sustained network performance, showcasing minimal round-trip time and jitter at 60 and 90 fps.","In addition, we uncover the characteristics and patterns of the generated traffic streams, unveiling a surprising video transmission approach inherent to WebRTC-based services.","This approach involves the fragmentation of video frames into discrete batches of packets, transmitted at regular intervals regardless of the targeted frame rate.","This segmentation mechanism maintains consistent video packet delays across video frame rates but leads to increased Wi-Fi airtime consumption at higher frame rates.","The presented results demonstrate that shortening the interval between batches is advantageous as it improves Wi-Fi efficiency and reduces delays in delivering complete frames."],"url":"http://arxiv.org/abs/2402.00540v1"}
{"created":"2024-02-01 11:57:53","title":"Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning","abstract":"Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of our approach.","sentences":["Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data.","Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process.","But it also leads to extra cost and computation due to the involvement of LLMs in this process.","To reduce the filtering cost, we study Superfiltering:","Can we use a smaller and weaker model to select data for finetuning a larger and stronger model?","Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results.","This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model.","Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks.","Extensive experiments validate the efficacy and efficiency of our approach."],"url":"http://arxiv.org/abs/2402.00530v1"}
{"created":"2024-02-01 11:39:19","title":"Towards Summarizing Code Snippets Using Pre-Trained Transformers","abstract":"When comprehending code, a helping hand may come from the natural language comments documenting it that, unfortunately, are not always there. To support developers in such a scenario, several techniques have been presented to automatically generate natural language summaries for a given code. Most recent approaches exploit deep learning (DL) to automatically document classes or functions, while little effort has been devoted to more fine-grained documentation (e.g., documenting code snippets or even a single statement). Such a design choice is dictated by the availability of training data: For example, in the case of Java, it is easy to create datasets composed of pairs <Method, Javadoc> that can be fed to DL models to teach them how to summarize a method. Such a comment-to-code linking is instead non-trivial when it comes to inner comments documenting a few statements. In this work, we take all the steps needed to train a DL model to document code snippets. First, we manually built a dataset featuring 6.6k comments that have been (i) classified based on their type (e.g., code summary, TODO), and (ii) linked to the code statements they document. Second, we used such a dataset to train a multi-task DL model, taking as input a comment and being able to (i) classify whether it represents a \"code summary\" or not and (ii) link it to the code statements it documents. Our model identifies code summaries with 84% accuracy and is able to link them to the documented lines of code with recall and precision higher than 80%. Third, we run this model on 10k projects, identifying and linking code summaries to the documented code. This unlocked the possibility of building a large-scale dataset of documented code snippets that have then been used to train a new DL model able to document code snippets. A comparison with state-of-the-art baselines shows the superiority of the proposed approach.","sentences":["When comprehending code, a helping hand may come from the natural language comments documenting it that, unfortunately, are not always there.","To support developers in such a scenario, several techniques have been presented to automatically generate natural language summaries for a given code.","Most recent approaches exploit deep learning (DL) to automatically document classes or functions, while little effort has been devoted to more fine-grained documentation (e.g., documenting code snippets or even a single statement).","Such a design choice is dictated by the availability of training data: For example, in the case of Java, it is easy to create datasets composed of pairs <Method, Javadoc> that can be fed to DL models to teach them how to summarize a method.","Such a comment-to-code linking is instead non-trivial when it comes to inner comments documenting a few statements.","In this work, we take all the steps needed to train a DL model to document code snippets.","First, we manually built a dataset featuring 6.6k comments that have been (i) classified based on their type (e.g., code summary, TODO), and (ii) linked to the code statements they document.","Second, we used such a dataset to train a multi-task DL model, taking as input a comment and being able to (i) classify whether it represents a \"code summary\" or not and (ii) link it to the code statements it documents.","Our model identifies code summaries with 84% accuracy and is able to link them to the documented lines of code with recall and precision higher than 80%.","Third, we run this model on 10k projects, identifying and linking code summaries to the documented code.","This unlocked the possibility of building a large-scale dataset of documented code snippets that have then been used to train a new DL model able to document code snippets.","A comparison with state-of-the-art baselines shows the superiority of the proposed approach."],"url":"http://arxiv.org/abs/2402.00519v1"}
{"created":"2024-02-01 11:39:04","title":"EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models","abstract":"This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.","sentences":["This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs).","In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data.","Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism.","Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget.","In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM."],"url":"http://arxiv.org/abs/2402.00518v1"}
{"created":"2024-02-01 10:57:00","title":"EXMOS: Explanatory Model Steering Through Multifaceted Explanations and Data Configurations","abstract":"Explanations in interactive machine-learning systems facilitate debugging and improving prediction models. However, the effectiveness of various global model-centric and data-centric explanations in aiding domain experts to detect and resolve potential data issues for model improvement remains unexplored. This research investigates the influence of data-centric and model-centric global explanations in systems that support healthcare experts in optimising models through automated and manual data configurations. We conducted quantitative (n=70) and qualitative (n=30) studies with healthcare experts to explore the impact of different explanations on trust, understandability and model improvement. Our results reveal the insufficiency of global model-centric explanations for guiding users during data configuration. Although data-centric explanations enhanced understanding of post-configuration system changes, a hybrid fusion of both explanation types demonstrated the highest effectiveness. Based on our study results, we also present design implications for effective explanation-driven interactive machine-learning systems.","sentences":["Explanations in interactive machine-learning systems facilitate debugging and improving prediction models.","However, the effectiveness of various global model-centric and data-centric explanations in aiding domain experts to detect and resolve potential data issues for model improvement remains unexplored.","This research investigates the influence of data-centric and model-centric global explanations in systems that support healthcare experts in optimising models through automated and manual data configurations.","We conducted quantitative (n=70) and qualitative (n=30) studies with healthcare experts to explore the impact of different explanations on trust, understandability and model improvement.","Our results reveal the insufficiency of global model-centric explanations for guiding users during data configuration.","Although data-centric explanations enhanced understanding of post-configuration system changes, a hybrid fusion of both explanation types demonstrated the highest effectiveness.","Based on our study results, we also present design implications for effective explanation-driven interactive machine-learning systems."],"url":"http://arxiv.org/abs/2402.00491v1"}
{"created":"2024-02-01 10:42:05","title":"A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems","abstract":"In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications. Recommender systems are prominent examples of these machine learning (ML) systems that aid users in making decisions. The majority of past literature research on RS fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace. In this paper, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework. The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics. For instance, we demonstrate that the system may jointly increase consumer and producer fairness when (un)protected consumer groups are defined on the basis of their activity level and main-streamness, while producer groups are defined according to their popularity level. For empirical validation, through large-scale on eight datasets and four mainstream collaborative filtering (CF) recommendation models, we demonstrate that our proposed strategy is able to improve both consumer and producer fairness without compromising or very little overall recommendation quality, demonstrating the role algorithms may play in avoiding data biases.","sentences":["In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications.","Recommender systems are prominent examples of these machine learning (ML) systems that aid users in making decisions.","The majority of past literature research on RS fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace.","In this paper, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework.","The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics.","For instance, we demonstrate that the system may jointly increase consumer and producer fairness when (un)protected consumer groups are defined on the basis of their activity level and main-streamness, while producer groups are defined according to their popularity level.","For empirical validation, through large-scale on eight datasets and four mainstream collaborative filtering (CF) recommendation models, we demonstrate that our proposed strategy is able to improve both consumer and producer fairness without compromising or very little overall recommendation quality, demonstrating the role algorithms may play in avoiding data biases."],"url":"http://arxiv.org/abs/2402.00485v1"}
{"created":"2024-02-01 10:31:30","title":"HERITRACE: Tracing Evolution and Bridging Data for Streamlined Curatorial Work in the GLAM Domain","abstract":"HERITRACE is a semantic data management system tailored for the GLAM sector. It is engineered to streamline data curation for non-technical users while also offering an efficient administrative interface for technical staff. The paper compares HERITRACE with other established platforms such as OmekaS, Semantic MediaWiki, Research Space, and CLEF, emphasizing its advantages in user friendliness, provenance management, change tracking, customization capabilities, and data integration. The system leverages SHACL for data modeling and employs the OpenCitations Data Model (OCDM) for provenance and change tracking, ensuring a harmonious blend of advanced technical features and user accessibility. Future developments include the integration of a robust authentication system and the expansion of data compatibility via the RDF Mapping Language (RML), enhancing HERITRACE's utility in digital heritage management.","sentences":["HERITRACE is a semantic data management system tailored for the GLAM sector.","It is engineered to streamline data curation for non-technical users while also offering an efficient administrative interface for technical staff.","The paper compares HERITRACE with other established platforms such as OmekaS, Semantic MediaWiki, Research Space, and CLEF, emphasizing its advantages in user friendliness, provenance management, change tracking, customization capabilities, and data integration.","The system leverages SHACL for data modeling and employs the OpenCitations Data Model (OCDM) for provenance and change tracking, ensuring a harmonious blend of advanced technical features and user accessibility.","Future developments include the integration of a robust authentication system and the expansion of data compatibility via the RDF Mapping Language (RML), enhancing HERITRACE's utility in digital heritage management."],"url":"http://arxiv.org/abs/2402.00477v1"}
{"created":"2024-02-01 10:26:27","title":"SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models","abstract":"Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the original LLMs. Notably, this improvement is particularly pronounced for unseen medical tasks, showing an improvement of up to 30%.","sentences":["Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks.","However, their effective application in the medical domain is hampered by a lack of medical domain knowledge.","In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks.","SA-MDKIF consists of two stages: skill training and skill adaptation.","In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed.","In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference.","Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the original LLMs.","Notably, this improvement is particularly pronounced for unseen medical tasks, showing an improvement of up to 30%."],"url":"http://arxiv.org/abs/2402.00474v1"}
{"created":"2024-02-01 10:12:51","title":"Coded Multi-User Information Retrieval with a Multi-Antenna Helper Node","abstract":"A novel coding design is proposed to enhance information retrieval in a wireless network of users with partial access to the data, in the sense of observation, measurement, computation, or storage. Information exchange in the network is assisted by a multi-antenna base station (BS), with no direct access to the data. Accordingly, the missing parts of data are exchanged among users through an uplink (UL) step followed by a downlink (DL) step. In this paper, new coding strategies, inspired by coded caching (CC) techniques, are devised to enhance both UL and DL steps. In the UL step, users transmit encoded and properly combined parts of their accessible data to the BS. Then, during the DL step, the BS carries out the required processing on its received signals and forwards a proper combination of the resulting signal terms back to the users, enabling each user to retrieve the desired information. Using the devised coded data retrieval strategy, the data exchange in both UL and DL steps requires the same communication delay, measured by normalized delivery time (NDT). Furthermore, the NDT of the UL/DL step is shown to coincide with the optimal NDT of the original DL multi-input single-output CC scheme, in which the BS is connected to a centralized data library.","sentences":["A novel coding design is proposed to enhance information retrieval in a wireless network of users with partial access to the data, in the sense of observation, measurement, computation, or storage.","Information exchange in the network is assisted by a multi-antenna base station (BS), with no direct access to the data.","Accordingly, the missing parts of data are exchanged among users through an uplink (UL) step followed by a downlink (DL) step.","In this paper, new coding strategies, inspired by coded caching (CC) techniques, are devised to enhance both UL and DL steps.","In the UL step, users transmit encoded and properly combined parts of their accessible data to the BS.","Then, during the DL step, the BS carries out the required processing on its received signals and forwards a proper combination of the resulting signal terms back to the users, enabling each user to retrieve the desired information.","Using the devised coded data retrieval strategy, the data exchange in both UL and DL steps requires the same communication delay, measured by normalized delivery time (NDT).","Furthermore, the NDT of the UL/DL step is shown to coincide with the optimal NDT of the original DL multi-input single-output CC scheme, in which the BS is connected to a centralized data library."],"url":"http://arxiv.org/abs/2402.00465v1"}
{"created":"2024-02-01 10:07:12","title":"Data Management Challenges in Agile Software Projects: A Systematic Literature Review","abstract":"Agile software development follows an adaptive and iterative approach. However, the management of data (e.g., development data or product data) can pose significant challenges for projects and agile teams. We aim to identify and characterize key challenges faced in data management within agile projects and to examine potential solutions proposed in the literature. We used a Systematic Literature Review (SLR) to collect and analyse relevant studies. We identified 45 studies related to data management in agile software development. We then manually analysed and mapped data from these studies to categorise different data management aspects and identify challenges and solutions as identified in those studies. Our findings reveal major challenges such as data integration and quality assurance. We found implications of challenges on team members and the product delivery process. We found that teams frequently struggle to integrate heterogeneous data sources, ensuring data reliability and real-time analytics. Additionally, fragmented data collection and a lack of standardized practices can impede team collaboration and project transparency. The studies have also proposed various solutions to address those challenges, including the use of ontologies, diverse data management strategies, automated tools, and the adoption of quality-focused development methods. Solutions also include training to enhance data quality and analysis. This SLR provides in-depth insights and recommendations for practitioners, emphasizing the importance of robust data management strategies. It suggests integrating advanced data management techniques into agile frameworks to enhance decision-making and improve software project outcomes. The study highlights the need for a more focused approach to data management in agile environments, advocating tailored solutions to meet the unique demands of agile software development.","sentences":["Agile software development follows an adaptive and iterative approach.","However, the management of data (e.g., development data or product data) can pose significant challenges for projects and agile teams.","We aim to identify and characterize key challenges faced in data management within agile projects and to examine potential solutions proposed in the literature.","We used a Systematic Literature Review (SLR) to collect and analyse relevant studies.","We identified 45 studies related to data management in agile software development.","We then manually analysed and mapped data from these studies to categorise different data management aspects and identify challenges and solutions as identified in those studies.","Our findings reveal major challenges such as data integration and quality assurance.","We found implications of challenges on team members and the product delivery process.","We found that teams frequently struggle to integrate heterogeneous data sources, ensuring data reliability and real-time analytics.","Additionally, fragmented data collection and a lack of standardized practices can impede team collaboration and project transparency.","The studies have also proposed various solutions to address those challenges, including the use of ontologies, diverse data management strategies, automated tools, and the adoption of quality-focused development methods.","Solutions also include training to enhance data quality and analysis.","This SLR provides in-depth insights and recommendations for practitioners, emphasizing the importance of robust data management strategies.","It suggests integrating advanced data management techniques into agile frameworks to enhance decision-making and improve software project outcomes.","The study highlights the need for a more focused approach to data management in agile environments, advocating tailored solutions to meet the unique demands of agile software development."],"url":"http://arxiv.org/abs/2402.00462v1"}
{"created":"2024-02-01 09:36:56","title":"CPT: Competence-progressive Training Strategy for Few-shot Node Classification","abstract":"Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhancing overall performance. Specifically, in CPT's initial stage, the focus is on simpler tasks, fostering foundational skills for engaging with complex tasks later. Importantly, the second stage dynamically adjusts task difficulty based on the meta-learner's growing competence, aiming for optimal knowledge acquisition. Extensive experiments on popular node classification datasets demonstrate significant improvements of our strategy over existing methods.","sentences":["Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data.","Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data.","Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels.","This could lead the meta-learner to face complex tasks too soon, hindering proper learning.","Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning.","So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhancing overall performance.","Specifically, in CPT's initial stage, the focus is on simpler tasks, fostering foundational skills for engaging with complex tasks later.","Importantly, the second stage dynamically adjusts task difficulty based on the meta-learner's growing competence, aiming for optimal knowledge acquisition.","Extensive experiments on popular node classification datasets demonstrate significant improvements of our strategy over existing methods."],"url":"http://arxiv.org/abs/2402.00450v1"}
{"created":"2024-02-01 09:36:26","title":"Efficient Training Spiking Neural Networks with Parallel Spiking Unit","abstract":"Efficient parallel computing has become a pivotal element in advancing artificial intelligence. Yet, the deployment of Spiking Neural Networks (SNNs) in this domain is hampered by their inherent sequential computational dependency. This constraint arises from the need for each time step's processing to rely on the preceding step's outcomes, significantly impeding the adaptability of SNN models to massively parallel computing environments. Addressing this challenge, our paper introduces the innovative Parallel Spiking Unit (PSU) and its two derivatives, the Input-aware PSU (IPSU) and Reset-aware PSU (RPSU). These variants skillfully decouple the leaky integration and firing mechanisms in spiking neurons while probabilistically managing the reset process. By preserving the fundamental computational attributes of the spiking neuron model, our approach enables the concurrent computation of all membrane potential instances within the SNN, facilitating parallel spike output generation and substantially enhancing computational efficiency. Comprehensive testing across various datasets, including static and sequential images, Dynamic Vision Sensor (DVS) data, and speech datasets, demonstrates that the PSU and its variants not only significantly boost performance and simulation speed but also augment the energy efficiency of SNNs through enhanced sparsity in neural activity. These advancements underscore the potential of our method in revolutionizing SNN deployment for high-performance parallel computing applications.","sentences":["Efficient parallel computing has become a pivotal element in advancing artificial intelligence.","Yet, the deployment of Spiking Neural Networks (SNNs) in this domain is hampered by their inherent sequential computational dependency.","This constraint arises from the need for each time step's processing to rely on the preceding step's outcomes, significantly impeding the adaptability of SNN models to massively parallel computing environments.","Addressing this challenge, our paper introduces the innovative Parallel Spiking Unit (PSU) and its two derivatives, the Input-aware PSU (IPSU) and Reset-aware PSU (RPSU).","These variants skillfully decouple the leaky integration and firing mechanisms in spiking neurons while probabilistically managing the reset process.","By preserving the fundamental computational attributes of the spiking neuron model, our approach enables the concurrent computation of all membrane potential instances within the SNN, facilitating parallel spike output generation and substantially enhancing computational efficiency.","Comprehensive testing across various datasets, including static and sequential images, Dynamic Vision Sensor (DVS) data, and speech datasets, demonstrates that the PSU and its variants not only significantly boost performance and simulation speed but also augment the energy efficiency of SNNs through enhanced sparsity in neural activity.","These advancements underscore the potential of our method in revolutionizing SNN deployment for high-performance parallel computing applications."],"url":"http://arxiv.org/abs/2402.00449v1"}
{"created":"2024-02-01 09:32:39","title":"Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection","abstract":"Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies. First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks. Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions. In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination. We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments. The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks.","sentences":["Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies.","However, vanilla S-T network is not stable.","Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies.","But using different structures can increase the likelihood of divergent performance on normal data.","To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture.","Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures.","This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation.","To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies.","First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks.","Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions.","In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination.","We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments.","The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks."],"url":"http://arxiv.org/abs/2402.00448v1"}
{"created":"2024-02-01 09:28:48","title":"A Survey of Data-Efficient Graph Learning","abstract":"Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including self-supervised graph learning, semi-supervised graph learning, and few-shot graph learning. Also, we state promising directions for future research, contributing to the evolution of graph machine learning.","sentences":["Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems.","While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources.","To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision.","In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL.","We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL.","Next, we systematically review recent advances on this topic from several key aspects, including self-supervised graph learning, semi-supervised graph learning, and few-shot graph learning.","Also, we state promising directions for future research, contributing to the evolution of graph machine learning."],"url":"http://arxiv.org/abs/2402.00447v1"}
{"created":"2024-02-01 08:45:28","title":"Reproducibility of Build Environments through Space and Time","abstract":"Modern software engineering builds up on the composability of software components, that rely on more and more direct and transitive dependencies to build their functionalities. This principle of reusability however makes it harder to reproduce projects' build environments, even though reproducibility of build environments is essential for collaboration, maintenance and component lifetime. In this work, we argue that functional package managers provide the tooling to make build environments reproducible in space and time, and we produce a preliminary evaluation to justify this claim. Using historical data, we show that we are able to reproduce build environments of about 7 million Nix packages, and to rebuild 99.94% of the 14 thousand packages from a 6-year-old Nixpkgs revision.","sentences":["Modern software engineering builds up on the composability of software components, that rely on more and more direct and transitive dependencies to build their functionalities.","This principle of reusability however makes it harder to reproduce projects' build environments, even though reproducibility of build environments is essential for collaboration, maintenance and component lifetime.","In this work, we argue that functional package managers provide the tooling to make build environments reproducible in space and time, and we produce a preliminary evaluation to justify this claim.","Using historical data, we show that we are able to reproduce build environments of about 7 million Nix packages, and to rebuild 99.94% of the 14 thousand packages from a 6-year-old Nixpkgs revision."],"url":"http://arxiv.org/abs/2402.00424v1"}
{"created":"2024-02-01 08:37:13","title":"From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models","abstract":"In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of our proposed hybrid recommender system tailored for OA (both LLM-based and non-LLM-based) (study 3), the quality of response generation (study 4), and the practical value of the systems in real-world scenarios via user studies (study 5). Results demonstrate that both PARIS and LE-PARIS significantly meet key metrics and positively impact attorney performance.","sentences":["In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect.","To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS).","These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses.","The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation.","Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years.","Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of our proposed hybrid recommender system tailored for OA (both LLM-based and non-LLM-based) (study 3), the quality of response generation (study 4), and the practical value of the systems in real-world scenarios via user studies (study 5).","Results demonstrate that both PARIS and LE-PARIS significantly meet key metrics and positively impact attorney performance."],"url":"http://arxiv.org/abs/2402.00421v1"}
{"created":"2024-02-01 08:02:10","title":"InfMAE: A Foundation Model in Infrared Modality","abstract":"In recent years, the foundation models have swept the computer vision field and facilitated the development of various tasks within different modalities. However, it remains an open question on how to design an infrared foundation model. In this paper, we propose InfMAE, a foundation model in infrared modality. We release an infrared dataset, called Inf30 to address the problem of lacking large-scale data for self-supervised learning in the infrared vision community. Besides, we design an information-aware masking strategy, which is suitable for infrared images. This masking strategy allows for a greater emphasis on the regions with richer information in infrared images during the self-supervised learning process, which is conducive to learning the generalized representation. In addition, we adopt a multi-scale encoder to enhance the performance of the pre-trained encoders in downstream tasks. Finally, based on the fact that infrared images do not have a lot of details and texture information, we design an infrared decoder module, which further improves the performance of downstream tasks. Extensive experiments show that our proposed method InfMAE outperforms other supervised methods and self-supervised learning methods in three downstream tasks. Our code will be made public at https://github.com/liufangcen/InfMAE.","sentences":["In recent years, the foundation models have swept the computer vision field and facilitated the development of various tasks within different modalities.","However, it remains an open question on how to design an infrared foundation model.","In this paper, we propose InfMAE, a foundation model in infrared modality.","We release an infrared dataset, called Inf30 to address the problem of lacking large-scale data for self-supervised learning in the infrared vision community.","Besides, we design an information-aware masking strategy, which is suitable for infrared images.","This masking strategy allows for a greater emphasis on the regions with richer information in infrared images during the self-supervised learning process, which is conducive to learning the generalized representation.","In addition, we adopt a multi-scale encoder to enhance the performance of the pre-trained encoders in downstream tasks.","Finally, based on the fact that infrared images do not have a lot of details and texture information, we design an infrared decoder module, which further improves the performance of downstream tasks.","Extensive experiments show that our proposed method InfMAE outperforms other supervised methods and self-supervised learning methods in three downstream tasks.","Our code will be made public at https://github.com/liufangcen/InfMAE."],"url":"http://arxiv.org/abs/2402.00407v1"}
{"created":"2024-02-01 07:33:31","title":"Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting","abstract":"Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, the traffic data of the data-scarce target city could query the traffic pattern bank, facilitating the aggregation of meta-knowledge. This meta-knowledge, in turn, assumes a pivotal role as a robust guide in subsequent processes involving graph reconstruction and forecasting. Empirical assessments conducted on real-world traffic datasets affirm the superior performance of MTPB, surpassing existing methods across various categories and exhibiting numerous attributes conducive to the advancement of cross-city few-shot forecasting methodologies. The code is available in https://github.com/zhyliu00/MTPB.","sentences":["Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control.","However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting.","Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities.","Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB).","Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process.","Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge.","Next, the traffic data of the data-scarce target city could query the traffic pattern bank, facilitating the aggregation of meta-knowledge.","This meta-knowledge, in turn, assumes a pivotal role as a robust guide in subsequent processes involving graph reconstruction and forecasting.","Empirical assessments conducted on real-world traffic datasets affirm the superior performance of MTPB, surpassing existing methods across various categories and exhibiting numerous attributes conducive to the advancement of cross-city few-shot forecasting methodologies.","The code is available in https://github.com/zhyliu00/MTPB."],"url":"http://arxiv.org/abs/2402.00397v1"}
{"created":"2024-02-01 07:28:55","title":"Loss Function Considering Dead Zone for Neural Networks","abstract":"It is important to reveal the inverse dynamics of manipulators to improve control performance of model-based control. Neural networks (NNs) are promising techniques to represent complicated inverse dynamics while they require a large amount of motion data. However, motion data in dead zones of actuators is not suitable for training models decreasing the number of useful training data. In this study, based on the fact that the manipulator joint does not work irrespective of input torque in dead zones, we propose a new loss function that considers only errors of joints not in dead zones. The proposed method enables to increase in the amount of motion data available for training and the accuracy of the inverse dynamics computation. Experiments on actual equipment using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than conventional methods. We also confirmed and discussed the behavior of the model of the proposed method in dead zones.","sentences":["It is important to reveal the inverse dynamics of manipulators to improve control performance of model-based control.","Neural networks (NNs) are promising techniques to represent complicated inverse dynamics while they require a large amount of motion data.","However, motion data in dead zones of actuators is not suitable for training models decreasing the number of useful training data.","In this study, based on the fact that the manipulator joint does not work irrespective of input torque in dead zones, we propose a new loss function that considers only errors of joints not in dead zones.","The proposed method enables to increase in the amount of motion data available for training and the accuracy of the inverse dynamics computation.","Experiments on actual equipment using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than conventional methods.","We also confirmed and discussed the behavior of the model of the proposed method in dead zones."],"url":"http://arxiv.org/abs/2402.00393v1"}
{"created":"2024-02-01 07:22:52","title":"EASRec: Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems","abstract":"In this age where data is abundant, the ability to distill meaningful insights from the sea of information is essential. Our research addresses the computational and resource inefficiencies that current Sequential Recommender Systems (SRSs) suffer from. especially those employing attention-based models like SASRec, These systems are designed for next-item recommendations in various applications, from e-commerce to social networks. However, such systems suffer from substantial computational costs and resource consumption during the inference stage. To tackle these issues, our research proposes a novel method that combines automatic pruning techniques with advanced model architectures. We also explore the potential of resource-constrained Neural Architecture Search (NAS), a technique prevalent in the realm of recommendation systems, to fine-tune models for reduced FLOPs, latency, and energy usage while retaining or even enhancing accuracy. The main contribution of our work is developing the Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems (EASRec). This approach aims to find optimal compact architectures for attention-based SRSs, ensuring accuracy retention. EASRec introduces data-aware gates that leverage historical information from input data batch to improve the performance of the recommendation network. Additionally, it utilizes a dynamic resource constraint approach, which standardizes the search process and results in more appropriate architectures. The effectiveness of our methodology is validated through exhaustive experiments on three benchmark datasets, which demonstrates EASRec's superiority in SRSs. Our research set a new standard for future exploration into efficient and accurate recommender systems, signifying a substantial advancement within this swiftly advancing field.","sentences":["In this age where data is abundant, the ability to distill meaningful insights from the sea of information is essential.","Our research addresses the computational and resource inefficiencies that current Sequential Recommender Systems (SRSs) suffer from.","especially those employing attention-based models like SASRec, These systems are designed for next-item recommendations in various applications, from e-commerce to social networks.","However, such systems suffer from substantial computational costs and resource consumption during the inference stage.","To tackle these issues, our research proposes a novel method that combines automatic pruning techniques with advanced model architectures.","We also explore the potential of resource-constrained Neural Architecture Search (NAS), a technique prevalent in the realm of recommendation systems, to fine-tune models for reduced FLOPs, latency, and energy usage while retaining or even enhancing accuracy.","The main contribution of our work is developing the Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems (EASRec).","This approach aims to find optimal compact architectures for attention-based SRSs, ensuring accuracy retention.","EASRec introduces data-aware gates that leverage historical information from input data batch to improve the performance of the recommendation network.","Additionally, it utilizes a dynamic resource constraint approach, which standardizes the search process and results in more appropriate architectures.","The effectiveness of our methodology is validated through exhaustive experiments on three benchmark datasets, which demonstrates EASRec's superiority in SRSs.","Our research set a new standard for future exploration into efficient and accurate recommender systems, signifying a substantial advancement within this swiftly advancing field."],"url":"http://arxiv.org/abs/2402.00390v1"}
{"created":"2024-02-01 07:21:30","title":"Cumulative Distribution Function based General Temporal Point Processes","abstract":"Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies. Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends. However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns. The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models. While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively. In this study, we introduce the CuFun model, representing a novel approach to TPPs that revolves around the Cumulative Distribution Function (CDF). CuFun stands out by uniquely employing a monotonic neural network for CDF representation, utilizing past events as a scaling factor. This innovation significantly bolsters the model's adaptability and precision across a wide range of data scenarios. Our approach addresses several critical issues inherent in traditional TPP modeling: it simplifies log-likelihood calculations, extends applicability beyond predefined density function forms, and adeptly captures long-range temporal patterns. Our contributions encompass the introduction of a pioneering CDF-based TPP model, the development of a methodology for incorporating past event information into future event prediction, and empirical validation of CuFun's effectiveness through extensive experimentation on synthetic and real-world datasets.","sentences":["Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies.","Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends.","However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns.","The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models.","While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively.","In this study, we introduce the CuFun model, representing a novel approach to TPPs that revolves around the Cumulative Distribution Function (CDF).","CuFun stands out by uniquely employing a monotonic neural network for CDF representation, utilizing past events as a scaling factor.","This innovation significantly bolsters the model's adaptability and precision across a wide range of data scenarios.","Our approach addresses several critical issues inherent in traditional TPP modeling: it simplifies log-likelihood calculations, extends applicability beyond predefined density function forms, and adeptly captures long-range temporal patterns.","Our contributions encompass the introduction of a pioneering CDF-based TPP model, the development of a methodology for incorporating past event information into future event prediction, and empirical validation of CuFun's effectiveness through extensive experimentation on synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2402.00388v1"}
{"created":"2024-02-01 06:59:00","title":"A Joint Communication and Computation Framework for Digital Twin over Wireless Networks","abstract":"In this paper, the problem of low-latency communication and computation resource allocation for digital twin (DT) over wireless networks is investigated. In the considered model, multiple physical devices in the physical network (PN) needs to frequently offload the computation task related data to the digital network twin (DNT), which is generated and controlled by the central server. Due to limited energy budget of the physical devices, both computation accuracy and wireless transmission power must be considered during the DT procedure. This joint communication and computation problem is formulated as an optimization problem whose goal is to minimize the overall transmission delay of the system under total PN energy and DNT model accuracy constraints. To solve this problem, an alternating algorithm with iteratively solving device scheduling, power control, and data offloading subproblems. For the device scheduling subproblem, the optimal solution is obtained in closed form through the dual method. For the special case with one physical device, the optimal number of transmission times is reveled. Based on the theoretical findings, the original problem is transformed into a simplified problem and the optimal device scheduling can be found. Numerical results verify that the proposed algorithm can reduce the transmission delay of the system by up to 51.2\\% compared to the conventional schemes.","sentences":["In this paper, the problem of low-latency communication and computation resource allocation for digital twin (DT) over wireless networks is investigated.","In the considered model, multiple physical devices in the physical network (PN) needs to frequently offload the computation task related data to the digital network twin (DNT), which is generated and controlled by the central server.","Due to limited energy budget of the physical devices, both computation accuracy and wireless transmission power must be considered during the DT procedure.","This joint communication and computation problem is formulated as an optimization problem whose goal is to minimize the overall transmission delay of the system under total PN energy and DNT model accuracy constraints.","To solve this problem, an alternating algorithm with iteratively solving device scheduling, power control, and data offloading subproblems.","For the device scheduling subproblem, the optimal solution is obtained in closed form through the dual method.","For the special case with one physical device, the optimal number of transmission times is reveled.","Based on the theoretical findings, the original problem is transformed into a simplified problem and the optimal device scheduling can be found.","Numerical results verify that the proposed algorithm can reduce the transmission delay of the system by up to 51.2\\% compared to the conventional schemes."],"url":"http://arxiv.org/abs/2402.00381v1"}
{"created":"2024-02-01 06:06:59","title":"Legged Robot State Estimation With Invariant Extended Kalman Filter Using Neural Measurement Network","abstract":"This paper introduces a novel proprioceptive state estimator for legged robots that combines model-based filters and deep neural networks. Recent studies have shown that neural networks such as multi-layer perceptron or recurrent neural networks can estimate the robot states, including contact probability and linear velocity. Inspired by this, we develop a state estimation framework that integrates a neural measurement network (NMN) with an invariant extended Kalman filter. We show that our framework improves estimation performance in various terrains. Existing studies that combine model-based filters and learning-based approaches typically use real-world data. However, our approach relies solely on simulation data, as it allows us to easily obtain extensive data. This difference leads to a gap between the learning and the inference domain, commonly referred to as a sim-to-real gap. We address this challenge by adapting existing learning techniques and regularization. To validate our proposed method, we conduct experiments using a quadruped robot on four types of terrain: \\textit{flat}, \\textit{debris}, \\textit{soft}, and \\textit{slippery}. We observe that our approach significantly reduces position drift compared to the existing model-based state estimator.","sentences":["This paper introduces a novel proprioceptive state estimator for legged robots that combines model-based filters and deep neural networks.","Recent studies have shown that neural networks such as multi-layer perceptron or recurrent neural networks can estimate the robot states, including contact probability and linear velocity.","Inspired by this, we develop a state estimation framework that integrates a neural measurement network (NMN) with an invariant extended Kalman filter.","We show that our framework improves estimation performance in various terrains.","Existing studies that combine model-based filters and learning-based approaches typically use real-world data.","However, our approach relies solely on simulation data, as it allows us to easily obtain extensive data.","This difference leads to a gap between the learning and the inference domain, commonly referred to as a sim-to-real gap.","We address this challenge by adapting existing learning techniques and regularization.","To validate our proposed method, we conduct experiments using a quadruped robot on four types of terrain: \\textit{flat}, \\textit{debris}, \\textit{soft}, and \\textit{slippery}.","We observe that our approach significantly reduces position drift compared to the existing model-based state estimator."],"url":"http://arxiv.org/abs/2402.00366v1"}
{"created":"2024-02-01 05:55:43","title":"Securing Cloud-Based Internet of Things: Challenges and Mitigations","abstract":"The Internet of Things (IoT) has seen remarkable advancements in recent years, leading to a paradigm shift in the digital landscape. However, these technological strides have also brought new challenges, particularly in terms of cybersecurity. IoT devices are inherently connected to the internet, which makes them more vulnerable to attack. In addition, IoT services often handle sensitive user data, which could be misused by malicious actors or unauthorized service providers. As more mainstream service providers emerge without uniform regulations, these security risks are expected to escalate exponentially. The task of maintaining the security of IoT devices while they interact with cloud services is also challenging. Newer IoT services, especially those developed and deployed via Platform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) models, pose additional security threats. Although IoT devices are becoming more affordable and ubiquitous, their growing complexity could expose users to heightened security and privacy risks. This paper highlights these pressing security concerns associated with the widespread adoption of IoT devices and services. We propose potential solutions to bridge the existing security gaps and expect future challenges. Our approach entails a comprehensive exploration of the key security challenges that IoT services are currently facing. We also suggest proactive strategies to mitigate these risks, strengthening the overall security of IoT devices and services.","sentences":["The Internet of Things (IoT) has seen remarkable advancements in recent years, leading to a paradigm shift in the digital landscape.","However, these technological strides have also brought new challenges, particularly in terms of cybersecurity.","IoT devices are inherently connected to the internet, which makes them more vulnerable to attack.","In addition, IoT services often handle sensitive user data, which could be misused by malicious actors or unauthorized service providers.","As more mainstream service providers emerge without uniform regulations, these security risks are expected to escalate exponentially.","The task of maintaining the security of IoT devices while they interact with cloud services is also challenging.","Newer IoT services, especially those developed and deployed via Platform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) models, pose additional security threats.","Although IoT devices are becoming more affordable and ubiquitous, their growing complexity could expose users to heightened security and privacy risks.","This paper highlights these pressing security concerns associated with the widespread adoption of IoT devices and services.","We propose potential solutions to bridge the existing security gaps and expect future challenges.","Our approach entails a comprehensive exploration of the key security challenges that IoT services are currently facing.","We also suggest proactive strategies to mitigate these risks, strengthening the overall security of IoT devices and services."],"url":"http://arxiv.org/abs/2402.00356v1"}
{"created":"2024-02-01 05:51:03","title":"High-Quality Medical Image Generation from Free-hand Sketch","abstract":"Generating medical images from human-drawn free-hand sketches holds promise for various important medical imaging applications. Due to the extreme difficulty in collecting free-hand sketch data in the medical domain, most deep learning-based methods have been proposed to generate medical images from the synthesized sketches (e.g., edge maps or contours of segmentation masks from real images). However, these models often fail to generalize on the free-hand sketches, leading to unsatisfactory results. In this paper, we propose a practical free-hand sketch-to-image generation model called Sketch2MedI that learns to represent sketches in StyleGAN's latent space and generate medical images from it. Thanks to the ability to encode sketches into this meaningful representation space, Sketch2MedI only requires synthesized sketches for training, enabling a cost-effective learning process. Our Sketch2MedI demonstrates a robust generalization to free-hand sketches, resulting in high-quality and realistic medical image generations. Comparative evaluations of Sketch2MedI against the pix2pix, CycleGAN, UNIT, and U-GAT-IT models show superior performance in generating pharyngeal images, both quantitative and qualitative across various metrics.","sentences":["Generating medical images from human-drawn free-hand sketches holds promise for various important medical imaging applications.","Due to the extreme difficulty in collecting free-hand sketch data in the medical domain, most deep learning-based methods have been proposed to generate medical images from the synthesized sketches (e.g., edge maps or contours of segmentation masks from real images).","However, these models often fail to generalize on the free-hand sketches, leading to unsatisfactory results.","In this paper, we propose a practical free-hand sketch-to-image generation model called Sketch2MedI that learns to represent sketches in StyleGAN's latent space and generate medical images from it.","Thanks to the ability to encode sketches into this meaningful representation space, Sketch2MedI only requires synthesized sketches for training, enabling a cost-effective learning process.","Our Sketch2MedI demonstrates a robust generalization to free-hand sketches, resulting in high-quality and realistic medical image generations.","Comparative evaluations of Sketch2MedI against the pix2pix, CycleGAN, UNIT, and U-GAT-IT models show superior performance in generating pharyngeal images, both quantitative and qualitative across various metrics."],"url":"http://arxiv.org/abs/2402.00353v1"}
{"created":"2024-02-01 05:35:25","title":"Machine Unlearning for Image-to-Image Generative Models","abstract":"Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models. Our code is available at https://github.com/jpmorganchase/l2l-generator-unlearning.","sentences":["Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations.","However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored.","This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models.","Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples.","Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy.","To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models.","Our code is available at https://github.com/jpmorganchase/l2l-generator-unlearning."],"url":"http://arxiv.org/abs/2402.00351v1"}
{"created":"2024-02-01 05:28:28","title":"Diverse Explanations from Data-driven and Domain-driven Perspectives for Machine Learning Models","abstract":"Explanations of machine learning models are important, especially in scientific areas such as chemistry, biology, and physics, where they guide future laboratory experiments and resource requirements. These explanations can be derived from well-trained machine learning models (data-driven perspective) or specific domain knowledge (domain-driven perspective). However, there exist inconsistencies between these perspectives due to accurate yet misleading machine learning models and various stakeholders with specific needs, wants, or aims. This paper calls attention to these inconsistencies and suggests a way to find an accurate model with expected explanations that reinforce physical laws and meet stakeholders' requirements from a set of equally-good models, also known as Rashomon sets. Our goal is to foster a comprehensive understanding of these inconsistencies and ultimately contribute to the integration of eXplainable Artificial Intelligence (XAI) into scientific domains.","sentences":["Explanations of machine learning models are important, especially in scientific areas such as chemistry, biology, and physics, where they guide future laboratory experiments and resource requirements.","These explanations can be derived from well-trained machine learning models (data-driven perspective) or specific domain knowledge (domain-driven perspective).","However, there exist inconsistencies between these perspectives due to accurate yet misleading machine learning models and various stakeholders with specific needs, wants, or aims.","This paper calls attention to these inconsistencies and suggests a way to find an accurate model with expected explanations that reinforce physical laws and meet stakeholders' requirements from a set of equally-good models, also known as Rashomon sets.","Our goal is to foster a comprehensive understanding of these inconsistencies and ultimately contribute to the integration of eXplainable Artificial Intelligence (XAI) into scientific domains."],"url":"http://arxiv.org/abs/2402.00347v1"}
{"created":"2024-02-01 05:20:07","title":"IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators","abstract":"This study focuses on media bias detection, crucial in today's era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input's bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions). Experimental results on four political bias datasets highlight IndiVec's significant superiority over baselines. Furthermore, additional experiments and analysis provide profound insights into the framework's effectiveness.","sentences":["This study focuses on media bias detection, crucial in today's era of influential social media platforms shaping individual attitudes and opinions.","In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models.","IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques.","When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input's bias label.","IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions).","Experimental results on four political bias datasets highlight IndiVec's significant superiority over baselines.","Furthermore, additional experiments and analysis provide profound insights into the framework's effectiveness."],"url":"http://arxiv.org/abs/2402.00345v1"}
{"created":"2024-02-01 05:19:15","title":"Reimagining TaxiVis through an Immersive Space-Time Cube metaphor and reflecting on potential benefits of Immersive Analytics for urban data exploration","abstract":"Current visualization research has identified the potential of more immersive settings for data exploration, leveraging VR and AR technologies. To explore how a traditional visualization system could be adapted into an immersive framework, and how it could benefit from this, we decided to revisit a landmark paper presented ten years ago at IEEE VIS. TaxiVis, by Ferreira et al., enabled interactive spatio-temporal querying of a large dataset of taxi trips in New York City. Here, we reimagine how TaxiVis' functionalities could be implemented and extended in a 3D immersive environment. Among the unique features we identify as being enabled by the Immersive TaxiVis prototype are alternative uses of the additional visual dimension, a fully visual 3D spatio-temporal query framework, and the opportunity to explore the data at different scales and frames of reference. By revisiting the case studies from the original paper, we demonstrate workflows that can benefit from this immersive perspective. Through reporting on our experience, and on the vision and reasoning behind our design decisions, we hope to contribute to the debate on how conventional and immersive visualization paradigms can complement each other and on how the exploration of urban datasets can be facilitated in the coming years.","sentences":["Current visualization research has identified the potential of more immersive settings for data exploration, leveraging VR and AR technologies.","To explore how a traditional visualization system could be adapted into an immersive framework, and how it could benefit from this, we decided to revisit a landmark paper presented ten years ago at IEEE VIS.","TaxiVis, by Ferreira et al., enabled interactive spatio-temporal querying of a large dataset of taxi trips in New York City.","Here, we reimagine how TaxiVis' functionalities could be implemented and extended in a 3D immersive environment.","Among the unique features we identify as being enabled by the Immersive TaxiVis prototype are alternative uses of the additional visual dimension, a fully visual 3D spatio-temporal query framework, and the opportunity to explore the data at different scales and frames of reference.","By revisiting the case studies from the original paper, we demonstrate workflows that can benefit from this immersive perspective.","Through reporting on our experience, and on the vision and reasoning behind our design decisions, we hope to contribute to the debate on how conventional and immersive visualization paradigms can complement each other and on how the exploration of urban datasets can be facilitated in the coming years."],"url":"http://arxiv.org/abs/2402.00344v1"}
{"created":"2024-02-01 05:13:14","title":"Survey of Privacy Threats and Countermeasures in Federated Learning","abstract":"Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients. Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied. However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way. In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning.","sentences":["Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients.","Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied.","However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way.","In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning."],"url":"http://arxiv.org/abs/2402.00342v1"}
{"created":"2024-02-01 05:03:05","title":"Can you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features?","abstract":"Self-supervised features are typically used in place of filter-banks in speaker verification models. However, these models were originally designed to ingest filter-banks as inputs, and thus, training them on top of self-supervised features assumes that both feature types require the same amount of learning for the task. In this work, we observe that pre-trained self-supervised speech features inherently include information required for downstream speaker verification task, and therefore, we can simplify the downstream model without sacrificing performance. To this end, we revisit the design of the downstream model for speaker verification using self-supervised features. We show that we can simplify the model to use 97.51% fewer parameters while achieving a 29.93% average improvement in performance on SUPERB. Consequently, we show that the simplified downstream model is more data efficient compared to baseline--it achieves better performance with only 60% of the training data.","sentences":["Self-supervised features are typically used in place of filter-banks in speaker verification models.","However, these models were originally designed to ingest filter-banks as inputs, and thus, training them on top of self-supervised features assumes that both feature types require the same amount of learning for the task.","In this work, we observe that pre-trained self-supervised speech features inherently include information required for downstream speaker verification task, and therefore, we can simplify the downstream model without sacrificing performance.","To this end, we revisit the design of the downstream model for speaker verification using self-supervised features.","We show that we can simplify the model to use 97.51% fewer parameters while achieving a 29.93% average improvement in performance on SUPERB.","Consequently, we show that the simplified downstream model is more data efficient compared to baseline--it achieves better performance with only 60% of the training data."],"url":"http://arxiv.org/abs/2402.00340v1"}
{"created":"2024-02-01 04:23:32","title":"Night-Rider: Nocturnal Vision-aided Localization in Streetlight Maps Using Invariant Extended Kalman Filtering","abstract":"Vision-aided localization for low-cost mobile robots in diverse environments has attracted widespread attention recently. Although many current systems are applicable in daytime environments, nocturnal visual localization is still an open problem owing to the lack of stable visual information. An insight from most nocturnal scenes is that the static and bright streetlights are reliable visual information for localization. Hence we propose a nocturnal vision-aided localization system in streetlight maps with a novel data association and matching scheme using object detection methods. We leverage the Invariant Extended Kalman Filter (InEKF) to fuse IMU, odometer, and camera measurements for consistent state estimation at night. Furthermore, a tracking recovery module is also designed for tracking failures. Experiments on multiple real nighttime scenes validate that the system can achieve remarkably accurate and robust localization in nocturnal environments.","sentences":["Vision-aided localization for low-cost mobile robots in diverse environments has attracted widespread attention recently.","Although many current systems are applicable in daytime environments, nocturnal visual localization is still an open problem owing to the lack of stable visual information.","An insight from most nocturnal scenes is that the static and bright streetlights are reliable visual information for localization.","Hence we propose a nocturnal vision-aided localization system in streetlight maps with a novel data association and matching scheme using object detection methods.","We leverage the Invariant Extended Kalman Filter (InEKF) to fuse IMU, odometer, and camera measurements for consistent state estimation at night.","Furthermore, a tracking recovery module is also designed for tracking failures.","Experiments on multiple real nighttime scenes validate that the system can achieve remarkably accurate and robust localization in nocturnal environments."],"url":"http://arxiv.org/abs/2402.00330v1"}
{"created":"2024-02-01 04:17:56","title":"PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks","abstract":"While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initialization scheme allows us to encode appropriate inductive biases corresponding to a given PDE system into the network architecture. We provide comprehensive empirical evidence showing that PirateNets are easier to optimize and can gain accuracy from considerably increased depth, ultimately achieving state-of-the-art results across various benchmarks. All code and data accompanying this manuscript will be made publicly available at \\url{https://github.com/PredictiveIntelligenceLab/jaxpi}.","sentences":["While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed.","Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss.","To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models.","PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training.","We also show that the proposed initialization scheme allows us to encode appropriate inductive biases corresponding to a given PDE system into the network architecture.","We provide comprehensive empirical evidence showing that PirateNets are easier to optimize and can gain accuracy from considerably increased depth, ultimately achieving state-of-the-art results across various benchmarks.","All code and data accompanying this manuscript will be made publicly available at \\url{https://github.com/PredictiveIntelligenceLab/jaxpi}."],"url":"http://arxiv.org/abs/2402.00326v1"}
{"created":"2024-02-01 04:15:59","title":"Bias in Opinion Summarisation from Pre-training to Adaptation: A Case Study in Political Bias","abstract":"Opinion summarisation aims to summarise the salient information and opinions presented in documents such as product reviews, discussion forums, and social media texts into short summaries that enable users to effectively understand the opinions therein. Generating biased summaries has the risk of potentially swaying public opinion. Previous studies focused on studying bias in opinion summarisation using extractive models, but limited research has paid attention to abstractive summarisation models. In this study, using political bias as a case study, we first establish a methodology to quantify bias in abstractive models, then trace it from the pre-trained models to the task of summarising social media opinions using different models and adaptation methods. We find that most models exhibit intrinsic bias. Using a social media text summarisation dataset and contrasting various adaptation methods, we find that tuning a smaller number of parameters is less biased compared to standard fine-tuning; however, the diversity of topics in training data used for fine-tuning is critical.","sentences":["Opinion summarisation aims to summarise the salient information and opinions presented in documents such as product reviews, discussion forums, and social media texts into short summaries that enable users to effectively understand the opinions therein.","Generating biased summaries has the risk of potentially swaying public opinion.","Previous studies focused on studying bias in opinion summarisation using extractive models, but limited research has paid attention to abstractive summarisation models.","In this study, using political bias as a case study, we first establish a methodology to quantify bias in abstractive models, then trace it from the pre-trained models to the task of summarising social media opinions using different models and adaptation methods.","We find that most models exhibit intrinsic bias.","Using a social media text summarisation dataset and contrasting various adaptation methods, we find that tuning a smaller number of parameters is less biased compared to standard fine-tuning; however, the diversity of topics in training data used for fine-tuning is critical."],"url":"http://arxiv.org/abs/2402.00322v1"}
{"created":"2024-02-01 04:15:39","title":"SmartCooper: Vehicular Collaborative Perception with Adaptive Fusion and Judger Mechanism","abstract":"In recent years, autonomous driving has garnered significant attention due to its potential for improving road safety through collaborative perception among connected and autonomous vehicles (CAVs). However, time-varying channel variations in vehicular transmission environments demand dynamic allocation of communication resources. Moreover, in the context of collaborative perception, it is important to recognize that not all CAVs contribute valuable data, and some CAV data even have detrimental effects on collaborative perception. In this paper, we introduce SmartCooper, an adaptive collaborative perception framework that incorporates communication optimization and a judger mechanism to facilitate CAV data fusion. Our approach begins with optimizing the connectivity of vehicles while considering communication constraints. We then train a learnable encoder to dynamically adjust the compression ratio based on the channel state information (CSI). Subsequently, we devise a judger mechanism to filter the detrimental image data reconstructed by adaptive decoders. We evaluate the effectiveness of our proposed algorithm on the OpenCOOD platform. Our results demonstrate a substantial reduction in communication costs by 23.10\\% compared to the non-judger scheme. Additionally, we achieve a significant improvement on the average precision of Intersection over Union (AP@IoU) by 7.15\\% compared with state-of-the-art schemes.","sentences":["In recent years, autonomous driving has garnered significant attention due to its potential for improving road safety through collaborative perception among connected and autonomous vehicles (CAVs).","However, time-varying channel variations in vehicular transmission environments demand dynamic allocation of communication resources.","Moreover, in the context of collaborative perception, it is important to recognize that not all CAVs contribute valuable data, and some CAV data even have detrimental effects on collaborative perception.","In this paper, we introduce SmartCooper, an adaptive collaborative perception framework that incorporates communication optimization and a judger mechanism to facilitate CAV data fusion.","Our approach begins with optimizing the connectivity of vehicles while considering communication constraints.","We then train a learnable encoder to dynamically adjust the compression ratio based on the channel state information (CSI).","Subsequently, we devise a judger mechanism to filter the detrimental image data reconstructed by adaptive decoders.","We evaluate the effectiveness of our proposed algorithm on the OpenCOOD platform.","Our results demonstrate a substantial reduction in communication costs by 23.10\\% compared to the non-judger scheme.","Additionally, we achieve a significant improvement on the average precision of Intersection over Union (AP@IoU) by 7.15\\% compared with state-of-the-art schemes."],"url":"http://arxiv.org/abs/2402.00321v1"}
{"created":"2024-02-01 03:56:48","title":"Online Distribution Learning with Local Private Constraints","abstract":"We study the problem of online conditional distribution estimation with \\emph{unbounded} label sets under local differential privacy. Let $\\mathcal{F}$ be a distribution-valued function class with unbounded label set. We aim at estimating an \\emph{unknown} function $f\\in \\mathcal{F}$ in an online fashion so that at time $t$ when the context $\\boldsymbol{x}_t$ is provided we can generate an estimate of $f(\\boldsymbol{x}_t)$ under KL-divergence knowing only a privatized version of the true labels sampling from $f(\\boldsymbol{x}_t)$. The ultimate objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\\epsilon,0)$-local differential privacy of the privatized labels, the KL-risk grows as $\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$ upto poly-logarithmic factors where $K=|\\mathcal{F}|$. This is in stark contrast to the $\\tilde{\\Theta}(\\sqrt{T\\log K})$ bound demonstrated by Wu et al. (2023a) for bounded label sets. As a byproduct, our results recover a nearly tight upper bound for the hypothesis selection problem of gopi et al. (2020) established only for the batch setting.","sentences":["We study the problem of online conditional distribution estimation with \\emph{unbounded} label sets under local differential privacy.","Let $\\mathcal{F}$ be a distribution-valued function class with unbounded label set.","We aim at estimating an \\emph{unknown} function $f\\in \\mathcal{F}$ in an online fashion so that at time $t$ when the context $\\boldsymbol{x}_t$ is provided we can generate an estimate of $f(\\boldsymbol{x}_t)$ under KL-divergence knowing only a privatized version of the true labels sampling from $f(\\boldsymbol{x}_t)$. The ultimate objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\\epsilon,0)$-local differential privacy of the privatized labels, the KL-risk grows as $\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$ upto poly-logarithmic factors where $K=|\\mathcal{F}|$. This is in stark contrast to the $\\tilde{\\Theta}(\\sqrt{T\\log K})$ bound demonstrated by Wu et al. (2023a) for bounded label sets.","As a byproduct, our results recover a nearly tight upper bound for the hypothesis selection problem of gopi et al.","(2020) established only for the batch setting."],"url":"http://arxiv.org/abs/2402.00315v1"}
{"created":"2024-02-01 03:27:26","title":"Self-supervised learning of video representations from a child's perspective","abstract":"Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities. Video models also learn more robust object representations than image-based models trained with the exact same data. These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases.","sentences":["Children learn powerful internal models of the world around them from a few years of egocentric visual experience.","Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases?","Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question.","However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world.","To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months).","The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities.","Video models also learn more robust object representations than image-based models trained with the exact same data.","These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases."],"url":"http://arxiv.org/abs/2402.00300v1"}
{"created":"2024-02-01 02:54:49","title":"Comparative Evaluation of Traditional and Deep Learning-Based Segmentation Methods for Spoil Pile Delineation Using UAV Images","abstract":"The stability of mine dumps is contingent upon the precise arrangement of spoil piles, taking into account their geological and geotechnical attributes. Yet, on-site characterisation of individual piles poses a formidable challenge. The utilisation of image-based techniques for spoil pile characterisation, employing remotely acquired data through unmanned aerial systems, is a promising complementary solution. Image processing, such as object-based classification and feature extraction, are dependent upon effective segmentation. This study refines and juxtaposes various segmentation approaches, specifically colour-based and morphology-based techniques. The objective is to enhance and evaluate avenues for object-based analysis for spoil characterisation within the context of mining environments. Furthermore, a comparative analysis is conducted between conventional segmentation approaches and those rooted in deep learning methodologies. Among the diverse segmentation approaches evaluated, the morphology-based deep learning segmentation approach, Segment Anything Model (SAM), exhibited superior performance in comparison to other approaches. This outcome underscores the efficacy of incorporating advanced morphological and deep learning techniques for accurate and efficient spoil pile characterisation. The findings of this study contribute valuable insights to the optimisation of segmentation strategies, thereby advancing the application of image-based techniques for the characterisation of spoil piles in mining environments.","sentences":["The stability of mine dumps is contingent upon the precise arrangement of spoil piles, taking into account their geological and geotechnical attributes.","Yet, on-site characterisation of individual piles poses a formidable challenge.","The utilisation of image-based techniques for spoil pile characterisation, employing remotely acquired data through unmanned aerial systems, is a promising complementary solution.","Image processing, such as object-based classification and feature extraction, are dependent upon effective segmentation.","This study refines and juxtaposes various segmentation approaches, specifically colour-based and morphology-based techniques.","The objective is to enhance and evaluate avenues for object-based analysis for spoil characterisation within the context of mining environments.","Furthermore, a comparative analysis is conducted between conventional segmentation approaches and those rooted in deep learning methodologies.","Among the diverse segmentation approaches evaluated, the morphology-based deep learning segmentation approach, Segment Anything Model (SAM), exhibited superior performance in comparison to other approaches.","This outcome underscores the efficacy of incorporating advanced morphological and deep learning techniques for accurate and efficient spoil pile characterisation.","The findings of this study contribute valuable insights to the optimisation of segmentation strategies, thereby advancing the application of image-based techniques for the characterisation of spoil piles in mining environments."],"url":"http://arxiv.org/abs/2402.00295v1"}
{"created":"2024-02-01 02:46:24","title":"Effective Bug Detection in Graph Database Engines: An LLM-based Approach","abstract":"Graph database engines play a pivotal role in efficiently storing and managing graph data across various domains, including bioinformatics, knowledge graphs, and recommender systems. Ensuring data accuracy within graph database engines is paramount, as inaccuracies can yield unreliable analytical outcomes. Current bug-detection approaches are confined to specific graph query languages, limiting their applicabilities when handling graph database engines that use various graph query languages across various domains. Moreover, they require extensive prior knowledge to generate queries for detecting bugs. To address these challenges, we introduces DGDB, a novel paradigm harnessing large language models(LLM), such as ChatGPT, for comprehensive bug detection in graph database engines. DGDB leverages ChatGPT to generate high-quality queries for different graph query languages. It subsequently employs differential testing to identify bugs in graph database engines. We applied this paradigm to graph database engines using the Gremlin query language and those using the Cypher query language, generating approximately 4,000 queries each. In the latest versions of Neo4j, Agensgraph, and JanusGraph databases, we detected 2, 5, and 3 wrong-result bugs, respectively.","sentences":["Graph database engines play a pivotal role in efficiently storing and managing graph data across various domains, including bioinformatics, knowledge graphs, and recommender systems.","Ensuring data accuracy within graph database engines is paramount, as inaccuracies can yield unreliable analytical outcomes.","Current bug-detection approaches are confined to specific graph query languages, limiting their applicabilities when handling graph database engines that use various graph query languages across various domains.","Moreover, they require extensive prior knowledge to generate queries for detecting bugs.","To address these challenges, we introduces DGDB, a novel paradigm harnessing large language models(LLM), such as ChatGPT, for comprehensive bug detection in graph database engines.","DGDB leverages ChatGPT to generate high-quality queries for different graph query languages.","It subsequently employs differential testing to identify bugs in graph database engines.","We applied this paradigm to graph database engines using the Gremlin query language and those using the Cypher query language, generating approximately 4,000 queries each.","In the latest versions of Neo4j, Agensgraph, and JanusGraph databases, we detected 2, 5, and 3 wrong-result bugs, respectively."],"url":"http://arxiv.org/abs/2402.00292v1"}
{"created":"2024-02-01 02:29:16","title":"PAP-REC: Personalized Automatic Prompt for Recommendation Language Model","abstract":"Recently emerged prompt-based Recommendation Language Models (RLM) can solve multiple recommendation tasks uniformly. The RLMs make full use of the inherited knowledge learned from the abundant pre-training data to solve the downstream recommendation tasks by prompts, without introducing additional parameters or network training. However, handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes. In this paper, we propose PAP-REC, a framework to generate the Personalized Automatic Prompt for RECommendation language models to mitigate the inefficiency and ineffectiveness problems derived from manually designed prompts. Specifically, personalized automatic prompts allow different users to have different prompt tokens for the same task, automatically generated using a gradient-based method. One challenge for personalized automatic prompt generation for recommendation language models is the extremely large search space, leading to a long convergence time. To effectively and efficiently address the problem, we develop surrogate metrics and leverage an alternative updating schedule for prompting recommendation language models. Experimental results show that our PAP-REC framework manages to generate personalized prompts, and the automatically generated prompts outperform manually constructed prompts and also outperform various baseline recommendation models. The source code of the work is available at https://github.com/rutgerswiselab/PAP-REC.","sentences":["Recently emerged prompt-based Recommendation Language Models (RLM) can solve multiple recommendation tasks uniformly.","The RLMs make full use of the inherited knowledge learned from the abundant pre-training data to solve the downstream recommendation tasks by prompts, without introducing additional parameters or network training.","However, handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes.","In this paper, we propose PAP-REC, a framework to generate the Personalized Automatic Prompt for RECommendation language models to mitigate the inefficiency and ineffectiveness problems derived from manually designed prompts.","Specifically, personalized automatic prompts allow different users to have different prompt tokens for the same task, automatically generated using a gradient-based method.","One challenge for personalized automatic prompt generation for recommendation language models is the extremely large search space, leading to a long convergence time.","To effectively and efficiently address the problem, we develop surrogate metrics and leverage an alternative updating schedule for prompting recommendation language models.","Experimental results show that our PAP-REC framework manages to generate personalized prompts, and the automatically generated prompts outperform manually constructed prompts and also outperform various baseline recommendation models.","The source code of the work is available at https://github.com/rutgerswiselab/PAP-REC."],"url":"http://arxiv.org/abs/2402.00284v1"}
{"created":"2024-02-01 02:00:32","title":"Improving Program Debloating with 1-DU Chain Minimality","abstract":"Modern software often struggles with bloat, leading to increased memory consumption and security vulnerabilities from unused code. In response, various program debloating techniques have been developed, typically utilizing test cases that represent functionalities users want to retain. These methods range from aggressive approaches, which prioritize maximal code reduction but may overfit to test cases and potentially reintroduce past security issues, to conservative strategies that aim to preserve all influenced code, often at the expense of less effective bloat reduction and security improvement. In this research, we present RLDebloatDU, an innovative debloating technique that employs 1-DU chain minimality within abstract syntax trees. Our approach maintains essential program data dependencies, striking a balance between aggressive code reduction and the preservation of program semantics. We evaluated RLDebloatDU on ten Linux kernel programs, comparing its performance with two leading debloating techniques: Chisel, known for its aggressive debloating approach, and Razor, recognized for its conservative strategy. RLDebloatDU significantly lowers the incidence of Common Vulnerabilities and Exposures (CVEs) and improves soundness compared to both, highlighting its efficacy in reducing security issues without reintroducing resolved security issues.","sentences":["Modern software often struggles with bloat, leading to increased memory consumption and security vulnerabilities from unused code.","In response, various program debloating techniques have been developed, typically utilizing test cases that represent functionalities users want to retain.","These methods range from aggressive approaches, which prioritize maximal code reduction but may overfit to test cases and potentially reintroduce past security issues, to conservative strategies that aim to preserve all influenced code, often at the expense of less effective bloat reduction and security improvement.","In this research, we present RLDebloatDU, an innovative debloating technique that employs 1-DU chain minimality within abstract syntax trees.","Our approach maintains essential program data dependencies, striking a balance between aggressive code reduction and the preservation of program semantics.","We evaluated RLDebloatDU on ten Linux kernel programs, comparing its performance with two leading debloating techniques: Chisel, known for its aggressive debloating approach, and Razor, recognized for its conservative strategy.","RLDebloatDU significantly lowers the incidence of Common Vulnerabilities and Exposures (CVEs) and improves soundness compared to both, highlighting its efficacy in reducing security issues without reintroducing resolved security issues."],"url":"http://arxiv.org/abs/2402.00276v1"}
{"created":"2024-02-01 01:39:09","title":"Not All Learnable Distribution Classes are Privately Learnable","abstract":"We give an example of a class of distributions that is learnable in total variation distance with a finite number of samples, but not learnable under $(\\varepsilon, \\delta)$-differential privacy. This refutes a conjecture of Ashtiani.","sentences":["We give an example of a class of distributions that is learnable in total variation distance with a finite number of samples, but not learnable under $(\\varepsilon, \\delta)$-differential privacy.","This refutes a conjecture of Ashtiani."],"url":"http://arxiv.org/abs/2402.00267v1"}
{"created":"2024-02-01 00:54:48","title":"Vertical Symbolic Regression via Deep Policy Gradient","abstract":"Vertical Symbolic Regression (VSR) recently has been proposed to expedite the discovery of symbolic equations with many independent variables from experimental data. VSR reduces the search spaces following the vertical discovery path by building from reduced-form equations involving a subset of independent variables to full-fledged ones. Proved successful by many symbolic regressors, deep neural networks are expected to further scale up VSR. Nevertheless, directly combining VSR with deep neural networks will result in difficulty in passing gradients and other engineering issues. We propose Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and demonstrate that VSR-DPG can recover ground-truth equations involving multiple input variables, significantly beyond both deep reinforcement learning-based approaches and previous VSR variants. Our VSR-DPG models symbolic regression as a sequential decision-making process, in which equations are built from repeated applications of grammar rules. The integrated deep model is trained to maximize a policy gradient objective. Experimental results demonstrate that our VSR-DPG significantly outperforms popular baselines in identifying both algebraic equations and ordinary differential equations on a series of benchmarks.","sentences":["Vertical Symbolic Regression (VSR) recently has been proposed to expedite the discovery of symbolic equations with many independent variables from experimental data.","VSR reduces the search spaces following the vertical discovery path by building from reduced-form equations involving a subset of independent variables to full-fledged ones.","Proved successful by many symbolic regressors, deep neural networks are expected to further scale up VSR.","Nevertheless, directly combining VSR with deep neural networks will result in difficulty in passing gradients and other engineering issues.","We propose Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and demonstrate that VSR-DPG can recover ground-truth equations involving multiple input variables, significantly beyond both deep reinforcement learning-based approaches and previous VSR variants.","Our VSR-DPG models symbolic regression as a sequential decision-making process, in which equations are built from repeated applications of grammar rules.","The integrated deep model is trained to maximize a policy gradient objective.","Experimental results demonstrate that our VSR-DPG significantly outperforms popular baselines in identifying both algebraic equations and ordinary differential equations on a series of benchmarks."],"url":"http://arxiv.org/abs/2402.00254v1"}
{"created":"2024-02-01 00:33:21","title":"A Survey on Hallucination in Large Vision-Language Models","abstract":"Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.","sentences":["Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential.","However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs.","In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation.","Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations.","Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs.","Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components.","We also critically review existing methods for mitigating hallucinations.","The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey."],"url":"http://arxiv.org/abs/2402.00253v1"}
{"created":"2024-01-31 23:52:14","title":"Capacity Constraint Analysis Using Object Detection for Smart Manufacturing","abstract":"The increasing popularity of Deep Learning (DL) based Object Detection (OD) methods and their real-world applications have opened new venues in smart manufacturing. Traditional industries struck by capacity constraints after Coronavirus Disease (COVID-19) require non-invasive methods for in-depth operations' analysis to optimize and increase their revenue. In this study, we have initially developed a Convolutional Neural Network (CNN) based OD model to tackle this issue. This model is trained to accurately identify the presence of chairs and individuals on the production floor. The identified objects are then passed to the CNN based tracker, which tracks them throughout their life cycle in the workstation. The extracted meta-data is further processed through a novel framework for the capacity constraint analysis. We identified that the Station C is only 70.6% productive through 6 months. Additionally, the time spent at each station is recorded and aggregated for each object. This data proves helpful in conducting annual audits and effectively managing labor and material over time.","sentences":["The increasing popularity of Deep Learning (DL) based Object Detection (OD) methods and their real-world applications have opened new venues in smart manufacturing.","Traditional industries struck by capacity constraints after Coronavirus Disease (COVID-19) require non-invasive methods for in-depth operations' analysis to optimize and increase their revenue.","In this study, we have initially developed a Convolutional Neural Network (CNN) based OD model to tackle this issue.","This model is trained to accurately identify the presence of chairs and individuals on the production floor.","The identified objects are then passed to the CNN based tracker, which tracks them throughout their life cycle in the workstation.","The extracted meta-data is further processed through a novel framework for the capacity constraint analysis.","We identified that the Station C is only 70.6% productive through 6 months.","Additionally, the time spent at each station is recorded and aggregated for each object.","This data proves helpful in conducting annual audits and effectively managing labor and material over time."],"url":"http://arxiv.org/abs/2402.00243v1"}
{"created":"2024-01-31 23:40:44","title":"CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano Things and Digital Twins","abstract":"Digital twins (DTs) are revolutionizing the biotechnology industry by enabling sophisticated digital representations of biological assets, microorganisms, drug development processes, and digital health applications. However, digital twinning at micro and nano scales, particularly in modeling complex entities like bacteria, presents significant challenges in terms of requiring advanced Internet of Things (IoT) infrastructure and computing approaches to achieve enhanced accuracy and scalability. In this work, we propose a novel framework that integrates the Internet of Bio-Nano Things (IoBNT) with advanced machine learning techniques, specifically convolutional neural networks (CNN) and federated learning (FL), to effectively tackle the identified challenges. Within our framework, IoBNT devices are deployed to gather image-based biological data across various physical environments, leveraging the strong capabilities of CNNs for robust machine vision and pattern recognition. Subsequently, FL is utilized to aggregate insights from these disparate data sources, creating a refined global model that continually enhances accuracy and predictive reliability, which is crucial for the effective deployment of DTs in biotechnology. The primary contribution is the development of a novel framework that synergistically combines CNN and FL, augmented by the capabilities of the IoBNT. This novel approach is specifically tailored to enhancing DTs in the biotechnology industry. The results showcase enhancements in the reliability and safety of microorganism DTs, while preserving their accuracy. Furthermore, the proposed framework excels in energy efficiency and security, offering a user-friendly and adaptable solution. This broadens its applicability across diverse sectors, including biotechnology and pharmaceutical industries, as well as clinical and hospital settings.","sentences":["Digital twins (DTs) are revolutionizing the biotechnology industry by enabling sophisticated digital representations of biological assets, microorganisms, drug development processes, and digital health applications.","However, digital twinning at micro and nano scales, particularly in modeling complex entities like bacteria, presents significant challenges in terms of requiring advanced Internet of Things (IoT) infrastructure and computing approaches to achieve enhanced accuracy and scalability.","In this work, we propose a novel framework that integrates the Internet of Bio-Nano Things (IoBNT) with advanced machine learning techniques, specifically convolutional neural networks (CNN) and federated learning (FL), to effectively tackle the identified challenges.","Within our framework, IoBNT devices are deployed to gather image-based biological data across various physical environments, leveraging the strong capabilities of CNNs for robust machine vision and pattern recognition.","Subsequently, FL is utilized to aggregate insights from these disparate data sources, creating a refined global model that continually enhances accuracy and predictive reliability, which is crucial for the effective deployment of DTs in biotechnology.","The primary contribution is the development of a novel framework that synergistically combines CNN and FL, augmented by the capabilities of the IoBNT.","This novel approach is specifically tailored to enhancing DTs in the biotechnology industry.","The results showcase enhancements in the reliability and safety of microorganism DTs, while preserving their accuracy.","Furthermore, the proposed framework excels in energy efficiency and security, offering a user-friendly and adaptable solution.","This broadens its applicability across diverse sectors, including biotechnology and pharmaceutical industries, as well as clinical and hospital settings."],"url":"http://arxiv.org/abs/2402.00238v1"}
{"created":"2024-01-31 23:32:20","title":"Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary","abstract":"This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding \"time-stamps\" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly \"redundant\". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural oscillations in biological brains.","sentences":["This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks.","Positional encoding \"time-stamps\" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order.","By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly \"redundant\".","Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations.","These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation.","Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural oscillations in biological brains."],"url":"http://arxiv.org/abs/2402.00236v1"}
{"created":"2024-01-31 23:29:42","title":"Exploring the limits of decoder-only models trained on public speech recognition corpora","abstract":"The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.","sentences":["The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines.","Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance.","In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone.","Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets.","We release our codebase and model checkpoints under permissive license."],"url":"http://arxiv.org/abs/2402.00235v1"}
{"created":"2024-01-31 23:24:37","title":"Are Generative AI systems Capable of Supporting Information Needs of Patients?","abstract":"Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurring themes across interactions, including clarifying medical terminology, locating the problems mentioned in the report in the scanned image, understanding disease prognosis, discussing the next diagnostic steps, and comparing treatment options. Based on these themes, we evaluated two state-of-the-art generative visual language models against the radiologist's responses. Our results reveal variability in the quality of responses generated by the models across various themes. We highlight the importance of patient-facing generative AI systems to accommodate a diverse range of conversational themes, catering to the real-world informational needs of patients.","sentences":["Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it.","Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome.","However, this approach is resource intensive and takes expert time away from other critical tasks.","Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data.","We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist.","Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurring themes across interactions, including clarifying medical terminology, locating the problems mentioned in the report in the scanned image, understanding disease prognosis, discussing the next diagnostic steps, and comparing treatment options.","Based on these themes, we evaluated two state-of-the-art generative visual language models against the radiologist's responses.","Our results reveal variability in the quality of responses generated by the models across various themes.","We highlight the importance of patient-facing generative AI systems to accommodate a diverse range of conversational themes, catering to the real-world informational needs of patients."],"url":"http://arxiv.org/abs/2402.00234v1"}
{"created":"2024-01-31 23:06:39","title":"Geometry aware 3D generation from in-the-wild images in ImageNet","abstract":"Generating accurate 3D models is a challenging problem that traditionally requires explicit learning from 3D datasets using supervised learning. Although recent advances have shown promise in learning 3D models from 2D images, these methods often rely on well-structured datasets with multi-view images of each instance or camera pose information. Furthermore, these datasets usually contain clean backgrounds with simple shapes, making them expensive to acquire and hard to generalize, which limits the applicability of these methods. To overcome these limitations, we propose a method for reconstructing 3D geometry from the diverse and unstructured Imagenet dataset without camera pose information. We use an efficient triplane representation to learn 3D models from 2D images and modify the architecture of the generator backbone based on StyleGAN2 to adapt to the highly diverse dataset. To prevent mode collapse and improve the training stability on diverse data, we propose to use multi-view discrimination. The trained generator can produce class-conditional 3D models as well as renderings from arbitrary viewpoints. The class-conditional generation results demonstrate significant improvement over the current state-of-the-art method. Additionally, using PTI, we can efficiently reconstruct the whole 3D geometry from single-view images.","sentences":["Generating accurate 3D models is a challenging problem that traditionally requires explicit learning from 3D datasets using supervised learning.","Although recent advances have shown promise in learning 3D models from 2D images, these methods often rely on well-structured datasets with multi-view images of each instance or camera pose information.","Furthermore, these datasets usually contain clean backgrounds with simple shapes, making them expensive to acquire and hard to generalize, which limits the applicability of these methods.","To overcome these limitations, we propose a method for reconstructing 3D geometry from the diverse and unstructured Imagenet dataset without camera pose information.","We use an efficient triplane representation to learn 3D models from 2D images and modify the architecture of the generator backbone based on StyleGAN2 to adapt to the highly diverse dataset.","To prevent mode collapse and improve the training stability on diverse data, we propose to use multi-view discrimination.","The trained generator can produce class-conditional 3D models as well as renderings from arbitrary viewpoints.","The class-conditional generation results demonstrate significant improvement over the current state-of-the-art method.","Additionally, using PTI, we can efficiently reconstruct the whole 3D geometry from single-view images."],"url":"http://arxiv.org/abs/2402.00225v1"}
{"created":"2024-01-31 22:40:49","title":"FedCore: Straggler-Free Federated Learning with Distributed Coresets","abstract":"Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model while keeping their data on-premise. However, the straggler issue, due to slow clients, often hinders the efficiency and scalability of FL. This paper presents FedCore, an algorithm that innovatively tackles the straggler problem via the decentralized selection of coresets, representative subsets of a dataset. Contrary to existing centralized coreset methods, FedCore creates coresets directly on each client in a distributed manner, ensuring privacy preservation in FL. FedCore translates the coreset optimization problem into a more tractable k-medoids clustering problem and operates distributedly on each client. Theoretical analysis confirms FedCore's convergence, and practical evaluations demonstrate an 8x reduction in FL training time, without compromising model accuracy. Our extensive evaluations also show that FedCore generalizes well to existing FL frameworks.","sentences":["Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model while keeping their data on-premise.","However, the straggler issue, due to slow clients, often hinders the efficiency and scalability of FL.","This paper presents FedCore, an algorithm that innovatively tackles the straggler problem via the decentralized selection of coresets, representative subsets of a dataset.","Contrary to existing centralized coreset methods, FedCore creates coresets directly on each client in a distributed manner, ensuring privacy preservation in FL.","FedCore translates the coreset optimization problem into a more tractable k-medoids clustering problem and operates distributedly on each client.","Theoretical analysis confirms FedCore's convergence, and practical evaluations demonstrate an 8x reduction in FL training time, without compromising model accuracy.","Our extensive evaluations also show that FedCore generalizes well to existing FL frameworks."],"url":"http://arxiv.org/abs/2402.00219v1"}
{"created":"2024-01-31 22:09:40","title":"MP-SL: Multihop Parallel Split Learning","abstract":"Federated Learning (FL) stands out as a widely adopted protocol facilitating the training of Machine Learning (ML) models while maintaining decentralized data. However, challenges arise when dealing with a heterogeneous set of participating devices, causing delays in the training process, particularly among devices with limited resources. Moreover, the task of training ML models with a vast number of parameters demands computing and memory resources beyond the capabilities of small devices, such as mobile and Internet of Things (IoT) devices. To address these issues, techniques like Parallel Split Learning (SL) have been introduced, allowing multiple resource-constrained devices to actively participate in collaborative training processes with assistance from resourceful compute nodes. Nonetheless, a drawback of Parallel SL is the substantial memory allocation required at the compute nodes, for instance training VGG-19 with 100 participants needs 80 GB. In this paper, we introduce Multihop Parallel SL (MP-SL), a modular and extensible ML as a Service (MLaaS) framework designed to facilitate the involvement of resource-constrained devices in collaborative and distributed ML model training. Notably, to alleviate memory demands per compute node, MP-SL supports multihop Parallel SL-based training. This involves splitting the model into multiple parts and utilizing multiple compute nodes in a pipelined manner. Extensive experimentation validates MP-SL's capability to handle system heterogeneity, demonstrating that the multihop configuration proves more efficient than horizontally scaled one-hop Parallel SL setups, especially in scenarios involving more cost-effective compute nodes.","sentences":["Federated Learning (FL) stands out as a widely adopted protocol facilitating the training of Machine Learning (ML) models while maintaining decentralized data.","However, challenges arise when dealing with a heterogeneous set of participating devices, causing delays in the training process, particularly among devices with limited resources.","Moreover, the task of training ML models with a vast number of parameters demands computing and memory resources beyond the capabilities of small devices, such as mobile and Internet of Things (IoT) devices.","To address these issues, techniques like Parallel Split Learning (SL) have been introduced, allowing multiple resource-constrained devices to actively participate in collaborative training processes with assistance from resourceful compute nodes.","Nonetheless, a drawback of Parallel SL is the substantial memory allocation required at the compute nodes, for instance training VGG-19 with 100 participants needs 80 GB.","In this paper, we introduce Multihop Parallel SL (MP-SL), a modular and extensible ML as a Service (MLaaS) framework designed to facilitate the involvement of resource-constrained devices in collaborative and distributed ML model training.","Notably, to alleviate memory demands per compute node, MP-SL supports multihop Parallel SL-based training.","This involves splitting the model into multiple parts and utilizing multiple compute nodes in a pipelined manner.","Extensive experimentation validates MP-SL's capability to handle system heterogeneity, demonstrating that the multihop configuration proves more efficient than horizontally scaled one-hop Parallel SL setups, especially in scenarios involving more cost-effective compute nodes."],"url":"http://arxiv.org/abs/2402.00208v1"}
{"created":"2024-01-31 22:06:10","title":"Decentralised, Collaborative, and Privacy-preserving Machine Learning for Multi-Hospital Data","abstract":"Machine Learning (ML) has demonstrated its great potential on medical data analysis. Large datasets collected from diverse sources and settings are essential for ML models in healthcare to achieve better accuracy and generalizability. Sharing data across different healthcare institutions is challenging because of complex and varying privacy and regulatory requirements. Hence, it is hard but crucial to allow multiple parties to collaboratively train an ML model leveraging the private datasets available at each party without the need for direct sharing of those datasets or compromising the privacy of the datasets through collaboration. In this paper, we address this challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it allows different parties to collaboratively train an ML model without transferring their private datasets; (2) it safeguards patient privacy by limiting the potential privacy leakage arising from any contents shared across the parties during the training process; and (3) it facilitates the ML model training without relying on a centralized server. We demonstrate the generalizability and power of DeCaPH on three distinct tasks using real-world distributed medical datasets: patient mortality prediction using electronic health records, cell-type classification using single-cell human genomes, and pathology identification using chest radiology images. We demonstrate that the ML models trained with DeCaPH framework have an improved utility-privacy trade-off, showing it enables the models to have good performance while preserving the privacy of the training data points. In addition, the ML models trained with DeCaPH framework in general outperform those trained solely with the private datasets from individual parties, showing that DeCaPH enhances the model generalizability.","sentences":["Machine Learning (ML) has demonstrated its great potential on medical data analysis.","Large datasets collected from diverse sources and settings are essential for ML models in healthcare to achieve better accuracy and generalizability.","Sharing data across different healthcare institutions is challenging because of complex and varying privacy and regulatory requirements.","Hence, it is hard but crucial to allow multiple parties to collaboratively train an ML model leveraging the private datasets available at each party without the need for direct sharing of those datasets or compromising the privacy of the datasets through collaboration.","In this paper, we address this challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML for Multi-Hospital Data (DeCaPH).","It offers the following key benefits: (1) it allows different parties to collaboratively train an ML model without transferring their private datasets; (2) it safeguards patient privacy by limiting the potential privacy leakage arising from any contents shared across the parties during the training process; and (3) it facilitates the ML model training without relying on a centralized server.","We demonstrate the generalizability and power of DeCaPH on three distinct tasks using real-world distributed medical datasets: patient mortality prediction using electronic health records, cell-type classification using single-cell human genomes, and pathology identification using chest radiology images.","We demonstrate that the ML models trained with DeCaPH framework have an improved utility-privacy trade-off, showing it enables the models to have good performance while preserving the privacy of the training data points.","In addition, the ML models trained with DeCaPH framework in general outperform those trained solely with the private datasets from individual parties, showing that DeCaPH enhances the model generalizability."],"url":"http://arxiv.org/abs/2402.00205v1"}
{"created":"2024-01-31 21:49:40","title":"Determination of Trace Organic Contaminant Concentration via Machine Classification of Surface-Enhanced Raman Spectra","abstract":"Accurate detection and analysis of traces of persistent organic pollutants in water is important in many areas, including environmental monitoring and food quality control, due to their long environmental stability and potential bioaccumulation. While conventional analysis of organic pollutants requires expensive equipment, surface enhanced Raman spectroscopy (SERS) has demonstrated great potential for accurate detection of these contaminants. However, SERS analytical difficulties, such as spectral preprocessing, denoising, and substrate-based spectral variation, have hindered widespread use of the technique. Here, we demonstrate an approach for predicting the concentration of sample pollutants from messy, unprocessed Raman data using machine learning. Frequency domain transform methods, including the Fourier and Walsh Hadamard transforms, are applied to sets of Raman spectra of three model micropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are then used to train machine learning algorithms. Using standard machine learning models, the concentration of sample pollutants are predicted with more than 80 percent cross-validation accuracy from raw Raman data. cross-validation accuracy of 85 percent was achieved using deep learning for a moderately sized dataset (100 spectra), and 70 to 80 percent cross-validation accuracy was achieved even for very small datasets (50 spectra). Additionally, standard models were shown to accurately identify characteristic peaks via analysis of their importance scores. The approach shown here has the potential to be applied to facilitate accurate detection and analysis of persistent organic pollutants by surface-enhanced Raman spectroscopy.","sentences":["Accurate detection and analysis of traces of persistent organic pollutants in water is important in many areas, including environmental monitoring and food quality control, due to their long environmental stability and potential bioaccumulation.","While conventional analysis of organic pollutants requires expensive equipment, surface enhanced Raman spectroscopy (SERS) has demonstrated great potential for accurate detection of these contaminants.","However, SERS analytical difficulties, such as spectral preprocessing, denoising, and substrate-based spectral variation, have hindered widespread use of the technique.","Here, we demonstrate an approach for predicting the concentration of sample pollutants from messy, unprocessed Raman data using machine learning.","Frequency domain transform methods, including the Fourier and Walsh Hadamard transforms, are applied to sets of Raman spectra of three model micropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are then used to train machine learning algorithms.","Using standard machine learning models, the concentration of sample pollutants are predicted with more than 80 percent cross-validation accuracy from raw Raman data.","cross-validation accuracy of 85 percent was achieved using deep learning for a moderately sized dataset (100 spectra), and 70 to 80 percent cross-validation accuracy was achieved even for very small datasets (50 spectra).","Additionally, standard models were shown to accurately identify characteristic peaks via analysis of their importance scores.","The approach shown here has the potential to be applied to facilitate accurate detection and analysis of persistent organic pollutants by surface-enhanced Raman spectroscopy."],"url":"http://arxiv.org/abs/2402.00197v1"}
{"created":"2024-01-31 21:48:25","title":"Dataset Condensation Driven Machine Unlearning","abstract":"The current trend in data regulation requirements and privacy-preserving machine learning has emphasized the importance of machine unlearning. The naive approach to unlearning training data by retraining over the complement of the forget samples is susceptible to computational challenges. These challenges have been effectively addressed through a collection of techniques falling under the umbrella of machine unlearning. However, there still exists a lack of sufficiency in handling persistent computational challenges in harmony with the utility and privacy of unlearned model. We attribute this to the lack of work on improving the computational complexity of approximate unlearning from the perspective of the training dataset. In this paper, we aim to fill this gap by introducing dataset condensation as an essential component of machine unlearning in the context of image classification. To achieve this goal, we propose new dataset condensation techniques and an innovative unlearning scheme that strikes a balance between machine unlearning privacy, utility, and efficiency. Furthermore, we present a novel and effective approach to instrumenting machine unlearning and propose its application in defending against membership inference and model inversion attacks. Additionally, we explore a new application of our approach, which involves removing data from `condensed model', which can be employed to quickly train any arbitrary model without being influenced by unlearning samples.","sentences":["The current trend in data regulation requirements and privacy-preserving machine learning has emphasized the importance of machine unlearning.","The naive approach to unlearning training data by retraining over the complement of the forget samples is susceptible to computational challenges.","These challenges have been effectively addressed through a collection of techniques falling under the umbrella of machine unlearning.","However, there still exists a lack of sufficiency in handling persistent computational challenges in harmony with the utility and privacy of unlearned model.","We attribute this to the lack of work on improving the computational complexity of approximate unlearning from the perspective of the training dataset.","In this paper, we aim to fill this gap by introducing dataset condensation as an essential component of machine unlearning in the context of image classification.","To achieve this goal, we propose new dataset condensation techniques and an innovative unlearning scheme that strikes a balance between machine unlearning privacy, utility, and efficiency.","Furthermore, we present a novel and effective approach to instrumenting machine unlearning and propose its application in defending against membership inference and model inversion attacks.","Additionally, we explore a new application of our approach, which involves removing data from `condensed model', which can be employed to quickly train any arbitrary model without being influenced by unlearning samples."],"url":"http://arxiv.org/abs/2402.00195v1"}
{"created":"2024-01-31 21:28:40","title":"Distance and Collision Probability Estimation from Gaussian Surface Models","abstract":"This paper describes continuous-space methodologies to estimate the collision probability, Euclidean distance and gradient between an ellipsoidal robot model and an environment surface modeled as a set of Gaussian distributions. Continuous-space collision probability estimation is critical for uncertainty-aware motion planning. Most collision detection and avoidance approaches assume the robot is modeled as a sphere, but ellipsoidal representations provide tighter approximations and enable navigation in cluttered and narrow spaces. State-of-the-art methods derive the Euclidean distance and gradient by processing raw point clouds, which is computationally expensive for large workspaces. Recent advances in Gaussian surface modeling (e.g. mixture models, splatting) enable compressed and high-fidelity surface representations. Few methods exist to estimate continuous-space occupancy from such models. They require Gaussians to model free space and are unable to estimate the collision probability, Euclidean distance and gradient for an ellipsoidal robot. The proposed methods bridge this gap by extending prior work in ellipsoid-to-ellipsoid Euclidean distance and collision probability estimation to Gaussian surface models. A geometric blending approach is also proposed to improve collision probability estimation. The approaches are evaluated with numerical 2D and 3D experiments using real-world point cloud data.","sentences":["This paper describes continuous-space methodologies to estimate the collision probability, Euclidean distance and gradient between an ellipsoidal robot model and an environment surface modeled as a set of Gaussian distributions.","Continuous-space collision probability estimation is critical for uncertainty-aware motion planning.","Most collision detection and avoidance approaches assume the robot is modeled as a sphere, but ellipsoidal representations provide tighter approximations and enable navigation in cluttered and narrow spaces.","State-of-the-art methods derive the Euclidean distance and gradient by processing raw point clouds, which is computationally expensive for large workspaces.","Recent advances in Gaussian surface modeling (e.g. mixture models, splatting) enable compressed and high-fidelity surface representations.","Few methods exist to estimate continuous-space occupancy from such models.","They require Gaussians to model free space and are unable to estimate the collision probability, Euclidean distance and gradient for an ellipsoidal robot.","The proposed methods bridge this gap by extending prior work in ellipsoid-to-ellipsoid Euclidean distance and collision probability estimation to Gaussian surface models.","A geometric blending approach is also proposed to improve collision probability estimation.","The approaches are evaluated with numerical 2D and 3D experiments using real-world point cloud data."],"url":"http://arxiv.org/abs/2402.00186v1"}
{"created":"2024-01-31 21:14:01","title":"De-identification is not always enough","abstract":"For sharing privacy-sensitive data, de-identification is commonly regarded as adequate for safeguarding privacy. Synthetic data is also being considered as a privacy-preserving alternative. Recent successes with numerical and tabular data generative models and the breakthroughs in large generative language models raise the question of whether synthetically generated clinical notes could be a viable alternative to real notes for research purposes. In this work, we demonstrated that (i) de-identification of real clinical notes does not protect records against a membership inference attack, (ii) proposed a novel approach to generate synthetic clinical notes using the current state-of-the-art large language models, (iii) evaluated the performance of the synthetically generated notes in a clinical domain task, and (iv) proposed a way to mount a membership inference attack where the target model is trained with synthetic data. We observed that when synthetically generated notes closely match the performance of real data, they also exhibit similar privacy concerns to the real data. Whether other approaches to synthetically generated clinical notes could offer better trade-offs and become a better alternative to sensitive real notes warrants further investigation.","sentences":["For sharing privacy-sensitive data, de-identification is commonly regarded as adequate for safeguarding privacy.","Synthetic data is also being considered as a privacy-preserving alternative.","Recent successes with numerical and tabular data generative models and the breakthroughs in large generative language models raise the question of whether synthetically generated clinical notes could be a viable alternative to real notes for research purposes.","In this work, we demonstrated that (i) de-identification of real clinical notes does not protect records against a membership inference attack, (ii) proposed a novel approach to generate synthetic clinical notes using the current state-of-the-art large language models, (iii) evaluated the performance of the synthetically generated notes in a clinical domain task, and (iv) proposed a way to mount a membership inference attack where the target model is trained with synthetic data.","We observed that when synthetically generated notes closely match the performance of real data, they also exhibit similar privacy concerns to the real data.","Whether other approaches to synthetically generated clinical notes could offer better trade-offs and become a better alternative to sensitive real notes warrants further investigation."],"url":"http://arxiv.org/abs/2402.00179v1"}
{"created":"2024-01-31 20:31:56","title":"Multimodal Clinical Pseudo-notes for Emergency Department Prediction Tasks using Multiple Embedding Model for EHR (MEME)","abstract":"In this work, we introduce Multiple Embedding Model for EHR (MEME), an approach that views Electronic Health Records (EHR) as multimodal data. This approach incorporates \"pseudo-notes\", textual representations of tabular EHR concepts such as diagnoses and medications, and allows us to effectively employ Large Language Models (LLMs) for EHR representation. This framework also adopts a multimodal approach, embedding each EHR modality separately. We demonstrate the effectiveness of MEME by applying it to several tasks within the Emergency Department across multiple hospital systems. Our findings show that MEME surpasses the performance of both single modality embedding methods and traditional machine learning approaches. However, we also observe notable limitations in generalizability across hospital institutions for all tested models.","sentences":["In this work, we introduce Multiple Embedding Model for EHR (MEME), an approach that views Electronic Health Records (EHR) as multimodal data.","This approach incorporates \"pseudo-notes\", textual representations of tabular EHR concepts such as diagnoses and medications, and allows us to effectively employ Large Language Models (LLMs) for EHR representation.","This framework also adopts a multimodal approach, embedding each EHR modality separately.","We demonstrate the effectiveness of MEME by applying it to several tasks within the Emergency Department across multiple hospital systems.","Our findings show that MEME surpasses the performance of both single modality embedding methods and traditional machine learning approaches.","However, we also observe notable limitations in generalizability across hospital institutions for all tested models."],"url":"http://arxiv.org/abs/2402.00160v1"}
{"created":"2024-01-31 20:29:50","title":"Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","abstract":"Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.","sentences":["Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported.","In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them.","As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations.","To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials.","In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work.","In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents.","We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing.","Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling."],"url":"http://arxiv.org/abs/2402.00159v1"}
