{"created":"2025-04-03 17:59:56","title":"Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing","abstract":"Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.","sentences":["Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats.","To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE).","RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning.","We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach.","Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored.","As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research.","Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems.","Our code and data will be released at https://github.com/PhoenixZ810/RISEBench."],"url":"http://arxiv.org/abs/2504.02826v1"}
{"created":"2025-04-03 17:59:12","title":"STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection","abstract":"Advancements in Computer-Aided Screening (CAS) systems are essential for improving the detection of security threats in X-ray baggage scans. However, current datasets are limited in representing real-world, sophisticated threats and concealment tactics, and existing approaches are constrained by a closed-set paradigm with predefined labels. To address these challenges, we introduce STCray, the first multimodal X-ray baggage security dataset, comprising 46,642 image-caption paired scans across 21 threat categories, generated using an X-ray scanner for airport security. STCray is meticulously developed with our specialized protocol that ensures domain-aware, coherent captions, that lead to the multi-modal instruction following data in X-ray baggage security. This allows us to train a domain-aware visual AI assistant named STING-BEE that supports a range of vision-language tasks, including scene comprehension, referring threat localization, visual grounding, and visual question answering (VQA), establishing novel baselines for multi-modal learning in X-ray baggage security. Further, STING-BEE shows state-of-the-art generalization in cross-domain settings. Code, data, and models are available at https://divs1159.github.io/STING-BEE/.","sentences":["Advancements in Computer-Aided Screening (CAS) systems are essential for improving the detection of security threats in X-ray baggage scans.","However, current datasets are limited in representing real-world, sophisticated threats and concealment tactics, and existing approaches are constrained by a closed-set paradigm with predefined labels.","To address these challenges, we introduce STCray, the first multimodal X-ray baggage security dataset, comprising 46,642 image-caption paired scans across 21 threat categories, generated using an X-ray scanner for airport security.","STCray is meticulously developed with our specialized protocol that ensures domain-aware, coherent captions, that lead to the multi-modal instruction following data in X-ray baggage security.","This allows us to train a domain-aware visual AI assistant named STING-BEE that supports a range of vision-language tasks, including scene comprehension, referring threat localization, visual grounding, and visual question answering (VQA), establishing novel baselines for multi-modal learning in X-ray baggage security.","Further, STING-BEE shows state-of-the-art generalization in cross-domain settings.","Code, data, and models are available at https://divs1159.github.io/STING-BEE/."],"url":"http://arxiv.org/abs/2504.02823v1"}
{"created":"2025-04-03 17:58:44","title":"Do Two AI Scientists Agree?","abstract":"When two AI models are trained on the same scientific task, do they learn the same theory or two different theories? Throughout history of science, we have witnessed the rise and fall of theories driven by experimental validation or falsification: many theories may co-exist when experimental data is lacking, but the space of survived theories become more constrained with more experimental data becoming available. We show the same story is true for AI scientists. With increasingly more systems provided in training data, AI scientists tend to converge in the theories they learned, although sometimes they form distinct groups corresponding to different theories. To mechanistically interpret what theories AI scientists learn and quantify their agreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI Scientists, trained on standard problems in physics, aggregating training results across many seeds simulating the different configurations of AI scientists. Our findings suggests for AI scientists switch from learning a Hamiltonian theory in simple setups to a Lagrangian formulation when more complex systems are introduced. We also observe strong seed dependence of the training dynamics and final learned weights, controlling the rise and fall of relevant theories. We finally demonstrate that not only can our neural networks aid interpretability, it can also be applied to higher dimensional problems.","sentences":["When two AI models are trained on the same scientific task, do they learn the same theory or two different theories?","Throughout history of science, we have witnessed the rise and fall of theories driven by experimental validation or falsification: many theories may co-exist when experimental data is lacking, but the space of survived theories become more constrained with more experimental data becoming available.","We show the same story is true for AI scientists.","With increasingly more systems provided in training data, AI scientists tend to converge in the theories they learned, although sometimes they form distinct groups corresponding to different theories.","To mechanistically interpret what theories AI scientists learn and quantify their agreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI Scientists, trained on standard problems in physics, aggregating training results across many seeds simulating the different configurations of AI scientists.","Our findings suggests for AI scientists switch from learning a Hamiltonian theory in simple setups to a Lagrangian formulation when more complex systems are introduced.","We also observe strong seed dependence of the training dynamics and final learned weights, controlling the rise and fall of relevant theories.","We finally demonstrate that not only can our neural networks aid interpretability, it can also be applied to higher dimensional problems."],"url":"http://arxiv.org/abs/2504.02822v1"}
{"created":"2025-04-03 17:58:18","title":"GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings","abstract":"Symmetry, where certain features remain invariant under geometric transformations, can often serve as a powerful prior in designing convolutional neural networks (CNNs). While conventional CNNs inherently support translational equivariance, extending this property to rotation and reflection has proven challenging, often forcing a compromise between equivariance, efficiency, and information loss. In this work, we introduce Gaussian Mixture Ring Convolution (GMR-Conv), an efficient convolution kernel that smooths radial symmetry using a mixture of Gaussian-weighted rings. This design mitigates discretization errors of circular kernels, thereby preserving robust rotation and reflection equivariance without incurring computational overhead. We further optimize both the space and speed efficiency of GMR-Conv via a novel parameterization and computation strategy, allowing larger kernels at an acceptable cost. Extensive experiments on eight classification and one segmentation datasets demonstrate that GMR-Conv not only matches conventional CNNs' performance but can also surpass it in applications with orientation-less data. GMR-Conv is also proven to be more robust and efficient than the state-of-the-art equivariant learning methods. Our work provides inspiring empirical evidence that carefully applied radial symmetry can alleviate the challenges of information loss, marking a promising advance in equivariant network architectures. The code is available at https://github.com/XYPB/GMR-Conv.","sentences":["Symmetry, where certain features remain invariant under geometric transformations, can often serve as a powerful prior in designing convolutional neural networks (CNNs).","While conventional CNNs inherently support translational equivariance, extending this property to rotation and reflection has proven challenging, often forcing a compromise between equivariance, efficiency, and information loss.","In this work, we introduce Gaussian Mixture Ring Convolution (GMR-Conv), an efficient convolution kernel that smooths radial symmetry using a mixture of Gaussian-weighted rings.","This design mitigates discretization errors of circular kernels, thereby preserving robust rotation and reflection equivariance without incurring computational overhead.","We further optimize both the space and speed efficiency of GMR-Conv via a novel parameterization and computation strategy, allowing larger kernels at an acceptable cost.","Extensive experiments on eight classification and one segmentation datasets demonstrate that GMR-Conv not only matches conventional CNNs' performance but can also surpass it in applications with orientation-less data.","GMR-Conv is also proven to be more robust and efficient than the state-of-the-art equivariant learning methods.","Our work provides inspiring empirical evidence that carefully applied radial symmetry can alleviate the challenges of information loss, marking a promising advance in equivariant network architectures.","The code is available at https://github.com/XYPB/GMR-Conv."],"url":"http://arxiv.org/abs/2504.02819v1"}
{"created":"2025-04-03 17:57:52","title":"Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization","abstract":"Many 3D generative models rely on variational autoencoders (VAEs) to learn compact shape representations. However, existing methods encode all shapes into a fixed-size token, disregarding the inherent variations in scale and complexity across 3D data. This leads to inefficient latent representations that can compromise downstream generation. We address this challenge by introducing Octree-based Adaptive Tokenization, a novel framework that adjusts the dimension of latent representations according to shape complexity. Our approach constructs an adaptive octree structure guided by a quadric-error-based subdivision criterion and allocates a shape latent vector to each octree cell using a query-based transformer. Building upon this tokenization, we develop an octree-based autoregressive generative model that effectively leverages these variable-sized representations in shape generation. Extensive experiments demonstrate that our approach reduces token counts by 50% compared to fixed-size methods while maintaining comparable visual quality. When using a similar token length, our method produces significantly higher-quality shapes. When incorporated with our downstream generative model, our method creates more detailed and diverse 3D content than existing approaches.","sentences":["Many 3D generative models rely on variational autoencoders (VAEs) to learn compact shape representations.","However, existing methods encode all shapes into a fixed-size token, disregarding the inherent variations in scale and complexity across 3D data.","This leads to inefficient latent representations that can compromise downstream generation.","We address this challenge by introducing Octree-based Adaptive Tokenization, a novel framework that adjusts the dimension of latent representations according to shape complexity.","Our approach constructs an adaptive octree structure guided by a quadric-error-based subdivision criterion and allocates a shape latent vector to each octree cell using a query-based transformer.","Building upon this tokenization, we develop an octree-based autoregressive generative model that effectively leverages these variable-sized representations in shape generation.","Extensive experiments demonstrate that our approach reduces token counts by 50% compared to fixed-size methods while maintaining comparable visual quality.","When using a similar token length, our method produces significantly higher-quality shapes.","When incorporated with our downstream generative model, our method creates more detailed and diverse 3D content than existing approaches."],"url":"http://arxiv.org/abs/2504.02817v1"}
{"created":"2025-04-03 17:52:07","title":"MegaMath: Pushing the Limits of Open Math Corpora","abstract":"Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.","sentences":["Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs).","However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training.","We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet.","(2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity.","(3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data.","By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets."],"url":"http://arxiv.org/abs/2504.02807v1"}
{"created":"2025-04-03 17:47:06","title":"F-ViTA: Foundation Model Guided Visible to Thermal Translation","abstract":"Thermal imaging is crucial for scene understanding, particularly in low-light and nighttime conditions. However, collecting large thermal datasets is costly and labor-intensive due to the specialized equipment required for infrared image capture. To address this challenge, researchers have explored visible-to-thermal image translation. Most existing methods rely on Generative Adversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a style transfer problem. As a result, these approaches attempt to learn both the modality distribution shift and underlying physical principles from limited training data. In this paper, we propose F-ViTA, a novel approach that leverages the general world knowledge embedded in foundation models to guide the diffusion process for improved translation. Specifically, we condition an InstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation models such as SAM and Grounded DINO. This allows the model to learn meaningful correlations between scene objects and their thermal signatures in infrared imagery. Extensive experiments on five public datasets demonstrate that F-ViTA outperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes well to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared (LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the same visible image. Code: https://github.com/JayParanjape/F-ViTA/tree/master.","sentences":["Thermal imaging is crucial for scene understanding, particularly in low-light and nighttime conditions.","However, collecting large thermal datasets is costly and labor-intensive due to the specialized equipment required for infrared image capture.","To address this challenge, researchers have explored visible-to-thermal image translation.","Most existing methods rely on Generative Adversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a style transfer problem.","As a result, these approaches attempt to learn both the modality distribution shift and underlying physical principles from limited training data.","In this paper, we propose F-ViTA, a novel approach that leverages the general world knowledge embedded in foundation models to guide the diffusion process for improved translation.","Specifically, we condition an InstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation models such as SAM and Grounded DINO.","This allows the model to learn meaningful correlations between scene objects and their thermal signatures in infrared imagery.","Extensive experiments on five public datasets demonstrate that F-ViTA outperforms state-of-the-art (SOTA) methods.","Furthermore, our model generalizes well to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared (LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the same visible image.","Code: https://github.com/JayParanjape/F-ViTA/tree/master."],"url":"http://arxiv.org/abs/2504.02801v1"}
{"created":"2025-04-03 17:43:14","title":"A Survey of Large Language Models in Mental Health Disorder Detection on Social Media","abstract":"The detection and intervention of mental health issues represent a critical global research focus, and social media data has been recognized as an important resource for mental health research. However, how to utilize Large Language Models (LLMs) for mental health problem detection on social media poses significant challenges. Hence, this paper aims to explore the potential of LLM applications in social media data analysis, focusing not only on the most common psychological disorders such as depression and anxiety but also incorporating psychotic disorders and externalizing disorders, summarizing the application methods of LLM from different dimensions, such as text data analysis and detection of mental disorders, and revealing the major challenges and shortcomings of current research. In addition, the paper provides an overview of popular datasets, and evaluation metrics. The survey in this paper provides a comprehensive frame of reference for researchers in the field of mental health, while demonstrating the great potential of LLMs in mental health detection to facilitate the further application of LLMs in future mental health interventions.","sentences":["The detection and intervention of mental health issues represent a critical global research focus, and social media data has been recognized as an important resource for mental health research.","However, how to utilize Large Language Models (LLMs) for mental health problem detection on social media poses significant challenges.","Hence, this paper aims to explore the potential of LLM applications in social media data analysis, focusing not only on the most common psychological disorders such as depression and anxiety but also incorporating psychotic disorders and externalizing disorders, summarizing the application methods of LLM from different dimensions, such as text data analysis and detection of mental disorders, and revealing the major challenges and shortcomings of current research.","In addition, the paper provides an overview of popular datasets, and evaluation metrics.","The survey in this paper provides a comprehensive frame of reference for researchers in the field of mental health, while demonstrating the great potential of LLMs in mental health detection to facilitate the further application of LLMs in future mental health interventions."],"url":"http://arxiv.org/abs/2504.02800v1"}
{"created":"2025-04-03 17:42:56","title":"Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence","abstract":"Large Vision-Language Models offer a new paradigm for AI-driven image understanding, enabling models to perform tasks without task-specific training. This flexibility holds particular promise across medicine, where expert-annotated data is scarce. Yet, VLMs' practical utility in intervention-focused domains--especially surgery, where decision-making is subjective and clinical scenarios are variable--remains uncertain. Here, we present a comprehensive analysis of 11 state-of-the-art VLMs across 17 key visual understanding tasks in surgical AI--from anatomy recognition to skill assessment--using 13 datasets spanning laparoscopic, robotic, and open procedures. In our experiments, VLMs demonstrate promising generalizability, at times outperforming supervised models when deployed outside their training setting. In-context learning, incorporating examples during testing, boosted performance up to three-fold, suggesting adaptability as a key strength. Still, tasks requiring spatial or temporal reasoning remained difficult. Beyond surgery, our findings offer insights into VLMs' potential for tackling complex and dynamic scenarios in clinical and broader real-world applications.","sentences":["Large Vision-Language Models offer a new paradigm for AI-driven image understanding, enabling models to perform tasks without task-specific training.","This flexibility holds particular promise across medicine, where expert-annotated data is scarce.","Yet, VLMs' practical utility in intervention-focused domains--especially surgery, where decision-making is subjective and clinical scenarios are variable--remains uncertain.","Here, we present a comprehensive analysis of 11 state-of-the-art VLMs across 17 key visual understanding tasks in surgical AI--from anatomy recognition to skill assessment--using 13 datasets spanning laparoscopic, robotic, and open procedures.","In our experiments, VLMs demonstrate promising generalizability, at times outperforming supervised models when deployed outside their training setting.","In-context learning, incorporating examples during testing, boosted performance up to three-fold, suggesting adaptability as a key strength.","Still, tasks requiring spatial or temporal reasoning remained difficult.","Beyond surgery, our findings offer insights into VLMs' potential for tackling complex and dynamic scenarios in clinical and broader real-world applications."],"url":"http://arxiv.org/abs/2504.02799v1"}
{"created":"2025-04-03 17:40:11","title":"A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models","abstract":"Large artificial intelligence (AI) models have garnered significant attention for their remarkable, often \"superhuman\", performance on standardized benchmarks. However, when these models are deployed in high-stakes verticals such as healthcare, education, and law, they often reveal notable limitations. For instance, they exhibit brittleness to minor variations in input data, present contextually uninformed decisions in critical settings, and undermine user trust by confidently producing or reproducing inaccuracies. These challenges in applying large models necessitate cross-disciplinary innovations to align the models' capabilities with the needs of real-world applications. We introduce a framework that addresses this gap through a layer-wise abstraction of innovations aimed at meeting users' requirements with large models. Through multiple case studies, we illustrate how researchers and practitioners across various fields can operationalize this framework. Beyond modularizing the pipeline of transforming large models into useful \"vertical systems\", we also highlight the dynamism that exists within different layers of the framework. Finally, we discuss how our framework can guide researchers and practitioners to (i) optimally situate their innovations (e.g., when vertical-specific insights can empower broadly impactful vertical-agnostic innovations), (ii) uncover overlooked opportunities (e.g., spotting recurring problems across verticals to develop practically useful foundation models instead of chasing benchmarks), and (iii) facilitate cross-disciplinary communication of critical challenges (e.g., enabling a shared vocabulary for AI developers, domain experts, and human-computer interaction scholars).","sentences":["Large artificial intelligence (AI) models have garnered significant attention for their remarkable, often \"superhuman\", performance on standardized benchmarks.","However, when these models are deployed in high-stakes verticals such as healthcare, education, and law, they often reveal notable limitations.","For instance, they exhibit brittleness to minor variations in input data, present contextually uninformed decisions in critical settings, and undermine user trust by confidently producing or reproducing inaccuracies.","These challenges in applying large models necessitate cross-disciplinary innovations to align the models' capabilities with the needs of real-world applications.","We introduce a framework that addresses this gap through a layer-wise abstraction of innovations aimed at meeting users' requirements with large models.","Through multiple case studies, we illustrate how researchers and practitioners across various fields can operationalize this framework.","Beyond modularizing the pipeline of transforming large models into useful \"vertical systems\", we also highlight the dynamism that exists within different layers of the framework.","Finally, we discuss how our framework can guide researchers and practitioners to (i) optimally situate their innovations (e.g., when vertical-specific insights can empower broadly impactful vertical-agnostic innovations), (ii) uncover overlooked opportunities (e.g., spotting recurring problems across verticals to develop practically useful foundation models instead of chasing benchmarks), and (iii) facilitate cross-disciplinary communication of critical challenges (e.g., enabling a shared vocabulary for AI developers, domain experts, and human-computer interaction scholars)."],"url":"http://arxiv.org/abs/2504.02793v1"}
{"created":"2025-04-03 17:38:59","title":"Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets","abstract":"Imitation learning has emerged as a promising approach towards building generalist robots. However, scaling imitation learning for large robot foundation models remains challenging due to its reliance on high-quality expert demonstrations. Meanwhile, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data directly for imitation learning, however, has proven difficult due to the lack of action annotation required for most contemporary methods. In this work, we present Unified World Models (UWM), a framework that allows for leveraging both video and action data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. We show that by simply controlling each diffusion timestep, UWM can flexibly represent a policy, a forward dynamics, an inverse dynamics, and a video generator. Through simulated and real-world experiments, we show that: (1) UWM enables effective pretraining on large-scale multitask robot datasets with both dynamics and action predictions, resulting in more generalizable and robust policies than imitation learning, (2) UWM naturally facilitates learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWM offers a promising step toward harnessing large, heterogeneous datasets for scalable robot learning, and provides a simple unification between the often disparate paradigms of imitation learning and world modeling. Videos and code are available at https://weirdlabuw.github.io/uwm/.","sentences":["Imitation learning has emerged as a promising approach towards building generalist robots.","However, scaling imitation learning for large robot foundation models remains challenging due to its reliance on high-quality expert demonstrations.","Meanwhile, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available.","This data provides a rich source of information about real-world dynamics and agent-environment interactions.","Leveraging this data directly for imitation learning, however, has proven difficult due to the lack of action annotation required for most contemporary methods.","In this work, we present Unified World Models (UWM), a framework that allows for leveraging both video and action data for policy learning.","Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality.","We show that by simply controlling each diffusion timestep, UWM can flexibly represent a policy, a forward dynamics, an inverse dynamics, and a video generator.","Through simulated and real-world experiments, we show that: (1) UWM enables effective pretraining on large-scale multitask robot datasets with both dynamics and action predictions, resulting in more generalizable and robust policies than imitation learning, (2) UWM naturally facilitates learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies.","Our results suggest that UWM offers a promising step toward harnessing large, heterogeneous datasets for scalable robot learning, and provides a simple unification between the often disparate paradigms of imitation learning and world modeling.","Videos and code are available at https://weirdlabuw.github.io/uwm/."],"url":"http://arxiv.org/abs/2504.02792v1"}
{"created":"2025-04-03 17:37:46","title":"Dynamic Treewidth in Logarithmic Time","abstract":"We present a dynamic data structure that maintains a tree decomposition of width at most $9k+8$ of a dynamic graph with treewidth at most $k$, which is updated by edge insertions and deletions. The amortized update time of our data structure is $2^{O(k)} \\log n$, where $n$ is the number of vertices. The data structure also supports maintaining any ``dynamic programming scheme'' on the tree decomposition, providing, for example, a dynamic version of Courcelle's theorem with $O_{k}(\\log n)$ amortized update time; the $O_{k}(\\cdot)$ notation hides factors that depend on $k$. This improves upon a result of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\\l}owski [FOCS 2023], who gave a similar data structure but with amortized update time $2^{k^{O(1)}} n^{o(1)}$. Furthermore, our data structure is arguably simpler.   Our main novel idea is to maintain a tree decomposition that is ``downwards well-linked'', which allows us to implement local rotations and analysis similar to those for splay trees.","sentences":["We present a dynamic data structure that maintains a tree decomposition of width at most $9k+8$ of a dynamic graph with treewidth at most $k$, which is updated by edge insertions and deletions.","The amortized update time of our data structure is $2^{O(k)} \\log n$, where $n$ is the number of vertices.","The data structure also supports maintaining any ``dynamic programming scheme'' on the tree decomposition, providing, for example, a dynamic version of Courcelle's theorem with $O_{k}(\\log n)$ amortized update time; the $O_{k}(\\cdot)$ notation hides factors that depend on $k$. This improves upon a result of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\\l}owski","[FOCS 2023], who gave a similar data structure but with amortized update time $2^{k^{O(1)}} n^{o(1)}$.","Furthermore, our data structure is arguably simpler.   ","Our main novel idea is to maintain a tree decomposition that is ``downwards well-linked'', which allows us to implement local rotations and analysis similar to those for splay trees."],"url":"http://arxiv.org/abs/2504.02790v1"}
{"created":"2025-04-03 17:23:16","title":"GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation","abstract":"The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.","sentences":["The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community.","This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis.","Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities.","Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures.","We also provide a complete speculation on GPT-4o's overall architecture.","In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation.","We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models.","We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond.","The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval."],"url":"http://arxiv.org/abs/2504.02782v1"}
{"created":"2025-04-03 17:19:20","title":"Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition","abstract":"Human activity recognition is increasingly vital for supporting independent living, particularly for the elderly and those in need of assistance. Domestic service robots with monitoring capabilities can enhance safety and provide essential support. Although image-based methods have advanced considerably in the past decade, their adoption remains limited by concerns over privacy and sensitivity to low-light or dark conditions. As an alternative, millimetre-wave (mmWave) radar can produce point cloud data which is privacy-preserving. However, processing the sparse and noisy point clouds remains a long-standing challenge. While graph-based methods and attention mechanisms show promise, they predominantly rely on \"fixed\" kernels; kernels that are applied uniformly across all neighbourhoods, highlighting the need for adaptive approaches that can dynamically adjust their kernels to the specific geometry of each local neighbourhood in point cloud data. To overcome this limitation, we introduce an adaptive approach within the graph convolutional framework. Instead of a single shared weight function, our Multi-Head Adaptive Kernel (MAK) module generates multiple dynamic kernels, each capturing different aspects of the local feature space. By progressively refining local features while maintaining global spatial context, our method enables convolution kernels to adapt to varying local features. Experimental results on benchmark datasets confirm the effectiveness of our approach, achieving state-of-the-art performance in human activity recognition. Our source code is made publicly available at: https://github.com/Gbouna/MAK-GCN","sentences":["Human activity recognition is increasingly vital for supporting independent living, particularly for the elderly and those in need of assistance.","Domestic service robots with monitoring capabilities can enhance safety and provide essential support.","Although image-based methods have advanced considerably in the past decade, their adoption remains limited by concerns over privacy and sensitivity to low-light or dark conditions.","As an alternative, millimetre-wave (mmWave) radar can produce point cloud data which is privacy-preserving.","However, processing the sparse and noisy point clouds remains a long-standing challenge.","While graph-based methods and attention mechanisms show promise, they predominantly rely on \"fixed\" kernels; kernels that are applied uniformly across all neighbourhoods, highlighting the need for adaptive approaches that can dynamically adjust their kernels to the specific geometry of each local neighbourhood in point cloud data.","To overcome this limitation, we introduce an adaptive approach within the graph convolutional framework.","Instead of a single shared weight function, our Multi-Head Adaptive Kernel (MAK) module generates multiple dynamic kernels, each capturing different aspects of the local feature space.","By progressively refining local features while maintaining global spatial context, our method enables convolution kernels to adapt to varying local features.","Experimental results on benchmark datasets confirm the effectiveness of our approach, achieving state-of-the-art performance in human activity recognition.","Our source code is made publicly available at: https://github.com/Gbouna/MAK-GCN"],"url":"http://arxiv.org/abs/2504.02778v1"}
{"created":"2025-04-03 17:06:39","title":"Efficient Algorithms for Cardinality Estimation and Conjunctive Query Evaluation With Simple Degree Constraints","abstract":"Cardinality estimation and conjunctive query evaluation are two of the most fundamental problems in database query processing. Recent work proposed, studied, and implemented a robust and practical information-theoretic cardinality estimation framework. In this framework, the estimator is the cardinality upper bound of a conjunctive query subject to ``degree-constraints'', which model a rich set of input data statistics. For general degree constraints, computing this bound is computationally hard. Researchers have naturally sought efficiently computable relaxed upper bounds that are as tight as possible. The polymatroid bound is the tightest among those relaxed upper bounds. While it is an open question whether the polymatroid bound can be computed in polynomial-time in general, it is known to be computable in polynomial-time for some classes of degree constraints.   Our focus is on a common class of degree constraints called simple degree constraints. Researchers had not previously determined how to compute the polymatroid bound in polynomial time for this class of constraints. Our first main result is a polynomial time algorithm to compute the polymatroid bound given simple degree constraints. Our second main result is a polynomial-time algorithm to compute a ``proof sequence'' establishing this bound. This proof sequence can then be incorporated in the PANDA-framework to give a faster algorithm to evaluate a conjunctive query. In addition, we show computational limitations to extending our results to broader classes of degree constraints. Finally, our technique leads naturally to a new relaxed upper bound called the {\\em flow bound}, which is computationally tractable.","sentences":["Cardinality estimation and conjunctive query evaluation are two of the most fundamental problems in database query processing.","Recent work proposed, studied, and implemented a robust and practical information-theoretic cardinality estimation framework.","In this framework, the estimator is the cardinality upper bound of a conjunctive query subject to ``degree-constraints'', which model a rich set of input data statistics.","For general degree constraints, computing this bound is computationally hard.","Researchers have naturally sought efficiently computable relaxed upper bounds that are as tight as possible.","The polymatroid bound is the tightest among those relaxed upper bounds.","While it is an open question whether the polymatroid bound can be computed in polynomial-time in general, it is known to be computable in polynomial-time for some classes of degree constraints.   ","Our focus is on a common class of degree constraints called simple degree constraints.","Researchers had not previously determined how to compute the polymatroid bound in polynomial time for this class of constraints.","Our first main result is a polynomial time algorithm to compute the polymatroid bound given simple degree constraints.","Our second main result is a polynomial-time algorithm to compute a ``proof sequence'' establishing this bound.","This proof sequence can then be incorporated in the PANDA-framework to give a faster algorithm to evaluate a conjunctive query.","In addition, we show computational limitations to extending our results to broader classes of degree constraints.","Finally, our technique leads naturally to a new relaxed upper bound called the {\\em flow bound}, which is computationally tractable."],"url":"http://arxiv.org/abs/2504.02770v1"}
{"created":"2025-04-03 16:49:58","title":"Echoes of the hidden: Uncovering coordination beyond network structure","abstract":"The study of connectivity and coordination has drawn increasing attention in recent decades due to their central role in driving markets, shaping societal dynamics, and influencing biological systems. Traditionally, observable connections, such as phone calls, financial transactions, or social media connections, have been used to infer coordination and connectivity. However, incomplete, encrypted, or fragmented data, alongside the ubiquity of communication platforms and deliberate obfuscation, often leave many real-world connections hidden. In this study, we demonstrate that coordinating individuals exhibit shared bursty activity patterns, enabling their detection even when observable links between them are sparse or entirely absent. We further propose a generative model based on the network of networks formalism to account for the mechanisms driving this collaborative burstiness, attributing it to shock propagation across networks rather than isolated individual behavior. Model simulations demonstrate that when observable connection density is below 70\\%, burstiness significantly improves coordination detection compared to state-of-the-art temporal and structural methods. This work provides a new perspective on community and coordination dynamics, advancing both theoretical understanding and practical detection. By laying the foundation for identifying hidden connections beyond observable network structures, it enables detection across different platforms, alongside enhancing system behavior understanding, informed decision-making, and risk mitigation.","sentences":["The study of connectivity and coordination has drawn increasing attention in recent decades due to their central role in driving markets, shaping societal dynamics, and influencing biological systems.","Traditionally, observable connections, such as phone calls, financial transactions, or social media connections, have been used to infer coordination and connectivity.","However, incomplete, encrypted, or fragmented data, alongside the ubiquity of communication platforms and deliberate obfuscation, often leave many real-world connections hidden.","In this study, we demonstrate that coordinating individuals exhibit shared bursty activity patterns, enabling their detection even when observable links between them are sparse or entirely absent.","We further propose a generative model based on the network of networks formalism to account for the mechanisms driving this collaborative burstiness, attributing it to shock propagation across networks rather than isolated individual behavior.","Model simulations demonstrate that when observable connection density is below 70\\%, burstiness significantly improves coordination detection compared to state-of-the-art temporal and structural methods.","This work provides a new perspective on community and coordination dynamics, advancing both theoretical understanding and practical detection.","By laying the foundation for identifying hidden connections beyond observable network structures, it enables detection across different platforms, alongside enhancing system behavior understanding, informed decision-making, and risk mitigation."],"url":"http://arxiv.org/abs/2504.02757v1"}
{"created":"2025-04-03 16:35:49","title":"Atrial constitutive neural networks","abstract":"This work presents a novel approach for characterizing the mechanical behavior of atrial tissue using constitutive neural networks. Based on experimental biaxial tensile test data of healthy human atria, we automatically discover the most appropriate constitutive material model, thereby overcoming the limitations of traditional, pre-defined models. This approach offers a new perspective on modeling atrial mechanics and is a significant step towards improved simulation and prediction of cardiac health.","sentences":["This work presents a novel approach for characterizing the mechanical behavior of atrial tissue using constitutive neural networks.","Based on experimental biaxial tensile test data of healthy human atria, we automatically discover the most appropriate constitutive material model, thereby overcoming the limitations of traditional, pre-defined models.","This approach offers a new perspective on modeling atrial mechanics and is a significant step towards improved simulation and prediction of cardiac health."],"url":"http://arxiv.org/abs/2504.02748v1"}
{"created":"2025-04-03 16:26:20","title":"Faster Mixing of the Jerrum-Sinclair Chain","abstract":"We show that the Jerrum-Sinclair Markov chain on matchings mixes in time $\\widetilde{O}(\\Delta^2 m)$ on any graph with $n$ vertices, $m$ edges, and maximum degree $\\Delta$, for any constant edge weight $\\lambda>0$. For general graphs with arbitrary, potentially unbounded $\\Delta$, this provides the first improvement over the classic $\\widetilde{O}(n^2 m)$ mixing time bound of Jerrum and Sinclair (1989) and Sinclair (1992).   To achieve this, we develop a general framework for analyzing mixing times, combining ideas from the classic canonical path method with the \"local-to-global\" approaches recently developed in high-dimensional expanders, introducing key innovations to both techniques.","sentences":["We show that the Jerrum-Sinclair Markov chain on matchings mixes in time $\\widetilde{O}(\\Delta^2 m)$ on any graph with $n$ vertices, $m$ edges, and maximum degree $\\Delta$, for any constant edge weight $\\lambda>0$. For general graphs with arbitrary, potentially unbounded $\\Delta$, this provides the first improvement over the classic $\\widetilde{O}(n^2 m)$ mixing time bound of Jerrum and Sinclair (1989) and Sinclair (1992).   ","To achieve this, we develop a general framework for analyzing mixing times, combining ideas from the classic canonical path method with the \"local-to-global\" approaches recently developed in high-dimensional expanders, introducing key innovations to both techniques."],"url":"http://arxiv.org/abs/2504.02740v1"}
{"created":"2025-04-03 16:22:15","title":"Pushing the Limit of PPG Sensing in Sedentary Conditions by Addressing Poor Skin-sensor Contact","abstract":"Photoplethysmography (PPG) is a widely used non-invasive technique for monitoring cardiovascular health and various physiological parameters on consumer and medical devices. While motion artifacts are well-known challenges in dynamic settings, suboptimal skin-sensor contact in sedentary conditions - a critical issue often overlooked in existing literature - can distort PPG signal morphology, leading to the loss or shift of essential waveform features and therefore degrading sensing performance. In this work, we propose CP-PPG, a novel approach that transforms Contact Pressure-distorted PPG signals into ones with the ideal morphology. CP-PPG incorporates a novel data collection approach, a well-crafted signal processing pipeline, and an advanced deep adversarial model trained with a custom PPG-aware loss function. We validated CP-PPG through comprehensive evaluations, including 1) morphology transformation performance on our self-collected dataset, 2) downstream physiological monitoring performance on public datasets, and 3) in-the-wild performance. Extensive experiments demonstrate substantial and consistent improvements in signal fidelity (Mean Absolute Error: 0.09, 40% improvement over the original signal) as well as downstream performance across all evaluations in Heart Rate (HR), Heart Rate Variability (HRV), Respiration Rate (RR), and Blood Pressure (BP) estimation (on average, 21% improvement in HR; 41-46% in HRV; 6% in RR; and 4-5% in BP). These findings highlight the critical importance of addressing skin-sensor contact issues for accurate and dependable PPG-based physiological monitoring. Furthermore, CP-PPG can serve as a generic, plug-in API to enhance PPG signal quality.","sentences":["Photoplethysmography (PPG) is a widely used non-invasive technique for monitoring cardiovascular health and various physiological parameters on consumer and medical devices.","While motion artifacts are well-known challenges in dynamic settings, suboptimal skin-sensor contact in sedentary conditions - a critical issue often overlooked in existing literature - can distort PPG signal morphology, leading to the loss or shift of essential waveform features and therefore degrading sensing performance.","In this work, we propose CP-PPG, a novel approach that transforms Contact Pressure-distorted PPG signals into ones with the ideal morphology.","CP-PPG incorporates a novel data collection approach, a well-crafted signal processing pipeline, and an advanced deep adversarial model trained with a custom PPG-aware loss function.","We validated CP-PPG through comprehensive evaluations, including 1) morphology transformation performance on our self-collected dataset, 2) downstream physiological monitoring performance on public datasets, and 3) in-the-wild performance.","Extensive experiments demonstrate substantial and consistent improvements in signal fidelity (Mean Absolute Error: 0.09, 40% improvement over the original signal) as well as downstream performance across all evaluations in Heart Rate (HR), Heart Rate Variability (HRV), Respiration Rate (RR), and Blood Pressure (BP) estimation (on average, 21% improvement in HR; 41-46% in HRV; 6% in RR; and 4-5% in BP).","These findings highlight the critical importance of addressing skin-sensor contact issues for accurate and dependable PPG-based physiological monitoring.","Furthermore, CP-PPG can serve as a generic, plug-in API to enhance PPG signal quality."],"url":"http://arxiv.org/abs/2504.02735v1"}
{"created":"2025-04-03 16:17:56","title":"Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study","abstract":"Large Language Models (LLMs) are highly vulnerable to input perturbations, as even a small prompt change may result in a substantially different output. Existing methods to enhance LLM robustness are primarily focused on perturbed data samples, whereas improving resiliency to perturbations of task-level instructions has remained relatively underexplored. In this work, we focus on character- and word-level edits of task-specific instructions, which substantially degrade downstream performance. We experiment with a variety of techniques to enhance the robustness of LLMs, including self-denoising and representation alignment, testing different models (Llama 3 and Flan-T5), datasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and role-oriented). We find that, on average, self-denoising -- whether performed by a frozen LLM or a fine-tuned model -- achieves substantially higher performance gains than alternative strategies, including more complex baselines such as ensembling and supervised methods.","sentences":["Large Language Models (LLMs) are highly vulnerable to input perturbations, as even a small prompt change may result in a substantially different output.","Existing methods to enhance LLM robustness are primarily focused on perturbed data samples, whereas improving resiliency to perturbations of task-level instructions has remained relatively underexplored.","In this work, we focus on character- and word-level edits of task-specific instructions, which substantially degrade downstream performance.","We experiment with a variety of techniques to enhance the robustness of LLMs, including self-denoising and representation alignment, testing different models (Llama 3 and Flan-T5), datasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and role-oriented).","We find that, on average, self-denoising -- whether performed by a frozen LLM or a fine-tuned model -- achieves substantially higher performance gains than alternative strategies, including more complex baselines such as ensembling and supervised methods."],"url":"http://arxiv.org/abs/2504.02733v1"}
{"created":"2025-04-03 16:17:55","title":"Why do LLMs attend to the first token?","abstract":"Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.","sentences":["Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink.","Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it.","Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention.","Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used?","In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers.","We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour.","We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training."],"url":"http://arxiv.org/abs/2504.02732v1"}
{"created":"2025-04-03 16:13:34","title":"HQViT: Hybrid Quantum Vision Transformer for Image Classification","abstract":"Transformer-based architectures have revolutionized the landscape of deep learning. In computer vision domain, Vision Transformer demonstrates remarkable performance on par with or even surpassing that of convolutional neural networks. However, the quadratic computational complexity of its self-attention mechanism poses challenges for classical computing, making model training with high-dimensional input data, e.g., images, particularly expensive. To address such limitations, we propose a Hybrid Quantum Vision Transformer (HQViT), that leverages the principles of quantum computing to accelerate model training while enhancing model performance. HQViT introduces whole-image processing with amplitude encoding to better preserve global image information without additional positional encoding. By leveraging quantum computation on the most critical steps and selectively handling other components in a classical way, we lower the cost of quantum resources for HQViT. The qubit requirement is minimized to $O(log_2N)$ and the number of parameterized quantum gates is only $O(log_2d)$, making it well-suited for Noisy Intermediate-Scale Quantum devices. By offloading the computationally intensive attention coefficient matrix calculation to the quantum framework, HQViT reduces the classical computational load by $O(T^2d)$. Extensive experiments across various computer vision datasets demonstrate that HQViT outperforms existing models, achieving a maximum improvement of up to $10.9\\%$ (on the MNIST 10-classification task) over the state of the art. This work highlights the great potential to combine quantum and classical computing to cope with complex image classification tasks.","sentences":["Transformer-based architectures have revolutionized the landscape of deep learning.","In computer vision domain, Vision Transformer demonstrates remarkable performance on par with or even surpassing that of convolutional neural networks.","However, the quadratic computational complexity of its self-attention mechanism poses challenges for classical computing, making model training with high-dimensional input data, e.g., images, particularly expensive.","To address such limitations, we propose a Hybrid Quantum Vision Transformer (HQViT), that leverages the principles of quantum computing to accelerate model training while enhancing model performance.","HQViT introduces whole-image processing with amplitude encoding to better preserve global image information without additional positional encoding.","By leveraging quantum computation on the most critical steps and selectively handling other components in a classical way, we lower the cost of quantum resources for HQViT.","The qubit requirement is minimized to $O(log_2N)$ and the number of parameterized quantum gates is only $O(log_2d)$, making it well-suited for Noisy Intermediate-Scale Quantum devices.","By offloading the computationally intensive attention coefficient matrix calculation to the quantum framework, HQViT reduces the classical computational load by $O(T^2d)$. Extensive experiments across various computer vision datasets demonstrate that HQViT outperforms existing models, achieving a maximum improvement of up to $10.9\\%$ (on the MNIST 10-classification task) over the state of the art.","This work highlights the great potential to combine quantum and classical computing to cope with complex image classification tasks."],"url":"http://arxiv.org/abs/2504.02730v1"}
{"created":"2025-04-03 16:06:44","title":"Autonomous Human-Robot Interaction via Operator Imitation","abstract":"Teleoperated robotic characters can perform expressive interactions with humans, relying on the operators' experience and social intuition. In this work, we propose to create autonomous interactive robots, by training a model to imitate operator data. Our model is trained on a dataset of human-robot interactions, where an expert operator is asked to vary the interactions and mood of the robot, while the operator commands as well as the pose of the human and robot are recorded. Our approach learns to predict continuous operator commands through a diffusion process and discrete commands through a classifier, all unified within a single transformer architecture. We evaluate the resulting model in simulation and with a user study on the real system. We show that our method enables simple autonomous human-robot interactions that are comparable to the expert-operator baseline, and that users can recognize the different robot moods as generated by our model. Finally, we demonstrate a zero-shot transfer of our model onto a different robotic platform with the same operator interface.","sentences":["Teleoperated robotic characters can perform expressive interactions with humans, relying on the operators' experience and social intuition.","In this work, we propose to create autonomous interactive robots, by training a model to imitate operator data.","Our model is trained on a dataset of human-robot interactions, where an expert operator is asked to vary the interactions and mood of the robot, while the operator commands as well as the pose of the human and robot are recorded.","Our approach learns to predict continuous operator commands through a diffusion process and discrete commands through a classifier, all unified within a single transformer architecture.","We evaluate the resulting model in simulation and with a user study on the real system.","We show that our method enables simple autonomous human-robot interactions that are comparable to the expert-operator baseline, and that users can recognize the different robot moods as generated by our model.","Finally, we demonstrate a zero-shot transfer of our model onto a different robotic platform with the same operator interface."],"url":"http://arxiv.org/abs/2504.02724v1"}
{"created":"2025-04-03 16:05:10","title":"Computing High-dimensional Confidence Sets for Arbitrary Distributions","abstract":"We study the problem of learning a high-density region of an arbitrary distribution over $\\mathbb{R}^d$. Given a target coverage parameter $\\delta$, and sample access to an arbitrary distribution $D$, we want to output a confidence set $S \\subset \\mathbb{R}^d$ such that $S$ achieves $\\delta$ coverage of $D$, i.e., $\\mathbb{P}_{y \\sim D} \\left[ y \\in S \\right] \\ge \\delta$, and the volume of $S$ is as small as possible. This is a central problem in high-dimensional statistics with applications in finding confidence sets, uncertainty quantification, and support estimation.   In the most general setting, this problem is statistically intractable, so we restrict our attention to competing with sets from a concept class $C$ with bounded VC-dimension. An algorithm is competitive with class $C$ if, given samples from an arbitrary distribution $D$, it outputs in polynomial time a set that achieves $\\delta$ coverage of $D$, and whose volume is competitive with the smallest set in $C$ with the required coverage $\\delta$. This problem is computationally challenging even in the basic setting when $C$ is the set of all Euclidean balls. Existing algorithms based on coresets find in polynomial time a ball whose volume is $\\exp(\\tilde{O}( d/ \\log d))$-factor competitive with the volume of the best ball.   Our main result is an algorithm that finds a confidence set whose volume is $\\exp(\\tilde{O}(d^{2/3}))$ factor competitive with the optimal ball having the desired coverage. The algorithm is improper (it outputs an ellipsoid). Combined with our computational intractability result for proper learning balls within an $\\exp(\\tilde{O}(d^{1-o(1)}))$ approximation factor in volume, our results provide an interesting separation between proper and (improper) learning of confidence sets.","sentences":["We study the problem of learning a high-density region of an arbitrary distribution over $\\mathbb{R}^d$. Given a target coverage parameter $\\delta$, and sample access to an arbitrary distribution $D$, we want to output a confidence set $S \\subset \\mathbb{R}^d$ such that $S$ achieves $\\delta$ coverage of $D$, i.e., $\\mathbb{P}_{y \\sim D} \\left","[ y \\in S \\right] \\ge \\delta$, and the volume of $S$ is as small as possible.","This is a central problem in high-dimensional statistics with applications in finding confidence sets, uncertainty quantification, and support estimation.   ","In the most general setting, this problem is statistically intractable, so we restrict our attention to competing with sets from a concept class $C$ with bounded VC-dimension.","An algorithm is competitive with class $C$ if, given samples from an arbitrary distribution $D$, it outputs in polynomial time a set that achieves $\\delta$ coverage of $D$, and whose volume is competitive with the smallest set in $C$ with the required coverage $\\delta$.","This problem is computationally challenging even in the basic setting when $C$ is the set of all Euclidean balls.","Existing algorithms based on coresets find in polynomial time a ball whose volume is $\\exp(\\tilde{O}( d/ \\log d))$-factor competitive with the volume of the best ball.   ","Our main result is an algorithm that finds a confidence set whose volume is $\\exp(\\tilde{O}(d^{2/3}))$ factor competitive with the optimal ball having the desired coverage.","The algorithm is improper (it outputs an ellipsoid).","Combined with our computational intractability result for proper learning balls within an $\\exp(\\tilde{O}(d^{1-o(1)}))$ approximation factor in volume, our results provide an interesting separation between proper and (improper) learning of confidence sets."],"url":"http://arxiv.org/abs/2504.02723v1"}
{"created":"2025-04-03 16:05:03","title":"Dynamic Directional Routing of Freight in the Physical Internet","abstract":"The Physical Internet (PI) envisions an interconnected, modular, and dynamically managed logistics system inspired by the Digital Internet. It enables open-access networks where shipments traverse a hyperconnected system of hubs, adjusting routes based on real-time conditions. A key challenge in scalable and adaptive freight movement is routing determining how shipments navigate the network to balance service levels, consolidation, and adaptability. This paper introduces directional routing, a dynamic approach that flexibly adjusts shipment paths, optimizing efficiency and consolidation using real-time logistics data. Unlike shortest-path routing, which follows fixed routes, directional routing dynamically selects feasible next-hop hubs based on network conditions, consolidation opportunities, and service level constraints. It consists of two phases: area discovery, which identifies candidate hubs, and node selection, which determines the next hub based on real-time parameters. This paper advances the area discovery phase by introducing a Reduced Search Space Breadth-First Search (RSS-BFS) method to systematically identify feasible routing areas while balancing service levels and consolidation. The proposed approach enhances network fluidity, scalability, and adaptability in PI-based logistics, advancing autonomous and sustainable freight movement.","sentences":["The Physical Internet (PI) envisions an interconnected, modular, and dynamically managed logistics system inspired by the Digital Internet.","It enables open-access networks where shipments traverse a hyperconnected system of hubs, adjusting routes based on real-time conditions.","A key challenge in scalable and adaptive freight movement is routing determining how shipments navigate the network to balance service levels, consolidation, and adaptability.","This paper introduces directional routing, a dynamic approach that flexibly adjusts shipment paths, optimizing efficiency and consolidation using real-time logistics data.","Unlike shortest-path routing, which follows fixed routes, directional routing dynamically selects feasible next-hop hubs based on network conditions, consolidation opportunities, and service level constraints.","It consists of two phases: area discovery, which identifies candidate hubs, and node selection, which determines the next hub based on real-time parameters.","This paper advances the area discovery phase by introducing a Reduced Search Space Breadth-First Search (RSS-BFS) method to systematically identify feasible routing areas while balancing service levels and consolidation.","The proposed approach enhances network fluidity, scalability, and adaptability in PI-based logistics, advancing autonomous and sustainable freight movement."],"url":"http://arxiv.org/abs/2504.02722v1"}
{"created":"2025-04-03 15:52:39","title":"Web3DB: Web 3.0 RDBMS for Individual Data Ownership","abstract":"This paper introduces Web3DB, a decentralized relational database management system (RDBMS) designed to align with the principles of Web 3.0, addressing critical shortcomings of traditional centralized DBMS, such as data privacy, security vulnerabilities, and single points of failure. Several similar systems have been proposed, but they are not compatible with the legacy systems based on RDBMS. Motivated by the necessity for enhanced data sovereignty and the decentralization of data control, Web3DB leverages blockchain technology for fine-grained access control and utilizes decentralized data storage. This system leverages a novel, modular architecture that contributes to enhanced flexibility, scalability, and user-centric functionality. Central to the Web3DB innovation is its decentralized query execution, which uses cryptographic sortition and blockchain verification to ensure secure and fair query processing across network nodes. The motivation for integrating relational databases within decentralized DBMS primarily stems from the need to combine the robustness and ease of use of relational database structures with the benefits of decentralization. This paper outlines the architecture of Web3DB, its practical implementation, and the system's ability to support SQL-like operations on relational data, manage multi-tenancy, and facilitate open data sharing, setting new standards for decentralized databases in the Web 3.0 era.","sentences":["This paper introduces Web3DB, a decentralized relational database management system (RDBMS) designed to align with the principles of Web 3.0, addressing critical shortcomings of traditional centralized DBMS, such as data privacy, security vulnerabilities, and single points of failure.","Several similar systems have been proposed, but they are not compatible with the legacy systems based on RDBMS.","Motivated by the necessity for enhanced data sovereignty and the decentralization of data control, Web3DB leverages blockchain technology for fine-grained access control and utilizes decentralized data storage.","This system leverages a novel, modular architecture that contributes to enhanced flexibility, scalability, and user-centric functionality.","Central to the Web3DB innovation is its decentralized query execution, which uses cryptographic sortition and blockchain verification to ensure secure and fair query processing across network nodes.","The motivation for integrating relational databases within decentralized DBMS primarily stems from the need to combine the robustness and ease of use of relational database structures with the benefits of decentralization.","This paper outlines the architecture of Web3DB, its practical implementation, and the system's ability to support SQL-like operations on relational data, manage multi-tenancy, and facilitate open data sharing, setting new standards for decentralized databases in the Web 3.0 era."],"url":"http://arxiv.org/abs/2504.02713v1"}
{"created":"2025-04-03 15:41:48","title":"EvoChain: A Framework for Tracking and Visualizing Smart Contract Evolution","abstract":"Tracking the evolution of smart contracts is challenging due to their immutable nature and complex upgrade mechanisms. We introduce EvoChain, a comprehensive framework and dataset designed to track and visualize smart contract evolution. Building upon data from our previous empirical study, EvoChain models contract relationships using a Neo4j graph database and provides an interactive web interface for exploration. The framework consists of a data layer, an API layer, and a user interface layer. EvoChain allows stakeholders to analyze contract histories, upgrade paths, and associated vulnerabilities by leveraging these components. Our dataset encompasses approximately 1.3 million upgradeable proxies and nearly 15,000 historical versions, enhancing transparency and trust in blockchain ecosystems by providing an accessible platform for understanding smart contract evolution.","sentences":["Tracking the evolution of smart contracts is challenging due to their immutable nature and complex upgrade mechanisms.","We introduce EvoChain, a comprehensive framework and dataset designed to track and visualize smart contract evolution.","Building upon data from our previous empirical study, EvoChain models contract relationships using a Neo4j graph database and provides an interactive web interface for exploration.","The framework consists of a data layer, an API layer, and a user interface layer.","EvoChain allows stakeholders to analyze contract histories, upgrade paths, and associated vulnerabilities by leveraging these components.","Our dataset encompasses approximately 1.3 million upgradeable proxies and nearly 15,000 historical versions, enhancing transparency and trust in blockchain ecosystems by providing an accessible platform for understanding smart contract evolution."],"url":"http://arxiv.org/abs/2504.02704v1"}
{"created":"2025-04-03 15:32:32","title":"Mind the Gap? Not for SVP Hardness under ETH!","abstract":"We prove new hardness results for fundamental lattice problems under the Exponential Time Hypothesis (ETH). Building on a recent breakthrough by Bitansky et al. [BHIRW24], who gave a polynomial-time reduction from $\\mathsf{3SAT}$ to the (gap) $\\mathsf{MAXLIN}$ problem-a class of CSPs with linear equations over finite fields-we derive ETH-hardness for several lattice problems.   First, we show that for any $p \\in [1, \\infty)$, there exists an explicit constant $\\gamma > 1$ such that $\\mathsf{CVP}_{p,\\gamma}$ (the $\\ell_p$-norm approximate Closest Vector Problem) does not admit a $2^{o(n)}$-time algorithm unless ETH is false. Our reduction is deterministic and proceeds via a direct reduction from (gap) $\\mathsf{MAXLIN}$ to $\\mathsf{CVP}_{p,\\gamma}$.   Next, we prove a randomized ETH-hardness result for $\\mathsf{SVP}_{p,\\gamma}$ (the $\\ell_p$-norm approximate Shortest Vector Problem) for all $p > 2$. This result relies on a novel property of the integer lattice $\\mathbb{Z}^n$ in the $\\ell_p$ norm and a randomized reduction from $\\mathsf{CVP}_{p,\\gamma}$ to $\\mathsf{SVP}_{p,\\gamma'}$.   Finally, we improve over prior reductions from $\\mathsf{3SAT}$ to $\\mathsf{BDD}_{p, \\alpha}$ (the Bounded Distance Decoding problem), yielding better ETH-hardness results for $\\mathsf{BDD}_{p, \\alpha}$ for any $p \\in [1, \\infty)$ and $\\alpha > \\alpha_p^{\\ddagger}$, where $\\alpha_p^{\\ddagger}$ is an explicit threshold depending on $p$.   We additionally observe that prior work implies ETH hardness for the gap minimum distance problem ($\\gamma$-$\\mathsf{MDP}$) in codes.","sentences":["We prove new hardness results for fundamental lattice problems under the Exponential Time Hypothesis (ETH).","Building on a recent breakthrough by Bitansky et al.","[BHIRW24], who gave a polynomial-time reduction from $\\mathsf{3SAT}$ to the (gap) $\\mathsf{MAXLIN}$ problem-a class of CSPs with linear equations over finite fields-we derive ETH-hardness for several lattice problems.   ","First, we show that for any $p \\in [1, \\infty)$, there exists an explicit constant $\\gamma > 1$ such that $\\mathsf{CVP}_{p,\\gamma}$ (the $\\ell_p$-norm approximate Closest Vector Problem) does not admit a $2^{o(n)}$-time algorithm unless ETH is false.","Our reduction is deterministic and proceeds via a direct reduction from (gap) $\\mathsf{MAXLIN}$ to $\\mathsf{CVP}_{p,\\gamma}$.   Next, we prove a randomized ETH-hardness result for $\\mathsf{SVP}_{p,\\gamma}$ (the $\\ell_p$-norm approximate Shortest Vector Problem) for all $p > 2$.","This result relies on a novel property of the integer lattice $\\mathbb{Z}^n$ in the $\\ell_p$ norm and a randomized reduction from $\\mathsf{CVP}_{p,\\gamma}$ to $\\mathsf{SVP}_{p,\\gamma'}$.   Finally, we improve over prior reductions from $\\mathsf{3SAT}$ to $\\mathsf{BDD}_{p, \\alpha}$ (the Bounded Distance Decoding problem), yielding better ETH-hardness results for $\\mathsf{BDD}_{p, \\alpha}$ for any $p \\in [1, \\infty)$ and $\\alpha > \\alpha_p^{\\ddagger}$, where $\\alpha_p^{\\ddagger}$ is an explicit threshold depending on $p$.   We additionally observe that prior work implies ETH hardness for the gap minimum distance problem ($\\gamma$-$\\mathsf{MDP}$) in codes."],"url":"http://arxiv.org/abs/2504.02695v1"}
{"created":"2025-04-03 15:28:04","title":"Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication using DRL","abstract":"Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted next-generation wireless networks is critical for mobility management and ensuring UAV safety and ubiquitous connectivity, especially in dense urban environments with street canyons and tall buildings. Traditional statistical and model-based techniques have been successfully used for path optimization in communication networks. However, when dynamic channel propagation characteristics such as line-of-sight (LOS), interference, handover, and signal-to-interference and noise ratio (SINR) are included in path optimization, statistical and model-based path planning solutions become obsolete since they cannot adapt to the dynamic and time-varying wireless channels, especially in the mmWave bands. In this paper, we propose a novel model-free actor-critic deep reinforcement learning (AC-DRL) framework for path optimization in UAV-assisted 5G mmWave wireless networks, which combines four important aspects of UAV communication: \\textit{flight time, handover, connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to a gNB to determine the optimal path to a desired destination in the shortest possible time with minimal gNB handover, while maintaining connectivity and the highest possible SINR. We train our model with data from a powerful ray tracing tool called Wireless InSite, which uses 3D images of the propagation environment and provides data that closely resembles the real propagation environment. The simulation results show that our system has superior performance in tracking high SINR compared to other selected RL algorithms.","sentences":["Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted next-generation wireless networks is critical for mobility management and ensuring UAV safety and ubiquitous connectivity, especially in dense urban environments with street canyons and tall buildings.","Traditional statistical and model-based techniques have been successfully used for path optimization in communication networks.","However, when dynamic channel propagation characteristics such as line-of-sight (LOS), interference, handover, and signal-to-interference and noise ratio (SINR) are included in path optimization, statistical and model-based path planning solutions become obsolete since they cannot adapt to the dynamic and time-varying wireless channels, especially in the mmWave bands.","In this paper, we propose a novel model-free actor-critic deep reinforcement learning (AC-DRL) framework for path optimization in UAV-assisted 5G mmWave wireless networks, which combines four important aspects of UAV communication: \\textit{flight time, handover, connectivity and SINR}.","We train an AC-RL agent that enables a UAV connected to a gNB to determine the optimal path to a desired destination in the shortest possible time with minimal gNB handover, while maintaining connectivity and the highest possible SINR.","We train our model with data from a powerful ray tracing tool called Wireless InSite, which uses 3D images of the propagation environment and provides data that closely resembles the real propagation environment.","The simulation results show that our system has superior performance in tracking high SINR compared to other selected RL algorithms."],"url":"http://arxiv.org/abs/2504.02688v1"}
{"created":"2025-04-03 15:14:19","title":"Limitations of Religious Data and the Importance of the Target Domain: Towards Machine Translation for Guinea-Bissau Creole","abstract":"We introduce a new dataset for machine translation of Guinea-Bissau Creole (Kiriol), comprising around 40 thousand parallel sentences to English and Portuguese. This dataset is made up of predominantly religious data (from the Bible and texts from the Jehovah's Witnesses), but also a small amount of general domain data (from a dictionary). This mirrors the typical resource availability of many low resource languages. We train a number of transformer-based models to investigate how to improve domain transfer from religious data to a more general domain. We find that adding even 300 sentences from the target domain when training substantially improves the translation performance, highlighting the importance and need for data collection for low-resource languages, even on a small-scale. We additionally find that Portuguese-to-Kiriol translation models perform better on average than other source and target language pairs, and investigate how this relates to the morphological complexity of the languages involved and the degree of lexical overlap between creoles and lexifiers. Overall, we hope our work will stimulate research into Kiriol and into how machine translation might better support creole languages in general.","sentences":["We introduce a new dataset for machine translation of Guinea-Bissau Creole (Kiriol), comprising around 40 thousand parallel sentences to English and Portuguese.","This dataset is made up of predominantly religious data (from the Bible and texts from the Jehovah's Witnesses), but also a small amount of general domain data (from a dictionary).","This mirrors the typical resource availability of many low resource languages.","We train a number of transformer-based models to investigate how to improve domain transfer from religious data to a more general domain.","We find that adding even 300 sentences from the target domain when training substantially improves the translation performance, highlighting the importance and need for data collection for low-resource languages, even on a small-scale.","We additionally find that Portuguese-to-Kiriol translation models perform better on average than other source and target language pairs, and investigate how this relates to the morphological complexity of the languages involved and the degree of lexical overlap between creoles and lexifiers.","Overall, we hope our work will stimulate research into Kiriol and into how machine translation might better support creole languages in general."],"url":"http://arxiv.org/abs/2504.02674v1"}
{"created":"2025-04-03 15:03:08","title":"Development of Automated Data Quality Assessment and Evaluation Indices by Analytical Experience","abstract":"The societal need to leverage third-party data has driven the data-distribution market and increased the importance of data quality assessment (DQA) in data transactions between organizations. However, DQA requires expert knowledge of raw data and related data attributes, which hinders consensus-building in data purchasing. This study focused on the differences in DQAs between experienced and inexperienced data handlers. We performed two experiments: The first was a questionnaire survey involving 41 participants with varying levels of data-handling experience, who evaluated 12 data samples using 10 predefined indices with and without quality metadata generated by the automated tool. The second was an eye-tracking experiment to reveal the viewing behavior of participants during data evaluation. It was revealed that using quality metadata generated by the automated tool can reduce misrecognition in DQA. While experienced data handlers rated the quality metadata highly, semi-experienced users gave it the lowest ratings. This study contributes to enhancing data understanding within organizations and promoting the distribution of valuable data by proposing an automated tool to support DQAs.","sentences":["The societal need to leverage third-party data has driven the data-distribution market and increased the importance of data quality assessment (DQA) in data transactions between organizations.","However, DQA requires expert knowledge of raw data and related data attributes, which hinders consensus-building in data purchasing.","This study focused on the differences in DQAs between experienced and inexperienced data handlers.","We performed two experiments: The first was a questionnaire survey involving 41 participants with varying levels of data-handling experience, who evaluated 12 data samples using 10 predefined indices with and without quality metadata generated by the automated tool.","The second was an eye-tracking experiment to reveal the viewing behavior of participants during data evaluation.","It was revealed that using quality metadata generated by the automated tool can reduce misrecognition in DQA.","While experienced data handlers rated the quality metadata highly, semi-experienced users gave it the lowest ratings.","This study contributes to enhancing data understanding within organizations and promoting the distribution of valuable data by proposing an automated tool to support DQAs."],"url":"http://arxiv.org/abs/2504.02663v1"}
{"created":"2025-04-03 14:54:17","title":"MiLo: Efficient Quantized MoE Inference with Mixture of Low-Rank Compensators","abstract":"A critical approach for efficiently deploying Mixture-of-Experts (MoE) models with massive parameters is quantization. However, state-of-the-art MoE models suffer from non-negligible accuracy loss with extreme quantization, such as under 4 bits. To address this, we introduce MiLo, a novel method that augments highly quantized MoEs with a mixture of low-rank compensators. These compensators consume only a small amount of additional memory but significantly recover accuracy loss from extreme quantization. MiLo also identifies that MoEmodels exhibit distinctive characteristics across weights due to their hybrid dense-sparse architectures, and employs adaptive rank selection policies along with iterative optimizations to close the accuracy gap. MiLo does not rely on calibration data, allowing it to generalize to different MoE models and datasets without overfitting to a calibration set. To avoid the hardware inefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor Core-friendly 3-bit kernels, enabling measured latency speedups on 3-bit quantized MoE models. Our evaluation shows that MiLo outperforms existing methods on SoTA MoE models across various tasks.","sentences":["A critical approach for efficiently deploying Mixture-of-Experts (MoE) models with massive parameters is quantization.","However, state-of-the-art MoE models suffer from non-negligible accuracy loss with extreme quantization, such as under 4 bits.","To address this, we introduce MiLo, a novel method that augments highly quantized MoEs with a mixture of low-rank compensators.","These compensators consume only a small amount of additional memory but significantly recover accuracy loss from extreme quantization.","MiLo also identifies that MoEmodels exhibit distinctive characteristics across weights due to their hybrid dense-sparse architectures, and employs adaptive rank selection policies along with iterative optimizations to close the accuracy gap.","MiLo does not rely on calibration data, allowing it to generalize to different MoE models and datasets without overfitting to a calibration set.","To avoid the hardware inefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor Core-friendly 3-bit kernels, enabling measured latency speedups on 3-bit quantized MoE models.","Our evaluation shows that MiLo outperforms existing methods on SoTA MoE models across various tasks."],"url":"http://arxiv.org/abs/2504.02658v1"}
{"created":"2025-04-03 14:49:15","title":"Optimizing Resource Allocation to Mitigate the Risk of Disruptive Events in Homeland Security and Emergency Management","abstract":"Homeland security in the United States faces a daunting task due to the multiple threats and hazards that can occur. Natural disasters, human-caused incidents such as terrorist attacks, and technological failures can result in significant damage, fatalities, injuries, and economic losses. The increasing frequency and severity of disruptive events in the United States highlight the urgent need for effectively allocating resources in homeland security and emergency preparedness. This article presents an optimization-based decision support model to help homeland security policymakers identify and select projects that best mitigate the risk of threats and hazards while satisfying a budget constraint. The model incorporates multiple hazards, probabilistic risk assessments, and multidimensional consequences and integrates historical data and publicly available sources to evaluate and select the most effective risk mitigation projects and optimize resource allocation across various disaster scenarios. We apply this model to the state of Iowa, considering 16 hazards, six types of consequences, and 52 mitigation projects. Our results demonstrate how different budget levels influence project selection, emphasizing cost-effective solutions that maximize risk reduction. Sensitivity analysis examines the robustness of project selection under varying effectiveness assumptions and consequence estimations. The findings offer critical insights for policymakers in homeland security and emergency management and provide a basis for more efficient resource allocation and improved disaster resilience.","sentences":["Homeland security in the United States faces a daunting task due to the multiple threats and hazards that can occur.","Natural disasters, human-caused incidents such as terrorist attacks, and technological failures can result in significant damage, fatalities, injuries, and economic losses.","The increasing frequency and severity of disruptive events in the United States highlight the urgent need for effectively allocating resources in homeland security and emergency preparedness.","This article presents an optimization-based decision support model to help homeland security policymakers identify and select projects that best mitigate the risk of threats and hazards while satisfying a budget constraint.","The model incorporates multiple hazards, probabilistic risk assessments, and multidimensional consequences and integrates historical data and publicly available sources to evaluate and select the most effective risk mitigation projects and optimize resource allocation across various disaster scenarios.","We apply this model to the state of Iowa, considering 16 hazards, six types of consequences, and 52 mitigation projects.","Our results demonstrate how different budget levels influence project selection, emphasizing cost-effective solutions that maximize risk reduction.","Sensitivity analysis examines the robustness of project selection under varying effectiveness assumptions and consequence estimations.","The findings offer critical insights for policymakers in homeland security and emergency management and provide a basis for more efficient resource allocation and improved disaster resilience."],"url":"http://arxiv.org/abs/2504.02652v1"}
{"created":"2025-04-03 14:40:40","title":"Prompt Optimization with Logged Bandit Data","abstract":"We study how to use naturally available user feedback, such as clicks, to optimize large language model (LLM) pipelines for generating personalized sentences using prompts. Naive approaches, which estimate the policy gradient in the prompt space, suffer either from variance caused by the large action space of prompts or bias caused by inaccurate reward predictions. To circumvent these challenges, we propose a novel kernel-based off-policy gradient method, which estimates the policy gradient by leveraging similarity among generated sentences, substantially reducing variance while suppressing the bias. Empirical results on our newly established suite of benchmarks demonstrate the effectiveness of the proposed approach in generating personalized descriptions for movie recommendations, particularly when the number of candidate prompts is large.","sentences":["We study how to use naturally available user feedback, such as clicks, to optimize large language model (LLM) pipelines for generating personalized sentences using prompts.","Naive approaches, which estimate the policy gradient in the prompt space, suffer either from variance caused by the large action space of prompts or bias caused by inaccurate reward predictions.","To circumvent these challenges, we propose a novel kernel-based off-policy gradient method, which estimates the policy gradient by leveraging similarity among generated sentences, substantially reducing variance while suppressing the bias.","Empirical results on our newly established suite of benchmarks demonstrate the effectiveness of the proposed approach in generating personalized descriptions for movie recommendations, particularly when the number of candidate prompts is large."],"url":"http://arxiv.org/abs/2504.02646v1"}
{"created":"2025-04-03 14:36:08","title":"RoSMM: A Robust and Secure Multi-Modal Watermarking Framework for Diffusion Models","abstract":"Current image watermarking technologies are predominantly categorized into text watermarking techniques and image steganography; however, few methods can simultaneously handle text and image-based watermark data, which limits their applicability in complex digital environments. This paper introduces an innovative multi-modal watermarking approach, drawing on the concept of vector discretization in encoder-based vector quantization. By constructing adjacency matrices, the proposed method enables the transformation of text watermarks into robust image-based representations, providing a novel multi-modal watermarking paradigm for image generation applications. Additionally, this study presents a newly designed image restoration module to mitigate image degradation caused by transmission losses and various noise interferences, thereby ensuring the reliability and integrity of the watermark. Experimental results validate the robustness of the method under multiple noise attacks, providing a secure, scalable, and efficient solution for digital image copyright protection.","sentences":["Current image watermarking technologies are predominantly categorized into text watermarking techniques and image steganography; however, few methods can simultaneously handle text and image-based watermark data, which limits their applicability in complex digital environments.","This paper introduces an innovative multi-modal watermarking approach, drawing on the concept of vector discretization in encoder-based vector quantization.","By constructing adjacency matrices, the proposed method enables the transformation of text watermarks into robust image-based representations, providing a novel multi-modal watermarking paradigm for image generation applications.","Additionally, this study presents a newly designed image restoration module to mitigate image degradation caused by transmission losses and various noise interferences, thereby ensuring the reliability and integrity of the watermark.","Experimental results validate the robustness of the method under multiple noise attacks, providing a secure, scalable, and efficient solution for digital image copyright protection."],"url":"http://arxiv.org/abs/2504.02640v1"}
{"created":"2025-04-03 14:34:38","title":"Medium Access for Push-Pull Data Transmission in 6G Wireless Systems","abstract":"Medium access in 5G systems was tailored to accommodate diverse traffic classes through network resource slicing. 6G wireless systems are expected to be significantly reliant on Artificial Intelligence (AI), leading to data-driven and goal-oriented communication. This leads to augmentation of the design space for Medium Access Control (MAC) protocols, which is the focus of this article. We introduce a taxonomy based on push-based and pull-based communication, which is useful to categorize both the legacy and the AI-driven access schemes. We provide MAC protocol design guidelines for pull- and push-based communication in terms of goal-oriented criteria, such as timing and data relevance. We articulate a framework for co-existence between pull and push-based communications in 6G systems, combining their advantages. We highlight the design principles and main tradeoffs, as well as the architectural considerations for integrating these designs in Open-Radio Access Network (O-RAN) and 6G systems.","sentences":["Medium access in 5G systems was tailored to accommodate diverse traffic classes through network resource slicing.","6G wireless systems are expected to be significantly reliant on Artificial Intelligence (AI), leading to data-driven and goal-oriented communication.","This leads to augmentation of the design space for Medium Access Control (MAC) protocols, which is the focus of this article.","We introduce a taxonomy based on push-based and pull-based communication, which is useful to categorize both the legacy and the AI-driven access schemes.","We provide MAC protocol design guidelines for pull- and push-based communication in terms of goal-oriented criteria, such as timing and data relevance.","We articulate a framework for co-existence between pull and push-based communications in 6G systems, combining their advantages.","We highlight the design principles and main tradeoffs, as well as the architectural considerations for integrating these designs in Open-Radio Access Network (O-RAN) and 6G systems."],"url":"http://arxiv.org/abs/2504.02637v1"}
{"created":"2025-04-03 14:31:20","title":"Data-Driven Design of 3GPP Handover Parameters with Bayesian Optimization and Transfer Learning","abstract":"Mobility management in dense cellular networks is challenging due to varying user speeds and deployment conditions. Traditional 3GPP handover (HO) schemes, relying on fixed A3-offset and time-to-trigger (TTT) parameters, struggle to balance radio link failures (RLFs) and ping-pongs. We propose a data-driven HO optimization framework based on high-dimensional Bayesian optimization (HD-BO) and enhanced with transfer learning to reduce training time and improve generalization across different user speeds. Evaluations on a real-world deployment show that HD-BO outperforms 3GPP set-1 and set-5 benchmarks, while transfer learning enables rapid adaptation without loss in performance. This highlights the potential of data-driven, site-specific mobility management in large-scale networks.","sentences":["Mobility management in dense cellular networks is challenging due to varying user speeds and deployment conditions.","Traditional 3GPP handover (HO) schemes, relying on fixed A3-offset and time-to-trigger (TTT) parameters, struggle to balance radio link failures (RLFs) and ping-pongs.","We propose a data-driven HO optimization framework based on high-dimensional Bayesian optimization (HD-BO) and enhanced with transfer learning to reduce training time and improve generalization across different user speeds.","Evaluations on a real-world deployment show that HD-BO outperforms 3GPP set-1 and set-5 benchmarks, while transfer learning enables rapid adaptation without loss in performance.","This highlights the potential of data-driven, site-specific mobility management in large-scale networks."],"url":"http://arxiv.org/abs/2504.02633v1"}
{"created":"2025-04-03 14:21:33","title":"Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions","abstract":"Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly access agents in single-mission scenarios, failing to capture real-world complexity. To bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark, each test case comprises multiple interrelated missions. This design requires agents to dynamically adapt to evolving demands. Moreover, the proposed benchmark explores all possible mission-switching patterns within a fixed mission number. Specifically, we propose a multi-agent data generation framework to construct the benchmark. We also propose a novel method to evaluate the accuracy and efficiency of agent decisions with dynamic decision trees. Experiments on diverse open-source and closed-source LLMs reveal critical factors influencing agent robustness and provide actionable insights to the tool invocation society.","sentences":["Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities.","Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions.","However, existing benchmarks predominantly access agents in single-mission scenarios, failing to capture real-world complexity.","To bridge this gap, we propose the Multi-Mission Tool Bench.","In the benchmark, each test case comprises multiple interrelated missions.","This design requires agents to dynamically adapt to evolving demands.","Moreover, the proposed benchmark explores all possible mission-switching patterns within a fixed mission number.","Specifically, we propose a multi-agent data generation framework to construct the benchmark.","We also propose a novel method to evaluate the accuracy and efficiency of agent decisions with dynamic decision trees.","Experiments on diverse open-source and closed-source LLMs reveal critical factors influencing agent robustness and provide actionable insights to the tool invocation society."],"url":"http://arxiv.org/abs/2504.02623v1"}
{"created":"2025-04-03 14:09:17","title":"Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic RBF Networks","abstract":"The practical deployment of learning-based autonomous systems would greatly benefit from tools that flexibly obtain safety guarantees in the form of certificate functions from data. While the geometrical properties of such certificate functions are well understood, synthesizing them using machine learning techniques still remains a challenge. To mitigate this issue, we propose a diffeomorphic function learning framework where prior structural knowledge of the desired output is encoded in the geometry of a simple surrogate function, which is subsequently augmented through an expressive, topology-preserving state-space transformation. Thereby, we achieve an indirect function approximation framework that is guaranteed to remain in the desired hypothesis space. To this end, we introduce a novel approach to construct diffeomorphic maps based on RBF networks, which facilitate precise, local transformations around data. Finally, we demonstrate our approach by learning diffeomorphic Lyapunov functions from real-world data and apply our method to different attractor systems.","sentences":["The practical deployment of learning-based autonomous systems would greatly benefit from tools that flexibly obtain safety guarantees in the form of certificate functions from data.","While the geometrical properties of such certificate functions are well understood, synthesizing them using machine learning techniques still remains a challenge.","To mitigate this issue, we propose a diffeomorphic function learning framework where prior structural knowledge of the desired output is encoded in the geometry of a simple surrogate function, which is subsequently augmented through an expressive, topology-preserving state-space transformation.","Thereby, we achieve an indirect function approximation framework that is guaranteed to remain in the desired hypothesis space.","To this end, we introduce a novel approach to construct diffeomorphic maps based on RBF networks, which facilitate precise, local transformations around data.","Finally, we demonstrate our approach by learning diffeomorphic Lyapunov functions from real-world data and apply our method to different attractor systems."],"url":"http://arxiv.org/abs/2504.02607v1"}
{"created":"2025-04-03 14:06:17","title":"Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving","abstract":"The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.","sentences":["The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue.","However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems.","To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++.","It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation.","Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights.","In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks.","As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain.","More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset.","We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI."],"url":"http://arxiv.org/abs/2504.02605v1"}
{"created":"2025-04-03 14:05:56","title":"LinTO Audio and Textual Datasets to Train and Evaluate Automatic Speech Recognition in Tunisian Arabic Dialect","abstract":"Developing Automatic Speech Recognition (ASR) systems for Tunisian Arabic Dialect is challenging due to the dialect's linguistic complexity and the scarcity of annotated speech datasets. To address these challenges, we propose the LinTO audio and textual datasets -- comprehensive resources that capture phonological and lexical features of Tunisian Arabic Dialect. These datasets include a variety of texts from numerous sources and real-world audio samples featuring diverse speakers and code-switching between Tunisian Arabic Dialect and English or French. By providing high-quality audio paired with precise transcriptions, the LinTO audio and textual datasets aim to provide qualitative material to build and benchmark ASR systems for the Tunisian Arabic Dialect.   Keywords -- Tunisian Arabic Dialect, Speech-to-Text, Low-Resource Languages, Audio Data Augmentation","sentences":["Developing Automatic Speech Recognition (ASR) systems for Tunisian Arabic Dialect is challenging due to the dialect's linguistic complexity and the scarcity of annotated speech datasets.","To address these challenges, we propose the LinTO audio and textual datasets -- comprehensive resources that capture phonological and lexical features of Tunisian Arabic Dialect.","These datasets include a variety of texts from numerous sources and real-world audio samples featuring diverse speakers and code-switching between Tunisian Arabic Dialect and English or French.","By providing high-quality audio paired with precise transcriptions, the LinTO audio and textual datasets aim to provide qualitative material to build and benchmark ASR systems for the Tunisian Arabic Dialect.   ","Keywords -- Tunisian Arabic Dialect, Speech-to-Text, Low-Resource Languages, Audio Data Augmentation"],"url":"http://arxiv.org/abs/2504.02604v1"}
{"created":"2025-04-03 14:00:52","title":"Graphs are everywhere -- Psst! In Music Recommendation too","abstract":"In recent years, graphs have gained prominence across various domains, especially in recommendation systems. Within the realm of music recommendation, graphs play a crucial role in enhancing genre-based recommendations by integrating Mel-Frequency Cepstral Coefficients (MFCC) with advanced graph embeddings. This study explores the efficacy of Graph Convolutional Networks (GCN), GraphSAGE, and Graph Transformer (GT) models in learning embeddings that effectively capture intricate relationships between music items and genres represented within graph structures. Through comprehensive empirical evaluations on diverse real-world music datasets, our findings consistently demonstrate that these graph-based approaches outperform traditional methods that rely solely on MFCC features or collaborative filtering techniques. Specifically, the graph-enhanced models achieve notably higher accuracy in predicting genre-specific preferences and offering relevant music suggestions to users. These results underscore the effectiveness of utilizing graph embeddings to enrich feature representations and exploit latent associations within music data, thereby illustrating their potential to advance the capabilities of personalized and context-aware music recommendation systems. Keywords: graphs, recommendation systems, neural networks, MFCC","sentences":["In recent years, graphs have gained prominence across various domains, especially in recommendation systems.","Within the realm of music recommendation, graphs play a crucial role in enhancing genre-based recommendations by integrating Mel-Frequency Cepstral Coefficients (MFCC) with advanced graph embeddings.","This study explores the efficacy of Graph Convolutional Networks (GCN), GraphSAGE, and Graph Transformer (GT) models in learning embeddings that effectively capture intricate relationships between music items and genres represented within graph structures.","Through comprehensive empirical evaluations on diverse real-world music datasets, our findings consistently demonstrate that these graph-based approaches outperform traditional methods that rely solely on MFCC features or collaborative filtering techniques.","Specifically, the graph-enhanced models achieve notably higher accuracy in predicting genre-specific preferences and offering relevant music suggestions to users.","These results underscore the effectiveness of utilizing graph embeddings to enrich feature representations and exploit latent associations within music data, thereby illustrating their potential to advance the capabilities of personalized and context-aware music recommendation systems.","Keywords: graphs, recommendation systems, neural networks, MFCC"],"url":"http://arxiv.org/abs/2504.02598v1"}
{"created":"2025-04-03 13:54:43","title":"Knowledge Graph Completion with Mixed Geometry Tensor Factorization","abstract":"In this paper, we propose a new geometric approach for knowledge graph completion via low rank tensor approximation. We augment a pretrained and well-established Euclidean model based on a Tucker tensor decomposition with a novel hyperbolic interaction term. This correction enables more nuanced capturing of distributional properties in data better aligned with real-world knowledge graphs. By combining two geometries together, our approach improves expressivity of the resulting model achieving new state-of-the-art link prediction accuracy with a significantly lower number of parameters compared to the previous Euclidean and hyperbolic models.","sentences":["In this paper, we propose a new geometric approach for knowledge graph completion via low rank tensor approximation.","We augment a pretrained and well-established Euclidean model based on a Tucker tensor decomposition with a novel hyperbolic interaction term.","This correction enables more nuanced capturing of distributional properties in data better aligned with real-world knowledge graphs.","By combining two geometries together, our approach improves expressivity of the resulting model achieving new state-of-the-art link prediction accuracy with a significantly lower number of parameters compared to the previous Euclidean and hyperbolic models."],"url":"http://arxiv.org/abs/2504.02589v1"}
{"created":"2025-04-03 13:53:28","title":"Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme","abstract":"Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.","sentences":["Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs).","However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics.","This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets.","In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors.","Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data.","These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research."],"url":"http://arxiv.org/abs/2504.02587v1"}
{"created":"2025-04-03 13:40:55","title":"Reasoning Inconsistencies and How to Mitigate Them in Deep Learning","abstract":"The recent advancements in Deep Learning models and techniques have led to significant strides in performance across diverse tasks and modalities. However, while the overall capabilities of models show promising growth, our understanding of their internal reasoning processes remains limited, particularly concerning systematic inconsistencies or errors patterns of logical or inferential flaws. These inconsistencies may manifest as contradictory outputs, failure to generalize across similar tasks, or erroneous conclusions in specific contexts. Even detecting and measuring such reasoning discrepancies is challenging, as they may arise from opaque internal procedures, biases and imbalances in training data, or the inherent complexity of the task. Without effective methods to detect, measure, and mitigate these errors, there is a risk of deploying models that are biased, exploitable, or logically unreliable. This thesis aims to address these issues by producing novel methods for deep learning models that reason over knowledge graphs, natural language, and images. The thesis contributes two techniques for detecting and quantifying predictive inconsistencies originating from opaque internal procedures in natural language and image processing models. To mitigate inconsistencies from biases in training data, this thesis presents a data efficient sampling method to improve fairness and performance and a synthetic dataset generation approach in low resource scenarios. Finally, the thesis offers two techniques to optimize the models for complex reasoning tasks. These methods enhance model performance while allowing for more faithful and interpretable exploration and exploitation during inference. Critically, this thesis provides a comprehensive framework to improve the robustness, fairness, and interpretability of deep learning models across diverse tasks and modalities.","sentences":["The recent advancements in Deep Learning models and techniques have led to significant strides in performance across diverse tasks and modalities.","However, while the overall capabilities of models show promising growth, our understanding of their internal reasoning processes remains limited, particularly concerning systematic inconsistencies or errors patterns of logical or inferential flaws.","These inconsistencies may manifest as contradictory outputs, failure to generalize across similar tasks, or erroneous conclusions in specific contexts.","Even detecting and measuring such reasoning discrepancies is challenging, as they may arise from opaque internal procedures, biases and imbalances in training data, or the inherent complexity of the task.","Without effective methods to detect, measure, and mitigate these errors, there is a risk of deploying models that are biased, exploitable, or logically unreliable.","This thesis aims to address these issues by producing novel methods for deep learning models that reason over knowledge graphs, natural language, and images.","The thesis contributes two techniques for detecting and quantifying predictive inconsistencies originating from opaque internal procedures in natural language and image processing models.","To mitigate inconsistencies from biases in training data, this thesis presents a data efficient sampling method to improve fairness and performance and a synthetic dataset generation approach in low resource scenarios.","Finally, the thesis offers two techniques to optimize the models for complex reasoning tasks.","These methods enhance model performance while allowing for more faithful and interpretable exploration and exploitation during inference.","Critically, this thesis provides a comprehensive framework to improve the robustness, fairness, and interpretability of deep learning models across diverse tasks and modalities."],"url":"http://arxiv.org/abs/2504.02577v1"}
{"created":"2025-04-03 13:37:45","title":"Language Models reach higher Agreement than Humans in Historical Interpretation","abstract":"This paper compares historical annotations by humans and Large Language Models. The findings reveal that both exhibit some cultural bias, but Large Language Models achieve a higher consensus on the interpretation of historical facts from short texts. While humans tend to disagree on the basis of their personal biases, Large Models disagree when they skip information or produce hallucinations. These findings have significant implications for digital humanities, enabling large-scale annotation and quantitative analysis of historical data. This offers new educational and research opportunities to explore historical interpretations from different Language Models, fostering critical thinking about bias.","sentences":["This paper compares historical annotations by humans and Large Language Models.","The findings reveal that both exhibit some cultural bias, but Large Language Models achieve a higher consensus on the interpretation of historical facts from short texts.","While humans tend to disagree on the basis of their personal biases, Large Models disagree when they skip information or produce hallucinations.","These findings have significant implications for digital humanities, enabling large-scale annotation and quantitative analysis of historical data.","This offers new educational and research opportunities to explore historical interpretations from different Language Models, fostering critical thinking about bias."],"url":"http://arxiv.org/abs/2504.02572v1"}
{"created":"2025-04-03 13:15:45","title":"L-LBVC: Long-Term Motion Estimation and Prediction for Learned Bi-Directional Video Compression","abstract":"Recently, learned video compression (LVC) has shown superior performance under low-delay configuration. However, the performance of learned bi-directional video compression (LBVC) still lags behind traditional bi-directional coding. The performance gap mainly arises from inaccurate long-term motion estimation and prediction of distant frames, especially in large motion scenes. To solve these two critical problems, this paper proposes a novel LBVC framework, namely L-LBVC. Firstly, we propose an adaptive motion estimation module that can handle both short-term and long-term motions. Specifically, we directly estimate the optical flows for adjacent frames and non-adjacent frames with small motions. For non-adjacent frames with large motions, we recursively accumulate local flows between adjacent frames to estimate long-term flows. Secondly, we propose an adaptive motion prediction module that can largely reduce the bit cost for motion coding. To improve the accuracy of long-term motion prediction, we adaptively downsample reference frames during testing to match the motion ranges observed during training. Experiments show that our L-LBVC significantly outperforms previous state-of-the-art LVC methods and even surpasses VVC (VTM) on some test datasets under random access configuration.","sentences":["Recently, learned video compression (LVC) has shown superior performance under low-delay configuration.","However, the performance of learned bi-directional video compression (LBVC) still lags behind traditional bi-directional coding.","The performance gap mainly arises from inaccurate long-term motion estimation and prediction of distant frames, especially in large motion scenes.","To solve these two critical problems, this paper proposes a novel LBVC framework, namely L-LBVC.","Firstly, we propose an adaptive motion estimation module that can handle both short-term and long-term motions.","Specifically, we directly estimate the optical flows for adjacent frames and non-adjacent frames with small motions.","For non-adjacent frames with large motions, we recursively accumulate local flows between adjacent frames to estimate long-term flows.","Secondly, we propose an adaptive motion prediction module that can largely reduce the bit cost for motion coding.","To improve the accuracy of long-term motion prediction, we adaptively downsample reference frames during testing to match the motion ranges observed during training.","Experiments show that our L-LBVC significantly outperforms previous state-of-the-art LVC methods and even surpasses VVC (VTM) on some test datasets under random access configuration."],"url":"http://arxiv.org/abs/2504.02560v1"}
{"created":"2025-04-03 13:15:18","title":"Leveraging LLM For Synchronizing Information Across Multilingual Tables","abstract":"The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution. We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance. Our findings reveal that single-prompt approaches often produce suboptimal results, prompting us to introduce a task decomposition strategy that enhances coherence and accuracy. Our proposed method outperforms existing baselines, particularly in Information Updation (1.79%) and Information Addition (20.58%), highlighting the model strength in dynamically updating and enriching data across architectures","sentences":["The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French.","Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete.","Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods.","These approaches can be effective, but they struggle with complexity and generalization.","This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution.","We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance.","Our findings reveal that single-prompt approaches often produce suboptimal results, prompting us to introduce a task decomposition strategy that enhances coherence and accuracy.","Our proposed method outperforms existing baselines, particularly in Information Updation (1.79%) and Information Addition (20.58%), highlighting the model strength in dynamically updating and enriching data across architectures"],"url":"http://arxiv.org/abs/2504.02559v1"}
{"created":"2025-04-03 13:11:57","title":"Noise Calibration and Spatial-Frequency Interactive Network for STEM Image Enhancement","abstract":"Scanning Transmission Electron Microscopy (STEM) enables the observation of atomic arrangements at sub-angstrom resolution, allowing for atomically resolved analysis of the physical and chemical properties of materials. However, due to the effects of noise, electron beam damage, sample thickness, etc, obtaining satisfactory atomic-level images is often challenging. Enhancing STEM images can reveal clearer structural details of materials. Nonetheless, existing STEM image enhancement methods usually overlook unique features in the frequency domain, and existing datasets lack realism and generality. To resolve these issues, in this paper, we develop noise calibration, data synthesis, and enhancement methods for STEM images. We first present a STEM noise calibration method, which is used to synthesize more realistic STEM images. The parameters of background noise, scan noise, and pointwise noise are obtained by statistical analysis and fitting of real STEM images containing atoms. Then we use these parameters to develop a more general dataset that considers both regular and random atomic arrangements and includes both HAADF and BF mode images. Finally, we design a spatial-frequency interactive network for STEM image enhancement, which can explore the information in the frequency domain formed by the periodicity of atomic arrangement. Experimental results show that our data is closer to real STEM images and achieves better enhancement performances together with our network. Code will be available at https://github.com/HeasonLee/SFIN}{https://github.com/HeasonLee/SFIN.","sentences":["Scanning Transmission Electron Microscopy (STEM) enables the observation of atomic arrangements at sub-angstrom resolution, allowing for atomically resolved analysis of the physical and chemical properties of materials.","However, due to the effects of noise, electron beam damage, sample thickness, etc, obtaining satisfactory atomic-level images is often challenging.","Enhancing STEM images can reveal clearer structural details of materials.","Nonetheless, existing STEM image enhancement methods usually overlook unique features in the frequency domain, and existing datasets lack realism and generality.","To resolve these issues, in this paper, we develop noise calibration, data synthesis, and enhancement methods for STEM images.","We first present a STEM noise calibration method, which is used to synthesize more realistic STEM images.","The parameters of background noise, scan noise, and pointwise noise are obtained by statistical analysis and fitting of real STEM images containing atoms.","Then we use these parameters to develop a more general dataset that considers both regular and random atomic arrangements and includes both HAADF and BF mode images.","Finally, we design a spatial-frequency interactive network for STEM image enhancement, which can explore the information in the frequency domain formed by the periodicity of atomic arrangement.","Experimental results show that our data is closer to real STEM images and achieves better enhancement performances together with our network.","Code will be available at https://github.com/HeasonLee/SFIN}{https://github.com/HeasonLee/SFIN."],"url":"http://arxiv.org/abs/2504.02555v1"}
{"created":"2025-04-03 13:05:46","title":"Human-Centered Development of an Explainable AI Framework for Real-Time Surgical Risk Surveillance","abstract":"Background: Artificial Intelligence (AI) clinical decision support (CDS) systems have the potential to augment surgical risk assessments, but successful adoption depends on an understanding of end-user needs and current workflows. This study reports the initial co-design of MySurgeryRisk, an AI CDS tool to predict the risk of nine post-operative complications in surgical patients. Methods: Semi-structured focus groups and interviews were held as co-design sessions with perioperative physicians at a tertiary academic hospital in the Southeastern United States. Participants were read a surgical vignette and asked questions to elicit an understanding of their current decision-making practices before being introduced to the MySurgeryRisk prototype web interface. They were asked to provide feedback on the user interface and system features. Session transcripts were qualitatively coded, after which thematic analysis took place. Results: Data saturation was reached after 20 surgeons and anesthesiologists from varying career stages participated across 11 co-design sessions. Thematic analysis resulted in five themes: (1) decision-making cognitive processes, (2) current approach to decision-making, (3) future approach to decision-making with MySurgeryRisk, (4) feedback on current MySurgeryRisk prototype, and (5) trustworthy considerations. Conclusion: Clinical providers perceived MySurgeryRisk as a promising CDS tool that factors in a large volume of data and is computed in real-time without any need for manual input. Participants provided feedback on the design of the interface and imaged applications of the tool in the clinical workflow. However, its successful implementation will depend on its actionability and explainability of model outputs, integration into current electronic systems, and calibration of trust among end-users.","sentences":["Background: Artificial Intelligence (AI) clinical decision support (CDS) systems have the potential to augment surgical risk assessments, but successful adoption depends on an understanding of end-user needs and current workflows.","This study reports the initial co-design of MySurgeryRisk, an AI CDS tool to predict the risk of nine post-operative complications in surgical patients.","Methods: Semi-structured focus groups and interviews were held as co-design sessions with perioperative physicians at a tertiary academic hospital in the Southeastern United States.","Participants were read a surgical vignette and asked questions to elicit an understanding of their current decision-making practices before being introduced to the MySurgeryRisk prototype web interface.","They were asked to provide feedback on the user interface and system features.","Session transcripts were qualitatively coded, after which thematic analysis took place.","Results:","Data saturation was reached after 20 surgeons and anesthesiologists from varying career stages participated across 11 co-design sessions.","Thematic analysis resulted in five themes: (1) decision-making cognitive processes, (2) current approach to decision-making, (3) future approach to decision-making with MySurgeryRisk, (4) feedback on current MySurgeryRisk prototype, and (5) trustworthy considerations.","Conclusion: Clinical providers perceived MySurgeryRisk as a promising CDS tool that factors in a large volume of data and is computed in real-time without any need for manual input.","Participants provided feedback on the design of the interface and imaged applications of the tool in the clinical workflow.","However, its successful implementation will depend on its actionability and explainability of model outputs, integration into current electronic systems, and calibration of trust among end-users."],"url":"http://arxiv.org/abs/2504.02551v1"}
{"created":"2025-04-03 12:36:01","title":"ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions","abstract":"Modern data-driven applications expose limitations of von Neumann architectures - extensive data movement, low throughput, and poor energy efficiency. Accelerators improve performance but lack flexibility and require data transfers. Existing compute in- and near-memory solutions mitigate these issues but face usability challenges due to data placement constraints. We propose a novel cache architecture that doubles as a tightly-coupled compute-near-memory coprocessor. Our \\riscv cache controller executes custom instructions from the host CPU using vector operations dispatched to near-memory vector processing units within the cache memory subsystem. This architecture abstracts memory synchronization and data mapping from application software while offering software-based \\isa extensibility. Our implementation shows $30\\times$ to $84\\times$ performance improvement when operating on 8-bit data over the same system with a traditional cache when executing a worst-case 32-bit CNN workload, with only $41.3\\%$ area overhead.","sentences":["Modern data-driven applications expose limitations of von Neumann architectures - extensive data movement, low throughput, and poor energy efficiency.","Accelerators improve performance but lack flexibility and require data transfers.","Existing compute in- and near-memory solutions mitigate these issues but face usability challenges due to data placement constraints.","We propose a novel cache architecture that doubles as a tightly-coupled compute-near-memory coprocessor.","Our \\riscv cache controller executes custom instructions from the host CPU using vector operations dispatched to near-memory vector processing units within the cache memory subsystem.","This architecture abstracts memory synchronization and data mapping from application software while offering software-based \\isa extensibility.","Our implementation shows $30\\times$ to $84\\times$ performance improvement when operating on 8-bit data over the same system with a traditional cache when executing a worst-case 32-bit CNN workload, with only $41.3\\%$ area overhead."],"url":"http://arxiv.org/abs/2504.02533v1"}
{"created":"2025-04-03 12:28:21","title":"SelfMedHPM: Self Pre-training With Hard Patches Mining Masked Autoencoders For Medical Image Segmentation","abstract":"In recent years, deep learning methods such as convolutional neural network (CNN) and transformers have made significant progress in CT multi-organ segmentation. However, CT multi-organ segmentation methods based on masked image modeling (MIM) are very limited. There are already methods using MAE for CT multi-organ segmentation task, we believe that the existing methods do not identify the most difficult areas to reconstruct. To this end, we propose a MIM self-training framework with hard patches mining masked autoencoders for CT multi-organ segmentation tasks (selfMedHPM). The method performs ViT self-pretraining on the training set of the target data and introduces an auxiliary loss predictor, which first predicts the patch loss and determines the location of the next mask. SelfMedHPM implementation is better than various competitive methods in abdominal CT multi-organ segmentation and body CT multi-organ segmentation. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for abdomen mult-organ segmentation and the SinoMed Whole Body (SMWB) dataset for body multi-organ segmentation tasks.","sentences":["In recent years, deep learning methods such as convolutional neural network (CNN) and transformers have made significant progress in CT multi-organ segmentation.","However, CT multi-organ segmentation methods based on masked image modeling (MIM) are very limited.","There are already methods using MAE for CT multi-organ segmentation task, we believe that the existing methods do not identify the most difficult areas to reconstruct.","To this end, we propose a MIM self-training framework with hard patches mining masked autoencoders for CT multi-organ segmentation tasks (selfMedHPM).","The method performs ViT self-pretraining on the training set of the target data and introduces an auxiliary loss predictor, which first predicts the patch loss and determines the location of the next mask.","SelfMedHPM implementation is better than various competitive methods in abdominal CT multi-organ segmentation and body CT multi-organ segmentation.","We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for abdomen mult-organ segmentation and the SinoMed Whole Body (SMWB) dataset for body multi-organ segmentation tasks."],"url":"http://arxiv.org/abs/2504.02524v1"}
{"created":"2025-04-03 12:18:51","title":"UNDO: Understanding Distillation as Optimization","abstract":"Knowledge distillation has emerged as an effective strategy for compressing large language models' (LLMs) knowledge into smaller, more efficient student models. However, standard one-shot distillation methods often produce suboptimal results due to a mismatch between teacher-generated rationales and the student's specific learning requirements. In this paper, we introduce the UNDO: UNderstanding Distillation as Optimization framework, designed to bridge this gap by iteratively identifying the student's errors and prompting the teacher to refine its explanations accordingly. Each iteration directly targets the student's learning deficiencies, motivating the teacher to provide tailored and enhanced rationales that specifically address these weaknesses. Empirical evaluations on various challenging mathematical and commonsense reasoning tasks demonstrate that our iterative distillation method, UNDO, significantly outperforms standard one-step distillation methods, achieving performance gains of up to 20%. Additionally, we show that teacher-generated data refined through our iterative process remains effective even when applied to different student models, underscoring the broad applicability of our approach. Our work fundamentally reframes knowledge distillation as an iterative teacher-student interaction, effectively leveraging dynamic refinement by the teacher for better knowledge distillation.","sentences":["Knowledge distillation has emerged as an effective strategy for compressing large language models' (LLMs) knowledge into smaller, more efficient student models.","However, standard one-shot distillation methods often produce suboptimal results due to a mismatch between teacher-generated rationales and the student's specific learning requirements.","In this paper, we introduce the UNDO: UNderstanding Distillation as Optimization framework, designed to bridge this gap by iteratively identifying the student's errors and prompting the teacher to refine its explanations accordingly.","Each iteration directly targets the student's learning deficiencies, motivating the teacher to provide tailored and enhanced rationales that specifically address these weaknesses.","Empirical evaluations on various challenging mathematical and commonsense reasoning tasks demonstrate that our iterative distillation method, UNDO, significantly outperforms standard one-step distillation methods, achieving performance gains of up to 20%.","Additionally, we show that teacher-generated data refined through our iterative process remains effective even when applied to different student models, underscoring the broad applicability of our approach.","Our work fundamentally reframes knowledge distillation as an iterative teacher-student interaction, effectively leveraging dynamic refinement by the teacher for better knowledge distillation."],"url":"http://arxiv.org/abs/2504.02521v1"}
{"created":"2025-04-03 12:13:38","title":"Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework","abstract":"This paper presents novel Machine Learning (ML) methodologies for Multi-Object Tracking (MOT), specifically designed to meet the increasing complexity and precision demands of Advanced Driver Assistance Systems (ADAS). We introduce three Neural Network (NN) models that address key challenges in MOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii) the Single-Association Network (SANT) for mapping individual Sensor Object (SO) to existing tracks, and (iii) the Multi-Association Network (MANTa) for associating multiple SOs to multiple tracks. These models are seamlessly integrated into a traditional Kalman Filter (KF) framework, maintaining the system's modularity by replacing relevant components without disrupting the overall architecture. Importantly, all three networks are designed to be run in a realtime, embedded environment. Each network contains less than 50k trainable parameters. Our evaluation, conducted on the public KITTI tracking dataset, demonstrates significant improvements in tracking performance. SPENT reduces the Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT and MANTa achieve up to 95% accuracy in sensor object-to-track assignments. These results underscore the effectiveness of incorporating task-specific NNs into traditional tracking systems, boosting performance and robustness while preserving modularity, maintainability, and interpretability.","sentences":["This paper presents novel Machine Learning (ML) methodologies for Multi-Object Tracking (MOT), specifically designed to meet the increasing complexity and precision demands of Advanced Driver Assistance Systems (ADAS).","We introduce three Neural Network (NN) models that address key challenges in MOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii) the Single-Association Network (SANT) for mapping individual Sensor Object (SO) to existing tracks, and (iii) the Multi-Association Network (MANTa) for associating multiple SOs to multiple tracks.","These models are seamlessly integrated into a traditional Kalman Filter (KF) framework, maintaining the system's modularity by replacing relevant components without disrupting the overall architecture.","Importantly, all three networks are designed to be run in a realtime, embedded environment.","Each network contains less than 50k trainable parameters.","Our evaluation, conducted on the public KITTI tracking dataset, demonstrates significant improvements in tracking performance.","SPENT reduces the Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT and MANTa achieve up to 95% accuracy in sensor object-to-track assignments.","These results underscore the effectiveness of incorporating task-specific NNs into traditional tracking systems, boosting performance and robustness while preserving modularity, maintainability, and interpretability."],"url":"http://arxiv.org/abs/2504.02519v1"}
{"created":"2025-04-03 12:01:41","title":"Exploration-Driven Generative Interactive Environments","abstract":"Modern world models require costly and time-consuming collection of large video datasets with action demonstrations by people or by environment-specific agents. To simplify training, we focus on using many virtual environments for inexpensive, automatically collected interaction data. Genie, a recent multi-environment world model, demonstrates simulation abilities of many environments with shared behavior. Unfortunately, training their model requires expensive demonstrations. Therefore, we propose a training framework merely using a random agent in virtual environments. While the model trained in this manner exhibits good controls, it is limited by the random exploration possibilities. To address this limitation, we propose AutoExplore Agent - an exploration agent that entirely relies on the uncertainty of the world model, delivering diverse data from which it can learn the best. Our agent is fully independent of environment-specific rewards and thus adapts easily to new environments. With this approach, the pretrained multi-environment model can quickly adapt to new environments achieving video fidelity and controllability improvement. In order to obtain automatically large-scale interaction datasets for pretraining, we group environments with similar behavior and controls. To this end, we annotate the behavior and controls of 974 virtual environments - a dataset that we name RetroAct. For building our model, we first create an open implementation of Genie - GenieRedux and apply enhancements and adaptations in our version GenieRedux-G. Our code and data are available at https://github.com/insait-institute/GenieRedux.","sentences":["Modern world models require costly and time-consuming collection of large video datasets with action demonstrations by people or by environment-specific agents.","To simplify training, we focus on using many virtual environments for inexpensive, automatically collected interaction data.","Genie, a recent multi-environment world model, demonstrates simulation abilities of many environments with shared behavior.","Unfortunately, training their model requires expensive demonstrations.","Therefore, we propose a training framework merely using a random agent in virtual environments.","While the model trained in this manner exhibits good controls, it is limited by the random exploration possibilities.","To address this limitation, we propose AutoExplore Agent - an exploration agent that entirely relies on the uncertainty of the world model, delivering diverse data from which it can learn the best.","Our agent is fully independent of environment-specific rewards and thus adapts easily to new environments.","With this approach, the pretrained multi-environment model can quickly adapt to new environments achieving video fidelity and controllability improvement.","In order to obtain automatically large-scale interaction datasets for pretraining, we group environments with similar behavior and controls.","To this end, we annotate the behavior and controls of 974 virtual environments - a dataset that we name RetroAct.","For building our model, we first create an open implementation of Genie - GenieRedux and apply enhancements and adaptations in our version GenieRedux-G. Our code and data are available at https://github.com/insait-institute/GenieRedux."],"url":"http://arxiv.org/abs/2504.02515v1"}
{"created":"2025-04-03 11:41:55","title":"ZClip: Adaptive Spike Mitigation for LLM Pre-Training","abstract":"Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.","sentences":["Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes.","These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping.","Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention.","In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time.","Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms.","At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise.","Our code is available at: https://github.com/bluorion-com/ZClip."],"url":"http://arxiv.org/abs/2504.02507v1"}
{"created":"2025-04-03 11:20:49","title":"VISTA: Unsupervised 2D Temporal Dependency Representations for Time Series Anomaly Detection","abstract":"Time Series Anomaly Detection (TSAD) is essential for uncovering rare and potentially harmful events in unlabeled time series data. Existing methods are highly dependent on clean, high-quality inputs, making them susceptible to noise and real-world imperfections. Additionally, intricate temporal relationships in time series data are often inadequately captured in traditional 1D representations, leading to suboptimal modeling of dependencies. We introduce VISTA, a training-free, unsupervised TSAD algorithm designed to overcome these challenges. VISTA features three core modules: 1) Time Series Decomposition using Seasonal and Trend Decomposition via Loess (STL) to decompose noisy time series into trend, seasonal, and residual components; 2) Temporal Self-Attention, which transforms 1D time series into 2D temporal correlation matrices for richer dependency modeling and anomaly detection; and 3) Multivariate Temporal Aggregation, which uses a pretrained feature extractor to integrate cross-variable information into a unified, memory-efficient representation. VISTA's training-free approach enables rapid deployment and easy hyperparameter tuning, making it suitable for industrial applications. It achieves state-of-the-art performance on five multivariate TSAD benchmarks.","sentences":["Time Series Anomaly Detection (TSAD) is essential for uncovering rare and potentially harmful events in unlabeled time series data.","Existing methods are highly dependent on clean, high-quality inputs, making them susceptible to noise and real-world imperfections.","Additionally, intricate temporal relationships in time series data are often inadequately captured in traditional 1D representations, leading to suboptimal modeling of dependencies.","We introduce VISTA, a training-free, unsupervised TSAD algorithm designed to overcome these challenges.","VISTA features three core modules: 1) Time Series Decomposition using Seasonal and Trend Decomposition via Loess (STL) to decompose noisy time series into trend, seasonal, and residual components; 2) Temporal Self-Attention, which transforms 1D time series into 2D temporal correlation matrices for richer dependency modeling and anomaly detection; and 3) Multivariate Temporal Aggregation, which uses a pretrained feature extractor to integrate cross-variable information into a unified, memory-efficient representation.","VISTA's training-free approach enables rapid deployment and easy hyperparameter tuning, making it suitable for industrial applications.","It achieves state-of-the-art performance on five multivariate TSAD benchmarks."],"url":"http://arxiv.org/abs/2504.02498v1"}
{"created":"2025-04-03 11:18:00","title":"Semiconductor Wafer Map Defect Classification with Tiny Vision Transformers","abstract":"Semiconductor wafer defect classification is critical for ensuring high precision and yield in manufacturing. Traditional CNN-based models often struggle with class imbalances and recognition of the multiple overlapping defect types in wafer maps. To address these challenges, we propose ViT-Tiny, a lightweight Vision Transformer (ViT) framework optimized for wafer defect classification. Trained on the WM-38k dataset. ViT-Tiny outperforms its ViT-Base counterpart and state-of-the-art (SOTA) models, such as MSF-Trans and CNN-based architectures. Through extensive ablation studies, we determine that a patch size of 16 provides optimal performance. ViT-Tiny achieves an F1-score of 98.4%, surpassing MSF-Trans by 2.94% in four-defect classification, improving recall by 2.86% in two-defect classification, and increasing precision by 3.13% in three-defect classification. Additionally, it demonstrates enhanced robustness under limited labeled data conditions, making it a computationally efficient and reliable solution for real-world semiconductor defect detection.","sentences":["Semiconductor wafer defect classification is critical for ensuring high precision and yield in manufacturing.","Traditional CNN-based models often struggle with class imbalances and recognition of the multiple overlapping defect types in wafer maps.","To address these challenges, we propose ViT-Tiny, a lightweight Vision Transformer (ViT) framework optimized for wafer defect classification.","Trained on the WM-38k dataset.","ViT-Tiny outperforms its ViT-Base counterpart and state-of-the-art (SOTA) models, such as MSF-Trans and CNN-based architectures.","Through extensive ablation studies, we determine that a patch size of 16 provides optimal performance.","ViT-Tiny achieves an F1-score of 98.4%, surpassing MSF-Trans by 2.94% in four-defect classification, improving recall by 2.86% in two-defect classification, and increasing precision by 3.13% in three-defect classification.","Additionally, it demonstrates enhanced robustness under limited labeled data conditions, making it a computationally efficient and reliable solution for real-world semiconductor defect detection."],"url":"http://arxiv.org/abs/2504.02494v1"}
{"created":"2025-04-03 11:15:10","title":"Industrial Internet Robot Collaboration System and Edge Computing Optimization","abstract":"In a complex environment, for a mobile robot to safely and collision - free avoid all obstacles, it poses high requirements for its intelligence level. Given that the information such as the position and geometric characteristics of obstacles is random, the control parameters of the robot, such as velocity and angular velocity, are also prone to random deviations. To address this issue in the framework of the Industrial Internet Robot Collaboration System, this paper proposes a global path control scheme for mobile robots based on deep learning. First of all, the dynamic equation of the mobile robot is established. According to the linear velocity and angular velocity of the mobile robot, its motion behaviors are divided into obstacle - avoidance behavior, target - turning behavior, and target approaching behavior. Subsequently, the neural network method in deep learning is used to build a global path planning model for the robot. On this basis, a fuzzy controller is designed with the help of a fuzzy control algorithm to correct the deviations that occur during path planning, thereby achieving optimized control of the robot's global path. In addition, considering edge computing optimization, the proposed model can process local data at the edge device, reducing the communication burden between the robot and the central server, and improving the real time performance of path planning. The experimental results show that for the mobile robot controlled by the research method in this paper, the deviation distance of the path angle is within 5 cm, the deviation convergence can be completed within 10 ms, and the planned path is shorter. This indicates that the proposed scheme can effectively improve the global path planning ability of mobile robots in the industrial Internet environment and promote the collaborative operation of robots through edge computing optimization.","sentences":["In a complex environment, for a mobile robot to safely and collision - free avoid all obstacles, it poses high requirements for its intelligence level.","Given that the information such as the position and geometric characteristics of obstacles is random, the control parameters of the robot, such as velocity and angular velocity, are also prone to random deviations.","To address this issue in the framework of the Industrial Internet Robot Collaboration System, this paper proposes a global path control scheme for mobile robots based on deep learning.","First of all, the dynamic equation of the mobile robot is established.","According to the linear velocity and angular velocity of the mobile robot, its motion behaviors are divided into obstacle - avoidance behavior, target - turning behavior, and target approaching behavior.","Subsequently, the neural network method in deep learning is used to build a global path planning model for the robot.","On this basis, a fuzzy controller is designed with the help of a fuzzy control algorithm to correct the deviations that occur during path planning, thereby achieving optimized control of the robot's global path.","In addition, considering edge computing optimization, the proposed model can process local data at the edge device, reducing the communication burden between the robot and the central server, and improving the real time performance of path planning.","The experimental results show that for the mobile robot controlled by the research method in this paper, the deviation distance of the path angle is within 5 cm, the deviation convergence can be completed within 10 ms, and the planned path is shorter.","This indicates that the proposed scheme can effectively improve the global path planning ability of mobile robots in the industrial Internet environment and promote the collaborative operation of robots through edge computing optimization."],"url":"http://arxiv.org/abs/2504.02492v1"}
{"created":"2025-04-03 11:13:31","title":"The Self-Learning Agent with a Progressive Neural Network Integrated Transformer","abstract":"This paper introduces a self-learning agent that integrates LLaMA 3.2 with a Progressive Neural Network (PNN) for continual learning in conversational AI and code generation. The framework dynamically collects data, fine-tunes tasks with minimal samples, and leverages Meta-Learning for rapid adaptation. LoRA optimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances knowledge retention. Experimental results demonstrate improved adaptability and memory stability, positioning this approach as a scalable step toward Artificial General Intelligence (AGI).","sentences":["This paper introduces a self-learning agent that integrates LLaMA 3.2 with a Progressive Neural Network (PNN) for continual learning in conversational AI and code generation.","The framework dynamically collects data, fine-tunes tasks with minimal samples, and leverages Meta-Learning for rapid adaptation.","LoRA optimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances knowledge retention.","Experimental results demonstrate improved adaptability and memory stability, positioning this approach as a scalable step toward Artificial General Intelligence (AGI)."],"url":"http://arxiv.org/abs/2504.02489v1"}
{"created":"2025-04-03 11:07:52","title":"We Need Improved Data Curation and Attribution in AI for Scientific Discovery","abstract":"As the interplay between human-generated and synthetic data evolves, new challenges arise in scientific discovery concerning the integrity of the data and the stability of the models. In this work, we examine the role of synthetic data as opposed to that of real experimental data for scientific research. Our analyses indicate that nearly three-quarters of experimental datasets available on open-access platforms have relatively low adoption rates, opening new opportunities to enhance their discoverability and usability by automated methods. Additionally, we observe an increasing difficulty in distinguishing synthetic from real experimental data. We propose supplementing ongoing efforts in automating synthetic data detection by increasing the focus on watermarking real experimental data, thereby strengthening data traceability and integrity. Our estimates suggest that watermarking even less than half of the real world data generated annually could help sustain model robustness, while promoting a balanced integration of synthetic and human-generated content.","sentences":["As the interplay between human-generated and synthetic data evolves, new challenges arise in scientific discovery concerning the integrity of the data and the stability of the models.","In this work, we examine the role of synthetic data as opposed to that of real experimental data for scientific research.","Our analyses indicate that nearly three-quarters of experimental datasets available on open-access platforms have relatively low adoption rates, opening new opportunities to enhance their discoverability and usability by automated methods.","Additionally, we observe an increasing difficulty in distinguishing synthetic from real experimental data.","We propose supplementing ongoing efforts in automating synthetic data detection by increasing the focus on watermarking real experimental data, thereby strengthening data traceability and integrity.","Our estimates suggest that watermarking even less than half of the real world data generated annually could help sustain model robustness, while promoting a balanced integration of synthetic and human-generated content."],"url":"http://arxiv.org/abs/2504.02486v1"}
{"created":"2025-04-03 10:57:26","title":"Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging","abstract":"Single-photon Lidar imaging offers a significant advantage in 3D imaging due to its high resolution and long-range capabilities, however it is challenging to apply in noisy environments with multiple targets per pixel. To tackle these challenges, several methods have been proposed. Statistical methods demonstrate interpretability on the inferred parameters, but they are often limited in their ability to handle complex scenes. Deep learning-based methods have shown superior performance in terms of accuracy and robustness, but they lack interpretability or they are limited to a single-peak per pixel. In this paper, we propose a deep unrolling algorithm for dual-peak single-photon Lidar imaging. We introduce a hierarchical Bayesian model for multiple targets and propose a neural network that unrolls the underlying statistical method. To support multiple targets, we adopt a dual depth maps representation and exploit geometric deep learning to extract features from the point cloud. The proposed method takes advantages of statistical methods and learning-based methods in terms of accuracy and quantifying uncertainty. The experimental results on synthetic and real data demonstrate the competitive performance when compared to existing methods, while also providing uncertainty information.","sentences":["Single-photon Lidar imaging offers a significant advantage in 3D imaging due to its high resolution and long-range capabilities, however it is challenging to apply in noisy environments with multiple targets per pixel.","To tackle these challenges, several methods have been proposed.","Statistical methods demonstrate interpretability on the inferred parameters, but they are often limited in their ability to handle complex scenes.","Deep learning-based methods have shown superior performance in terms of accuracy and robustness, but they lack interpretability or they are limited to a single-peak per pixel.","In this paper, we propose a deep unrolling algorithm for dual-peak single-photon Lidar imaging.","We introduce a hierarchical Bayesian model for multiple targets and propose a neural network that unrolls the underlying statistical method.","To support multiple targets, we adopt a dual depth maps representation and exploit geometric deep learning to extract features from the point cloud.","The proposed method takes advantages of statistical methods and learning-based methods in terms of accuracy and quantifying uncertainty.","The experimental results on synthetic and real data demonstrate the competitive performance when compared to existing methods, while also providing uncertainty information."],"url":"http://arxiv.org/abs/2504.02480v1"}
{"created":"2025-04-03 10:47:25","title":"Semantic segmentation of forest stands using deep learning","abstract":"Forest stands are the fundamental units in forest management inventories, silviculture, and financial analysis within operational forestry. Over the past two decades, a common method for mapping stand borders has involved delineation through manual interpretation of stereographic aerial images. This is a time-consuming and subjective process, limiting operational efficiency and introducing inconsistencies. Substantial effort has been devoted to automating the process, using various algorithms together with aerial images and canopy height models constructed from airborne laser scanning (ALS) data, but manual interpretation remains the preferred method. Deep learning (DL) methods have demonstrated great potential in computer vision, yet their application to forest stand delineation remains unexplored in published research. This study presents a novel approach, framing stand delineation as a multiclass segmentation problem and applying a U-Net based DL framework. The model was trained and evaluated using multispectral images, ALS data, and an existing stand map created by an expert interpreter. Performance was assessed on independent data using overall accuracy, a standard metric for classification tasks that measures the proportions of correctly classified pixels. The model achieved an overall accuracy of 0.73. These results demonstrate strong potential for DL in automated stand delineation. However, a few key challenges were noted, especially for complex forest environments.","sentences":["Forest stands are the fundamental units in forest management inventories, silviculture, and financial analysis within operational forestry.","Over the past two decades, a common method for mapping stand borders has involved delineation through manual interpretation of stereographic aerial images.","This is a time-consuming and subjective process, limiting operational efficiency and introducing inconsistencies.","Substantial effort has been devoted to automating the process, using various algorithms together with aerial images and canopy height models constructed from airborne laser scanning (ALS) data, but manual interpretation remains the preferred method.","Deep learning (DL) methods have demonstrated great potential in computer vision, yet their application to forest stand delineation remains unexplored in published research.","This study presents a novel approach, framing stand delineation as a multiclass segmentation problem and applying a U-Net based DL framework.","The model was trained and evaluated using multispectral images, ALS data, and an existing stand map created by an expert interpreter.","Performance was assessed on independent data using overall accuracy, a standard metric for classification tasks that measures the proportions of correctly classified pixels.","The model achieved an overall accuracy of 0.73.","These results demonstrate strong potential for DL in automated stand delineation.","However, a few key challenges were noted, especially for complex forest environments."],"url":"http://arxiv.org/abs/2504.02471v1"}
{"created":"2025-04-03 10:38:45","title":"BOOST: Bootstrapping Strategy-Driven Reasoning Programs for Program-Guided Fact-Checking","abstract":"Program-guided reasoning has shown promise in complex claim fact-checking by decomposing claims into function calls and executing reasoning programs. However, prior work primarily relies on few-shot in-context learning (ICL) with ad-hoc demonstrations, which limit program diversity and require manual design with substantial domain knowledge. Fundamentally, the underlying principles of effective reasoning program generation still remain underexplored, making it challenging to construct effective demonstrations. To address this, we propose BOOST, a bootstrapping-based framework for few-shot reasoning program generation. BOOST explicitly integrates claim decomposition and information-gathering strategies as structural guidance for program generation, iteratively refining bootstrapped demonstrations in a strategy-driven and data-centric manner without human intervention. This enables a seamless transition from zero-shot to few-shot strategic program-guided learning, enhancing interpretability and effectiveness. Experimental results show that BOOST outperforms prior few-shot baselines in both zero-shot and few-shot settings for complex claim verification.","sentences":["Program-guided reasoning has shown promise in complex claim fact-checking by decomposing claims into function calls and executing reasoning programs.","However, prior work primarily relies on few-shot in-context learning (ICL) with ad-hoc demonstrations, which limit program diversity and require manual design with substantial domain knowledge.","Fundamentally, the underlying principles of effective reasoning program generation still remain underexplored, making it challenging to construct effective demonstrations.","To address this, we propose BOOST, a bootstrapping-based framework for few-shot reasoning program generation.","BOOST explicitly integrates claim decomposition and information-gathering strategies as structural guidance for program generation, iteratively refining bootstrapped demonstrations in a strategy-driven and data-centric manner without human intervention.","This enables a seamless transition from zero-shot to few-shot strategic program-guided learning, enhancing interpretability and effectiveness.","Experimental results show that BOOST outperforms prior few-shot baselines in both zero-shot and few-shot settings for complex claim verification."],"url":"http://arxiv.org/abs/2504.02467v1"}
{"created":"2025-04-03 10:33:43","title":"Evaluating AI Recruitment Sourcing Tools by Human Preference","abstract":"This study introduces a benchmarking methodology designed to evaluate the performance of AI-driven recruitment sourcing tools. We created and utilized a dataset to perform a comparative analysis of search results generated by leading AI-based solutions, LinkedIn Recruiter, and our proprietary system, Pearch.ai. Human experts assessed the relevance of the returned candidates, and an Elo rating system was applied to quantitatively measure each tool's comparative performance. Our findings indicate that AI-driven recruitment sourcing tools consistently outperform LinkedIn Recruiter in candidate relevance, with Pearch.ai achieving the highest performance scores. Furthermore, we found a strong alignment between AI-based evaluations and human judgments, highlighting the potential for advanced AI technologies to substantially enhance talent acquisition effectiveness. Code and supporting data are publicly available at https://github.com/vslaykovsky/ai-sourcing-benchmark","sentences":["This study introduces a benchmarking methodology designed to evaluate the performance of AI-driven recruitment sourcing tools.","We created and utilized a dataset to perform a comparative analysis of search results generated by leading AI-based solutions, LinkedIn Recruiter, and our proprietary system, Pearch.ai.","Human experts assessed the relevance of the returned candidates, and an Elo rating system was applied to quantitatively measure each tool's comparative performance.","Our findings indicate that AI-driven recruitment sourcing tools consistently outperform LinkedIn Recruiter in candidate relevance, with Pearch.ai achieving the highest performance scores.","Furthermore, we found a strong alignment between AI-based evaluations and human judgments, highlighting the potential for advanced AI technologies to substantially enhance talent acquisition effectiveness.","Code and supporting data are publicly available at https://github.com/vslaykovsky/ai-sourcing-benchmark"],"url":"http://arxiv.org/abs/2504.02463v1"}
{"created":"2025-04-03 10:20:16","title":"QPanda3: A High-Performance Software-Hardware Collaborative Framework for Large-Scale Quantum-Classical Computing Integration","abstract":"QPanda3 is a high-performance quantum programming framework that enhances quantum computing efficiency through optimized circuit compilation, an advanced instruction stream format (OriginBIS), and hardware-aware execution strategies. These engineering optimizations significantly improve both processing speed and system performance, addressing key challenges in the NISQ era. A core innovation, OriginBIS, accelerates encoding speeds by up to 86.9x compared to OpenQASM 2.0, while decoding is 35.6x faster, leading to more efficient data handling, reduced memory overhead, and improved communication efficiency. This directly enhances the execution of quantum circuits, making large-scale quantum simulations more feasible. Comprehensive benchmarking demonstrates QPanda3's superior performance: quantum circuit construction is 20.7x faster, execution speeds improve by 3.4x, and transpilation efficiency increases by 14.97x over Qiskit. Notably, in compiling a 118-qubit W-state circuit on a 2D-grid topology, QPanda3 achieves an unprecedented 869.9x speedup, underscoring its ability to handle complex quantum workloads at scale. By combining high-speed quantum processing with a modular and extensible software architecture, QPanda3 provides a practical bridge between today's NISQ devices and future fault-tolerant quantum computing. It facilitates real-world applications in financial modeling, materials science, and combinatorial optimization, while its robust and scalable design supports industrial adoption and cloud-based deployment.","sentences":["QPanda3 is a high-performance quantum programming framework that enhances quantum computing efficiency through optimized circuit compilation, an advanced instruction stream format (OriginBIS), and hardware-aware execution strategies.","These engineering optimizations significantly improve both processing speed and system performance, addressing key challenges in the NISQ era.","A core innovation, OriginBIS, accelerates encoding speeds by up to 86.9x compared to OpenQASM 2.0, while decoding is 35.6x faster, leading to more efficient data handling, reduced memory overhead, and improved communication efficiency.","This directly enhances the execution of quantum circuits, making large-scale quantum simulations more feasible.","Comprehensive benchmarking demonstrates QPanda3's superior performance: quantum circuit construction is 20.7x faster, execution speeds improve by 3.4x, and transpilation efficiency increases by 14.97x over Qiskit.","Notably, in compiling a 118-qubit W-state circuit on a 2D-grid topology, QPanda3 achieves an unprecedented 869.9x speedup, underscoring its ability to handle complex quantum workloads at scale.","By combining high-speed quantum processing with a modular and extensible software architecture, QPanda3 provides a practical bridge between today's NISQ devices and future fault-tolerant quantum computing.","It facilitates real-world applications in financial modeling, materials science, and combinatorial optimization, while its robust and scalable design supports industrial adoption and cloud-based deployment."],"url":"http://arxiv.org/abs/2504.02455v1"}
{"created":"2025-04-03 10:19:06","title":"Taylor Series-Inspired Local Structure Fitting Network for Few-shot Point Cloud Semantic Segmentation","abstract":"Few-shot point cloud semantic segmentation aims to accurately segment \"unseen\" new categories in point cloud scenes using limited labeled data. However, pretraining-based methods not only introduce excessive time overhead but also overlook the local structure representation among irregular point clouds. To address these issues, we propose a pretraining-free local structure fitting network for few-shot point cloud semantic segmentation, named TaylorSeg. Specifically, inspired by Taylor series, we treat the local structure representation of irregular point clouds as a polynomial fitting problem and propose a novel local structure fitting convolution, called TaylorConv. This convolution learns the low-order basic information and high-order refined information of point clouds from explicit encoding of local geometric structures. Then, using TaylorConv as the basic component, we construct two variants of TaylorSeg: a non-parametric TaylorSeg-NN and a parametric TaylorSeg-PN. The former can achieve performance comparable to existing parametric models without pretraining. For the latter, we equip it with an Adaptive Push-Pull (APP) module to mitigate the feature distribution differences between the query set and the support set. Extensive experiments validate the effectiveness of the proposed method. Notably, under the 2-way 1-shot setting, TaylorSeg-PN achieves improvements of +2.28% and +4.37% mIoU on the S3DIS and ScanNet datasets respectively, compared to the previous state-of-the-art methods. Our code is available at https://github.com/changshuowang/TaylorSeg.","sentences":["Few-shot point cloud semantic segmentation aims to accurately segment \"unseen\" new categories in point cloud scenes using limited labeled data.","However, pretraining-based methods not only introduce excessive time overhead but also overlook the local structure representation among irregular point clouds.","To address these issues, we propose a pretraining-free local structure fitting network for few-shot point cloud semantic segmentation, named TaylorSeg.","Specifically, inspired by Taylor series, we treat the local structure representation of irregular point clouds as a polynomial fitting problem and propose a novel local structure fitting convolution, called TaylorConv.","This convolution learns the low-order basic information and high-order refined information of point clouds from explicit encoding of local geometric structures.","Then, using TaylorConv as the basic component, we construct two variants of TaylorSeg: a non-parametric TaylorSeg-NN and a parametric TaylorSeg-PN.","The former can achieve performance comparable to existing parametric models without pretraining.","For the latter, we equip it with an Adaptive Push-Pull (APP) module to mitigate the feature distribution differences between the query set and the support set.","Extensive experiments validate the effectiveness of the proposed method.","Notably, under the 2-way 1-shot setting, TaylorSeg-PN achieves improvements of +2.28% and +4.37% mIoU on the S3DIS and ScanNet datasets respectively, compared to the previous state-of-the-art methods.","Our code is available at https://github.com/changshuowang/TaylorSeg."],"url":"http://arxiv.org/abs/2504.02454v1"}
{"created":"2025-04-03 10:00:42","title":"Revolutionizing Medical Data Transmission with IoMT: A Comprehensive Survey of Wireless Communication Solutions and Future Directions","abstract":"Traditional hospital-based medical examination methods face unprecedented challenges due to the aging global population. The Internet of Medical Things (IoMT), an advanced extension of the Internet of Things (IoT) tailored for the medical field, offers a transformative solution for delivering medical care. IoMT consists of interconnected medical devices that collect and transmit patients' vital signs online. This data can be analyzed to identify potential health issues, support medical decision-making, enhance patient outcomes, and streamline healthcare operations. Additionally, IoMT helps individuals make informed decisions about their health and fitness. There is a natural synergy with emerging communication technologies to ensure the secure and timely transmission of medical data. This paper presents the first comprehensive tutorial on cutting-edge IoMT research focusing on wireless communication-based solutions. It introduces a systematic three-tier framework to analyze IoMT networks and identify application scenarios. The paper examines the medical data transmission process, including intra-wireless Body Area Networks (WBAN), inter-WBAN, and beyond-WBAN communications. It also discusses the challenges of implementing IoMT applications, such as the longevity of biosensors, co-channel interference management, information security, and data processing delays. Proposed solutions to these challenges are explored from a wireless communication perspective, and future research directions are outlined. The survey concludes with a summary of key findings and insights.","sentences":["Traditional hospital-based medical examination methods face unprecedented challenges due to the aging global population.","The Internet of Medical Things (IoMT), an advanced extension of the Internet of Things (IoT) tailored for the medical field, offers a transformative solution for delivering medical care.","IoMT consists of interconnected medical devices that collect and transmit patients' vital signs online.","This data can be analyzed to identify potential health issues, support medical decision-making, enhance patient outcomes, and streamline healthcare operations.","Additionally, IoMT helps individuals make informed decisions about their health and fitness.","There is a natural synergy with emerging communication technologies to ensure the secure and timely transmission of medical data.","This paper presents the first comprehensive tutorial on cutting-edge IoMT research focusing on wireless communication-based solutions.","It introduces a systematic three-tier framework to analyze IoMT networks and identify application scenarios.","The paper examines the medical data transmission process, including intra-wireless Body Area Networks (WBAN), inter-WBAN, and beyond-WBAN communications.","It also discusses the challenges of implementing IoMT applications, such as the longevity of biosensors, co-channel interference management, information security, and data processing delays.","Proposed solutions to these challenges are explored from a wireless communication perspective, and future research directions are outlined.","The survey concludes with a summary of key findings and insights."],"url":"http://arxiv.org/abs/2504.02446v1"}
{"created":"2025-04-03 09:58:52","title":"Language-Integrated Recursive Queries","abstract":"Performance-critical industrial applications, including large-scale program, network, and distributed system analyses, rely on fixed-point computations. The introduction of recursive common table expressions (CTEs) using the WITH RECURSIVE keyword in SQL:1999 extended the ability of relational database systems to handle fixed-point computations, unlocking significant performance advantages by allowing computation to move closer to the data. Yet with recursion, SQL becomes a Turing-complete programming language and, with that, unrecoverable safety and correctness risks. SQL itself lacks a fixed semantics, as the SQL specification is written in natural language, full of ambiguities that database vendors resolve in divergent ways. As a result, reasoning about the correctness of recursive SQL programs must rely on isolated mathematical properties of queries rather than wrestling a unified formal model out of a language with notoriously inconsistent semantics. To address these challenges, we propose a calculus that automatically derives mathematical properties from embedded recursive queries and, depending on the database backend, rejects queries that may lead to the three classes of recursive query errors - database errors, incorrect results, and non-termination. We introduce TyQL, a practical implementation in Scala for safe, recursive language-integrated query. Using Named-Tuples and type-level pattern matching, TyQL ensures query portability and safety, showing no performance penalty compared to raw SQL strings while unlocking a three-orders-of-magnitude speedup over non-recursive SQL queries.","sentences":["Performance-critical industrial applications, including large-scale program, network, and distributed system analyses, rely on fixed-point computations.","The introduction of recursive common table expressions (CTEs) using the WITH RECURSIVE keyword in SQL:1999 extended the ability of relational database systems to handle fixed-point computations, unlocking significant performance advantages by allowing computation to move closer to the data.","Yet with recursion, SQL becomes a Turing-complete programming language and, with that, unrecoverable safety and correctness risks.","SQL itself lacks a fixed semantics, as the SQL specification is written in natural language, full of ambiguities that database vendors resolve in divergent ways.","As a result, reasoning about the correctness of recursive SQL programs must rely on isolated mathematical properties of queries rather than wrestling a unified formal model out of a language with notoriously inconsistent semantics.","To address these challenges, we propose a calculus that automatically derives mathematical properties from embedded recursive queries and, depending on the database backend, rejects queries that may lead to the three classes of recursive query errors - database errors, incorrect results, and non-termination.","We introduce TyQL, a practical implementation in Scala for safe, recursive language-integrated query.","Using Named-Tuples and type-level pattern matching, TyQL ensures query portability and safety, showing no performance penalty compared to raw SQL strings while unlocking a three-orders-of-magnitude speedup over non-recursive SQL queries."],"url":"http://arxiv.org/abs/2504.02443v1"}
{"created":"2025-04-03 09:57:51","title":"Estimating Scene Flow in Robot Surroundings with Distributed Miniaturized Time-of-Flight Sensors","abstract":"Tracking motions of humans or objects in the surroundings of the robot is essential to improve safe robot motions and reactions. In this work, we present an approach for scene flow estimation from low-density and noisy point clouds acquired from miniaturized Time of Flight (ToF) sensors distributed on the robot body. The proposed method clusters points from consecutive frames and applies Iterative Closest Point (ICP) to estimate a dense motion flow, with additional steps introduced to mitigate the impact of sensor noise and low-density data points. Specifically, we employ a fitness-based classification to distinguish between stationary and moving points and an inlier removal strategy to refine geometric correspondences. The proposed approach is validated in an experimental setup where 24 ToF are used to estimate the velocity of an object moving at different controlled speeds. Experimental results show that the method consistently approximates the direction of the motion and its magnitude with an error which is in line with sensor noise.","sentences":["Tracking motions of humans or objects in the surroundings of the robot is essential to improve safe robot motions and reactions.","In this work, we present an approach for scene flow estimation from low-density and noisy point clouds acquired from miniaturized Time of Flight (ToF) sensors distributed on the robot body.","The proposed method clusters points from consecutive frames and applies Iterative Closest Point (ICP) to estimate a dense motion flow, with additional steps introduced to mitigate the impact of sensor noise and low-density data points.","Specifically, we employ a fitness-based classification to distinguish between stationary and moving points and an inlier removal strategy to refine geometric correspondences.","The proposed approach is validated in an experimental setup where 24 ToF are used to estimate the velocity of an object moving at different controlled speeds.","Experimental results show that the method consistently approximates the direction of the motion and its magnitude with an error which is in line with sensor noise."],"url":"http://arxiv.org/abs/2504.02439v1"}
{"created":"2025-04-03 09:50:50","title":"SkyReels-A2: Compose Anything in Video Diffusion Transformers","abstract":"This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.","sentences":["This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element.","We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs.","To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training.","Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment.","We also optimize the inference pipeline for both speed and output stability.","Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench.","Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control.","SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models.","We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation."],"url":"http://arxiv.org/abs/2504.02436v1"}
{"created":"2025-04-03 09:43:27","title":"Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection","abstract":"Robust low-rank approximation under row-wise adversarial corruption can be achieved with a single pass, randomized procedure that detects and removes outlier rows by thresholding their projected norms. We propose a scalable, non-iterative algorithm that efficiently recovers the underlying low-rank structure in the presence of row-wise adversarial corruption. By first compressing the data with a Johnson Lindenstrauss projection, our approach preserves the geometry of clean rows while dramatically reducing dimensionality. Robust statistical techniques based on the median and median absolute deviation then enable precise identification and removal of outlier rows with abnormally high norms. The subsequent rank-k approximation achieves near-optimal error bounds with a one pass procedure that scales linearly with the number of observations. Empirical results confirm that combining random sketches with robust statistics yields efficient, accurate decompositions even in the presence of large fractions of corrupted rows.","sentences":["Robust low-rank approximation under row-wise adversarial corruption can be achieved with a single pass, randomized procedure that detects and removes outlier rows by thresholding their projected norms.","We propose a scalable, non-iterative algorithm that efficiently recovers the underlying low-rank structure in the presence of row-wise adversarial corruption.","By first compressing the data with a Johnson Lindenstrauss projection, our approach preserves the geometry of clean rows while dramatically reducing dimensionality.","Robust statistical techniques based on the median and median absolute deviation then enable precise identification and removal of outlier rows with abnormally high norms.","The subsequent rank-k approximation achieves near-optimal error bounds with a one pass procedure that scales linearly with the number of observations.","Empirical results confirm that combining random sketches with robust statistics yields efficient, accurate decompositions even in the presence of large fractions of corrupted rows."],"url":"http://arxiv.org/abs/2504.02432v1"}
{"created":"2025-04-03 09:23:31","title":"Edge-weighted balanced connected partitions: Hardness and formulations","abstract":"The balanced connected $k$-partition problem (BCP) is a classic problem which consists in partitioning the set of vertices of a vertex-weighted connected graph into a collection of $k$ sets such that each of them induces a connected subgraph of roughly the same weight. There exists a vast literature on BCP that includes hardness results, approximation algorithms, integer programming formulations, and a polyhedral study. We investigate edge-weighted variants of BCP where we are given a connected graph $G$, $k \\in \\mathbb{Z}_\\ge$, and an edge-weight function $w \\colon E(G)\\to\\mathbb{Q}_\\ge$, and the goal is to compute a spanning $k$-forest $\\mathcal{T}$ of $G$ (i.e., a forest with exactly $k$ trees) that minimizes the weight of the heaviest tree in $\\mathcal{T}$ in the min-max version, or maximizes the weight of the lightest tree in $\\mathcal{T}$ in the max-min version. We show that both versions of this problem are $\\mathsf{NP}$-hard on complete graphs with $k=2$, unweighted split graphs, and unweighted bipartite graphs with $k\\geq 2$ fixed. Moreover, we prove that these problems do not admit subexponential-time algorithms, unless the Exponential-Time Hypothesis fails. Finally, we devise compact and non-compact integer linear programming formulations, valid inequalities, and separation algorithms.","sentences":["The balanced connected $k$-partition problem (BCP) is a classic problem which consists in partitioning the set of vertices of a vertex-weighted connected graph into a collection of $k$ sets such that each of them induces a connected subgraph of roughly the same weight.","There exists a vast literature on BCP that includes hardness results, approximation algorithms, integer programming formulations, and a polyhedral study.","We investigate edge-weighted variants of BCP where we are given a connected graph $G$, $k \\in \\mathbb{Z}_\\ge$, and an edge-weight function $w \\colon E(G)\\to\\mathbb{Q}_\\ge$, and the goal is to compute a spanning $k$-forest $\\mathcal{T}$ of $G$ (i.e., a forest with exactly $k$ trees) that minimizes the weight of the heaviest tree in $\\mathcal{T}$ in the min-max version, or maximizes the weight of the lightest tree in $\\mathcal{T}$ in the max-min version.","We show that both versions of this problem are $\\mathsf{NP}$-hard on complete graphs with $k=2$, unweighted split graphs, and unweighted bipartite graphs with $k\\geq 2$ fixed.","Moreover, we prove that these problems do not admit subexponential-time algorithms, unless the Exponential-Time Hypothesis fails.","Finally, we devise compact and non-compact integer linear programming formulations, valid inequalities, and separation algorithms."],"url":"http://arxiv.org/abs/2504.02421v1"}
{"created":"2025-04-03 08:52:42","title":"DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers","abstract":"Large Language Models (LLMs) have seen widespread societal adoption. However, while they are able to interact with users in languages beyond English, they have been shown to lack cultural awareness, providing anglocentric or inappropriate responses for underrepresented language communities. To investigate this gap and disentangle linguistic versus cultural proficiency, we conduct the first cultural evaluation study for the mid-resource language of Danish, in which native speakers prompt different models to solve tasks requiring cultural awareness. Our analysis of the resulting 1,038 interactions from 63 demographically diverse participants highlights open challenges to cultural adaptation: Particularly, how currently employed automatically translated data are insufficient to train or measure cultural adaptation, and how training on native-speaker data can more than double response acceptance rates. We release our study data as DaKultur - the first native Danish cultural awareness dataset.","sentences":["Large Language Models (LLMs) have seen widespread societal adoption.","However, while they are able to interact with users in languages beyond English, they have been shown to lack cultural awareness, providing anglocentric or inappropriate responses for underrepresented language communities.","To investigate this gap and disentangle linguistic versus cultural proficiency, we conduct the first cultural evaluation study for the mid-resource language of Danish, in which native speakers prompt different models to solve tasks requiring cultural awareness.","Our analysis of the resulting 1,038 interactions from 63 demographically diverse participants highlights open challenges to cultural adaptation:","Particularly, how currently employed automatically translated data are insufficient to train or measure cultural adaptation, and how training on native-speaker data can more than double response acceptance rates.","We release our study data as DaKultur - the first native Danish cultural awareness dataset."],"url":"http://arxiv.org/abs/2504.02403v1"}
{"created":"2025-04-03 08:51:17","title":"EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling","abstract":"When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.","sentences":["When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound.","Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path.","Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals.","However, existing event-based vibration recovery methods are still sub-optimal for sound recovery.","In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream.","We first generate a large training set using a novel simulation pipeline.","Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information.","Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality.","To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing.","Experimental results on synthetic and real-world data demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2504.02402v1"}
{"created":"2025-04-03 08:46:56","title":"Scaling Analysis of Interleaved Speech-Text Language Models","abstract":"Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.","sentences":["Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.","They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs.","However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer.","This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs?","In this paper we answer a resounding, yes!","We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends.","We see that under this setup SLMs scale more efficiently with compute.","Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens.","We also study the role of synthetic data and TextLM model families in unlocking this potential.","Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches.","We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims."],"url":"http://arxiv.org/abs/2504.02398v1"}
{"created":"2025-04-03 08:41:26","title":"The quasi-semantic competence of LLMs: a case study on the part-whole relation","abstract":"Understanding the extent and depth of the semantic competence of \\emph{Large Language Models} (LLMs) is at the center of the current scientific agenda in Artificial Intelligence (AI) and Computational Linguistics (CL). We contribute to this endeavor by investigating their knowledge of the \\emph{part-whole} relation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical organization, but it is significantly understudied. We used data from ConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic feature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with \\textit{part-whole} relations. We employed several methods based on three levels of analysis: i.) \\textbf{behavioral} testing via prompting, where we directly queried the models on their knowledge of meronymy, ii.) sentence \\textbf{probability} scoring, where we tested models' abilities to discriminate correct (real) and incorrect (asymmetric counterfactual) \\textit{part-whole} relations, and iii.) \\textbf{concept representation} analysis in vector space, where we proved the linear organization of the \\textit{part-whole} concept in the embedding and unembedding spaces. These analyses present a complex picture that reveals that the LLMs' knowledge of this relation is only partial. They have just a ``\\emph{quasi}-semantic'' competence and still fall short of capturing deep inferential properties.","sentences":["Understanding the extent and depth of the semantic competence of \\emph{Large Language Models} (LLMs) is at the center of the current scientific agenda in Artificial Intelligence (AI) and Computational Linguistics (CL).","We contribute to this endeavor by investigating their knowledge of the \\emph{part-whole} relation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical organization, but it is significantly understudied.","We used data from ConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic feature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with \\textit{part-whole} relations.","We employed several methods based on three levels of analysis: i.) \\textbf{behavioral} testing via prompting, where we directly queried the models on their knowledge of meronymy, ii.) sentence \\textbf{probability} scoring, where we tested models' abilities to discriminate correct (real) and incorrect (asymmetric counterfactual) \\textit{part-whole} relations, and iii.) \\textbf{concept representation} analysis in vector space, where we proved the linear organization of the \\textit{part-whole} concept in the embedding and unembedding spaces.","These analyses present a complex picture that reveals that the LLMs' knowledge of this relation is only partial.","They have just a ``\\emph{quasi}-semantic'' competence and still fall short of capturing deep inferential properties."],"url":"http://arxiv.org/abs/2504.02395v1"}
{"created":"2025-04-03 08:01:41","title":"Finding Diverse Solutions in Combinatorial Problems with a Distributive Lattice Structure","abstract":"We generalize the polynomial-time solvability of $k$-\\textsc{Diverse Minimum s-t Cuts} (De Berg et al., ISAAC'23) to a wider class of combinatorial problems whose solution sets have a distributive lattice structure. We identify three structural conditions that, when met by a problem, ensure that a $k$-sized multiset of maximally-diverse solutions -- measured by the sum of pairwise Hamming distances -- can be found in polynomial time. We apply this framework to obtain polynomial time algorithms for finding diverse minimum $s$-$t$ cuts and diverse stable matchings. Moreover, we show that the framework extends to two other natural measures of diversity. Lastly, we present a simpler algorithmic framework for finding a largest set of pairwise disjoint solutions in problems that meet these structural conditions.","sentences":["We generalize the polynomial-time solvability of $k$-\\textsc{Diverse Minimum s-t Cuts} (De Berg et al., ISAAC'23) to a wider class of combinatorial problems whose solution sets have a distributive lattice structure.","We identify three structural conditions that, when met by a problem, ensure that a $k$-sized multiset of maximally-diverse solutions -- measured by the sum of pairwise Hamming distances -- can be found in polynomial time.","We apply this framework to obtain polynomial time algorithms for finding diverse minimum $s$-$t$ cuts and diverse stable matchings.","Moreover, we show that the framework extends to two other natural measures of diversity.","Lastly, we present a simpler algorithmic framework for finding a largest set of pairwise disjoint solutions in problems that meet these structural conditions."],"url":"http://arxiv.org/abs/2504.02369v1"}
{"created":"2025-04-03 07:54:49","title":"SProBench: Stream Processing Benchmark for High Performance Computing Infrastructure","abstract":"Recent advancements in data stream processing frameworks have improved real-time data handling, however, scalability remains a significant challenge affecting throughput and latency. While studies have explored this issue on local machines and cloud clusters, research on modern high performance computing (HPC) infrastructures is yet limited due to the lack of scalable measurement tools. This work presents SProBench, a novel benchmark suite designed to evaluate the performance of data stream processing frameworks in large-scale computing systems. Building on best practices, SProBench incorporates a modular architecture, offers native support for SLURM-based clusters, and seamlessly integrates with popular stream processing frameworks such as Apache Flink, Apache Spark Streaming, and Apache Kafka Streams. Experiments conducted on HPC clusters demonstrate its exceptional scalability, delivering throughput that surpasses existing benchmarks by more than tenfold. The distinctive features of SProBench, including complete customization options, built-in automated experiment management tools, seamless interoperability, and an open-source license, distinguish it as an innovative benchmark suite tailored to meet the needs of modern data stream processing frameworks.","sentences":["Recent advancements in data stream processing frameworks have improved real-time data handling, however, scalability remains a significant challenge affecting throughput and latency.","While studies have explored this issue on local machines and cloud clusters, research on modern high performance computing (HPC) infrastructures is yet limited due to the lack of scalable measurement tools.","This work presents SProBench, a novel benchmark suite designed to evaluate the performance of data stream processing frameworks in large-scale computing systems.","Building on best practices, SProBench incorporates a modular architecture, offers native support for SLURM-based clusters, and seamlessly integrates with popular stream processing frameworks such as Apache Flink, Apache Spark Streaming, and Apache Kafka Streams.","Experiments conducted on HPC clusters demonstrate its exceptional scalability, delivering throughput that surpasses existing benchmarks by more than tenfold.","The distinctive features of SProBench, including complete customization options, built-in automated experiment management tools, seamless interoperability, and an open-source license, distinguish it as an innovative benchmark suite tailored to meet the needs of modern data stream processing frameworks."],"url":"http://arxiv.org/abs/2504.02364v1"}
{"created":"2025-04-03 07:52:12","title":"MG-Gen: Single Image to Motion Graphics Generation with Layer Decomposition","abstract":"General image-to-video generation methods often produce suboptimal animations that do not meet the requirements of animated graphics, as they lack active text motion and exhibit object distortion. Also, code-based animation generation methods typically require layer-structured vector data which are often not readily available for motion graphic generation. To address these challenges, we propose a novel framework named MG-Gen that reconstructs data in vector format from a single raster image to extend the capabilities of code-based methods to enable motion graphics generation from a raster image in the framework of general image-to-video generation. MG-Gen first decomposes the input image into layer-wise elements, reconstructs them as HTML format data and then generates executable JavaScript code for the reconstructed HTML data. We experimentally confirm that \\ours{} generates motion graphics while preserving text readability and input consistency. These successful results indicate that combining layer decomposition and animation code generation is an effective strategy for motion graphics generation.","sentences":["General image-to-video generation methods often produce suboptimal animations that do not meet the requirements of animated graphics, as they lack active text motion and exhibit object distortion.","Also, code-based animation generation methods typically require layer-structured vector data which are often not readily available for motion graphic generation.","To address these challenges, we propose a novel framework named MG-Gen that reconstructs data in vector format from a single raster image to extend the capabilities of code-based methods to enable motion graphics generation from a raster image in the framework of general image-to-video generation.","MG-Gen first decomposes the input image into layer-wise elements, reconstructs them as HTML format data and then generates executable JavaScript code for the reconstructed HTML data.","We experimentally confirm that \\ours{} generates motion graphics while preserving text readability and input consistency.","These successful results indicate that combining layer decomposition and animation code generation is an effective strategy for motion graphics generation."],"url":"http://arxiv.org/abs/2504.02361v1"}
{"created":"2025-04-03 07:41:04","title":"Liquid Neural Networks: Next-Generation AI for Telecom from First Principles","abstract":"Artificial intelligence (AI) has emerged as a transformative technology with immense potential to reshape the next-generation of wireless networks. By leveraging advanced algorithms and machine learning techniques, AI offers unprecedented capabilities in optimizing network performance, enhancing data processing efficiency, and enabling smarter decision-making processes. However, existing AI solutions face significant challenges in terms of robustness and interpretability. Specifically, current AI models exhibit substantial performance degradation in dynamic environments with varying data distributions, and the black-box nature of these algorithms raises concerns regarding safety, transparency, and fairness. This presents a major challenge in integrating AI into practical communication systems. Recently, a novel type of neural network, known as the liquid neural networks (LNNs), has been designed from first principles to address these issues. In this paper, we explore the potential of LNNs in telecommunications. First, we illustrate the mechanisms of LNNs and highlight their unique advantages over traditional networks. Then we unveil the opportunities that LNNs bring to future wireless networks. Furthermore, we discuss the challenges and design directions for the implementation of LNNs. Finally, we summarize the performance of LNNs in two case studies.","sentences":["Artificial intelligence (AI) has emerged as a transformative technology with immense potential to reshape the next-generation of wireless networks.","By leveraging advanced algorithms and machine learning techniques, AI offers unprecedented capabilities in optimizing network performance, enhancing data processing efficiency, and enabling smarter decision-making processes.","However, existing AI solutions face significant challenges in terms of robustness and interpretability.","Specifically, current AI models exhibit substantial performance degradation in dynamic environments with varying data distributions, and the black-box nature of these algorithms raises concerns regarding safety, transparency, and fairness.","This presents a major challenge in integrating AI into practical communication systems.","Recently, a novel type of neural network, known as the liquid neural networks (LNNs), has been designed from first principles to address these issues.","In this paper, we explore the potential of LNNs in telecommunications.","First, we illustrate the mechanisms of LNNs and highlight their unique advantages over traditional networks.","Then we unveil the opportunities that LNNs bring to future wireless networks.","Furthermore, we discuss the challenges and design directions for the implementation of LNNs.","Finally, we summarize the performance of LNNs in two case studies."],"url":"http://arxiv.org/abs/2504.02352v1"}
{"created":"2025-04-03 07:28:16","title":"SemiISP/SemiIE: Semi-Supervised Image Signal Processor and Image Enhancement Leveraging One-to-Many Mapping sRGB-to-RAW","abstract":"DNN-based methods have been successful in Image Signal Processor (ISP) and image enhancement (IE) tasks. However, the cost of creating training data for these tasks is considerably higher than for other tasks, making it difficult to prepare large-scale datasets. Also, creating personalized ISP and IE with minimal training data can lead to new value streams since preferred image quality varies depending on the person and use case. While semi-supervised learning could be a potential solution in such cases, it has rarely been utilized for these tasks. In this paper, we realize semi-supervised learning for ISP and IE leveraging a RAW image reconstruction (sRGB-to-RAW) method. Although existing sRGB-to-RAW methods can generate pseudo-RAW image datasets that improve the accuracy of RAW-based high-level computer vision tasks such as object detection, their quality is not sufficient for ISP and IE tasks that require precise image quality definition. Therefore, we also propose a sRGB-to-RAW method that can improve the image quality of these tasks. The proposed semi-supervised learning with the proposed sRGB-to-RAW method successfully improves the image quality of various models on various datasets.","sentences":["DNN-based methods have been successful in Image Signal Processor (ISP) and image enhancement (IE) tasks.","However, the cost of creating training data for these tasks is considerably higher than for other tasks, making it difficult to prepare large-scale datasets.","Also, creating personalized ISP and IE with minimal training data can lead to new value streams since preferred image quality varies depending on the person and use case.","While semi-supervised learning could be a potential solution in such cases, it has rarely been utilized for these tasks.","In this paper, we realize semi-supervised learning for ISP and IE leveraging a RAW image reconstruction (sRGB-to-RAW) method.","Although existing sRGB-to-RAW methods can generate pseudo-RAW image datasets that improve the accuracy of RAW-based high-level computer vision tasks such as object detection, their quality is not sufficient for ISP and IE tasks that require precise image quality definition.","Therefore, we also propose a sRGB-to-RAW method that can improve the image quality of these tasks.","The proposed semi-supervised learning with the proposed sRGB-to-RAW method successfully improves the image quality of various models on various datasets."],"url":"http://arxiv.org/abs/2504.02345v1"}
{"created":"2025-04-03 07:24:18","title":"Toward General and Robust LLM-enhanced Text-attributed Graph Learning","abstract":"Recent advancements in Large Language Models (LLMs) and the proliferation of Text-Attributed Graphs (TAGs) across various domains have positioned LLM-enhanced TAG learning as a critical research area. By utilizing rich graph descriptions, this paradigm leverages LLMs to generate high-quality embeddings, thereby enhancing the representational capacity of Graph Neural Networks (GNNs). However, the field faces significant challenges: (1) the absence of a unified framework to systematize the diverse optimization perspectives arising from the complex interactions between LLMs and GNNs, and (2) the lack of a robust method capable of handling real-world TAGs, which often suffer from texts and edge sparsity, leading to suboptimal performance.   To address these challenges, we propose UltraTAG, a unified pipeline for LLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and domain-adaptive framework that not only organizes existing methodologies but also paves the way for future advancements in the field. Building on this framework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed to tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs LLM-based text propagation and text augmentation to mitigate text sparsity, while leveraging LLM-augmented node selection techniques based on PageRank and edge reconfiguration strategies to address edge sparsity. Our extensive experiments demonstrate that UltraTAG-S significantly outperforms existing baselines, achieving improvements of 2.12\\% and 17.47\\% in ideal and sparse settings, respectively. Moreover, as the data sparsity ratio increases, the performance improvement of UltraTAG-S also rises, which underscores the effectiveness and robustness of UltraTAG-S.","sentences":["Recent advancements in Large Language Models (LLMs) and the proliferation of Text-Attributed Graphs (TAGs) across various domains have positioned LLM-enhanced TAG learning as a critical research area.","By utilizing rich graph descriptions, this paradigm leverages LLMs to generate high-quality embeddings, thereby enhancing the representational capacity of Graph Neural Networks (GNNs).","However, the field faces significant challenges: (1) the absence of a unified framework to systematize the diverse optimization perspectives arising from the complex interactions between LLMs and GNNs, and (2) the lack of a robust method capable of handling real-world TAGs, which often suffer from texts and edge sparsity, leading to suboptimal performance.   ","To address these challenges, we propose UltraTAG, a unified pipeline for LLM-enhanced TAG learning.","UltraTAG provides a unified comprehensive and domain-adaptive framework that not only organizes existing methodologies but also paves the way for future advancements in the field.","Building on this framework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed to tackle the inherent sparsity issues in real-world TAGs.","UltraTAG-S employs LLM-based text propagation and text augmentation to mitigate text sparsity, while leveraging LLM-augmented node selection techniques based on PageRank and edge reconfiguration strategies to address edge sparsity.","Our extensive experiments demonstrate that UltraTAG-S significantly outperforms existing baselines, achieving improvements of 2.12\\% and 17.47\\% in ideal and sparse settings, respectively.","Moreover, as the data sparsity ratio increases, the performance improvement of UltraTAG-S also rises, which underscores the effectiveness and robustness of UltraTAG-S."],"url":"http://arxiv.org/abs/2504.02343v1"}
{"created":"2025-04-03 06:44:25","title":"X-Capture: An Open-Source Portable Device for Multi-Sensory Learning","abstract":"Understanding objects through multiple sensory modalities is fundamental to human perception, enabling cross-sensory integration and richer comprehension. For AI and robotic systems to replicate this ability, access to diverse, high-quality multi-sensory data is critical. Existing datasets are often limited by their focus on controlled environments, simulated objects, or restricted modality pairings. We introduce X-Capture, an open-source, portable, and cost-effective device for real-world multi-sensory data collection, capable of capturing correlated RGBD images, tactile readings, and impact audio. With a build cost under $1,000, X-Capture democratizes the creation of multi-sensory datasets, requiring only consumer-grade tools for assembly. Using X-Capture, we curate a sample dataset of 3,000 total points on 500 everyday objects from diverse, real-world environments, offering both richness and variety. Our experiments demonstrate the value of both the quantity and the sensory breadth of our data for both pretraining and fine-tuning multi-modal representations for object-centric tasks such as cross-sensory retrieval and reconstruction. X-Capture lays the groundwork for advancing human-like sensory representations in AI, emphasizing scalability, accessibility, and real-world applicability.","sentences":["Understanding objects through multiple sensory modalities is fundamental to human perception, enabling cross-sensory integration and richer comprehension.","For AI and robotic systems to replicate this ability, access to diverse, high-quality multi-sensory data is critical.","Existing datasets are often limited by their focus on controlled environments, simulated objects, or restricted modality pairings.","We introduce X-Capture, an open-source, portable, and cost-effective device for real-world multi-sensory data collection, capable of capturing correlated RGBD images, tactile readings, and impact audio.","With a build cost under $1,000, X-Capture democratizes the creation of multi-sensory datasets, requiring only consumer-grade tools for assembly.","Using X-Capture, we curate a sample dataset of 3,000 total points on 500 everyday objects from diverse, real-world environments, offering both richness and variety.","Our experiments demonstrate the value of both the quantity and the sensory breadth of our data for both pretraining and fine-tuning multi-modal representations for object-centric tasks such as cross-sensory retrieval and reconstruction.","X-Capture lays the groundwork for advancing human-like sensory representations in AI, emphasizing scalability, accessibility, and real-world applicability."],"url":"http://arxiv.org/abs/2504.02318v1"}
{"created":"2025-04-03 06:44:05","title":"Temporal Gaussian Copula For Clinical Multivariate Time Series Data Imputation","abstract":"The imputation of the Multivariate time series (MTS) is particularly challenging since the MTS typically contains irregular patterns of missing values due to various factors such as instrument failures, interference from irrelevant data, and privacy regulations. Existing statistical methods and deep learning methods have shown promising results in time series imputation. In this paper, we propose a Temporal Gaussian Copula Model (TGC) for three-order MTS imputation. The key idea is to leverage the Gaussian Copula to explore the cross-variable and temporal relationships based on the latent Gaussian representation. Subsequently, we employ an Expectation-Maximization (EM) algorithm to improve robustness in managing data with varying missing rates. Comprehensive experiments were conducted on three real-world MTS datasets. The results demonstrate that our TGC substantially outperforms the state-of-the-art imputation methods. Additionally, the TGC model exhibits stronger robustness to the varying missing ratios in the test dataset. Our code is available at https://github.com/MVL-Lab/TGC-MTS.","sentences":["The imputation of the Multivariate time series (MTS) is particularly challenging since the MTS typically contains irregular patterns of missing values due to various factors such as instrument failures, interference from irrelevant data, and privacy regulations.","Existing statistical methods and deep learning methods have shown promising results in time series imputation.","In this paper, we propose a Temporal Gaussian Copula Model (TGC) for three-order MTS imputation.","The key idea is to leverage the Gaussian Copula to explore the cross-variable and temporal relationships based on the latent Gaussian representation.","Subsequently, we employ an Expectation-Maximization (EM) algorithm to improve robustness in managing data with varying missing rates.","Comprehensive experiments were conducted on three real-world MTS datasets.","The results demonstrate that our TGC substantially outperforms the state-of-the-art imputation methods.","Additionally, the TGC model exhibits stronger robustness to the varying missing ratios in the test dataset.","Our code is available at https://github.com/MVL-Lab/TGC-MTS."],"url":"http://arxiv.org/abs/2504.02317v1"}
{"created":"2025-04-03 06:42:26","title":"Distributed Temporal Graph Learning with Provenance for APT Detection in Supply Chains","abstract":"Cyber supply chain, encompassing digital asserts, software, hardware, has become an essential component of modern Information and Communications Technology (ICT) provisioning. However, the growing inter-dependencies have introduced numerous attack vectors, making supply chains a prime target for exploitation. In particular, advanced persistent threats (APTs) frequently leverage supply chain vulnerabilities (SCVs) as entry points, benefiting from their inherent stealth. Current defense strategies primarly focus on prevention through blockchain for integrity assurance or detection using plain-text source code analysis in open-source software (OSS). However, these approaches overlook scenarios where source code is unavailable and fail to address detection and defense during runtime. To bridge this gap, we propose a novel approach that integrates multi-source data, constructs a comprehensive dynamic provenance graph, and detects APT behavior in real time using temporal graph learning. Given the lack of tailored datasets in both industry and academia, we also aim to simulate a custom dataset by replaying real-world supply chain exploits with multi-source monitoring.","sentences":["Cyber supply chain, encompassing digital asserts, software, hardware, has become an essential component of modern Information and Communications Technology (ICT) provisioning.","However, the growing inter-dependencies have introduced numerous attack vectors, making supply chains a prime target for exploitation.","In particular, advanced persistent threats (APTs) frequently leverage supply chain vulnerabilities (SCVs) as entry points, benefiting from their inherent stealth.","Current defense strategies primarly focus on prevention through blockchain for integrity assurance or detection using plain-text source code analysis in open-source software (OSS).","However, these approaches overlook scenarios where source code is unavailable and fail to address detection and defense during runtime.","To bridge this gap, we propose a novel approach that integrates multi-source data, constructs a comprehensive dynamic provenance graph, and detects APT behavior in real time using temporal graph learning.","Given the lack of tailored datasets in both industry and academia, we also aim to simulate a custom dataset by replaying real-world supply chain exploits with multi-source monitoring."],"url":"http://arxiv.org/abs/2504.02313v1"}
{"created":"2025-04-03 06:32:06","title":"Solving adhesive rough contact problems with Atomic Force Microscope data","abstract":"This study presents an advanced numerical framework that integrates experimentally acquired Atomic Force Microscope (AFM) data into high-fidelity simulations for adhesive rough contact problems, bridging the gap between experimental physics and computational mechanics. The proposed approach extends the eMbedded Profile for Joint Roughness (MPJR) interface finite element method to incorporate both surface topography and spatially varying adhesion properties, imported directly from AFM measurements. The adhesion behavior is modeled using a modified Lennard-Jones potential, which is locally parameterized based on the AFM-extracted adhesion peak force and energy dissipation data. The effectiveness of this method is demonstrated through 2D and 3D finite element simulations of a heterogeneous PS-LDPE (polystyrene matrix with low-density polyethylene inclusions) sample, where the bulk elastic properties are also experimentally characterized via AFM. The results highlight the significance of accounting for both surface adhesion variability and material bulk heterogeneity in accurately predicting contact responses.","sentences":["This study presents an advanced numerical framework that integrates experimentally acquired Atomic Force Microscope (AFM) data into high-fidelity simulations for adhesive rough contact problems, bridging the gap between experimental physics and computational mechanics.","The proposed approach extends the eMbedded Profile for Joint Roughness (MPJR) interface finite element method to incorporate both surface topography and spatially varying adhesion properties, imported directly from AFM measurements.","The adhesion behavior is modeled using a modified Lennard-Jones potential, which is locally parameterized based on the AFM-extracted adhesion peak force and energy dissipation data.","The effectiveness of this method is demonstrated through 2D and 3D finite element simulations of a heterogeneous PS-LDPE (polystyrene matrix with low-density polyethylene inclusions) sample, where the bulk elastic properties are also experimentally characterized via AFM.","The results highlight the significance of accounting for both surface adhesion variability and material bulk heterogeneity in accurately predicting contact responses."],"url":"http://arxiv.org/abs/2504.02307v1"}
{"created":"2025-04-03 06:22:19","title":"Measurement of LLM's Philosophies of Human Nature","abstract":"The widespread application of artificial intelligence (AI) in various tasks, along with frequent reports of conflicts or violations involving AI, has sparked societal concerns about interactions with AI systems. Based on Wrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically validated over decades to effectively assess individuals' attitudes toward human nature, we design the standardized psychological scale specifically targeting large language models (LLM), named the Machine-based Philosophies of Human Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature across six dimensions, we reveal that current LLMs exhibit a systemic lack of trust in humans, and there is a significant negative correlation between the model's intelligence level and its trust in humans. Furthermore, we propose a mental loop learning framework, which enables LLM to continuously optimize its value system during virtual interactions by constructing moral scenarios, thereby improving its attitude toward human nature. Experiments demonstrate that mental loop learning significantly enhances their trust in humans compared to persona or instruction prompts. This finding highlights the potential of human-based psychological assessments for LLM, which can not only diagnose cognitive biases but also provide a potential solution for ethical learning in artificial intelligence. We release the M-PHNS evaluation code and data at https://github.com/kodenii/M-PHNS.","sentences":["The widespread application of artificial intelligence (AI) in various tasks, along with frequent reports of conflicts or violations involving AI, has sparked societal concerns about interactions with AI systems.","Based on Wrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically validated over decades to effectively assess individuals' attitudes toward human nature, we design the standardized psychological scale specifically targeting large language models (LLM), named the Machine-based Philosophies of Human Nature Scale (M-PHNS).","By evaluating LLMs' attitudes toward human nature across six dimensions, we reveal that current LLMs exhibit a systemic lack of trust in humans, and there is a significant negative correlation between the model's intelligence level and its trust in humans.","Furthermore, we propose a mental loop learning framework, which enables LLM to continuously optimize its value system during virtual interactions by constructing moral scenarios, thereby improving its attitude toward human nature.","Experiments demonstrate that mental loop learning significantly enhances their trust in humans compared to persona or instruction prompts.","This finding highlights the potential of human-based psychological assessments for LLM, which can not only diagnose cognitive biases but also provide a potential solution for ethical learning in artificial intelligence.","We release the M-PHNS evaluation code and data at https://github.com/kodenii/M-PHNS."],"url":"http://arxiv.org/abs/2504.02304v1"}
{"created":"2025-04-03 06:10:00","title":"Asymmetric graph alignment and the phase transition for asymmetric tree correlation testing","abstract":"Graph alignment - identifying node correspondences between two graphs - is a fundamental problem with applications in network analysis, biology, and privacy research. While substantial progress has been made in aligning correlated Erd\\H{o}s-R\\'enyi graphs under symmetric settings, real-world networks often exhibit asymmetry in both node numbers and edge densities. In this work, we introduce a novel framework for asymmetric correlated Erd\\H{o}s-R\\'enyi graphs, generalizing existing models to account for these asymmetries. We conduct a rigorous theoretical analysis of graph alignment in the sparse regime, where local neighborhoods exhibit tree-like structures. Our approach leverages tree correlation testing as the central tool in our polynomial-time algorithm, MPAlign, which achieves one-sided partial alignment under certain conditions.   A key contribution of our work is characterizing these conditions under which asymmetric tree correlation testing is feasible: If two correlated graphs $G$ and $G'$ have average degrees $\\lambda s$ and $\\lambda s'$ respectively, where $\\lambda$ is their common density and $s,s'$ are marginal correlation parameters, their tree neighborhoods can be aligned if $ss' > \\alpha$, where $\\alpha$ denotes Otter's constant and $\\lambda$ is supposed large enough. The feasibility of this tree comparison problem undergoes a sharp phase transition since $ss' \\leq \\alpha$ implies its impossibility. These new results on tree correlation testing allow us to solve a class of random subgraph isomorphism problems, resolving an open problem in the field.","sentences":["Graph alignment - identifying node correspondences between two graphs - is a fundamental problem with applications in network analysis, biology, and privacy research.","While substantial progress has been made in aligning correlated Erd\\H{o}s-R\\'enyi graphs under symmetric settings, real-world networks often exhibit asymmetry in both node numbers and edge densities.","In this work, we introduce a novel framework for asymmetric correlated Erd\\H{o}s-R\\'enyi graphs, generalizing existing models to account for these asymmetries.","We conduct a rigorous theoretical analysis of graph alignment in the sparse regime, where local neighborhoods exhibit tree-like structures.","Our approach leverages tree correlation testing as the central tool in our polynomial-time algorithm, MPAlign, which achieves one-sided partial alignment under certain conditions.   ","A key contribution of our work is characterizing these conditions under which asymmetric tree correlation testing is feasible: If two correlated graphs $G$ and $G'$ have average degrees $\\lambda s$ and $\\lambda s'$ respectively, where $\\lambda$ is their common density and $s,s'$ are marginal correlation parameters, their tree neighborhoods can be aligned if $ss' > \\alpha$, where $\\alpha$ denotes Otter's constant and $\\lambda$ is supposed large enough.","The feasibility of this tree comparison problem undergoes a sharp phase transition since $ss' \\leq \\alpha$ implies its impossibility.","These new results on tree correlation testing allow us to solve a class of random subgraph isomorphism problems, resolving an open problem in the field."],"url":"http://arxiv.org/abs/2504.02299v1"}
{"created":"2025-04-03 06:05:05","title":"SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks","abstract":"Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility. However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios. Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior. To address these challenges, we propose $\\textbf{SP}$ike-$\\textbf{A}$ware $\\textbf{C}$onsistency $\\textbf{E}$nhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data. We evaluate SPACE on multiple datasets, including CIFAR-10-C, CIFAR-100-C, Tiny-ImageNet-C and DVS Gesture-C. Furthermore, SPACE demonstrates strong generalization across different model architectures, achieving consistent performance improvements on both VGG9 and ResNet11. Experimental results show that SPACE outperforms state-of-the-art methods, highlighting its effectiveness and robustness in real-world settings.","sentences":["Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility.","However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios.","Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior.","To address these challenges, we propose $\\textbf{SP}$ike-$\\textbf{A}$ware $\\textbf{C}$onsistency $\\textbf{E}$nhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs.","SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data.","We evaluate SPACE on multiple datasets, including CIFAR-10-C, CIFAR-100-C, Tiny-ImageNet-C and DVS Gesture-C. Furthermore, SPACE demonstrates strong generalization across different model architectures, achieving consistent performance improvements on both VGG9 and ResNet11.","Experimental results show that SPACE outperforms state-of-the-art methods, highlighting its effectiveness and robustness in real-world settings."],"url":"http://arxiv.org/abs/2504.02298v1"}
{"created":"2025-04-03 05:47:51","title":"State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla","abstract":"Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language (BdSL) remains a understudied domain. Specifically, there are no works on Bangla text-to-gloss translation task. To address this gap, we begin by addressing the dataset problem. We take inspiration from grammatical rule based gloss generation used in Germany and American sign langauage (ASL) and adapt it for BdSL. We also leverage LLM to generate synthetic data and use back-translation, text generation for data augmentation. With dataset prepared, we started experimentation. We fine-tuned pretrained mBART-50 and mBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a novel seq-to-seq model with multi-head attention. We observe significant high performance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual model from Facebook. We then explored why we observe such high performance with mBART. We soon notice an interesting property of mBART -- it was trained on shuffled and masked text data. And as we know, gloss form has shuffling property. So we hypothesize that mBART is inherently good at text-to-gloss tasks. To find support against this hypothesis, we trained mBART-50 on PHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50 finetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark, far outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 = 55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on the results, this study proposes a new paradigm for text-to-gloss task using mBART models. Additionally, our results show that BdSL text-to-gloss task can greatly benefit from rule-based synthetic dataset.","sentences":["Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language (BdSL) remains a understudied domain.","Specifically, there are no works on Bangla text-to-gloss translation task.","To address this gap, we begin by addressing the dataset problem.","We take inspiration from grammatical rule based gloss generation used in Germany and American sign langauage (ASL) and adapt it for BdSL.","We also leverage LLM to generate synthetic data and use back-translation, text generation for data augmentation.","With dataset prepared, we started experimentation.","We fine-tuned pretrained mBART-50 and mBERT-multiclass-uncased model on our dataset.","We also trained GRU, RNN and a novel seq-to-seq model with multi-head attention.","We observe significant high performance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual model from Facebook.","We then explored why we observe such high performance with mBART.","We soon notice an interesting property of mBART -- it was trained on shuffled and masked text data.","And as we know, gloss form has shuffling property.","So we hypothesize that mBART is inherently good at text-to-gloss tasks.","To find support against this hypothesis, we trained mBART-50 on PHOENIX-14T benchmark and evaluated it with existing literature.","Our mBART-50 finetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark, far outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 = 55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624).","Based on the results, this study proposes a new paradigm for text-to-gloss task using mBART models.","Additionally, our results show that BdSL text-to-gloss task can greatly benefit from rule-based synthetic dataset."],"url":"http://arxiv.org/abs/2504.02293v1"}
{"created":"2025-04-03 05:27:55","title":"FEASE: Shallow AutoEncoding Recommender with Cold Start Handling via Side Features","abstract":"User and item cold starts present significant challenges in industrial applications of recommendation systems. Supplementing user-item interaction data with metadata is a common solution-but often at the cost of introducing additional biases. In this work, we introduce an augmented EASE model, i.e. FEASE, that seamlessly integrates both user and item side information to address these cold start issues. Our straightforward, autoencoder-based method produces a closed-form solution that leverages rich content signals for cold items while refining user representations in data-sparse environments. Importantly, our method strikes a balance by effectively recommending cold start items and handling cold start users without incurring extra bias, and it maintains strong performance in warm settings. Experimental results demonstrate improved recommendation accuracy and robustness compared to previous collaborative filtering approaches. Moreover, our model serves as a strong baseline for future comparative studies.","sentences":["User and item cold starts present significant challenges in industrial applications of recommendation systems.","Supplementing user-item interaction data with metadata is a common solution-but often at the cost of introducing additional biases.","In this work, we introduce an augmented EASE model, i.e. FEASE, that seamlessly integrates both user and item side information to address these cold start issues.","Our straightforward, autoencoder-based method produces a closed-form solution that leverages rich content signals for cold items while refining user representations in data-sparse environments.","Importantly, our method strikes a balance by effectively recommending cold start items and handling cold start users without incurring extra bias, and it maintains strong performance in warm settings.","Experimental results demonstrate improved recommendation accuracy and robustness compared to previous collaborative filtering approaches.","Moreover, our model serves as a strong baseline for future comparative studies."],"url":"http://arxiv.org/abs/2504.02288v1"}
{"created":"2025-04-03 05:23:08","title":"MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion","abstract":"Multi-modal multi-view action recognition is a rapidly growing field in computer vision, offering significant potential for applications in surveillance. However, current datasets often fail to address real-world challenges such as wide-area environmental conditions, asynchronous data streams, and the lack of frame-level annotations. Furthermore, existing methods face difficulties in effectively modeling inter-view relationships and enhancing spatial feature learning. In this study, we propose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF) method and introduce the MultiSensor-Home dataset, a novel benchmark designed for comprehensive action recognition in home environments. The MultiSensor-Home dataset features untrimmed videos captured by distributed sensors, providing high-resolution RGB and audio data along with detailed multi-view frame-level action labels. The proposed MultiTSF method leverages a Transformer-based fusion mechanism to dynamically model inter-view relationships. Furthermore, the method also integrates a external human detection module to enhance spatial feature learning. Experiments on MultiSensor-Home and MM-Office datasets demonstrate the superiority of MultiTSF over the state-of-the-art methods. The quantitative and qualitative results highlight the effectiveness of the proposed method in advancing real-world multi-modal multi-view action recognition.","sentences":["Multi-modal multi-view action recognition is a rapidly growing field in computer vision, offering significant potential for applications in surveillance.","However, current datasets often fail to address real-world challenges such as wide-area environmental conditions, asynchronous data streams, and the lack of frame-level annotations.","Furthermore, existing methods face difficulties in effectively modeling inter-view relationships and enhancing spatial feature learning.","In this study, we propose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF) method and introduce the MultiSensor-Home dataset, a novel benchmark designed for comprehensive action recognition in home environments.","The MultiSensor-Home dataset features untrimmed videos captured by distributed sensors, providing high-resolution RGB and audio data along with detailed multi-view frame-level action labels.","The proposed MultiTSF method leverages a Transformer-based fusion mechanism to dynamically model inter-view relationships.","Furthermore, the method also integrates a external human detection module to enhance spatial feature learning.","Experiments on MultiSensor-Home and MM-Office datasets demonstrate the superiority of MultiTSF over the state-of-the-art methods.","The quantitative and qualitative results highlight the effectiveness of the proposed method in advancing real-world multi-modal multi-view action recognition."],"url":"http://arxiv.org/abs/2504.02287v1"}
{"created":"2025-04-03 05:09:43","title":"Ga$_2$O$_3$ TCAD Mobility Parameter Calibration using Simulation Augmented Machine Learning with Physics Informed Neural Network","abstract":"In this paper, we demonstrate the possibility of performing automatic Technology Computer-Aided-Design (TCAD) parameter calibration using machine learning, verified with experimental data. The machine only needs to be trained by TCAD data. Schottky Barrier Diode (SBD) fabricated with emerging ultra-wide-bandgap material, Gallium Oxide (Ga$_2$O$_3$), is measured and its current-voltage (IV) is used for Ga$_2$O$_3$ Philips Unified Mobility (PhuMob) model parameters, effective anode workfunction, and ambient temperature extraction (7 parameters). A machine comprised of an autoencoder (AE) and a neural network (NN) (AE-NN) is used. Ga$_2$O$_3$ PhuMob parameters are extracted from the noisy experimental curves. TCAD simulation with the extracted parameters shows that the quality of the parameters is as good as an expert's calibration at the pre-turned-on regime but not in the on-state regime. By using a simple physics-informed neural network (PINN) (AE-PINN), the machine performs as well as the human expert in all regimes.","sentences":["In this paper, we demonstrate the possibility of performing automatic Technology Computer-Aided-Design (TCAD) parameter calibration using machine learning, verified with experimental data.","The machine only needs to be trained by TCAD data.","Schottky Barrier Diode (SBD) fabricated with emerging ultra-wide-bandgap material, Gallium Oxide (Ga$_2$O$_3$), is measured and its current-voltage (IV) is used for Ga$_2$O$_3$ Philips Unified Mobility (PhuMob) model parameters, effective anode workfunction, and ambient temperature extraction (7 parameters).","A machine comprised of an autoencoder (AE) and a neural network (NN) (AE-NN) is used.","Ga$_2$O$_3$","PhuMob parameters are extracted from the noisy experimental curves.","TCAD simulation with the extracted parameters shows that the quality of the parameters is as good as an expert's calibration at the pre-turned-on regime but not in the on-state regime.","By using a simple physics-informed neural network (PINN) (AE-PINN), the machine performs as well as the human expert in all regimes."],"url":"http://arxiv.org/abs/2504.02283v1"}
{"created":"2025-04-03 05:08:04","title":"Parallel Market Environments for FinRL Contests","abstract":"Reinforcement learning has shown great potential in finance. We have organized the FinRL Contests 2023-2025 featuring different financial tasks. Large language models have a strong capability to process financial texts. Integrating LLM-generated signals into FinRL is a new task, enabling agents to use both structured market data and unstructured financial text. To address the sampling bottleneck during training, we introduce GPU-based parallel market environments to improve sampling speed. In this paper, we summarize the parallel market environments used in FinRL Contests 2023-2025. Two new environments incorporate LLM-generated signals and support massively parallel simulation. Contestants utilize these environments to train agents for stock and cryptocurrency trading tasks.","sentences":["Reinforcement learning has shown great potential in finance.","We have organized the FinRL Contests 2023-2025 featuring different financial tasks.","Large language models have a strong capability to process financial texts.","Integrating LLM-generated signals into FinRL is a new task, enabling agents to use both structured market data and unstructured financial text.","To address the sampling bottleneck during training, we introduce GPU-based parallel market environments to improve sampling speed.","In this paper, we summarize the parallel market environments used in FinRL Contests 2023-2025.","Two new environments incorporate LLM-generated signals and support massively parallel simulation.","Contestants utilize these environments to train agents for stock and cryptocurrency trading tasks."],"url":"http://arxiv.org/abs/2504.02281v1"}
{"created":"2025-04-03 05:06:06","title":"LLM-Guided Evolution: An Autonomous Model Optimization for Object Detection","abstract":"In machine learning, Neural Architecture Search (NAS) requires domain knowledge of model design and a large amount of trial-and-error to achieve promising performance. Meanwhile, evolutionary algorithms have traditionally relied on fixed rules and pre-defined building blocks. The Large Language Model (LLM)-Guided Evolution (GE) framework transformed this approach by incorporating LLMs to directly modify model source code for image classification algorithms on CIFAR data and intelligently guide mutations and crossovers. A key element of LLM-GE is the \"Evolution of Thought\" (EoT) technique, which establishes feedback loops, allowing LLMs to refine their decisions iteratively based on how previous operations performed. In this study, we perform NAS for object detection by improving LLM-GE to modify the architecture of You Only Look Once (YOLO) models to enhance performance on the KITTI dataset. Our approach intelligently adjusts the design and settings of YOLO to find the optimal algorithms against objective such as detection accuracy and speed. We show that LLM-GE produced variants with significant performance improvements, such as an increase in Mean Average Precision from 92.5% to 94.5%. This result highlights the flexibility and effectiveness of LLM-GE on real-world challenges, offering a novel paradigm for automated machine learning that combines LLM-driven reasoning with evolutionary strategies.","sentences":["In machine learning, Neural Architecture Search (NAS) requires domain knowledge of model design and a large amount of trial-and-error to achieve promising performance.","Meanwhile, evolutionary algorithms have traditionally relied on fixed rules and pre-defined building blocks.","The Large Language Model (LLM)-Guided Evolution (GE) framework transformed this approach by incorporating LLMs to directly modify model source code for image classification algorithms on CIFAR data and intelligently guide mutations and crossovers.","A key element of LLM-GE is the \"Evolution of Thought\" (EoT) technique, which establishes feedback loops, allowing LLMs to refine their decisions iteratively based on how previous operations performed.","In this study, we perform NAS for object detection by improving LLM-GE to modify the architecture of You Only Look Once (YOLO) models to enhance performance on the KITTI dataset.","Our approach intelligently adjusts the design and settings of YOLO to find the optimal algorithms against objective such as detection accuracy and speed.","We show that LLM-GE produced variants with significant performance improvements, such as an increase in Mean Average Precision from 92.5% to 94.5%.","This result highlights the flexibility and effectiveness of LLM-GE on real-world challenges, offering a novel paradigm for automated machine learning that combines LLM-driven reasoning with evolutionary strategies."],"url":"http://arxiv.org/abs/2504.02280v1"}
{"created":"2025-04-03 04:50:45","title":"Enhancing Customer Contact Efficiency with Graph Neural Networks in Credit Card Fraud Detection Workflow","abstract":"Credit card fraud has been a persistent issue since the last century, causing significant financial losses to the industry. The most effective way to prevent fraud is by contacting customers to verify suspicious transactions. However, while these systems are designed to detect fraudulent activity, they often mistakenly flag legitimate transactions, leading to unnecessary declines that disrupt the user experience and erode customer trust. Frequent false positives can frustrate customers, resulting in dissatisfaction, increased complaints, and a diminished sense of security. To address these limitations, we propose a fraud detection framework incorporating Relational Graph Convolutional Networks (RGCN) to enhance the accuracy and efficiency of identifying fraudulent transactions. By leveraging the relational structure of transaction data, our model reduces the need for direct customer confirmation while maintaining high detection performance. Our experiments are conducted using the IBM credit card transaction dataset to evaluate the effectiveness of this approach.","sentences":["Credit card fraud has been a persistent issue since the last century, causing significant financial losses to the industry.","The most effective way to prevent fraud is by contacting customers to verify suspicious transactions.","However, while these systems are designed to detect fraudulent activity, they often mistakenly flag legitimate transactions, leading to unnecessary declines that disrupt the user experience and erode customer trust.","Frequent false positives can frustrate customers, resulting in dissatisfaction, increased complaints, and a diminished sense of security.","To address these limitations, we propose a fraud detection framework incorporating Relational Graph Convolutional Networks (RGCN) to enhance the accuracy and efficiency of identifying fraudulent transactions.","By leveraging the relational structure of transaction data, our model reduces the need for direct customer confirmation while maintaining high detection performance.","Our experiments are conducted using the IBM credit card transaction dataset to evaluate the effectiveness of this approach."],"url":"http://arxiv.org/abs/2504.02275v1"}
{"created":"2025-04-03 04:32:37","title":"Efficient Computation of Hyper-triangles on Hypergraphs","abstract":"Hypergraphs, which use hyperedges to capture groupwise interactions among different entities, have gained increasing attention recently for their versatility in effectively modeling real-world networks. In this paper, we study the problem of computing hyper-triangles (formed by three fully-connected hyperedges), which is a basic structural unit in hypergraphs. Although existing approaches can be adopted to compute hyper-triangles by exhaustively examining hyperedge combinations, they overlook the structural characteristics distinguishing different hyper-triangle patterns. Consequently, these approaches lack specificity in computing particular hyper-triangle patterns and exhibit low efficiency. In this paper, we unveil a new formation pathway for hyper-triangles, transitioning from hyperedges to hyperwedges before assembling into hyper-triangles, and classify hyper-triangle patterns based on hyperwedges. Leveraging this insight, we introduce a two-step framework to reduce the redundant checking of hyperedge combinations. Under this framework, we propose efficient algorithms for computing a specific pattern of hyper-triangles. Approximate algorithms are also devised to support estimated counting scenarios. Furthermore, we introduce a fine-grained hypergraph clustering coefficient measurement that can reflect diverse properties of hypergraphs based on different hyper-triangle patterns. Extensive experimental evaluations conducted on 11 real-world datasets validate the effectiveness and efficiency of our proposed techniques.","sentences":["Hypergraphs, which use hyperedges to capture groupwise interactions among different entities, have gained increasing attention recently for their versatility in effectively modeling real-world networks.","In this paper, we study the problem of computing hyper-triangles (formed by three fully-connected hyperedges), which is a basic structural unit in hypergraphs.","Although existing approaches can be adopted to compute hyper-triangles by exhaustively examining hyperedge combinations, they overlook the structural characteristics distinguishing different hyper-triangle patterns.","Consequently, these approaches lack specificity in computing particular hyper-triangle patterns and exhibit low efficiency.","In this paper, we unveil a new formation pathway for hyper-triangles, transitioning from hyperedges to hyperwedges before assembling into hyper-triangles, and classify hyper-triangle patterns based on hyperwedges.","Leveraging this insight, we introduce a two-step framework to reduce the redundant checking of hyperedge combinations.","Under this framework, we propose efficient algorithms for computing a specific pattern of hyper-triangles.","Approximate algorithms are also devised to support estimated counting scenarios.","Furthermore, we introduce a fine-grained hypergraph clustering coefficient measurement that can reflect diverse properties of hypergraphs based on different hyper-triangle patterns.","Extensive experimental evaluations conducted on 11 real-world datasets validate the effectiveness and efficiency of our proposed techniques."],"url":"http://arxiv.org/abs/2504.02271v1"}
{"created":"2025-04-03 04:31:56","title":"MinkOcc: Towards real-time label-efficient semantic occupancy prediction","abstract":"Developing 3D semantic occupancy prediction models often relies on dense 3D annotations for supervised learning, a process that is both labor and resource-intensive, underscoring the need for label-efficient or even label-free approaches. To address this, we introduce MinkOcc, a multi-modal 3D semantic occupancy prediction framework for cameras and LiDARs that proposes a two-step semi-supervised training procedure. Here, a small dataset of explicitly 3D annotations warm-starts the training process; then, the supervision is continued by simpler-to-annotate accumulated LiDAR sweeps and images -- semantically labelled through vision foundational models. MinkOcc effectively utilizes these sensor-rich supervisory cues and reduces reliance on manual labeling by 90\\% while maintaining competitive accuracy. In addition, the proposed model incorporates information from LiDAR and camera data through early fusion and leverages sparse convolution networks for real-time prediction. With its efficiency in both supervision and computation, we aim to extend MinkOcc beyond curated datasets, enabling broader real-world deployment of 3D semantic occupancy prediction in autonomous driving.","sentences":["Developing 3D semantic occupancy prediction models often relies on dense 3D annotations for supervised learning, a process that is both labor and resource-intensive, underscoring the need for label-efficient or even label-free approaches.","To address this, we introduce MinkOcc, a multi-modal 3D semantic occupancy prediction framework for cameras and LiDARs that proposes a two-step semi-supervised training procedure.","Here, a small dataset of explicitly 3D annotations warm-starts the training process; then, the supervision is continued by simpler-to-annotate accumulated LiDAR sweeps and images -- semantically labelled through vision foundational models.","MinkOcc effectively utilizes these sensor-rich supervisory cues and reduces reliance on manual labeling by 90\\% while maintaining competitive accuracy.","In addition, the proposed model incorporates information from LiDAR and camera data through early fusion and leverages sparse convolution networks for real-time prediction.","With its efficiency in both supervision and computation, we aim to extend MinkOcc beyond curated datasets, enabling broader real-world deployment of 3D semantic occupancy prediction in autonomous driving."],"url":"http://arxiv.org/abs/2504.02270v1"}
{"created":"2025-04-03 04:30:10","title":"Engineering Artificial Intelligence: Framework, Challenges, and Future Direction","abstract":"Over the past ten years, the application of artificial intelligence (AI) and machine learning (ML) in engineering domains has gained significant popularity, showcasing their potential in data-driven contexts. However, the complexity and diversity of engineering problems often require the development of domain-specific AI approaches, which are frequently hindered by a lack of systematic methodologies, scalability, and robustness during the development process. To address this gap, this paper introduces the \"ABCDE\" as the key elements of Engineering AI and proposes a unified, systematic engineering AI ecosystem framework, including eight essential layers, along with attributes, goals, and applications, to guide the development and deployment of AI solutions for specific engineering needs. Additionally, key challenges are examined, and nine future research directions are highlighted. By providing a comprehensive perspective, this paper aims to advance the strategic implementation of AI, fostering the development of next-generation engineering AI solutions.","sentences":["Over the past ten years, the application of artificial intelligence (AI) and machine learning (ML) in engineering domains has gained significant popularity, showcasing their potential in data-driven contexts.","However, the complexity and diversity of engineering problems often require the development of domain-specific AI approaches, which are frequently hindered by a lack of systematic methodologies, scalability, and robustness during the development process.","To address this gap, this paper introduces the \"ABCDE\" as the key elements of Engineering AI and proposes a unified, systematic engineering AI ecosystem framework, including eight essential layers, along with attributes, goals, and applications, to guide the development and deployment of AI solutions for specific engineering needs.","Additionally, key challenges are examined, and nine future research directions are highlighted.","By providing a comprehensive perspective, this paper aims to advance the strategic implementation of AI, fostering the development of next-generation engineering AI solutions."],"url":"http://arxiv.org/abs/2504.02269v1"}
{"created":"2025-04-03 04:27:02","title":"Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and Synthetic Data","abstract":"This report investigates enhancing semantic caching effectiveness by employing specialized, fine-tuned embedding models. Semantic caching relies on embedding similarity rather than exact key matching, presenting unique challenges in balancing precision, query latency, and computational efficiency. We propose leveraging smaller, domain-specific embedding models, fine-tuned with targeted real-world and synthetically generated datasets. Our empirical evaluations demonstrate that compact embedding models fine-tuned for just one epoch on specialized datasets significantly surpass both state-of-the-art open-source and proprietary alternatives in precision and recall. Moreover, we introduce a novel synthetic data generation pipeline for the semantic cache that mitigates the challenge of limited domain-specific annotated data, further boosting embedding performance. Our approach effectively balances computational overhead and accuracy, establishing a viable and efficient strategy for practical semantic caching implementations.","sentences":["This report investigates enhancing semantic caching effectiveness by employing specialized, fine-tuned embedding models.","Semantic caching relies on embedding similarity rather than exact key matching, presenting unique challenges in balancing precision, query latency, and computational efficiency.","We propose leveraging smaller, domain-specific embedding models, fine-tuned with targeted real-world and synthetically generated datasets.","Our empirical evaluations demonstrate that compact embedding models fine-tuned for just one epoch on specialized datasets significantly surpass both state-of-the-art open-source and proprietary alternatives in precision and recall.","Moreover, we introduce a novel synthetic data generation pipeline for the semantic cache that mitigates the challenge of limited domain-specific annotated data, further boosting embedding performance.","Our approach effectively balances computational overhead and accuracy, establishing a viable and efficient strategy for practical semantic caching implementations."],"url":"http://arxiv.org/abs/2504.02268v1"}
{"created":"2025-04-03 04:20:44","title":"MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism","abstract":"Mixture-of-Experts (MoE) showcases tremendous potential to scale large language models (LLMs) with enhanced performance and reduced computational complexity. However, its sparsely activated architecture shifts feed-forward networks (FFNs) from being compute-intensive to memory-intensive during inference, leading to substantially lower GPU utilization and increased operational costs. We present MegaScale-Infer, an efficient and cost-effective system for serving large-scale MoE models. MegaScale-Infer disaggregates attention and FFN modules within each model layer, enabling independent scaling, tailored parallelism strategies, and heterogeneous deployment for both modules. To fully exploit disaggregation in the presence of MoE's sparsity, MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a request batch into micro-batches and shuttles them between attention and FFNs for inference. Combined with distinct model parallelism for each module, MegaScale-Infer effectively hides communication overhead and maximizes GPU utilization. To adapt to disaggregated attention and FFN modules and minimize data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a high-performance M2N communication library that eliminates unnecessary GPU-to-CPU data copies, group initialization overhead, and GPU synchronization. Experimental results indicate that MegaScale-Infer achieves up to 1.90x higher per-GPU throughput than state-of-the-art solutions.","sentences":["Mixture-of-Experts (MoE) showcases tremendous potential to scale large language models (LLMs) with enhanced performance and reduced computational complexity.","However, its sparsely activated architecture shifts feed-forward networks (FFNs) from being compute-intensive to memory-intensive during inference, leading to substantially lower GPU utilization and increased operational costs.","We present MegaScale-Infer, an efficient and cost-effective system for serving large-scale MoE models.","MegaScale-Infer disaggregates attention and FFN modules within each model layer, enabling independent scaling, tailored parallelism strategies, and heterogeneous deployment for both modules.","To fully exploit disaggregation in the presence of MoE's sparsity, MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a request batch into micro-batches and shuttles them between attention and FFNs for inference.","Combined with distinct model parallelism for each module, MegaScale-Infer effectively hides communication overhead and maximizes GPU utilization.","To adapt to disaggregated attention and FFN modules and minimize data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a high-performance M2N communication library that eliminates unnecessary GPU-to-CPU data copies, group initialization overhead, and GPU synchronization.","Experimental results indicate that MegaScale-Infer achieves up to 1.90x higher per-GPU throughput than state-of-the-art solutions."],"url":"http://arxiv.org/abs/2504.02263v1"}
{"created":"2025-04-03 04:07:18","title":"Implicit Neural Differential Model for Spatiotemporal Dynamics","abstract":"Hybrid neural-physics modeling frameworks through differentiable programming have emerged as powerful tools in scientific machine learning, enabling the integration of known physics with data-driven learning to improve prediction accuracy and generalizability. However, most existing hybrid frameworks rely on explicit recurrent formulations, which suffer from numerical instability and error accumulation during long-horizon forecasting. In this work, we introduce Im-PiNDiff, a novel implicit physics-integrated neural differentiable solver for stable and accurate modeling of spatiotemporal dynamics. Inspired by deep equilibrium models, Im-PiNDiff advances the state using implicit fixed-point layers, enabling robust long-term simulation while remaining fully end-to-end differentiable. To enable scalable training, we introduce a hybrid gradient propagation strategy that integrates adjoint-state methods with reverse-mode automatic differentiation. This approach eliminates the need to store intermediate solver states and decouples memory complexity from the number of solver iterations, significantly reducing training overhead. We further incorporate checkpointing techniques to manage memory in long-horizon rollouts. Numerical experiments on various spatiotemporal PDE systems, including advection-diffusion processes, Burgers' dynamics, and multi-physics chemical vapor infiltration processes, demonstrate that Im-PiNDiff achieves superior predictive performance, enhanced numerical stability, and substantial reductions in memory and runtime cost relative to explicit and naive implicit baselines. This work provides a principled, efficient, and scalable framework for hybrid neural-physics modeling.","sentences":["Hybrid neural-physics modeling frameworks through differentiable programming have emerged as powerful tools in scientific machine learning, enabling the integration of known physics with data-driven learning to improve prediction accuracy and generalizability.","However, most existing hybrid frameworks rely on explicit recurrent formulations, which suffer from numerical instability and error accumulation during long-horizon forecasting.","In this work, we introduce Im-PiNDiff, a novel implicit physics-integrated neural differentiable solver for stable and accurate modeling of spatiotemporal dynamics.","Inspired by deep equilibrium models, Im-PiNDiff advances the state using implicit fixed-point layers, enabling robust long-term simulation while remaining fully end-to-end differentiable.","To enable scalable training, we introduce a hybrid gradient propagation strategy that integrates adjoint-state methods with reverse-mode automatic differentiation.","This approach eliminates the need to store intermediate solver states and decouples memory complexity from the number of solver iterations, significantly reducing training overhead.","We further incorporate checkpointing techniques to manage memory in long-horizon rollouts.","Numerical experiments on various spatiotemporal PDE systems, including advection-diffusion processes, Burgers' dynamics, and multi-physics chemical vapor infiltration processes, demonstrate that Im-PiNDiff achieves superior predictive performance, enhanced numerical stability, and substantial reductions in memory and runtime cost relative to explicit and naive implicit baselines.","This work provides a principled, efficient, and scalable framework for hybrid neural-physics modeling."],"url":"http://arxiv.org/abs/2504.02260v1"}
{"created":"2025-04-03 03:41:30","title":"Adapting World Models with Latent-State Dynamics Residuals","abstract":"Simulation-to-reality reinforcement learning (RL) faces the critical challenge of reconciling discrepancies between simulated and real-world dynamics, which can severely degrade agent performance. A promising approach involves learning corrections to simulator forward dynamics represented as a residual error function, however this operation is impractical with high-dimensional states such as images. To overcome this, we propose ReDRAW, a latent-state autoregressive world model pretrained in simulation and calibrated to target environments through residual corrections of latent-state dynamics rather than of explicit observed states. Using this adapted world model, ReDRAW enables RL agents to be optimized with imagined rollouts under corrected dynamics and then deployed in the real world. In multiple vision-based MuJoCo domains and a physical robot visual lane-following task, ReDRAW effectively models changes to dynamics and avoids overfitting in low data regimes where traditional transfer methods fail.","sentences":["Simulation-to-reality reinforcement learning (RL) faces the critical challenge of reconciling discrepancies between simulated and real-world dynamics, which can severely degrade agent performance.","A promising approach involves learning corrections to simulator forward dynamics represented as a residual error function, however this operation is impractical with high-dimensional states such as images.","To overcome this, we propose ReDRAW, a latent-state autoregressive world model pretrained in simulation and calibrated to target environments through residual corrections of latent-state dynamics rather than of explicit observed states.","Using this adapted world model, ReDRAW enables RL agents to be optimized with imagined rollouts under corrected dynamics and then deployed in the real world.","In multiple vision-based MuJoCo domains and a physical robot visual lane-following task, ReDRAW effectively models changes to dynamics and avoids overfitting in low data regimes where traditional transfer methods fail."],"url":"http://arxiv.org/abs/2504.02252v1"}
{"created":"2025-04-03 03:30:50","title":"Stock Price Prediction Using Triple Barrier Labeling and Raw OHLCV Data: Evidence from Korean Markets","abstract":"This paper demonstrates that deep learning models trained on raw OHLCV (open-high-low-close-volume) data can achieve comparable performance to traditional machine learning models using technical indicators for stock price prediction in Korean markets. While previous studies have emphasized the importance of technical indicators and feature engineering, we show that a simple LSTM network trained on raw OHLCV data alone can match the performance of sophisticated ML models that incorporate technical indicators. Using a dataset of Korean stocks from 2006 to 2024, we optimize the triple barrier labeling parameters to achieve balanced label proportions with a 29-day window and 9\\% barriers. Our experiments reveal that LSTM networks achieve similar performance to traditional machine learning models like XGBoost, despite using only raw OHLCV data without any technical indicators. Furthermore, we identify that the optimal window size varies with model hidden size, with a configuration of window size 100 and hidden size 8 yielding the best performance. Additionally, our results confirm that using full OHLCV data provides better predictive accuracy compared to using only close price or close price with volume. These findings challenge conventional approaches to feature engineering in financial forecasting and suggest that simpler approaches focusing on raw data and appropriate model selection may be more effective than complex feature engineering strategies.","sentences":["This paper demonstrates that deep learning models trained on raw OHLCV (open-high-low-close-volume) data can achieve comparable performance to traditional machine learning models using technical indicators for stock price prediction in Korean markets.","While previous studies have emphasized the importance of technical indicators and feature engineering, we show that a simple LSTM network trained on raw OHLCV data alone can match the performance of sophisticated ML models that incorporate technical indicators.","Using a dataset of Korean stocks from 2006 to 2024, we optimize the triple barrier labeling parameters to achieve balanced label proportions with a 29-day window and 9\\% barriers.","Our experiments reveal that LSTM networks achieve similar performance to traditional machine learning models like XGBoost, despite using only raw OHLCV data without any technical indicators.","Furthermore, we identify that the optimal window size varies with model hidden size, with a configuration of window size 100 and hidden size 8 yielding the best performance.","Additionally, our results confirm that using full OHLCV data provides better predictive accuracy compared to using only close price or close price with volume.","These findings challenge conventional approaches to feature engineering in financial forecasting and suggest that simpler approaches focusing on raw data and appropriate model selection may be more effective than complex feature engineering strategies."],"url":"http://arxiv.org/abs/2504.02249v1"}
{"created":"2025-04-03 03:01:26","title":"LLM Social Simulations Are a Promising Research Method","abstract":"Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted these methods. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a literature survey of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions with prompting, fine-tuning, and complementary methods. We believe that LLM social simulations can already be used for exploratory research, such as pilot experiments for psychology, economics, sociology, and marketing. More widespread use may soon be possible with rapidly advancing LLM capabilities, and researchers should prioritize developing conceptual models and evaluations that can be iteratively deployed and refined at pace with ongoing AI advances.","sentences":["Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems.","However, results to date have been limited, and few social scientists have adopted these methods.","In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges.","We ground our argument in a literature survey of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work.","We identify promising directions with prompting, fine-tuning, and complementary methods.","We believe that LLM social simulations can already be used for exploratory research, such as pilot experiments for psychology, economics, sociology, and marketing.","More widespread use may soon be possible with rapidly advancing LLM capabilities, and researchers should prioritize developing conceptual models and evaluations that can be iteratively deployed and refined at pace with ongoing AI advances."],"url":"http://arxiv.org/abs/2504.02234v1"}
{"created":"2025-04-03 02:13:00","title":"The Plot Thickens: Quantitative Part-by-Part Exploration of MLLM Visualization Literacy","abstract":"Multimodal Large Language Models (MLLMs) can interpret data visualizations, but what makes a visualization understandable to these models? Do factors like color, shape, and text influence legibility, and how does this compare to human perception? In this paper, we build on prior work to systematically assess which visualization characteristics impact MLLM interpretability. We expanded the Visualization Literacy Assessment Test (VLAT) test set from 12 to 380 visualizations by varying plot types, colors, and titles. This allowed us to statistically analyze how these features affect model performance. Our findings suggest that while color palettes have no significant impact on accuracy, plot types and the type of title significantly affect MLLM performance. We observe similar trends for model omissions. Based on these insights, we look into which plot types are beneficial for MLLMs in different tasks and propose visualization design principles that enhance MLLM readability. Additionally, we make the extended VLAT test set, VLAT ex, publicly available on https://osf.io/ermwx/ together with our supplemental material for future model testing and evaluation.","sentences":["Multimodal Large Language Models (MLLMs) can interpret data visualizations, but what makes a visualization understandable to these models?","Do factors like color, shape, and text influence legibility, and how does this compare to human perception?","In this paper, we build on prior work to systematically assess which visualization characteristics impact MLLM interpretability.","We expanded the Visualization Literacy Assessment Test (VLAT) test set from 12 to 380 visualizations by varying plot types, colors, and titles.","This allowed us to statistically analyze how these features affect model performance.","Our findings suggest that while color palettes have no significant impact on accuracy, plot types and the type of title significantly affect MLLM performance.","We observe similar trends for model omissions.","Based on these insights, we look into which plot types are beneficial for MLLMs in different tasks and propose visualization design principles that enhance MLLM readability.","Additionally, we make the extended VLAT test set, VLAT ex, publicly available on https://osf.io/ermwx/ together with our supplemental material for future model testing and evaluation."],"url":"http://arxiv.org/abs/2504.02217v1"}
{"created":"2025-04-03 02:08:22","title":"Geospatial Artificial Intelligence for Satellite-based Flood Extent Mapping: Concepts, Advances, and Future Perspectives","abstract":"Geospatial Artificial Intelligence (GeoAI) for satellite-based flood extent mapping systematically integrates artificial intelligence techniques with satellite data to identify flood events and assess their impacts, for disaster management and spatial decision-making. The primary output often includes flood extent maps, which delineate the affected areas, along with additional analytical outputs such as uncertainty estimation and change detection.","sentences":["Geospatial Artificial Intelligence (GeoAI) for satellite-based flood extent mapping systematically integrates artificial intelligence techniques with satellite data to identify flood events and assess their impacts, for disaster management and spatial decision-making.","The primary output often includes flood extent maps, which delineate the affected areas, along with additional analytical outputs such as uncertainty estimation and change detection."],"url":"http://arxiv.org/abs/2504.02214v1"}
{"created":"2025-04-03 02:06:57","title":"Secure Generalization through Stochastic Bidirectional Parameter Updates Using Dual-Gradient Mechanism","abstract":"Federated learning (FL) has gained increasing attention due to privacy-preserving collaborative training on decentralized clients, mitigating the need to upload sensitive data to a central server directly. Nonetheless, recent research has underscored the risk of exposing private data to adversaries, even within FL frameworks. In general, existing methods sacrifice performance while ensuring resistance to privacy leakage in FL. We overcome these issues and generate diverse models at a global server through the proposed stochastic bidirectional parameter update mechanism. Using diverse models, we improved the generalization and feature representation in the FL setup, which also helped to improve the robustness of the model against privacy leakage without hurting the model's utility. We use global models from past FL rounds to follow systematic perturbation in parameter space at the server to ensure model generalization and resistance against privacy attacks. We generate diverse models (in close neighborhoods) for each client by using systematic perturbations in model parameters at a fine-grained level (i.e., altering each convolutional filter across the layers of the model) to improve the generalization and security perspective. We evaluated our proposed approach on four benchmark datasets to validate its superiority. We surpassed the state-of-the-art methods in terms of model utility and robustness towards privacy leakage. We have proven the effectiveness of our method by evaluating performance using several quantitative and qualitative results.","sentences":["Federated learning (FL) has gained increasing attention due to privacy-preserving collaborative training on decentralized clients, mitigating the need to upload sensitive data to a central server directly.","Nonetheless, recent research has underscored the risk of exposing private data to adversaries, even within FL frameworks.","In general, existing methods sacrifice performance while ensuring resistance to privacy leakage in FL.","We overcome these issues and generate diverse models at a global server through the proposed stochastic bidirectional parameter update mechanism.","Using diverse models, we improved the generalization and feature representation in the FL setup, which also helped to improve the robustness of the model against privacy leakage without hurting the model's utility.","We use global models from past FL rounds to follow systematic perturbation in parameter space at the server to ensure model generalization and resistance against privacy attacks.","We generate diverse models (in close neighborhoods) for each client by using systematic perturbations in model parameters at a fine-grained level (i.e., altering each convolutional filter across the layers of the model) to improve the generalization and security perspective.","We evaluated our proposed approach on four benchmark datasets to validate its superiority.","We surpassed the state-of-the-art methods in terms of model utility and robustness towards privacy leakage.","We have proven the effectiveness of our method by evaluating performance using several quantitative and qualitative results."],"url":"http://arxiv.org/abs/2504.02213v1"}
{"created":"2025-04-03 02:05:08","title":"FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention","abstract":"Transformer models leverage self-attention mechanisms to capture complex dependencies, demonstrating exceptional performance in various applications. However, the long-duration high-load computations required for model inference impose stringent reliability demands on the computing platform, as soft errors that occur during execution can significantly degrade model performance. Existing fault tolerance methods protect each operation separately using decoupled kernels, incurring substantial computational and memory overhead. In this paper, we propose a novel error-resilient framework for Transformer models, integrating end-to-end fault tolerant attention (EFTA) to improve inference reliability against soft errors. Our approach enables error detection and correction within a fully fused attention kernel, reducing redundant data access and thereby mitigating memory faults. To further enhance error coverage and reduce overhead, we design a hybrid fault tolerance scheme tailored for the EFTA, introducing for the first time: 1) architecture-aware algorithm-based fault tolerance (ABFT) using tensor checksum, which minimizes inter-thread communication overhead on tensor cores during error detection; 2) selective neuron value restriction, which selectively applies adaptive fault tolerance constraints to neuron values, balancing error coverage and overhead; 3) unified verification, reusing checksums to streamline multiple computation steps into a single verification process. Experimental results show that EFTA achieves up to 7.56x speedup over traditional methods with an average fault tolerance overhead of 13.9%.","sentences":["Transformer models leverage self-attention mechanisms to capture complex dependencies, demonstrating exceptional performance in various applications.","However, the long-duration high-load computations required for model inference impose stringent reliability demands on the computing platform, as soft errors that occur during execution can significantly degrade model performance.","Existing fault tolerance methods protect each operation separately using decoupled kernels, incurring substantial computational and memory overhead.","In this paper, we propose a novel error-resilient framework for Transformer models, integrating end-to-end fault tolerant attention (EFTA) to improve inference reliability against soft errors.","Our approach enables error detection and correction within a fully fused attention kernel, reducing redundant data access and thereby mitigating memory faults.","To further enhance error coverage and reduce overhead, we design a hybrid fault tolerance scheme tailored for the EFTA, introducing for the first time: 1) architecture-aware algorithm-based fault tolerance (ABFT) using tensor checksum, which minimizes inter-thread communication overhead on tensor cores during error detection; 2) selective neuron value restriction, which selectively applies adaptive fault tolerance constraints to neuron values, balancing error coverage and overhead; 3) unified verification, reusing checksums to streamline multiple computation steps into a single verification process.","Experimental results show that EFTA achieves up to 7.56x speedup over traditional methods with an average fault tolerance overhead of 13.9%."],"url":"http://arxiv.org/abs/2504.02211v1"}
{"created":"2025-04-03 01:31:09","title":"Characterizing Creativity in Visualization Design","abstract":"Understanding the role of creativity in visualization design becomes increasingly important as the field matures, particularly with the emergence of various visualization authoring and recommendation systems. In this paper, we examine how creativity manifests in visualization design processes and how academic research has conceptualized it over time. Through a systematic review of 58 visualization papers that use the terms \"creativity\" or \"creative,\" we analyze the evolution of creative practices in visualization design. Our findings show that prior literature predominantly used atypical designs through free-form drawings, infographics, pictorials, and data comics to define creative representations. However, creativity in visualization design extends beyond visual representations to encompass early needfinding design activities such as sketching, storyboarding, discussion, and card sorting. Data visualization can also support a wide variety of creative tasks (e.g., fiction writing). We discuss the implications of these findings for fostering innovation within established design paradigms and for developing more sophisticated visualization authoring systems. The full list of coded papers are available here: https://vizcreativity.notion.site/coded-papers.","sentences":["Understanding the role of creativity in visualization design becomes increasingly important as the field matures, particularly with the emergence of various visualization authoring and recommendation systems.","In this paper, we examine how creativity manifests in visualization design processes and how academic research has conceptualized it over time.","Through a systematic review of 58 visualization papers that use the terms \"creativity\" or \"creative,\" we analyze the evolution of creative practices in visualization design.","Our findings show that prior literature predominantly used atypical designs through free-form drawings, infographics, pictorials, and data comics to define creative representations.","However, creativity in visualization design extends beyond visual representations to encompass early needfinding design activities such as sketching, storyboarding, discussion, and card sorting.","Data visualization can also support a wide variety of creative tasks (e.g., fiction writing).","We discuss the implications of these findings for fostering innovation within established design paradigms and for developing more sophisticated visualization authoring systems.","The full list of coded papers are available here: https://vizcreativity.notion.site/coded-papers."],"url":"http://arxiv.org/abs/2504.02204v1"}
{"created":"2025-04-03 00:47:55","title":"Design and Implementation of the Transparent, Interpretable, and Multimodal (TIM) AR Personal Assistant","abstract":"The concept of an AI assistant for task guidance is rapidly shifting from a science fiction staple to an impending reality. Such a system is inherently complex, requiring models for perceptual grounding, attention, and reasoning, an intuitive interface that adapts to the performer's needs, and the orchestration of data streams from many sensors. Moreover, all data acquired by the system must be readily available for post-hoc analysis to enable developers to understand performer behavior and quickly detect failures. We introduce TIM, the first end-to-end AI-enabled task guidance system in augmented reality which is capable of detecting both the user and scene as well as providing adaptable, just-in-time feedback. We discuss the system challenges and propose design solutions. We also demonstrate how TIM adapts to domain applications with varying needs, highlighting how the system components can be customized for each scenario.","sentences":["The concept of an AI assistant for task guidance is rapidly shifting from a science fiction staple to an impending reality.","Such a system is inherently complex, requiring models for perceptual grounding, attention, and reasoning, an intuitive interface that adapts to the performer's needs, and the orchestration of data streams from many sensors.","Moreover, all data acquired by the system must be readily available for post-hoc analysis to enable developers to understand performer behavior and quickly detect failures.","We introduce TIM, the first end-to-end AI-enabled task guidance system in augmented reality which is capable of detecting both the user and scene as well as providing adaptable, just-in-time feedback.","We discuss the system challenges and propose design solutions.","We also demonstrate how TIM adapts to domain applications with varying needs, highlighting how the system components can be customized for each scenario."],"url":"http://arxiv.org/abs/2504.02197v1"}
{"created":"2025-04-03 00:40:09","title":"LLM-Augmented Graph Neural Recommenders: Integrating User Reviews","abstract":"Recommender systems increasingly aim to combine signals from both user reviews and purchase (or other interaction) behaviors. While user-written comments provide explicit insights about preferences, merging these textual representations from large language models (LLMs) with graph-based embeddings of user actions remains a challenging task. In this work, we propose a framework that employs both a Graph Neural Network (GNN)-based model and an LLM to produce review-aware representations, preserving review semantics while mitigating textual noise. Our approach utilizes a hybrid objective that balances user-item interactions against text-derived features, ensuring that user's both behavioral and linguistic signals are effectively captured. We evaluate this method on multiple datasets from diverse application domains, demonstrating consistent improvements over a baseline GNN-based recommender model. Notably, our model achieves significant gains in recommendation accuracy when review data is sparse or unevenly distributed. These findings highlight the importance of integrating LLM-driven textual feedback with GNN-derived user behavioral patterns to develop robust, context-aware recommender systems.","sentences":["Recommender systems increasingly aim to combine signals from both user reviews and purchase (or other interaction) behaviors.","While user-written comments provide explicit insights about preferences, merging these textual representations from large language models (LLMs) with graph-based embeddings of user actions remains a challenging task.","In this work, we propose a framework that employs both a Graph Neural Network (GNN)-based model and an LLM to produce review-aware representations, preserving review semantics while mitigating textual noise.","Our approach utilizes a hybrid objective that balances user-item interactions against text-derived features, ensuring that user's both behavioral and linguistic signals are effectively captured.","We evaluate this method on multiple datasets from diverse application domains, demonstrating consistent improvements over a baseline GNN-based recommender model.","Notably, our model achieves significant gains in recommendation accuracy when review data is sparse or unevenly distributed.","These findings highlight the importance of integrating LLM-driven textual feedback with GNN-derived user behavioral patterns to develop robust, context-aware recommender systems."],"url":"http://arxiv.org/abs/2504.02195v1"}
{"created":"2025-04-03 00:36:40","title":"More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment","abstract":"Aligning large language models (LLMs) with human values is an increasingly critical step in post-training. Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF). Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data. Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training. This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts. The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes. Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool. We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints. Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings.","sentences":["Aligning large language models (LLMs) with human values is an increasingly critical step in post-training.","Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF).","Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data.","Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training.","This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts.","The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes.","Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool.","We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints.","Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings."],"url":"http://arxiv.org/abs/2504.02193v1"}
{"created":"2025-04-03 00:20:09","title":"A PTAS for Travelling Salesman Problem with Neighbourhoods Over Parallel Line Segments of Similar Length","abstract":"We consider the Travelling Salesman Problem with Neighbourhoods (TSPN) on the Euclidean plane ($\\RR^2$) and present a Polynomial-Time Approximation Scheme (PTAS) when the neighbourhoods are parallel line segments with lengths between $ [1, \\lambda] $ for any constant value $ \\lambda \\ge 1 $.   In TSPN (which generalizes classic TSP), each client represents a set (or neighbourhood) of points in a metric   and the goal is to find a minimum cost TSP tour that visits   at least one point from each client set. In the Euclidean setting, each neighbourhood is a region on the plane.   TSPN is significantly more difficult than classic TSP even in the Euclidean setting, as it captures group TSP.   A notable case of TSPN is when each neighbourhood is a line segment. Although there are PTASs for when   neighbourhoods are fat objects (with limited overlap), TSPN over line segments is \\textbf{APX}-hard even if all   the line segments have unit length. For parallel (unit) line segments, the best approximation factor is $3\\sqrt2$ from more than two decades ago \\cite{DM03}.   The PTAS we present in this paper settles the approximability of this case of the problem. Our algorithm finds a $ (1 + \\eps) $-factor approximation for an instance of the problem for $n$ segments with lengths in $ [1,\\lambda] $ in time $ n^{O(\\lambda/\\eps^3)} $.","sentences":["We consider the Travelling Salesman Problem with Neighbourhoods (TSPN) on the Euclidean plane ($\\RR^2$) and present a Polynomial-Time Approximation Scheme (PTAS) when the neighbourhoods are parallel line segments with lengths between $ [1, \\lambda] $ for any constant value $ \\lambda \\ge 1 $.   ","In TSPN (which generalizes classic TSP), each client represents a set (or neighbourhood) of points in a metric   and the goal is to find a minimum cost TSP tour that visits   at least one point from each client set.","In the Euclidean setting, each neighbourhood is a region on the plane.   ","TSPN is significantly more difficult than classic TSP even in the Euclidean setting, as it captures group TSP.   ","A notable case of TSPN is when each neighbourhood is a line segment.","Although there are PTASs for when   neighbourhoods are fat objects (with limited overlap), TSPN over line segments is \\textbf{APX}-hard even if all   the line segments have unit length.","For parallel (unit) line segments, the best approximation factor is $3\\sqrt2$ from more than two decades ago \\cite{DM03}.   ","The PTAS we present in this paper settles the approximability of this case of the problem.","Our algorithm finds a $ (1 + \\eps) $-factor approximation for an instance of the problem for $n$ segments with lengths in $ [1,\\lambda] $ in time $ n^{O(\\lambda/\\eps^3)} $."],"url":"http://arxiv.org/abs/2504.02190v1"}
{"created":"2025-04-02 23:51:27","title":"A Survey of Scaling in Large Language Model Reasoning","abstract":"The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.","sentences":["The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration.","However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness.","In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities.","We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning.","Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency.","We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes.","Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement.","Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning.","By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems."],"url":"http://arxiv.org/abs/2504.02181v1"}
{"created":"2025-04-02 23:51:13","title":"Foreground Focus: Enhancing Coherence and Fidelity in Camouflaged Image Generation","abstract":"Camouflaged image generation is emerging as a solution to data scarcity in camouflaged vision perception, offering a cost-effective alternative to data collection and labeling. Recently, the state-of-the-art approach successfully generates camouflaged images using only foreground objects. However, it faces two critical weaknesses: 1) the background knowledge does not integrate effectively with foreground features, resulting in a lack of foreground-background coherence (e.g., color discrepancy); 2) the generation process does not prioritize the fidelity of foreground objects, which leads to distortion, particularly for small objects. To address these issues, we propose a Foreground-Aware Camouflaged Image Generation (FACIG) model. Specifically, we introduce a Foreground-Aware Feature Integration Module (FAFIM) to strengthen the integration between foreground features and background knowledge. In addition, a Foreground-Aware Denoising Loss is designed to enhance foreground reconstruction supervision. Experiments on various datasets show our method outperforms previous methods in overall camouflaged image quality and foreground fidelity.","sentences":["Camouflaged image generation is emerging as a solution to data scarcity in camouflaged vision perception, offering a cost-effective alternative to data collection and labeling.","Recently, the state-of-the-art approach successfully generates camouflaged images using only foreground objects.","However, it faces two critical weaknesses: 1) the background knowledge does not integrate effectively with foreground features, resulting in a lack of foreground-background coherence (e.g., color discrepancy); 2) the generation process does not prioritize the fidelity of foreground objects, which leads to distortion, particularly for small objects.","To address these issues, we propose a Foreground-Aware Camouflaged Image Generation (FACIG) model.","Specifically, we introduce a Foreground-Aware Feature Integration Module (FAFIM) to strengthen the integration between foreground features and background knowledge.","In addition, a Foreground-Aware Denoising Loss is designed to enhance foreground reconstruction supervision.","Experiments on various datasets show our method outperforms previous methods in overall camouflaged image quality and foreground fidelity."],"url":"http://arxiv.org/abs/2504.02180v1"}
{"created":"2025-04-02 23:17:14","title":"FastFlow: Early Yet Robust Network Flow Classification using the Minimal Number of Time-Series Packets","abstract":"Network traffic classification is of great importance for network operators in their daily routines, such as analyzing the usage patterns of multimedia applications and optimizing network configurations. Internet service providers (ISPs) that operate high-speed links expect network flow classifiers to accurately classify flows early, using the minimal number of necessary initial packets per flow. These classifiers must also be robust to packet sequence disorders in candidate flows and capable of detecting unseen flow types that are not within the existing classification scope, which are not well achieved by existing methods. In this paper, we develop FastFlow, a time-series flow classification method that accurately classifies network flows as one of the known types or the unknown type, which dynamically selects the minimal number of packets to balance accuracy and efficiency. Toward the objectives, we first develop a flow representation process that converts packet streams at both per-packet and per-slot granularity for precise packet statistics with robustness to packet sequence disorders. Second, we develop a sequential decision-based classification model that leverages LSTM architecture trained with reinforcement learning. Our model makes dynamic decisions on the minimal number of time-series data points per flow for the confident classification as one of the known flow types or an unknown one. We evaluated our method on public datasets and demonstrated its superior performance in early and accurate flow classification. Deployment insights on the classification of over 22.9 million flows across seven application types and 33 content providers in a campus network over one week are discussed, showing that FastFlow requires an average of only 8.37 packets and 0.5 seconds to classify the application type of a flow with over 91% accuracy and over 96% accuracy for the content providers.","sentences":["Network traffic classification is of great importance for network operators in their daily routines, such as analyzing the usage patterns of multimedia applications and optimizing network configurations.","Internet service providers (ISPs) that operate high-speed links expect network flow classifiers to accurately classify flows early, using the minimal number of necessary initial packets per flow.","These classifiers must also be robust to packet sequence disorders in candidate flows and capable of detecting unseen flow types that are not within the existing classification scope, which are not well achieved by existing methods.","In this paper, we develop FastFlow, a time-series flow classification method that accurately classifies network flows as one of the known types or the unknown type, which dynamically selects the minimal number of packets to balance accuracy and efficiency.","Toward the objectives, we first develop a flow representation process that converts packet streams at both per-packet and per-slot granularity for precise packet statistics with robustness to packet sequence disorders.","Second, we develop a sequential decision-based classification model that leverages LSTM architecture trained with reinforcement learning.","Our model makes dynamic decisions on the minimal number of time-series data points per flow for the confident classification as one of the known flow types or an unknown one.","We evaluated our method on public datasets and demonstrated its superior performance in early and accurate flow classification.","Deployment insights on the classification of over 22.9 million flows across seven application types and 33 content providers in a campus network over one week are discussed, showing that FastFlow requires an average of only 8.37 packets and 0.5 seconds to classify the application type of a flow with over 91% accuracy and over 96% accuracy for the content providers."],"url":"http://arxiv.org/abs/2504.02174v1"}
{"created":"2025-04-02 23:08:04","title":"LogLSHD: Fast Log Parsing with Locality-Sensitive Hashing and Dynamic Time Warping","abstract":"Large-scale software systems generate vast volumes of system logs that are essential for monitoring, diagnosing, and performance optimization. However, the unstructured nature and ever-growing scale of these logs present significant challenges for manual analysis and automated downstream tasks such as anomaly detection. Log parsing addresses these challenges by converting raw logs into structured formats, enabling efficient log analysis. Despite its importance, existing log parsing methods suffer from limitations in efficiency and scalability, due to the large size of log data and their heterogeneous formats. To overcome these challenges, this study proposes a log parsing approach, LogLSHD, which leverages Locality-Sensitive Hashing (LSH) to group similar logs and integrates Dynamic Time Warping (DTW) to enhance the accuracy of template extraction. LogLSHD demonstrates exceptional efficiency in parsing time, significantly outperforming state-of-the-art methods. For example, compared to Drain, LogLSHD reduces the average parsing time by 73% while increasing the average parsing accuracy by 15% on the LogHub 2.0 benchmark.","sentences":["Large-scale software systems generate vast volumes of system logs that are essential for monitoring, diagnosing, and performance optimization.","However, the unstructured nature and ever-growing scale of these logs present significant challenges for manual analysis and automated downstream tasks such as anomaly detection.","Log parsing addresses these challenges by converting raw logs into structured formats, enabling efficient log analysis.","Despite its importance, existing log parsing methods suffer from limitations in efficiency and scalability, due to the large size of log data and their heterogeneous formats.","To overcome these challenges, this study proposes a log parsing approach, LogLSHD, which leverages Locality-Sensitive Hashing (LSH) to group similar logs and integrates Dynamic Time Warping (DTW) to enhance the accuracy of template extraction.","LogLSHD demonstrates exceptional efficiency in parsing time, significantly outperforming state-of-the-art methods.","For example, compared to Drain, LogLSHD reduces the average parsing time by 73% while increasing the average parsing accuracy by 15% on the LogHub 2.0 benchmark."],"url":"http://arxiv.org/abs/2504.02172v1"}
{"created":"2025-04-02 22:42:45","title":"Responsible Innovation: A Strategic Framework for Financial LLM Integration","abstract":"Financial institutions of all sizes are increasingly adopting Large Language Models (LLMs) to enhance credit assessments, deliver personalized client advisory services, and automate various language-intensive processes. However, effectively deploying LLMs requires careful management of stringent data governance requirements, heightened demands for interpretability, ethical responsibilities, and rapidly evolving regulatory landscapes. To address these challenges, we introduce a structured six-decision framework specifically designed for the financial sector, guiding organizations systematically from initial feasibility assessments to final deployment strategies.   The framework encourages institutions to: (1) evaluate whether an advanced LLM is necessary at all, (2) formalize robust data governance and privacy safeguards, (3) establish targeted risk management mechanisms, (4) integrate ethical considerations early in the development process, (5) justify the initiative's return on investment (ROI) and strategic value, and only then (6) choose the optimal implementation pathway -- open-source versus proprietary, or in-house versus vendor-supported -- aligned with regulatory requirements and operational realities. By linking strategic considerations with practical steps such as pilot testing, maintaining comprehensive audit trails, and conducting ongoing compliance evaluations, this decision framework offers a structured roadmap for responsibly leveraging LLMs. Rather than acting as a rigid, one-size-fits-all solution, it shows how advanced language models can be thoughtfully integrated into existing workflows -- balancing innovation with accountability to uphold stakeholder trust and regulatory integrity.","sentences":["Financial institutions of all sizes are increasingly adopting Large Language Models (LLMs) to enhance credit assessments, deliver personalized client advisory services, and automate various language-intensive processes.","However, effectively deploying LLMs requires careful management of stringent data governance requirements, heightened demands for interpretability, ethical responsibilities, and rapidly evolving regulatory landscapes.","To address these challenges, we introduce a structured six-decision framework specifically designed for the financial sector, guiding organizations systematically from initial feasibility assessments to final deployment strategies.   ","The framework encourages institutions to: (1) evaluate whether an advanced LLM is necessary at all, (2) formalize robust data governance and privacy safeguards, (3) establish targeted risk management mechanisms, (4) integrate ethical considerations early in the development process, (5) justify the initiative's return on investment (ROI) and strategic value, and only then (6) choose the optimal implementation pathway -- open-source versus proprietary, or in-house versus vendor-supported -- aligned with regulatory requirements and operational realities.","By linking strategic considerations with practical steps such as pilot testing, maintaining comprehensive audit trails, and conducting ongoing compliance evaluations, this decision framework offers a structured roadmap for responsibly leveraging LLMs.","Rather than acting as a rigid, one-size-fits-all solution, it shows how advanced language models can be thoughtfully integrated into existing workflows -- balancing innovation with accountability to uphold stakeholder trust and regulatory integrity."],"url":"http://arxiv.org/abs/2504.02165v1"}
{"created":"2025-04-02 22:30:45","title":"Neural Style Transfer for Synthesising a Dataset of Ancient Egyptian Hieroglyphs","abstract":"The limited availability of training data for low-resource languages makes applying machine learning techniques challenging. Ancient Egyptian is one such language with few resources. However, innovative applications of data augmentation methods, such as Neural Style Transfer, could overcome these barriers. This paper presents a novel method for generating datasets of ancient Egyptian hieroglyphs by applying NST to a digital typeface. Experimental results found that image classification models trained on NST-generated examples and photographs demonstrate equal performance and transferability to real unseen images of hieroglyphs.","sentences":["The limited availability of training data for low-resource languages makes applying machine learning techniques challenging.","Ancient Egyptian is one such language with few resources.","However, innovative applications of data augmentation methods, such as Neural Style Transfer, could overcome these barriers.","This paper presents a novel method for generating datasets of ancient Egyptian hieroglyphs by applying NST to a digital typeface.","Experimental results found that image classification models trained on NST-generated examples and photographs demonstrate equal performance and transferability to real unseen images of hieroglyphs."],"url":"http://arxiv.org/abs/2504.02163v1"}
{"created":"2025-04-02 22:20:21","title":"Less-to-More Generalization: Unlocking More Controllability by In-Context Generation","abstract":"Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.","sentences":["Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility.","For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult.","For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios.","In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge.","This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data.","Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding.","It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model.","Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation."],"url":"http://arxiv.org/abs/2504.02160v1"}
{"created":"2025-04-02 22:17:30","title":"UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting","abstract":"We present UAVTwin, a method for creating digital twins from real-world environments and facilitating data augmentation for training downstream models embedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses on synthesizing foreground components, such as various human instances in motion within complex scene backgrounds, from UAV perspectives. This is achieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing backgrounds along with controllable synthetic human models that display diverse appearances and actions in multiple poses. To the best of our knowledge, UAVTwin is the first approach for UAV-based perception that is capable of generating high-fidelity digital twins based on 3DGS. The proposed work significantly enhances downstream models through data augmentation for real-world environments with multiple dynamic objects and significant appearance variations-both of which typically introduce artifacts in 3DGS-based modeling. To tackle these challenges, we propose a novel appearance modeling strategy and a mask refinement module to enhance the training of 3D Gaussian Splatting. We demonstrate the high quality of neural rendering by achieving a 1.23 dB improvement in PSNR compared to recent methods. Furthermore, we validate the effectiveness of data augmentation by showing a 2.5% to 13.7% improvement in mAP for the human detection task.","sentences":["We present UAVTwin, a method for creating digital twins from real-world environments and facilitating data augmentation for training downstream models embedded in unmanned aerial vehicles (UAVs).","Specifically, our approach focuses on synthesizing foreground components, such as various human instances in motion within complex scene backgrounds, from UAV perspectives.","This is achieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing backgrounds along with controllable synthetic human models that display diverse appearances and actions in multiple poses.","To the best of our knowledge, UAVTwin is the first approach for UAV-based perception that is capable of generating high-fidelity digital twins based on 3DGS.","The proposed work significantly enhances downstream models through data augmentation for real-world environments with multiple dynamic objects and significant appearance variations-both of which typically introduce artifacts in 3DGS-based modeling.","To tackle these challenges, we propose a novel appearance modeling strategy and a mask refinement module to enhance the training of 3D Gaussian Splatting.","We demonstrate the high quality of neural rendering by achieving a 1.23 dB improvement in PSNR compared to recent methods.","Furthermore, we validate the effectiveness of data augmentation by showing a 2.5% to 13.7% improvement in mAP for the human detection task."],"url":"http://arxiv.org/abs/2504.02158v1"}
{"created":"2025-04-02 21:53:03","title":"Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP","abstract":"The rapid use of artificial intelligence (AI) in processes such as coding, image processing, and data prediction means it is crucial to understand and validate the data we are working with fully. This paper dives into the hurdles of analyzing high-dimensional data, especially when it gets too complex. Traditional methods in data analysis often look at direct connections between input variables, which can miss out on the more complicated relationships within the data.   To address these issues, we explore several tested techniques, such as removing specific variables to see their impact and using statistical analysis to find connections between multiple variables. We also consider the role of synthetic data and how information can sometimes be redundant across different sensors. These analyses are typically very computationally demanding and often require much human effort to make sense of the results.   A common approach is to treat the entire dataset as one unit and apply advanced models to handle it. However, this can become problematic with larger, noisier datasets and more complex models. So, we suggest methods to identify overall patterns that can help with tasks like classification or regression based on the idea that more straightforward approaches might be more understandable.   Our research looks at two datasets: a real-world dataset and a synthetic one. The goal is to create a methodology that highlights key features on a global scale that lead to predictions, making it easier to validate or quantify the data set. By reducing the dimensionality with this method, we can simplify the models used and thus clarify the insights we gain. Furthermore, our method can reveal unexplored relationships between specific inputs and outcomes, providing a way to validate these new connections further.","sentences":["The rapid use of artificial intelligence (AI) in processes such as coding, image processing, and data prediction means it is crucial to understand and validate the data we are working with fully.","This paper dives into the hurdles of analyzing high-dimensional data, especially when it gets too complex.","Traditional methods in data analysis often look at direct connections between input variables, which can miss out on the more complicated relationships within the data.   ","To address these issues, we explore several tested techniques, such as removing specific variables to see their impact and using statistical analysis to find connections between multiple variables.","We also consider the role of synthetic data and how information can sometimes be redundant across different sensors.","These analyses are typically very computationally demanding and often require much human effort to make sense of the results.   ","A common approach is to treat the entire dataset as one unit and apply advanced models to handle it.","However, this can become problematic with larger, noisier datasets and more complex models.","So, we suggest methods to identify overall patterns that can help with tasks like classification or regression based on the idea that more straightforward approaches might be more understandable.   ","Our research looks at two datasets: a real-world dataset and a synthetic one.","The goal is to create a methodology that highlights key features on a global scale that lead to predictions, making it easier to validate or quantify the data set.","By reducing the dimensionality with this method, we can simplify the models used and thus clarify the insights we gain.","Furthermore, our method can reveal unexplored relationships between specific inputs and outcomes, providing a way to validate these new connections further."],"url":"http://arxiv.org/abs/2504.02151v1"}
{"created":"2025-04-02 21:49:43","title":"LakeVisage: Towards Scalable, Flexible and Interactive Visualization Recommendation for Data Discovery over Data Lakes","abstract":"Data discovery from data lakes is an essential application in modern data science. While many previous studies focused on improving the efficiency and effectiveness of data discovery, little attention has been paid to the usability of such applications. In particular, exploring data discovery results can be cumbersome due to the cognitive load involved in understanding raw tabular results and identifying insights to draw conclusions. To address this challenge, we introduce a new problem -- visualization recommendation for data discovery over data lakes -- which aims at automatically identifying visualizations that highlight relevant or desired trends in the results returned by data discovery engines. We propose LakeVisage, an end-to-end framework as the first solution to this problem. Given a data lake, a data discovery engine, and a user-specified query table, LakeVisage intelligently explores the space of visualizations and recommends the most useful and ``interesting'' visualization plans. To this end, we developed (i) approaches to smartly construct the candidate visualization plans from the results of the data discovery engine and (ii) effective pruning strategies to filter out less interesting plans so as to accelerate the visual analysis. Experimental results on real data lakes show that our proposed techniques can lead to an order of magnitude speedup in visualization recommendation. We also conduct a comprehensive user study to demonstrate that LakeVisage offers convenience to users in real data analysis applications by enabling them seamlessly get started with the tasks and performing explorations flexibly.","sentences":["Data discovery from data lakes is an essential application in modern data science.","While many previous studies focused on improving the efficiency and effectiveness of data discovery, little attention has been paid to the usability of such applications.","In particular, exploring data discovery results can be cumbersome due to the cognitive load involved in understanding raw tabular results and identifying insights to draw conclusions.","To address this challenge, we introduce a new problem -- visualization recommendation for data discovery over data lakes -- which aims at automatically identifying visualizations that highlight relevant or desired trends in the results returned by data discovery engines.","We propose LakeVisage, an end-to-end framework as the first solution to this problem.","Given a data lake, a data discovery engine, and a user-specified query table, LakeVisage intelligently explores the space of visualizations and recommends the most useful and ``interesting'' visualization plans.","To this end, we developed (i) approaches to smartly construct the candidate visualization plans from the results of the data discovery engine and (ii) effective pruning strategies to filter out less interesting plans so as to accelerate the visual analysis.","Experimental results on real data lakes show that our proposed techniques can lead to an order of magnitude speedup in visualization recommendation.","We also conduct a comprehensive user study to demonstrate that LakeVisage offers convenience to users in real data analysis applications by enabling them seamlessly get started with the tasks and performing explorations flexibly."],"url":"http://arxiv.org/abs/2504.02150v1"}
{"created":"2025-04-02 21:47:58","title":"OmniCellTOSG: The First Cell Text-Omic Signaling Graphs Dataset for Joint LLM and GNN Modeling","abstract":"Complex cell signaling systems -- governed by varying protein abundances and interactions -- generate diverse cell types across organs. These systems evolve under influences such as age, sex, diet, environmental exposures, and diseases, making them challenging to decode given the involvement of tens of thousands of genes and proteins. Recently, hundreds of millions of single-cell omics data have provided a robust foundation for understanding these signaling networks within various cell subpopulations and conditions. Inspired by the success of large foundation models (for example, large language models and large vision models) pre-trained on massive datasets, we introduce OmniCellTOSG, the first dataset of cell text-omic signaling graphs (TOSGs). Each TOSG represents the signaling network of an individual or meta-cell and is labeled with information such as organ, disease, sex, age, and cell subtype. OmniCellTOSG offers two key contributions. First, it introduces a novel graph model that integrates human-readable annotations -- such as biological functions, cellular locations, signaling pathways, related diseases, and drugs -- with quantitative gene and protein abundance data, enabling graph reasoning to decode cell signaling. This approach calls for new joint models combining large language models and graph neural networks. Second, the dataset is built from single-cell RNA sequencing data of approximately 120 million cells from diverse tissues and conditions (healthy and diseased) and is fully compatible with PyTorch. This facilitates the development of innovative cell signaling models that could transform research in life sciences, healthcare, and precision medicine. The OmniCellTOSG dataset is continuously expanding and will be updated regularly. The dataset and code are available at https://github.com/FuhaiLiAiLab/OmniCellTOSG.","sentences":["Complex cell signaling systems -- governed by varying protein abundances and interactions -- generate diverse cell types across organs.","These systems evolve under influences such as age, sex, diet, environmental exposures, and diseases, making them challenging to decode given the involvement of tens of thousands of genes and proteins.","Recently, hundreds of millions of single-cell omics data have provided a robust foundation for understanding these signaling networks within various cell subpopulations and conditions.","Inspired by the success of large foundation models (for example, large language models and large vision models) pre-trained on massive datasets, we introduce OmniCellTOSG, the first dataset of cell text-omic signaling graphs (TOSGs).","Each TOSG represents the signaling network of an individual or meta-cell and is labeled with information such as organ, disease, sex, age, and cell subtype.","OmniCellTOSG offers two key contributions.","First, it introduces a novel graph model that integrates human-readable annotations -- such as biological functions, cellular locations, signaling pathways, related diseases, and drugs -- with quantitative gene and protein abundance data, enabling graph reasoning to decode cell signaling.","This approach calls for new joint models combining large language models and graph neural networks.","Second, the dataset is built from single-cell RNA sequencing data of approximately 120 million cells from diverse tissues and conditions (healthy and diseased) and is fully compatible with PyTorch.","This facilitates the development of innovative cell signaling models that could transform research in life sciences, healthcare, and precision medicine.","The OmniCellTOSG dataset is continuously expanding and will be updated regularly.","The dataset and code are available at https://github.com/FuhaiLiAiLab/OmniCellTOSG."],"url":"http://arxiv.org/abs/2504.02148v1"}
{"created":"2025-04-02 21:46:30","title":"LL4G: Self-Supervised Dynamic Optimization for Graph-Based Personality Detection","abstract":"Graph-based personality detection constructs graph structures from textual data, particularly social media posts. Current methods often struggle with sparse or noisy data and rely on static graphs, limiting their ability to capture dynamic changes between nodes and relationships. This paper introduces LL4G, a self-supervised framework leveraging large language models (LLMs) to optimize graph neural networks (GNNs). LLMs extract rich semantic features to generate node representations and to infer explicit and implicit relationships. The graph structure adaptively adds nodes and edges based on input data, continuously optimizing itself. The GNN then uses these optimized representations for joint training on node reconstruction, edge prediction, and contrastive learning tasks. This integration of semantic and structural information generates robust personality profiles. Experimental results on Kaggle and Pandora datasets show LL4G outperforms state-of-the-art models.","sentences":["Graph-based personality detection constructs graph structures from textual data, particularly social media posts.","Current methods often struggle with sparse or noisy data and rely on static graphs, limiting their ability to capture dynamic changes between nodes and relationships.","This paper introduces LL4G, a self-supervised framework leveraging large language models (LLMs) to optimize graph neural networks (GNNs).","LLMs extract rich semantic features to generate node representations and to infer explicit and implicit relationships.","The graph structure adaptively adds nodes and edges based on input data, continuously optimizing itself.","The GNN then uses these optimized representations for joint training on node reconstruction, edge prediction, and contrastive learning tasks.","This integration of semantic and structural information generates robust personality profiles.","Experimental results on Kaggle and Pandora datasets show LL4G outperforms state-of-the-art models."],"url":"http://arxiv.org/abs/2504.02146v1"}
