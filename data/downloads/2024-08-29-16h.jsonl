{"created":"2024-08-27 17:59:55","title":"Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty","abstract":"Robust and realistic rendering for large-scale road scenes is essential in autonomous driving simulation. Recently, 3D Gaussian Splatting (3D-GS) has made groundbreaking progress in neural rendering, but the general fidelity of large-scale road scene renderings is often limited by the input imagery, which usually has a narrow field of view and focuses mainly on the street-level local area. Intuitively, the data from the drone's perspective can provide a complementary viewpoint for the data from the ground vehicle's perspective, enhancing the completeness of scene reconstruction and rendering. However, training naively with aerial and ground images, which exhibit large view disparity, poses a significant convergence challenge for 3D-GS, and does not demonstrate remarkable improvements in performance on road views. In order to enhance the novel view synthesis of road views and to effectively use the aerial information, we design an uncertainty-aware training method that allows aerial images to assist in the synthesis of areas where ground images have poor learning outcomes instead of weighting all pixels equally in 3D-GS training like prior work did. We are the first to introduce the cross-view uncertainty to 3D-GS by matching the car-view ensemble-based rendering uncertainty to aerial images, weighting the contribution of each pixel to the training process. Additionally, to systematically quantify evaluation metrics, we assemble a high-quality synthesized dataset comprising both aerial and ground images for road scenes.","sentences":["Robust and realistic rendering for large-scale road scenes is essential in autonomous driving simulation.","Recently, 3D Gaussian Splatting (3D-GS) has made groundbreaking progress in neural rendering, but the general fidelity of large-scale road scene renderings is often limited by the input imagery, which usually has a narrow field of view and focuses mainly on the street-level local area.","Intuitively, the data from the drone's perspective can provide a complementary viewpoint for the data from the ground vehicle's perspective, enhancing the completeness of scene reconstruction and rendering.","However, training naively with aerial and ground images, which exhibit large view disparity, poses a significant convergence challenge for 3D-GS, and does not demonstrate remarkable improvements in performance on road views.","In order to enhance the novel view synthesis of road views and to effectively use the aerial information, we design an uncertainty-aware training method that allows aerial images to assist in the synthesis of areas where ground images have poor learning outcomes instead of weighting all pixels equally in 3D-GS training like prior work did.","We are the first to introduce the cross-view uncertainty to 3D-GS by matching the car-view ensemble-based rendering uncertainty to aerial images, weighting the contribution of each pixel to the training process.","Additionally, to systematically quantify evaluation metrics, we assemble a high-quality synthesized dataset comprising both aerial and ground images for road scenes."],"url":"http://arxiv.org/abs/2408.15242v1"}
{"created":"2024-08-27 17:48:29","title":"DCT-CryptoNets: Scaling Private Inference in the Frequency Domain","abstract":"The convergence of fully homomorphic encryption (FHE) and machine learning offers unprecedented opportunities for private inference of sensitive data. FHE enables computation directly on encrypted data, safeguarding the entire machine learning pipeline, including data and model confidentiality. However, existing FHE-based implementations for deep neural networks face significant challenges in computational cost, latency, and scalability, limiting their practical deployment. This paper introduces DCT-CryptoNets, a novel approach that leverages frequency-domain learning to tackle these issues. Our method operates directly in the frequency domain, utilizing the discrete cosine transform (DCT) commonly employed in JPEG compression. This approach is inherently compatible with remote computing services, where images are usually transmitted and stored in compressed formats. DCT-CryptoNets reduces the computational burden of homomorphic operations by focusing on perceptually relevant low-frequency components. This is demonstrated by substantial latency reduction of up to 5.3$\\times$ compared to prior work on image classification tasks, including a novel demonstration of ImageNet inference within 2.5 hours, down from 12.5 hours compared to prior work on equivalent compute resources. Moreover, DCT-CryptoNets improves the reliability of encrypted accuracy by reducing variability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study demonstrates a promising avenue for achieving efficient and practical privacy-preserving deep learning on high resolution images seen in real-world applications.","sentences":["The convergence of fully homomorphic encryption (FHE) and machine learning offers unprecedented opportunities for private inference of sensitive data.","FHE enables computation directly on encrypted data, safeguarding the entire machine learning pipeline, including data and model confidentiality.","However, existing FHE-based implementations for deep neural networks face significant challenges in computational cost, latency, and scalability, limiting their practical deployment.","This paper introduces DCT-CryptoNets, a novel approach that leverages frequency-domain learning to tackle these issues.","Our method operates directly in the frequency domain, utilizing the discrete cosine transform (DCT) commonly employed in JPEG compression.","This approach is inherently compatible with remote computing services, where images are usually transmitted and stored in compressed formats.","DCT-CryptoNets reduces the computational burden of homomorphic operations by focusing on perceptually relevant low-frequency components.","This is demonstrated by substantial latency reduction of up to 5.3$\\times$ compared to prior work on image classification tasks, including a novel demonstration of ImageNet inference within 2.5 hours, down from 12.5 hours compared to prior work on equivalent compute resources.","Moreover, DCT-CryptoNets improves the reliability of encrypted accuracy by reducing variability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet).","This study demonstrates a promising avenue for achieving efficient and practical privacy-preserving deep learning on high resolution images seen in real-world applications."],"url":"http://arxiv.org/abs/2408.15231v1"}
{"created":"2024-08-27 17:19:57","title":"Classifying populist language in American presidential and governor speeches using automatic text analysis","abstract":"Populism is a concept that is often used but notoriously difficult to measure. Common qualitative measurements like holistic grading or content analysis require great amounts of time and labour, making it difficult to quickly scope out which politicians should be classified as populist and which should not, while quantitative methods show mixed results when it comes to classifying populist rhetoric. In this paper, we develop a pipeline to train and validate an automated classification model to estimate the use of populist language. We train models based on sentences that were identified as populist and pluralist in 300 US governors' speeches from 2010 to 2018 and in 45 speeches of presidential candidates in 2016. We find that these models classify most speeches correctly, including 84% of governor speeches and 89% of presidential speeches. These results extend to different time periods (with 92% accuracy on more recent American governors), different amounts of data (with as few as 70 training sentences per category achieving similar results), and when classifying politicians instead of individual speeches. This pipeline is thus an effective tool that can optimise the systematic and swift classification of the use of populist language in politicians' speeches.","sentences":["Populism is a concept that is often used but notoriously difficult to measure.","Common qualitative measurements like holistic grading or content analysis require great amounts of time and labour, making it difficult to quickly scope out which politicians should be classified as populist and which should not, while quantitative methods show mixed results when it comes to classifying populist rhetoric.","In this paper, we develop a pipeline to train and validate an automated classification model to estimate the use of populist language.","We train models based on sentences that were identified as populist and pluralist in 300 US governors' speeches from 2010 to 2018 and in 45 speeches of presidential candidates in 2016.","We find that these models classify most speeches correctly, including 84% of governor speeches and 89% of presidential speeches.","These results extend to different time periods (with 92% accuracy on more recent American governors), different amounts of data (with as few as 70 training sentences per category achieving similar results), and when classifying politicians instead of individual speeches.","This pipeline is thus an effective tool that can optimise the systematic and swift classification of the use of populist language in politicians' speeches."],"url":"http://arxiv.org/abs/2408.15213v1"}
{"created":"2024-08-27 17:03:18","title":"Can Unconfident LLM Annotations Be Used for Confident Conclusions?","abstract":"Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-Driven Inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of Confidence-Driven Inference over baselines in statistical estimation tasks across three CSS settings--text politeness, stance, and bias--reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, Confidence-Driven Inference can be used to estimate most standard quantities across a broad range of NLP problems.","sentences":["Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection.","In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations.","Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited.","We introduce Confidence-Driven Inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed.","Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations.","We demonstrate the effectiveness of Confidence-Driven Inference over baselines in statistical estimation tasks across three CSS settings--text politeness, stance, and bias--reducing the needed number of human annotations by over 25% in each.","Although we use CSS settings for demonstration, Confidence-Driven Inference can be used to estimate most standard quantities across a broad range of NLP problems."],"url":"http://arxiv.org/abs/2408.15204v1"}
{"created":"2024-08-27 17:03:10","title":"On the Encoding Process in Decentralized Systems","abstract":"We consider the problem of encoding information in a system of N=K+R processors that operate in a decentralized manner, i.e., without a central processor which orchestrates the operation. The system involves K source processors, each holding some data modeled as a vector over a finite field. The remaining R processors are sinks, and each of which requires a linear combination of all data vectors. These linear combinations are distinct from one sink processor to another, and are specified by a generator matrix of a systematic linear error correcting code. To capture the communication cost of decentralized encoding, we adopt a linear network model in which the process proceeds in consecutive communication rounds. In every round, every processor sends and receives one message through each one of its p ports. Moreover, inspired by linear network coding literature, we allow processors to transfer linear combinations of their own data and previously received data. We propose a framework that addresses the decentralized encoding problem on two levels. On the universal level, we provide a solution to the decentralized encoding problem for any possible linear code. On the specific level, we further optimize our solution towards systematic Reed-Solomon codes, as well as their variant, Lagrange codes, for their prevalent use in coded storage and computation systems. Our solutions are based on a newly-defined collective communication operation we call all-to-all encode.","sentences":["We consider the problem of encoding information in a system of N=K+R processors that operate in a decentralized manner, i.e., without a central processor which orchestrates the operation.","The system involves K source processors, each holding some data modeled as a vector over a finite field.","The remaining R processors are sinks, and each of which requires a linear combination of all data vectors.","These linear combinations are distinct from one sink processor to another, and are specified by a generator matrix of a systematic linear error correcting code.","To capture the communication cost of decentralized encoding, we adopt a linear network model in which the process proceeds in consecutive communication rounds.","In every round, every processor sends and receives one message through each one of its p ports.","Moreover, inspired by linear network coding literature, we allow processors to transfer linear combinations of their own data and previously received data.","We propose a framework that addresses the decentralized encoding problem on two levels.","On the universal level, we provide a solution to the decentralized encoding problem for any possible linear code.","On the specific level, we further optimize our solution towards systematic Reed-Solomon codes, as well as their variant, Lagrange codes, for their prevalent use in coded storage and computation systems.","Our solutions are based on a newly-defined collective communication operation we call all-to-all encode."],"url":"http://arxiv.org/abs/2408.15203v1"}
{"created":"2024-08-27 16:41:13","title":"Easy-access online social media metrics can effectively identify misinformation sharing users","abstract":"Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is costly and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms.","sentences":["Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is costly and challenging.","To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely.","Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter).","We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it.","We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has.","Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms."],"url":"http://arxiv.org/abs/2408.15186v1"}
{"created":"2024-08-27 16:40:14","title":"PoseWatch: A Transformer-based Architecture for Human-centric Video Anomaly Detection Using Spatio-temporal Pose Tokenization","abstract":"Video Anomaly Detection (VAD) presents a significant challenge in computer vision, particularly due to the unpredictable and infrequent nature of anomalous events, coupled with the diverse and dynamic environments in which they occur. Human-centric VAD, a specialized area within this domain, faces additional complexities, including variations in human behavior, potential biases in data, and substantial privacy concerns related to human subjects. These issues complicate the development of models that are both robust and generalizable. To address these challenges, recent advancements have focused on pose-based VAD, which leverages human pose as a high-level feature to mitigate privacy concerns, reduce appearance biases, and minimize background interference. In this paper, we introduce PoseWatch, a novel transformer-based architecture designed specifically for human-centric pose-based VAD. PoseWatch features an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP) tokenization method that enhances the representation of human motion over time, which is also beneficial for broader human behavior analysis tasks. The architecture's core, a Unified Encoder Twin Decoders (UETD) transformer, significantly improves the detection of anomalous behaviors in video data. Extensive evaluations across multiple benchmark datasets demonstrate that PoseWatch consistently outperforms existing methods, establishing a new state-of-the-art in pose-based VAD. This work not only demonstrates the efficacy of PoseWatch but also highlights the potential of integrating Natural Language Processing techniques with computer vision to advance human behavior analysis.","sentences":["Video Anomaly Detection (VAD) presents a significant challenge in computer vision, particularly due to the unpredictable and infrequent nature of anomalous events, coupled with the diverse and dynamic environments in which they occur.","Human-centric VAD, a specialized area within this domain, faces additional complexities, including variations in human behavior, potential biases in data, and substantial privacy concerns related to human subjects.","These issues complicate the development of models that are both robust and generalizable.","To address these challenges, recent advancements have focused on pose-based VAD, which leverages human pose as a high-level feature to mitigate privacy concerns, reduce appearance biases, and minimize background interference.","In this paper, we introduce PoseWatch, a novel transformer-based architecture designed specifically for human-centric pose-based VAD.","PoseWatch features an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP) tokenization method that enhances the representation of human motion over time, which is also beneficial for broader human behavior analysis tasks.","The architecture's core, a Unified Encoder Twin Decoders (UETD) transformer, significantly improves the detection of anomalous behaviors in video data.","Extensive evaluations across multiple benchmark datasets demonstrate that PoseWatch consistently outperforms existing methods, establishing a new state-of-the-art in pose-based VAD.","This work not only demonstrates the efficacy of PoseWatch but also highlights the potential of integrating Natural Language Processing techniques with computer vision to advance human behavior analysis."],"url":"http://arxiv.org/abs/2408.15185v1"}
{"created":"2024-08-27 16:33:37","title":"On the parameterized complexity of computing good edge-labelings","abstract":"A good edge-labeling (gel for short) of a graph $G$ is a function $\\lambda: E(G) \\to \\mathbb{R}$ such that, for any ordered pair of vertices $(x, y)$ of $G$, there do not exist two distinct increasing paths from $x$ to $y$, where ``increasing'' means that the sequence of labels is non-decreasing. This notion was introduced by Bermond et al. [Theor. Comput. Sci. 2013] motivated by practical applications arising from routing and wavelength assignment problems in optical networks. Prompted by the lack of algorithmic results about the problem of deciding whether an input graph admits a gel, called GEL, we initiate its study from the viewpoint of parameterized complexity. We first introduce the natural version of GEL where one wants to use at most $c$ distinct labels, which we call $c$-GEL, and we prove that it is NP-complete for every $c \\geq 2$ on very restricted instances. We then provide several positive results, starting with simple polynomial kernels for GEL and $c$-\\GEL parameterized by neighborhood diversity or vertex cover. As one of our main technical contributions, we present an FPT algorithm for GEL parameterized by the size of a modulator to a forest of stars, based on a novel approach via a 2-SAT formulation which we believe to be of independent interest. We also present FPT algorithms based on dynamic programming for $c$-GEL parameterized by treewidth and $c$, and for GEL parameterized by treewidth and the maximum degree. Finally, we answer positively a question of Bermond et al. [Theor. Comput. Sci. 2013] by proving the NP-completeness of a problem strongly related to GEL, namely that of deciding whether an input graph admits a so-called UPP-orientation.","sentences":["A good edge-labeling (gel for short) of a graph $G$ is a function $\\lambda: E(G) \\to \\mathbb{R}$ such that, for any ordered pair of vertices $(x, y)$ of $G$, there do not exist two distinct increasing paths from $x$ to $y$, where ``increasing'' means that the sequence of labels is non-decreasing.","This notion was introduced by Bermond et al.","[Theor.","Comput.","Sci. 2013] motivated by practical applications arising from routing and wavelength assignment problems in optical networks.","Prompted by the lack of algorithmic results about the problem of deciding whether an input graph admits a gel, called GEL, we initiate its study from the viewpoint of parameterized complexity.","We first introduce the natural version of GEL where one wants to use at most $c$ distinct labels, which we call $c$-GEL, and we prove that it is NP-complete for every $c \\geq 2$ on very restricted instances.","We then provide several positive results, starting with simple polynomial kernels for GEL and $c$-\\GEL parameterized by neighborhood diversity or vertex cover.","As one of our main technical contributions, we present an FPT algorithm for GEL parameterized by the size of a modulator to a forest of stars, based on a novel approach via a 2-SAT formulation which we believe to be of independent interest.","We also present FPT algorithms based on dynamic programming for $c$-GEL parameterized by treewidth and $c$, and for GEL parameterized by treewidth and the maximum degree.","Finally, we answer positively a question of Bermond et al.","[Theor.","Comput.","Sci. 2013]","by proving the NP-completeness of a problem strongly related to GEL, namely that of deciding whether an input graph admits a so-called UPP-orientation."],"url":"http://arxiv.org/abs/2408.15181v1"}
