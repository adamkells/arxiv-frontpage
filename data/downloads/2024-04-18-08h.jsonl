{"created":"2024-04-17 17:55:27","title":"Private federated discovery of out-of-vocabulary words for Gboard","abstract":"The vocabulary of language models in Gboard, Google's keyboard application, plays a crucial role for improving user experience. One way to improve the vocabulary is to discover frequently typed out-of-vocabulary (OOV) words on user devices. This task requires strong privacy protection due to the sensitive nature of user input data. In this report, we present a private OOV discovery algorithm for Gboard, which builds on recent advances in private federated analytics. The system offers local differential privacy (LDP) guarantees for user contributed words. With anonymous aggregation, the final released words satisfy central differential privacy guarantees with $\\epsilon = 0.315, \\delta = 10^{-10}$ for OOV discovery in en-US (English in United States).","sentences":["The vocabulary of language models in Gboard, Google's keyboard application, plays a crucial role for improving user experience.","One way to improve the vocabulary is to discover frequently typed out-of-vocabulary (OOV) words on user devices.","This task requires strong privacy protection due to the sensitive nature of user input data.","In this report, we present a private OOV discovery algorithm for Gboard, which builds on recent advances in private federated analytics.","The system offers local differential privacy (LDP) guarantees for user contributed words.","With anonymous aggregation, the final released words satisfy central differential privacy guarantees with $\\epsilon = 0.315, \\delta = 10^{-10}$ for OOV discovery in en-US (English in United States)."],"url":"http://arxiv.org/abs/2404.11607v1"}
{"created":"2024-04-17 17:55:17","title":"Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models","abstract":"We propose a self-supervised learning approach for solving the following constrained optimization task in log-linear models or Markov networks. Let $f$ and $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and $\\mathbf{Y}$ of random variables respectively. Given an assignment $\\mathbf{x}$ to all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the constrained most-probable explanation (CMPE) task seeks to find an assignment $\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x}, \\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our proposed self-supervised approach, given assignments $\\mathbf{x}$ to $\\mathbf{X}$ (data), we train a deep neural network that learns to output near-optimal solutions to the CMPE problem without requiring access to any pre-computed solutions. The key idea in our approach is to use first principles and approximate inference methods for CMPE to derive novel loss functions that seek to push infeasible solutions towards feasible ones and feasible solutions towards optimal ones. We analyze the properties of our proposed method and experimentally demonstrate its efficacy on several benchmark problems.","sentences":["We propose a self-supervised learning approach for solving the following constrained optimization task in log-linear models or Markov networks.","Let $f$ and $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and $\\mathbf{Y}$ of random variables respectively.","Given an assignment $\\mathbf{x}$ to all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the constrained most-probable explanation (CMPE) task seeks to find an assignment $\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x}, \\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our proposed self-supervised approach, given assignments $\\mathbf{x}$ to $\\mathbf{X}$ (data), we train a deep neural network that learns to output near-optimal solutions to the CMPE problem without requiring access to any pre-computed solutions.","The key idea in our approach is to use first principles and approximate inference methods for CMPE to derive novel loss functions that seek to push infeasible solutions towards feasible ones and feasible solutions towards optimal ones.","We analyze the properties of our proposed method and experimentally demonstrate its efficacy on several benchmark problems."],"url":"http://arxiv.org/abs/2404.11606v1"}
{"created":"2024-04-17 17:51:15","title":"Interaction Techniques for Exploratory Data Visualization on Mobile Devices","abstract":"The ubiquity and on-the-go availability of mobile devices makes them central to many tasks such as interpersonal communication and media consumption. However, despite the potential of mobile devices for on-demand exploratory data visualization, existing mobile interactions are difficult, often using highly custom interactions, complex gestures, or multi-modal input. We synthesize limitations from the literature and outline four motivating principles for improved mobile interaction: leverage ubiquitous modalities, prioritize discoverability, enable rapid in-context data exploration, and promote graceful recovery. We then contribute thirteen interaction candidates and conduct a formative study with twelve participants who experienced our interactions in a testbed prototype. Based on these interviews, we discuss design considerations and tradeoffs from four main themes: precise and rapid inspection, focused navigation, single-touch and fixed orientation interaction, and judicious use of motion.","sentences":["The ubiquity and on-the-go availability of mobile devices makes them central to many tasks such as interpersonal communication and media consumption.","However, despite the potential of mobile devices for on-demand exploratory data visualization, existing mobile interactions are difficult, often using highly custom interactions, complex gestures, or multi-modal input.","We synthesize limitations from the literature and outline four motivating principles for improved mobile interaction: leverage ubiquitous modalities, prioritize discoverability, enable rapid in-context data exploration, and promote graceful recovery.","We then contribute thirteen interaction candidates and conduct a formative study with twelve participants who experienced our interactions in a testbed prototype.","Based on these interviews, we discuss design considerations and tradeoffs from four main themes: precise and rapid inspection, focused navigation, single-touch and fixed orientation interaction, and judicious use of motion."],"url":"http://arxiv.org/abs/2404.11602v1"}
{"created":"2024-04-17 17:45:08","title":"IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination","abstract":"This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code will be available at https://zju3dv.github.io/IntrinsicAnything.","sentences":["This paper aims to recover object materials from posed images captured under an unknown static lighting condition.","Recent methods solve this task by optimizing material parameters through differentiable physically based rendering.","However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results.","To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process.","We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular.","Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images.","In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results.","Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery.","The code will be available at https://zju3dv.github.io/IntrinsicAnything."],"url":"http://arxiv.org/abs/2404.11593v1"}
{"created":"2024-04-17 17:42:48","title":"The EDGE Language: Extended General Einsums for Graph Algorithms","abstract":"In this work, we propose a unified abstraction for graph algorithms: the Extended General Einsums language, or EDGE. The EDGE language expresses graph algorithms in the language of tensor algebra, providing a rigorous, succinct, and expressive mathematical framework. EDGE leverages two ideas: (1) the well-known foundations provided by the graph-matrix duality, where a graph is simply a 2D tensor, and (2) the power and expressivity of Einsum notation in the tensor algebra world. In this work, we describe our design goals for EDGE and walk through the extensions we add to Einsums to support more complex operations common in graph algorithms. Additionally, we provide a few examples of how to express graph algorithms in our proposed notation. We hope that a single, mathematical notation for graph algorithms will (1) allow researchers to more easily compare different algorithms and different implementations of a graph algorithm; (2) enable developers to factor complexity by separating the concerns of what to compute (described with the extended Einsum notation) from the lower level details of how to compute; and (3) enable the discovery of different algorithmic variants of a problem through algebraic manipulations and transformations on a given EDGE expression.","sentences":["In this work, we propose a unified abstraction for graph algorithms: the Extended General Einsums language, or EDGE.","The EDGE language expresses graph algorithms in the language of tensor algebra, providing a rigorous, succinct, and expressive mathematical framework.","EDGE leverages two ideas: (1) the well-known foundations provided by the graph-matrix duality, where a graph is simply a 2D tensor, and (2) the power and expressivity of Einsum notation in the tensor algebra world.","In this work, we describe our design goals for EDGE and walk through the extensions we add to Einsums to support more complex operations common in graph algorithms.","Additionally, we provide a few examples of how to express graph algorithms in our proposed notation.","We hope that a single, mathematical notation for graph algorithms will (1) allow researchers to more easily compare different algorithms and different implementations of a graph algorithm; (2) enable developers to factor complexity by separating the concerns of what to compute (described with the extended Einsum notation) from the lower level details of how to compute; and (3) enable the discovery of different algorithmic variants of a problem through algebraic manipulations and transformations on a given EDGE expression."],"url":"http://arxiv.org/abs/2404.11591v1"}
{"created":"2024-04-17 17:28:05","title":"LLMTune: Accelerate Database Knob Tuning with Large Language Models","abstract":"Database knob tuning is a critical challenge in the database community, aiming to optimize knob values to enhance database performance for specific workloads. DBMS often feature hundreds of tunable knobs, posing a significant challenge for DBAs to recommend optimal configurations. Consequently, many machine learning-based tuning methods have been developed to automate this process. Despite the introduction of various optimizers, practical applications have unveiled a new problem: they typically require numerous workload runs to achieve satisfactory performance, a process that is both time-consuming and resource-intensive. This inefficiency largely stems from the optimal configuration often being substantially different from the default setting, necessitating multiple iterations during tuning. Recognizing this, we argue that an effective starting point could significantly reduce redundant exploration in less efficient areas, thereby potentially speeding up the tuning process for the optimizers. Based on this assumption, we introduce LLMTune, a large language model-based configuration generator designed to produce an initial, high-quality configuration for new workloads. These generated configurations can then serve as starting points for various base optimizers, accelerating their tuning processes. To obtain training data for LLMTune's supervised fine-tuning, we have devised a new automatic data generation framework capable of efficiently creating a large number of <workload, configuration> pairs. We have conducted thorough experiments to evaluate LLMTune's effectiveness with different workloads, such as TPC-H and JOB. In comparison to leading methods, LLMTune demonstrates a quicker ability to identify superior configurations. For instance, with the challenging TPC-H workload, our LLMTune achieves a significant 15.6x speed-up ratio in finding the best-performing configurations.","sentences":["Database knob tuning is a critical challenge in the database community, aiming to optimize knob values to enhance database performance for specific workloads.","DBMS often feature hundreds of tunable knobs, posing a significant challenge for DBAs to recommend optimal configurations.","Consequently, many machine learning-based tuning methods have been developed to automate this process.","Despite the introduction of various optimizers, practical applications have unveiled a new problem: they typically require numerous workload runs to achieve satisfactory performance, a process that is both time-consuming and resource-intensive.","This inefficiency largely stems from the optimal configuration often being substantially different from the default setting, necessitating multiple iterations during tuning.","Recognizing this, we argue that an effective starting point could significantly reduce redundant exploration in less efficient areas, thereby potentially speeding up the tuning process for the optimizers.","Based on this assumption, we introduce LLMTune, a large language model-based configuration generator designed to produce an initial, high-quality configuration for new workloads.","These generated configurations can then serve as starting points for various base optimizers, accelerating their tuning processes.","To obtain training data for LLMTune's supervised fine-tuning, we have devised a new automatic data generation framework capable of efficiently creating a large number of <workload, configuration> pairs.","We have conducted thorough experiments to evaluate LLMTune's effectiveness with different workloads, such as TPC-H and JOB.","In comparison to leading methods, LLMTune demonstrates a quicker ability to identify superior configurations.","For instance, with the challenging TPC-H workload, our LLMTune achieves a significant 15.6x speed-up ratio in finding the best-performing configurations."],"url":"http://arxiv.org/abs/2404.11581v1"}
{"created":"2024-04-17 17:20:27","title":"Towards Reliable Empirical Machine Unlearning Evaluation: A Game-Theoretic View","abstract":"Machine unlearning is the process of updating machine learning models to remove the information of specific training data samples, in order to comply with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics that lack reliability. Specifically, we propose a game-theoretic framework that formalizes the evaluation process as a game between unlearning algorithms and MIA adversaries, measuring the data removal efficacy of unlearning algorithms by the capability of the MIA adversaries. Through careful design of the game, we demonstrate that the natural evaluation metric induced from the game enjoys provable guarantees that the existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient algorithm to estimate the evaluation metric induced from the game, and demonstrate its effectiveness through both theoretical analysis and empirical experiments. This work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.","sentences":["Machine unlearning is the process of updating machine learning models to remove the information of specific training data samples, in order to comply with data protection regulations that allow individuals to request the removal of their personal data.","Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question.","In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics that lack reliability.","Specifically, we propose a game-theoretic framework that formalizes the evaluation process as a game between unlearning algorithms and MIA adversaries, measuring the data removal efficacy of unlearning algorithms by the capability of the MIA adversaries.","Through careful design of the game, we demonstrate that the natural evaluation metric induced from the game enjoys provable guarantees that the existing evaluation metrics fail to satisfy.","Furthermore, we propose a practical and efficient algorithm to estimate the evaluation metric induced from the game, and demonstrate its effectiveness through both theoretical analysis and empirical experiments.","This work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques."],"url":"http://arxiv.org/abs/2404.11577v1"}
{"created":"2024-04-17 17:11:31","title":"On the Scalability of GNNs for Molecular Graphs","abstract":"Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation. Practitioners have observed a strong relationship between model size, dataset size, and performance. However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to the lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures. We address this drawback of GNNs by studying their scaling behavior. Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs. For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules, number of labels, and the diversity in the pretraining datasets, resulting in a 30.25% improvement when scaling to 1 billion parameters and 28.98% improvement when increasing size of dataset to eightfold. We further demonstrate strong finetuning scaling behavior on 38 tasks, outclassing previous large models. We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery.","sentences":["Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation.","Practitioners have observed a strong relationship between model size, dataset size, and performance.","However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to the lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures.","We address this drawback of GNNs by studying their scaling behavior.","Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs.","For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules, number of labels, and the diversity in the pretraining datasets, resulting in a 30.25% improvement when scaling to 1 billion parameters and 28.98% improvement when increasing size of dataset to eightfold.","We further demonstrate strong finetuning scaling behavior on 38 tasks, outclassing previous large models.","We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery."],"url":"http://arxiv.org/abs/2404.11568v1"}
{"created":"2024-04-17 17:00:12","title":"Hierarchical storage management in user space for neuroimaging applications","abstract":"Neuroimaging open-data initiatives have led to increased availability of large scientific datasets. While these datasets are shifting the processing bottleneck from compute-intensive to data-intensive, current standardized analysis tools have yet to adopt strategies that mitigate the costs associated with large data transfers. A major challenge in adapting neuroimaging applications for data-intensive processing is that they must be entirely rewritten. To facilitate data management for standardized neuroimaging tools, we developed Sea, a library that intercepts and redirects application read and write calls to minimize data transfer time. In this paper, we investigate the performance of Sea on three preprocessing pipelines implemented using standard toolboxes (FSL, SPM and AFNI), using three neuroimaging datasets of different sizes (OpenNeuro's ds001545, PREVENT-AD and the HCP dataset) on two high-performance computing clusters. Our results demonstrate that Sea provides large speedups (up to 32X) when the shared file system's (e.g. Lustre) performance is deteriorated. When the shared file system is not overburdened by other users, performance is unaffected by Sea, suggesting that Sea's overhead is minimal even in cases where its benefits are limited. Overall, Sea is beneficial, even when performance gain is minimal, as it can be used to limit the number of files created on parallel file systems.","sentences":["Neuroimaging open-data initiatives have led to increased availability of large scientific datasets.","While these datasets are shifting the processing bottleneck from compute-intensive to data-intensive, current standardized analysis tools have yet to adopt strategies that mitigate the costs associated with large data transfers.","A major challenge in adapting neuroimaging applications for data-intensive processing is that they must be entirely rewritten.","To facilitate data management for standardized neuroimaging tools, we developed Sea, a library that intercepts and redirects application read and write calls to minimize data transfer time.","In this paper, we investigate the performance of Sea on three preprocessing pipelines implemented using standard toolboxes (FSL, SPM and AFNI), using three neuroimaging datasets of different sizes (OpenNeuro's ds001545, PREVENT-AD and the HCP dataset) on two high-performance computing clusters.","Our results demonstrate that Sea provides large speedups (up to 32X) when the shared file system's (e.g. Lustre) performance is deteriorated.","When the shared file system is not overburdened by other users, performance is unaffected by Sea, suggesting that Sea's overhead is minimal even in cases where its benefits are limited.","Overall, Sea is beneficial, even when performance gain is minimal, as it can be used to limit the number of files created on parallel file systems."],"url":"http://arxiv.org/abs/2404.11556v1"}
{"created":"2024-04-17 16:53:16","title":"Quantifying Multilingual Performance of Large Language Models Across Languages","abstract":"The training process of Large Language Models (LLMs) requires extensive text corpus. However, these data are often unevenly distributed in different languages. As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages. However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages. To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages. We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English. We have the following three findings: 1. The performance rankings of different LLMs in all languages are roughly the same. 2. LLMs with different sizes have the same partial order of performance. 3. There is a strong correlation between LlaMa2's performance in different languages and the proportion of the pre-training corpus. These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs.","sentences":["The training process of Large Language Models (LLMs) requires extensive text corpus.","However, these data are often unevenly distributed in different languages.","As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages.","However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages.","To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages.","We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English.","We have the following three findings: 1.","The performance rankings of different LLMs in all languages are roughly the same.","2. LLMs with different sizes have the same partial order of performance.","3.","There is a strong correlation between LlaMa2's performance in different languages and the proportion of the pre-training corpus.","These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs."],"url":"http://arxiv.org/abs/2404.11553v1"}
{"created":"2024-04-17 16:32:13","title":"GenFighter: A Generative and Evolutive Textual Attack Removal","abstract":"Adversarial attacks pose significant challenges to deep neural networks (DNNs) such as Transformer models in natural language processing (NLP). This paper introduces a novel defense strategy, called GenFighter, which enhances adversarial robustness by learning and reasoning on the training classification distribution. GenFighter identifies potentially malicious instances deviating from the distribution, transforms them into semantically equivalent instances aligned with the training data, and employs ensemble techniques for a unified and robust response. By conducting extensive experiments, we show that GenFighter outperforms state-of-the-art defenses in accuracy under attack and attack success rate metrics. Additionally, it requires a high number of queries per attack, making the attack more challenging in real scenarios. The ablation study shows that our approach integrates transfer learning, a generative/evolutive procedure, and an ensemble method, providing an effective defense against NLP adversarial attacks.","sentences":["Adversarial attacks pose significant challenges to deep neural networks (DNNs) such as Transformer models in natural language processing (NLP).","This paper introduces a novel defense strategy, called GenFighter, which enhances adversarial robustness by learning and reasoning on the training classification distribution.","GenFighter identifies potentially malicious instances deviating from the distribution, transforms them into semantically equivalent instances aligned with the training data, and employs ensemble techniques for a unified and robust response.","By conducting extensive experiments, we show that GenFighter outperforms state-of-the-art defenses in accuracy under attack and attack success rate metrics.","Additionally, it requires a high number of queries per attack, making the attack more challenging in real scenarios.","The ablation study shows that our approach integrates transfer learning, a generative/evolutive procedure, and an ensemble method, providing an effective defense against NLP adversarial attacks."],"url":"http://arxiv.org/abs/2404.11538v1"}
{"created":"2024-04-17 16:30:06","title":"FedPFT: Federated Proxy Fine-Tuning of Foundation Models","abstract":"Adapting Foundation Models (FMs) for downstream tasks through Federated Learning (FL) emerges a promising strategy for protecting data privacy and valuable FMs. Existing methods fine-tune FM by allocating sub-FM to clients in FL, however, leading to suboptimal performance due to insufficient tuning and inevitable error accumulations of gradients. In this paper, we propose Federated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation in downstream tasks through FL by two key modules. First, the sub-FM construction module employs a layer-wise compression approach, facilitating comprehensive FM fine-tuning across all layers by emphasizing those crucial neurons. Second, the sub-FM alignment module conducts a two-step distillations-layer-level and neuron-level-before and during FL fine-tuning respectively, to reduce error of gradient by accurately aligning sub-FM with FM under theoretical guarantees. Experimental results on seven commonly used datasets (i.e., four text and three vision) demonstrate the superiority of FedPFT.","sentences":["Adapting Foundation Models (FMs) for downstream tasks through Federated Learning (FL) emerges a promising strategy for protecting data privacy and valuable FMs.","Existing methods fine-tune FM by allocating sub-FM to clients in FL, however, leading to suboptimal performance due to insufficient tuning and inevitable error accumulations of gradients.","In this paper, we propose Federated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation in downstream tasks through FL by two key modules.","First, the sub-FM construction module employs a layer-wise compression approach, facilitating comprehensive FM fine-tuning across all layers by emphasizing those crucial neurons.","Second, the sub-FM alignment module conducts a two-step distillations-layer-level and neuron-level-before and during FL fine-tuning respectively, to reduce error of gradient by accurately aligning sub-FM with FM under theoretical guarantees.","Experimental results on seven commonly used datasets (i.e., four text and three vision) demonstrate the superiority of FedPFT."],"url":"http://arxiv.org/abs/2404.11536v1"}
{"created":"2024-04-17 16:25:19","title":"Select and Reorder: A Novel Approach for Neural Sign Language Production","abstract":"Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.","sentences":["Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets.","This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR).","Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment.","Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds.","Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation.","This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings."],"url":"http://arxiv.org/abs/2404.11532v1"}
{"created":"2024-04-17 16:16:12","title":"JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on Long-Tailed OCTA","abstract":"The oxygen saturation level in the blood (SaO2) is crucial for health, particularly in relation to sleep-related breathing disorders. However, continuous monitoring of SaO2 is time-consuming and highly variable depending on patients' conditions. Recently, optical coherence tomography angiography (OCTA) has shown promising development in rapidly and effectively screening eye-related lesions, offering the potential for diagnosing sleep-related disorders. To bridge this gap, our paper presents three key contributions. Firstly, we propose JointViT, a novel model based on the Vision Transformer architecture, incorporating a joint loss function for supervision. Secondly, we introduce a balancing augmentation technique during data preprocessing to improve the model's performance, particularly on the long-tail distribution within the OCTA dataset. Lastly, through comprehensive experiments on the OCTA dataset, our proposed method significantly outperforms other state-of-the-art methods, achieving improvements of up to 12.28% in overall accuracy. This advancement lays the groundwork for the future utilization of OCTA in diagnosing sleep-related disorders. See project website https://steve-zeyu-zhang.github.io/JointViT","sentences":["The oxygen saturation level in the blood (SaO2) is crucial for health, particularly in relation to sleep-related breathing disorders.","However, continuous monitoring of SaO2 is time-consuming and highly variable depending on patients' conditions.","Recently, optical coherence tomography angiography (OCTA) has shown promising development in rapidly and effectively screening eye-related lesions, offering the potential for diagnosing sleep-related disorders.","To bridge this gap, our paper presents three key contributions.","Firstly, we propose JointViT, a novel model based on the Vision Transformer architecture, incorporating a joint loss function for supervision.","Secondly, we introduce a balancing augmentation technique during data preprocessing to improve the model's performance, particularly on the long-tail distribution within the OCTA dataset.","Lastly, through comprehensive experiments on the OCTA dataset, our proposed method significantly outperforms other state-of-the-art methods, achieving improvements of up to 12.28% in overall accuracy.","This advancement lays the groundwork for the future utilization of OCTA in diagnosing sleep-related disorders.","See project website https://steve-zeyu-zhang.github.io/JointViT"],"url":"http://arxiv.org/abs/2404.11525v1"}
{"created":"2024-04-17 16:10:55","title":"Disentangled Cascaded Graph Convolution Networks for Multi-Behavior Recommendation","abstract":"Multi-behavioral recommender systems have emerged as a solution to address data sparsity and cold-start issues by incorporating auxiliary behaviors alongside target behaviors. However, existing models struggle to accurately capture varying user preferences across different behaviors and fail to account for diverse item preferences within behaviors. Various user preference factors (such as price or quality) entangled in the behavior may lead to sub-optimization problems. Furthermore, these models overlook the personalized nature of user behavioral preferences by employing uniform transformation networks for all users and items. To tackle these challenges, we propose the Disentangled Cascaded Graph Convolutional Network (Disen-CGCN), a novel multi-behavior recommendation model. Disen-CGCN employs disentangled representation techniques to effectively separate factors within user and item representations, ensuring their independence. In addition, it incorporates a multi-behavioral meta-network, enabling personalized feature transformation across user and item behaviors. Furthermore, an attention mechanism captures user preferences for different item factors within each behavior. By leveraging attention weights, we aggregate user and item embeddings separately for each behavior, computing preference scores that predict overall user preferences for items. Our evaluation on benchmark datasets demonstrates the superiority of Disen-CGCN over state-of-the-art models, showcasing an average performance improvement of 7.07% and 9.00% on respective datasets. These results highlight Disen-CGCN's ability to effectively leverage multi-behavioral data, leading to more accurate recommendations.","sentences":["Multi-behavioral recommender systems have emerged as a solution to address data sparsity and cold-start issues by incorporating auxiliary behaviors alongside target behaviors.","However, existing models struggle to accurately capture varying user preferences across different behaviors and fail to account for diverse item preferences within behaviors.","Various user preference factors (such as price or quality) entangled in the behavior may lead to sub-optimization problems.","Furthermore, these models overlook the personalized nature of user behavioral preferences by employing uniform transformation networks for all users and items.","To tackle these challenges, we propose the Disentangled Cascaded Graph Convolutional Network (Disen-CGCN), a novel multi-behavior recommendation model.","Disen-CGCN employs disentangled representation techniques to effectively separate factors within user and item representations, ensuring their independence.","In addition, it incorporates a multi-behavioral meta-network, enabling personalized feature transformation across user and item behaviors.","Furthermore, an attention mechanism captures user preferences for different item factors within each behavior.","By leveraging attention weights, we aggregate user and item embeddings separately for each behavior, computing preference scores that predict overall user preferences for items.","Our evaluation on benchmark datasets demonstrates the superiority of Disen-CGCN over state-of-the-art models, showcasing an average performance improvement of 7.07% and 9.00% on respective datasets.","These results highlight Disen-CGCN's ability to effectively leverage multi-behavioral data, leading to more accurate recommendations."],"url":"http://arxiv.org/abs/2404.11519v1"}
{"created":"2024-04-17 16:07:53","title":"Embedding Privacy in Computational Social Science and Artificial Intelligence Research","abstract":"Privacy is a human right. It ensures that individuals are free to engage in discussions, participate in groups, and form relationships online or offline without fear of their data being inappropriately harvested, analyzed, or otherwise used to harm them. Preserving privacy has emerged as a critical factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and data science domains, given their reliance on individuals' data for novel insights. The increasing use of advanced computational models stands to exacerbate privacy concerns because, if inappropriately used, they can quickly infringe privacy rights and lead to adverse effects for individuals - especially vulnerable groups - and society. We have already witnessed a host of privacy issues emerge with the advent of large language models (LLMs), such as ChatGPT, which further demonstrate the importance of embedding privacy from the start. This article contributes to the field by discussing the role of privacy and the primary issues that researchers working in CSS, AI, data science and related domains are likely to face. It then presents several key considerations for researchers to ensure participant privacy is best preserved in their research design, data collection and use, analysis, and dissemination of research results.","sentences":["Privacy is a human right.","It ensures that individuals are free to engage in discussions, participate in groups, and form relationships online or offline without fear of their data being inappropriately harvested, analyzed, or otherwise used to harm them.","Preserving privacy has emerged as a critical factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and data science domains, given their reliance on individuals' data for novel insights.","The increasing use of advanced computational models stands to exacerbate privacy concerns because, if inappropriately used, they can quickly infringe privacy rights and lead to adverse effects for individuals - especially vulnerable groups - and society.","We have already witnessed a host of privacy issues emerge with the advent of large language models (LLMs), such as ChatGPT, which further demonstrate the importance of embedding privacy from the start.","This article contributes to the field by discussing the role of privacy and the primary issues that researchers working in CSS, AI, data science and related domains are likely to face.","It then presents several key considerations for researchers to ensure participant privacy is best preserved in their research design, data collection and use, analysis, and dissemination of research results."],"url":"http://arxiv.org/abs/2404.11515v1"}
{"created":"2024-04-17 15:59:25","title":"Testing Intersectingness of Uniform Families","abstract":"A set family ${\\cal F}$ is called intersecting if every two members of ${\\cal F}$ intersect, and it is called uniform if all members of ${\\cal F}$ share a common size. A uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$ of $k$-subsets of $[n]$ is $\\varepsilon$-far from intersecting if one has to remove more than $\\varepsilon \\cdot \\binom{n}{k}$ of the sets of ${\\cal F}$ to make it intersecting. We study the property testing problem that given query access to a uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$, asks to distinguish between the case that ${\\cal F}$ is intersecting and the case that it is $\\varepsilon$-far from intersecting. We prove that for every fixed integer $r$, the problem admits a non-adaptive two-sided error tester with query complexity $O(\\frac{\\ln n}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k}{n})^r)$ and a non-adaptive one-sided error tester with query complexity $O(\\frac{\\ln k}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^r)$. The query complexities are optimal up to the logarithmic terms. For $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^2)$, we further provide a non-adaptive one-sided error tester with optimal query complexity of $O(\\frac{1}{\\varepsilon})$. Our findings show that the query complexity of the problem differs substantially from that of testing intersectingness of non-uniform families, studied recently by Chen, De, Li, Nadimpalli, and Servedio (ITCS, 2024).","sentences":["A set family ${\\cal F}$ is called intersecting if every two members of ${\\cal F}$ intersect, and it is called uniform if all members of ${\\cal F}$ share a common size.","A uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$ of $k$-subsets of $[n]$ is $\\varepsilon$-far from intersecting if one has to remove more than $\\varepsilon \\cdot \\binom{n}{k}$ of the sets of ${\\cal F}$ to make it intersecting.","We study the property testing problem that given query access to a uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$, asks to distinguish between the case that ${\\cal F}$ is intersecting and the case that it is $\\varepsilon$-far from intersecting.","We prove that for every fixed integer $r$, the problem admits a non-adaptive two-sided error tester with query complexity $O(\\frac{\\ln n}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k}{n})^r)$ and a non-adaptive one-sided error tester with query complexity $O(\\frac{\\ln k}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^r)$. The query complexities are optimal up to the logarithmic terms.","For $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^2)$, we further provide a non-adaptive one-sided error tester with optimal query complexity of $O(\\frac{1}{\\varepsilon})$. Our findings show that the query complexity of the problem differs substantially from that of testing intersectingness of non-uniform families, studied recently by Chen, De, Li, Nadimpalli, and Servedio (ITCS, 2024)."],"url":"http://arxiv.org/abs/2404.11504v1"}
{"created":"2024-04-17 15:52:38","title":"A Data-Driven Representation for Sign Language Production","abstract":"Phonetic representations are used when recording spoken languages, but no equivalent exists for recording signed languages. As a result, linguists have proposed several annotation systems that operate on the gloss or sub-unit level; however, these resources are notably irregular and scarce.   Sign Language Production (SLP) aims to automatically translate spoken language sentences into continuous sequences of sign language. However, current state-of-the-art approaches rely on scarce linguistic resources to work. This has limited progress in the field. This paper introduces an innovative solution by transforming the continuous pose generation problem into a discrete sequence generation problem. Thus, overcoming the need for costly annotation. Although, if available, we leverage the additional information to enhance our approach.   By applying Vector Quantisation (VQ) to sign language data, we first learn a codebook of short motions that can be combined to create a natural sequence of sign. Where each token in the codebook can be thought of as the lexicon of our representation. Then using a transformer we perform a translation from spoken language text to a sequence of codebook tokens. Each token can be directly mapped to a sequence of poses allowing the translation to be performed by a single network. Furthermore, we present a sign stitching method to effectively join tokens together. We evaluate on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets. An extensive evaluation shows our approach outperforms previous methods, increasing the BLEU-1 back translation score by up to 72%.","sentences":["Phonetic representations are used when recording spoken languages, but no equivalent exists for recording signed languages.","As a result, linguists have proposed several annotation systems that operate on the gloss or sub-unit level; however, these resources are notably irregular and scarce.   ","Sign Language Production (SLP) aims to automatically translate spoken language sentences into continuous sequences of sign language.","However, current state-of-the-art approaches rely on scarce linguistic resources to work.","This has limited progress in the field.","This paper introduces an innovative solution by transforming the continuous pose generation problem into a discrete sequence generation problem.","Thus, overcoming the need for costly annotation.","Although, if available, we leverage the additional information to enhance our approach.   ","By applying Vector Quantisation (VQ) to sign language data, we first learn a codebook of short motions that can be combined to create a natural sequence of sign.","Where each token in the codebook can be thought of as the lexicon of our representation.","Then using a transformer we perform a translation from spoken language text to a sequence of codebook tokens.","Each token can be directly mapped to a sequence of poses allowing the translation to be performed by a single network.","Furthermore, we present a sign stitching method to effectively join tokens together.","We evaluate on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets.","An extensive evaluation shows our approach outperforms previous methods, increasing the BLEU-1 back translation score by up to 72%."],"url":"http://arxiv.org/abs/2404.11499v1"}
{"created":"2024-04-17 15:34:55","title":"IoTSim-Osmosis-RES: Towards autonomic renewable energy-aware osmotic computing","abstract":"Internet of Things systems exists in various areas of our everyday life. For example, sensors installed in smart cities and homes are processed in edge and cloud computing centres providing several benefits that improve our lives. The place of data processing is related to the required system response times -- processing data closer to its source results in a shorter system response time. The Osmotic Computing concept enables flexible deployment of data processing services and their possible movement, just like particles in the osmosis phenomenon move between regions of different densities. At the same time, the impact of complex computer architecture on the environment is increasingly being compensated by the use of renewable and low-carbon energy sources. However, the uncertainty of supplying green energy makes the management of Osmotic Computing demanding, and therefore their autonomy is desirable. In the paper, we present a framework enabling osmotic computing simulation based on renewable energy sources and autonomic osmotic agents, allowing the analysis of distributed management algorithms. We discuss the challenges posed to the framework and analyze various management algorithms for cooperating osmotic agents. In the evaluation we show that changing the adaptation logic of the osmotic agents, it is possible to increase the self-consumption of renewable energy sources or increase the usage of low emission ones.","sentences":["Internet of Things systems exists in various areas of our everyday life.","For example, sensors installed in smart cities and homes are processed in edge and cloud computing centres providing several benefits that improve our lives.","The place of data processing is related to the required system response times -- processing data closer to its source results in a shorter system response time.","The Osmotic Computing concept enables flexible deployment of data processing services and their possible movement, just like particles in the osmosis phenomenon move between regions of different densities.","At the same time, the impact of complex computer architecture on the environment is increasingly being compensated by the use of renewable and low-carbon energy sources.","However, the uncertainty of supplying green energy makes the management of Osmotic Computing demanding, and therefore their autonomy is desirable.","In the paper, we present a framework enabling osmotic computing simulation based on renewable energy sources and autonomic osmotic agents, allowing the analysis of distributed management algorithms.","We discuss the challenges posed to the framework and analyze various management algorithms for cooperating osmotic agents.","In the evaluation we show that changing the adaptation logic of the osmotic agents, it is possible to increase the self-consumption of renewable energy sources or increase the usage of low emission ones."],"url":"http://arxiv.org/abs/2404.11481v1"}
{"created":"2024-04-17 15:26:55","title":"Assessing The Effectiveness Of Current Cybersecurity Regulations And Policies In The US","abstract":"This article assesses the effectiveness of current cybersecurity regulations and policies in the United States amidst the escalating frequency and sophistication of cyber threats. The focus is on the comprehensive framework established by the U.S. government, with a spotlight on the National Institute of Standards and Technology (NIST) Cybersecurity Framework and key regulations such as HIPAA, GLBA, FISMA, CISA, CCPA, and the DOD Cybersecurity Maturity Model Certification. The study evaluates the impact of these regulations on different sectors and analyzes trends in cybercrime data from 2000 to 2022. The findings highlight the challenges, successes, and the need for continuous adaptation in the face of evolving cyber threats","sentences":["This article assesses the effectiveness of current cybersecurity regulations and policies in the United States amidst the escalating frequency and sophistication of cyber threats.","The focus is on the comprehensive framework established by the U.S. government, with a spotlight on the National Institute of Standards and Technology (NIST) Cybersecurity Framework and key regulations such as HIPAA, GLBA, FISMA, CISA, CCPA, and the DOD Cybersecurity Maturity Model Certification.","The study evaluates the impact of these regulations on different sectors and analyzes trends in cybercrime data from 2000 to 2022.","The findings highlight the challenges, successes, and the need for continuous adaptation in the face of evolving cyber threats"],"url":"http://arxiv.org/abs/2404.11473v1"}
{"created":"2024-04-17 15:23:12","title":"A Federated Learning Approach to Privacy Preserving Offensive Language Identification","abstract":"The spread of various forms of offensive speech online is an important concern in social media. While platforms have been investing heavily in ways of coping with this problem, the question of privacy remains largely unaddressed. Models trained to detect offensive language on social media are trained and/or fine-tuned using large amounts of data often stored in centralized servers. Since most social media data originates from end users, we propose a privacy preserving decentralized architecture for identifying offensive language online by introducing Federated Learning (FL) in the context of offensive language identification. FL is a decentralized architecture that allows multiple models to be trained locally without the need for data sharing hence preserving users' privacy. We propose a model fusion approach to perform FL. We trained multiple deep learning models on four publicly available English benchmark datasets (AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We also present initial cross-lingual experiments in English and Spanish. We show that the proposed model fusion approach outperforms baselines in all the datasets while preserving privacy.","sentences":["The spread of various forms of offensive speech online is an important concern in social media.","While platforms have been investing heavily in ways of coping with this problem, the question of privacy remains largely unaddressed.","Models trained to detect offensive language on social media are trained and/or fine-tuned using large amounts of data often stored in centralized servers.","Since most social media data originates from end users, we propose a privacy preserving decentralized architecture for identifying offensive language online by introducing Federated Learning (FL) in the context of offensive language identification.","FL is a decentralized architecture that allows multiple models to be trained locally without the need for data sharing hence preserving users' privacy.","We propose a model fusion approach to perform FL.","We trained multiple deep learning models on four publicly available English benchmark datasets (AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail.","We also present initial cross-lingual experiments in English and Spanish.","We show that the proposed model fusion approach outperforms baselines in all the datasets while preserving privacy."],"url":"http://arxiv.org/abs/2404.11470v1"}
{"created":"2024-04-17 15:17:57","title":"Designing Touchscreen Menu Interfaces for In-Vehicle Infotainment Systems: the Effect of Depth and Breadth Trade-off and Task Types on Visual-Manual Distraction","abstract":"Multitasking with a touch screen user-interface while driving is known to impact negatively driving performance and safety. Literature shows that list scrolling interfaces generate more visual-manual distraction than structured menus and sequential navigation. Depth and breadth trade-offs for structured navigation have been studied. However, little is known on how secondary task characteristics interact with those trade-offs. In this study, we make the hypothesis that both menu's depth and task complexity interact in generating visual-manual distraction. Using a driving simulation setup, we collected telemetry and eye-tracking data to evaluate driving performance. Participants were multitasking with a mobile app, presenting a range of eight depth and breadth trade-offs under three types of secondary tasks, involving different cognitive operations (Systematic reading, Search for an item, Memorize items' state). The results confirm our hypothesis. Systematic interaction with menu items generated a visual demand that increased with menu's depth, while visual demand reach an optimum for Search and Memory tasks. We discuss implications for design: In a multitasking context, display design effectiveness must be assessed while considering menu's layout but also cognitive processes involved.","sentences":["Multitasking with a touch screen user-interface while driving is known to impact negatively driving performance and safety.","Literature shows that list scrolling interfaces generate more visual-manual distraction than structured menus and sequential navigation.","Depth and breadth trade-offs for structured navigation have been studied.","However, little is known on how secondary task characteristics interact with those trade-offs.","In this study, we make the hypothesis that both menu's depth and task complexity interact in generating visual-manual distraction.","Using a driving simulation setup, we collected telemetry and eye-tracking data to evaluate driving performance.","Participants were multitasking with a mobile app, presenting a range of eight depth and breadth trade-offs under three types of secondary tasks, involving different cognitive operations (Systematic reading, Search for an item, Memorize items' state).","The results confirm our hypothesis.","Systematic interaction with menu items generated a visual demand that increased with menu's depth, while visual demand reach an optimum for Search and Memory tasks.","We discuss implications for design: In a multitasking context, display design effectiveness must be assessed while considering menu's layout but also cognitive processes involved."],"url":"http://arxiv.org/abs/2404.11469v1"}
{"created":"2024-04-17 15:16:01","title":"A Large-scale Fine-grained Analysis of Packages in Open-Source Software Ecosystems","abstract":"Package managers such as NPM, Maven, and PyPI play a pivotal role in open-source software (OSS) ecosystems, streamlining the distribution and management of various freely available packages. The fine-grained details within software packages can unveil potential risks within existing OSS ecosystems, offering valuable insights for detecting malicious packages. In this study, we undertake a large-scale empirical analysis focusing on fine-grained information (FGI): the metadata, static, and dynamic functions. Specifically, we investigate the FGI usage across a diverse set of 50,000+ legitimate and 1,000+ malicious packages. Based on this diverse data collection, we conducted a comparative analysis between legitimate and malicious packages. Our findings reveal that (1) malicious packages have less metadata content and utilize fewer static and dynamic functions than legitimate ones; (2) malicious packages demonstrate a higher tendency to invoke HTTP/URL functions as opposed to other application services, such as FTP or SMTP; (3) FGI serves as a distinguishable indicator between legitimate and malicious packages; and (4) one dimension in FGI has sufficient distinguishable capability to detect malicious packages, and combining all dimensions in FGI cannot significantly improve overall performance.","sentences":["Package managers such as NPM, Maven, and PyPI play a pivotal role in open-source software (OSS) ecosystems, streamlining the distribution and management of various freely available packages.","The fine-grained details within software packages can unveil potential risks within existing OSS ecosystems, offering valuable insights for detecting malicious packages.","In this study, we undertake a large-scale empirical analysis focusing on fine-grained information (FGI): the metadata, static, and dynamic functions.","Specifically, we investigate the FGI usage across a diverse set of 50,000+ legitimate and 1,000+ malicious packages.","Based on this diverse data collection, we conducted a comparative analysis between legitimate and malicious packages.","Our findings reveal that (1) malicious packages have less metadata content and utilize fewer static and dynamic functions than legitimate ones; (2) malicious packages demonstrate a higher tendency to invoke HTTP/URL functions as opposed to other application services, such as FTP or SMTP; (3) FGI serves as a distinguishable indicator between legitimate and malicious packages; and (4) one dimension in FGI has sufficient distinguishable capability to detect malicious packages, and combining all dimensions in FGI cannot significantly improve overall performance."],"url":"http://arxiv.org/abs/2404.11467v1"}
{"created":"2024-04-17 15:07:06","title":"Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent","abstract":"A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions. Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging. In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications. To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters. Like GPT-4, our model can process both English and Chinese. We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi.","sentences":["A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions.","Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging.","In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications.","To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters.","Like GPT-4, our model can process both English and Chinese.","We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi."],"url":"http://arxiv.org/abs/2404.11459v1"}
{"created":"2024-04-17 15:05:03","title":"Unifying Bias and Unfairness in Information Retrieval: A Survey of Challenges and Opportunities with Large Language Models","abstract":"With the rapid advancement of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.","sentences":["With the rapid advancement of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift.","This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem.","In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs.","We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment.","Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation.","In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues.","Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era.","We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey."],"url":"http://arxiv.org/abs/2404.11457v1"}
{"created":"2024-04-17 15:05:00","title":"Deep Pattern Network for Click-Through Rate Prediction","abstract":"Click-through rate (CTR) prediction tasks play a pivotal role in real-world applications, particularly in recommendation systems and online advertising. A significant research branch in this domain focuses on user behavior modeling. Current research predominantly centers on modeling co-occurrence relationships between the target item and items previously interacted with by users in their historical data. However, this focus neglects the intricate modeling of user behavior patterns. In reality, the abundance of user interaction records encompasses diverse behavior patterns, indicative of a spectrum of habitual paradigms. These patterns harbor substantial potential to significantly enhance CTR prediction performance. To harness the informational potential within user behavior patterns, we extend Target Attention (TA) to Target Pattern Attention (TPA) to model pattern-level dependencies. Furthermore, three critical challenges demand attention: the inclusion of unrelated items within behavior patterns, data sparsity in behavior patterns, and computational complexity arising from numerous patterns. To address these challenges, we introduce the Deep Pattern Network (DPN), designed to comprehensively leverage information from user behavior patterns. DPN efficiently retrieves target-related user behavior patterns using a target-aware attention mechanism. Additionally, it contributes to refining user behavior patterns through a pre-training paradigm based on self-supervised learning while promoting dependency learning within sparse patterns. Our comprehensive experiments, conducted across three public datasets, substantiate the superior performance and broad compatibility of DPN.","sentences":["Click-through rate (CTR) prediction tasks play a pivotal role in real-world applications, particularly in recommendation systems and online advertising.","A significant research branch in this domain focuses on user behavior modeling.","Current research predominantly centers on modeling co-occurrence relationships between the target item and items previously interacted with by users in their historical data.","However, this focus neglects the intricate modeling of user behavior patterns.","In reality, the abundance of user interaction records encompasses diverse behavior patterns, indicative of a spectrum of habitual paradigms.","These patterns harbor substantial potential to significantly enhance CTR prediction performance.","To harness the informational potential within user behavior patterns, we extend Target Attention (TA) to Target Pattern Attention (TPA) to model pattern-level dependencies.","Furthermore, three critical challenges demand attention: the inclusion of unrelated items within behavior patterns, data sparsity in behavior patterns, and computational complexity arising from numerous patterns.","To address these challenges, we introduce the Deep Pattern Network (DPN), designed to comprehensively leverage information from user behavior patterns.","DPN efficiently retrieves target-related user behavior patterns using a target-aware attention mechanism.","Additionally, it contributes to refining user behavior patterns through a pre-training paradigm based on self-supervised learning while promoting dependency learning within sparse patterns.","Our comprehensive experiments, conducted across three public datasets, substantiate the superior performance and broad compatibility of DPN."],"url":"http://arxiv.org/abs/2404.11456v1"}
{"created":"2024-04-17 14:55:49","title":"Real-Time Trajectory Synthesis with Local Differential Privacy","abstract":"Trajectory streams are being generated from location-aware devices, such as smartphones and in-vehicle navigation systems. Due to the sensitive nature of the location data, directly sharing user trajectories suffers from privacy leakage issues. Local differential privacy (LDP), which perturbs sensitive data on the user side before it is shared or analyzed, emerges as a promising solution for private trajectory stream collection and analysis. Unfortunately, existing stream release approaches often neglect the rich spatial-temporal context information within trajectory streams, resulting in suboptimal utility and limited types of downstream applications. To this end, we propose RetraSyn, a novel real-time trajectory synthesis framework, which is able to perform on-the-fly trajectory synthesis based on the mobility patterns privately extracted from users' trajectory streams. Thus, the downstream trajectory analysis can be performed on the high-utility synthesized data with privacy protection. We also take the genuine behaviors of real-world mobile travelers into consideration, ensuring authenticity and practicality. The key components of RetraSyn include the global mobility model, dynamic mobility update mechanism, real-time synthesis, and adaptive allocation strategy. We conduct extensive experiments on multiple real-world and synthetic trajectory datasets under various location-based utility metrics, encompassing both streaming and historical scenarios. The empirical results demonstrate the superiority and versatility of our proposed framework.","sentences":["Trajectory streams are being generated from location-aware devices, such as smartphones and in-vehicle navigation systems.","Due to the sensitive nature of the location data, directly sharing user trajectories suffers from privacy leakage issues.","Local differential privacy (LDP), which perturbs sensitive data on the user side before it is shared or analyzed, emerges as a promising solution for private trajectory stream collection and analysis.","Unfortunately, existing stream release approaches often neglect the rich spatial-temporal context information within trajectory streams, resulting in suboptimal utility and limited types of downstream applications.","To this end, we propose RetraSyn, a novel real-time trajectory synthesis framework, which is able to perform on-the-fly trajectory synthesis based on the mobility patterns privately extracted from users' trajectory streams.","Thus, the downstream trajectory analysis can be performed on the high-utility synthesized data with privacy protection.","We also take the genuine behaviors of real-world mobile travelers into consideration, ensuring authenticity and practicality.","The key components of RetraSyn include the global mobility model, dynamic mobility update mechanism, real-time synthesis, and adaptive allocation strategy.","We conduct extensive experiments on multiple real-world and synthetic trajectory datasets under various location-based utility metrics, encompassing both streaming and historical scenarios.","The empirical results demonstrate the superiority and versatility of our proposed framework."],"url":"http://arxiv.org/abs/2404.11450v1"}
{"created":"2024-04-17 14:55:27","title":"AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large Language Models for Extracting Cognitive Pathways from Social Media Texts","abstract":"Cognitive Behavioral Therapy (CBT) is an effective technique for addressing the irrational thoughts stemming from mental illnesses, but it necessitates precise identification of cognitive pathways to be successfully implemented in patient care. In current society, individuals frequently express negative emotions on social media on specific topics, often exhibiting cognitive distortions, including suicidal behaviors in extreme cases. Yet, there is a notable absence of methodologies for analyzing cognitive pathways that could aid psychotherapists in conducting effective interventions online. In this study, we gathered data from social media and established the task of extracting cognitive pathways, annotating the data based on a cognitive theoretical framework. We initially categorized the task of extracting cognitive pathways as a hierarchical text classification with four main categories and nineteen subcategories. Following this, we structured a text summarization task to help psychotherapists quickly grasp the essential information. Our experiments evaluate the performance of deep learning and large language models (LLMs) on these tasks. The results demonstrate that our deep learning method achieved a micro-F1 score of 62.34% in the hierarchical text classification task. Meanwhile, in the text summarization task, GPT-4 attained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the experimental deep learning model's performance. However, it may suffer from an issue of hallucination. We have made all models and codes publicly available to support further research in this field.","sentences":["Cognitive Behavioral Therapy (CBT) is an effective technique for addressing the irrational thoughts stemming from mental illnesses, but it necessitates precise identification of cognitive pathways to be successfully implemented in patient care.","In current society, individuals frequently express negative emotions on social media on specific topics, often exhibiting cognitive distortions, including suicidal behaviors in extreme cases.","Yet, there is a notable absence of methodologies for analyzing cognitive pathways that could aid psychotherapists in conducting effective interventions online.","In this study, we gathered data from social media and established the task of extracting cognitive pathways, annotating the data based on a cognitive theoretical framework.","We initially categorized the task of extracting cognitive pathways as a hierarchical text classification with four main categories and nineteen subcategories.","Following this, we structured a text summarization task to help psychotherapists quickly grasp the essential information.","Our experiments evaluate the performance of deep learning and large language models (LLMs) on these tasks.","The results demonstrate that our deep learning method achieved a micro-F1 score of 62.34% in the hierarchical text classification task.","Meanwhile, in the text summarization task, GPT-4 attained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the experimental deep learning model's performance.","However, it may suffer from an issue of hallucination.","We have made all models and codes publicly available to support further research in this field."],"url":"http://arxiv.org/abs/2404.11449v1"}
{"created":"2024-04-17 14:53:03","title":"Prediction of Unmanned Surface Vessel Motion Attitude Based on CEEMDAN-PSO-SVM","abstract":"Unmanned boats, while navigating at sea, utilize active compensation systems to mitigate wave disturbances experienced by onboard instruments and equipment. However, there exists a lag in the measurement of unmanned boat attitudes, thus introducing unmanned boat motion attitude prediction to compensate for the lag in the signal acquisition process. This paper, based on the basic principles of waves, derives the disturbance patterns of waves on unmanned boats from the wave energy spectrum. Through simulation analysis of unmanned boat motion attitudes, motion attitude data is obtained, providing experimental data for subsequent work. A combined prediction model based on Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), Particle Swarm Optimization (PSO), and Support Vector Machine (SVM) is designed to predict the motion attitude of unmanned boats. Simulation results validate its superior prediction accuracy compared to traditional prediction models. For example, in terms of mean absolute error, it improves by 17% compared to the EMD-PSO-SVM model.","sentences":["Unmanned boats, while navigating at sea, utilize active compensation systems to mitigate wave disturbances experienced by onboard instruments and equipment.","However, there exists a lag in the measurement of unmanned boat attitudes, thus introducing unmanned boat motion attitude prediction to compensate for the lag in the signal acquisition process.","This paper, based on the basic principles of waves, derives the disturbance patterns of waves on unmanned boats from the wave energy spectrum.","Through simulation analysis of unmanned boat motion attitudes, motion attitude data is obtained, providing experimental data for subsequent work.","A combined prediction model based on Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), Particle Swarm Optimization (PSO), and Support Vector Machine (SVM) is designed to predict the motion attitude of unmanned boats.","Simulation results validate its superior prediction accuracy compared to traditional prediction models.","For example, in terms of mean absolute error, it improves by 17% compared to the EMD-PSO-SVM model."],"url":"http://arxiv.org/abs/2404.11443v1"}
{"created":"2024-04-17 14:49:03","title":"A waypoint based approach to visibility in performance based fire safety design","abstract":"In performance-based fire safety design, ensuring safe egress, e.g. by visibility of safety signs, is a crucial safety goal. Compliance with the building requirements is often demonstrated by simulations of smoke spread. Numerical models like the Fire Dynamics Simulator generally compute visibility as a local quantity using the light extinction coefficient, without the consideration of the actual light path to a safety sign. Here, visibility maps are introduced, providing an approach for post-processing fire simulation data. They indicate safe areas along egress routes, with respect to visibility. At each location, the available visibility is calculated using Jin's law, as an integrated value of the extinction coefficient along the line of sight to the closest exit sign. The required visibility results from the distance between those points. Additional parameters like view angle or visual obstructions are considered. The presented method allows for temporal visibility assessment, e.g. in an ASET-RSET analysis.","sentences":["In performance-based fire safety design, ensuring safe egress, e.g. by visibility of safety signs, is a crucial safety goal.","Compliance with the building requirements is often demonstrated by simulations of smoke spread.","Numerical models like the Fire Dynamics Simulator generally compute visibility as a local quantity using the light extinction coefficient, without the consideration of the actual light path to a safety sign.","Here, visibility maps are introduced, providing an approach for post-processing fire simulation data.","They indicate safe areas along egress routes, with respect to visibility.","At each location, the available visibility is calculated using Jin's law, as an integrated value of the extinction coefficient along the line of sight to the closest exit sign.","The required visibility results from the distance between those points.","Additional parameters like view angle or visual obstructions are considered.","The presented method allows for temporal visibility assessment, e.g. in an ASET-RSET analysis."],"url":"http://arxiv.org/abs/2404.11439v1"}
{"created":"2024-04-17 14:33:41","title":"SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow","abstract":"Increasing the annotation efficiency of trajectory annotations from videos has the potential to enable the next generation of data-hungry tracking algorithms to thrive on large-scale datasets. Despite the importance of this task, there are currently very few works exploring how to efficiently label tracking datasets comprehensively. In this work, we introduce SPAM, a tracking data engine that provides high-quality labels with minimal human intervention. SPAM is built around two key insights: i) most tracking scenarios can be easily resolved. To take advantage of this, we utilize a pre-trained model to generate high-quality pseudo-labels, reserving human involvement for a smaller subset of more difficult instances; ii) handling the spatiotemporal dependencies of track annotations across time can be elegantly and efficiently formulated through graphs. Therefore, we use a unified graph formulation to address the annotation of both detections and identity association for tracks across time. Based on these insights, SPAM produces high-quality annotations with a fraction of ground truth labeling cost. We demonstrate that trackers trained on SPAM labels achieve comparable performance to those trained on human annotations while requiring only 3-20% of the human labeling effort. Hence, SPAM paves the way towards highly efficient labeling of large-scale tracking datasets. Our code and models will be available upon acceptance.","sentences":["Increasing the annotation efficiency of trajectory annotations from videos has the potential to enable the next generation of data-hungry tracking algorithms to thrive on large-scale datasets.","Despite the importance of this task, there are currently very few works exploring how to efficiently label tracking datasets comprehensively.","In this work, we introduce SPAM, a tracking data engine that provides high-quality labels with minimal human intervention.","SPAM is built around two key insights: i) most tracking scenarios can be easily resolved.","To take advantage of this, we utilize a pre-trained model to generate high-quality pseudo-labels, reserving human involvement for a smaller subset of more difficult instances; ii) handling the spatiotemporal dependencies of track annotations across time can be elegantly and efficiently formulated through graphs.","Therefore, we use a unified graph formulation to address the annotation of both detections and identity association for tracks across time.","Based on these insights, SPAM produces high-quality annotations with a fraction of ground truth labeling cost.","We demonstrate that trackers trained on SPAM labels achieve comparable performance to those trained on human annotations while requiring only 3-20% of the human labeling effort.","Hence, SPAM paves the way towards highly efficient labeling of large-scale tracking datasets.","Our code and models will be available upon acceptance."],"url":"http://arxiv.org/abs/2404.11426v1"}
{"created":"2024-04-17 13:55:05","title":"What-if Analysis Framework for Digital Twins in 6G Wireless Network Management","abstract":"This study explores implementing a digital twin network (DTN) for efficient 6G wireless network management, aligning with the fault, configuration, accounting, performance, and security (FCAPS) model. The DTN architecture comprises the Physical Twin Layer, implemented using NS-3, and the Service Layer, featuring machine learning and reinforcement learning for optimizing carrier sensitivity threshold and transmit power control in wireless networks. We introduce a robust \"What-if Analysis\" module, utilizing conditional tabular generative adversarial network (CTGAN) for synthetic data generation to mimic various network scenarios. These scenarios assess four network performance metrics: throughput, latency, packet loss, and coverage. Our findings demonstrate the efficiency of the proposed what-if analysis framework in managing complex network conditions, highlighting the importance of the scenario-maker step and the impact of twinning intervals on network performance.","sentences":["This study explores implementing a digital twin network (DTN) for efficient 6G wireless network management, aligning with the fault, configuration, accounting, performance, and security (FCAPS) model.","The DTN architecture comprises the Physical Twin Layer, implemented using NS-3, and the Service Layer, featuring machine learning and reinforcement learning for optimizing carrier sensitivity threshold and transmit power control in wireless networks.","We introduce a robust \"What-if Analysis\" module, utilizing conditional tabular generative adversarial network (CTGAN) for synthetic data generation to mimic various network scenarios.","These scenarios assess four network performance metrics: throughput, latency, packet loss, and coverage.","Our findings demonstrate the efficiency of the proposed what-if analysis framework in managing complex network conditions, highlighting the importance of the scenario-maker step and the impact of twinning intervals on network performance."],"url":"http://arxiv.org/abs/2404.11394v1"}
{"created":"2024-04-17 13:48:30","title":"Enhancing Data Privacy In Wireless Sensor Networks: Investigating Techniques And Protocols To Protect Privacy Of Data Transmitted Over Wireless Sensor Networks In Critical Applications Of Healthcare And National Security","abstract":"The article discusses the emergence of Wireless Sensor Networks (WSNs) as a groundbreaking technology in data processing and communication. It outlines how WSNs, composed of dispersed autonomous sensors, are utilized to monitor physical and environmental factors, transmitting data wirelessly for analysis. The article explores various applications of WSNs in healthcare, national security, emergency response, and infrastructure monitoring, highlighting their roles in enhancing patient care, public health surveillance, border security, disaster management, and military operations. Additionally, it examines the foundational concepts of data privacy in WSNs, focusing on encryption techniques, authentication mechanisms, anonymization techniques, and access control mechanisms. The article also addresses vulnerabilities, threats, and challenges related to data privacy in healthcare and national security contexts, emphasizing regulatory compliance, ethical considerations, and socio-economic factors. Furthermore, it introduces the Diffusion of Innovation Theory as a framework for understanding the adoption of privacy-enhancing technologies in WSNs. Finally, the article reviews empirical studies demonstrating the efficacy of security solutions in preserving data privacy in WSNs, offering insights into advancements in safeguarding sensitive information.","sentences":["The article discusses the emergence of Wireless Sensor Networks (WSNs) as a groundbreaking technology in data processing and communication.","It outlines how WSNs, composed of dispersed autonomous sensors, are utilized to monitor physical and environmental factors, transmitting data wirelessly for analysis.","The article explores various applications of WSNs in healthcare, national security, emergency response, and infrastructure monitoring, highlighting their roles in enhancing patient care, public health surveillance, border security, disaster management, and military operations.","Additionally, it examines the foundational concepts of data privacy in WSNs, focusing on encryption techniques, authentication mechanisms, anonymization techniques, and access control mechanisms.","The article also addresses vulnerabilities, threats, and challenges related to data privacy in healthcare and national security contexts, emphasizing regulatory compliance, ethical considerations, and socio-economic factors.","Furthermore, it introduces the Diffusion of Innovation Theory as a framework for understanding the adoption of privacy-enhancing technologies in WSNs.","Finally, the article reviews empirical studies demonstrating the efficacy of security solutions in preserving data privacy in WSNs, offering insights into advancements in safeguarding sensitive information."],"url":"http://arxiv.org/abs/2404.11388v1"}
{"created":"2024-04-17 13:32:05","title":"Tensor Factorisation for Polypharmacy Side Effect Prediction","abstract":"Adverse reactions caused by drug combinations are an increasingly common phenomenon, making their accurate prediction an important challenge in modern medicine. However, the polynomial nature of this problem renders lab-based identification of adverse reactions insufficient. Dozens of computational approaches have therefore been proposed for the task in recent years, with varying degrees of success. One group of methods that has seemingly been under-utilised in this area is tensor factorisation, despite their clear applicability to this type of data. In this work, we apply three such models to a benchmark dataset in order to compare them against established techniques. We find, in contrast to previous reports, that for this task tensor factorisation models are competitive with state-of-the-art graph neural network models and we recommend that future work in this field considers cheaper methods with linear complexity before running costly deep learning processes.","sentences":["Adverse reactions caused by drug combinations are an increasingly common phenomenon, making their accurate prediction an important challenge in modern medicine.","However, the polynomial nature of this problem renders lab-based identification of adverse reactions insufficient.","Dozens of computational approaches have therefore been proposed for the task in recent years, with varying degrees of success.","One group of methods that has seemingly been under-utilised in this area is tensor factorisation, despite their clear applicability to this type of data.","In this work, we apply three such models to a benchmark dataset in order to compare them against established techniques.","We find, in contrast to previous reports, that for this task tensor factorisation models are competitive with state-of-the-art graph neural network models and we recommend that future work in this field considers cheaper methods with linear complexity before running costly deep learning processes."],"url":"http://arxiv.org/abs/2404.11374v1"}
{"created":"2024-04-17 13:31:50","title":"S3PHER: Secure and Searchable System for Patient-driven HEalth data shaRing","abstract":"Healthcare data contains some of the most sensitive information about an individual, yet sharing this data with healthcare practitioners can significantly enhance patient care and support research efforts. However, current systems for sharing health data between patients and caregivers do not fully address the critical security requirements of privacy, confidentiality, and consent management. Furthermore, compliance with regulatory laws such as GDPR and HIPAA is often deficient, largely because patients typically are asked to provide general consent for healthcare entities to access their data. Recognizing the limitations of existing systems, we present S3PHER, a novel approach to sharing health data that provides patients with control over who accesses their data, what data is accessed, and when. Our system ensures end to end privacy by integrating a Proxy ReEncryption Scheme with a Searchable Encryption Scheme, utilizing Homomorphic Encryption to enable healthcare practitioners to privately search and access patients' documents. The practicality and benefits of S3PHER are further validated through end to end deployment and use case analyses, with tests on real datasets demonstrating promising execution times.","sentences":["Healthcare data contains some of the most sensitive information about an individual, yet sharing this data with healthcare practitioners can significantly enhance patient care and support research efforts.","However, current systems for sharing health data between patients and caregivers do not fully address the critical security requirements of privacy, confidentiality, and consent management.","Furthermore, compliance with regulatory laws such as GDPR and HIPAA is often deficient, largely because patients typically are asked to provide general consent for healthcare entities to access their data.","Recognizing the limitations of existing systems, we present S3PHER, a novel approach to sharing health data that provides patients with control over who accesses their data, what data is accessed, and when.","Our system ensures end to end privacy by integrating a Proxy ReEncryption Scheme with a Searchable Encryption Scheme, utilizing Homomorphic Encryption to enable healthcare practitioners to privately search and access patients' documents.","The practicality and benefits of S3PHER are further validated through end to end deployment and use case analyses, with tests on real datasets demonstrating promising execution times."],"url":"http://arxiv.org/abs/2404.11372v1"}
{"created":"2024-04-17 13:21:04","title":"Sinking an Algorithmic Isthmus: (1 + \u03b5)-Approximate Min-Sum Subset Convolution","abstract":"Given functions $f$ and $g$ defined on the subset lattice of order $n$, their min-sum subset convolution, defined for all $S \\subseteq [n]$ as \\[   (f \\star g)(S) = \\min_{T \\subseteq S}\\:\\big(f(T) + g(S \\setminus T)\\big), \\] is a fundamental tool in parameterized algorithms. However, since its na\\\"ive $O(3^n)$-time evaluation is also the fastest known, it has been used only in settings where the input functions have a bounded integer range $\\{-M, \\ldots, M\\}$. In this case, the running time becomes $\\tilde O(2^n M)$ by resorting to fast subset convolution in the sum-product ring. This is disadvantageous due to the dependence on $M$, limiting its practicality.   In this light, we study whether the problem admits an $(1 + \\varepsilon)$-approximation scheme in time independent of $M$. Our main result is the first $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$-time algorithm for the $(1 + \\varepsilon)$-approximate min-sum subset convolution. To show its applicability, we present $(1 + \\varepsilon)$-approximation schemes in the same exponential time bound for several NP-hard problems using this convolution, such as the minimum-cost $k$-coloring problem -- in time $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$, and the prize-collecting Steiner tree problem -- in time $\\tilde O(2^\\frac{3s^+}{2} / \\sqrt{\\varepsilon})$, where $n$ is the number of vertices and $s^+$ is the number of proper potential terminals. We also discuss two other applications in computational biology.   Our algorithms lie at the intersection of two lines of research that have been considered separately: $\\textit{sequence}$ and $\\textit{subset}$ convolutions in semi-rings. In particular, we extend the recent framework of Bringmann, K\\\"unnemann, and W\\k{e}grzycki [STOC 2019] to the context of subset convolutions.","sentences":["Given functions $f$ and $g$ defined on the subset lattice of order $n$, their min-sum subset convolution, defined for all $S \\subseteq","[n]$ as \\[   (f \\star g)(S) = \\min_{T \\subseteq S}\\:\\big(f(T) +","g(S \\setminus T)\\big), \\] is a fundamental tool in parameterized algorithms.","However, since its na\\\"ive $O(3^n)$-time evaluation is also the fastest known, it has been used only in settings where the input functions have a bounded integer range $\\{-M, \\ldots, M\\}$.","In this case, the running time becomes $\\tilde O(2^n M)$ by resorting to fast subset convolution in the sum-product ring.","This is disadvantageous due to the dependence on $M$, limiting its practicality.   ","In this light, we study whether the problem admits an $(1 + \\varepsilon)$-approximation scheme in time independent of $M$. Our main result is the first $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$-time algorithm for the $(1 + \\varepsilon)$-approximate min-sum subset convolution.","To show its applicability, we present $(1 + \\varepsilon)$-approximation schemes in the same exponential time bound for several NP-hard problems using this convolution, such as the minimum-cost $k$-coloring problem -- in time $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$, and the prize-collecting Steiner tree problem -- in time $\\tilde O(2^\\frac{3s^+}{2} / \\sqrt{\\varepsilon})$, where $n$ is the number of vertices and $s^+$ is the number of proper potential terminals.","We also discuss two other applications in computational biology.   ","Our algorithms lie at the intersection of two lines of research that have been considered separately: $\\textit{sequence}$ and $\\textit{subset}$ convolutions in semi-rings.","In particular, we extend the recent framework of Bringmann, K\\\"unnemann, and W\\k{e}grzycki","[STOC 2019] to the context of subset convolutions."],"url":"http://arxiv.org/abs/2404.11364v1"}
{"created":"2024-04-17 13:09:44","title":"Consisaug: A Consistency-based Augmentation for Polyp Detection in Endoscopy Image Analysis","abstract":"Colorectal cancer (CRC), which frequently originates from initially benign polyps, remains a significant contributor to global cancer-related mortality. Early and accurate detection of these polyps via colonoscopy is crucial for CRC prevention. However, traditional colonoscopy methods depend heavily on the operator's experience, leading to suboptimal polyp detection rates. Besides, the public database are limited in polyp size and shape diversity. To enhance the available data for polyp detection, we introduce Consisaug, an innovative and effective methodology to augment data that leverages deep learning. We utilize the constraint that when the image is flipped the class label should be equal and the bonding boxes should be consistent. We implement our Consisaug on five public polyp datasets and at three backbones, and the results show the effectiveness of our method.","sentences":["Colorectal cancer (CRC), which frequently originates from initially benign polyps, remains a significant contributor to global cancer-related mortality.","Early and accurate detection of these polyps via colonoscopy is crucial for CRC prevention.","However, traditional colonoscopy methods depend heavily on the operator's experience, leading to suboptimal polyp detection rates.","Besides, the public database are limited in polyp size and shape diversity.","To enhance the available data for polyp detection, we introduce Consisaug, an innovative and effective methodology to augment data that leverages deep learning.","We utilize the constraint that when the image is flipped the class label should be equal and the bonding boxes should be consistent.","We implement our Consisaug on five public polyp datasets and at three backbones, and the results show the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.11355v1"}
{"created":"2024-04-17 13:09:15","title":"Accelerating Geo-distributed Machine Learning with Network-Aware Adaptive Tree and Auxiliary Route","abstract":"Distributed machine learning is becoming increasingly popular for geo-distributed data analytics, facilitating the collaborative analysis of data scattered across data centers in different regions. This paradigm eliminates the need for centralizing sensitive raw data in one location but faces the significant challenge of high parameter synchronization delays, which stems from the constraints of bandwidth-limited, heterogeneous, and fluctuating wide-area networks. Prior research has focused on optimizing the synchronization topology, evolving from starlike to tree-based structures. However, these solutions typically depend on regular tree structures and lack an adequate topology metric, resulting in limited improvements. This paper proposes NetStorm, an adaptive and highly efficient communication scheduler designed to speed up parameter synchronization across geo-distributed data centers. First, it establishes an effective metric for optimizing a multi-root FAPT synchronization topology. Second, a network awareness module is developed to acquire network knowledge, aiding in topology decisions. Third, a multipath auxiliary transmission mechanism is introduced to enhance network awareness and facilitate multipath transmissions. Lastly, we design policy consistency protocols to guarantee seamless updates of transmission policies. Empirical results demonstrate that NetStorm significantly outperforms distributed training systems like MXNET, MLNET, and TSEngine, with a speedup of 6.5~9.2 times over MXNET.","sentences":["Distributed machine learning is becoming increasingly popular for geo-distributed data analytics, facilitating the collaborative analysis of data scattered across data centers in different regions.","This paradigm eliminates the need for centralizing sensitive raw data in one location but faces the significant challenge of high parameter synchronization delays, which stems from the constraints of bandwidth-limited, heterogeneous, and fluctuating wide-area networks.","Prior research has focused on optimizing the synchronization topology, evolving from starlike to tree-based structures.","However, these solutions typically depend on regular tree structures and lack an adequate topology metric, resulting in limited improvements.","This paper proposes NetStorm, an adaptive and highly efficient communication scheduler designed to speed up parameter synchronization across geo-distributed data centers.","First, it establishes an effective metric for optimizing a multi-root FAPT synchronization topology.","Second, a network awareness module is developed to acquire network knowledge, aiding in topology decisions.","Third, a multipath auxiliary transmission mechanism is introduced to enhance network awareness and facilitate multipath transmissions.","Lastly, we design policy consistency protocols to guarantee seamless updates of transmission policies.","Empirical results demonstrate that NetStorm significantly outperforms distributed training systems like MXNET, MLNET, and TSEngine, with a speedup of 6.5~9.2 times over MXNET."],"url":"http://arxiv.org/abs/2404.11352v1"}
{"created":"2024-04-17 13:07:56","title":"TeClass: A Human-Annotated Relevance-based Headline Classification and Generation Dataset for Telugu","abstract":"News headline generation is a crucial task in increasing productivity for both the readers and producers of news. This task can easily be aided by automated News headline-generation models. However, the presence of irrelevant headlines in scraped news articles results in sub-optimal performance of generation models. We propose that relevance-based headline classification can greatly aid the task of generating relevant headlines. Relevance-based headline classification involves categorizing news headlines based on their relevance to the corresponding news articles. While this task is well-established in English, it remains under-explored in low-resource languages like Telugu due to a lack of annotated data. To address this gap, we present TeClass, the first-ever human-annotated Telugu news headline classification dataset, containing 78,534 annotations across 26,178 article-headline pairs. We experiment with various baseline models and provide a comprehensive analysis of their results. We further demonstrate the impact of this work by fine-tuning various headline generation models using TeClass dataset. The headlines generated by the models fine-tuned on highly relevant article-headline pairs, showed about a 5 point increment in the ROUGE-L scores. To encourage future research, the annotated dataset as well as the annotation guidelines will be made publicly available.","sentences":["News headline generation is a crucial task in increasing productivity for both the readers and producers of news.","This task can easily be aided by automated News headline-generation models.","However, the presence of irrelevant headlines in scraped news articles results in sub-optimal performance of generation models.","We propose that relevance-based headline classification can greatly aid the task of generating relevant headlines.","Relevance-based headline classification involves categorizing news headlines based on their relevance to the corresponding news articles.","While this task is well-established in English, it remains under-explored in low-resource languages like Telugu due to a lack of annotated data.","To address this gap, we present TeClass, the first-ever human-annotated Telugu news headline classification dataset, containing 78,534 annotations across 26,178 article-headline pairs.","We experiment with various baseline models and provide a comprehensive analysis of their results.","We further demonstrate the impact of this work by fine-tuning various headline generation models using TeClass dataset.","The headlines generated by the models fine-tuned on highly relevant article-headline pairs, showed about a 5 point increment in the ROUGE-L scores.","To encourage future research, the annotated dataset as well as the annotation guidelines will be made publicly available."],"url":"http://arxiv.org/abs/2404.11349v1"}
{"created":"2024-04-17 13:00:52","title":"The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology","abstract":"In some fields of AI, machine learning and statistics, the validation of new methods and algorithms is often hindered by the scarcity of suitable real-world datasets. Researchers must often turn to simulated data, which yields limited information about the applicability of the proposed methods to real problems. As a step forward, we have constructed two devices that allow us to quickly and inexpensively produce large datasets from non-trivial but well-understood physical systems. The devices, which we call causal chambers, are computer-controlled laboratories that allow us to manipulate and measure an array of variables from these physical systems, providing a rich testbed for algorithms from a variety of fields. We illustrate potential applications through a series of case studies in fields such as causal discovery, out-of-distribution generalization, change point detection, independent component analysis, and symbolic regression. For applications to causal inference, the chambers allow us to carefully perform interventions. We also provide and empirically validate a causal model of each chamber, which can be used as ground truth for different tasks. All hardware and software is made open source, and the datasets are publicly available at causalchamber.org or through the Python package causalchamber.","sentences":["In some fields of AI, machine learning and statistics, the validation of new methods and algorithms is often hindered by the scarcity of suitable real-world datasets.","Researchers must often turn to simulated data, which yields limited information about the applicability of the proposed methods to real problems.","As a step forward, we have constructed two devices that allow us to quickly and inexpensively produce large datasets from non-trivial but well-understood physical systems.","The devices, which we call causal chambers, are computer-controlled laboratories that allow us to manipulate and measure an array of variables from these physical systems, providing a rich testbed for algorithms from a variety of fields.","We illustrate potential applications through a series of case studies in fields such as causal discovery, out-of-distribution generalization, change point detection, independent component analysis, and symbolic regression.","For applications to causal inference, the chambers allow us to carefully perform interventions.","We also provide and empirically validate a causal model of each chamber, which can be used as ground truth for different tasks.","All hardware and software is made open source, and the datasets are publicly available at causalchamber.org or through the Python package causalchamber."],"url":"http://arxiv.org/abs/2404.11341v1"}
{"created":"2024-04-17 12:41:01","title":"Channel Estimation in TDD Cell-free Scenario using OTFS Modulation","abstract":"Channel estimation techniques for orthogonal time frequency space (OTFS) modulation scheme are investigated. The orthogonal matching pursuit algorithm is investigated with and without side channel information and an efficient data placement is proposed alongside the pilot in the multi-user scenario based on impulse pilot-based estimation. Finally, the algorithms are compared in different multi-user scenarios with numerical results.","sentences":["Channel estimation techniques for orthogonal time frequency space (OTFS) modulation scheme are investigated.","The orthogonal matching pursuit algorithm is investigated with and without side channel information and an efficient data placement is proposed alongside the pilot in the multi-user scenario based on impulse pilot-based estimation.","Finally, the algorithms are compared in different multi-user scenarios with numerical results."],"url":"http://arxiv.org/abs/2404.11328v1"}
{"created":"2024-04-17 12:38:58","title":"Single-temporal Supervised Remote Change Detection for Domain Generalization","abstract":"Change detection is widely applied in remote sensing image analysis. Existing methods require training models separately for each dataset, which leads to poor domain generalization. Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical. In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization. Additionally, we propose a dynamic context optimization for prompt learning. Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN). This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization. Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods. Code will be available.","sentences":["Change detection is widely applied in remote sensing image analysis.","Existing methods require training models separately for each dataset, which leads to poor domain generalization.","Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical.","In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization.","Additionally, we propose a dynamic context optimization for prompt learning.","Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN).","This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization.","Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods.","Code will be available."],"url":"http://arxiv.org/abs/2404.11326v1"}
{"created":"2024-04-17 12:36:20","title":"On Learning Parities with Dependent Noise","abstract":"In this expository note we show that the learning parities with noise (LPN) assumption is robust to weak dependencies in the noise distribution of small batches of samples. This provides a partial converse to the linearization technique of [AG11]. The material in this note is drawn from a recent work by the authors [GMR24], where the robustness guarantee was a key component in a cryptographic separation between reinforcement learning and supervised learning.","sentences":["In this expository note we show that the learning parities with noise (LPN) assumption is robust to weak dependencies in the noise distribution of small batches of samples.","This provides a partial converse to the linearization technique of [AG11].","The material in this note is drawn from a recent work by the authors [GMR24], where the robustness guarantee was a key component in a cryptographic separation between reinforcement learning and supervised learning."],"url":"http://arxiv.org/abs/2404.11325v1"}
{"created":"2024-04-17 12:34:49","title":"VBR: A Vision Benchmark in Rome","abstract":"This paper presents a vision and perception research dataset collected in Rome, featuring RGB data, 3D point clouds, IMU, and GPS data. We introduce a new benchmark targeting visual odometry and SLAM, to advance the research in autonomous robotics and computer vision. This work complements existing datasets by simultaneously addressing several issues, such as environment diversity, motion patterns, and sensor frequency. It uses up-to-date devices and presents effective procedures to accurately calibrate the intrinsic and extrinsic of the sensors while addressing temporal synchronization. During recording, we cover multi-floor buildings, gardens, urban and highway scenarios. Combining handheld and car-based data collections, our setup can simulate any robot (quadrupeds, quadrotors, autonomous vehicles). The dataset includes an accurate 6-dof ground truth based on a novel methodology that refines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment. All sequences divided in training and testing are accessible through our website.","sentences":["This paper presents a vision and perception research dataset collected in Rome, featuring RGB data, 3D point clouds, IMU, and GPS data.","We introduce a new benchmark targeting visual odometry and SLAM, to advance the research in autonomous robotics and computer vision.","This work complements existing datasets by simultaneously addressing several issues, such as environment diversity, motion patterns, and sensor frequency.","It uses up-to-date devices and presents effective procedures to accurately calibrate the intrinsic and extrinsic of the sensors while addressing temporal synchronization.","During recording, we cover multi-floor buildings, gardens, urban and highway scenarios.","Combining handheld and car-based data collections, our setup can simulate any robot (quadrupeds, quadrotors, autonomous vehicles).","The dataset includes an accurate 6-dof ground truth based on a novel methodology that refines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment.","All sequences divided in training and testing are accessible through our website."],"url":"http://arxiv.org/abs/2404.11322v1"}
{"created":"2024-04-17 12:32:10","title":"Leveraging Fine-Grained Information and Noise Decoupling for Remote Sensing Change Detection","abstract":"Change detection aims to identify remote sense object changes by analyzing data between bitemporal image pairs. Due to the large temporal and spatial span of data collection in change detection image pairs, there are often a significant amount of task-specific and task-agnostic noise. Previous effort has focused excessively on denoising, with this goes a great deal of loss of fine-grained information. In this paper, we revisit the importance of fine-grained features in change detection and propose a series of operations for fine-grained information compensation and noise decoupling (FINO). First, the context is utilized to compensate for the fine-grained information in the feature space. Next, a shape-aware and a brightness-aware module are designed to improve the capacity for representation learning. The shape-aware module guides the backbone for more precise shape estimation, guiding the backbone network in extracting object shape features. The brightness-aware module learns a overall brightness estimation to improve the model's robustness to task-agnostic noise. Finally, a task-specific noise decoupling structure is designed as a way to improve the model's ability to separate noise interference from feature similarity. With these training schemes, our proposed method achieves new state-of-the-art (SOTA) results in multiple change detection benchmarks. The code will be made available.","sentences":["Change detection aims to identify remote sense object changes by analyzing data between bitemporal image pairs.","Due to the large temporal and spatial span of data collection in change detection image pairs, there are often a significant amount of task-specific and task-agnostic noise.","Previous effort has focused excessively on denoising, with this goes a great deal of loss of fine-grained information.","In this paper, we revisit the importance of fine-grained features in change detection and propose a series of operations for fine-grained information compensation and noise decoupling (FINO).","First, the context is utilized to compensate for the fine-grained information in the feature space.","Next, a shape-aware and a brightness-aware module are designed to improve the capacity for representation learning.","The shape-aware module guides the backbone for more precise shape estimation, guiding the backbone network in extracting object shape features.","The brightness-aware module learns a overall brightness estimation to improve the model's robustness to task-agnostic noise.","Finally, a task-specific noise decoupling structure is designed as a way to improve the model's ability to separate noise interference from feature similarity.","With these training schemes, our proposed method achieves new state-of-the-art (SOTA) results in multiple change detection benchmarks.","The code will be made available."],"url":"http://arxiv.org/abs/2404.11318v1"}
{"created":"2024-04-17 12:30:54","title":"Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives","abstract":"The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text. Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples. However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples. Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model. To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR. To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly. The above two improvements can be effectively stacked and designed to be plug-and-play, easily applied to existing CIR models without changing their original architectures. Extensive experiments and ablation analysis demonstrate that our method effectively scales positives and negatives and achieves state-of-the-art results on both FashionIQ and CIRR datasets. In addition, our methods also perform well in zero-shot composed image retrieval, providing a new CIR solution for the low-resources scenario.","sentences":["The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text.","Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples.","However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples.","Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model.","To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR.","To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly.","The above two improvements can be effectively stacked and designed to be plug-and-play, easily applied to existing CIR models without changing their original architectures.","Extensive experiments and ablation analysis demonstrate that our method effectively scales positives and negatives and achieves state-of-the-art results on both FashionIQ and CIRR datasets.","In addition, our methods also perform well in zero-shot composed image retrieval, providing a new CIR solution for the low-resources scenario."],"url":"http://arxiv.org/abs/2404.11317v1"}
{"created":"2024-04-17 12:26:52","title":"To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case Study in Japanese","abstract":"Speakers sometimes omit certain arguments of a predicate in a sentence; such omission is especially frequent in pro-drop languages. This study addresses a question about ellipsis -- what can explain the native speakers' ellipsis decisions? -- motivated by the interest in human discourse processing and writing assistance for this choice. To this end, we first collect large-scale human annotations of whether and why a particular argument should be omitted across over 2,000 data points in the balanced corpus of Japanese, a prototypical pro-drop language. The data indicate that native speakers overall share common criteria for such judgments and further clarify their quantitative characteristics, e.g., the distribution of related linguistic factors in the balanced corpus. Furthermore, the performance of the language model-based argument ellipsis judgment model is examined, and the gap between the systems' prediction and human judgments in specific linguistic aspects is revealed. We hope our fundamental resource encourages further studies on natural human ellipsis judgment.","sentences":["Speakers sometimes omit certain arguments of a predicate in a sentence; such omission is especially frequent in pro-drop languages.","This study addresses a question about ellipsis -- what can explain the native speakers' ellipsis decisions?","-- motivated by the interest in human discourse processing and writing assistance for this choice.","To this end, we first collect large-scale human annotations of whether and why a particular argument should be omitted across over 2,000 data points in the balanced corpus of Japanese, a prototypical pro-drop language.","The data indicate that native speakers overall share common criteria for such judgments and further clarify their quantitative characteristics, e.g., the distribution of related linguistic factors in the balanced corpus.","Furthermore, the performance of the language model-based argument ellipsis judgment model is examined, and the gap between the systems' prediction and human judgments in specific linguistic aspects is revealed.","We hope our fundamental resource encourages further studies on natural human ellipsis judgment."],"url":"http://arxiv.org/abs/2404.11315v1"}
{"created":"2024-04-17 12:21:57","title":"Achieving Rotation Invariance in Convolution Operations: Shifting from Data-Driven to Mechanism-Assured","abstract":"Achieving rotation invariance in deep neural networks without relying on data has always been a hot research topic. Intrinsic rotation invariance can enhance the model's feature representation capability, enabling better performance in tasks such as multi-orientation object recognition and detection. Based on various types of non-learnable operators, including gradient, sort, local binary pattern, maximum, etc., this paper designs a set of new convolution operations that are natually invariant to arbitrary rotations. Unlike most previous studies, these rotation-invariant convolutions (RIConvs) have the same number of learnable parameters and a similar computational process as conventional convolution operations, allowing them to be interchangeable. Using the MNIST-Rot dataset, we first verify the invariance of these RIConvs under various rotation angles and compare their performance with previous rotation-invariant convolutional neural networks (RI-CNNs). Two types of RIConvs based on gradient operators achieve state-of-the-art results. Subsequently, we combine RIConvs with different types and depths of classic CNN backbones. Using the OuTex_00012, MTARSI, and NWPU-RESISC-45 datasets, we test their performance on texture recognition, aircraft type recognition, and remote sensing image classification tasks. The results show that RIConvs significantly improve the accuracy of these CNN backbones, especially when the training data is limited. Furthermore, we find that even with data augmentation, RIConvs can further enhance model performance.","sentences":["Achieving rotation invariance in deep neural networks without relying on data has always been a hot research topic.","Intrinsic rotation invariance can enhance the model's feature representation capability, enabling better performance in tasks such as multi-orientation object recognition and detection.","Based on various types of non-learnable operators, including gradient, sort, local binary pattern, maximum, etc., this paper designs a set of new convolution operations that are natually invariant to arbitrary rotations.","Unlike most previous studies, these rotation-invariant convolutions (RIConvs) have the same number of learnable parameters and a similar computational process as conventional convolution operations, allowing them to be interchangeable.","Using the MNIST-Rot dataset, we first verify the invariance of these RIConvs under various rotation angles and compare their performance with previous rotation-invariant convolutional neural networks (RI-CNNs).","Two types of RIConvs based on gradient operators achieve state-of-the-art results.","Subsequently, we combine RIConvs with different types and depths of classic CNN backbones.","Using the OuTex_00012, MTARSI, and NWPU-RESISC-45 datasets, we test their performance on texture recognition, aircraft type recognition, and remote sensing image classification tasks.","The results show that RIConvs significantly improve the accuracy of these CNN backbones, especially when the training data is limited.","Furthermore, we find that even with data augmentation, RIConvs can further enhance model performance."],"url":"http://arxiv.org/abs/2404.11309v1"}
{"created":"2024-04-17 12:19:33","title":"Undo and Redo Support for Replicated Registers","abstract":"Undo and redo functionality is ubiquitous in collaboration software. In single user settings, undo and redo are well understood. However, when multiple users edit a document, concurrency may arise, leading to a non-linear operation history. This renders undo and redo more complex both in terms of their semantics and implementation. We survey the undo and redo semantics of current mainstream collaboration software and derive principles for undo and redo behavior in a collaborative setting. We then apply these principles to a simple CRDT, the Multi-Valued Replicated Register, and present a novel undo and redo algorithm that implements the undo and redo semantics that we believe are most consistent with users' expectations.","sentences":["Undo and redo functionality is ubiquitous in collaboration software.","In single user settings, undo and redo are well understood.","However, when multiple users edit a document, concurrency may arise, leading to a non-linear operation history.","This renders undo and redo more complex both in terms of their semantics and implementation.","We survey the undo and redo semantics of current mainstream collaboration software and derive principles for undo and redo behavior in a collaborative setting.","We then apply these principles to a simple CRDT, the Multi-Valued Replicated Register, and present a novel undo and redo algorithm that implements the undo and redo semantics that we believe are most consistent with users' expectations."],"url":"http://arxiv.org/abs/2404.11308v1"}
{"created":"2024-04-17 12:13:18","title":"A Semantic Segmentation-guided Approach for Ground-to-Aerial Image Matching","abstract":"Nowadays the accurate geo-localization of ground-view images has an important role across domains as diverse as journalism, forensics analysis, transports, and Earth Observation. This work addresses the problem of matching a query ground-view image with the corresponding satellite image without GPS data. This is done by comparing the features from a ground-view image and a satellite one, innovatively leveraging the corresponding latter's segmentation mask through a three-stream Siamese-like network. The proposed method, Semantic Align Net (SAN), focuses on limited Field-of-View (FoV) and ground panorama images (images with a FoV of 360{\\deg}). The novelty lies in the fusion of satellite images in combination with their semantic segmentation masks, aimed at ensuring that the model can extract useful features and focus on the significant parts of the images. This work shows how SAN through semantic analysis of images improves the performance on the unlabelled CVUSA dataset for all the tested FoVs.","sentences":["Nowadays the accurate geo-localization of ground-view images has an important role across domains as diverse as journalism, forensics analysis, transports, and Earth Observation.","This work addresses the problem of matching a query ground-view image with the corresponding satellite image without GPS data.","This is done by comparing the features from a ground-view image and a satellite one, innovatively leveraging the corresponding latter's segmentation mask through a three-stream Siamese-like network.","The proposed method, Semantic Align Net (SAN), focuses on limited Field-of-View (FoV) and ground panorama images (images with a FoV of 360{\\deg}).","The novelty lies in the fusion of satellite images in combination with their semantic segmentation masks, aimed at ensuring that the model can extract useful features and focus on the significant parts of the images.","This work shows how SAN through semantic analysis of images improves the performance on the unlabelled CVUSA dataset for all the tested FoVs."],"url":"http://arxiv.org/abs/2404.11302v1"}
{"created":"2024-04-17 12:12:48","title":"Learning from Unlabelled Data with Transformers: Domain Adaptation for Semantic Segmentation of High Resolution Aerial Images","abstract":"Data from satellites or aerial vehicles are most of the times unlabelled. Annotating such data accurately is difficult, requires expertise, and is costly in terms of time. Even if Earth Observation (EO) data were correctly labelled, labels might change over time. Learning from unlabelled data within a semi-supervised learning framework for segmentation of aerial images is challenging. In this paper, we develop a new model for semantic segmentation of unlabelled images, the Non-annotated Earth Observation Semantic Segmentation (NEOS) model. NEOS performs domain adaptation as the target domain does not have ground truth semantic segmentation masks. The distribution inconsistencies between the target and source domains are due to differences in acquisition scenes, environment conditions, sensors, and times. Our model aligns the learned representations of the different domains to make them coincide. The evaluation results show that NEOS is successful and outperforms other models for semantic segmentation of unlabelled data.","sentences":["Data from satellites or aerial vehicles are most of the times unlabelled.","Annotating such data accurately is difficult, requires expertise, and is costly in terms of time.","Even if Earth Observation (EO) data were correctly labelled, labels might change over time.","Learning from unlabelled data within a semi-supervised learning framework for segmentation of aerial images is challenging.","In this paper, we develop a new model for semantic segmentation of unlabelled images, the Non-annotated Earth Observation Semantic Segmentation (NEOS) model.","NEOS performs domain adaptation as the target domain does not have ground truth semantic segmentation masks.","The distribution inconsistencies between the target and source domains are due to differences in acquisition scenes, environment conditions, sensors, and times.","Our model aligns the learned representations of the different domains to make them coincide.","The evaluation results show that NEOS is successful and outperforms other models for semantic segmentation of unlabelled data."],"url":"http://arxiv.org/abs/2404.11299v1"}
{"created":"2024-04-17 12:00:09","title":"LogSD: Detecting Anomalies from System Logs through Self-supervised Learning and Frequency-based Masking","abstract":"Log analysis is one of the main techniques that engineers use for troubleshooting large-scale software systems. Over the years, many supervised, semi-supervised, and unsupervised log analysis methods have been proposed to detect system anomalies by analyzing system logs. Among these, semi-supervised methods have garnered increasing attention as they strike a balance between relaxed labeled data requirements and optimal detection performance, contrasting with their supervised and unsupervised counterparts. However, existing semi-supervised methods overlook the potential bias introduced by highly frequent log messages on the learned normal patterns, which leads to their less than satisfactory performance. In this study, we propose LogSD, a novel semi-supervised self-supervised learning approach. LogSD employs a dual-network architecture and incorporates a frequency-based masking scheme, a global-to-local reconstruction paradigm and three self-supervised learning tasks. These features enable LogSD to focus more on relatively infrequent log messages, thereby effectively learning less biased and more discriminative patterns from historical normal data. This emphasis ultimately leads to improved anomaly detection performance. Extensive experiments have been conducted on three commonly-used datasets and the results show that LogSD significantly outperforms eight state-of-the-art benchmark methods.","sentences":["Log analysis is one of the main techniques that engineers use for troubleshooting large-scale software systems.","Over the years, many supervised, semi-supervised, and unsupervised log analysis methods have been proposed to detect system anomalies by analyzing system logs.","Among these, semi-supervised methods have garnered increasing attention as they strike a balance between relaxed labeled data requirements and optimal detection performance, contrasting with their supervised and unsupervised counterparts.","However, existing semi-supervised methods overlook the potential bias introduced by highly frequent log messages on the learned normal patterns, which leads to their less than satisfactory performance.","In this study, we propose LogSD, a novel semi-supervised self-supervised learning approach.","LogSD employs a dual-network architecture and incorporates a frequency-based masking scheme, a global-to-local reconstruction paradigm and three self-supervised learning tasks.","These features enable LogSD to focus more on relatively infrequent log messages, thereby effectively learning less biased and more discriminative patterns from historical normal data.","This emphasis ultimately leads to improved anomaly detection performance.","Extensive experiments have been conducted on three commonly-used datasets and the results show that LogSD significantly outperforms eight state-of-the-art benchmark methods."],"url":"http://arxiv.org/abs/2404.11294v1"}
{"created":"2024-04-17 11:52:47","title":"A Preference-driven Paradigm for Enhanced Translation with Large Language Models","abstract":"Recent research has shown that large language models (LLMs) can achieve remarkable translation performance through supervised fine-tuning (SFT) using only a small amount of parallel data. However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references. Hence, the assistance from SFT often reaches a plateau once the LLMs have achieved a certain level of translation capability, and further increasing the size of parallel data does not provide additional benefits. To overcome this plateau associated with imitation-based SFT, we propose a preference-based approach built upon the Plackett-Luce model. The objective is to steer LLMs towards a more nuanced understanding of translation preferences from a holistic view, while also being more resilient in the absence of gold translations. We further build a dataset named MAPLE to verify the effectiveness of our approach, which includes multiple translations of varying quality for each source sentence. Extensive experiments demonstrate the superiority of our approach in \"breaking the plateau\" across diverse LLMs and test settings. Our in-depth analysis underscores the pivotal role of diverse translations and accurate preference scores in the success of our approach.","sentences":["Recent research has shown that large language models (LLMs) can achieve remarkable translation performance through supervised fine-tuning (SFT) using only a small amount of parallel data.","However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references.","Hence, the assistance from SFT often reaches a plateau once the LLMs have achieved a certain level of translation capability, and further increasing the size of parallel data does not provide additional benefits.","To overcome this plateau associated with imitation-based SFT, we propose a preference-based approach built upon the Plackett-Luce model.","The objective is to steer LLMs towards a more nuanced understanding of translation preferences from a holistic view, while also being more resilient in the absence of gold translations.","We further build a dataset named MAPLE to verify the effectiveness of our approach, which includes multiple translations of varying quality for each source sentence.","Extensive experiments demonstrate the superiority of our approach in \"breaking the plateau\" across diverse LLMs and test settings.","Our in-depth analysis underscores the pivotal role of diverse translations and accurate preference scores in the success of our approach."],"url":"http://arxiv.org/abs/2404.11288v1"}
{"created":"2024-04-17 11:49:43","title":"Novel View Synthesis for Cinematic Anatomy on Mobile and Immersive Displays","abstract":"Interactive photorealistic visualization of 3D anatomy (i.e., Cinematic Anatomy) is used in medical education to explain the structure of the human body. It is currently restricted to frontal teaching scenarios, where the demonstrator needs a powerful GPU and high-speed access to a large storage device where the dataset is hosted. We demonstrate the use of novel view synthesis via compressed 3D Gaussian splatting to overcome this restriction and to enable students to perform cinematic anatomy on lightweight mobile devices and in virtual reality environments. We present an automatic approach for finding a set of images that captures all potentially seen structures in the data. By mixing closeup views with images from a distance, the splat representation can recover structures up to the voxel resolution. The use of Mip-Splatting enables smooth transitions when the focal length is increased. Even for GB datasets, the final renderable representation can usually be compressed to less than 70 MB, enabling interactive rendering on low-end devices using rasterization.","sentences":["Interactive photorealistic visualization of 3D anatomy (i.e., Cinematic Anatomy) is used in medical education to explain the structure of the human body.","It is currently restricted to frontal teaching scenarios, where the demonstrator needs a powerful GPU and high-speed access to a large storage device where the dataset is hosted.","We demonstrate the use of novel view synthesis via compressed 3D Gaussian splatting to overcome this restriction and to enable students to perform cinematic anatomy on lightweight mobile devices and in virtual reality environments.","We present an automatic approach for finding a set of images that captures all potentially seen structures in the data.","By mixing closeup views with images from a distance, the splat representation can recover structures up to the voxel resolution.","The use of Mip-Splatting enables smooth transitions when the focal length is increased.","Even for GB datasets, the final renderable representation can usually be compressed to less than 70 MB, enabling interactive rendering on low-end devices using rasterization."],"url":"http://arxiv.org/abs/2404.11285v1"}
{"created":"2024-04-17 11:48:14","title":"Amplifying Main Memory-Based Timing Covert and Side Channels using Processing-in-Memory Operations","abstract":"The adoption of processing-in-memory (PiM) architectures has been gaining momentum because they provide high performance and low energy consumption by alleviating the data movement bottleneck. Yet, the security of such architectures has not been thoroughly explored. The adoption of PiM solutions provides a new way to directly access main memory, which can be potentially exploited by malicious user applications. We show that this new way to access main memory opens opportunities for high-throughput timing attack vectors that are hard-to-mitigate without significant performance overhead.   We introduce IMPACT, a set of high-throughput main memory-based timing attacks that leverage characteristics of PiM architectures to establish covert and side channels. IMPACT enables high-throughput communication and private information leakage. To achieve this, IMPACT (i) eliminates expensive cache bypassing steps required by processor-centric main memory and cache-based timing attacks and (ii) leverages the intrinsic parallelism of PiM operations. First, we showcase two covert-channel attack variants that run on the host CPU and leverage PiM architectures to gain direct and fast access to main memory and establish high-throughput communication covert channels. Second, we showcase a side-channel attack on a DNA sequence analysis application that leaks the private characteristics of a user's sample genome by leveraging PiM operations. Our results demonstrate that (i) our covert channels achieve up to 14.16 Mb/s communication throughput, which is 6.38x faster than the state-of-the-art main memory-based covert channels, and (ii) our side-channel attack allows the attacker to determine the properties of a sample genome at a throughput of 7.5 Mb/s with 96% accuracy. We discuss and evaluate several countermeasures for IMPACT to enable secure and robust PiM architectures.","sentences":["The adoption of processing-in-memory (PiM) architectures has been gaining momentum because they provide high performance and low energy consumption by alleviating the data movement bottleneck.","Yet, the security of such architectures has not been thoroughly explored.","The adoption of PiM solutions provides a new way to directly access main memory, which can be potentially exploited by malicious user applications.","We show that this new way to access main memory opens opportunities for high-throughput timing attack vectors that are hard-to-mitigate without significant performance overhead.   ","We introduce IMPACT, a set of high-throughput main memory-based timing attacks that leverage characteristics of PiM architectures to establish covert and side channels.","IMPACT enables high-throughput communication and private information leakage.","To achieve this, IMPACT (i) eliminates expensive cache bypassing steps required by processor-centric main memory and cache-based timing attacks and (ii) leverages the intrinsic parallelism of PiM operations.","First, we showcase two covert-channel attack variants that run on the host CPU and leverage PiM architectures to gain direct and fast access to main memory and establish high-throughput communication covert channels.","Second, we showcase a side-channel attack on a DNA sequence analysis application that leaks the private characteristics of a user's sample genome by leveraging PiM operations.","Our results demonstrate that (i) our covert channels achieve up to 14.16 Mb/s communication throughput, which is 6.38x faster than the state-of-the-art main memory-based covert channels, and (ii) our side-channel attack allows the attacker to determine the properties of a sample genome at a throughput of 7.5 Mb/s with 96% accuracy.","We discuss and evaluate several countermeasures for IMPACT to enable secure and robust PiM architectures."],"url":"http://arxiv.org/abs/2404.11284v1"}
{"created":"2024-04-17 11:42:39","title":"Image Generative Semantic Communication with Multi-Modal Similarity Estimation for Resource-Limited Networks","abstract":"To reduce network traffic and support environments with limited resources, a method for transmitting images with low amounts of transmission data is required. Machine learning-based image compression methods, which compress the data size of images while maintaining their features, have been proposed. However, in certain situations, reconstructing a part of semantic information of images at the receiver end may be sufficient. To realize this concept, semantic-information-based communication, called semantic communication, has been proposed, along with an image transmission method using semantic communication. This method transmits only the semantic information of an image, and the receiver reconstructs the image using an image-generation model. This method utilizes one type of semantic information, but reconstructing images similar to the original image using only it is challenging. This study proposes a multi-modal image transmission method that leverages diverse semantic information for efficient semantic communication. The proposed method extracts multi-modal semantic information from an image and transmits only it. Subsequently, the receiver generates multiple images using an image-generation model and selects an output based on semantic similarity. The receiver must select the output based only on the received features; however, evaluating semantic similarity using conventional metrics is challenging. Therefore, this study explored new metrics to evaluate the similarity between semantic features of images and proposes two scoring procedures. The results indicate that the proposed procedures can compare semantic similarities, such as position and composition, between semantic features of the original and generated images. Thus, the proposed method can facilitate the transmission and utilization of photographs through mobile networks for various service applications.","sentences":["To reduce network traffic and support environments with limited resources, a method for transmitting images with low amounts of transmission data is required.","Machine learning-based image compression methods, which compress the data size of images while maintaining their features, have been proposed.","However, in certain situations, reconstructing a part of semantic information of images at the receiver end may be sufficient.","To realize this concept, semantic-information-based communication, called semantic communication, has been proposed, along with an image transmission method using semantic communication.","This method transmits only the semantic information of an image, and the receiver reconstructs the image using an image-generation model.","This method utilizes one type of semantic information, but reconstructing images similar to the original image using only it is challenging.","This study proposes a multi-modal image transmission method that leverages diverse semantic information for efficient semantic communication.","The proposed method extracts multi-modal semantic information from an image and transmits only it.","Subsequently, the receiver generates multiple images using an image-generation model and selects an output based on semantic similarity.","The receiver must select the output based only on the received features; however, evaluating semantic similarity using conventional metrics is challenging.","Therefore, this study explored new metrics to evaluate the similarity between semantic features of images and proposes two scoring procedures.","The results indicate that the proposed procedures can compare semantic similarities, such as position and composition, between semantic features of the original and generated images.","Thus, the proposed method can facilitate the transmission and utilization of photographs through mobile networks for various service applications."],"url":"http://arxiv.org/abs/2404.11280v1"}
{"created":"2024-04-17 11:33:21","title":"RD2Bench: Toward Data-Centric Automatic R&D","abstract":"The progress of humanity is driven by those successful discoveries accompanied by countless failed experiments. Researchers often seek the potential research directions by reading and then verifying them through experiments. The process imposes a significant burden on researchers. In the past decade, the data-driven black-box deep learning method demonstrates its effectiveness in a wide range of real-world scenarios, which exacerbates the experimental burden of researchers and thus renders the potential successful discoveries veiled. Therefore, automating such a research and development (R&D) process is an urgent need. In this paper, we serve as the first effort to formalize the goal by proposing a Real-world Data-centric automatic R&D Benchmark, namely RD2Bench. RD2Bench benchmarks all the operations in data-centric automatic R&D (D-CARD) as a whole to navigate future work toward our goal directly. We focuses on evaluating the interaction and synergistic effects of various model capabilities and aiding to select the well-performed trustworthy models. Although RD2Bench is very challenging to the state-of-the-art (SOTA) large language model (LLM) named GPT-4, indicating ample research opportunities and more research efforts, LLMs possess promising potential to bring more significant development to D-CARD: They are able to implement some simple methods without adopting any additional techniques. We appeal to future work to take developing techniques for tackling automatic R&D into consideration, thus bringing the opportunities of the potential revolutionary upgrade to human productivity.","sentences":["The progress of humanity is driven by those successful discoveries accompanied by countless failed experiments.","Researchers often seek the potential research directions by reading and then verifying them through experiments.","The process imposes a significant burden on researchers.","In the past decade, the data-driven black-box deep learning method demonstrates its effectiveness in a wide range of real-world scenarios, which exacerbates the experimental burden of researchers and thus renders the potential successful discoveries veiled.","Therefore, automating such a research and development (R&D) process is an urgent need.","In this paper, we serve as the first effort to formalize the goal by proposing a Real-world Data-centric automatic R&D Benchmark, namely RD2Bench.","RD2Bench benchmarks all the operations in data-centric automatic R&D (D-CARD) as a whole to navigate future work toward our goal directly.","We focuses on evaluating the interaction and synergistic effects of various model capabilities and aiding to select the well-performed trustworthy models.","Although RD2Bench is very challenging to the state-of-the-art (SOTA) large language model (LLM) named GPT-4, indicating ample research opportunities and more research efforts, LLMs possess promising potential to bring more significant development to D-CARD: They are able to implement some simple methods without adopting any additional techniques.","We appeal to future work to take developing techniques for tackling automatic R&D into consideration, thus bringing the opportunities of the potential revolutionary upgrade to human productivity."],"url":"http://arxiv.org/abs/2404.11276v1"}
{"created":"2024-04-17 11:20:14","title":"DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series","abstract":"Time series anomaly detection (TAD) faces a significant challenge due to the scarcity of labelled data, which hinders the development of accurate detection models. Unsupervised domain adaptation (UDA) addresses this challenge by leveraging a labelled dataset from a related domain to detect anomalies in a target dataset. Existing domain adaptation techniques assume that the number of anomalous classes does not change between the source and target domains. In this paper, we propose a novel Domain Adaptation Contrastive learning for Anomaly Detection in multivariate time series (DACAD) model to address this issue by combining UDA and contrastive representation learning. DACAD's approach includes an anomaly injection mechanism that introduces various types of synthetic anomalies, enhancing the model's ability to generalise across unseen anomalous classes in different domains. This method significantly broadens the model's adaptability and robustness. Additionally, we propose a supervised contrastive loss for the source domain and a self-supervised contrastive triplet loss for the target domain, improving comprehensive feature representation learning and extraction of domain-invariant features. Finally, an effective Centre-based Entropy Classifier (CEC) is proposed specifically for anomaly detection, facilitating accurate learning of normal boundaries in the source domain. Our extensive evaluation across multiple real-world datasets against leading models in time series anomaly detection and UDA underscores DACAD's effectiveness. The results validate DACAD's superiority in transferring knowledge across domains and its potential to mitigate the challenge of limited labelled data in time series anomaly detection.","sentences":["Time series anomaly detection (TAD) faces a significant challenge due to the scarcity of labelled data, which hinders the development of accurate detection models.","Unsupervised domain adaptation (UDA) addresses this challenge by leveraging a labelled dataset from a related domain to detect anomalies in a target dataset.","Existing domain adaptation techniques assume that the number of anomalous classes does not change between the source and target domains.","In this paper, we propose a novel Domain Adaptation Contrastive learning for Anomaly Detection in multivariate time series (DACAD) model to address this issue by combining UDA and contrastive representation learning.","DACAD's approach includes an anomaly injection mechanism that introduces various types of synthetic anomalies, enhancing the model's ability to generalise across unseen anomalous classes in different domains.","This method significantly broadens the model's adaptability and robustness.","Additionally, we propose a supervised contrastive loss for the source domain and a self-supervised contrastive triplet loss for the target domain, improving comprehensive feature representation learning and extraction of domain-invariant features.","Finally, an effective Centre-based Entropy Classifier (CEC) is proposed specifically for anomaly detection, facilitating accurate learning of normal boundaries in the source domain.","Our extensive evaluation across multiple real-world datasets against leading models in time series anomaly detection and UDA underscores DACAD's effectiveness.","The results validate DACAD's superiority in transferring knowledge across domains and its potential to mitigate the challenge of limited labelled data in time series anomaly detection."],"url":"http://arxiv.org/abs/2404.11269v1"}
{"created":"2024-04-17 11:17:12","title":"Criteria for Uncertainty-based Corner Cases Detection in Instance Segmentation","abstract":"The operating environment of a highly automated vehicle is subject to change, e.g., weather, illumination, or the scenario containing different objects and other participants in which the highly automated vehicle has to navigate its passengers safely. These situations must be considered when developing and validating highly automated driving functions. This already poses a problem for training and evaluating deep learning models because without the costly labeling of thousands of recordings, not knowing whether the data contains relevant, interesting data for further model training, it is a guess under which conditions and situations the model performs poorly. For this purpose, we present corner case criteria based on the predictive uncertainty. With our corner case criteria, we are able to detect uncertainty-based corner cases of an object instance segmentation model without relying on ground truth (GT) data. We evaluated each corner case criterion using the COCO and the NuImages dataset to analyze the potential of our approach. We also provide a corner case decision function that allows us to distinguish each object into True Positive (TP), localization and/or classification corner case, or False Positive (FP). We also present our first results of an iterative training cycle that outperforms the baseline and where the data added to the training dataset is selected based on the corner case decision function.","sentences":["The operating environment of a highly automated vehicle is subject to change, e.g., weather, illumination, or the scenario containing different objects and other participants in which the highly automated vehicle has to navigate its passengers safely.","These situations must be considered when developing and validating highly automated driving functions.","This already poses a problem for training and evaluating deep learning models because without the costly labeling of thousands of recordings, not knowing whether the data contains relevant, interesting data for further model training, it is a guess under which conditions and situations the model performs poorly.","For this purpose, we present corner case criteria based on the predictive uncertainty.","With our corner case criteria, we are able to detect uncertainty-based corner cases of an object instance segmentation model without relying on ground truth (GT) data.","We evaluated each corner case criterion using the COCO and the NuImages dataset to analyze the potential of our approach.","We also provide a corner case decision function that allows us to distinguish each object into True Positive (TP), localization and/or classification corner case, or False Positive (FP).","We also present our first results of an iterative training cycle that outperforms the baseline and where the data added to the training dataset is selected based on the corner case decision function."],"url":"http://arxiv.org/abs/2404.11266v1"}
{"created":"2024-04-17 11:15:58","title":"The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data","abstract":"Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs). The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present. However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios. In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy. This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&B), which exploits a poisoned model to train a clean model without extra benign samples. Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples. Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection. Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance. Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&B framework. Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB.","sentences":["Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs).","The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present.","However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios.","In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy.","This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&B), which exploits a poisoned model to train a clean model without extra benign samples.","Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples.","Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection.","Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance.","Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&B framework.","Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples.","Our code is available at https://github.com/Zixuan-Zhu/VaB."],"url":"http://arxiv.org/abs/2404.11265v1"}
{"created":"2024-04-17 11:12:59","title":"Sampling-based Pseudo-Likelihood for Membership Inference Attacks","abstract":"Large Language Models (LLMs) are trained on large-scale web data, which makes it difficult to grasp the contribution of each text. This poses the risk of leaking inappropriate data such as benchmarks, personal information, and copyrighted texts in the training data. Membership Inference Attacks (MIA), which determine whether a given text is included in the model's training data, have been attracting attention. Previous studies of MIAs revealed that likelihood-based classification is effective for detecting leaks in LLMs. However, the existing methods cannot be applied to some proprietary models like ChatGPT or Claude 3 because the likelihood is unavailable to the user. In this study, we propose a Sampling-based Pseudo-Likelihood (\\textbf{SPL}) method for MIA (\\textbf{SaMIA}) that calculates SPL using only the text generated by an LLM to detect leaks. The SaMIA treats the target text as the reference text and multiple outputs from the LLM as text samples, calculates the degree of $n$-gram match as SPL, and determines the membership of the text in the training data. Even without likelihoods, SaMIA performed on par with existing likelihood-based methods.","sentences":["Large Language Models (LLMs) are trained on large-scale web data, which makes it difficult to grasp the contribution of each text.","This poses the risk of leaking inappropriate data such as benchmarks, personal information, and copyrighted texts in the training data.","Membership Inference Attacks (MIA), which determine whether a given text is included in the model's training data, have been attracting attention.","Previous studies of MIAs revealed that likelihood-based classification is effective for detecting leaks in LLMs.","However, the existing methods cannot be applied to some proprietary models like ChatGPT or Claude 3 because the likelihood is unavailable to the user.","In this study, we propose a Sampling-based Pseudo-Likelihood (\\textbf{SPL}) method for MIA (\\textbf{SaMIA}) that calculates SPL using only the text generated by an LLM to detect leaks.","The SaMIA treats the target text as the reference text and multiple outputs from the LLM as text samples, calculates the degree of $n$-gram match as SPL, and determines the membership of the text in the training data.","Even without likelihoods, SaMIA performed on par with existing likelihood-based methods."],"url":"http://arxiv.org/abs/2404.11262v1"}
{"created":"2024-04-17 10:56:06","title":"A Progressive Framework of Vision-language Knowledge Distillation and Alignment for Multilingual Scene","abstract":"Pre-trained vision-language (V-L) models such as CLIP have shown excellent performance in many downstream cross-modal tasks. However, most of them are only applicable to the English context. Subsequent research has focused on this problem and proposed improved models, such as CN-CLIP and AltCLIP, to facilitate their applicability to Chinese and even other languages. Nevertheless, these models suffer from high latency and a large memory footprint in inference, which limits their further deployment on resource-constrained edge devices. In this work, we propose a conceptually simple yet effective multilingual CLIP Compression framework and train a lightweight multilingual vision-language model, called DC-CLIP, for both Chinese and English context. In this framework, we collect high-quality Chinese and English text-image pairs and design two training stages, including multilingual vision-language feature distillation and alignment. During the first stage, lightweight image/text student models are designed to learn robust visual/multilingual textual feature representation ability from corresponding teacher models, respectively. Subsequently, the multilingual vision-language alignment stage enables effective alignment of visual and multilingual textual features to further improve the model's multilingual performance. Comprehensive experiments in zero-shot image classification, conducted based on the ELEVATER benchmark, showcase that DC-CLIP achieves superior performance in the English context and competitive performance in the Chinese context, even with less training data, when compared to existing models of similar parameter magnitude. The evaluation demonstrates the effectiveness of our designed training mechanism.","sentences":["Pre-trained vision-language (V-L) models such as CLIP have shown excellent performance in many downstream cross-modal tasks.","However, most of them are only applicable to the English context.","Subsequent research has focused on this problem and proposed improved models, such as CN-CLIP and AltCLIP, to facilitate their applicability to Chinese and even other languages.","Nevertheless, these models suffer from high latency and a large memory footprint in inference, which limits their further deployment on resource-constrained edge devices.","In this work, we propose a conceptually simple yet effective multilingual CLIP Compression framework and train a lightweight multilingual vision-language model, called DC-CLIP, for both Chinese and English context.","In this framework, we collect high-quality Chinese and English text-image pairs and design two training stages, including multilingual vision-language feature distillation and alignment.","During the first stage, lightweight image/text student models are designed to learn robust visual/multilingual textual feature representation ability from corresponding teacher models, respectively.","Subsequently, the multilingual vision-language alignment stage enables effective alignment of visual and multilingual textual features to further improve the model's multilingual performance.","Comprehensive experiments in zero-shot image classification, conducted based on the ELEVATER benchmark, showcase that DC-CLIP achieves superior performance in the English context and competitive performance in the Chinese context, even with less training data, when compared to existing models of similar parameter magnitude.","The evaluation demonstrates the effectiveness of our designed training mechanism."],"url":"http://arxiv.org/abs/2404.11249v1"}
{"created":"2024-04-17 10:51:36","title":"Learning Social Navigation from Demonstrations with Deep Neural Networks","abstract":"Traditional path-planning techniques treat humans as obstacles. This has changed since robots started to enter human environments. On modern robots, social navigation has become an important aspect of navigation systems. To use learning-based techniques to achieve social navigation, a powerful framework that is capable of representing complex functions with as few data as possible is required. In this study, we benefited from recent advances in deep learning at both global and local planning levels to achieve human-aware navigation on a simulated robot. Two distinct deep models are trained with respective objectives: one for global planning and one for local planning. These models are then employed in the simulated robot. In the end, it has been shown that our model can successfully carry out both global and local planning tasks. We have shown that our system could generate paths that successfully reach targets while avoiding obstacles with better performance compared to feed-forward neural networks.","sentences":["Traditional path-planning techniques treat humans as obstacles.","This has changed since robots started to enter human environments.","On modern robots, social navigation has become an important aspect of navigation systems.","To use learning-based techniques to achieve social navigation, a powerful framework that is capable of representing complex functions with as few data as possible is required.","In this study, we benefited from recent advances in deep learning at both global and local planning levels to achieve human-aware navigation on a simulated robot.","Two distinct deep models are trained with respective objectives: one for global planning and one for local planning.","These models are then employed in the simulated robot.","In the end, it has been shown that our model can successfully carry out both global and local planning tasks.","We have shown that our system could generate paths that successfully reach targets while avoiding obstacles with better performance compared to feed-forward neural networks."],"url":"http://arxiv.org/abs/2404.11246v1"}
{"created":"2024-04-17 10:49:00","title":"Optical Image-to-Image Translation Using Denoising Diffusion Models: Heterogeneous Change Detection as a Use Case","abstract":"We introduce an innovative deep learning-based method that uses a denoising diffusion-based model to translate low-resolution images to high-resolution ones from different optical sensors while preserving the contents and avoiding undesired artifacts. The proposed method is trained and tested on a large and diverse data set of paired Sentinel-II and Planet Dove images. We show that it can solve serious image generation issues observed when the popular classifier-free guided Denoising Diffusion Implicit Model (DDIM) framework is used in the task of Image-to-Image Translation of multi-sensor optical remote sensing images and that it can generate large images with highly consistent patches, both in colors and in features. Moreover, we demonstrate how our method improves heterogeneous change detection results in two urban areas: Beirut, Lebanon, and Austin, USA. Our contributions are: i) a new training and testing algorithm based on denoising diffusion models for optical image translation; ii) a comprehensive image quality evaluation and ablation study; iii) a comparison with the classifier-free guided DDIM framework; and iv) change detection experiments on heterogeneous data.","sentences":["We introduce an innovative deep learning-based method that uses a denoising diffusion-based model to translate low-resolution images to high-resolution ones from different optical sensors while preserving the contents and avoiding undesired artifacts.","The proposed method is trained and tested on a large and diverse data set of paired Sentinel-II and Planet Dove images.","We show that it can solve serious image generation issues observed when the popular classifier-free guided Denoising Diffusion Implicit Model (DDIM) framework is used in the task of Image-to-Image Translation of multi-sensor optical remote sensing images and that it can generate large images with highly consistent patches, both in colors and in features.","Moreover, we demonstrate how our method improves heterogeneous change detection results in two urban areas: Beirut, Lebanon, and Austin, USA.","Our contributions are: i) a new training and testing algorithm based on denoising diffusion models for optical image translation; ii) a comprehensive image quality evaluation and ablation study; iii) a comparison with the classifier-free guided DDIM framework; and iv) change detection experiments on heterogeneous data."],"url":"http://arxiv.org/abs/2404.11243v1"}
{"created":"2024-04-17 10:20:16","title":"Simple In-place Data Augmentation for Surveillance Object Detection","abstract":"Motivated by the need to improve model performance in traffic monitoring tasks with limited labeled samples, we propose a straightforward augmentation technique tailored for object detection datasets, specifically designed for stationary camera-based applications. Our approach focuses on placing objects in the same positions as the originals to ensure its effectiveness. By applying in-place augmentation on objects from the same camera input image, we address the challenge of overlapping with original and previously selected objects. Through extensive testing on two traffic monitoring datasets, we illustrate the efficacy of our augmentation strategy in improving model performance, particularly in scenarios with limited labeled samples and imbalanced class distributions. Notably, our method achieves comparable performance to models trained on the entire dataset while utilizing only 8.5 percent of the original data. Moreover, we report significant improvements, with mAP@.5 increasing from 0.4798 to 0.5025, and the mAP@.5:.95 rising from 0.29 to 0.3138 on the FishEye8K dataset. These results highlight the potential of our augmentation approach in enhancing object detection models for traffic monitoring applications.","sentences":["Motivated by the need to improve model performance in traffic monitoring tasks with limited labeled samples, we propose a straightforward augmentation technique tailored for object detection datasets, specifically designed for stationary camera-based applications.","Our approach focuses on placing objects in the same positions as the originals to ensure its effectiveness.","By applying in-place augmentation on objects from the same camera input image, we address the challenge of overlapping with original and previously selected objects.","Through extensive testing on two traffic monitoring datasets, we illustrate the efficacy of our augmentation strategy in improving model performance, particularly in scenarios with limited labeled samples and imbalanced class distributions.","Notably, our method achieves comparable performance to models trained on the entire dataset while utilizing only 8.5 percent of the original data.","Moreover, we report significant improvements, with mAP@.5 increasing from 0.4798 to 0.5025, and the mAP@.5:.95 rising from 0.29 to 0.3138 on the FishEye8K dataset.","These results highlight the potential of our augmentation approach in enhancing object detection models for traffic monitoring applications."],"url":"http://arxiv.org/abs/2404.11226v1"}
{"created":"2024-04-17 10:16:20","title":"Analytical results for uncertainty propagation through trained machine learning regression models","abstract":"Machine learning (ML) models are increasingly being used in metrology applications. However, for ML models to be credible in a metrology context they should be accompanied by principled uncertainty quantification. This paper addresses the challenge of uncertainty propagation through trained/fixed machine learning (ML) regression models. Analytical expressions for the mean and variance of the model output are obtained/presented for certain input data distributions and for a variety of ML models. Our results cover several popular ML models including linear regression, penalised linear regression, kernel ridge regression, Gaussian Processes (GPs), support vector machines (SVMs) and relevance vector machines (RVMs). We present numerical experiments in which we validate our methods and compare them with a Monte Carlo approach from a computational efficiency point of view. We also illustrate our methods in the context of a metrology application, namely modelling the state-of-health of lithium-ion cells based upon Electrical Impedance Spectroscopy (EIS) data","sentences":["Machine learning (ML) models are increasingly being used in metrology applications.","However, for ML models to be credible in a metrology context they should be accompanied by principled uncertainty quantification.","This paper addresses the challenge of uncertainty propagation through trained/fixed machine learning (ML) regression models.","Analytical expressions for the mean and variance of the model output are obtained/presented for certain input data distributions and for a variety of ML models.","Our results cover several popular ML models including linear regression, penalised linear regression, kernel ridge regression, Gaussian Processes (GPs), support vector machines (SVMs) and relevance vector machines (RVMs).","We present numerical experiments in which we validate our methods and compare them with a Monte Carlo approach from a computational efficiency point of view.","We also illustrate our methods in the context of a metrology application, namely modelling the state-of-health of lithium-ion cells based upon Electrical Impedance Spectroscopy (EIS) data"],"url":"http://arxiv.org/abs/2404.11224v1"}
{"created":"2024-04-17 10:13:44","title":"AndroLog: Android Instrumentation and Code Coverage Analysis","abstract":"Dynamic analysis has emerged as a pivotal technique for testing Android apps, enabling the detection of bugs, malicious code, and vulnerabilities. A key metric in evaluating the efficacy of tools employed by both research and practitioner communities for this purpose is code coverage. Obtaining code coverage typically requires planting probes within apps to gather coverage data during runtime. Due to the general unavailability of source code to analysts, there is a necessity for instrumenting apps to insert these probes in black-box environments. However, the tools available for such instrumentation are limited in their reliability and require intrusive changes interfering with apps' functionalities.   This paper introduces AndroLog a novel tool developed on top of the Soot framework, designed to provide fine-grained coverage information at multiple levels, including class, methods, statements, and Android components. In contrast to existing tools, AndroLog leaves the responsibility to test apps to analysts, and its motto is simplicity. As demonstrated in this paper, AndroLog can instrument up to 98% of recent Android apps compared to existing tools with 79% and 48% respectively for COSMO and ACVTool. AndroLog also stands out for its potential for future enhancements to increase granularity on demand. We make AndroLog available to the community and provide a video demonstration of AndroLog (see section 8).","sentences":["Dynamic analysis has emerged as a pivotal technique for testing Android apps, enabling the detection of bugs, malicious code, and vulnerabilities.","A key metric in evaluating the efficacy of tools employed by both research and practitioner communities for this purpose is code coverage.","Obtaining code coverage typically requires planting probes within apps to gather coverage data during runtime.","Due to the general unavailability of source code to analysts, there is a necessity for instrumenting apps to insert these probes in black-box environments.","However, the tools available for such instrumentation are limited in their reliability and require intrusive changes interfering with apps' functionalities.   ","This paper introduces AndroLog a novel tool developed on top of the Soot framework, designed to provide fine-grained coverage information at multiple levels, including class, methods, statements, and Android components.","In contrast to existing tools, AndroLog leaves the responsibility to test apps to analysts, and its motto is simplicity.","As demonstrated in this paper, AndroLog can instrument up to 98% of recent Android apps compared to existing tools with 79% and 48% respectively for COSMO and ACVTool.","AndroLog also stands out for its potential for future enhancements to increase granularity on demand.","We make AndroLog available to the community and provide a video demonstration of AndroLog (see section 8)."],"url":"http://arxiv.org/abs/2404.11223v1"}
{"created":"2024-04-17 09:43:54","title":"CAGE: Causality-Aware Shapley Value for Global Explanations","abstract":"As Artificial Intelligence (AI) is having more influence on our everyday lives, it becomes important that AI-based decisions are transparent and explainable. As a consequence, the field of eXplainable AI (or XAI) has become popular in recent years. One way to explain AI models is to elucidate the predictive importance of the input features for the AI model in general, also referred to as global explanations. Inspired by cooperative game theory, Shapley values offer a convenient way for quantifying the feature importance as explanations. However many methods based on Shapley values are built on the assumption of feature independence and often overlook causal relations of the features which could impact their importance for the ML model. Inspired by studies of explanations at the local level, we propose CAGE (Causally-Aware Shapley Values for Global Explanations). In particular, we introduce a novel sampling procedure for out-coalition features that respects the causal relations of the input features. We derive a practical approach that incorporates causal knowledge into global explanation and offers the possibility to interpret the predictive feature importance considering their causal relation. We evaluate our method on synthetic data and real-world data. The explanations from our approach suggest that they are not only more intuitive but also more faithful compared to previous global explanation methods.","sentences":["As Artificial Intelligence (AI) is having more influence on our everyday lives, it becomes important that AI-based decisions are transparent and explainable.","As a consequence, the field of eXplainable AI (or XAI) has become popular in recent years.","One way to explain AI models is to elucidate the predictive importance of the input features for the AI model in general, also referred to as global explanations.","Inspired by cooperative game theory, Shapley values offer a convenient way for quantifying the feature importance as explanations.","However many methods based on Shapley values are built on the assumption of feature independence and often overlook causal relations of the features which could impact their importance for the ML model.","Inspired by studies of explanations at the local level, we propose CAGE (Causally-Aware Shapley Values for Global Explanations).","In particular, we introduce a novel sampling procedure for out-coalition features that respects the causal relations of the input features.","We derive a practical approach that incorporates causal knowledge into global explanation and offers the possibility to interpret the predictive feature importance considering their causal relation.","We evaluate our method on synthetic data and real-world data.","The explanations from our approach suggest that they are not only more intuitive but also more faithful compared to previous global explanation methods."],"url":"http://arxiv.org/abs/2404.11208v1"}
{"created":"2024-04-17 09:37:25","title":"Kathakali Hand Gesture Recognition With Minimal Data","abstract":"The Indian classical dance-drama Kathakali has a set of hand gestures called Mudras, which form the fundamental units of all its dance moves and postures. Recognizing the depicted mudra becomes one of the first steps in its digital processing. The work treats the problem as a 24-class classification task and proposes a vector-similarity-based approach using pose estimation, eliminating the need for further training or fine-tuning. This approach overcomes the challenge of data scarcity that limits the application of AI in similar domains. The method attains 92% accuracy which is a similar or better performance as other model-training-based works existing in the domain, with the added advantage that the method can still work with data sizes as small as 1 or 5 samples with a slightly reduced performance. Working with images, videos, and even real-time streams is possible. The system can work with hand-cropped or full-body images alike. We have developed and made public a dataset for the Kathakali Mudra Recognition as part of this work.","sentences":["The Indian classical dance-drama Kathakali has a set of hand gestures called Mudras, which form the fundamental units of all its dance moves and postures.","Recognizing the depicted mudra becomes one of the first steps in its digital processing.","The work treats the problem as a 24-class classification task and proposes a vector-similarity-based approach using pose estimation, eliminating the need for further training or fine-tuning.","This approach overcomes the challenge of data scarcity that limits the application of AI in similar domains.","The method attains 92% accuracy which is a similar or better performance as other model-training-based works existing in the domain, with the added advantage that the method can still work with data sizes as small as 1 or 5 samples with a slightly reduced performance.","Working with images, videos, and even real-time streams is possible.","The system can work with hand-cropped or full-body images alike.","We have developed and made public a dataset for the Kathakali Mudra Recognition as part of this work."],"url":"http://arxiv.org/abs/2404.11205v1"}
{"created":"2024-04-17 09:33:31","title":"GhostNetV3: Exploring the Training Strategies for Compact Models","abstract":"Compact neural networks are specially designed for applications on edge devices with faster inference speed yet modest performance. However, training strategies of compact models are borrowed from that of conventional models at present, which ignores their difference in model capacity and thus may impede the performance of compact models. In this paper, by systematically investigating the impact of different training ingredients, we introduce a strong training strategy for compact models. We find that the appropriate designs of re-parameterization and knowledge distillation are crucial for training high-performance compact models, while some commonly used data augmentations for training conventional models, such as Mixup and CutMix, lead to worse performance. Our experiments on ImageNet-1K dataset demonstrate that our specialized training strategy for compact models is applicable to various architectures, including GhostNetV2, MobileNetV2 and ShuffleNetV2. Specifically, equipped with our strategy, GhostNetV3 1.3$\\times$ achieves a top-1 accuracy of 79.1% with only 269M FLOPs and a latency of 14.46ms on mobile devices, surpassing its ordinarily trained counterpart by a large margin. Moreover, our observation can also be extended to object detection scenarios. PyTorch code and checkpoints can be found at https://github.com/huawei-noah/Efficient-AI-Backbones/tree/master/ghostnetv3_pytorch.","sentences":["Compact neural networks are specially designed for applications on edge devices with faster inference speed yet modest performance.","However, training strategies of compact models are borrowed from that of conventional models at present, which ignores their difference in model capacity and thus may impede the performance of compact models.","In this paper, by systematically investigating the impact of different training ingredients, we introduce a strong training strategy for compact models.","We find that the appropriate designs of re-parameterization and knowledge distillation are crucial for training high-performance compact models, while some commonly used data augmentations for training conventional models, such as Mixup and CutMix, lead to worse performance.","Our experiments on ImageNet-1K dataset demonstrate that our specialized training strategy for compact models is applicable to various architectures, including GhostNetV2, MobileNetV2 and ShuffleNetV2.","Specifically, equipped with our strategy, GhostNetV3 1.3$\\times$ achieves a top-1 accuracy of 79.1% with only 269M FLOPs and a latency of 14.46ms on mobile devices, surpassing its ordinarily trained counterpart by a large margin.","Moreover, our observation can also be extended to object detection scenarios.","PyTorch code and checkpoints can be found at https://github.com/huawei-noah/Efficient-AI-Backbones/tree/master/ghostnetv3_pytorch."],"url":"http://arxiv.org/abs/2404.11202v1"}
{"created":"2024-04-17 09:08:42","title":"The Writing is on the Wall: Analyzing the Boom of Inscriptions and its Impact on Rollup Performance and Cost Efficiency","abstract":"Late 2023 witnessed significant user activity on EVM chains, resulting in a surge in transaction activity and putting many rollups into the first live test. While some rollups performed well, some others experienced downtime during this period, affecting transaction finality time and gas fees. To address the lack of empirical research on rollups, we perform the first study during a heightened activity during the late 2023 transaction boom, as attributed to inscriptions - a novel technique that enables NFT and ERC-20 token creation on Bitcoin and other blockchains. We observe that minting inscription-based meme tokens on zkSync Era allows for trading at a fraction of the costs, compared to the Bitcoin or Ethereum networks. We also found that the increased transaction activity, over 99% attributed to the minting of new inscription tokens, positively affected other users of zkSync Era, resulting in lowered gas fees. Unlike L1 blockchains, ZK rollups may experience lower gas fees with increased transaction volume. Lastly, the introduction of blobs - a form of temporary data storage - decreased the gas costs of Ethereum rollups, but also raised a number of questions about the security of inscription-based tokens.","sentences":["Late 2023 witnessed significant user activity on EVM chains, resulting in a surge in transaction activity and putting many rollups into the first live test.","While some rollups performed well, some others experienced downtime during this period, affecting transaction finality time and gas fees.","To address the lack of empirical research on rollups, we perform the first study during a heightened activity during the late 2023 transaction boom, as attributed to inscriptions - a novel technique that enables NFT and ERC-20 token creation on Bitcoin and other blockchains.","We observe that minting inscription-based meme tokens on zkSync Era allows for trading at a fraction of the costs, compared to the Bitcoin or Ethereum networks.","We also found that the increased transaction activity, over 99% attributed to the minting of new inscription tokens, positively affected other users of zkSync Era, resulting in lowered gas fees.","Unlike L1 blockchains, ZK rollups may experience lower gas fees with increased transaction volume.","Lastly, the introduction of blobs - a form of temporary data storage - decreased the gas costs of Ethereum rollups, but also raised a number of questions about the security of inscription-based tokens."],"url":"http://arxiv.org/abs/2404.11189v1"}
{"created":"2024-04-17 08:50:29","title":"Causal Deconfounding via Confounder Disentanglement for Dual-Target Cross-Domain Recommendation","abstract":"In recent years, dual-target Cross-Domain Recommendation (CDR) has been proposed to capture comprehensive user preferences in order to ultimately enhance the recommendation accuracy in both data-richer and data-sparser domains simultaneously. However, in addition to users' true preferences, the user-item interactions might also be affected by confounders (e.g., free shipping, sales promotion). As a result, dual-target CDR has to meet two challenges: (1) how to effectively decouple observed confounders, including single-domain confounders and cross-domain confounders, and (2) how to preserve the positive effects of observed confounders on predicted interactions, while eliminating their negative effects on capturing comprehensive user preferences. To address the above two challenges, we propose a Causal Deconfounding framework via Confounder Disentanglement for dual-target Cross-Domain Recommendation, called CD2CDR. In CD2CDR, we first propose a confounder disentanglement module to effectively decouple observed single-domain and cross-domain confounders. We then propose a causal deconfounding module to preserve the positive effects of such observed confounders and eliminate their negative effects via backdoor adjustment, thereby enhancing the recommendation accuracy in each domain. Extensive experiments conducted on five real-world datasets demonstrate that CD2CDR significantly outperforms the state-of-the-art methods.","sentences":["In recent years, dual-target Cross-Domain Recommendation (CDR) has been proposed to capture comprehensive user preferences in order to ultimately enhance the recommendation accuracy in both data-richer and data-sparser domains simultaneously.","However, in addition to users' true preferences, the user-item interactions might also be affected by confounders (e.g., free shipping, sales promotion).","As a result, dual-target CDR has to meet two challenges: (1) how to effectively decouple observed confounders, including single-domain confounders and cross-domain confounders, and (2) how to preserve the positive effects of observed confounders on predicted interactions, while eliminating their negative effects on capturing comprehensive user preferences.","To address the above two challenges, we propose a Causal Deconfounding framework via Confounder Disentanglement for dual-target Cross-Domain Recommendation, called CD2CDR.","In CD2CDR, we first propose a confounder disentanglement module to effectively decouple observed single-domain and cross-domain confounders.","We then propose a causal deconfounding module to preserve the positive effects of such observed confounders and eliminate their negative effects via backdoor adjustment, thereby enhancing the recommendation accuracy in each domain.","Extensive experiments conducted on five real-world datasets demonstrate that CD2CDR significantly outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.11180v1"}
{"created":"2024-04-17 08:42:42","title":"Deep Neural Networks via Complex Network Theory: a Perspective","abstract":"Deep Neural Networks (DNNs) can be represented as graphs whose links and vertices iteratively process data and solve tasks sub-optimally. Complex Network Theory (CNT), merging statistical physics with graph theory, provides a method for interpreting neural networks by analysing their weights and neuron structures. However, classic works adapt CNT metrics that only permit a topological analysis as they do not account for the effect of the input data. In addition, CNT metrics have been applied to a limited range of architectures, mainly including Fully Connected neural networks. In this work, we extend the existing CNT metrics with measures that sample from the DNNs' training distribution, shifting from a purely topological analysis to one that connects with the interpretability of deep learning. For the novel metrics, in addition to the existing ones, we provide a mathematical formalisation for Fully Connected, AutoEncoder, Convolutional and Recurrent neural networks, of which we vary the activation functions and the number of hidden layers. We show that these metrics differentiate DNNs based on the architecture, the number of hidden layers, and the activation function. Our contribution provides a method rooted in physics for interpreting DNNs that offers insights beyond the traditional input-output relationship and the CNT topological analysis.","sentences":["Deep Neural Networks (DNNs) can be represented as graphs whose links and vertices iteratively process data and solve tasks sub-optimally.","Complex Network Theory (CNT), merging statistical physics with graph theory, provides a method for interpreting neural networks by analysing their weights and neuron structures.","However, classic works adapt CNT metrics that only permit a topological analysis as they do not account for the effect of the input data.","In addition, CNT metrics have been applied to a limited range of architectures, mainly including Fully Connected neural networks.","In this work, we extend the existing CNT metrics with measures that sample from the DNNs' training distribution, shifting from a purely topological analysis to one that connects with the interpretability of deep learning.","For the novel metrics, in addition to the existing ones, we provide a mathematical formalisation for Fully Connected, AutoEncoder, Convolutional and Recurrent neural networks, of which we vary the activation functions and the number of hidden layers.","We show that these metrics differentiate DNNs based on the architecture, the number of hidden layers, and the activation function.","Our contribution provides a method rooted in physics for interpreting DNNs that offers insights beyond the traditional input-output relationship and the CNT topological analysis."],"url":"http://arxiv.org/abs/2404.11172v1"}
{"created":"2024-04-17 08:40:54","title":"Personalized Heart Disease Detection via ECG Digital Twin Generation","abstract":"Heart diseases rank among the leading causes of global mortality, demonstrating a crucial need for early diagnosis and intervention. Most traditional electrocardiogram (ECG) based automated diagnosis methods are trained at population level, neglecting the customization of personalized ECGs to enhance individual healthcare management. A potential solution to address this limitation is to employ digital twins to simulate symptoms of diseases in real patients. In this paper, we present an innovative prospective learning approach for personalized heart disease detection, which generates digital twins of healthy individuals' anomalous ECGs and enhances the model sensitivity to the personalized symptoms. In our approach, a vector quantized feature separator is proposed to locate and isolate the disease symptom and normal segments in ECG signals with ECG report guidance. Thus, the ECG digital twins can simulate specific heart diseases used to train a personalized heart disease detection model. Experiments demonstrate that our approach not only excels in generating high-fidelity ECG signals but also improves personalized heart disease detection. Moreover, our approach ensures robust privacy protection, safeguarding patient data in model development.","sentences":["Heart diseases rank among the leading causes of global mortality, demonstrating a crucial need for early diagnosis and intervention.","Most traditional electrocardiogram (ECG) based automated diagnosis methods are trained at population level, neglecting the customization of personalized ECGs to enhance individual healthcare management.","A potential solution to address this limitation is to employ digital twins to simulate symptoms of diseases in real patients.","In this paper, we present an innovative prospective learning approach for personalized heart disease detection, which generates digital twins of healthy individuals' anomalous ECGs and enhances the model sensitivity to the personalized symptoms.","In our approach, a vector quantized feature separator is proposed to locate and isolate the disease symptom and normal segments in ECG signals with ECG report guidance.","Thus, the ECG digital twins can simulate specific heart diseases used to train a personalized heart disease detection model.","Experiments demonstrate that our approach not only excels in generating high-fidelity ECG signals but also improves personalized heart disease detection.","Moreover, our approach ensures robust privacy protection, safeguarding patient data in model development."],"url":"http://arxiv.org/abs/2404.11171v1"}
{"created":"2024-04-17 08:38:04","title":"Mutiny! How does Kubernetes fail, and what can we do about it?","abstract":"In this paper, we i) analyze and classify real-world failures of Kubernetes (the most popular container orchestration system), ii) develop a framework to perform a fault/error injection campaign targeting the data store preserving the cluster state, and iii) compare results of our fault/error injection experiments with real-world failures, showing that our fault/error injections can recreate many real-world failure patterns. The paper aims to address the lack of studies on systematic analyses of Kubernetes failures to date.   Our results show that even a single fault/error (e.g., a bit-flip) in the data stored can propagate, causing cluster-wide failures (3% of injections), service networking issues (4%), and service under/overprovisioning (24%). Errors in the fields tracking dependencies between object caused 51% of such cluster-wide failures. We argue that controlled fault/error injection-based testing should be employed to proactively assess Kubernetes' resiliency and guide the design of failure mitigation strategies.","sentences":["In this paper, we i) analyze and classify real-world failures of Kubernetes (the most popular container orchestration system), ii) develop a framework to perform a fault/error injection campaign targeting the data store preserving the cluster state, and iii) compare results of our fault/error injection experiments with real-world failures, showing that our fault/error injections can recreate many real-world failure patterns.","The paper aims to address the lack of studies on systematic analyses of Kubernetes failures to date.   ","Our results show that even a single fault/error (e.g., a bit-flip) in the data stored can propagate, causing cluster-wide failures (3% of injections), service networking issues (4%), and service under/overprovisioning (24%).","Errors in the fields tracking dependencies between object caused 51% of such cluster-wide failures.","We argue that controlled fault/error injection-based testing should be employed to proactively assess Kubernetes' resiliency and guide the design of failure mitigation strategies."],"url":"http://arxiv.org/abs/2404.11169v1"}
{"created":"2024-04-17 08:21:02","title":"Pre-processing matters: A segment search method for WSI classification","abstract":"Pre-processing for whole slide images can affect classification performance both in the training and inference stages. Our study analyzes the impact of pre-processing parameters on inference and training across single- and multiple-domain datasets. However, searching for an optimal parameter set is time-consuming. To overcome this, we propose a novel Similarity-based Simulated Annealing approach for fast parameter tuning to enhance inference performance on single-domain data. Our method demonstrates significant performance improvements in accuracy, which raise accuracy from 0.512 to 0.847 in a single domain. We further extend our insight into training performance in multi-domain data by employing a novel Bayesian optimization to search optimal pre-processing parameters, resulting in a high AUC of 0.967. We highlight that better pre-processing for WSI can contribute to further accuracy improvement in the histology area.","sentences":["Pre-processing for whole slide images can affect classification performance both in the training and inference stages.","Our study analyzes the impact of pre-processing parameters on inference and training across single- and multiple-domain datasets.","However, searching for an optimal parameter set is time-consuming.","To overcome this, we propose a novel Similarity-based Simulated Annealing approach for fast parameter tuning to enhance inference performance on single-domain data.","Our method demonstrates significant performance improvements in accuracy, which raise accuracy from 0.512 to 0.847 in a single domain.","We further extend our insight into training performance in multi-domain data by employing a novel Bayesian optimization to search optimal pre-processing parameters, resulting in a high AUC of 0.967.","We highlight that better pre-processing for WSI can contribute to further accuracy improvement in the histology area."],"url":"http://arxiv.org/abs/2404.11161v1"}
{"created":"2024-04-17 07:59:33","title":"Explainable Machine Learning System for Predicting Chronic Kidney Disease in High-Risk Cardiovascular Patients","abstract":"As the global population ages, the incidence of Chronic Kidney Disease (CKD) is rising. CKD often remains asymptomatic until advanced stages, which significantly burdens both the healthcare system and patient quality of life. This research developed an explainable machine learning system for predicting CKD in patients with cardiovascular risks, utilizing medical history and laboratory data. The Random Forest model achieved the highest sensitivity of 88.2%. The study introduces a comprehensive explainability framework that extends beyond traditional feature importance methods, incorporating global and local interpretations, bias inspection, biomedical relevance, and safety assessments. Key predictive features identified in global interpretation were the use of diabetic and ACEI/ARB medications, and initial eGFR values. Local interpretation provided model insights through counterfactual explanations, which aligned with other system parts. After conducting a bias inspection, it was found that the initial eGFR values and CKD predictions exhibited some bias, but no significant gender bias was identified. The model's logic, extracted by scoped rules, was confirmed to align with existing medical literature. The safety assessment tested potentially dangerous cases and confirmed that the model behaved safely. This system enhances the explainability, reliability, and accountability of the model, promoting its potential integration into healthcare settings and compliance with upcoming regulatory standards, and showing promise for broader applications in healthcare machine learning.","sentences":["As the global population ages, the incidence of Chronic Kidney Disease (CKD) is rising.","CKD often remains asymptomatic until advanced stages, which significantly burdens both the healthcare system and patient quality of life.","This research developed an explainable machine learning system for predicting CKD in patients with cardiovascular risks, utilizing medical history and laboratory data.","The Random Forest model achieved the highest sensitivity of 88.2%.","The study introduces a comprehensive explainability framework that extends beyond traditional feature importance methods, incorporating global and local interpretations, bias inspection, biomedical relevance, and safety assessments.","Key predictive features identified in global interpretation were the use of diabetic and ACEI/ARB medications, and initial eGFR values.","Local interpretation provided model insights through counterfactual explanations, which aligned with other system parts.","After conducting a bias inspection, it was found that the initial eGFR values and CKD predictions exhibited some bias, but no significant gender bias was identified.","The model's logic, extracted by scoped rules, was confirmed to align with existing medical literature.","The safety assessment tested potentially dangerous cases and confirmed that the model behaved safely.","This system enhances the explainability, reliability, and accountability of the model, promoting its potential integration into healthcare settings and compliance with upcoming regulatory standards, and showing promise for broader applications in healthcare machine learning."],"url":"http://arxiv.org/abs/2404.11148v1"}
{"created":"2024-04-17 07:35:06","title":"Accuracy and repeatability of a parallel robot for personalised minimally invasive surgery","abstract":"The paper presents the methodology used for accuracy and repeatability measurements of the experimental model of a parallel robot developed for surgical applications. The experimental setup uses a motion tracking system (for accuracy) and a high precision measuring arm for position (for repeatability). The accuracy was obtained by comparing the trajectory data from the experimental measurement with a baseline trajectory defined with the kinematic models of the parallel robotic system. The repeatability was experi-mentally determined by moving (repeatedly) the robot platform in predefined points.","sentences":["The paper presents the methodology used for accuracy and repeatability measurements of the experimental model of a parallel robot developed for surgical applications.","The experimental setup uses a motion tracking system (for accuracy) and a high precision measuring arm for position (for repeatability).","The accuracy was obtained by comparing the trajectory data from the experimental measurement with a baseline trajectory defined with the kinematic models of the parallel robotic system.","The repeatability was experi-mentally determined by moving (repeatedly) the robot platform in predefined points."],"url":"http://arxiv.org/abs/2404.11140v1"}
{"created":"2024-04-17 07:34:21","title":"GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement","abstract":"Object pose refinement is essential for robust object pose estimation. Previous work has made significant progress towards instance-level object pose refinement. Yet, category-level pose refinement is a more challenging problem due to large shape variations within a category and the discrepancies between the target object and the shape prior. To address these challenges, we introduce a novel architecture for category-level object pose refinement. Our approach integrates an HS-layer and learnable affine transformations, which aims to enhance the extraction and alignment of geometric information. Additionally, we introduce a cross-cloud transformation mechanism that efficiently merges diverse data sources. Finally, we push the limits of our model by incorporating the shape prior information for translation and size error prediction. We conducted extensive experiments to demonstrate the effectiveness of the proposed framework. Through extensive quantitative experiments, we demonstrate significant improvement over the baseline method by a large margin across all metrics.","sentences":["Object pose refinement is essential for robust object pose estimation.","Previous work has made significant progress towards instance-level object pose refinement.","Yet, category-level pose refinement is a more challenging problem due to large shape variations within a category and the discrepancies between the target object and the shape prior.","To address these challenges, we introduce a novel architecture for category-level object pose refinement.","Our approach integrates an HS-layer and learnable affine transformations, which aims to enhance the extraction and alignment of geometric information.","Additionally, we introduce a cross-cloud transformation mechanism that efficiently merges diverse data sources.","Finally, we push the limits of our model by incorporating the shape prior information for translation and size error prediction.","We conducted extensive experiments to demonstrate the effectiveness of the proposed framework.","Through extensive quantitative experiments, we demonstrate significant improvement over the baseline method by a large margin across all metrics."],"url":"http://arxiv.org/abs/2404.11139v1"}
{"created":"2024-04-17 07:26:23","title":"A Novel ICD Coding Framework Based on Associated and Hierarchical Code Description Distillation","abstract":"ICD(International Classification of Diseases) coding involves assigning ICD codes to patients visit based on their medical notes. ICD coding is a challenging multilabel text classification problem due to noisy medical document inputs. Recent advancements in automated ICD coding have enhanced performance by integrating additional data and knowledge bases with the encoding of medical notes and codes. However, most of them ignore the code hierarchy, leading to improper code assignments. To address these problems, we propose a novel framework based on associated and hierarchical code description distillation (AHDD) for better code representation learning and avoidance of improper code assignment.we utilize the code description and the hierarchical structure inherent to the ICD codes. Therefore, in this paper, we leverage the code description and the hierarchical structure inherent to the ICD codes. The code description is also applied to aware the attention layer and output layer. Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baselines.","sentences":["ICD(International Classification of Diseases) coding involves assigning ICD codes to patients visit based on their medical notes.","ICD coding is a challenging multilabel text classification problem due to noisy medical document inputs.","Recent advancements in automated ICD coding have enhanced performance by integrating additional data and knowledge bases with the encoding of medical notes and codes.","However, most of them ignore the code hierarchy, leading to improper code assignments.","To address these problems, we propose a novel framework based on associated and hierarchical code description distillation (AHDD) for better code representation learning and avoidance of improper code assignment.we utilize the code description and the hierarchical structure inherent to the ICD codes.","Therefore, in this paper, we leverage the code description and the hierarchical structure inherent to the ICD codes.","The code description is also applied to aware the attention layer and output layer.","Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baselines."],"url":"http://arxiv.org/abs/2404.11132v1"}
{"created":"2024-04-17 07:17:47","title":"D-Aug: Enhancing Data Augmentation for Dynamic LiDAR Scenes","abstract":"Creating large LiDAR datasets with pixel-level labeling poses significant challenges. While numerous data augmentation methods have been developed to reduce the reliance on manual labeling, these methods predominantly focus on static scenes and they overlook the importance of data augmentation for dynamic scenes, which is critical for autonomous driving. To address this issue, we propose D-Aug, a LiDAR data augmentation method tailored for augmenting dynamic scenes. D-Aug extracts objects and inserts them into dynamic scenes, considering the continuity of these objects across consecutive frames. For seamless insertion into dynamic scenes, we propose a reference-guided method that involves dynamic collision detection and rotation alignment. Additionally, we present a pixel-level road identification strategy to efficiently determine suitable insertion positions. We validated our method using the nuScenes dataset with various 3D detection and tracking methods. Comparative experiments demonstrate the superiority of D-Aug.","sentences":["Creating large LiDAR datasets with pixel-level labeling poses significant challenges.","While numerous data augmentation methods have been developed to reduce the reliance on manual labeling, these methods predominantly focus on static scenes and they overlook the importance of data augmentation for dynamic scenes, which is critical for autonomous driving.","To address this issue, we propose D-Aug, a LiDAR data augmentation method tailored for augmenting dynamic scenes.","D-Aug extracts objects and inserts them into dynamic scenes, considering the continuity of these objects across consecutive frames.","For seamless insertion into dynamic scenes, we propose a reference-guided method that involves dynamic collision detection and rotation alignment.","Additionally, we present a pixel-level road identification strategy to efficiently determine suitable insertion positions.","We validated our method using the nuScenes dataset with various 3D detection and tracking methods.","Comparative experiments demonstrate the superiority of D-Aug."],"url":"http://arxiv.org/abs/2404.11127v1"}
{"created":"2024-04-17 07:10:28","title":"Small Language Models are Good Too: An Empirical Study of Zero-Shot Classification","abstract":"This study is part of the debate on the efficiency of large versus small language models for text classification by prompting.We assess the performance of small language models in zero-shot text classification, challenging the prevailing dominance of large models.Across 15 datasets, our investigation benchmarks language models from 77M to 40B parameters using different architectures and scoring functions. Our findings reveal that small models can effectively classify texts, getting on par with or surpassing their larger counterparts.We developed and shared a comprehensive open-source repository that encapsulates our methodologies. This research underscores the notion that bigger isn't always better, suggesting that resource-efficient small models may offer viable solutions for specific data classification challenges.","sentences":["This study is part of the debate on the efficiency of large versus small language models for text classification by prompting.","We assess the performance of small language models in zero-shot text classification, challenging the prevailing dominance of large models.","Across 15 datasets, our investigation benchmarks language models from 77M to 40B parameters using different architectures and scoring functions.","Our findings reveal that small models can effectively classify texts, getting on par with or surpassing their larger counterparts.","We developed and shared a comprehensive open-source repository that encapsulates our methodologies.","This research underscores the notion that bigger isn't always better, suggesting that resource-efficient small models may offer viable solutions for specific data classification challenges."],"url":"http://arxiv.org/abs/2404.11122v1"}
{"created":"2024-04-17 07:00:20","title":"Reuse out-of-year data to enhance land cover mappingvia feature disentanglement and contrastive learning","abstract":"Timely up-to-date land use/land cover (LULC) maps play a pivotal role in supporting agricultural territory management, environmental monitoring and facilitating well-informed and sustainable decision-making. Typically, when creating a land cover (LC) map, precise ground truth data is collected through time-consuming and expensive field campaigns. This data is then utilized in conjunction with satellite image time series (SITS) through advanced machine learning algorithms to get the final map. Unfortunately, each time this process is repeated (e.g., annually over a region to estimate agricultural production or potential biodiversity loss), new ground truth data must be collected, leading to the complete disregard of previously gathered reference data despite the substantial financial and time investment they have required. How to make value of historical data, from the same or similar study sites, to enhance the current LULC mapping process constitutes a significant challenge that could enable the financial and human-resource efforts invested in previous data campaigns to be valued again. Aiming to tackle this important challenge, we here propose a deep learning framework based on recent advances in domain adaptation and generalization to combine remote sensing and reference data coming from two different domains (e.g. historical data and fresh ones) to ameliorate the current LC mapping process. Our approach, namely REFeD (data Reuse with Effective Feature Disentanglement for land cover mapping), leverages a disentanglement strategy, based on contrastive learning, where invariant and specific per-domain features are derived to recover the intrinsic information related to the downstream LC mapping task and alleviate possible distribution shifts between domains. Additionally, REFeD is equipped with an effective supervision scheme where feature disentanglement is further enforced via multiple levels of supervision at different granularities. The experimental assessment over two study areas covering extremely diverse and contrasted landscapes, namely Koumbia (located in the West-Africa region, in Burkina Faso) and Centre Val de Loire (located in centre Europe, France), underlines the quality of our framework and the obtained findings demonstrate that out-of-year information coming from the same (or similar) study site, at different periods of time, can constitute a valuable additional source of information to enhance the LC mapping process.","sentences":["Timely up-to-date land use/land cover (LULC) maps play a pivotal role in supporting agricultural territory management, environmental monitoring and facilitating well-informed and sustainable decision-making.","Typically, when creating a land cover (LC) map, precise ground truth data is collected through time-consuming and expensive field campaigns.","This data is then utilized in conjunction with satellite image time series (SITS) through advanced machine learning algorithms to get the final map.","Unfortunately, each time this process is repeated (e.g., annually over a region to estimate agricultural production or potential biodiversity loss), new ground truth data must be collected, leading to the complete disregard of previously gathered reference data despite the substantial financial and time investment they have required.","How to make value of historical data, from the same or similar study sites, to enhance the current LULC mapping process constitutes a significant challenge that could enable the financial and human-resource efforts invested in previous data campaigns to be valued again.","Aiming to tackle this important challenge, we here propose a deep learning framework based on recent advances in domain adaptation and generalization to combine remote sensing and reference data coming from two different domains (e.g. historical data and fresh ones) to ameliorate the current LC mapping process.","Our approach, namely REFeD (data Reuse with Effective Feature Disentanglement for land cover mapping), leverages a disentanglement strategy, based on contrastive learning, where invariant and specific per-domain features are derived to recover the intrinsic information related to the downstream LC mapping task and alleviate possible distribution shifts between domains.","Additionally, REFeD is equipped with an effective supervision scheme where feature disentanglement is further enforced via multiple levels of supervision at different granularities.","The experimental assessment over two study areas covering extremely diverse and contrasted landscapes, namely Koumbia (located in the West-Africa region, in Burkina Faso) and Centre Val de Loire (located in centre Europe, France), underlines the quality of our framework and the obtained findings demonstrate that out-of-year information coming from the same (or similar) study site, at different periods of time, can constitute a valuable additional source of information to enhance the LC mapping process."],"url":"http://arxiv.org/abs/2404.11114v1"}
{"created":"2024-04-17 06:49:14","title":"Consistency Training by Synthetic Question Generation for Conversational Question Answering","abstract":"Efficiently modeling historical information is a critical component in addressing user queries within a conversational question-answering (QA) context, as historical context plays a vital role in clarifying the user's questions. However, irrelevant history induces noise in the reasoning process, especially for those questions with a considerable historical context. In our novel model-agnostic approach, referred to as CoTaH (Consistency-Trained augmented History), we augment the historical information with synthetic questions and subsequently employ consistency training to train a model that utilizes both real and augmented historical data to implicitly make the reasoning robust to irrelevant history. To the best of our knowledge, this is the first instance of research using question generation as a form of data augmentation to model conversational QA settings. By citing a common modeling error prevalent in previous research, we introduce a new baseline model and compare our model's performance against it, demonstrating an improvement in results, particularly when dealing with questions that include a substantial amount of historical context. The source code can be found on our GitHub page.","sentences":["Efficiently modeling historical information is a critical component in addressing user queries within a conversational question-answering (QA) context, as historical context plays a vital role in clarifying the user's questions.","However, irrelevant history induces noise in the reasoning process, especially for those questions with a considerable historical context.","In our novel model-agnostic approach, referred to as CoTaH (Consistency-Trained augmented History), we augment the historical information with synthetic questions and subsequently employ consistency training to train a model that utilizes both real and augmented historical data to implicitly make the reasoning robust to irrelevant history.","To the best of our knowledge, this is the first instance of research using question generation as a form of data augmentation to model conversational QA settings.","By citing a common modeling error prevalent in previous research, we introduce a new baseline model and compare our model's performance against it, demonstrating an improvement in results, particularly when dealing with questions that include a substantial amount of historical context.","The source code can be found on our GitHub page."],"url":"http://arxiv.org/abs/2404.11109v1"}
{"created":"2024-04-17 06:42:26","title":"XMiner: Efficient Directed Subgraph Matching with Pattern Reduction","abstract":"Graph pattern matching, one of the fundamental graph mining problems, aims to extract structural patterns of interest from an input graph. The state-of-the-art graph matching algorithms and systems are mainly designed for undirected graphs. Directed graph matching is more complex than undirected graph matching because the edge direction must be taken into account before the exploration of each directed edge. Thus, the technologies (e.g. storage, exploiting symmetry for graph matching) for undirected graph matching may not be fully applicable to directed graphs. For example, the redundancy implied in directed graph pattern can not be detected using the symmetry breaking for undirected pattern graph. Here, we present XMiner for efficient directed graph pattern matching whose core idea is 'pattern reduction'. It first analyzes the relationship between constraints implied in a pattern digraph. Then it reduces the pattern graph into a simplified form by finding a minimum constraint cover. Finally, XMiner generates an execution plan and follows it to extract matchings of the pattern graph. So, XMiner works on simplified pattern graph and avoids much data access and redundant computation throughout the matching process. Our experimental results show that XMiner outperforms state-of the-art stand-alone graph matching systems, and scales to complex graph pattern matching tasks on larger graph.","sentences":["Graph pattern matching, one of the fundamental graph mining problems, aims to extract structural patterns of interest from an input graph.","The state-of-the-art graph matching algorithms and systems are mainly designed for undirected graphs.","Directed graph matching is more complex than undirected graph matching because the edge direction must be taken into account before the exploration of each directed edge.","Thus, the technologies (e.g. storage, exploiting symmetry for graph matching) for undirected graph matching may not be fully applicable to directed graphs.","For example, the redundancy implied in directed graph pattern can not be detected using the symmetry breaking for undirected pattern graph.","Here, we present XMiner for efficient directed graph pattern matching whose core idea is 'pattern reduction'.","It first analyzes the relationship between constraints implied in a pattern digraph.","Then it reduces the pattern graph into a simplified form by finding a minimum constraint cover.","Finally, XMiner generates an execution plan and follows it to extract matchings of the pattern graph.","So, XMiner works on simplified pattern graph and avoids much data access and redundant computation throughout the matching process.","Our experimental results show that XMiner outperforms state-of the-art stand-alone graph matching systems, and scales to complex graph pattern matching tasks on larger graph."],"url":"http://arxiv.org/abs/2404.11105v1"}
{"created":"2024-04-17 06:39:03","title":"Distribution-Free Testing of Decision Lists with a Sublinear Number of Queries","abstract":"We give a distribution-free testing algorithm for decision lists with $\\tilde{O}(n^{11/12}/\\varepsilon^3)$ queries. This is the first sublinear algorithm for this problem, which shows that, unlike halfspaces, testing is strictly easier than learning for decision lists. Complementing the algorithm, we show that any distribution-free tester for decision lists must make $\\tilde{\\Omega}(\\sqrt{n})$ queries, or draw $\\tilde{\\Omega}(n)$ samples when the algorithm is sample-based.","sentences":["We give a distribution-free testing algorithm for decision lists with $\\tilde{O}(n^{11/12}/\\varepsilon^3)$ queries.","This is the first sublinear algorithm for this problem, which shows that, unlike halfspaces, testing is strictly easier than learning for decision lists.","Complementing the algorithm, we show that any distribution-free tester for decision lists must make $\\tilde{\\Omega}(\\sqrt{n})$ queries, or draw $\\tilde{\\Omega}(n)$ samples when the algorithm is sample-based."],"url":"http://arxiv.org/abs/2404.11103v1"}
{"created":"2024-04-17 06:36:17","title":"Synthesizing Realistic Data for Table Recognition","abstract":"To overcome the limitations and challenges of current automatic table data annotation methods and random table data synthesis approaches, we propose a novel method for synthesizing annotation data specifically designed for table recognition. This method utilizes the structure and content of existing complex tables, facilitating the efficient creation of tables that closely replicate the authentic styles found in the target domain. By leveraging the actual structure and content of tables from Chinese financial announcements, we have developed the first extensive table annotation dataset in this domain. We used this dataset to train several recent deep learning-based end-to-end table recognition models. Additionally, we have established the inaugural benchmark for real-world complex tables in the Chinese financial announcement domain, using it to assess the performance of models trained on our synthetic data, thereby effectively validating our method's practicality and effectiveness. Furthermore, we applied our synthesis method to augment the FinTabNet dataset, extracted from English financial announcements, by increasing the proportion of tables with multiple spanning cells to introduce greater complexity. Our experiments show that models trained on this augmented dataset achieve comprehensive improvements in performance, especially in the recognition of tables with multiple spanning cells.","sentences":["To overcome the limitations and challenges of current automatic table data annotation methods and random table data synthesis approaches, we propose a novel method for synthesizing annotation data specifically designed for table recognition.","This method utilizes the structure and content of existing complex tables, facilitating the efficient creation of tables that closely replicate the authentic styles found in the target domain.","By leveraging the actual structure and content of tables from Chinese financial announcements, we have developed the first extensive table annotation dataset in this domain.","We used this dataset to train several recent deep learning-based end-to-end table recognition models.","Additionally, we have established the inaugural benchmark for real-world complex tables in the Chinese financial announcement domain, using it to assess the performance of models trained on our synthetic data, thereby effectively validating our method's practicality and effectiveness.","Furthermore, we applied our synthesis method to augment the FinTabNet dataset, extracted from English financial announcements, by increasing the proportion of tables with multiple spanning cells to introduce greater complexity.","Our experiments show that models trained on this augmented dataset achieve comprehensive improvements in performance, especially in the recognition of tables with multiple spanning cells."],"url":"http://arxiv.org/abs/2404.11100v1"}
{"created":"2024-04-17 04:08:38","title":"LMEraser: Large Model Unlearning through Adaptive Prompt Tuning","abstract":"To address the growing demand for privacy protection in machine learning, we propose a novel and efficient machine unlearning approach for \\textbf{L}arge \\textbf{M}odels, called \\textbf{LM}Eraser. Existing unlearning research suffers from entangled training data and complex model architectures, incurring extremely high computational costs for large models. LMEraser takes a divide-and-conquer strategy with a prompt tuning architecture to isolate data influence. The training dataset is partitioned into public and private datasets. Public data are used to train the backbone of the model. Private data are adaptively clustered based on their diversity, and each cluster is used to optimize a prompt separately. This adaptive prompt tuning mechanism reduces unlearning costs and maintains model performance. Experiments demonstrate that LMEraser achieves a $100$-fold reduction in unlearning costs without compromising accuracy compared to prior work. Our code is available at: \\url{https://github.com/lmeraser/lmeraser}.","sentences":["To address the growing demand for privacy protection in machine learning, we propose a novel and efficient machine unlearning approach for \\textbf{L}arge \\textbf{M}odels, called \\textbf{LM}Eraser.","Existing unlearning research suffers from entangled training data and complex model architectures, incurring extremely high computational costs for large models.","LMEraser takes a divide-and-conquer strategy with a prompt tuning architecture to isolate data influence.","The training dataset is partitioned into public and private datasets.","Public data are used to train the backbone of the model.","Private data are adaptively clustered based on their diversity, and each cluster is used to optimize a prompt separately.","This adaptive prompt tuning mechanism reduces unlearning costs and maintains model performance.","Experiments demonstrate that LMEraser achieves a $100$-fold reduction in unlearning costs without compromising accuracy compared to prior work.","Our code is available at: \\url{https://github.com/lmeraser/lmeraser}."],"url":"http://arxiv.org/abs/2404.11056v1"}
{"created":"2024-04-17 03:51:55","title":"Supervised Contrastive Vision Transformer for Breast Histopathological Image Classification","abstract":"Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer. Breast tissue histopathological examination is critical in diagnosing and classifying breast cancer. Although existing methods have shown promising results, there is still room for improvement in the classification accuracy and generalization of IDC using histopathology images. We present a novel approach, Supervised Contrastive Vision Transformer (SupCon-ViT), for improving the classification of invasive ductal carcinoma in terms of accuracy and generalization by leveraging the inherent strengths and advantages of both transfer learning, i.e., pre-trained vision transformer, and supervised contrastive learning. Our results on a benchmark breast cancer dataset demonstrate that SupCon-Vit achieves state-of-the-art performance in IDC classification, with an F1-score of 0.8188, precision of 0.7692, and specificity of 0.8971, outperforming existing methods. In addition, the proposed model demonstrates resilience in scenarios with minimal labeled data, making it highly efficient in real-world clinical settings where labelled data is limited. Our findings suggest that supervised contrastive learning in conjunction with pre-trained vision transformers appears to be a viable strategy for an accurate classification of IDC, thus paving the way for a more efficient and reliable diagnosis of breast cancer through histopathological image analysis.","sentences":["Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer.","Breast tissue histopathological examination is critical in diagnosing and classifying breast cancer.","Although existing methods have shown promising results, there is still room for improvement in the classification accuracy and generalization of IDC using histopathology images.","We present a novel approach, Supervised Contrastive Vision Transformer (SupCon-ViT), for improving the classification of invasive ductal carcinoma in terms of accuracy and generalization by leveraging the inherent strengths and advantages of both transfer learning, i.e., pre-trained vision transformer, and supervised contrastive learning.","Our results on a benchmark breast cancer dataset demonstrate that SupCon-Vit achieves state-of-the-art performance in IDC classification, with an F1-score of 0.8188, precision of 0.7692, and specificity of 0.8971, outperforming existing methods.","In addition, the proposed model demonstrates resilience in scenarios with minimal labeled data, making it highly efficient in real-world clinical settings where labelled data is limited.","Our findings suggest that supervised contrastive learning in conjunction with pre-trained vision transformers appears to be a viable strategy for an accurate classification of IDC, thus paving the way for a more efficient and reliable diagnosis of breast cancer through histopathological image analysis."],"url":"http://arxiv.org/abs/2404.11052v1"}
{"created":"2024-04-17 03:42:48","title":"Lightweight Unsupervised Federated Learning with Pretrained Vision Language Model","abstract":"Federated learning aims to tackle the ``isolated data island\" problem, where it trains a collective model from physically isolated clients while safeguarding the privacy of users' data. However, supervised federated learning necessitates that each client labels their data for training, which can be both time-consuming and resource-intensive, and may even be impractical for edge devices. Moreover, the training and transmission of deep models present challenges to the computation and communication capabilities of the clients. To address these two inherent challenges in supervised federated learning, we propose a novel lightweight unsupervised federated learning approach that leverages unlabeled data on each client to perform lightweight model training and communication by harnessing pretrained vision-language models, such as CLIP. By capitalizing on the zero-shot prediction capability and the well-trained image encoder of the pre-trained CLIP model, we have carefully crafted an efficient and resilient self-training approach. This method refines the initial zero-shot predicted pseudo-labels of unlabeled instances through the sole training of a linear classifier on top of the fixed image encoder. Additionally, to address data heterogeneity within each client, we propose a class-balanced text feature sampling strategy for generating synthetic instances in the feature space to support local training. Experiments are conducted on multiple benchmark datasets. The experimental results demonstrate that our proposed method greatly enhances model performance in comparison to CLIP's zero-shot predictions and even outperforms supervised federated learning benchmark methods given limited computational and communication overhead.","sentences":["Federated learning aims to tackle the ``isolated data island\" problem, where it trains a collective model from physically isolated clients while safeguarding the privacy of users' data.","However, supervised federated learning necessitates that each client labels their data for training, which can be both time-consuming and resource-intensive, and may even be impractical for edge devices.","Moreover, the training and transmission of deep models present challenges to the computation and communication capabilities of the clients.","To address these two inherent challenges in supervised federated learning, we propose a novel lightweight unsupervised federated learning approach that leverages unlabeled data on each client to perform lightweight model training and communication by harnessing pretrained vision-language models, such as CLIP.","By capitalizing on the zero-shot prediction capability and the well-trained image encoder of the pre-trained CLIP model, we have carefully crafted an efficient and resilient self-training approach.","This method refines the initial zero-shot predicted pseudo-labels of unlabeled instances through the sole training of a linear classifier on top of the fixed image encoder.","Additionally, to address data heterogeneity within each client, we propose a class-balanced text feature sampling strategy for generating synthetic instances in the feature space to support local training.","Experiments are conducted on multiple benchmark datasets.","The experimental results demonstrate that our proposed method greatly enhances model performance in comparison to CLIP's zero-shot predictions and even outperforms supervised federated learning benchmark methods given limited computational and communication overhead."],"url":"http://arxiv.org/abs/2404.11046v1"}
{"created":"2024-04-17 03:39:51","title":"Offset Unlearning for Large Language Models","abstract":"Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.","sentences":["Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns.","In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data.","However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction.","We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs.","Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models.","Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks.","$\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs."],"url":"http://arxiv.org/abs/2404.11045v1"}
{"created":"2024-04-17 03:39:02","title":"Asynchronous Memory Access Unit: Exploiting Massive Parallelism for Far Memory Access","abstract":"The growing memory demands of modern applications have driven the adoption of far memory technologies in data centers to provide cost-effective, high-capacity memory solutions. However, far memory presents new performance challenges because its access latencies are significantly longer and more variable than local DRAM. For applications to achieve acceptable performance on far memory, a high degree of memory-level parallelism (MLP) is needed to tolerate the long access latency. While modern out-of-order processors are capable of exploiting a certain degree of MLP, they are constrained by resource limitations and hardware complexity. The key obstacle is the synchronous memory access semantics of traditional load/store instructions, which occupy critical hardware resources for a long time. The longer far memory latencies exacerbate this limitation.   This paper proposes a set of Asynchronous Memory Access Instructions (AMI) and its supporting function unit, Asynchronous Memory Access Unit (AMU), inside a contemporary Out-of-Order Core. AMI separates memory request issuing from response handling to reduce resource occupation. Additionally, AMU architecture supports up to several hundreds of asynchronous memory requests through re-purposing a portion of L2 Cache as scratchpad memory (SPM) to provide sufficient temporal storage. Together with a coroutine-based programming framework, this scheme can achieve significantly higher MLP for hiding far memory latencies.   Evaluation with a cycle-accurate simulation shows AMI achieves 2.42x speedup on average for memory-bound benchmarks with 1us additional far memory latency. Over 130 outstanding requests are supported with 26.86x speedup for GUPS (random access) with 5 us latency. These demonstrate how the techniques tackle far memory performance impacts through explicit MLP expression and latency adaptation.","sentences":["The growing memory demands of modern applications have driven the adoption of far memory technologies in data centers to provide cost-effective, high-capacity memory solutions.","However, far memory presents new performance challenges because its access latencies are significantly longer and more variable than local DRAM.","For applications to achieve acceptable performance on far memory, a high degree of memory-level parallelism (MLP) is needed to tolerate the long access latency.","While modern out-of-order processors are capable of exploiting a certain degree of MLP, they are constrained by resource limitations and hardware complexity.","The key obstacle is the synchronous memory access semantics of traditional load/store instructions, which occupy critical hardware resources for a long time.","The longer far memory latencies exacerbate this limitation.   ","This paper proposes a set of Asynchronous Memory Access Instructions (AMI) and its supporting function unit, Asynchronous Memory Access Unit (AMU), inside a contemporary Out-of-Order Core.","AMI separates memory request issuing from response handling to reduce resource occupation.","Additionally, AMU architecture supports up to several hundreds of asynchronous memory requests through re-purposing a portion of L2 Cache as scratchpad memory (SPM) to provide sufficient temporal storage.","Together with a coroutine-based programming framework, this scheme can achieve significantly higher MLP for hiding far memory latencies.   ","Evaluation with a cycle-accurate simulation shows AMI achieves 2.42x speedup on average for memory-bound benchmarks with 1us additional far memory latency.","Over 130 outstanding requests are supported with 26.86x speedup for GUPS (random access) with 5 us latency.","These demonstrate how the techniques tackle far memory performance impacts through explicit MLP expression and latency adaptation."],"url":"http://arxiv.org/abs/2404.11044v1"}
{"created":"2024-04-17 03:34:13","title":"The Effect of Defect (Re) Prediction on Software Testing","abstract":"Background: Cross-project defect prediction (CPDP) aims to use data from external projects as historical data may not be available from the same project. In CPDP, deciding on a particular historical project to build a training model can be difficult. To help with this decision, a Bandit Algorithm (BA) based approach has been proposed in prior research to select the most suitable learning project. However, this BA method could lead to the selection of unsuitable data during the early iteration of BA (i.e., early stage of software testing). Selecting an unsuitable model can reduce the prediction accuracy, leading to potential defect overlooking. This study aims to improve the BA method to reduce defects overlooking, especially during the early stage of testing. Once all modules have been tested, modules tested in the early stage are re-predicted, and some modules are retested based on the re-prediction. To assess the impact of re-prediction and retesting, we applied five kinds of BA methods, using 8, 16, and 32 OSS projects as learning data. The results show that the newly proposed approach steadily reduced the probability of defect overlooking on 86.7% of the BA methods and projects combinations.","sentences":["Background: Cross-project defect prediction (CPDP) aims to use data from external projects as historical data may not be available from the same project.","In CPDP, deciding on a particular historical project to build a training model can be difficult.","To help with this decision, a Bandit Algorithm (BA) based approach has been proposed in prior research to select the most suitable learning project.","However, this BA method could lead to the selection of unsuitable data during the early iteration of BA (i.e., early stage of software testing).","Selecting an unsuitable model can reduce the prediction accuracy, leading to potential defect overlooking.","This study aims to improve the BA method to reduce defects overlooking, especially during the early stage of testing.","Once all modules have been tested, modules tested in the early stage are re-predicted, and some modules are retested based on the re-prediction.","To assess the impact of re-prediction and retesting, we applied five kinds of BA methods, using 8, 16, and 32 OSS projects as learning data.","The results show that the newly proposed approach steadily reduced the probability of defect overlooking on 86.7% of the BA methods and projects combinations."],"url":"http://arxiv.org/abs/2404.11040v1"}
{"created":"2024-04-17 03:23:52","title":"Approximate Wireless Communication for Lossy Gradient Updates in IoT Federated Learning","abstract":"Federated learning (FL) has emerged as a distributed machine learning (ML) technique that can protect local data privacy for participating clients and improve system efficiency. Instead of sharing raw data, FL exchanges intermediate learning parameters, such as gradients, among clients. This article presents an efficient wireless communication approach tailored for FL parameter transmission, especially for Internet of Things (IoT) devices, to facilitate model aggregation. Our study considers practical wireless channels that can lead to random bit errors, which can substantially affect FL performance. Motivated by empirical gradient value distribution, we introduce a novel received bit masking method that confines received gradient values within prescribed limits. Moreover, given the intrinsic error resilience of ML gradients, our approach enables the delivery of approximate gradient values with errors without resorting to extensive error correction coding or retransmission. This strategy reduces computational overhead at both the transmitter and the receiver and minimizes communication latency. Consequently, our scheme is particularly well-suited for resource-constrained IoT devices. Additionally, we explore the inherent protection of the most significant bits (MSBs) through gray coding in high-order modulation. Our simulations demonstrate that our proposed scheme can effectively mitigate random bit errors in FL performance, achieving similar learning objectives, but with the 50% air time required by existing methods involving error correction and retransmission.","sentences":["Federated learning (FL) has emerged as a distributed machine learning (ML) technique that can protect local data privacy for participating clients and improve system efficiency.","Instead of sharing raw data, FL exchanges intermediate learning parameters, such as gradients, among clients.","This article presents an efficient wireless communication approach tailored for FL parameter transmission, especially for Internet of Things (IoT) devices, to facilitate model aggregation.","Our study considers practical wireless channels that can lead to random bit errors, which can substantially affect FL performance.","Motivated by empirical gradient value distribution, we introduce a novel received bit masking method that confines received gradient values within prescribed limits.","Moreover, given the intrinsic error resilience of ML gradients, our approach enables the delivery of approximate gradient values with errors without resorting to extensive error correction coding or retransmission.","This strategy reduces computational overhead at both the transmitter and the receiver and minimizes communication latency.","Consequently, our scheme is particularly well-suited for resource-constrained IoT devices.","Additionally, we explore the inherent protection of the most significant bits (MSBs) through gray coding in high-order modulation.","Our simulations demonstrate that our proposed scheme can effectively mitigate random bit errors in FL performance, achieving similar learning objectives, but with the 50% air time required by existing methods involving error correction and retransmission."],"url":"http://arxiv.org/abs/2404.11035v1"}
{"created":"2024-04-17 03:20:54","title":"Exploring the Path of Transformation and Development for Study Abroad Consultancy Firms in China","abstract":"In recent years, with the changing landscape of international education and the growing demand from Chinese students, study abroad consultancy firms in China need to adopt transformational development strategies to address challenges and maintain competitiveness. This study investigated the relationships between key performance indicators and several factors through a questionnaire survey of 158 consultancy firms. The factors examined included service diversification, technology adoption, talent management, and regulatory compliance. Descriptive statistical analysis was employed to analyze the data. The results showed that service scope diversification was positively correlated with firm performance. Technology adoption was positively correlated with operational efficiency. Talent management was positively correlated with service quality. Regulatory compliance was positively correlated with firm reputation. Consultancy firms that took progressive approaches in diversifying services, adopting new technologies, cultivating talent, and ensuring compliance demonstrated superior performance, efficiency, quality, and reputation compared to their less innovative counterparts. This research provides empirical evidence to support the transformation of Chinese study abroad consultancy firms. It also highlights the need for future studies to consider causality and contextual variations to gain deeper insights into this issue.","sentences":["In recent years, with the changing landscape of international education and the growing demand from Chinese students, study abroad consultancy firms in China need to adopt transformational development strategies to address challenges and maintain competitiveness.","This study investigated the relationships between key performance indicators and several factors through a questionnaire survey of 158 consultancy firms.","The factors examined included service diversification, technology adoption, talent management, and regulatory compliance.","Descriptive statistical analysis was employed to analyze the data.","The results showed that service scope diversification was positively correlated with firm performance.","Technology adoption was positively correlated with operational efficiency.","Talent management was positively correlated with service quality.","Regulatory compliance was positively correlated with firm reputation.","Consultancy firms that took progressive approaches in diversifying services, adopting new technologies, cultivating talent, and ensuring compliance demonstrated superior performance, efficiency, quality, and reputation compared to their less innovative counterparts.","This research provides empirical evidence to support the transformation of Chinese study abroad consultancy firms.","It also highlights the need for future studies to consider causality and contextual variations to gain deeper insights into this issue."],"url":"http://arxiv.org/abs/2404.11034v1"}
{"created":"2024-04-17 03:20:46","title":"Building Defect Prediction Models by Online Learning Considering Defect Overlooking","abstract":"Building defect prediction models based on online learning can enhance prediction accuracy. It continuously rebuilds a new prediction model, when a new data point is added. However, a module predicted as \"non-defective\" can result in fewer test cases for such modules. Thus, a defective module can be overlooked during testing. The erroneous test results are used as learning data by online learning, which could negatively affect prediction accuracy. To suppress the negative influence, we propose to apply a method that fixes the prediction as positive during the initial stage of online learning. Additionally, we improved the method to consider the probability of the overlooking. In our experiment, we demonstrate this negative influence on prediction accuracy, and the effectiveness of our approach. The results show that our approach did not negatively affect AUC but significantly improved recall.","sentences":["Building defect prediction models based on online learning can enhance prediction accuracy.","It continuously rebuilds a new prediction model, when a new data point is added.","However, a module predicted as \"non-defective\" can result in fewer test cases for such modules.","Thus, a defective module can be overlooked during testing.","The erroneous test results are used as learning data by online learning, which could negatively affect prediction accuracy.","To suppress the negative influence, we propose to apply a method that fixes the prediction as positive during the initial stage of online learning.","Additionally, we improved the method to consider the probability of the overlooking.","In our experiment, we demonstrate this negative influence on prediction accuracy, and the effectiveness of our approach.","The results show that our approach did not negatively affect AUC but significantly improved recall."],"url":"http://arxiv.org/abs/2404.11033v1"}
{"created":"2024-04-17 03:20:42","title":"CORE: Data Augmentation for Link Prediction via Information Bottleneck","abstract":"Link prediction (LP) is a fundamental task in graph representation learning, with numerous applications in diverse domains. However, the generalizability of LP models is often compromised due to the presence of noisy or spurious information in graphs and the inherent incompleteness of graph data. To address these challenges, we draw inspiration from the Information Bottleneck principle and propose a novel data augmentation method, COmplete and REduce (CORE) to learn compact and predictive augmentations for LP models. In particular, CORE aims to recover missing edges in graphs while simultaneously removing noise from the graph structures, thereby enhancing the model's robustness and performance. Extensive experiments on multiple benchmark datasets demonstrate the applicability and superiority of CORE over state-of-the-art methods, showcasing its potential as a leading approach for robust LP in graph representation learning.","sentences":["Link prediction (LP) is a fundamental task in graph representation learning, with numerous applications in diverse domains.","However, the generalizability of LP models is often compromised due to the presence of noisy or spurious information in graphs and the inherent incompleteness of graph data.","To address these challenges, we draw inspiration from the Information Bottleneck principle and propose a novel data augmentation method, COmplete and REduce (CORE) to learn compact and predictive augmentations for LP models.","In particular, CORE aims to recover missing edges in graphs while simultaneously removing noise from the graph structures, thereby enhancing the model's robustness and performance.","Extensive experiments on multiple benchmark datasets demonstrate the applicability and superiority of CORE over state-of-the-art methods, showcasing its potential as a leading approach for robust LP in graph representation learning."],"url":"http://arxiv.org/abs/2404.11032v1"}
{"created":"2024-04-17 03:01:47","title":"Spatial-Aware Image Retrieval: A Hyperdimensional Computing Approach for Efficient Similarity Hashing","abstract":"In the face of burgeoning image data, efficiently retrieving similar images poses a formidable challenge. Past research has focused on refining hash functions to distill images into compact indicators of resemblance. Initial attempts used shallow models, evolving to attention mechanism-based architectures from Convolutional Neural Networks (CNNs) to advanced models. Recognizing limitations in gradient-based models for spatial information embedding, we propose an innovative image hashing method, NeuroHash leveraging Hyperdimensional Computing (HDC). HDC symbolically encodes spatial information into high-dimensional vectors, reshaping image representation. Our approach combines pre-trained large vision models with HDC operations, enabling spatially encoded feature representations. Hashing with locality-sensitive hashing (LSH) ensures swift and efficient image retrieval. Notably, our framework allows dynamic hash manipulation for conditional image retrieval. Our work introduces a transformative image hashing framework enabling spatial-aware conditional retrieval. By seamlessly combining DNN-based neural and HDC-based symbolic models, our methodology breaks from traditional training, offering flexible and conditional image retrieval. Performance evaluations signify a paradigm shift in image-hashing methodologies, demonstrating enhanced retrieval accuracy.","sentences":["In the face of burgeoning image data, efficiently retrieving similar images poses a formidable challenge.","Past research has focused on refining hash functions to distill images into compact indicators of resemblance.","Initial attempts used shallow models, evolving to attention mechanism-based architectures from Convolutional Neural Networks (CNNs) to advanced models.","Recognizing limitations in gradient-based models for spatial information embedding, we propose an innovative image hashing method, NeuroHash leveraging Hyperdimensional Computing (HDC).","HDC symbolically encodes spatial information into high-dimensional vectors, reshaping image representation.","Our approach combines pre-trained large vision models with HDC operations, enabling spatially encoded feature representations.","Hashing with locality-sensitive hashing (LSH) ensures swift and efficient image retrieval.","Notably, our framework allows dynamic hash manipulation for conditional image retrieval.","Our work introduces a transformative image hashing framework enabling spatial-aware conditional retrieval.","By seamlessly combining DNN-based neural and HDC-based symbolic models, our methodology breaks from traditional training, offering flexible and conditional image retrieval.","Performance evaluations signify a paradigm shift in image-hashing methodologies, demonstrating enhanced retrieval accuracy."],"url":"http://arxiv.org/abs/2404.11025v1"}
{"created":"2024-04-17 02:52:11","title":"You do not have to train Graph Neural Networks at all on text-attributed graphs","abstract":"Graph structured data, specifically text-attributed graphs (TAG), effectively represent relationships among varied entities. Such graphs are essential for semi-supervised node classification tasks. Graph Neural Networks (GNNs) have emerged as a powerful tool for handling this graph-structured data. Although gradient descent is commonly utilized for training GNNs for node classification, this study ventures into alternative methods, eliminating the iterative optimization processes. We introduce TrainlessGNN, a linear GNN model capitalizing on the observation that text encodings from the same class often cluster together in a linear subspace. This model constructs a weight matrix to represent each class's node attribute subspace, offering an efficient approach to semi-supervised node classification on TAG. Extensive experiments reveal that our trainless models can either match or even surpass their conventionally trained counterparts, demonstrating the possibility of refraining from gradient descent in certain configurations.","sentences":["Graph structured data, specifically text-attributed graphs (TAG), effectively represent relationships among varied entities.","Such graphs are essential for semi-supervised node classification tasks.","Graph Neural Networks (GNNs) have emerged as a powerful tool for handling this graph-structured data.","Although gradient descent is commonly utilized for training GNNs for node classification, this study ventures into alternative methods, eliminating the iterative optimization processes.","We introduce TrainlessGNN, a linear GNN model capitalizing on the observation that text encodings from the same class often cluster together in a linear subspace.","This model constructs a weight matrix to represent each class's node attribute subspace, offering an efficient approach to semi-supervised node classification on TAG.","Extensive experiments reveal that our trainless models can either match or even surpass their conventionally trained counterparts, demonstrating the possibility of refraining from gradient descent in certain configurations."],"url":"http://arxiv.org/abs/2404.11019v1"}
{"created":"2024-04-17 02:46:59","title":"FedFa: A Fully Asynchronous Training Paradigm for Federated Learning","abstract":"Federated learning has been identified as an efficient decentralized training paradigm for scaling the machine learning model training on a large number of devices while guaranteeing the data privacy of the trainers. FedAvg has become a foundational parameter update strategy for federated learning, which has been promising to eliminate the effect of the heterogeneous data across clients and guarantee convergence. However, the synchronization parameter update barriers for each communication round during the training significant time on waiting, slowing down the training procedure. Therefore, recent state-of-the-art solutions propose using semi-asynchronous approaches to mitigate the waiting time cost with guaranteed convergence. Nevertheless, emerging semi-asynchronous approaches are unable to eliminate the waiting time completely.   We propose a full asynchronous training paradigm, called FedFa, which can guarantee model convergence and eliminate the waiting time completely for federated learning by using a few buffered results on the server for parameter updating. Further, we provide theoretical proof of the convergence rate for our proposed FedFa. Extensive experimental results indicate our approach effectively improves the training performance of federated learning by up to 6x and 4x speedup compared to the state-of-the-art synchronous and semi-asynchronous strategies while retaining high accuracy in both IID and Non-IID scenarios.","sentences":["Federated learning has been identified as an efficient decentralized training paradigm for scaling the machine learning model training on a large number of devices while guaranteeing the data privacy of the trainers.","FedAvg has become a foundational parameter update strategy for federated learning, which has been promising to eliminate the effect of the heterogeneous data across clients and guarantee convergence.","However, the synchronization parameter update barriers for each communication round during the training significant time on waiting, slowing down the training procedure.","Therefore, recent state-of-the-art solutions propose using semi-asynchronous approaches to mitigate the waiting time cost with guaranteed convergence.","Nevertheless, emerging semi-asynchronous approaches are unable to eliminate the waiting time completely.   ","We propose a full asynchronous training paradigm, called FedFa, which can guarantee model convergence and eliminate the waiting time completely for federated learning by using a few buffered results on the server for parameter updating.","Further, we provide theoretical proof of the convergence rate for our proposed FedFa.","Extensive experimental results indicate our approach effectively improves the training performance of federated learning by up to 6x and 4x speedup compared to the state-of-the-art synchronous and semi-asynchronous strategies while retaining high accuracy in both IID and Non-IID scenarios."],"url":"http://arxiv.org/abs/2404.11015v1"}
{"created":"2024-04-17 02:46:18","title":"Towards Multi-agent Reinforcement Learning based Traffic Signal Control through Spatio-temporal Hypergraphs","abstract":"Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow. Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control. To address this, we propose a novel TSCS framework to realize intelligent traffic control. This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network. To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm. Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the entire road network collectively. Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network. This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatial and temporal correlations between multiple intersections. Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance. This work facilitates the development of more intelligent and reactive urban traffic management solutions.","sentences":["Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow.","Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control.","To address this, we propose a novel TSCS framework to realize intelligent traffic control.","This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network.","To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm.","Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the entire road network collectively.","Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network.","This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatial and temporal correlations between multiple intersections.","Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance.","This work facilitates the development of more intelligent and reactive urban traffic management solutions."],"url":"http://arxiv.org/abs/2404.11014v1"}
{"created":"2024-04-17 02:36:02","title":"AKGNet: Attribute Knowledge-Guided Unsupervised Lung-Infected Area Segmentation","abstract":"Lung-infected area segmentation is crucial for assessing the severity of lung diseases. However, existing image-text multi-modal methods typically rely on labour-intensive annotations for model training, posing challenges regarding time and expertise. To address this issue, we propose a novel attribute knowledge-guided framework for unsupervised lung-infected area segmentation (AKGNet), which achieves segmentation solely based on image-text data without any mask annotation. AKGNet facilitates text attribute knowledge learning, attribute-image cross-attention fusion, and high-confidence-based pseudo-label exploration simultaneously. It can learn statistical information and capture spatial correlations between image and text attributes in the embedding space, iteratively refining the mask to enhance segmentation. Specifically, we introduce a text attribute knowledge learning module by extracting attribute knowledge and incorporating it into feature representations, enabling the model to learn statistical information and adapt to different attributes. Moreover, we devise an attribute-image cross-attention module by calculating the correlation between attributes and images in the embedding space to capture spatial dependency information, thus selectively focusing on relevant regions while filtering irrelevant areas. Finally, a self-training mask improvement process is employed by generating pseudo-labels using high-confidence predictions to iteratively enhance the mask and segmentation. Experimental results on a benchmark medical image dataset demonstrate the superior performance of our method compared to state-of-the-art segmentation techniques in unsupervised scenarios.","sentences":["Lung-infected area segmentation is crucial for assessing the severity of lung diseases.","However, existing image-text multi-modal methods typically rely on labour-intensive annotations for model training, posing challenges regarding time and expertise.","To address this issue, we propose a novel attribute knowledge-guided framework for unsupervised lung-infected area segmentation (AKGNet), which achieves segmentation solely based on image-text data without any mask annotation.","AKGNet facilitates text attribute knowledge learning, attribute-image cross-attention fusion, and high-confidence-based pseudo-label exploration simultaneously.","It can learn statistical information and capture spatial correlations between image and text attributes in the embedding space, iteratively refining the mask to enhance segmentation.","Specifically, we introduce a text attribute knowledge learning module by extracting attribute knowledge and incorporating it into feature representations, enabling the model to learn statistical information and adapt to different attributes.","Moreover, we devise an attribute-image cross-attention module by calculating the correlation between attributes and images in the embedding space to capture spatial dependency information, thus selectively focusing on relevant regions while filtering irrelevant areas.","Finally, a self-training mask improvement process is employed by generating pseudo-labels using high-confidence predictions to iteratively enhance the mask and segmentation.","Experimental results on a benchmark medical image dataset demonstrate the superior performance of our method compared to state-of-the-art segmentation techniques in unsupervised scenarios."],"url":"http://arxiv.org/abs/2404.11008v1"}
{"created":"2024-04-17 02:29:44","title":"InfoMatch: Entropy Neural Estimation for Semi-Supervised Image Classification","abstract":"Semi-supervised image classification, leveraging pseudo supervision and consistency regularization, has demonstrated remarkable success. However, the ongoing challenge lies in fully exploiting the potential of unlabeled data. To address this, we employ information entropy neural estimation to harness the potential of unlabeled samples. Inspired by contrastive learning, the entropy is estimated by maximizing a lower bound on mutual information across different augmented views. Moreover, we theoretically analyze that the information entropy of the posterior of an image classifier is approximated by maximizing the likelihood function of the softmax predictions. Guided by these insights, we optimize our model from both perspectives to ensure that the predicted probability distribution closely aligns with the ground-truth distribution. Given the theoretical connection to information entropy, we name our method \\textit{InfoMatch}. Through extensive experiments, we show its superior performance.","sentences":["Semi-supervised image classification, leveraging pseudo supervision and consistency regularization, has demonstrated remarkable success.","However, the ongoing challenge lies in fully exploiting the potential of unlabeled data.","To address this, we employ information entropy neural estimation to harness the potential of unlabeled samples.","Inspired by contrastive learning, the entropy is estimated by maximizing a lower bound on mutual information across different augmented views.","Moreover, we theoretically analyze that the information entropy of the posterior of an image classifier is approximated by maximizing the likelihood function of the softmax predictions.","Guided by these insights, we optimize our model from both perspectives to ensure that the predicted probability distribution closely aligns with the ground-truth distribution.","Given the theoretical connection to information entropy, we name our method \\textit{InfoMatch}.","Through extensive experiments, we show its superior performance."],"url":"http://arxiv.org/abs/2404.11003v1"}
{"created":"2024-04-17 02:18:48","title":"Machine-Learning-Enhanced Soft Robotic System Inspired by Rectal Functions for Investigating Fecal incontinence","abstract":"Fecal incontinence, arising from a myriad of pathogenic mechanisms, has attracted considerable global attention. Despite its significance, the replication of the defecatory system for studying fecal incontinence mechanisms remains limited largely due to social stigma and taboos. Inspired by the rectum's functionalities, we have developed a soft robotic system, encompassing a power supply, pressure sensing, data acquisition systems, a flushing mechanism, a stage, and a rectal module. The innovative soft rectal module includes actuators inspired by sphincter muscles, both soft and rigid covers, and soft rectum mold. The rectal mold, fabricated from materials that closely mimic human rectal tissue, is produced using the mold replication fabrication method. Both the soft and rigid components of the mold are realized through the application of 3D-printing technology. The sphincter muscles-inspired actuators featuring double-layer pouch structures are modeled and optimized based on multilayer perceptron methods aiming to obtain high contractions ratios (100%), high generated pressure (9.8 kPa), and small recovery time (3 s). Upon assembly, this defecation robot is capable of smoothly expelling liquid faeces, performing controlled solid fecal cutting, and defecating extremely solid long faeces, thus closely replicating the human rectum and anal canal's functions. This defecation robot has the potential to assist humans in understanding the complex defecation system and contribute to the development of well-being devices related to defecation.","sentences":["Fecal incontinence, arising from a myriad of pathogenic mechanisms, has attracted considerable global attention.","Despite its significance, the replication of the defecatory system for studying fecal incontinence mechanisms remains limited largely due to social stigma and taboos.","Inspired by the rectum's functionalities, we have developed a soft robotic system, encompassing a power supply, pressure sensing, data acquisition systems, a flushing mechanism, a stage, and a rectal module.","The innovative soft rectal module includes actuators inspired by sphincter muscles, both soft and rigid covers, and soft rectum mold.","The rectal mold, fabricated from materials that closely mimic human rectal tissue, is produced using the mold replication fabrication method.","Both the soft and rigid components of the mold are realized through the application of 3D-printing technology.","The sphincter muscles-inspired actuators featuring double-layer pouch structures are modeled and optimized based on multilayer perceptron methods aiming to obtain high contractions ratios (100%), high generated pressure (9.8 kPa), and small recovery time (3 s).","Upon assembly, this defecation robot is capable of smoothly expelling liquid faeces, performing controlled solid fecal cutting, and defecating extremely solid long faeces, thus closely replicating the human rectum and anal canal's functions.","This defecation robot has the potential to assist humans in understanding the complex defecation system and contribute to the development of well-being devices related to defecation."],"url":"http://arxiv.org/abs/2404.10999v1"}
{"created":"2024-04-17 02:17:23","title":"Online Algorithms with Limited Data Retention","abstract":"We introduce a model of online algorithms subject to strict constraints on data retention. An online learning algorithm encounters a stream of data points, one per round, generated by some stationary process. Crucially, each data point can request that it be removed from memory $m$ rounds after it arrives. To model the impact of removal, we do not allow the algorithm to store any information or calculations between rounds other than a subset of the data points (subject to the retention constraints). At the conclusion of the stream, the algorithm answers a statistical query about the full dataset. We ask: what level of performance can be guaranteed as a function of $m$?   We illustrate this framework for multidimensional mean estimation and linear regression problems. We show it is possible to obtain an exponential improvement over a baseline algorithm that retains all data as long as possible. Specifically, we show that $m = \\textsc{Poly}(d, \\log(1/\\epsilon))$ retention suffices to achieve mean squared error $\\epsilon$ after observing $O(1/\\epsilon)$ $d$-dimensional data points. This matches the error bound of the optimal, yet infeasible, algorithm that retains all data forever. We also show a nearly matching lower bound on the retention required to guarantee error $\\epsilon$. One implication of our results is that data retention laws are insufficient to guarantee the right to be forgotten even in a non-adversarial world in which firms merely strive to (approximately) optimize the performance of their algorithms.   Our approach makes use of recent developments in the multidimensional random subset sum problem to simulate the progression of stochastic gradient descent under a model of adversarial noise, which may be of independent interest.","sentences":["We introduce a model of online algorithms subject to strict constraints on data retention.","An online learning algorithm encounters a stream of data points, one per round, generated by some stationary process.","Crucially, each data point can request that it be removed from memory $m$ rounds after it arrives.","To model the impact of removal, we do not allow the algorithm to store any information or calculations between rounds other than a subset of the data points (subject to the retention constraints).","At the conclusion of the stream, the algorithm answers a statistical query about the full dataset.","We ask: what level of performance can be guaranteed as a function of $m$?   ","We illustrate this framework for multidimensional mean estimation and linear regression problems.","We show it is possible to obtain an exponential improvement over a baseline algorithm that retains all data as long as possible.","Specifically, we show that $m = \\textsc{Poly}(d, \\log(1/\\epsilon))$ retention suffices to achieve mean squared error $\\epsilon$ after observing $O(1/\\epsilon)$ $d$-dimensional data points.","This matches the error bound of the optimal, yet infeasible, algorithm that retains all data forever.","We also show a nearly matching lower bound on the retention required to guarantee error $\\epsilon$. One implication of our results is that data retention laws are insufficient to guarantee the right to be forgotten even in a non-adversarial world in which firms merely strive to (approximately) optimize the performance of their algorithms.   ","Our approach makes use of recent developments in the multidimensional random subset sum problem to simulate the progression of stochastic gradient descent under a model of adversarial noise, which may be of independent interest."],"url":"http://arxiv.org/abs/2404.10997v1"}
{"created":"2024-04-17 01:52:48","title":"From Paper to Platform: Evolution of a Novel Learning Environment for Tabletop Exercises","abstract":"For undergraduate students of computing, learning to solve complex practical problems in a team is an essential skill for their future careers. This skill is needed in various fields, such as in cybersecurity and IT governance. Tabletop exercises are an innovative teaching method used in practice for training teams in incident response and evaluation of contingency plans. However, tabletop exercises are not yet widely established in university education. This paper presents data and teaching experience from a cybersecurity course that introduces tabletop exercises in classrooms using a novel technology: INJECT Exercise Platform (IXP), a web-based learning environment for delivering and evaluating the exercises. This technology substantially improves the prior practice, since tabletop exercises worldwide have usually been conducted using pen and paper. Unlike in traditional tabletop exercises, which are difficult to evaluate manually, IXP provides insights into students' behavior and learning based on automated analysis of interaction data. We demonstrate IXP's capabilities and evolution by comparing exercise sessions hosted throughout three years at different stages of the platform's readiness. The analysis of student data is supplemented by the discussion of the lessons learned from employing IXP in computing education contexts. The data analytics enabled a detailed comparison of the teams' performance and behavior. Instructors who consider innovating their classes with tabletop exercises may use IXP and benefit from the insights in this paper.","sentences":["For undergraduate students of computing, learning to solve complex practical problems in a team is an essential skill for their future careers.","This skill is needed in various fields, such as in cybersecurity and IT governance.","Tabletop exercises are an innovative teaching method used in practice for training teams in incident response and evaluation of contingency plans.","However, tabletop exercises are not yet widely established in university education.","This paper presents data and teaching experience from a cybersecurity course that introduces tabletop exercises in classrooms using a novel technology: INJECT Exercise Platform (IXP), a web-based learning environment for delivering and evaluating the exercises.","This technology substantially improves the prior practice, since tabletop exercises worldwide have usually been conducted using pen and paper.","Unlike in traditional tabletop exercises, which are difficult to evaluate manually, IXP provides insights into students' behavior and learning based on automated analysis of interaction data.","We demonstrate IXP's capabilities and evolution by comparing exercise sessions hosted throughout three years at different stages of the platform's readiness.","The analysis of student data is supplemented by the discussion of the lessons learned from employing IXP in computing education contexts.","The data analytics enabled a detailed comparison of the teams' performance and behavior.","Instructors who consider innovating their classes with tabletop exercises may use IXP and benefit from the insights in this paper."],"url":"http://arxiv.org/abs/2404.10988v1"}
{"created":"2024-04-17 01:31:00","title":"Graph Continual Learning with Debiased Lossless Memory Replay","abstract":"Real-life graph data often expands continually, rendering the learning of graph neural networks (GNNs) on static graph data impractical. Graph continual learning (GCL) tackles this problem by continually adapting GNNs to the expanded graph of the current task while maintaining the performance over the graph of previous tasks. Memory replay-based methods, which aim to replay data of previous tasks when learning new tasks, have been explored as one principled approach to mitigate the forgetting of the knowledge learned from the previous tasks. In this paper we extend this methodology with a novel framework, called Debiased Lossless Memory replay (DeLoMe). Unlike existing methods that sample nodes/edges of previous graphs to construct the memory, DeLoMe learns small lossless synthetic node representations as the memory. The learned memory can not only preserve the graph data privacy but also capture the holistic graph information, for which the sampling-based methods are not viable. Further, prior methods suffer from bias toward the current task due to the data imbalance between the classes in the memory data and the current data. A debiased GCL loss function is devised in DeLoMe to effectively alleviate this bias. Extensive experiments on four graph datasets show the effectiveness of DeLoMe under both class- and task-incremental learning settings.","sentences":["Real-life graph data often expands continually, rendering the learning of graph neural networks (GNNs) on static graph data impractical.","Graph continual learning (GCL) tackles this problem by continually adapting GNNs to the expanded graph of the current task while maintaining the performance over the graph of previous tasks.","Memory replay-based methods, which aim to replay data of previous tasks when learning new tasks, have been explored as one principled approach to mitigate the forgetting of the knowledge learned from the previous tasks.","In this paper we extend this methodology with a novel framework, called Debiased Lossless Memory replay (DeLoMe).","Unlike existing methods that sample nodes/edges of previous graphs to construct the memory, DeLoMe learns small lossless synthetic node representations as the memory.","The learned memory can not only preserve the graph data privacy but also capture the holistic graph information, for which the sampling-based methods are not viable.","Further, prior methods suffer from bias toward the current task due to the data imbalance between the classes in the memory data and the current data.","A debiased GCL loss function is devised in DeLoMe to effectively alleviate this bias.","Extensive experiments on four graph datasets show the effectiveness of DeLoMe under both class- and task-incremental learning settings."],"url":"http://arxiv.org/abs/2404.10984v1"}
{"created":"2024-04-17 01:27:42","title":"A Survey on Retrieval-Augmented Text Generation for Large Language Models","abstract":"Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.","sentences":["Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information.","This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data.","As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint.","It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies.","Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions.","By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs."],"url":"http://arxiv.org/abs/2404.10981v1"}
{"created":"2024-04-17 01:26:15","title":"Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty","abstract":"Deep neural networks (DNNs) have been shown to perform well on exclusive, multi-class classification tasks. However, when different classes have similar visual features, it becomes challenging for human annotators to differentiate them. This scenario necessitates the use of composite class labels. In this paper, we propose a novel framework called Hyper-Evidential Neural Network (HENN) that explicitly models predictive uncertainty due to composite class labels in training data in the context of the belief theory called Subjective Logic (SL). By placing a grouped Dirichlet distribution on the class probabilities, we treat predictions of a neural network as parameters of hyper-subjective opinions and learn the network that collects both single and composite evidence leading to these hyper-opinions by a deterministic DNN from data. We introduce a new uncertainty type called vagueness originally designed for hyper-opinions in SL to quantify composite classification uncertainty for DNNs. Our results demonstrate that HENN outperforms its state-of-the-art counterparts based on four image datasets. The code and datasets are available at: https://github.com/Hugo101/HyperEvidentialNN.","sentences":["Deep neural networks (DNNs) have been shown to perform well on exclusive, multi-class classification tasks.","However, when different classes have similar visual features, it becomes challenging for human annotators to differentiate them.","This scenario necessitates the use of composite class labels.","In this paper, we propose a novel framework called Hyper-Evidential Neural Network (HENN) that explicitly models predictive uncertainty due to composite class labels in training data in the context of the belief theory called Subjective Logic (SL).","By placing a grouped Dirichlet distribution on the class probabilities, we treat predictions of a neural network as parameters of hyper-subjective opinions and learn the network that collects both single and composite evidence leading to these hyper-opinions by a deterministic DNN from data.","We introduce a new uncertainty type called vagueness originally designed for hyper-opinions in SL to quantify composite classification uncertainty for DNNs.","Our results demonstrate that HENN outperforms its state-of-the-art counterparts based on four image datasets.","The code and datasets are available at: https://github.com/Hugo101/HyperEvidentialNN."],"url":"http://arxiv.org/abs/2404.10980v1"}
{"created":"2024-04-17 01:23:49","title":"Leveraging 3D LiDAR Sensors to Enable Enhanced Urban Safety and Public Health: Pedestrian Monitoring and Abnormal Activity Detection","abstract":"The integration of Light Detection and Ranging (LiDAR) and Internet of Things (IoT) technologies offers transformative opportunities for public health informatics in urban safety and pedestrian well-being. This paper proposes a novel framework utilizing these technologies for enhanced 3D object detection and activity classification in urban traffic scenarios. By employing elevated LiDAR, we obtain detailed 3D point cloud data, enabling precise pedestrian activity monitoring. To overcome urban data scarcity, we create a specialized dataset through simulated traffic environments in Blender, facilitating targeted model training. Our approach employs a modified Point Voxel-Region-based Convolutional Neural Network (PV-RCNN) for robust 3D detection and PointNet for classifying pedestrian activities, significantly benefiting urban traffic management and public health by offering insights into pedestrian behavior and promoting safer urban environments. Our dual-model approach not only enhances urban traffic management but also contributes significantly to public health by providing insights into pedestrian behavior and promoting safer urban environment.","sentences":["The integration of Light Detection and Ranging (LiDAR) and Internet of Things (IoT) technologies offers transformative opportunities for public health informatics in urban safety and pedestrian well-being.","This paper proposes a novel framework utilizing these technologies for enhanced 3D object detection and activity classification in urban traffic scenarios.","By employing elevated LiDAR, we obtain detailed 3D point cloud data, enabling precise pedestrian activity monitoring.","To overcome urban data scarcity, we create a specialized dataset through simulated traffic environments in Blender, facilitating targeted model training.","Our approach employs a modified Point Voxel-Region-based Convolutional Neural Network (PV-RCNN) for robust 3D detection and PointNet for classifying pedestrian activities, significantly benefiting urban traffic management and public health by offering insights into pedestrian behavior and promoting safer urban environments.","Our dual-model approach not only enhances urban traffic management but also contributes significantly to public health by providing insights into pedestrian behavior and promoting safer urban environment."],"url":"http://arxiv.org/abs/2404.10978v1"}
{"created":"2024-04-17 00:47:41","title":"Integrated Communication, Navigation, and Remote Sensing in LEO Networks with Vehicular Applications","abstract":"Traditionally, communication, navigation, and remote sensing (CNR) satellites are separately performed, leading to resource waste, information isolation, and independent optimization for each functionality. Taking future automated driving as an example, it faces great challenges in providing high-reliable and low-latency lane-level positioning, decimeter-level transportation observation, and huge traffic sensing information downloading. To this end, this article proposes an integrated CNR (ICNR) framework based on low earth orbit (LEO) satellite mega-constellations from the perspective of vehicular applications. After introducing the main working principles of the CNR functionalities to serve as the technological basis, we characterize the potentials of the integration gain in vehicular use cases. Then, we investigate the ICNR framework in different integration levels, which sheds strong light on qualitative performance improvement by sophisticatedly sharing orbit constellation, wireless resource, and data information towards meeting the requirements of vehicular applications. We also instantiate a fundamental numerical case study to demonstrate the integration gain and highlight the main tradeoffs in managing the ICNR networks from the perspective of vehicular applications.","sentences":["Traditionally, communication, navigation, and remote sensing (CNR) satellites are separately performed, leading to resource waste, information isolation, and independent optimization for each functionality.","Taking future automated driving as an example, it faces great challenges in providing high-reliable and low-latency lane-level positioning, decimeter-level transportation observation, and huge traffic sensing information downloading.","To this end, this article proposes an integrated CNR (ICNR) framework based on low earth orbit (LEO) satellite mega-constellations from the perspective of vehicular applications.","After introducing the main working principles of the CNR functionalities to serve as the technological basis, we characterize the potentials of the integration gain in vehicular use cases.","Then, we investigate the ICNR framework in different integration levels, which sheds strong light on qualitative performance improvement by sophisticatedly sharing orbit constellation, wireless resource, and data information towards meeting the requirements of vehicular applications.","We also instantiate a fundamental numerical case study to demonstrate the integration gain and highlight the main tradeoffs in managing the ICNR networks from the perspective of vehicular applications."],"url":"http://arxiv.org/abs/2404.10969v1"}
{"created":"2024-04-17 00:21:36","title":"Domain-Specific Block Selection and Paired-View Pseudo-Labeling for Online Test-Time Adaptation","abstract":"Test-time adaptation (TTA) aims to adapt a pre-trained model to a new test domain without access to source data after deployment. Existing approaches typically rely on self-training with pseudo-labels since ground-truth cannot be obtained from test data. Although the quality of pseudo labels is important for stable and accurate long-term adaptation, it has not been previously addressed. In this work, we propose DPLOT, a simple yet effective TTA framework that consists of two components: (1) domain-specific block selection and (2) pseudo-label generation using paired-view images. Specifically, we select blocks that involve domain-specific feature extraction and train these blocks by entropy minimization. After blocks are adjusted for current test domain, we generate pseudo-labels by averaging given test images and corresponding flipped counterparts. By simply using flip augmentation, we prevent a decrease in the quality of the pseudo-labels, which can be caused by the domain gap resulting from strong augmentation. Our experimental results demonstrate that DPLOT outperforms previous TTA methods in CIFAR10-C, CIFAR100-C, and ImageNet-C benchmarks, reducing error by up to 5.4%, 9.1%, and 2.9%, respectively. Also, we provide an extensive analysis to demonstrate effectiveness of our framework. Code is available at https://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA.","sentences":["Test-time adaptation (TTA) aims to adapt a pre-trained model to a new test domain without access to source data after deployment.","Existing approaches typically rely on self-training with pseudo-labels since ground-truth cannot be obtained from test data.","Although the quality of pseudo labels is important for stable and accurate long-term adaptation, it has not been previously addressed.","In this work, we propose DPLOT, a simple yet effective TTA framework that consists of two components: (1) domain-specific block selection and (2) pseudo-label generation using paired-view images.","Specifically, we select blocks that involve domain-specific feature extraction and train these blocks by entropy minimization.","After blocks are adjusted for current test domain, we generate pseudo-labels by averaging given test images and corresponding flipped counterparts.","By simply using flip augmentation, we prevent a decrease in the quality of the pseudo-labels, which can be caused by the domain gap resulting from strong augmentation.","Our experimental results demonstrate that DPLOT outperforms previous TTA methods in CIFAR10-C, CIFAR100-C, and ImageNet-C benchmarks, reducing error by up to 5.4%, 9.1%, and 2.9%, respectively.","Also, we provide an extensive analysis to demonstrate effectiveness of our framework.","Code is available at https://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA."],"url":"http://arxiv.org/abs/2404.10966v1"}
{"created":"2024-04-17 00:09:41","title":"Drawing Competitive Districts in Redistricting","abstract":"In the process of redistricting, one important metric is the number of competitive districts, that is, districts where both parties have a reasonable chance of winning a majority of votes. Competitive districts are important for achieving proportionality, responsiveness, and other desirable qualities; some states even directly list competitiveness in their legally-codified districting requirements. In this work, we discuss the problem of drawing plans with at least a fixed number of competitive districts. In addition to the standard, ``vote-band'' measure of competitivenesss (i.e., how close was the last election?), we propose a measure that explicitly considers ``swing voters'' - the segment of the population that may choose to vote either way, or not vote at all, in a given election. We present two main, contrasting results. First, from a computational complexity perspective, we show that the task of drawing plans with competitive districts is NP-hard, even on very natural instances where the districting task itself is easy (e.g., small rectangular grids of population-balanced cells). Second, however, we show that a simple hill-climbing procedure can in practice find districtings on real states in which all the districts are competitive. We present the results of the latter on the precinct-level graphs of the U.S. states of North Carolina and Arizona, and discuss trade-offs between competitiveness and other desirable qualities.","sentences":["In the process of redistricting, one important metric is the number of competitive districts, that is, districts where both parties have a reasonable chance of winning a majority of votes.","Competitive districts are important for achieving proportionality, responsiveness, and other desirable qualities; some states even directly list competitiveness in their legally-codified districting requirements.","In this work, we discuss the problem of drawing plans with at least a fixed number of competitive districts.","In addition to the standard, ``vote-band'' measure of competitivenesss (i.e., how close was the last election?), we propose a measure that explicitly considers ``swing voters'' - the segment of the population that may choose to vote either way, or not vote at all, in a given election.","We present two main, contrasting results.","First, from a computational complexity perspective, we show that the task of drawing plans with competitive districts is NP-hard, even on very natural instances where the districting task itself is easy (e.g., small rectangular grids of population-balanced cells).","Second, however, we show that a simple hill-climbing procedure can in practice find districtings on real states in which all the districts are competitive.","We present the results of the latter on the precinct-level graphs of the U.S. states of North Carolina and Arizona, and discuss trade-offs between competitiveness and other desirable qualities."],"url":"http://arxiv.org/abs/2404.10964v1"}
{"created":"2024-04-16 23:54:55","title":"On approximability of the Permanent of PSD matrices","abstract":"We study the complexity of approximating the permanent of a positive semidefinite matrix $A\\in \\mathbb{C}^{n\\times n}$.   1. We design a new approximation algorithm for $\\mathrm{per}(A)$ with approximation ratio $e^{(0.9999 + \\gamma)n}$, exponentially improving upon the current best bound of $e^{(1+\\gamma-o(1))n}$ [AGOS17,YP22]. Here, $\\gamma \\approx 0.577$ is Euler's constant.   2. We prove that it is NP-hard to approximate $\\mathrm{per}(A)$ within a factor $e^{(\\gamma-\\epsilon)n}$ for any $\\epsilon>0$. This is the first exponential hardness of approximation for this problem. Along the way, we prove optimal hardness of approximation results for the $\\|\\cdot\\|_{2\\to q}$ ``norm'' problem of a matrix for all $-1 < q < 2$.","sentences":["We study the complexity of approximating the permanent of a positive semidefinite matrix $A\\in \\mathbb{C}^{n\\times n}$.   1.","We design a new approximation algorithm for $\\mathrm{per}(A)$ with approximation ratio $e^{(0.9999 + \\gamma)n}$, exponentially improving upon the current best bound of $e^{(1+\\gamma-o(1))n}$","[AGOS17,YP22].","Here, $\\gamma \\approx 0.577$ is Euler's constant.   ","2.","We prove that it is NP-hard to approximate $\\mathrm{per}(A)$ within a factor $e^{(\\gamma-\\epsilon)n}$ for any $\\epsilon>0$. This is the first exponential hardness of approximation for this problem.","Along the way, we prove optimal hardness of approximation results for the $\\|\\cdot\\|_{2\\to q}$ ``norm'' problem of a matrix for all $-1 < q < 2$."],"url":"http://arxiv.org/abs/2404.10959v1"}
{"created":"2024-04-16 23:47:23","title":"Personalized Federated Learning via Stacking","abstract":"Traditional Federated Learning (FL) methods typically train a single global model collaboratively without exchanging raw data. In contrast, Personalized Federated Learning (PFL) techniques aim to create multiple models that are better tailored to individual clients' data. We present a novel personalization approach based on stacked generalization where clients directly send each other privacy-preserving models to be used as base models to train a meta-model on private data. Our approach is flexible, accommodating various privacy-preserving techniques and model types, and can be applied in horizontal, hybrid, and vertically partitioned federations. Additionally, it offers a natural mechanism for assessing each client's contribution to the federation. Through comprehensive evaluations across diverse simulated data heterogeneity scenarios, we showcase the effectiveness of our method.","sentences":["Traditional Federated Learning (FL) methods typically train a single global model collaboratively without exchanging raw data.","In contrast, Personalized Federated Learning (PFL) techniques aim to create multiple models that are better tailored to individual clients' data.","We present a novel personalization approach based on stacked generalization where clients directly send each other privacy-preserving models to be used as base models to train a meta-model on private data.","Our approach is flexible, accommodating various privacy-preserving techniques and model types, and can be applied in horizontal, hybrid, and vertically partitioned federations.","Additionally, it offers a natural mechanism for assessing each client's contribution to the federation.","Through comprehensive evaluations across diverse simulated data heterogeneity scenarios, we showcase the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.10957v1"}
{"created":"2024-04-16 23:34:34","title":"The Traveling Tournament Problem: Improved Algorithms Based on Cycle Packing","abstract":"The Traveling Tournament Problem (TTP) is a well-known benchmark problem in the field of tournament timetabling, which asks us to design a double round-robin schedule such that each pair of teams plays one game in each other's home venue, minimizing the total distance traveled by all $n$ teams ($n$ is even). TTP-$k$ is the problem with one more constraint that each team can have at most $k$-consecutive home games or away games. In this paper, we investigate schedules for TTP-$k$ and analyze the approximation ratio of the solutions. Most previous schedules were constructed based on a Hamiltonian cycle of the graph. We will propose a novel construction based on a $k$-cycle packing. Then, combining our $k$-cycle packing schedule with the Hamiltonian cycle schedule, we obtain improved approximation ratios for TTP-$k$ with deep analysis. The case where $k=3$, TTP-3, is one of the most investigated cases. We improve the approximation ratio of TTP-3 from $(1.667+\\varepsilon)$ to $(1.598+\\varepsilon)$, for any $\\varepsilon>0$. For TTP-$4$, we improve the approximation ratio from $(1.750+\\varepsilon)$ to $(1.700+\\varepsilon)$. By a refined analysis of the Hamiltonian cycle construction, we also improve the approximation ratio of TTP-$k$ from $(\\frac{5k-7}{2k}+\\varepsilon)$ to $(\\frac{5k^2-4k+3}{2k(k+1)}+\\varepsilon)$ for any constant $k\\geq 5$. Our methods can be extended to solve a variant called LDTTP-$k$ (TTP-$k$ where all teams are allocated on a straight line). We show that the $k$-cycle packing construction can achieve an approximation ratio of $(\\frac{3k-3}{2k-1}+\\varepsilon)$, which improves the approximation ratio of LDTTP-3 from $4/3$ to $(6/5+\\varepsilon)$.","sentences":["The Traveling Tournament Problem (TTP) is a well-known benchmark problem in the field of tournament timetabling, which asks us to design a double round-robin schedule such that each pair of teams plays one game in each other's home venue, minimizing the total distance traveled by all $n$ teams ($n$ is even).","TTP-$k$ is the problem with one more constraint that each team can have at most $k$-consecutive home games or away games.","In this paper, we investigate schedules for TTP-$k$ and analyze the approximation ratio of the solutions.","Most previous schedules were constructed based on a Hamiltonian cycle of the graph.","We will propose a novel construction based on a $k$-cycle packing.","Then, combining our $k$-cycle packing schedule with the Hamiltonian cycle schedule, we obtain improved approximation ratios for TTP-$k$ with deep analysis.","The case where $k=3$, TTP-3, is one of the most investigated cases.","We improve the approximation ratio of TTP-3 from $(1.667+\\varepsilon)$ to $(1.598+\\varepsilon)$, for any $\\varepsilon>0$. For TTP-$4$, we improve the approximation ratio from $(1.750+\\varepsilon)$ to $(1.700+\\varepsilon)$. By a refined analysis of the Hamiltonian cycle construction, we also improve the approximation ratio of TTP-$k$ from $(\\frac{5k-7}{2k}+\\varepsilon)$ to $(\\frac{5k^2-4k+3}{2k(k+1)}+\\varepsilon)$ for any constant $k\\geq 5$. Our methods can be extended to solve a variant called LDTTP-$k$ (TTP-$k$ where all teams are allocated on a straight line).","We show that the $k$-cycle packing construction can achieve an approximation ratio of $(\\frac{3k-3}{2k-1}+\\varepsilon)$, which improves the approximation ratio of LDTTP-3 from $4/3$ to $(6/5+\\varepsilon)$."],"url":"http://arxiv.org/abs/2404.10955v1"}
