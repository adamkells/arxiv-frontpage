{"created":"2024-01-24 18:58:35","title":"Predicting the Impact of Crashes Across Release Channels","abstract":"Software maintenance faces a persistent challenge with crash bugs, especially across diverse release channels catering to distinct user bases. Nightly builds, favoured by enthusiasts, often reveal crashes that are cheaper to fix but may differ significantly from those in stable releases. In this paper, we emphasize the need for a data-driven solution to predict the impact of crashes happening on nightly channels once they are released to stable channels. We also list the challenges that need to be considered when approaching this problem.","sentences":["Software maintenance faces a persistent challenge with crash bugs, especially across diverse release channels catering to distinct user bases.","Nightly builds, favoured by enthusiasts, often reveal crashes that are cheaper to fix but may differ significantly from those in stable releases.","In this paper, we emphasize the need for a data-driven solution to predict the impact of crashes happening on nightly channels once they are released to stable channels.","We also list the challenges that need to be considered when approaching this problem."],"url":"http://arxiv.org/abs/2401.13667v1"}
{"created":"2024-01-24 18:49:30","title":"Inadequacy of common stochastic neural networks for reliable clinical decision support","abstract":"Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such as Bayesian neural network layers and model ensembles. Our models achieve state of the art performance in terms of discrimination performance (AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality prediction benchmark. However, epistemic uncertainty is critically underestimated by the selected stochastic deep learning methods. A heuristic proof for the responsible collapse of the posterior distribution is provided. Our findings reveal the inadequacy of commonly used stochastic deep learning approaches to reliably recognize OoD samples. In both methods, unsubstantiated model confidence is not prevented due to strongly biased functional posteriors, rendering them inappropriate for reliable clinical decision support. This highlights the need for approaches with more strictly enforced or inherent distance-awareness to known data points, e.g., using kernel-based techniques.","sentences":["Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns.","For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy.","Common deep learning approaches, however, have the tendency towards overconfidence under data shift.","Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences.","This highlights the importance of reliable estimation of local uncertainty and its communication to the end user.","While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications.","We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study.","For predictions on the EHR time series, Encoder-Only Transformer models were employed.","Stochasticity of model functions was achieved by incorporating common methods such as Bayesian neural network layers and model ensembles.","Our models achieve state of the art performance in terms of discrimination performance (AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality prediction benchmark.","However, epistemic uncertainty is critically underestimated by the selected stochastic deep learning methods.","A heuristic proof for the responsible collapse of the posterior distribution is provided.","Our findings reveal the inadequacy of commonly used stochastic deep learning approaches to reliably recognize OoD samples.","In both methods, unsubstantiated model confidence is not prevented due to strongly biased functional posteriors, rendering them inappropriate for reliable clinical decision support.","This highlights the need for approaches with more strictly enforced or inherent distance-awareness to known data points, e.g., using kernel-based techniques."],"url":"http://arxiv.org/abs/2401.13657v2"}
{"created":"2024-01-24 18:35:21","title":"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks","abstract":"Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.","sentences":["Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks.","However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve.","Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively.","To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}.","VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents.","To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives.","We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models.","Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents.","VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.","Our code, baseline models, and data is publicly available at https://jykoh.com/vwa."],"url":"http://arxiv.org/abs/2401.13649v1"}
{"created":"2024-01-24 18:28:52","title":"Employing polyhedral methods to optimize stencils on FPGAs with stencil-specific caches, data reuse, and wide data bursts","abstract":"It is well known that to accelerate stencil codes on CPUs or GPUs and to exploit hardware caches and their lines optimizers must find spatial and temporal locality of array accesses to harvest data-reuse opportunities. On FPGAs there is the burden that there are no built-in caches (or only pre-built hardware descriptions for cache blocks that are inefficient for stencil codes). But this paper demonstrates that this lack is also a chance as polyhedral methods can be used to generate stencil-specific cache-structures of the right sizes on the FPGA and to fill and flush them efficiently with wide bursts during stencil execution. The paper shows how to derive the appropriate directives and code restructurings from stencil codes so that the FPGA compiler generates fast stencil hardware. Switching on our optimization improves the runtime of a set of 10 stencils by between 43x and 156x.","sentences":["It is well known that to accelerate stencil codes on CPUs or GPUs and to exploit hardware caches and their lines optimizers must find spatial and temporal locality of array accesses to harvest data-reuse opportunities.","On FPGAs there is the burden that there are no built-in caches (or only pre-built hardware descriptions for cache blocks that are inefficient for stencil codes).","But this paper demonstrates that this lack is also a chance as polyhedral methods can be used to generate stencil-specific cache-structures of the right sizes on the FPGA and to fill and flush them efficiently with wide bursts during stencil execution.","The paper shows how to derive the appropriate directives and code restructurings from stencil codes so that the FPGA compiler generates fast stencil hardware.","Switching on our optimization improves the runtime of a set of 10 stencils by between 43x and 156x."],"url":"http://arxiv.org/abs/2401.13645v1"}
{"created":"2024-01-24 17:59:00","title":"Enabling Seamless Data Security, Consensus, and Trading in Vehicular Networks","abstract":"Cooperative driving is an emerging paradigm to enhance the safety and efficiency of autonomous vehicles. To ensure successful cooperation, road users must reach a consensus for making collective decisions, while recording vehicular data to analyze and address failures related to such agreements. This data has the potential to provide valuable insights into various vehicular events, while also potentially improving accountability measures. Furthermore, vehicles may benefit from the ability to negotiate and trade services among themselves, adding value to the cooperative driving framework. However, the majority of proposed systems aiming to ensure data security, consensus, or service trading, lack efficient and thoroughly validated mechanisms that consider the distinctive characteristics of vehicular networks. These limitations are amplified by a dependency on the centralized support provided by the infrastructure. Furthermore, corresponding mechanisms must diligently address security concerns, especially regarding potential malicious or misbehaving nodes, while also considering inherent constraints of the wireless medium. We introduce the Verifiable Event Extension (VEE), an applicational extension designed for Intelligent Transportation System (ITS) messages. The VEE operates seamlessly with any existing standardized vehicular communications protocol, addressing crucial aspects of data security, consensus, and trading with minimal overhead. To achieve this, we employ blockchain techniques, Byzantine fault tolerance (BFT) consensus protocols, and cryptocurrency-based mechanics. To assess our proposal's feasibility and lightweight nature, we employed a hardware-in-the-loop setup for analysis. Experimental results demonstrate the viability and efficiency of the VEE extension in overcoming the challenges posed by the distributed and opportunistic nature of wireless vehicular communications.","sentences":["Cooperative driving is an emerging paradigm to enhance the safety and efficiency of autonomous vehicles.","To ensure successful cooperation, road users must reach a consensus for making collective decisions, while recording vehicular data to analyze and address failures related to such agreements.","This data has the potential to provide valuable insights into various vehicular events, while also potentially improving accountability measures.","Furthermore, vehicles may benefit from the ability to negotiate and trade services among themselves, adding value to the cooperative driving framework.","However, the majority of proposed systems aiming to ensure data security, consensus, or service trading, lack efficient and thoroughly validated mechanisms that consider the distinctive characteristics of vehicular networks.","These limitations are amplified by a dependency on the centralized support provided by the infrastructure.","Furthermore, corresponding mechanisms must diligently address security concerns, especially regarding potential malicious or misbehaving nodes, while also considering inherent constraints of the wireless medium.","We introduce the Verifiable Event Extension (VEE), an applicational extension designed for Intelligent Transportation System (ITS) messages.","The VEE operates seamlessly with any existing standardized vehicular communications protocol, addressing crucial aspects of data security, consensus, and trading with minimal overhead.","To achieve this, we employ blockchain techniques, Byzantine fault tolerance (BFT) consensus protocols, and cryptocurrency-based mechanics.","To assess our proposal's feasibility and lightweight nature, we employed a hardware-in-the-loop setup for analysis.","Experimental results demonstrate the viability and efficiency of the VEE extension in overcoming the challenges posed by the distributed and opportunistic nature of wireless vehicular communications."],"url":"http://arxiv.org/abs/2401.13630v1"}
{"created":"2024-01-24 17:52:24","title":"What Makes a Great Software Quality Assurance Engineer?","abstract":"Software Quality Assurance (SQA) Engineers are responsible for assessing a product during every phase of the software development process to ensure that the outcomes of each phase and the final product possess the desired qualities. In general, a great SQA engineer needs to have a different set of abilities from development engineers to effectively oversee the entire product development process from beginning to end. Recent empirical studies identified important attributes of software engineers and managers, but the quality assurance role is overlooked. As software quality aspects have become more of a priority in the life cycle of software development, employers seek professionals that best suit the company's objectives and new graduates desire to make a valuable contribution through their job as an SQA engineer, but what makes them great? We addressed this knowledge gap by conducting 25 semi-structured interviews and 363 survey respondents with software quality assurance engineers from different companies around the world. We use the data collected from these activities to derive a comprehensive set of attributes that are considered important. As a result of the interviews, twenty-five attributes were identified and grouped into five main categories: personal, social, technical, management, and decision-making attributes. Through a rating survey, we confirmed that the distinguishing characteristics of great SQA engineers are curiosity, the ability to communicate effectively, and critical thinking skills. This work will guide further studies with SQA practitioners, by considering contextual factors and providing some implications for research and practice.","sentences":["Software Quality Assurance (SQA) Engineers are responsible for assessing a product during every phase of the software development process to ensure that the outcomes of each phase and the final product possess the desired qualities.","In general, a great SQA engineer needs to have a different set of abilities from development engineers to effectively oversee the entire product development process from beginning to end.","Recent empirical studies identified important attributes of software engineers and managers, but the quality assurance role is overlooked.","As software quality aspects have become more of a priority in the life cycle of software development, employers seek professionals that best suit the company's objectives and new graduates desire to make a valuable contribution through their job as an SQA engineer, but what makes them great?","We addressed this knowledge gap by conducting 25 semi-structured interviews and 363 survey respondents with software quality assurance engineers from different companies around the world.","We use the data collected from these activities to derive a comprehensive set of attributes that are considered important.","As a result of the interviews, twenty-five attributes were identified and grouped into five main categories: personal, social, technical, management, and decision-making attributes.","Through a rating survey, we confirmed that the distinguishing characteristics of great SQA engineers are curiosity, the ability to communicate effectively, and critical thinking skills.","This work will guide further studies with SQA practitioners, by considering contextual factors and providing some implications for research and practice."],"url":"http://arxiv.org/abs/2401.13623v1"}
{"created":"2024-01-24 17:48:45","title":"DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning","abstract":"Contrastive-learning-based methods have dominated sentence representation learning. These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks. However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples. In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective. By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form. Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in comparison to contrastive-learning-based methods. Notably, the proposed intra-sentence denoising objective complements existing inter-sentence contrastive methodologies and can be integrated with them to further enhance performance. Our code is available at https://github.com/xinghaow99/DenoSent.","sentences":["Contrastive-learning-based methods have dominated sentence representation learning.","These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks.","However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples.","In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective.","By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form.","Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in comparison to contrastive-learning-based methods.","Notably, the proposed intra-sentence denoising objective complements existing inter-sentence contrastive methodologies and can be integrated with them to further enhance performance.","Our code is available at https://github.com/xinghaow99/DenoSent."],"url":"http://arxiv.org/abs/2401.13621v1"}
{"created":"2024-01-24 17:35:11","title":"Intermittent Connectivity Maintenance With Heterogeneous Robots","abstract":"We consider a scenario of cooperative task servicing, with a team of heterogeneous robots with different maximum speeds and communication radii, in charge of keeping the network intermittently connected. We abstract the task locations into a $1D$ cycle graph that is traversed by the communicating robots, and we discuss intermittent communication strategies so that each task location is periodically visited, with a worst--case revisiting time. Robots move forward and backward along the cycle graph, exchanging data with their previous and next neighbors when they meet, and updating their region boundaries. Asymptotically, each robot is in charge of a region of the cycle graph, depending on its capabilities. The method is distributed, and robots only exchange data when they meet.","sentences":["We consider a scenario of cooperative task servicing, with a team of heterogeneous robots with different maximum speeds and communication radii, in charge of keeping the network intermittently connected.","We abstract the task locations into a $1D$ cycle graph that is traversed by the communicating robots, and we discuss intermittent communication strategies so that each task location is periodically visited, with a worst--case revisiting time.","Robots move forward and backward along the cycle graph, exchanging data with their previous and next neighbors when they meet, and updating their region boundaries.","Asymptotically, each robot is in charge of a region of the cycle graph, depending on its capabilities.","The method is distributed, and robots only exchange data when they meet."],"url":"http://arxiv.org/abs/2401.13612v1"}
{"created":"2024-01-24 17:31:07","title":"Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models","abstract":"Neural networks have been successfully used for non-intrusive speech intelligibility prediction. Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task. This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users. Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7.","sentences":["Neural networks have been successfully used for non-intrusive speech intelligibility prediction.","Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task.","This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users.","Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7."],"url":"http://arxiv.org/abs/2401.13611v1"}
{"created":"2024-01-24 17:27:08","title":"Building Contextual Knowledge Graphs for Personalized Learning Recommendations using Text Mining and Semantic Graph Completion","abstract":"Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms. This leads to a foundation for personalized recommendations of learning paths. In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated. We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model. We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. The results show that the relations in the KG are semantically comparable to those defined by domain experts, and that the proposed KG improves representing and linking the contexts of LOs through increasing graph communities and betweenness centrality.","sentences":["Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective.","While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms.","This leads to a foundation for personalized recommendations of learning paths.","In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated.","We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model.","We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones.","The results show that the relations in the KG are semantically comparable to those defined by domain experts, and that the proposed KG improves representing and linking the contexts of LOs through increasing graph communities and betweenness centrality."],"url":"http://arxiv.org/abs/2401.13609v1"}
{"created":"2024-01-24 17:14:50","title":"Stream-based perception for cognitive agents in mobile ecosystems","abstract":"Cognitive agent abstractions can help to engineer intelligent systems across mobile devices. On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation. Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data. Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation. In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts. In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams. We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations. We show how situations derived from smartphone sensor data can trigger and guide auctions, which the agents use to reach agreements. Experiments with real smartphone data demonstrate the benefits of stream-based agent perception.","sentences":["Cognitive agent abstractions can help to engineer intelligent systems across mobile devices.","On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation.","Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data.","Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation.","In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts.","In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams.","We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations.","We show how situations derived from smartphone sensor data can trigger and guide auctions, which the agents use to reach agreements.","Experiments with real smartphone data demonstrate the benefits of stream-based agent perception."],"url":"http://arxiv.org/abs/2401.13604v1"}
{"created":"2024-01-24 17:04:28","title":"Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction","abstract":"Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines.","sentences":["Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document.","Existing methods heavily rely on a substantial amount of fully labeled data.","However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive.","Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations.","In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK.","Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step.","To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge.","Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets.","We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets.","The experimental results illustrate that our GenRDK framework outperforms strong baselines."],"url":"http://arxiv.org/abs/2401.13598v1"}
{"created":"2024-01-24 17:01:42","title":"Graph Guided Question Answer Generation for Procedural Question-Answering","abstract":"In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our data achieve excellent performance on the target QA task, even exceeding that of GPT3 and ChatGPT despite being several orders of magnitude smaller. 2) semantic coverage is the key indicator for downstream QA performance. Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model. In contrast, the higher semantic coverage provided by our method is critical for QA performance.","sentences":["In this paper, we focus on task-specific question answering (QA).","To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants.","The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data.","While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model.","In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs.","We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner.","Comprehensive evaluations of our method show that: 1) small models trained with our data achieve excellent performance on the target QA task, even exceeding that of GPT3 and ChatGPT despite being several orders of magnitude smaller.","2) semantic coverage is the key indicator for downstream QA performance.","Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model.","In contrast, the higher semantic coverage provided by our method is critical for QA performance."],"url":"http://arxiv.org/abs/2401.13594v1"}
{"created":"2024-01-24 16:52:37","title":"Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes","abstract":"The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance. However, their performance in actual clinical applications has been underexplored. Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts. This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings. Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication. Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes. Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians. Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an in-depth analysis. Results: GPT-4 showed overall superior performance compared to other LLMs. In contrast, both GPT-3.5 and text-davinci-003 exhibit enhanced performance when the appropriate prompting strategies are employed. The GPT family models have demonstrated considerable efficiency, evidenced by their cost-effectiveness and time-saving capabilities. Conclusion: A comprehensive qualitative performance evaluation framework for LLMs is developed and operationalized. This framework goes beyond singular performance aspects. With expert annotations, this methodology not only validates LLMs' capabilities in processing complex medical data but also establishes a benchmark for future LLM evaluations across specialized domains.","sentences":["The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance.","However, their performance in actual clinical applications has been underexplored.","Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts.","This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings.","Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication.","Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes.","Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians.","Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an in-depth analysis.","Results:","GPT-4 showed overall superior performance compared to other LLMs.","In contrast, both GPT-3.5 and text-davinci-003 exhibit enhanced performance when the appropriate prompting strategies are employed.","The GPT family models have demonstrated considerable efficiency, evidenced by their cost-effectiveness and time-saving capabilities.","Conclusion: A comprehensive qualitative performance evaluation framework for LLMs is developed and operationalized.","This framework goes beyond singular performance aspects.","With expert annotations, this methodology not only validates LLMs' capabilities in processing complex medical data but also establishes a benchmark for future LLM evaluations across specialized domains."],"url":"http://arxiv.org/abs/2401.13588v1"}
{"created":"2024-01-24 16:43:35","title":"WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition","abstract":"This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack. In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference. Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods. In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings. We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions. Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram; 2) trigger generation; 3) the generation of the poisoned dataset. Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods. Besides, we also provide visualization analyses to explain why our method works.","sentences":["This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack.","In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference.","Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods.","In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings.","We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions.","Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram; 2) trigger generation; 3) the generation of the poisoned dataset.","Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods.","Besides, we also provide visualization analyses to explain why our method works."],"url":"http://arxiv.org/abs/2401.13578v1"}
{"created":"2024-01-24 16:30:21","title":"SPARC-LoRa: A Scalable, Power-efficient, Affordable, Reliable, and Cloud Service-enabled LoRa Networking System for Agriculture Applications","abstract":"With the rapid development of cloud and edge computing, Internet of Things (IoT) applications have been deployed in various aspects of human life. In this paper, we design and implement a holistic LoRa-based IoT system with LoRa communication capabilities, named SPARC-LoRa, which consists of field sensor nodes and a gateway connected to the Internet. SPARC-LoRa has the following important features. First, the proposed wireless network of SPARC-LoRa is even-driven and using off-the-shelf microcontroller and LoRa communication modules with a customized PCB design to integrate all the hardware. This enables SPARC-LoRa to achieve low power consumption, long range communication, and low cost. With a new connection-based upper layer protocol design, the scalability and communication reliability of SPARC-loRa can be achieved. Second, an open source software including sensor nodes and servers is designed based on Docker container with cloud storage, computing, and LTE functionalities. In order to achieve reliable wireless communication under extreme conditions, a relay module is designed and applied to SPARC-LoRa to forward the data from sensor nodes to the gateway node. The system design and implementation is completely open source and hosted on the DigitalOcean Droplet Cloud. Hence, the proposed system enables further research and applications in both academia and industry. The proposed system has been tested in real fields under different and extreme environmental conditions in Salt Lake City, Utah and the University of Nebraska-Lincoln. The experimental results validate the features of SPARC-LoRa including low power, reliability, and cloud services provided by SPARC-LoRa.","sentences":["With the rapid development of cloud and edge computing, Internet of Things (IoT) applications have been deployed in various aspects of human life.","In this paper, we design and implement a holistic LoRa-based IoT system with LoRa communication capabilities, named SPARC-LoRa, which consists of field sensor nodes and a gateway connected to the Internet.","SPARC-LoRa has the following important features.","First, the proposed wireless network of SPARC-LoRa is even-driven and using off-the-shelf microcontroller and LoRa communication modules with a customized PCB design to integrate all the hardware.","This enables SPARC-LoRa to achieve low power consumption, long range communication, and low cost.","With a new connection-based upper layer protocol design, the scalability and communication reliability of SPARC-loRa can be achieved.","Second, an open source software including sensor nodes and servers is designed based on Docker container with cloud storage, computing, and LTE functionalities.","In order to achieve reliable wireless communication under extreme conditions, a relay module is designed and applied to SPARC-LoRa to forward the data from sensor nodes to the gateway node.","The system design and implementation is completely open source and hosted on the DigitalOcean Droplet Cloud.","Hence, the proposed system enables further research and applications in both academia and industry.","The proposed system has been tested in real fields under different and extreme environmental conditions in Salt Lake City, Utah and the University of Nebraska-Lincoln.","The experimental results validate the features of SPARC-LoRa including low power, reliability, and cloud services provided by SPARC-LoRa."],"url":"http://arxiv.org/abs/2401.13569v1"}
{"created":"2024-01-24 16:23:14","title":"A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in Recommendation","abstract":"When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups. In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results. Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists. In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models. This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity. Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility. Source code and pre-processed data can be retrieved at https://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure.","sentences":["When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups.","In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results.","Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists.","In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models.","This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity.","Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility.","Source code and pre-processed data can be retrieved at https://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure."],"url":"http://arxiv.org/abs/2401.13566v1"}
{"created":"2024-01-24 16:11:42","title":"Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection","abstract":"Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other. In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly. Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process. To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields. For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data. We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner. Experiments demonstrate that the proposed UVAD method outperforms previous approaches.","sentences":["Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other.","In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly.","Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process.","To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD.","The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields.","For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data.","We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner.","Experiments demonstrate that the proposed UVAD method outperforms previous approaches."],"url":"http://arxiv.org/abs/2401.13551v1"}
