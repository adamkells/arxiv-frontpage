{"created":"2024-01-11 18:59:53","title":"Distilling Vision-Language Models on Millions of Videos","abstract":"The recent advance in vision-language models is largely attributed to the abundance of image-text data. We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available. We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data. The resulting video-language model is then used to auto-label millions of videos to generate high-quality captions. We show the adapted video-language model performs well on a wide range of video-language benchmarks. For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods. Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models. Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%.","sentences":["The recent advance in vision-language models is largely attributed to the abundance of image-text data.","We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available.","We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data.","The resulting video-language model is then used to auto-label millions of videos to generate high-quality captions.","We show the adapted video-language model performs well on a wide range of video-language benchmarks.","For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%.","Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods.","Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models.","Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%."],"url":"http://arxiv.org/abs/2401.06129v1"}
{"created":"2024-01-11 18:59:14","title":"E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation","abstract":"One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch. Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model. Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time. Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept.","sentences":["One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs).","This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models.","However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts.","In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient?","To achieve this goal, we propose a series of innovative techniques.","First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch.","Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model.","Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time.","Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept."],"url":"http://arxiv.org/abs/2401.06127v1"}
{"created":"2024-01-11 18:59:12","title":"Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors","abstract":"Visual dubbing is the process of generating lip motions of an actor in a video to synchronise with given audio. Recent advances have made progress towards this goal but have not been able to produce an approach suitable for mass adoption. Existing methods are split into either person-generic or person-specific models. Person-specific models produce results almost indistinguishable from reality but rely on long training times using large single-person datasets. Person-generic works have allowed for the visual dubbing of any video to any audio without further training, but these fail to capture the person-specific nuances and often suffer from visual artefacts. Our method, based on data-efficient neural rendering priors, overcomes the limitations of existing approaches. Our pipeline consists of learning a deferred neural rendering prior network and actor-specific adaptation using neural textures. This method allows for $\\textbf{high-quality visual dubbing with just a few seconds of data}$, that enables video dubbing for any actor - from A-list celebrities to background actors. We show that we achieve state-of-the-art in terms of $\\textbf{visual quality}$ and $\\textbf{recognisability}$ both quantitatively, and qualitatively through two user studies. Our prior learning and adaptation method $\\textbf{generalises to limited data}$ better and is more $\\textbf{scalable}$ than existing person-specific models. Our experiments on real-world, limited data scenarios find that our model is preferred over all others. The project page may be found at https://dubbingforeveryone.github.io/","sentences":["Visual dubbing is the process of generating lip motions of an actor in a video to synchronise with given audio.","Recent advances have made progress towards this goal but have not been able to produce an approach suitable for mass adoption.","Existing methods are split into either person-generic or person-specific models.","Person-specific models produce results almost indistinguishable from reality but rely on long training times using large single-person datasets.","Person-generic works have allowed for the visual dubbing of any video to any audio without further training, but these fail to capture the person-specific nuances and often suffer from visual artefacts.","Our method, based on data-efficient neural rendering priors, overcomes the limitations of existing approaches.","Our pipeline consists of learning a deferred neural rendering prior network and actor-specific adaptation using neural textures.","This method allows for $\\textbf{high-quality visual dubbing with just a few seconds of data}$, that enables video dubbing for any actor - from A-list celebrities to background actors.","We show that we achieve state-of-the-art in terms of $\\textbf{visual quality}$ and $\\textbf{recognisability}$ both quantitatively, and qualitatively through two user studies.","Our prior learning and adaptation method $\\textbf{generalises to limited data}$ better and is more $\\textbf{scalable}$ than existing person-specific models.","Our experiments on real-world, limited data scenarios find that our model is preferred over all others.","The project page may be found at https://dubbingforeveryone.github.io/"],"url":"http://arxiv.org/abs/2401.06126v1"}
{"created":"2024-01-11 18:57:12","title":"TOFU: A Task of Fictitious Unlearning for LLMs","abstract":"Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.","sentences":["Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns.","Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training.","Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place.","To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning.","We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning.","We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy.","Finally, we provide a set of baseline results from existing unlearning algorithms.","Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."],"url":"http://arxiv.org/abs/2401.06121v1"}
{"created":"2024-01-11 18:40:58","title":"Holey graphs: very large Betti numbers are testable","abstract":"We show that the graph property of having a (very) large $k$-th Betti number $\\beta_k$ for constant $k$ is testable with a constant number of queries in the dense graph model. More specifically, we consider a clique complex defined by an underlying graph and prove that for any $\\varepsilon>0$, there exists $\\delta(\\varepsilon,k)>0$ such that testing whether $\\beta_k \\geq (1-\\delta) d_k$ for $\\delta \\leq \\delta(\\varepsilon,k)$ reduces to tolerantly testing $(k+2)$-clique-freeness, which is known to be testable. This complements a result by Elek (2010) showing that Betti numbers are testable in the bounded-degree model. Our result combines the Euler characteristic, matroid theory and the graph removal lemma.","sentences":["We show that the graph property of having a (very) large $k$-th Betti number $\\beta_k$ for constant $k$ is testable with a constant number of queries in the dense graph model.","More specifically, we consider a clique complex defined by an underlying graph and prove that for any $\\varepsilon>0$, there exists $\\delta(\\varepsilon,k)>0$ such that testing whether $\\beta_k \\geq (1-\\delta) d_k$ for $\\delta \\leq \\delta(\\varepsilon,k)$ reduces to tolerantly testing $(k+2)$-clique-freeness, which is known to be testable.","This complements a result by Elek (2010) showing that Betti numbers are testable in the bounded-degree model.","Our result combines the Euler characteristic, matroid theory and the graph removal lemma."],"url":"http://arxiv.org/abs/2401.06109v1"}
{"created":"2024-01-11 18:08:56","title":"PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage Clustering on GPU","abstract":"This paper presents \\pandora, a novel parallel algorithm for efficiently constructing dendrograms for single-linkage hierarchical clustering, including \\hdbscan. Traditional dendrogram construction methods from a minimum spanning tree (MST), such as agglomerative or divisive techniques, often fail to efficiently parallelize, especially with skewed dendrograms common in real-world data.   \\pandora addresses these challenges through a unique recursive tree contraction method, which simplifies the tree for initial dendrogram construction and then progressively reconstructs the complete dendrogram. This process makes \\pandora asymptotically work-optimal, independent of dendrogram skewness. All steps in \\pandora are fully parallel and suitable for massively threaded accelerators such as GPUs.   Our implementation is written in Kokkos, providing support for both CPUs and multi-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \\pandora is 2.2$\\times$ faster than the current best-multithreaded implementation, while the GPU \\pandora implementation achieved 6-20$\\times$ on \\amdgpu and 10-37$\\times$ on \\nvidiagpu speed-up over multithreaded \\pandora. These advancements lead to up to a 6-fold speedup for \\hdbscan on GPUs over the current best, which only offload MST construction to GPUs and perform multithreaded dendrogram construction.","sentences":["This paper presents \\pandora, a novel parallel algorithm for efficiently constructing dendrograms for single-linkage hierarchical clustering, including \\hdbscan.","Traditional dendrogram construction methods from a minimum spanning tree (MST), such as agglomerative or divisive techniques, often fail to efficiently parallelize, especially with skewed dendrograms common in real-world data.   ","\\pandora addresses these challenges through a unique recursive tree contraction method, which simplifies the tree for initial dendrogram construction and then progressively reconstructs the complete dendrogram.","This process makes \\pandora asymptotically work-optimal, independent of dendrogram skewness.","All steps in \\pandora are fully parallel and suitable for massively threaded accelerators such as GPUs.   ","Our implementation is written in Kokkos, providing support for both CPUs and multi-vendor GPUs (e.g., Nvidia, AMD).","The multithreaded version of \\pandora is 2.2$\\times$ faster than the current best-multithreaded implementation, while the GPU \\pandora implementation achieved 6-20$\\times$ on \\amdgpu and 10-37$\\times$ on \\nvidiagpu speed-up over multithreaded \\pandora.","These advancements lead to up to a 6-fold speedup for \\hdbscan on GPUs over the current best, which only offload MST construction to GPUs and perform multithreaded dendrogram construction."],"url":"http://arxiv.org/abs/2401.06089v1"}
{"created":"2024-01-11 18:06:30","title":"Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models","abstract":"The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT-4. We evaluate the models' performance based on the perplexity score, modified BERTScore, and cosine similarity score. The results show that BioGPT-Large exhibits superior performance compared to the other models. It consistently achieves a remarkably low perplexity score of 1.65 when generating CC, whereas the baseline LSTM model achieves the best perplexity score of 170. Further, we evaluate and assess the proposed models' performance and the outcome of GPT-4.0. Our study demonstrates that utilizing LLMs such as BioGPT, leads to the development of an effective autocompletion tool for generating CC documentation in healthcare settings.","sentences":["The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care.","It provides critical information for healthcare providers to make informed decisions about patient care.","However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments.","To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses.","In this study, we utilized text generation techniques to develop machine learning models using CC data.","In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.","Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT-4.","We evaluate the models' performance based on the perplexity score, modified BERTScore, and cosine similarity score.","The results show that BioGPT-Large exhibits superior performance compared to the other models.","It consistently achieves a remarkably low perplexity score of 1.65 when generating CC, whereas the baseline LSTM model achieves the best perplexity score of 170.","Further, we evaluate and assess the proposed models' performance and the outcome of GPT-4.0.","Our study demonstrates that utilizing LLMs such as BioGPT, leads to the development of an effective autocompletion tool for generating CC documentation in healthcare settings."],"url":"http://arxiv.org/abs/2401.06088v1"}
{"created":"2024-01-11 18:03:17","title":"XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange","abstract":"We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races. We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents. After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation. Our initial findings presented here show that XGBoost trained in this way can indeed learn profitable betting strategies, and can generalise to learn strategies that outperform each of the set of strategies used for creation of the training data. To foster further research and enhancements, the complete version of our extended BBE, including the XGBoost integration, has been made freely available as an open-source release on GitHub.","sentences":["We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races.","We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents.","After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation.","Our initial findings presented here show that XGBoost trained in this way can indeed learn profitable betting strategies, and can generalise to learn strategies that outperform each of the set of strategies used for creation of the training data.","To foster further research and enhancements, the complete version of our extended BBE, including the XGBoost integration, has been made freely available as an open-source release on GitHub."],"url":"http://arxiv.org/abs/2401.06086v1"}
{"created":"2024-01-11 17:58:41","title":"Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint","abstract":"Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named \\textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach. Our code and data are available at \\url{https://github.com/RUCAIBox/RLMEC}.","sentences":["Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and errors.","However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness.","To address it, we propose a new RL method named \\textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training.","Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process.","And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens.","The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach.","Our code and data are available at \\url{https://github.com/RUCAIBox/RLMEC}."],"url":"http://arxiv.org/abs/2401.06081v1"}
{"created":"2024-01-11 17:56:59","title":"Secrets of RLHF in Large Language Models Part II: Reward Modeling","abstract":"Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.   In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental results confirm that data with varying preference strengths have different impacts on reward model performance. We introduce a series of novel methods to mitigate the influence of incorrect and ambiguous preferences in the dataset and fully leverage high-quality preference data. (2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization. Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses.","Reward models are trained as proxies for human preferences to drive reinforcement learning optimization.","While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent.","(2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.   ","In this report, we attempt to address these two issues.","(1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models.","Experimental results confirm that data with varying preference strengths have different impacts on reward model performance.","We introduce a series of novel methods to mitigate the influence of incorrect and ambiguous preferences in the dataset and fully leverage high-quality preference data.","(2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization.","Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization."],"url":"http://arxiv.org/abs/2401.06080v1"}
{"created":"2024-01-11 17:42:47","title":"Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion","abstract":"Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge. Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain. We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines. Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities. We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results. We also carry out sufficient ablation experiments to explore the key influencing factors when LLMs perform structured temporal knowledge inference tasks.","sentences":["Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge.","Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain.","We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines.","Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities.","We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results.","We also carry out sufficient ablation experiments to explore the key influencing factors when LLMs perform structured temporal knowledge inference tasks."],"url":"http://arxiv.org/abs/2401.06072v1"}
{"created":"2024-01-11 17:41:57","title":"LEGO:Language Enhanced Multi-modal Grounding Model","abstract":"Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification and localization of specific regions in images or moments in videos. To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training. The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/LEGO.","sentences":["Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities.","However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities.","Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding.","To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks.","In this paper, we propose LEGO, a language enhanced multi-modal grounding model.","Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input.","It demonstrates precise identification and localization of specific regions in images or moments in videos.","To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training.","The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/LEGO."],"url":"http://arxiv.org/abs/2401.06071v1"}
{"created":"2024-01-11 17:24:49","title":"Investigating Data Contamination for Pre-training Language Models","abstract":"Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}. We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies.","sentences":["Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks.","However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance.","There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks.","In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}.","We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data.","We also investigate the effects of repeating contamination for various downstream tasks.","Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy.","Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies."],"url":"http://arxiv.org/abs/2401.06059v1"}
{"created":"2024-01-11 17:20:34","title":"MatSynth: A Modern PBR Materials Dataset","abstract":"We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation and acquisition. However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications. The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth.","sentences":["We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR materials.","Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries.","Given their importance, significant research effort was dedicated to their representation, creation and acquisition.","However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials.","With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available.","We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications.","The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings.","The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth."],"url":"http://arxiv.org/abs/2401.06056v1"}
{"created":"2024-01-11 17:07:47","title":"Computing Data Distribution from Query Selectivities","abstract":"We are given a set $\\mathcal{Z}=\\{(R_1,s_1),\\ldots, (R_n,s_n)\\}$, where each $R_i$ is a \\emph{range} in $\\Re^d$, such as rectangle or ball, and $s_i \\in [0,1]$ denotes its \\emph{selectivity}. The goal is to compute a small-size \\emph{discrete data distribution} $\\mathcal{D}=\\{(q_1,w_1),\\ldots, (q_m,w_m)\\}$, where $q_j\\in \\Re^d$ and $w_j\\in [0,1]$ for each $1\\leq j\\leq m$, and $\\sum_{1\\leq j\\leq m}w_j= 1$, such that $\\mathcal{D}$ is the most \\emph{consistent} with $\\mathcal{Z}$, i.e., $\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})=\\frac{1}{n}\\sum_{i=1}^n\\! \\lvert{s_i-\\sum_{j=1}^m w_j\\cdot 1(q_j\\in R_i)}\\rvert^p$ is minimized. In a database setting, $\\mathcal{Z}$ corresponds to a workload of range queries over some table, together with their observed selectivities (i.e., fraction of tuples returned), and $\\mathcal{D}$ can be used as compact model for approximating the data distribution within the table without accessing the underlying contents.   In this paper, we obtain both upper and lower bounds for this problem. In particular, we show that the problem of finding the best data distribution from selectivity queries is $\\mathsf{NP}$-complete. On the positive side, we describe a Monte Carlo algorithm that constructs, in time $O((n+\\delta^{-d})\\delta^{-2}\\mathop{\\mathrm{polylog}})$, a discrete distribution $\\tilde{\\mathcal{D}}$ of size $O(\\delta^{-2})$, such that $\\mathrm{err}_p(\\tilde{\\mathcal{D}},\\mathcal{Z})\\leq \\min_{\\mathcal{D}}\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})+\\delta$ (for $p=1,2,\\infty$) where the minimum is taken over all discrete distributions. We also establish conditional lower bounds, which strongly indicate the infeasibility of relative approximations as well as removal of the exponential dependency on the dimension for additive approximations. This suggests that significant improvements to our algorithm are unlikely.","sentences":["We are given a set $\\mathcal{Z}=\\{(R_1,s_1),\\ldots, (R_n,s_n)\\}$, where each $R_i$ is a \\emph{range} in $\\Re^d$, such as rectangle or ball, and $s_i \\in","[0,1]$ denotes its \\emph{selectivity}.","The goal is to compute a small-size \\emph{discrete data distribution} $\\mathcal{D}=\\{(q_1,w_1),\\ldots, (q_m,w_m)\\}$, where $q_j\\in \\Re^d$ and $w_j\\in","[0,1]$ for each $1\\leq j\\leq m$, and $\\sum_{1\\leq j\\leq m}w_j= 1$, such that $\\mathcal{D}$ is the most \\emph{consistent} with $\\mathcal{Z}$, i.e., $\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})=\\frac{1}{n}\\sum_{i=1}^n\\! \\lvert{s_i-\\sum_{j=1}^m w_j\\cdot 1(q_j\\in R_i)}\\rvert^p$ is minimized.","In a database setting, $\\mathcal{Z}$ corresponds to a workload of range queries over some table, together with their observed selectivities (i.e., fraction of tuples returned), and $\\mathcal{D}$ can be used as compact model for approximating the data distribution within the table without accessing the underlying contents.   ","In this paper, we obtain both upper and lower bounds for this problem.","In particular, we show that the problem of finding the best data distribution from selectivity queries is $\\mathsf{NP}$-complete.","On the positive side, we describe a Monte Carlo algorithm that constructs, in time $O((n+\\delta^{-d})\\delta^{-2}\\mathop{\\mathrm{polylog}})$, a discrete distribution $\\tilde{\\mathcal{D}}$ of size $O(\\delta^{-2})$, such that $\\mathrm{err}_p(\\tilde{\\mathcal{D}},\\mathcal{Z})\\leq \\min_{\\mathcal{D}}\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})+\\delta$ (for $p=1,2,\\infty$) where the minimum is taken over all discrete distributions.","We also establish conditional lower bounds, which strongly indicate the infeasibility of relative approximations as well as removal of the exponential dependency on the dimension for additive approximations.","This suggests that significant improvements to our algorithm are unlikely."],"url":"http://arxiv.org/abs/2401.06047v1"}
{"created":"2024-01-11 16:55:48","title":"Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting","abstract":"Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the information from all streams for traffic metrics reconstruction and prediction. Furthermore, road-network-informed graphs and data-driven graph learning are combined to accurately capture spatial correlation. The proposed method can offer well-defined interpretability, powerful learning capability, and competitive forecasting performance on real-world traffic data sets.","sentences":["Traffic forecasting is the foundation for intelligent transportation systems.","Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting.","However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale.","To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method.","In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the information from all streams for traffic metrics reconstruction and prediction.","Furthermore, road-network-informed graphs and data-driven graph learning are combined to accurately capture spatial correlation.","The proposed method can offer well-defined interpretability, powerful learning capability, and competitive forecasting performance on real-world traffic data sets."],"url":"http://arxiv.org/abs/2401.06040v1"}
{"created":"2024-01-11 16:43:16","title":"GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model","abstract":"Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN","sentences":["Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio.","Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios.","The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications.","However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods.","Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation.","Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency.","The main approach is via optimising the training process of the generator parameters.","With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models.","Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms.","The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm.","The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN"],"url":"http://arxiv.org/abs/2401.06031v1"}
{"created":"2024-01-11 16:42:10","title":"Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation","abstract":"Model adaptation tackles the distribution shift problem with a pre-trained model instead of raw data, becoming a popular paradigm due to its great privacy protection. Existing methods always assume adapting to a clean target domain, overlooking the security risks of unlabeled samples. In this paper, we explore the potential backdoor attacks on model adaptation launched by well-designed poisoning target data. Concretely, we provide two backdoor triggers with two poisoning strategies for different prior knowledge owned by attackers. These attacks achieve a high success rate and keep the normal performance on clean samples in the test stage. To defend against backdoor embedding, we propose a plug-and-play method named MixAdapt, combining it with existing adaptation algorithms. Experiments across commonly used benchmarks and adaptation methods demonstrate the effectiveness of MixAdapt. We hope this work will shed light on the safety of learning with unlabeled data.","sentences":["Model adaptation tackles the distribution shift problem with a pre-trained model instead of raw data, becoming a popular paradigm due to its great privacy protection.","Existing methods always assume adapting to a clean target domain, overlooking the security risks of unlabeled samples.","In this paper, we explore the potential backdoor attacks on model adaptation launched by well-designed poisoning target data.","Concretely, we provide two backdoor triggers with two poisoning strategies for different prior knowledge owned by attackers.","These attacks achieve a high success rate and keep the normal performance on clean samples in the test stage.","To defend against backdoor embedding, we propose a plug-and-play method named MixAdapt, combining it with existing adaptation algorithms.","Experiments across commonly used benchmarks and adaptation methods demonstrate the effectiveness of MixAdapt.","We hope this work will shed light on the safety of learning with unlabeled data."],"url":"http://arxiv.org/abs/2401.06030v1"}
{"created":"2024-01-11 16:30:07","title":"Automatic UAV-based Airport Pavement Inspection Using Mixed Real and Virtual Scenarios","abstract":"Runway and taxiway pavements are exposed to high stress during their projected lifetime, which inevitably leads to a decrease in their condition over time. To make sure airport pavement condition ensure uninterrupted and resilient operations, it is of utmost importance to monitor their condition and conduct regular inspections. UAV-based inspection is recently gaining importance due to its wide range monitoring capabilities and reduced cost. In this work, we propose a vision-based approach to automatically identify pavement distress using images captured by UAVs. The proposed method is based on Deep Learning (DL) to segment defects in the image. The DL architecture leverages the low computational capacities of embedded systems in UAVs by using an optimised implementation of EfficientNet feature extraction and Feature Pyramid Network segmentation. To deal with the lack of annotated data for training we have developed a synthetic dataset generation methodology to extend available distress datasets. We demonstrate that the use of a mixed dataset composed of synthetic and real training images yields better results when testing the training models in real application scenarios.","sentences":["Runway and taxiway pavements are exposed to high stress during their projected lifetime, which inevitably leads to a decrease in their condition over time.","To make sure airport pavement condition ensure uninterrupted and resilient operations, it is of utmost importance to monitor their condition and conduct regular inspections.","UAV-based inspection is recently gaining importance due to its wide range monitoring capabilities and reduced cost.","In this work, we propose a vision-based approach to automatically identify pavement distress using images captured by UAVs.","The proposed method is based on Deep Learning (DL) to segment defects in the image.","The DL architecture leverages the low computational capacities of embedded systems in UAVs by using an optimised implementation of EfficientNet feature extraction and Feature Pyramid Network segmentation.","To deal with the lack of annotated data for training we have developed a synthetic dataset generation methodology to extend available distress datasets.","We demonstrate that the use of a mixed dataset composed of synthetic and real training images yields better results when testing the training models in real application scenarios."],"url":"http://arxiv.org/abs/2401.06019v1"}
{"created":"2024-01-11 16:14:30","title":"Sea ice detection using concurrent multispectral and synthetic aperture radar imagery","abstract":"Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions. Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image. Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months. To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\\_IceD). ViSual\\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions. The performance of ViSual\\_IceD is compared with U-Net models trained using concatenated MSI and SAR imagery as well as models trained exclusively on MSI or SAR imagery. ViSual\\_IceD outperforms the other networks, with a F1 score 1.60\\% points higher than the next best network, and results indicate that ViSual\\_IceD is selective in the image type it uses during image segmentation. Outputs from ViSual\\_IceD are compared to sea ice concentration products derived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how ViSual\\_IceD is a useful tool to use in conjunction with PMW data, particularly in coastal regions. As the spatial-temporal coverage of MSI and SAR imagery continues to increase, ViSual\\_IceD provides a new opportunity for robust, accurate sea ice coverage detection in polar regions.","sentences":["Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions.","Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image.","Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months.","To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\\_IceD).","ViSual\\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions.","The performance of ViSual\\_IceD is compared with U-Net models trained using concatenated MSI and SAR imagery as well as models trained exclusively on MSI or SAR imagery.","ViSual\\_IceD outperforms the other networks, with a F1 score 1.60\\% points higher than the next best network, and results indicate that ViSual\\_IceD is selective in the image type it uses during image segmentation.","Outputs from ViSual\\_IceD are compared to sea ice concentration products derived from the AMSR2 Passive Microwave (PMW) sensor.","Results highlight how ViSual\\_IceD is a useful tool to use in conjunction with PMW data, particularly in coastal regions.","As the spatial-temporal coverage of MSI and SAR imagery continues to increase, ViSual\\_IceD provides a new opportunity for robust, accurate sea ice coverage detection in polar regions."],"url":"http://arxiv.org/abs/2401.06009v1"}
{"created":"2024-01-11 15:52:55","title":"A Multi-Embedding Convergence Network on Siamese Architecture for Fake Reviews","abstract":"In this new digital era, accessibility to real-world events is moving towards web-based modules. This is mostly visible on e-commerce websites where there is limited availability of physical verification. With this unforeseen development, we depend on the verification in the virtual world to influence our decisions. One of the decision making process is deeply based on review reading. Reviews play an important part in this transactional process. And seeking a real review can be very tenuous work for the user. On the other hand, fake review heavily impacts these transaction records of a product. The article presents an implementation of a Siamese network for detecting fake reviews. The fake reviews dataset, consisting of 40K reviews, preprocessed with different techniques. The cleaned data is passed through embeddings generated by MiniLM BERT for contextual relationship and Word2Vec for semantic relationship to form vectors. Further, the embeddings are trained in a Siamese network with LSTM layers connected to fuzzy logic for decision-making. The results show that fake reviews can be detected with high accuracy on a siamese network for prediction and verification.","sentences":["In this new digital era, accessibility to real-world events is moving towards web-based modules.","This is mostly visible on e-commerce websites where there is limited availability of physical verification.","With this unforeseen development, we depend on the verification in the virtual world to influence our decisions.","One of the decision making process is deeply based on review reading.","Reviews play an important part in this transactional process.","And seeking a real review can be very tenuous work for the user.","On the other hand, fake review heavily impacts these transaction records of a product.","The article presents an implementation of a Siamese network for detecting fake reviews.","The fake reviews dataset, consisting of 40K reviews, preprocessed with different techniques.","The cleaned data is passed through embeddings generated by MiniLM BERT for contextual relationship and Word2Vec for semantic relationship to form vectors.","Further, the embeddings are trained in a Siamese network with LSTM layers connected to fuzzy logic for decision-making.","The results show that fake reviews can be detected with high accuracy on a siamese network for prediction and verification."],"url":"http://arxiv.org/abs/2401.05995v1"}
{"created":"2024-01-11 15:52:20","title":"MGARD: A multigrid framework for high-performance, error-controlled data compression and refactoring","abstract":"We describe MGARD, a software providing MultiGrid Adaptive Reduction for floating-point scientific data on structured and unstructured grids. With exceptional data compression capability and precise error control, MGARD addresses a wide range of requirements, including storage reduction, high-performance I/O, and in-situ data analysis. It features a unified application programming interface (API) that seamlessly operates across diverse computing architectures. MGARD has been optimized with highly-tuned GPU kernels and efficient memory and device management mechanisms, ensuring scalable and rapid operations.","sentences":["We describe MGARD, a software providing MultiGrid Adaptive Reduction for floating-point scientific data on structured and unstructured grids.","With exceptional data compression capability and precise error control, MGARD addresses a wide range of requirements, including storage reduction, high-performance I/O, and in-situ data analysis.","It features a unified application programming interface (API) that seamlessly operates across diverse computing architectures.","MGARD has been optimized with highly-tuned GPU kernels and efficient memory and device management mechanisms, ensuring scalable and rapid operations."],"url":"http://arxiv.org/abs/2401.05994v1"}
{"created":"2024-01-11 15:42:52","title":"Reconstruction as a service: a data space for off-site image reconstruction in magnetic particle imaging","abstract":"Magnetic particle imaging (MPI) is an emerging medical imaging modality which offers a unique combination of high temporal and spatial resolution, sensitivity and biocompatibility. For system-matrix (SM) based image reconstruction in MPI, a huge amount of calibration data needs to be acquired prior to reconstruction in a time-consuming procedure. Conventionally, the data is recorded on-site inside the scanning device, which significantly limits the time that the scanning device is available for patient care in a clinical setting. Due to its size, handling the calibration data can be challenging. To solve these issues of recording and handling the data, data spaces could be used, as it has been shown that the calibration data can be measured in dedicated devices off-site. We propose a data space aimed at improving the efficiency of SM-based image reconstruction in MPI. The data space consists of imaging facilities, calibration data providers and reconstruction experts. Its specifications follow the reference architecture model of international data spaces (IDS). Use-cases of image reconstruction in MPI are formulated. The stakeholders and tasks are listed and mapped to the terminology of IDS. The signal chain in MPI is analysed to identify a minimum information model which is used by the data space.","sentences":["Magnetic particle imaging (MPI) is an emerging medical imaging modality which offers a unique combination of high temporal and spatial resolution, sensitivity and biocompatibility.","For system-matrix (SM) based image reconstruction in MPI, a huge amount of calibration data needs to be acquired prior to reconstruction in a time-consuming procedure.","Conventionally, the data is recorded on-site inside the scanning device, which significantly limits the time that the scanning device is available for patient care in a clinical setting.","Due to its size, handling the calibration data can be challenging.","To solve these issues of recording and handling the data, data spaces could be used, as it has been shown that the calibration data can be measured in dedicated devices off-site.","We propose a data space aimed at improving the efficiency of SM-based image reconstruction in MPI.","The data space consists of imaging facilities, calibration data providers and reconstruction experts.","Its specifications follow the reference architecture model of international data spaces (IDS).","Use-cases of image reconstruction in MPI are formulated.","The stakeholders and tasks are listed and mapped to the terminology of IDS.","The signal chain in MPI is analysed to identify a minimum information model which is used by the data space."],"url":"http://arxiv.org/abs/2401.05987v1"}
{"created":"2024-01-11 15:38:31","title":"HybridOctree_Hex: Hybrid Octree-Based Adaptive All-Hexahedral Mesh Generation with Jacobian Control","abstract":"We present a new software package \"HybridOctree_Hex\" for adaptive all-hexahedral mesh generation based on hybrid octree and quality improvement with Jacobian control. The proposed HybridOctree_Hex begins by detecting curvatures and narrow regions of the input boundary to identify key surface features and initialize an octree structure. Subsequently, a strongly balanced octree is constructed using the balancing and pairing rules. Inspired by our earlier preliminary hybrid octree-based work, templates are designed to guarantee an all-hexahedral dual mesh generation directly from the strongly balanced octree. With these pre-defined templates, the sophisticated hybrid octree construction step is skipped to achieve an efficient implementation. After that, elements outside and around the boundary are removed to create a core mesh. The boundary points of the core mesh are connected to their corresponding closest points on the surface to fill the buffer zone and build the final mesh. Coupled with smart Laplacian smoothing, HybridOctree_Hex takes advantage of a delicate optimization-based quality improvement method considering geometric fitting, Jacobian and scaled Jacobian to achieve the minimum scaled Jacobian higher than $0.5$. We empirically verify the robustness and efficiency of our method by running the HybridOctree_Hex software on dozens of complex 3D models without any manual intervention or parameter adjustment. We provide the HybridOctree_Hex source codes, comprehensive results, encompassing mesh input/output files and statistical data presented at https://github.com/CMU-CBML/HybridOctree_Hex.","sentences":["We present a new software package \"HybridOctree_Hex\" for adaptive all-hexahedral mesh generation based on hybrid octree and quality improvement with Jacobian control.","The proposed HybridOctree_Hex begins by detecting curvatures and narrow regions of the input boundary to identify key surface features and initialize an octree structure.","Subsequently, a strongly balanced octree is constructed using the balancing and pairing rules.","Inspired by our earlier preliminary hybrid octree-based work, templates are designed to guarantee an all-hexahedral dual mesh generation directly from the strongly balanced octree.","With these pre-defined templates, the sophisticated hybrid octree construction step is skipped to achieve an efficient implementation.","After that, elements outside and around the boundary are removed to create a core mesh.","The boundary points of the core mesh are connected to their corresponding closest points on the surface to fill the buffer zone and build the final mesh.","Coupled with smart Laplacian smoothing, HybridOctree_Hex takes advantage of a delicate optimization-based quality improvement method considering geometric fitting, Jacobian and scaled Jacobian to achieve the minimum scaled Jacobian higher than $0.5$. We empirically verify the robustness and efficiency of our method by running the HybridOctree_Hex software on dozens of complex 3D models without any manual intervention or parameter adjustment.","We provide the HybridOctree_Hex source codes, comprehensive results, encompassing mesh input/output files and statistical data presented at https://github.com/CMU-CBML/HybridOctree_Hex."],"url":"http://arxiv.org/abs/2401.05984v1"}
{"created":"2024-01-11 15:22:55","title":"End-to-end Learnable Clustering for Intent Learning in Recommendation","abstract":"Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \\underline{ELCRec}, which integrates representation learning into an \\underline{E}nd-to-end \\underline{L}earnable \\underline{C}lustering framework for \\underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameters. Additionally, we design a clustering loss that guides the networks to differentiate between different cluster centers and pull similar samples towards their respective cluster centers. This allows simultaneous optimization of recommendation and clustering using mini-batch data. Moreover, we leverage the learned cluster centers as self-supervision signals for representation learning, resulting in further enhancement of recommendation performance. Extensive experiments conducted on open benchmarks and industry data validate the superiority, effectiveness, and efficiency of our proposed ELCRec method. Code is available at: https://github.com/yueliu1999/ELCRec.","sentences":["Mining users' intents plays a crucial role in sequential recommendation.","The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering.","While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues.","Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance.","Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data.","To address these challenges, we propose a novel intent learning method called \\underline{ELCRec}, which integrates representation learning into an \\underline{E}nd-to-end \\underline{L}earnable \\underline{C}lustering framework for \\underline{Rec}ommendation.","Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameters.","Additionally, we design a clustering loss that guides the networks to differentiate between different cluster centers and pull similar samples towards their respective cluster centers.","This allows simultaneous optimization of recommendation and clustering using mini-batch data.","Moreover, we leverage the learned cluster centers as self-supervision signals for representation learning, resulting in further enhancement of recommendation performance.","Extensive experiments conducted on open benchmarks and industry data validate the superiority, effectiveness, and efficiency of our proposed ELCRec method.","Code is available at: https://github.com/yueliu1999/ELCRec."],"url":"http://arxiv.org/abs/2401.05975v1"}
{"created":"2024-01-11 15:19:21","title":"UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization","abstract":"Despite significant progress in global localization of Unmanned Aerial Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained by the availability of datasets. Current datasets often focus on small-scale scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV build-in sensor data. To address these limitations, we introduce a large-scale 6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF localization pipeline (UAVLoc), which consists of offline synthetic data generation and online visual localization. Additionally, based on the 6-DoF estimator, we design a hierarchical system for tracking ground target in 3D space. Experimental results on the new dataset demonstrate the effectiveness of the proposed approach. Code and dataset are available at https://github.com/RingoWRW/UAVD4L","sentences":["Despite significant progress in global localization of Unmanned Aerial Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained by the availability of datasets.","Current datasets often focus on small-scale scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV build-in sensor data.","To address these limitations, we introduce a large-scale 6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF localization pipeline (UAVLoc), which consists of offline synthetic data generation and online visual localization.","Additionally, based on the 6-DoF estimator, we design a hierarchical system for tracking ground target in 3D space.","Experimental results on the new dataset demonstrate the effectiveness of the proposed approach.","Code and dataset are available at https://github.com/RingoWRW/UAVD4L"],"url":"http://arxiv.org/abs/2401.05971v1"}
{"created":"2024-01-11 15:16:20","title":"Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem","abstract":"The traveling officer problem (TOP) is a challenging stochastic optimization task. In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible. A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined. Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place. Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account. This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP. Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action. Furthermore, we propose a novel message-passing module for learning future inter-action correlations in the given environment. Thus, the agent can estimate the potential to fine further parking violations after executing an action. We evaluate our method using an environment based on real-world data from Melbourne. Our results show that SATOP consistently outperforms state-of-the-art TOP agents and is able to fine up to 22% more parking offenses.","sentences":["The traveling officer problem (TOP) is a challenging stochastic optimization task.","In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible.","A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined.","Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place.","Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account.","This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP.","Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action.","Furthermore, we propose a novel message-passing module for learning future inter-action correlations in the given environment.","Thus, the agent can estimate the potential to fine further parking violations after executing an action.","We evaluate our method using an environment based on real-world data from Melbourne.","Our results show that SATOP consistently outperforms state-of-the-art TOP agents and is able to fine up to 22% more parking offenses."],"url":"http://arxiv.org/abs/2401.05969v1"}
{"created":"2024-01-11 15:08:15","title":"HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis","abstract":"Single-Program-Multiple-Data (SPMD) parallelism has recently been adopted to train large deep neural networks (DNNs). Few studies have explored its applicability on heterogeneous clusters, to fully exploit available resources for large model learning. This paper presents \\OurSystem, an automated system designed to expedite SPMD DNN training on heterogeneous clusters. \\OurSystem jointly optimizes the tensor sharding strategy, sharding ratios across heterogeneous devices and the communication methods for tensor exchanges for optimized distributed training with SPMD parallelism. We novelly formulate model partitioning as a program synthesis problem, in which we generate a distributed program from scratch on a distributed instruction set that semantically resembles the program designed for a single device, and systematically explore the solution space with an A*-based search algorithm. We derive the optimal tensor sharding ratios by formulating it as a linear programming problem. Additionally, \\OurSystem explores tensor communication optimization in a heterogeneous cluster and integrates it as part of the program synthesis process, for automatically choosing optimal collective communication primitives and applying sufficient factor broadcasting technique. Extensive experiments on representative workloads demonstrate that \\OurSystem achieves up to 2.41x speed-up on heterogeneous clusters.","sentences":["Single-Program-Multiple-Data (SPMD) parallelism has recently been adopted to train large deep neural networks (DNNs).","Few studies have explored its applicability on heterogeneous clusters, to fully exploit available resources for large model learning.","This paper presents \\OurSystem, an automated system designed to expedite SPMD DNN training on heterogeneous clusters.","\\OurSystem jointly optimizes the tensor sharding strategy, sharding ratios across heterogeneous devices and the communication methods for tensor exchanges for optimized distributed training with SPMD parallelism.","We novelly formulate model partitioning as a program synthesis problem, in which we generate a distributed program from scratch on a distributed instruction set that semantically resembles the program designed for a single device, and systematically explore the solution space with an A*-based search algorithm.","We derive the optimal tensor sharding ratios by formulating it as a linear programming problem.","Additionally, \\OurSystem explores tensor communication optimization in a heterogeneous cluster and integrates it as part of the program synthesis process, for automatically choosing optimal collective communication primitives and applying sufficient factor broadcasting technique.","Extensive experiments on representative workloads demonstrate that \\OurSystem achieves up to 2.41x speed-up on heterogeneous clusters."],"url":"http://arxiv.org/abs/2401.05965v1"}
{"created":"2024-01-11 15:02:15","title":"Machine Learning Insides OptVerse AI Solver: Design Principles and Applications","abstract":"In an era of digital ubiquity, efficient resource management and decision-making are paramount across numerous industries. To this end, we present a comprehensive study on the integration of machine learning (ML) techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the scarcity of real-world mathematical programming instances, and to surpass the capabilities of traditional optimization techniques. We showcase our methods for generating complex SAT and MILP instances utilizing generative models that mirror multifaceted structures of real-world problem. Furthermore, we introduce a training framework leveraging augmentation policies to maintain solvers' utility in dynamic environments. Besides the data generation and augmentation, our proposed approaches also include novel ML-driven policies for personalized solver strategies, with an emphasis on applications like graph convolutional networks for initial basis selection and reinforcement learning for advanced presolving and cut selection. Additionally, we detail the incorporation of state-of-the-art parameter tuning algorithms which markedly elevate solver performance. Compared with traditional solvers such as Gurobi and SCIP, our ML-augmented OptVerse AI Solver demonstrates superior speed and precision across both established benchmarks and real-world scenarios, reinforcing the practical imperative and effectiveness of machine learning techniques in mathematical programming solvers.","sentences":["In an era of digital ubiquity, efficient resource management and decision-making are paramount across numerous industries.","To this end, we present a comprehensive study on the integration of machine learning (ML) techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the scarcity of real-world mathematical programming instances, and to surpass the capabilities of traditional optimization techniques.","We showcase our methods for generating complex SAT and MILP instances utilizing generative models that mirror multifaceted structures of real-world problem.","Furthermore, we introduce a training framework leveraging augmentation policies to maintain solvers' utility in dynamic environments.","Besides the data generation and augmentation, our proposed approaches also include novel ML-driven policies for personalized solver strategies, with an emphasis on applications like graph convolutional networks for initial basis selection and reinforcement learning for advanced presolving and cut selection.","Additionally, we detail the incorporation of state-of-the-art parameter tuning algorithms which markedly elevate solver performance.","Compared with traditional solvers such as Gurobi and SCIP, our ML-augmented OptVerse AI Solver demonstrates superior speed and precision across both established benchmarks and real-world scenarios, reinforcing the practical imperative and effectiveness of machine learning techniques in mathematical programming solvers."],"url":"http://arxiv.org/abs/2401.05960v1"}
{"created":"2024-01-11 14:53:26","title":"A k-swap Local Search for Makespan Scheduling","abstract":"Local search is a widely used technique for tackling challenging optimization problems, offering significant advantages in terms of computational efficiency and exhibiting strong empirical behavior across a wide range of problem domains. In this paper, we address a scheduling problem on two identical parallel machines with the objective of \\emph{makespan minimization}. For this problem, we consider a local search neighborhood, called \\emph{$k$-swap}, which is a more generalized version of the widely-used \\emph{swap} and \\emph{jump} neighborhoods. The $k$-swap neighborhood is obtained by swapping at most $k$ jobs between two machines in our schedule. First, we propose an algorithm for finding an improving neighbor in the $k$-swap neighborhood which is faster than the naive approach, and prove an almost matching lower bound on any such an algorithm. Then, we analyze the number of local search steps required to converge to a local optimum with respect to the $k$-swap neighborhood. For the case $k = 2$ (similar to the swap neighborhood), we provide a polynomial upper bound on the number of local search steps, and for the case $k = 3$, we provide an exponential lower bound. Finally, we conduct computational experiments on various families of instances, and we discuss extensions to more than two machines in our schedule.","sentences":["Local search is a widely used technique for tackling challenging optimization problems, offering significant advantages in terms of computational efficiency and exhibiting strong empirical behavior across a wide range of problem domains.","In this paper, we address a scheduling problem on two identical parallel machines with the objective of \\emph{makespan minimization}.","For this problem, we consider a local search neighborhood, called \\emph{$k$-swap}, which is a more generalized version of the widely-used \\emph{swap} and \\emph{jump} neighborhoods.","The $k$-swap neighborhood is obtained by swapping at most $k$ jobs between two machines in our schedule.","First, we propose an algorithm for finding an improving neighbor in the $k$-swap neighborhood which is faster than the naive approach, and prove an almost matching lower bound on any such an algorithm.","Then, we analyze the number of local search steps required to converge to a local optimum with respect to the $k$-swap neighborhood.","For the case $k = 2$ (similar to the swap neighborhood), we provide a polynomial upper bound on the number of local search steps, and for the case $k = 3$, we provide an exponential lower bound.","Finally, we conduct computational experiments on various families of instances, and we discuss extensions to more than two machines in our schedule."],"url":"http://arxiv.org/abs/2401.05956v1"}
{"created":"2024-01-11 14:34:56","title":"Blockchain-based Decentralized Time Lock Machines: Automated Reveal of Time-sensitive Information","abstract":"Conditional Information Reveal (CIR) automates the release of information upon meeting specific pre-defined conditions, such as time or location. This paper advances the understanding and implementation of CIR by introducing a new paradigm to highlight the security challenges in CIR design, and proposes a decentralized architecture as a design guideline for secure CIR systems. Furthermore, in the context of time-sensitive data sharing, this paper proposes a practical timed-release cryptography system employing the proposed architecture and a novel verifiable secret sharing scheme. Key achievements of this study include the creation of an open-source prototype for practical deployment and a comprehensive system evaluation that highlights the enhanced security and efficiency of the proposed system. Furthermore, the paper delves into the application of this system in E-voting scenarios, illustrating its capacity to secure and ensure fair electronic voting processes.","sentences":["Conditional Information Reveal (CIR) automates the release of information upon meeting specific pre-defined conditions, such as time or location.","This paper advances the understanding and implementation of CIR by introducing a new paradigm to highlight the security challenges in CIR design, and proposes a decentralized architecture as a design guideline for secure CIR systems.","Furthermore, in the context of time-sensitive data sharing, this paper proposes a practical timed-release cryptography system employing the proposed architecture and a novel verifiable secret sharing scheme.","Key achievements of this study include the creation of an open-source prototype for practical deployment and a comprehensive system evaluation that highlights the enhanced security and efficiency of the proposed system.","Furthermore, the paper delves into the application of this system in E-voting scenarios, illustrating its capacity to secure and ensure fair electronic voting processes."],"url":"http://arxiv.org/abs/2401.05947v1"}
{"created":"2024-01-11 14:28:13","title":"SoK: Analysis techniques for WebAssembly","abstract":"WebAssembly is a low-level bytecode language that allows high-level languages like C, C++, and Rust to be executed in the browser at near-native performance. In recent years, WebAssembly has gained widespread adoption is now natively supported by all modern browsers. However, vulnerabilities in memory-unsafe languages, like C and C++, can translate into vulnerabilities in WebAssembly binaries. Unfortunately, most WebAssembly binaries are compiled from such memory-unsafe languages, and these vulnerabilities have been shown to be practical in real-world scenarios. WebAssembly smart contracts have also been found to be vulnerable, causing significant financial loss. Additionally, WebAssembly has been used for malicious purposes like cryptojacking. To address these issues, several analysis techniques for WebAssembly binaries have been proposed. In this paper, we conduct a comprehensive literature review of these techniques and categorize them based on their analysis strategy and objectives. Furthermore, we compare and evaluate the techniques using quantitative data, highlighting their strengths and weaknesses. In addition, one of the main contributions of this paper is the identification of future research directions based on the thorough literature review conducted.","sentences":["WebAssembly is a low-level bytecode language that allows high-level languages like C, C++, and Rust to be executed in the browser at near-native performance.","In recent years, WebAssembly has gained widespread adoption is now natively supported by all modern browsers.","However, vulnerabilities in memory-unsafe languages, like C and C++, can translate into vulnerabilities in WebAssembly binaries.","Unfortunately, most WebAssembly binaries are compiled from such memory-unsafe languages, and these vulnerabilities have been shown to be practical in real-world scenarios.","WebAssembly smart contracts have also been found to be vulnerable, causing significant financial loss.","Additionally, WebAssembly has been used for malicious purposes like cryptojacking.","To address these issues, several analysis techniques for WebAssembly binaries have been proposed.","In this paper, we conduct a comprehensive literature review of these techniques and categorize them based on their analysis strategy and objectives.","Furthermore, we compare and evaluate the techniques using quantitative data, highlighting their strengths and weaknesses.","In addition, one of the main contributions of this paper is the identification of future research directions based on the thorough literature review conducted."],"url":"http://arxiv.org/abs/2401.05943v1"}
{"created":"2024-01-11 14:11:30","title":"Time Series Forecasting of HIV/AIDS in the Philippines Using Deep Learning: Does COVID-19 Epidemic Matter?","abstract":"With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS epidemic in the Philippines is the one that is spreading the quickest in the western Pacific. Although the full effects of COVID-19 on HIV services and development are still unknown, it is predicted that such disruptions could lead to a significant increase in HIV casualties. Therefore, the nation needs some modeling and forecasting techniques to foresee the spread pattern and enhance the governments prevention, treatment, testing, and care program. In this study, the researcher uses Multilayer Perceptron Neural Network to forecast time series during the period when the COVID-19 pandemic strikes the nation, using statistics taken from the HIV/AIDS and ART Registry of the Philippines. After training, validation, and testing of data, the study finds that the predicted cumulative cases in the nation by 2030 will reach 145,273. Additionally, there is very little difference between observed and anticipated HIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as well as a greater coefficient of determination. Further research revealed that the Philippines seems far from achieving Sustainable Development Goal 3 of Project 2030 due to an increase in the nations rate of new HIV infections. Despite the detrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, the Philippine government, under the Marcos administration, must continue to adhere to the United Nations 90-90-90 targets by enhancing its ART program and ensuring that all vital health services are readily accessible and available.","sentences":["With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS epidemic in the Philippines is the one that is spreading the quickest in the western Pacific.","Although the full effects of COVID-19 on HIV services and development are still unknown, it is predicted that such disruptions could lead to a significant increase in HIV casualties.","Therefore, the nation needs some modeling and forecasting techniques to foresee the spread pattern and enhance the governments prevention, treatment, testing, and care program.","In this study, the researcher uses Multilayer Perceptron Neural Network to forecast time series during the period when the COVID-19 pandemic strikes the nation, using statistics taken from the HIV/AIDS and ART Registry of the Philippines.","After training, validation, and testing of data, the study finds that the predicted cumulative cases in the nation by 2030 will reach 145,273.","Additionally, there is very little difference between observed and anticipated HIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as well as a greater coefficient of determination.","Further research revealed that the Philippines seems far from achieving Sustainable Development Goal 3 of Project 2030 due to an increase in the nations rate of new HIV infections.","Despite the detrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, the Philippine government, under the Marcos administration, must continue to adhere to the United Nations 90-90-90 targets by enhancing its ART program and ensuring that all vital health services are readily accessible and available."],"url":"http://arxiv.org/abs/2401.05933v1"}
{"created":"2024-01-11 14:11:12","title":"DiffDA: a diffusion model for weather-scale data assimilation","abstract":"The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling. We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations. We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model. Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only. As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution. The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss of lead time of at most 24 hours when compared to initial conditions of state-of-the-art data assimilation suites. This enables to apply the method to real world applications such as the creation of reanalysis datasets with autoregressive data assimilation.","sentences":["The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling.","We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations.","We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model.","Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only.","As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.","Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution.","The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss of lead time of at most 24 hours when compared to initial conditions of state-of-the-art data assimilation suites.","This enables to apply the method to real world applications such as the creation of reanalysis datasets with autoregressive data assimilation."],"url":"http://arxiv.org/abs/2401.05932v1"}
{"created":"2024-01-11 14:09:09","title":"SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully","abstract":"Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation. Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts. Significant and consistent improvements are achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.","sentences":["Large language models (LLMs) demonstrate great performance in text generation.","However, LLMs are still suffering from hallucinations.","In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully.","SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others.","Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives.","Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation.","During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.","Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts.","Significant and consistent improvements are achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks."],"url":"http://arxiv.org/abs/2401.05930v1"}
{"created":"2024-01-11 13:22:54","title":"ConKeD: Multiview contrastive descriptor learning for keypoint-based retinal image registration","abstract":"Retinal image registration is of utmost importance due to its wide applications in medical practice. In this context, we propose ConKeD, a novel deep learning approach to learn descriptors for retinal image registration. In contrast to current registration methods, our approach employs a novel multi-positive multi-negative contrastive learning strategy that enables the utilization of additional information from the available training samples. This makes it possible to learn high quality descriptors from limited training data. To train and evaluate ConKeD, we combine these descriptors with domain-specific keypoints, particularly blood vessel bifurcations and crossovers, that are detected using a deep neural network. Our experimental results demonstrate the benefits of the novel multi-positive multi-negative strategy, as it outperforms the widely used triplet loss technique (single-positive and single-negative) as well as the single-positive multi-negative alternative. Additionally, the combination of ConKeD with the domain-specific keypoints produces comparable results to the state-of-the-art methods for retinal image registration, while offering important advantages such as avoiding pre-processing, utilizing fewer training samples, and requiring fewer detected keypoints, among others. Therefore, ConKeD shows a promising potential towards facilitating the development and application of deep learning-based methods for retinal image registration.","sentences":["Retinal image registration is of utmost importance due to its wide applications in medical practice.","In this context, we propose ConKeD, a novel deep learning approach to learn descriptors for retinal image registration.","In contrast to current registration methods, our approach employs a novel multi-positive multi-negative contrastive learning strategy that enables the utilization of additional information from the available training samples.","This makes it possible to learn high quality descriptors from limited training data.","To train and evaluate ConKeD, we combine these descriptors with domain-specific keypoints, particularly blood vessel bifurcations and crossovers, that are detected using a deep neural network.","Our experimental results demonstrate the benefits of the novel multi-positive multi-negative strategy, as it outperforms the widely used triplet loss technique (single-positive and single-negative) as well as the single-positive multi-negative alternative.","Additionally, the combination of ConKeD with the domain-specific keypoints produces comparable results to the state-of-the-art methods for retinal image registration, while offering important advantages such as avoiding pre-processing, utilizing fewer training samples, and requiring fewer detected keypoints, among others.","Therefore, ConKeD shows a promising potential towards facilitating the development and application of deep learning-based methods for retinal image registration."],"url":"http://arxiv.org/abs/2401.05901v1"}
{"created":"2024-01-11 13:03:27","title":"LiDAR data acquisition and processing for ecology applications","abstract":"The collection of ecological data in the field is essential to diagnose, monitor and manage ecosystems in a sustainable way. Since acquisition of this information through traditional methods are generally time-consuming, due to the capability of recording large volumes of data in short time periods, automation of data acquisition sees a growing trend. Terrestrial laser scanners (TLS), particularly LiDAR sensors, have been used in ecology, allowing to reconstruct the 3D structure of vegetation, and thus, infer ecosystem characteristics based on the spatial variation of the density of points. However, the low amount of information obtained per beam, lack of data analysis tools and the high cost of the equipment limit their use. This way, a low-cost TLS (<10k$) was developed along with data acquisition and processing mechanisms applicable in two case studies: an urban garden and a target area for ecological restoration. The orientation of LiDAR was modified to make observations in the vertical plane and a motor was integrated for its rotation, enabling the acquisition of 360 degree data with high resolution. Motion and location sensors were also integrated for automatic error correction and georeferencing. From the data generated, histograms of point density variation along the vegetation height were created, where shrub stratum was easily distinguishable from tree stratum, and maximum tree height and shrub cover were calculated. These results agreed with the field data, whereby the developed TLS has proved to be effective in calculating metrics of structural complexity of vegetation.","sentences":["The collection of ecological data in the field is essential to diagnose, monitor and manage ecosystems in a sustainable way.","Since acquisition of this information through traditional methods are generally time-consuming, due to the capability of recording large volumes of data in short time periods, automation of data acquisition sees a growing trend.","Terrestrial laser scanners (TLS), particularly LiDAR sensors, have been used in ecology, allowing to reconstruct the 3D structure of vegetation, and thus, infer ecosystem characteristics based on the spatial variation of the density of points.","However, the low amount of information obtained per beam, lack of data analysis tools and the high cost of the equipment limit their use.","This way, a low-cost TLS (<10k$) was developed along with data acquisition and processing mechanisms applicable in two case studies: an urban garden and a target area for ecological restoration.","The orientation of LiDAR was modified to make observations in the vertical plane and a motor was integrated for its rotation, enabling the acquisition of 360 degree data with high resolution.","Motion and location sensors were also integrated for automatic error correction and georeferencing.","From the data generated, histograms of point density variation along the vegetation height were created, where shrub stratum was easily distinguishable from tree stratum, and maximum tree height and shrub cover were calculated.","These results agreed with the field data, whereby the developed TLS has proved to be effective in calculating metrics of structural complexity of vegetation."],"url":"http://arxiv.org/abs/2401.05891v1"}
{"created":"2024-01-11 12:57:04","title":"Incorporation of Confidence Interval into Rate Selection Based on the Extreme Value Theory for Ultra-Reliable Communications","abstract":"Proper determination of the transmission rate in ultra-reliable low latency communication (URLLC) needs to incorporate a confidence interval (CI) for the estimated parameters due to the large amount of data required for their accurate estimation. In this paper, we propose a framework based on the extreme value theory (EVT) for determining the transmission rate along with its corresponding CI for an ultra-reliable communication system. This framework consists of characterizing the statistics of extreme events by fitting the generalized Pareto distribution (GPD) to the channel tail, deriving the GPD parameters and their associated CIs, and obtaining the transmission rate within a confidence interval. Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the accuracy of the estimated rate obtained through the EVT-based framework considering the confidence interval for the GPD parameters. Additionally, we show that proper estimation of the transmission rate based on the proposed framework requires a lower number of samples compared to the traditional extrapolation-based approaches.","sentences":["Proper determination of the transmission rate in ultra-reliable low latency communication (URLLC) needs to incorporate a confidence interval (CI) for the estimated parameters due to the large amount of data required for their accurate estimation.","In this paper, we propose a framework based on the extreme value theory (EVT) for determining the transmission rate along with its corresponding CI for an ultra-reliable communication system.","This framework consists of characterizing the statistics of extreme events by fitting the generalized Pareto distribution (GPD) to the channel tail, deriving the GPD parameters and their associated CIs, and obtaining the transmission rate within a confidence interval.","Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the accuracy of the estimated rate obtained through the EVT-based framework considering the confidence interval for the GPD parameters.","Additionally, we show that proper estimation of the transmission rate based on the proposed framework requires a lower number of samples compared to the traditional extrapolation-based approaches."],"url":"http://arxiv.org/abs/2401.05888v1"}
{"created":"2024-01-11 12:43:26","title":"Generative Deduplication For Socia Media Data Selection","abstract":"Social media data is plagued by the redundancy problem caused by its noisy nature, leading to increased training time and model bias. To address this issue, we propose a novel approach called generative duplication. It aims to remove duplicate text from noisy social media data and mitigate model bias. By doing so, it can improve social media language understanding performance and save training time. Extensive experiments demonstrate that the proposed generative deduplication can effectively reduce training samples while improving performance. This evidence suggests the effectiveness of generative deduplication and its importance in social media language understanding.","sentences":["Social media data is plagued by the redundancy problem caused by its noisy nature, leading to increased training time and model bias.","To address this issue, we propose a novel approach called generative duplication.","It aims to remove duplicate text from noisy social media data and mitigate model bias.","By doing so, it can improve social media language understanding performance and save training time.","Extensive experiments demonstrate that the proposed generative deduplication can effectively reduce training samples while improving performance.","This evidence suggests the effectiveness of generative deduplication and its importance in social media language understanding."],"url":"http://arxiv.org/abs/2401.05883v1"}
{"created":"2024-01-11 12:41:33","title":"Extreme Value Theory Based Rate Selection for Ultra-Reliable Communications","abstract":"Ultra-reliable low latency communication (URLLC) requires the packet error rate to be on the order of $10^{-9}$-$10^{-5}$. Determining the appropriate transmission rate to satisfy this ultra-reliability constraint requires deriving the statistics of the channel in the ultra-reliable region and then incorporating these statistics into the rate selection. In this paper, we propose a framework for determining the rate selection for ultra-reliable communications based on the extreme value theory (EVT). We first model the wireless channel at URLLC by estimating the parameters of the generalized Pareto distribution (GPD) best fitting to the tail distribution of the received powers, i.e., the power values below a certain threshold. Then, we determine the maximum transmission rate by incorporating the Pareto distribution into the rate selection function. Finally, we validate the selected rate by computing the resulting error probability. Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the superior performance of the proposed methodology in determining the maximum transmission rate compared to the traditional extrapolation-based approaches.","sentences":["Ultra-reliable low latency communication (URLLC) requires the packet error rate to be on the order of $10^{-9}$-$10^{-5}$. Determining the appropriate transmission rate to satisfy this ultra-reliability constraint requires deriving the statistics of the channel in the ultra-reliable region and then incorporating these statistics into the rate selection.","In this paper, we propose a framework for determining the rate selection for ultra-reliable communications based on the extreme value theory (EVT).","We first model the wireless channel at URLLC by estimating the parameters of the generalized Pareto distribution (GPD) best fitting to the tail distribution of the received powers, i.e., the power values below a certain threshold.","Then, we determine the maximum transmission rate by incorporating the Pareto distribution into the rate selection function.","Finally, we validate the selected rate by computing the resulting error probability.","Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the superior performance of the proposed methodology in determining the maximum transmission rate compared to the traditional extrapolation-based approaches."],"url":"http://arxiv.org/abs/2401.05882v1"}
{"created":"2024-01-11 12:27:33","title":"Enhancing Personality Recognition in Dialogue by Data Augmentation and Heterogeneous Conversational Graph Networks","abstract":"Personality recognition is useful for enhancing robots' ability to tailor user-adaptive responses, thus fostering rich human-robot interactions. One of the challenges in this task is a limited number of speakers in existing dialogue corpora, which hampers the development of robust, speaker-independent personality recognition models. Additionally, accurately modeling both the interdependencies among interlocutors and the intra-dependencies within the speaker in dialogues remains a significant issue. To address the first challenge, we introduce personality trait interpolation for speaker data augmentation. For the second, we propose heterogeneous conversational graph networks to independently capture both contextual influences and inherent personality traits. Evaluations on the RealPersonaChat corpus demonstrate our method's significant improvements over existing baselines.","sentences":["Personality recognition is useful for enhancing robots' ability to tailor user-adaptive responses, thus fostering rich human-robot interactions.","One of the challenges in this task is a limited number of speakers in existing dialogue corpora, which hampers the development of robust, speaker-independent personality recognition models.","Additionally, accurately modeling both the interdependencies among interlocutors and the intra-dependencies within the speaker in dialogues remains a significant issue.","To address the first challenge, we introduce personality trait interpolation for speaker data augmentation.","For the second, we propose heterogeneous conversational graph networks to independently capture both contextual influences and inherent personality traits.","Evaluations on the RealPersonaChat corpus demonstrate our method's significant improvements over existing baselines."],"url":"http://arxiv.org/abs/2401.05871v1"}
{"created":"2024-01-11 12:20:50","title":"Efficient N-to-M Checkpointing Algorithm for Finite Element Simulations","abstract":"In this work, we introduce a new algorithm for N-to-M checkpointing in finite element simulations. This new algorithm allows efficient saving/loading of functions representing physical quantities associated with the mesh representing the physical domain. Specifically, the algorithm allows for using different numbers of parallel processes for saving and loading, allowing for restarting and post-processing on the process count appropriate to the given phase of the simulation and other conditions. For demonstration, we implemented this algorithm in PETSc, the Portable, Extensible Toolkit for Scientific Computation, and added a convenient high-level interface into Firedrake, a system for solving partial differential equations using finite element methods. We evaluated our new implementation by saving and loading data involving 8.2 billion finite element degrees of freedom using 8,192 parallel processes on ARCHER2, the UK National Supercomputing Service.","sentences":["In this work, we introduce a new algorithm for N-to-M checkpointing in finite element simulations.","This new algorithm allows efficient saving/loading of functions representing physical quantities associated with the mesh representing the physical domain.","Specifically, the algorithm allows for using different numbers of parallel processes for saving and loading, allowing for restarting and post-processing on the process count appropriate to the given phase of the simulation and other conditions.","For demonstration, we implemented this algorithm in PETSc, the Portable, Extensible Toolkit for Scientific Computation, and added a convenient high-level interface into Firedrake, a system for solving partial differential equations using finite element methods.","We evaluated our new implementation by saving and loading data involving 8.2 billion finite element degrees of freedom using 8,192 parallel processes on ARCHER2, the UK National Supercomputing Service."],"url":"http://arxiv.org/abs/2401.05868v1"}
{"created":"2024-01-11 12:04:11","title":"Seven Failure Points When Engineering a Retrieval Augmented Generation System","abstract":"Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.","sentences":["Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG).","A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM.","RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data.","However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs.","In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical.","We share the lessons learned and present 7 failure points to consider when designing a RAG system.","The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start.","We conclude with a list of potential research directions on RAG systems for the software engineering community."],"url":"http://arxiv.org/abs/2401.05856v1"}
{"created":"2024-01-11 11:39:21","title":"Contrastive Loss Based Frame-wise Feature disentanglement for Polyphonic Sound Event Detection","abstract":"Overlapping sound events are ubiquitous in real-world environments, but existing end-to-end sound event detection (SED) methods still struggle to detect them effectively. A critical reason is that these methods represent overlapping events using shared and entangled frame-wise features, which degrades the feature discrimination. To solve the problem, we propose a disentangled feature learning framework to learn a category-specific representation. Specifically, we employ different projectors to learn the frame-wise features for each category. To ensure that these feature does not contain information of other categories, we maximize the common information between frame-wise features within the same category and propose a frame-wise contrastive loss. In addition, considering that the labeled data used by the proposed method is limited, we propose a semi-supervised frame-wise contrastive loss that can leverage large amounts of unlabeled data to achieve feature disentanglement. The experimental results demonstrate the effectiveness of our method.","sentences":["Overlapping sound events are ubiquitous in real-world environments, but existing end-to-end sound event detection (SED) methods still struggle to detect them effectively.","A critical reason is that these methods represent overlapping events using shared and entangled frame-wise features, which degrades the feature discrimination.","To solve the problem, we propose a disentangled feature learning framework to learn a category-specific representation.","Specifically, we employ different projectors to learn the frame-wise features for each category.","To ensure that these feature does not contain information of other categories, we maximize the common information between frame-wise features within the same category and propose a frame-wise contrastive loss.","In addition, considering that the labeled data used by the proposed method is limited, we propose a semi-supervised frame-wise contrastive loss that can leverage large amounts of unlabeled data to achieve feature disentanglement.","The experimental results demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2401.05850v1"}
{"created":"2024-01-11 11:38:21","title":"Inferring Intentions to Speak Using Accelerometer Data In-the-Wild","abstract":"Humans have good natural intuition to recognize when another person has something to say. It would be interesting if an AI can also recognize intentions to speak. Especially in scenarios when an AI is guiding a group discussion, this can be a useful skill. This work studies the inference of successful and unsuccessful intentions to speak from accelerometer data. This is chosen because it is privacy-preserving and feasible for in-the-wild settings since it can be placed in a smart badge. Data from a real-life social networking event is used to train a machine-learning model that aims to infer intentions to speak. A subset of unsuccessful intention-to-speak cases in the data is annotated. The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases. In conclusion, there is useful information in accelerometer data, but not enough to reliably capture intentions to speak. For example, posture shifts are correlated with intentions to speak, but people also often shift posture without having an intention to speak, or have an intention to speak without shifting their posture. More modalities are likely needed to reliably infer intentions to speak.","sentences":["Humans have good natural intuition to recognize when another person has something to say.","It would be interesting if an AI can also recognize intentions to speak.","Especially in scenarios when an AI is guiding a group discussion, this can be a useful skill.","This work studies the inference of successful and unsuccessful intentions to speak from accelerometer data.","This is chosen because it is privacy-preserving and feasible for in-the-wild settings since it can be placed in a smart badge.","Data from a real-life social networking event is used to train a machine-learning model that aims to infer intentions to speak.","A subset of unsuccessful intention-to-speak cases in the data is annotated.","The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases.","In conclusion, there is useful information in accelerometer data, but not enough to reliably capture intentions to speak.","For example, posture shifts are correlated with intentions to speak, but people also often shift posture without having an intention to speak, or have an intention to speak without shifting their posture.","More modalities are likely needed to reliably infer intentions to speak."],"url":"http://arxiv.org/abs/2401.05849v1"}
{"created":"2024-01-11 11:29:40","title":"Graph Reconstruction via MIS Queries","abstract":"We consider the Graph Reconstruction problem given only query access to the input graph via a Maximal Independent Set oracle. In this setting, in each round, the player submits a query consisting of a subset of vertices to the oracle, and the oracle returns any maximal independent set in the subgraph induced by the queried vertices. The goal for the player is to learn all the edges of the input graph.   In this paper, we give tight (up to a logarithmic factor) upper and lower bounds for this problem:   1. We give a randomized query algorithm that uses $O(\\Delta^2 \\log n)$ non-adaptive queries and succeeds with high probability to reconstruct an $n$-vertex graph with maximum degree $\\Delta$. Using the probabilistic method, we also show that a non-adaptive deterministic algorithm that executes $O(\\Delta^3 \\log n)$ queries exists.   2. We give two lower bounds that apply to arbitrary adaptive randomized algorithms that succeed with probability greater than $\\frac{1}{2}$. We show that, for such algorithms, $\\Omega(\\Delta^2)$ rounds are necessary in graphs of maximum degree $\\Delta$, and that $\\Omega(\\log n)$ rounds are necessary even when the input graph is an $n$-vertex cycle.","sentences":["We consider the Graph Reconstruction problem given only query access to the input graph via a Maximal Independent Set oracle.","In this setting, in each round, the player submits a query consisting of a subset of vertices to the oracle, and the oracle returns any maximal independent set in the subgraph induced by the queried vertices.","The goal for the player is to learn all the edges of the input graph.   ","In this paper, we give tight (up to a logarithmic factor) upper and lower bounds for this problem:   1.","We give a randomized query algorithm that uses $O(\\Delta^2 \\log n)$ non-adaptive queries and succeeds with high probability to reconstruct an $n$-vertex graph with maximum degree $\\Delta$. Using the probabilistic method, we also show that a non-adaptive deterministic algorithm that executes $O(\\Delta^3 \\log n)$ queries exists.   ","2.","We give two lower bounds that apply to arbitrary adaptive randomized algorithms that succeed with probability greater than $\\frac{1}{2}$. We show that, for such algorithms, $\\Omega(\\Delta^2)$ rounds are necessary in graphs of maximum degree $\\Delta$, and that $\\Omega(\\log n)$ rounds are necessary even when the input graph is an $n$-vertex cycle."],"url":"http://arxiv.org/abs/2401.05845v1"}
{"created":"2024-01-11 11:22:44","title":"On the number of iterations of the DBA algorithm","abstract":"The DTW Barycenter Averaging (DBA) algorithm is a widely used algorithm for estimating the mean of a given set of point sequences. In this context, the mean is defined as a point sequence that minimises the sum of dynamic time warping distances (DTW). The algorithm is similar to the $k$-means algorithm in the sense that it alternately repeats two steps: (1) computing an optimal assignment to the points of the current mean, and (2) computing an optimal mean under the current assignment. The popularity of DBA can be attributed to the fact that it works well in practice, despite any theoretical guarantees to be known. In our paper, we aim to initiate a theoretical study of the number of iterations that DBA performs until convergence. We assume the algorithm is given $n$ sequences of $m$ points in $\\mathbb{R}^d$ and a parameter $k$ that specifies the length of the mean sequence to be computed. We show that, in contrast to its fast running time in practice, the number of iterations can be exponential in $k$ in the worst case - even if the number of input sequences is $n=2$. We complement these findings with experiments on real-world data that suggest this worst-case behaviour is likely degenerate. To better understand the performance of the algorithm on non-degenerate input, we study DBA in the model of smoothed analysis, upper-bounding the expected number of iterations in the worst case under random perturbations of the input. Our smoothed upper bound is polynomial in $k$, $n$ and $d$, and for constant $n$, it is also polynomial in $m$. For our analysis, we adapt the set of techniques that were developed for analysing $k$-means and observe that this set of techniques is not sufficient to obtain tight bounds for general $n$.","sentences":["The DTW Barycenter Averaging (DBA) algorithm is a widely used algorithm for estimating the mean of a given set of point sequences.","In this context, the mean is defined as a point sequence that minimises the sum of dynamic time warping distances (DTW).","The algorithm is similar to the $k$-means algorithm in the sense that it alternately repeats two steps: (1) computing an optimal assignment to the points of the current mean, and (2) computing an optimal mean under the current assignment.","The popularity of DBA can be attributed to the fact that it works well in practice, despite any theoretical guarantees to be known.","In our paper, we aim to initiate a theoretical study of the number of iterations that DBA performs until convergence.","We assume the algorithm is given $n$ sequences of $m$ points in $\\mathbb{R}^d$ and a parameter $k$ that specifies the length of the mean sequence to be computed.","We show that, in contrast to its fast running time in practice, the number of iterations can be exponential in $k$ in the worst case - even if the number of input sequences is $n=2$. We complement these findings with experiments on real-world data that suggest this worst-case behaviour is likely degenerate.","To better understand the performance of the algorithm on non-degenerate input, we study DBA in the model of smoothed analysis, upper-bounding the expected number of iterations in the worst case under random perturbations of the input.","Our smoothed upper bound is polynomial in $k$, $n$ and $d$, and for constant $n$, it is also polynomial in $m$. For our analysis, we adapt the set of techniques that were developed for analysing $k$-means and observe that this set of techniques is not sufficient to obtain tight bounds for general $n$."],"url":"http://arxiv.org/abs/2401.05841v1"}
{"created":"2024-01-11 11:22:36","title":"Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making","abstract":"With the rapid development of AI-based decision aids, different forms of AI assistance have been increasingly integrated into the human decision making processes. To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans' decision making behavior. To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box'' models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process. Meanwhile, methods that prioritize the interpretability of human behavior predictions are often tailored for one specific form of AI assistance, making adaptations to other forms of assistance difficult. In this paper, we propose a computational framework that can provide an interpretable characterization of the influence of different forms of AI assistance on decision makers in AI-assisted decision making. By conceptualizing AI assistance as the ``{\\em nudge}'' in human decision making processes, our approach centers around modelling how different forms of AI assistance modify humans' strategy in weighing different information in making their decisions. Evaluations on behavior data collected from real human decision makers show that the proposed framework outperforms various baselines in accurately predicting human behavior in AI-assisted decision making. Based on the proposed framework, we further provide insights into how individuals with different cognitive styles are nudged by AI assistance differently.","sentences":["With the rapid development of AI-based decision aids, different forms of AI assistance have been increasingly integrated into the human decision making processes.","To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans' decision making behavior.","To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box'' models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process.","Meanwhile, methods that prioritize the interpretability of human behavior predictions are often tailored for one specific form of AI assistance, making adaptations to other forms of assistance difficult.","In this paper, we propose a computational framework that can provide an interpretable characterization of the influence of different forms of AI assistance on decision makers in AI-assisted decision making.","By conceptualizing AI assistance as the ``{\\em nudge}'' in human decision making processes, our approach centers around modelling how different forms of AI assistance modify humans' strategy in weighing different information in making their decisions.","Evaluations on behavior data collected from real human decision makers show that the proposed framework outperforms various baselines in accurately predicting human behavior in AI-assisted decision making.","Based on the proposed framework, we further provide insights into how individuals with different cognitive styles are nudged by AI assistance differently."],"url":"http://arxiv.org/abs/2401.05840v1"}
{"created":"2024-01-11 11:05:33","title":"Modeling Online Paging in Multi-Core Systems","abstract":"Web requests are growing exponentially since the 90s due to the rapid development of the Internet. This process was further accelerated by the introduction of cloud services. It has been observed statistically that memory or web requests generally follow power-law distribution, Breslau et al. INFOCOM'99. That is, the $i^{\\text{th}}$ most popular web page is requested with a probability proportional to $1 / i^{\\alpha}$ ($\\alpha > 0$ is a constant). Furthermore, this study, which was performed more than 20 years ago, indicated Zipf-like behavior, i.e., that $\\alpha \\le 1$. Surprisingly, the memory access traces coming from petabyte-size modern cloud systems not only show that $\\alpha$ can be bigger than one but also illustrate a shifted power-law distribution -- called Pareto type II or Lomax. These previously not reported phenomenon calls for statistical explanation.   Our first contribution is a new statistical {\\it multi-core power-law} model indicating that double-power law can be attributed to the presence of multiple cores running many virtual machines in parallel on such systems. We verify experimentally the applicability of this model using the Kolmogorov-Smirnov test (K-S test).   The second contribution of this paper is a theoretical analysis indicating why LRU and LFU-based algorithms perform well in practice on data satisfying power-law or multi-core assumptions. We provide an explanation by studying the online paging problem in the stochastic input model, i.e., the input is a random sequence with each request independently drawn from a page set according to a distribution $\\pi$. We derive formulas (as a function of the page probabilities in $\\pi$) to upper bound their ratio-of-expectations, which help in establishing O(1) performance ratio given the random sequence following power-law and multi-core power-law distributions.","sentences":["Web requests are growing exponentially since the 90s due to the rapid development of the Internet.","This process was further accelerated by the introduction of cloud services.","It has been observed statistically that memory or web requests generally follow power-law distribution, Breslau et al. INFOCOM'99.","That is, the $i^{\\text{th}}$ most popular web page is requested with a probability proportional to $1 / i^{\\alpha}$ ($\\alpha > 0$ is a constant).","Furthermore, this study, which was performed more than 20 years ago, indicated Zipf-like behavior, i.e., that $\\alpha \\le 1$.","Surprisingly, the memory access traces coming from petabyte-size modern cloud systems not only show that $\\alpha$ can be bigger than one but also illustrate a shifted power-law distribution -- called Pareto type II or Lomax.","These previously not reported phenomenon calls for statistical explanation.   ","Our first contribution is a new statistical {\\it multi-core power-law} model indicating that double-power law can be attributed to the presence of multiple cores running many virtual machines in parallel on such systems.","We verify experimentally the applicability of this model using the Kolmogorov-Smirnov test (K-S test).   ","The second contribution of this paper is a theoretical analysis indicating why LRU and LFU-based algorithms perform well in practice on data satisfying power-law or multi-core assumptions.","We provide an explanation by studying the online paging problem in the stochastic input model, i.e., the input is a random sequence with each request independently drawn from a page set according to a distribution $\\pi$. We derive formulas (as a function of the page probabilities in $\\pi$) to upper bound their ratio-of-expectations, which help in establishing O(1) performance ratio given the random sequence following power-law and multi-core power-law distributions."],"url":"http://arxiv.org/abs/2401.05834v1"}
{"created":"2024-01-11 11:04:35","title":"Multivariate Extreme Value Theory Based Channel Modeling for Ultra-Reliable Communications","abstract":"Attaining ultra-reliable communication (URC) in fifth-generation (5G) and beyond networks requires deriving statistics of channel in ultra-reliable region by modeling the extreme events. Extreme value theory (EVT) has been previously adopted in channel modeling to characterize the lower tail of received powers in URC systems. In this paper, we propose a multivariate EVT (MEVT)-based channel modeling methodology for tail of the joint distribution of multi-channel by characterizing the multivariate extremes of multiple-input multiple-output (MIMO) system. The proposed approach derives lower tail statistics of received power of each channel by using the generalized Pareto distribution (GPD). Then, tail of the joint distribution is modeled as a function of estimated GPD parameters based on two approaches: logistic distribution, which utilizes logistic distribution to determine dependency factors among the Frechet transformed tail sequence and obtain a bi-variate extreme value model, and Poisson point process, which estimates probability measure function of the Pickands angular component to model bi-variate extreme values. Finally, validity of the proposed models is assessed by incorporating the mean constraint on probability measure function of Pichanks coordinates. Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the superiority of proposed methodology compared to the conventional extrapolation-based methods in providing the best fit to the multivariate extremes.","sentences":["Attaining ultra-reliable communication (URC) in fifth-generation (5G) and beyond networks requires deriving statistics of channel in ultra-reliable region by modeling the extreme events.","Extreme value theory (EVT) has been previously adopted in channel modeling to characterize the lower tail of received powers in URC systems.","In this paper, we propose a multivariate EVT (MEVT)-based channel modeling methodology for tail of the joint distribution of multi-channel by characterizing the multivariate extremes of multiple-input multiple-output (MIMO) system.","The proposed approach derives lower tail statistics of received power of each channel by using the generalized Pareto distribution (GPD).","Then, tail of the joint distribution is modeled as a function of estimated GPD parameters based on two approaches: logistic distribution, which utilizes logistic distribution to determine dependency factors among the Frechet transformed tail sequence and obtain a bi-variate extreme value model, and Poisson point process, which estimates probability measure function of the Pickands angular component to model bi-variate extreme values.","Finally, validity of the proposed models is assessed by incorporating the mean constraint on probability measure function of Pichanks coordinates.","Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the superiority of proposed methodology compared to the conventional extrapolation-based methods in providing the best fit to the multivariate extremes."],"url":"http://arxiv.org/abs/2401.05833v1"}
{"created":"2024-01-11 10:57:29","title":"Revisiting Silhouette: From Micro to Macro Aggregation","abstract":"Silhouette coefficient is an established internal clustering evaluation measure that produces a score per data point, assessing the quality of its clustering assignment. To assess the quality of the clustering of the whole dataset, the scores of all the points in the dataset are typically averaged into a single value, a strategy which we call as micro-averaging. As we illustrate in this work, by using a synthetic example, this micro-averaging strategy is sensitive both to cluster imbalance and outliers (background noise). To address these issues, we propose an alternative aggregation strategy, which first averages the silhouette scores at a cluster level and then (macro) averages the scores across the clusters. Based on the same synthetic example, we show that the proposed macro-averaged silhouette score is robust to cluster imbalance and background noise. We have conducted an experimental study showing that our macro-averaged variant provides better estimates of the ground truth number of clusters on several cases compared to the typical micro-averaged score.","sentences":["Silhouette coefficient is an established internal clustering evaluation measure that produces a score per data point, assessing the quality of its clustering assignment.","To assess the quality of the clustering of the whole dataset, the scores of all the points in the dataset are typically averaged into a single value, a strategy which we call as micro-averaging.","As we illustrate in this work, by using a synthetic example, this micro-averaging strategy is sensitive both to cluster imbalance and outliers (background noise).","To address these issues, we propose an alternative aggregation strategy, which first averages the silhouette scores at a cluster level and then (macro) averages the scores across the clusters.","Based on the same synthetic example, we show that the proposed macro-averaged silhouette score is robust to cluster imbalance and background noise.","We have conducted an experimental study showing that our macro-averaged variant provides better estimates of the ground truth number of clusters on several cases compared to the typical micro-averaged score."],"url":"http://arxiv.org/abs/2401.05831v1"}
{"created":"2024-01-11 10:49:14","title":"Crumbled Cookie Exploring E-commerce Websites Cookie Policies with Data Protection Regulations","abstract":"Despite stringent data protection regulations such as the General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and other country-specific regulations, many websites continue to use cookies to track user activities. Recent studies have revealed several data protection violations, resulting in significant penalties, especially for multinational corporations. Motivated by the question of why these data protection violations continue to occur despite strong data protection regulations, we examined 360 popular e-commerce websites in multiple countries to analyze whether they comply with regulations to protect user privacy from a cookie perspective.","sentences":["Despite stringent data protection regulations such as the General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and other country-specific regulations, many websites continue to use cookies to track user activities.","Recent studies have revealed several data protection violations, resulting in significant penalties, especially for multinational corporations.","Motivated by the question of why these data protection violations continue to occur despite strong data protection regulations, we examined 360 popular e-commerce websites in multiple countries to analyze whether they comply with regulations to protect user privacy from a cookie perspective."],"url":"http://arxiv.org/abs/2401.05826v1"}
{"created":"2024-01-11 10:28:17","title":"Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages","abstract":"This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generative counterparts as cross-lingual instructions; (4) AlignInstruct improved performance in 30 zero-shot directions.","sentences":["This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs).","One is the expansion of supported languages to previously unseen ones.","The second relates to the lack of data in low-resource languages.","Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge.","However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge.","AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments.","Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generative counterparts as cross-lingual instructions; (4) AlignInstruct improved performance in 30 zero-shot directions."],"url":"http://arxiv.org/abs/2401.05811v1"}
{"created":"2024-01-11 10:20:37","title":"On the representation and methodology for wide and short range head pose estimation","abstract":"Head pose estimation (HPE) is a problem of interest in computer vision to improve the performance of face processing tasks in semi-frontal or profile settings. Recent applications require the analysis of faces in the full 360{\\deg} rotation range. Traditional approaches to solve the semi-frontal and profile cases are not directly amenable for the full rotation case. In this paper we analyze the methodology for short- and wide-range HPE and discuss which representations and metrics are adequate for each case. We show that the popular Euler angles representation is a good choice for short-range HPE, but not at extreme rotations. However, the Euler angles' gimbal lock problem prevents them from being used as a valid metric in any setting. We also revisit the current cross-data set evaluation methodology and note that the lack of alignment between the reference systems of the training and test data sets negatively biases the results of all articles in the literature. We introduce a procedure to quantify this misalignment and a new methodology for cross-data set HPE that establishes new, more accurate, SOTA for the 300W-LP|Biwi benchmark. We also propose a generalization of the geodesic angular distance metric that enables the construction of a loss that controls the contribution of each training sample to the optimization of the model. Finally, we introduce a wide range HPE benchmark based on the CMU Panoptic data set.","sentences":["Head pose estimation (HPE) is a problem of interest in computer vision to improve the performance of face processing tasks in semi-frontal or profile settings.","Recent applications require the analysis of faces in the full 360{\\deg} rotation range.","Traditional approaches to solve the semi-frontal and profile cases are not directly amenable for the full rotation case.","In this paper we analyze the methodology for short- and wide-range HPE and discuss which representations and metrics are adequate for each case.","We show that the popular Euler angles representation is a good choice for short-range HPE, but not at extreme rotations.","However, the Euler angles' gimbal lock problem prevents them from being used as a valid metric in any setting.","We also revisit the current cross-data set evaluation methodology and note that the lack of alignment between the reference systems of the training and test data sets negatively biases the results of all articles in the literature.","We introduce a procedure to quantify this misalignment and a new methodology for cross-data set HPE that establishes new, more accurate, SOTA for the 300W-LP|Biwi benchmark.","We also propose a generalization of the geodesic angular distance metric that enables the construction of a loss that controls the contribution of each training sample to the optimization of the model.","Finally, we introduce a wide range HPE benchmark based on the CMU Panoptic data set."],"url":"http://arxiv.org/abs/2401.05807v1"}
{"created":"2024-01-11 10:10:16","title":"Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values","abstract":"The detection of anomalies in multivariate time series data is crucial for various practical applications, including smart power grids, traffic flow forecasting, and industrial process control. However, real-world time series data is usually not well-structured, posting significant challenges to existing approaches: (1) The existence of missing values in multivariate time series data along variable and time dimensions hinders the effective modeling of interwoven spatial and temporal dependencies, resulting in important patterns being overlooked during model training; (2) Anomaly scoring with irregularly-sampled observations is less explored, making it difficult to use existing detectors for multivariate series without fully-observed values. In this work, we introduce a novel framework called GST-Pro, which utilizes a graph spatiotemporal process and anomaly scorer to tackle the aforementioned challenges in detecting anomalies on irregularly-sampled multivariate time series. Our approach comprises two main components. First, we propose a graph spatiotemporal process based on neural controlled differential equations. This process enables effective modeling of multivariate time series from both spatial and temporal perspectives, even when the data contains missing values. Second, we present a novel distribution-based anomaly scoring mechanism that alleviates the reliance on complete uniform observations. By analyzing the predictions of the graph spatiotemporal process, our approach allows anomalies to be easily detected. Our experimental results show that the GST-Pro method can effectively detect anomalies in time series data and outperforms state-of-the-art methods, regardless of whether there are missing values present in the data. Our code is available: https://github.com/huankoh/GST-Pro.","sentences":["The detection of anomalies in multivariate time series data is crucial for various practical applications, including smart power grids, traffic flow forecasting, and industrial process control.","However, real-world time series data is usually not well-structured, posting significant challenges to existing approaches: (1) The existence of missing values in multivariate time series data along variable and time dimensions hinders the effective modeling of interwoven spatial and temporal dependencies, resulting in important patterns being overlooked during model training; (2) Anomaly scoring with irregularly-sampled observations is less explored, making it difficult to use existing detectors for multivariate series without fully-observed values.","In this work, we introduce a novel framework called GST-Pro, which utilizes a graph spatiotemporal process and anomaly scorer to tackle the aforementioned challenges in detecting anomalies on irregularly-sampled multivariate time series.","Our approach comprises two main components.","First, we propose a graph spatiotemporal process based on neural controlled differential equations.","This process enables effective modeling of multivariate time series from both spatial and temporal perspectives, even when the data contains missing values.","Second, we present a novel distribution-based anomaly scoring mechanism that alleviates the reliance on complete uniform observations.","By analyzing the predictions of the graph spatiotemporal process, our approach allows anomalies to be easily detected.","Our experimental results show that the GST-Pro method can effectively detect anomalies in time series data and outperforms state-of-the-art methods, regardless of whether there are missing values present in the data.","Our code is available: https://github.com/huankoh/GST-Pro."],"url":"http://arxiv.org/abs/2401.05800v1"}
{"created":"2024-01-11 10:06:42","title":"Designing Heterogeneous LLM Agents for Financial Sentiment Analysis","abstract":"Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.","sentences":["Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models.","This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context.","This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA.","Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed.","The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions.","Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial.","This study contributes to the design foundations and paves new avenues for LLMs-based FSA.","Implications on business and management are also discussed."],"url":"http://arxiv.org/abs/2401.05799v1"}
{"created":"2024-01-11 09:30:36","title":"EraseDiff: Erasing Data Influence in Diffusion Models","abstract":"In response to data protection regulations and the ``right to be forgotten'', in this work, we introduce an unlearning algorithm for diffusion models. Our algorithm equips a diffusion model with a mechanism to mitigate the concerns related to data memorization. To achieve this, we formulate the unlearning problem as a bi-level optimization problem, wherein the outer objective is to preserve the utility of the diffusion model on the remaining data. The inner objective aims to scrub the information associated with forgetting data by deviating the learnable generative process from the ground-truth denoising procedure. To solve the resulting bi-level problem, we adopt a first-order method, having superior practical performance while being vigilant about the diffusion process and solving a bi-level problem therein. Empirically, we demonstrate that our algorithm can preserve the model utility, effectiveness, and efficiency while removing across two widely-used diffusion models and in both conditional and unconditional image generation scenarios. In our experiments, we demonstrate the unlearning of classes, attributes, and even a race from face and object datasets such as UTKFace, CelebA, CelebA-HQ, and CIFAR10.","sentences":["In response to data protection regulations and the ``right to be forgotten'', in this work, we introduce an unlearning algorithm for diffusion models.","Our algorithm equips a diffusion model with a mechanism to mitigate the concerns related to data memorization.","To achieve this, we formulate the unlearning problem as a bi-level optimization problem, wherein the outer objective is to preserve the utility of the diffusion model on the remaining data.","The inner objective aims to scrub the information associated with forgetting data by deviating the learnable generative process from the ground-truth denoising procedure.","To solve the resulting bi-level problem, we adopt a first-order method, having superior practical performance while being vigilant about the diffusion process and solving a bi-level problem therein.","Empirically, we demonstrate that our algorithm can preserve the model utility, effectiveness, and efficiency while removing across two widely-used diffusion models and in both conditional and unconditional image generation scenarios.","In our experiments, we demonstrate the unlearning of classes, attributes, and even a race from face and object datasets such as UTKFace, CelebA, CelebA-HQ, and CIFAR10."],"url":"http://arxiv.org/abs/2401.05779v1"}
{"created":"2024-01-11 09:27:50","title":"Probing Structured Semantics Understanding and Generation of Language Models via Question Answering","abstract":"Recent advancement in the capabilities of large language models (LLMs) has triggered a new surge in LLMs' evaluation. Most recent evaluation works tends to evaluate the comprehensive ability of LLMs over series of tasks. However, the deep structure understanding of natural language is rarely explored. In this work, we examine the ability of LLMs to deal with structured semantics on the tasks of question answering with the help of the human-constructed formal language. Specifically, we implement the inter-conversion of natural and formal language through in-context learning of LLMs to verify their ability to understand and generate the structured logical forms. Extensive experiments with models of different sizes and in different formal languages show that today's state-of-the-art LLMs' understanding of the logical forms can approach human level overall, but there still are plenty of room in generating correct logical forms, which suggest that it is more effective to use LLMs to generate more natural language training data to reinforce a small model than directly answering questions with LLMs. Moreover, our results also indicate that models exhibit considerable sensitivity to different formal languages. In general, the formal language with the lower the formalization level, i.e. the more similar it is to natural language, is more LLMs-friendly.","sentences":["Recent advancement in the capabilities of large language models (LLMs) has triggered a new surge in LLMs' evaluation.","Most recent evaluation works tends to evaluate the comprehensive ability of LLMs over series of tasks.","However, the deep structure understanding of natural language is rarely explored.","In this work, we examine the ability of LLMs to deal with structured semantics on the tasks of question answering with the help of the human-constructed formal language.","Specifically, we implement the inter-conversion of natural and formal language through in-context learning of LLMs to verify their ability to understand and generate the structured logical forms.","Extensive experiments with models of different sizes and in different formal languages show that today's state-of-the-art LLMs' understanding of the logical forms can approach human level overall, but there still are plenty of room in generating correct logical forms, which suggest that it is more effective to use LLMs to generate more natural language training data to reinforce a small model than directly answering questions with LLMs.","Moreover, our results also indicate that models exhibit considerable sensitivity to different formal languages.","In general, the formal language with the lower the formalization level, i.e. the more similar it is to natural language, is more LLMs-friendly."],"url":"http://arxiv.org/abs/2401.05777v1"}
{"created":"2024-01-11 09:25:42","title":"Knowledge Translation: A New Pathway for Model Compression","abstract":"Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \\textbf{K}nowledge \\textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters. The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning. Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality. We propose a comprehensive framework for KT, introduce data augmentation strategies to enhance model performance despite restricted training data, and successfully demonstrate the feasibility of KT on the MNIST dataset. Code is available at \\url{https://github.com/zju-SWJ/KT}.","sentences":["Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead.","While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints.","To overcome these limitations, this paper presents a novel framework, termed \\textbf{K}nowledge \\textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters.","The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning.","Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality.","We propose a comprehensive framework for KT, introduce data augmentation strategies to enhance model performance despite restricted training data, and successfully demonstrate the feasibility of KT on the MNIST dataset.","Code is available at \\url{https://github.com/zju-SWJ/KT}."],"url":"http://arxiv.org/abs/2401.05772v1"}
{"created":"2024-01-11 09:22:36","title":"Evaluating Data Augmentation Techniques for Coffee Leaf Disease Classification","abstract":"The detection and classification of diseases in Robusta coffee leaves are essential to ensure that plants are healthy and the crop yield is kept high. However, this job requires extensive botanical knowledge and much wasted time. Therefore, this task and others similar to it have been extensively researched subjects in image classification. Regarding leaf disease classification, most approaches have used the more popular PlantVillage dataset while completely disregarding other datasets, like the Robusta Coffee Leaf (RoCoLe) dataset. As the RoCoLe dataset is imbalanced and does not have many samples, fine-tuning of pre-trained models and multiple augmentation techniques need to be used. The current paper uses the RoCoLe dataset and approaches based on deep learning for classifying coffee leaf diseases from images, incorporating the pix2pix model for segmentation and cycle-generative adversarial network (CycleGAN) for augmentation. Our study demonstrates the effectiveness of Transformer-based models, online augmentations, and CycleGAN augmentation in improving leaf disease classification. While synthetic data has limitations, it complements real data, enhancing model performance. These findings contribute to developing robust techniques for plant disease detection and classification.","sentences":["The detection and classification of diseases in Robusta coffee leaves are essential to ensure that plants are healthy and the crop yield is kept high.","However, this job requires extensive botanical knowledge and much wasted time.","Therefore, this task and others similar to it have been extensively researched subjects in image classification.","Regarding leaf disease classification, most approaches have used the more popular PlantVillage dataset while completely disregarding other datasets, like the Robusta Coffee Leaf (RoCoLe) dataset.","As the RoCoLe dataset is imbalanced and does not have many samples, fine-tuning of pre-trained models and multiple augmentation techniques need to be used.","The current paper uses the RoCoLe dataset and approaches based on deep learning for classifying coffee leaf diseases from images, incorporating the pix2pix model for segmentation and cycle-generative adversarial network (CycleGAN) for augmentation.","Our study demonstrates the effectiveness of Transformer-based models, online augmentations, and CycleGAN augmentation in improving leaf disease classification.","While synthetic data has limitations, it complements real data, enhancing model performance.","These findings contribute to developing robust techniques for plant disease detection and classification."],"url":"http://arxiv.org/abs/2401.05768v1"}
{"created":"2024-01-11 09:20:08","title":"Lifelogging As An Extreme Form of Personal Information Management -- What Lessons To Learn","abstract":"Personal data includes the digital footprints that we leave behind as part of our everyday activities, both online and offline in the real world. It includes data we collect ourselves, such as from wearables, as well as the data collected by others about our online behaviour and activities. Sometimes we are able to use the personal data we ourselves collect, in order to examine some parts of our lives but for the most part, our personal data is leveraged by third parties including internet companies, for services like targeted advertising and recommendations. Lifelogging is a form of extreme personal data gathering and in this article we present an overview of the tools used to manage access to lifelogs as demonstrated at the most recent of the annual Lifelog Search Challenge benchmarking workshops. Here, experimental systems are showcased in live, real time information seeking tasks by real users. This overview of these systems' capabilities show the range of possibilities for accessing our own personal data which may, in time, become more easily available as consumer-level services.","sentences":["Personal data includes the digital footprints that we leave behind as part of our everyday activities, both online and offline in the real world.","It includes data we collect ourselves, such as from wearables, as well as the data collected by others about our online behaviour and activities.","Sometimes we are able to use the personal data we ourselves collect, in order to examine some parts of our lives but for the most part, our personal data is leveraged by third parties including internet companies, for services like targeted advertising and recommendations.","Lifelogging is a form of extreme personal data gathering and in this article we present an overview of the tools used to manage access to lifelogs as demonstrated at the most recent of the annual Lifelog Search Challenge benchmarking workshops.","Here, experimental systems are showcased in live, real time information seeking tasks by real users.","This overview of these systems' capabilities show the range of possibilities for accessing our own personal data which may, in time, become more easily available as consumer-level services."],"url":"http://arxiv.org/abs/2401.05767v1"}
{"created":"2024-01-11 09:03:47","title":"BEC: Bit-Level Static Analysis for Reliability against Soft Errors","abstract":"Soft errors are a type of transient digital signal corruption that occurs in digital hardware components such as the internal flip-flops of CPU pipelines, the register file, memory cells, and even internal communication buses. Soft errors are caused by environmental radioactivity, magnetic interference, lasers, and temperature fluctuations, either unintentionally, or as part of a deliberate attempt to compromise a system and expose confidential data.   We propose a bit-level error coalescing (BEC) static program analysis and its two use cases to understand and improve program reliability against soft errors. The BEC analysis tracks each bit corruption in the register file and classifies the effect of the corruption by its semantics at compile time. The usefulness of the proposed analysis is demonstrated in two scenarios, fault injection campaign pruning, and reliability-aware program transformation. Experimental results show that bit-level analysis pruned up to 30.04 % of exhaustive fault injection campaigns (13.71 % on average), without loss of accuracy. Program vulnerability was reduced by up to 13.11 % (4.94 % on average) through bit-level vulnerability-aware instruction scheduling. The analysis has been implemented within LLVM and evaluated on the RISC-V architecture.   To the best of our knowledge, the proposed BEC analysis is the first bit-level compiler analysis for program reliability against soft errors. The proposed method is generic and not limited to a specific computer architecture.","sentences":["Soft errors are a type of transient digital signal corruption that occurs in digital hardware components such as the internal flip-flops of CPU pipelines, the register file, memory cells, and even internal communication buses.","Soft errors are caused by environmental radioactivity, magnetic interference, lasers, and temperature fluctuations, either unintentionally, or as part of a deliberate attempt to compromise a system and expose confidential data.   ","We propose a bit-level error coalescing (BEC) static program analysis and its two use cases to understand and improve program reliability against soft errors.","The BEC analysis tracks each bit corruption in the register file and classifies the effect of the corruption by its semantics at compile time.","The usefulness of the proposed analysis is demonstrated in two scenarios, fault injection campaign pruning, and reliability-aware program transformation.","Experimental results show that bit-level analysis pruned up to 30.04 % of exhaustive fault injection campaigns (13.71 % on average), without loss of accuracy.","Program vulnerability was reduced by up to 13.11 % (4.94 % on average) through bit-level vulnerability-aware instruction scheduling.","The analysis has been implemented within LLVM and evaluated on the RISC-V architecture.   ","To the best of our knowledge, the proposed BEC analysis is the first bit-level compiler analysis for program reliability against soft errors.","The proposed method is generic and not limited to a specific computer architecture."],"url":"http://arxiv.org/abs/2401.05753v1"}
{"created":"2024-01-11 09:00:22","title":"Learning Generalizable Models via Disentangling Spurious and Enhancing Potential Correlations","abstract":"Domain generalization (DG) intends to train a model on multiple source domains to ensure that it can generalize well to an arbitrary unseen target domain. The acquisition of domain-invariant representations is pivotal for DG as they possess the ability to capture the inherent semantic information of the data, mitigate the influence of domain shift, and enhance the generalization capability of the model. Adopting multiple perspectives, such as the sample and the feature, proves to be effective. The sample perspective facilitates data augmentation through data manipulation techniques, whereas the feature perspective enables the extraction of meaningful generalization features. In this paper, we focus on improving the generalization ability of the model by compelling it to acquire domain-invariant representations from both the sample and feature perspectives by disentangling spurious correlations and enhancing potential correlations. 1) From the sample perspective, we develop a frequency restriction module, guiding the model to focus on the relevant correlations between object features and labels, thereby disentangling spurious correlations. 2) From the feature perspective, the simple Tail Interaction module implicitly enhances potential correlations among all samples from all source domains, facilitating the acquisition of domain-invariant representations across multiple domains for the model. The experimental results show that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons (MLPs) with a strong baseline embedded with these two modules can achieve superior results, e.g., an average accuracy of 92.30% on Digits-DG.","sentences":["Domain generalization (DG) intends to train a model on multiple source domains to ensure that it can generalize well to an arbitrary unseen target domain.","The acquisition of domain-invariant representations is pivotal for DG as they possess the ability to capture the inherent semantic information of the data, mitigate the influence of domain shift, and enhance the generalization capability of the model.","Adopting multiple perspectives, such as the sample and the feature, proves to be effective.","The sample perspective facilitates data augmentation through data manipulation techniques, whereas the feature perspective enables the extraction of meaningful generalization features.","In this paper, we focus on improving the generalization ability of the model by compelling it to acquire domain-invariant representations from both the sample and feature perspectives by disentangling spurious correlations and enhancing potential correlations.","1) From the sample perspective, we develop a frequency restriction module, guiding the model to focus on the relevant correlations between object features and labels, thereby disentangling spurious correlations.","2) From the feature perspective, the simple Tail Interaction module implicitly enhances potential correlations among all samples from all source domains, facilitating the acquisition of domain-invariant representations across multiple domains for the model.","The experimental results show that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons (MLPs) with a strong baseline embedded with these two modules can achieve superior results, e.g., an average accuracy of 92.30% on Digits-DG."],"url":"http://arxiv.org/abs/2401.05752v1"}
{"created":"2024-01-11 08:56:13","title":"A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism","abstract":"We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.","sentences":["We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT).","Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages.","We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT.","Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web."],"url":"http://arxiv.org/abs/2401.05749v1"}
{"created":"2024-01-11 08:52:13","title":"Cross-Modality and Within-Modality Regularization for Audio-Visual DeepFake Detection","abstract":"Audio-visual deepfake detection scrutinizes manipulations in public video using complementary multimodal cues. Current methods, which train on fused multimodal data for multimodal targets face challenges due to uncertainties and inconsistencies in learned representations caused by independent modality manipulations in deepfake videos. To address this, we propose cross-modality and within-modality regularization to preserve modality distinctions during multimodal representation learning. Our approach includes an audio-visual transformer module for modality correspondence and a cross-modality regularization module to align paired audio-visual signals, preserving modality distinctions. Simultaneously, a within-modality regularization module refines unimodal representations with modality-specific targets to retain modal-specific details. Experimental results on the public audio-visual dataset, FakeAVCeleb, demonstrate the effectiveness and competitiveness of our approach.","sentences":["Audio-visual deepfake detection scrutinizes manipulations in public video using complementary multimodal cues.","Current methods, which train on fused multimodal data for multimodal targets face challenges due to uncertainties and inconsistencies in learned representations caused by independent modality manipulations in deepfake videos.","To address this, we propose cross-modality and within-modality regularization to preserve modality distinctions during multimodal representation learning.","Our approach includes an audio-visual transformer module for modality correspondence and a cross-modality regularization module to align paired audio-visual signals, preserving modality distinctions.","Simultaneously, a within-modality regularization module refines unimodal representations with modality-specific targets to retain modal-specific details.","Experimental results on the public audio-visual dataset, FakeAVCeleb, demonstrate the effectiveness and competitiveness of our approach."],"url":"http://arxiv.org/abs/2401.05746v1"}
{"created":"2024-01-11 08:48:40","title":"Consistent Query Answering for Existential Rules under Tuple-Deletion Semantics","abstract":"We study consistent query answering over knowledge bases expressed by existential rules. Specifically, we establish the data complexity of consistent query answering and repair checking under tuple-deletion semantics for a general class of disjunctive existential rules and for several subclasses thereof (acyclic, linear, full, guarded, and sticky). In particular, we identify several cases in which the above problems are tractable or even first-order rewritable, and present new query rewriting techniques that can be the basis for practical inconsistency-tolerant query answering systems.","sentences":["We study consistent query answering over knowledge bases expressed by existential rules.","Specifically, we establish the data complexity of consistent query answering and repair checking under tuple-deletion semantics for a general class of disjunctive existential rules and for several subclasses thereof (acyclic, linear, full, guarded, and sticky).","In particular, we identify several cases in which the above problems are tractable or even first-order rewritable, and present new query rewriting techniques that can be the basis for practical inconsistency-tolerant query answering systems."],"url":"http://arxiv.org/abs/2401.05743v1"}
{"created":"2024-01-11 08:12:47","title":"Zero Resource Cross-Lingual Part Of Speech Tagging","abstract":"Part of speech tagging in zero-resource settings can be an effective approach for low-resource languages when no labeled training data is available. Existing systems use two main techniques for POS tagging i.e. pretrained multilingual large language models(LLM) or project the source language labels into the zero resource target language and train a sequence labeling model on it. We explore the latter approach using the off-the-shelf alignment module and train a hidden Markov model(HMM) to predict the POS tags. We evaluate transfer learning setup with English as a source language and French, German, and Spanish as target languages for part-of-speech tagging. Our conclusion is that projected alignment data in zero-resource language can be beneficial to predict POS tags.","sentences":["Part of speech tagging in zero-resource settings can be an effective approach for low-resource languages when no labeled training data is available.","Existing systems use two main techniques for POS tagging i.e. pretrained multilingual large language models(LLM) or project the source language labels into the zero resource target language and train a sequence labeling model on it.","We explore the latter approach using the off-the-shelf alignment module and train a hidden Markov model(HMM) to predict the POS tags.","We evaluate transfer learning setup with English as a source language and French, German, and Spanish as target languages for part-of-speech tagging.","Our conclusion is that projected alignment data in zero-resource language can be beneficial to predict POS tags."],"url":"http://arxiv.org/abs/2401.05727v1"}
{"created":"2024-01-11 07:42:19","title":"Recoverable robust shortest path problem under interval uncertainty representations","abstract":"In this paper, the recoverable robust shortest path problem under interval uncertainty representations is discussed. This problem is known to be strongly NP-hard and also hard to approximate in general digraphs. In this paper, the class of acyclic digraphs is considered. It is shown that for the traditional interval uncertainty, the problem can be solved in polynomial time for all natural, known from the literature, neighborhoods. Efficient algorithms for various classes of acyclic digraphs are constructed. Some negative results for general digraphs are strengthened. Finally, some exact and approximate methods of solving the problem under budgeted interval uncertainty are proposed.","sentences":["In this paper, the recoverable robust shortest path problem under interval uncertainty representations is discussed.","This problem is known to be strongly NP-hard and also hard to approximate in general digraphs.","In this paper, the class of acyclic digraphs is considered.","It is shown that for the traditional interval uncertainty, the problem can be solved in polynomial time for all natural, known from the literature, neighborhoods.","Efficient algorithms for various classes of acyclic digraphs are constructed.","Some negative results for general digraphs are strengthened.","Finally, some exact and approximate methods of solving the problem under budgeted interval uncertainty are proposed."],"url":"http://arxiv.org/abs/2401.05715v1"}
{"created":"2024-01-11 07:27:58","title":"BOD: Blindly Optimal Data Discovery","abstract":"Combining discovery and augmentation is important in the era of data usage when it comes to predicting the outcome of tasks. However, having to ask the user the utility function to discover the goal to achieve the optimal small rightful dataset is not an optimal solution. The existing solutions do not make good use of this combination, hence underutilizing the data. In this paper, we introduce a novel goal-oriented framework, called BOD: Blindly Optimal Data Discovery, that involves humans in the loop and comparing utility scores every time querying in the process without knowing the utility function. This establishes the promise of using BOD: Blindly Optimal Data Discovery for modern data science solutions.","sentences":["Combining discovery and augmentation is important in the era of data usage when it comes to predicting the outcome of tasks.","However, having to ask the user the utility function to discover the goal to achieve the optimal small rightful dataset is not an optimal solution.","The existing solutions do not make good use of this combination, hence underutilizing the data.","In this paper, we introduce a novel goal-oriented framework, called BOD: Blindly Optimal Data Discovery, that involves humans in the loop and comparing utility scores every time querying in the process without knowing the utility function.","This establishes the promise of using BOD: Blindly Optimal Data Discovery for modern data science solutions."],"url":"http://arxiv.org/abs/2401.05712v1"}
{"created":"2024-01-11 07:26:32","title":"Dynamic Indoor Fingerprinting Localization based on Few-Shot Meta-Learning with CSI Images","abstract":"While fingerprinting localization is favored for its effectiveness, it is hindered by high data acquisition costs and the inaccuracy of static database-based estimates. Addressing these issues, this letter presents an innovative indoor localization method using a data-efficient meta-learning algorithm. This approach, grounded in the ``Learning to Learn'' paradigm of meta-learning, utilizes historical localization tasks to improve adaptability and learning efficiency in dynamic indoor environments. We introduce a task-weighted loss to enhance knowledge transfer within this framework. Our comprehensive experiments confirm the method's robustness and superiority over current benchmarks, achieving a notable 23.13\\% average gain in Mean Euclidean Distance, particularly effective in scenarios with limited CSI data.","sentences":["While fingerprinting localization is favored for its effectiveness, it is hindered by high data acquisition costs and the inaccuracy of static database-based estimates.","Addressing these issues, this letter presents an innovative indoor localization method using a data-efficient meta-learning algorithm.","This approach, grounded in the ``Learning to Learn'' paradigm of meta-learning, utilizes historical localization tasks to improve adaptability and learning efficiency in dynamic indoor environments.","We introduce a task-weighted loss to enhance knowledge transfer within this framework.","Our comprehensive experiments confirm the method's robustness and superiority over current benchmarks, achieving a notable 23.13\\% average gain in Mean Euclidean Distance, particularly effective in scenarios with limited CSI data."],"url":"http://arxiv.org/abs/2401.05711v1"}
{"created":"2024-01-11 07:20:59","title":"FeReX: A Reconfigurable Design of Multi-bit Ferroelectric Compute-in-Memory for Nearest Neighbor Search","abstract":"Rapid advancements in artificial intelligence have given rise to transformative models, profoundly impacting our lives. These models demand massive volumes of data to operate effectively, exacerbating the data-transfer bottleneck inherent in the conventional von-Neumann architecture. Compute-in-memory (CIM), a novel computing paradigm, tackles these issues by seamlessly embedding in-memory search functions, thereby obviating the need for data transfers. However, existing non-volatile memory (NVM)-based accelerators are application specific. During the similarity based associative search operation, they only support a single, specific distance metric, such as Hamming, Manhattan, or Euclidean distance in measuring the query against the stored data, calling for reconfigurable in-memory solutions adaptable to various applications. To overcome such a limitation, in this paper, we present FeReX, a reconfigurable associative memory (AM) that accommodates various distance metrics including Hamming, Manhattan, and Euclidean distances. Leveraging multi-bit ferroelectric field-effect transistors (FeFETs) as the proxy and a hardware-software co-design approach, we introduce a constrained satisfaction problem (CSP)-based method to automate AM search input voltage and stored voltage configurations for different distance based search functions. Device-circuit co-simulations first validate the effectiveness of the proposed FeReX methodology for reconfigurable search distance functions. Then, we benchmark FeReX in the context of k-nearest neighbor (KNN) and hyperdimensional computing (HDC), which highlights the robustness of FeReX and demonstrates up to 250x speedup and 10^4 energy savings compared with GPU.","sentences":["Rapid advancements in artificial intelligence have given rise to transformative models, profoundly impacting our lives.","These models demand massive volumes of data to operate effectively, exacerbating the data-transfer bottleneck inherent in the conventional von-Neumann architecture.","Compute-in-memory (CIM), a novel computing paradigm, tackles these issues by seamlessly embedding in-memory search functions, thereby obviating the need for data transfers.","However, existing non-volatile memory (NVM)-based accelerators are application specific.","During the similarity based associative search operation, they only support a single, specific distance metric, such as Hamming, Manhattan, or Euclidean distance in measuring the query against the stored data, calling for reconfigurable in-memory solutions adaptable to various applications.","To overcome such a limitation, in this paper, we present FeReX, a reconfigurable associative memory (AM) that accommodates various distance metrics including Hamming, Manhattan, and Euclidean distances.","Leveraging multi-bit ferroelectric field-effect transistors (FeFETs) as the proxy and a hardware-software co-design approach, we introduce a constrained satisfaction problem (CSP)-based method to automate AM search input voltage and stored voltage configurations for different distance based search functions.","Device-circuit co-simulations first validate the effectiveness of the proposed FeReX methodology for reconfigurable search distance functions.","Then, we benchmark FeReX in the context of k-nearest neighbor (KNN) and hyperdimensional computing (HDC), which highlights the robustness of FeReX and demonstrates up to 250x speedup and 10^4 energy savings compared with GPU."],"url":"http://arxiv.org/abs/2401.05708v1"}
{"created":"2024-01-11 07:09:44","title":"Video Anomaly Detection and Explanation via Large Language Models","abstract":"Video Anomaly Detection (VAD) aims to localize abnormal events on the timeline of long-range surveillance videos. Anomaly-scoring-based methods have been prevailing for years but suffer from the high complexity of thresholding and low explanability of detection results. In this paper, we conduct pioneer research on equipping video-based large language models (VLLMs) in the framework of VAD, making the VAD model free from thresholds and able to explain the reasons for the detected anomalies. We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling. We design a three-phase training method to improve the efficiency of fine-tuning VLLMs by substantially minimizing the requirements for VAD data and lowering the costs of annotating instruction-tuning data. Our trained model achieves the top performance on the anomaly videos of the UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\\% and +4.96\\%, respectively. More impressively, our approach can provide textual explanations for detected anomalies.","sentences":["Video Anomaly Detection (VAD) aims to localize abnormal events on the timeline of long-range surveillance videos.","Anomaly-scoring-based methods have been prevailing for years but suffer from the high complexity of thresholding and low explanability of detection results.","In this paper, we conduct pioneer research on equipping video-based large language models (VLLMs) in the framework of VAD, making the VAD model free from thresholds and able to explain the reasons for the detected anomalies.","We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling.","We design a three-phase training method to improve the efficiency of fine-tuning VLLMs by substantially minimizing the requirements for VAD data and lowering the costs of annotating instruction-tuning data.","Our trained model achieves the top performance on the anomaly videos of the UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\\% and +4.96\\%, respectively.","More impressively, our approach can provide textual explanations for detected anomalies."],"url":"http://arxiv.org/abs/2401.05702v1"}
{"created":"2024-01-11 07:00:07","title":"HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition","abstract":"Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in recent years for its critical role in creating emotion-ware intelligent machines. Previous efforts in this area are dominated by the supervised learning paradigm. Despite significant progress, supervised learning is meeting its bottleneck due to the longstanding data scarcity issue in AVER. Motivated by recent advances in self-supervised learning, we propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that leverages large-scale self-supervised pre-training on vast unlabeled audio-visual data to promote the advancement of AVER. Following prior arts in self-supervised audio-visual representation learning, HiCMAE adopts two primary forms of self-supervision for pre-training, namely masked data modeling and contrastive learning. Unlike them which focus exclusively on top-layer representations while neglecting explicit guidance of intermediate layers, HiCMAE develops a three-pronged strategy to foster hierarchical audio-visual feature learning and improve the overall quality of learned representations. To verify the effectiveness of HiCMAE, we conduct extensive experiments on 9 datasets covering both categorical and dimensional AVER tasks. Experimental results show that our method significantly outperforms state-of-the-art supervised and self-supervised audio-visual methods, which indicates that HiCMAE is a powerful audio-visual emotion representation learner. Codes and models will be publicly available at https://github.com/sunlicai/HiCMAE.","sentences":["Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in recent years for its critical role in creating emotion-ware intelligent machines.","Previous efforts in this area are dominated by the supervised learning paradigm.","Despite significant progress, supervised learning is meeting its bottleneck due to the longstanding data scarcity issue in AVER.","Motivated by recent advances in self-supervised learning, we propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that leverages large-scale self-supervised pre-training on vast unlabeled audio-visual data to promote the advancement of AVER.","Following prior arts in self-supervised audio-visual representation learning, HiCMAE adopts two primary forms of self-supervision for pre-training, namely masked data modeling and contrastive learning.","Unlike them which focus exclusively on top-layer representations while neglecting explicit guidance of intermediate layers, HiCMAE develops a three-pronged strategy to foster hierarchical audio-visual feature learning and improve the overall quality of learned representations.","To verify the effectiveness of HiCMAE, we conduct extensive experiments on 9 datasets covering both categorical and dimensional AVER tasks.","Experimental results show that our method significantly outperforms state-of-the-art supervised and self-supervised audio-visual methods, which indicates that HiCMAE is a powerful audio-visual emotion representation learner.","Codes and models will be publicly available at https://github.com/sunlicai/HiCMAE."],"url":"http://arxiv.org/abs/2401.05698v1"}
{"created":"2024-01-11 06:42:45","title":"Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback","abstract":"The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single-round medical Q&A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies. To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback. Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue tasks, showcasing its potential for improving medical dialogue generation.","sentences":["The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency.","While previous studies have made progress in optimizing model performance for single-round medical Q&A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies.","To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs.","PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process.","Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback.","Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue tasks, showcasing its potential for improving medical dialogue generation."],"url":"http://arxiv.org/abs/2401.05695v1"}
{"created":"2024-01-11 06:30:07","title":"UCorrect: An Unsupervised Framework for Automatic Speech Recognition Error Correction","abstract":"Error correction techniques have been used to refine the output sentences from automatic speech recognition (ASR) models and achieve a lower word error rate (WER). Previous works usually adopt end-to-end models and has strong dependency on Pseudo Paired Data and Original Paired Data. But when only pre-training on Pseudo Paired Data, previous models have negative effect on correction. While fine-tuning on Original Paired Data, the source side data must be transcribed by a well-trained ASR model, which takes a lot of time and not universal. In this paper, we propose UCorrect, an unsupervised Detector-Generator-Selector framework for ASR Error Correction. UCorrect has no dependency on the training data mentioned before. The whole procedure is first to detect whether the character is erroneous, then to generate some candidate characters and finally to select the most confident one to replace the error character. Experiments on the public AISHELL-1 dataset and WenetSpeech dataset show the effectiveness of UCorrect for ASR error correction: 1) it achieves significant WER reduction, achieves 6.83\\% even without fine-tuning and 14.29\\% after fine-tuning; 2) it outperforms the popular NAR correction models by a large margin with a competitive low latency; and 3) it is an universal method, as it reduces all WERs of the ASR model with different decoding strategies and reduces all WERs of ASR models trained on different scale datasets.","sentences":["Error correction techniques have been used to refine the output sentences from automatic speech recognition (ASR) models and achieve a lower word error rate (WER).","Previous works usually adopt end-to-end models and has strong dependency on Pseudo Paired Data and Original Paired Data.","But when only pre-training on Pseudo Paired Data, previous models have negative effect on correction.","While fine-tuning on Original Paired Data, the source side data must be transcribed by a well-trained ASR model, which takes a lot of time and not universal.","In this paper, we propose UCorrect, an unsupervised Detector-Generator-Selector framework for ASR Error Correction.","UCorrect has no dependency on the training data mentioned before.","The whole procedure is first to detect whether the character is erroneous, then to generate some candidate characters and finally to select the most confident one to replace the error character.","Experiments on the public AISHELL-1 dataset and WenetSpeech dataset show the effectiveness of UCorrect for ASR error correction: 1) it achieves significant WER reduction, achieves 6.83\\% even without fine-tuning and 14.29\\% after fine-tuning; 2) it outperforms the popular NAR correction models by a large margin with a competitive low latency; and 3) it is an universal method, as it reduces all WERs of the ASR model with different decoding strategies and reduces all WERs of ASR models trained on different scale datasets."],"url":"http://arxiv.org/abs/2401.05689v1"}
{"created":"2024-01-11 05:56:29","title":"Use of Graph Neural Networks in Aiding Defensive Cyber Operations","abstract":"In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most renowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address each phase of CKC and discuss how GNNs contribute to preparing and preventing an attack from a defensive standpoint. Furthermore, We also discuss open research areas and further improvement scopes.","sentences":["In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information.","Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems.","However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle.","These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle.","Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data.","In this paper, we look into the application of GNNs in aiding to break each stage of one of the most renowned attack life cycles, the Lockheed Martin Cyber Kill Chain.","We address each phase of CKC and discuss how GNNs contribute to preparing and preventing an attack from a defensive standpoint.","Furthermore, We also discuss open research areas and further improvement scopes."],"url":"http://arxiv.org/abs/2401.05680v1"}
{"created":"2024-01-11 04:59:44","title":"EsaCL: Efficient Continual Learning of Sparse Models","abstract":"A key challenge in the continual learning setting is to efficiently learn a sequence of tasks without forgetting how to perform previously learned tasks. Many existing approaches to this problem work by either retraining the model on previous tasks or by expanding the model to accommodate new tasks. However, these approaches typically suffer from increased storage and computational requirements, a problem that is worsened in the case of sparse models due to need for expensive re-training after sparsification. To address this challenge, we propose a new method for efficient continual learning of sparse models (EsaCL) that can automatically prune redundant parameters without adversely impacting the model's predictive power, and circumvent the need of retraining. We conduct a theoretical analysis of loss landscapes with parameter pruning, and design a directional pruning (SDP) strategy that is informed by the sharpness of the loss function with respect to the model parameters. SDP ensures model with minimal loss of predictive accuracy, accelerating the learning of sparse models at each stage. To accelerate model update, we introduce an intelligent data selection (IDS) strategy that can identify critical instances for estimating loss landscape, yielding substantially improved data efficiency. The results of our experiments show that EsaCL achieves performance that is competitive with the state-of-the-art methods on three continual learning benchmarks, while using substantially reduced memory and computational resources.","sentences":["A key challenge in the continual learning setting is to efficiently learn a sequence of tasks without forgetting how to perform previously learned tasks.","Many existing approaches to this problem work by either retraining the model on previous tasks or by expanding the model to accommodate new tasks.","However, these approaches typically suffer from increased storage and computational requirements, a problem that is worsened in the case of sparse models due to need for expensive re-training after sparsification.","To address this challenge, we propose a new method for efficient continual learning of sparse models (EsaCL) that can automatically prune redundant parameters without adversely impacting the model's predictive power, and circumvent the need of retraining.","We conduct a theoretical analysis of loss landscapes with parameter pruning, and design a directional pruning (SDP) strategy that is informed by the sharpness of the loss function with respect to the model parameters.","SDP ensures model with minimal loss of predictive accuracy, accelerating the learning of sparse models at each stage.","To accelerate model update, we introduce an intelligent data selection (IDS) strategy that can identify critical instances for estimating loss landscape, yielding substantially improved data efficiency.","The results of our experiments show that EsaCL achieves performance that is competitive with the state-of-the-art methods on three continual learning benchmarks, while using substantially reduced memory and computational resources."],"url":"http://arxiv.org/abs/2401.05667v1"}
{"created":"2024-01-11 04:57:08","title":"Augmented Reality User Interface for Command, Control, and Supervision of Large Multi-Agent Teams","abstract":"Multi-agent human-robot teaming allows for the potential to gather information about various environments more efficiently by exploiting and combining the strengths of humans and robots. In industries like defense, search and rescue, first-response, and others alike, heterogeneous human-robot teams show promise to accelerate data collection and improve team safety by removing humans from unknown and potentially hazardous situations. This work builds upon AugRE, an Augmented Reality (AR) based scalable human-robot teaming framework. It enables users to localize and communicate with 50+ autonomous agents. Through our efforts, users are able to command, control, and supervise agents in large teams, both line-of-sight and non-line-of-sight, without the need to modify the environment prior and without requiring users to use typical hardware (i.e. joysticks, keyboards, laptops, tablets, etc.) in the field. The demonstrated work shows early indications that combining these AR-HMD-based user interaction modalities for command, control, and supervision will help improve human-robot team collaboration, robustness, and trust.","sentences":["Multi-agent human-robot teaming allows for the potential to gather information about various environments more efficiently by exploiting and combining the strengths of humans and robots.","In industries like defense, search and rescue, first-response, and others alike, heterogeneous human-robot teams show promise to accelerate data collection and improve team safety by removing humans from unknown and potentially hazardous situations.","This work builds upon AugRE, an Augmented Reality (AR) based scalable human-robot teaming framework.","It enables users to localize and communicate with 50+ autonomous agents.","Through our efforts, users are able to command, control, and supervise agents in large teams, both line-of-sight and non-line-of-sight, without the need to modify the environment prior and without requiring users to use typical hardware (i.e. joysticks, keyboards, laptops, tablets, etc.) in the field.","The demonstrated work shows early indications that combining these AR-HMD-based user interaction modalities for command, control, and supervision will help improve human-robot team collaboration, robustness, and trust."],"url":"http://arxiv.org/abs/2401.05665v1"}
{"created":"2024-01-11 04:54:52","title":"Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow","abstract":"Energy efficiency is a big concern in industrial sectors. Finding the root cause of anomaly state of energy efficiency can help to improve energy efficiency of industrial systems and therefore save energy cost. In this research, we propose to use transfer entropy (TE) for root cause analysis on energy efficiency of industrial systems. A method, called TE flow, is proposed in that a TE flow from physical measurements of each subsystem to the energy efficiency indicator along timeline is considered as causal strength for diagnosing root cause of anomaly states of energy efficiency of a system. The copula entropy-based nonparametric TE estimator is used in the proposed method. We conducted experiments on real data collected from a compressing air system to verify the proposed method. Experimental results show that the TE flow method successfully identified the root cause of the energy (in)efficiency of the system.","sentences":["Energy efficiency is a big concern in industrial sectors.","Finding the root cause of anomaly state of energy efficiency can help to improve energy efficiency of industrial systems and therefore save energy cost.","In this research, we propose to use transfer entropy (TE) for root cause analysis on energy efficiency of industrial systems.","A method, called TE flow, is proposed in that a TE flow from physical measurements of each subsystem to the energy efficiency indicator along timeline is considered as causal strength for diagnosing root cause of anomaly states of energy efficiency of a system.","The copula entropy-based nonparametric TE estimator is used in the proposed method.","We conducted experiments on real data collected from a compressing air system to verify the proposed method.","Experimental results show that the TE flow method successfully identified the root cause of the energy (in)efficiency of the system."],"url":"http://arxiv.org/abs/2401.05664v1"}
{"created":"2024-01-11 04:28:02","title":"Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive Investigation of Accuracy, Fairness, and Generalizability","abstract":"Automatic Essay Scoring (AES) is a well-established educational pursuit that employs machine learning to evaluate student-authored essays. While much effort has been made in this area, current research primarily focuses on either (i) boosting the predictive accuracy of an AES model for a specific prompt (i.e., developing prompt-specific models), which often heavily relies on the use of the labeled data from the same target prompt; or (ii) assessing the applicability of AES models developed on non-target prompts to the intended target prompt (i.e., developing the AES models in a cross-prompt setting). Given the inherent bias in machine learning and its potential impact on marginalized groups, it is imperative to investigate whether such bias exists in current AES methods and, if identified, how it intervenes with an AES model's accuracy and generalizability. Thus, our study aimed to uncover the intricate relationship between an AES model's accuracy, fairness, and generalizability, contributing practical insights for developing effective AES models in real-world education. To this end, we meticulously selected nine prominent AES methods and evaluated their performance using seven metrics on an open-sourced dataset, which contains over 25,000 essays and various demographic information about students such as gender, English language learner status, and economic status. Through extensive evaluations, we demonstrated that: (1) prompt-specific models tend to outperform their cross-prompt counterparts in terms of predictive accuracy; (2) prompt-specific models frequently exhibit a greater bias towards students of different economic statuses compared to cross-prompt models; (3) in the pursuit of generalizability, traditional machine learning models coupled with carefully engineered features hold greater potential for achieving both high accuracy and fairness than complex neural network models.","sentences":["Automatic Essay Scoring (AES) is a well-established educational pursuit that employs machine learning to evaluate student-authored essays.","While much effort has been made in this area, current research primarily focuses on either (i) boosting the predictive accuracy of an AES model for a specific prompt (i.e., developing prompt-specific models), which often heavily relies on the use of the labeled data from the same target prompt; or (ii) assessing the applicability of AES models developed on non-target prompts to the intended target prompt (i.e., developing the AES models in a cross-prompt setting).","Given the inherent bias in machine learning and its potential impact on marginalized groups, it is imperative to investigate whether such bias exists in current AES methods and, if identified, how it intervenes with an AES model's accuracy and generalizability.","Thus, our study aimed to uncover the intricate relationship between an AES model's accuracy, fairness, and generalizability, contributing practical insights for developing effective AES models in real-world education.","To this end, we meticulously selected nine prominent AES methods and evaluated their performance using seven metrics on an open-sourced dataset, which contains over 25,000 essays and various demographic information about students such as gender, English language learner status, and economic status.","Through extensive evaluations, we demonstrated that: (1) prompt-specific models tend to outperform their cross-prompt counterparts in terms of predictive accuracy; (2) prompt-specific models frequently exhibit a greater bias towards students of different economic statuses compared to cross-prompt models; (3) in the pursuit of generalizability, traditional machine learning models coupled with carefully engineered features hold greater potential for achieving both high accuracy and fairness than complex neural network models."],"url":"http://arxiv.org/abs/2401.05655v1"}
{"created":"2024-01-11 04:24:19","title":"Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression","abstract":"This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM). Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions. Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct. Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel. We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches.","sentences":["This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM).","Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions.","Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct.","Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel.","We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches."],"url":"http://arxiv.org/abs/2401.05653v1"}
{"created":"2024-01-11 03:34:21","title":"Optimistic Prediction of Synchronization-Reversal Data Races","abstract":"Dynamic data race detection has emerged as a key technique for ensuring reliability of concurrent software in practice. However, dynamic approaches can often miss data races owing to nondeterminism in the thread scheduler. Predictive race detection techniques cater to this shortcoming by inferring alternate executions that may expose data races without re-executing the underlying program. More formally, the dynamic data race prediction problem asks, given a trace \\sigma of an execution of a concurrent program, can \\sigma be correctly reordered to expose a data race? Existing state-of-the art techniques for data race prediction either do not scale to executions arising from real world concurrent software, or only expose a limited class of data races, such as those that can be exposed without reversing the order of synchronization operations.   In general, exposing data races by reasoning about synchronization reversals is an intractable problem. In this work, we identify a class of data races, called Optimistic Sync(hronization)-Reversal races that can be detected in a tractable manner and often include non-trivial data races that cannot be exposed by prior tractable techniques. We also propose a sound algorithm OSR for detecting all optimistic sync-reversal data races in overall quadratic time, and show that the algorithm is optimal by establishing a matching lower bound. Our experiments demonstrate the effectiveness of OSR on our extensive suite of benchmarks, OSR reports the largest number of data races, and scales well to large execution traces.","sentences":["Dynamic data race detection has emerged as a key technique for ensuring reliability of concurrent software in practice.","However, dynamic approaches can often miss data races owing to nondeterminism in the thread scheduler.","Predictive race detection techniques cater to this shortcoming by inferring alternate executions that may expose data races without re-executing the underlying program.","More formally, the dynamic data race prediction problem asks, given a trace \\sigma of an execution of a concurrent program, can \\sigma be correctly reordered to expose a data race?","Existing state-of-the art techniques for data race prediction either do not scale to executions arising from real world concurrent software, or only expose a limited class of data races, such as those that can be exposed without reversing the order of synchronization operations.   ","In general, exposing data races by reasoning about synchronization reversals is an intractable problem.","In this work, we identify a class of data races, called Optimistic Sync(hronization)-Reversal races that can be detected in a tractable manner and often include non-trivial data races that cannot be exposed by prior tractable techniques.","We also propose a sound algorithm OSR for detecting all optimistic sync-reversal data races in overall quadratic time, and show that the algorithm is optimal by establishing a matching lower bound.","Our experiments demonstrate the effectiveness of OSR on our extensive suite of benchmarks, OSR reports the largest number of data races, and scales well to large execution traces."],"url":"http://arxiv.org/abs/2401.05642v1"}
{"created":"2024-01-11 03:30:50","title":"When eBPF Meets Machine Learning: On-the-fly OS Kernel Compartmentalization","abstract":"Compartmentalization effectively prevents initial corruption from turning into a successful attack. This paper presents O2C, a pioneering system designed to enforce OS kernel compartmentalization on the fly. It not only provides immediate remediation for sudden threats but also maintains consistent system availability through the enforcement process.   O2C is empowered by the newest advancements of the eBPF ecosystem which allows to instrument eBPF programs that perform enforcement actions into the kernel at runtime. O2C takes the lead in embedding a machine learning model into eBPF programs, addressing unique challenges in on-the-fly compartmentalization. Our comprehensive evaluation shows that O2C effectively confines damage within the compartment. Further, we validate that decision tree is optimally suited for O2C owing to its advantages in processing tabular data, its explainable nature, and its compliance with the eBPF ecosystem. Last but not least, O2C is lightweight, showing negligible overhead and excellent sacalability system-wide.","sentences":["Compartmentalization effectively prevents initial corruption from turning into a successful attack.","This paper presents O2C, a pioneering system designed to enforce OS kernel compartmentalization on the fly.","It not only provides immediate remediation for sudden threats but also maintains consistent system availability through the enforcement process.   ","O2C is empowered by the newest advancements of the eBPF ecosystem which allows to instrument eBPF programs that perform enforcement actions into the kernel at runtime.","O2C takes the lead in embedding a machine learning model into eBPF programs, addressing unique challenges in on-the-fly compartmentalization.","Our comprehensive evaluation shows that O2C effectively confines damage within the compartment.","Further, we validate that decision tree is optimally suited for O2C owing to its advantages in processing tabular data, its explainable nature, and its compliance with the eBPF ecosystem.","Last but not least, O2C is lightweight, showing negligible overhead and excellent sacalability system-wide."],"url":"http://arxiv.org/abs/2401.05641v1"}
{"created":"2024-01-11 02:50:59","title":"Faster Multi-Source Directed Reachability via Shortcuts and Matrix Multiplication","abstract":"Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a set $S \\subseteq V$, $|S| = n^{\\sigma}$ (for some $0 < \\sigma \\le 1$) of designated sources, the $S \\times V$-direcahability problem is to compute for every $s \\in S$, the set of all the vertices reachable from $s$ in $G$. Known naive algorithms for this problem either run a BFS/DFS separately from every source, and as a result require $O(m \\cdot n^{\\sigma})$ time, or compute the transitive closure of $G$ in $\\tilde O(n^{\\omega})$ time, where $\\omega < 2.371552\\ldots$ is the matrix multiplication exponent. Hence, the current state-of-the-art bound for the problem on graphs with $m = \\Theta(n^{\\mu})$ edges in $\\tilde O(n^{\\min \\{\\mu + \\sigma, \\omega \\}})$. Our first contribution is an algorithm with running time $\\tilde O(n^{1 + \\tiny{\\frac{2}{3}} \\omega(\\sigma)})$ for this problem, where $\\omega(\\sigma)$ is the rectangular matrix multiplication exponent. Using current state-of-the-art estimates on $\\omega(\\sigma)$, our exponent is better than $\\min \\{2 + \\sigma, \\omega \\}$ for $\\tilde \\sigma \\le \\sigma \\le 0.53$, where $1/3 < \\tilde \\sigma < 0.3336$ is a universal constant.   Our second contribution is a sequence of algorithms $\\mathcal A_0, \\mathcal A_1, \\mathcal A_2, \\ldots$ for the $S \\times V$-direachability problem. We argue that under a certain assumption that we introduce, for every $\\tilde \\sigma \\le \\sigma < 1$, there exists a sufficiently large index $k = k(\\sigma)$ so that $\\mathcal A_k$ improves upon the current state-of-the-art bounds for $S \\times V$-direachability with $|S| = n^{\\sigma}$, in the densest regime $\\mu =2$. We show that to prove this assumption, it is sufficient to devise an algorithm that computes a rectangular max-min matrix product roughly as efficiently as ordinary $(+, \\cdot)$ matrix product.   Our algorithms heavily exploit recent constructions of directed shortcuts by Kogan and Parter.","sentences":["Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a set $S \\subseteq V$, $|S| = n^{\\sigma}$ (for some $0 <","\\sigma \\le 1$) of designated sources, the $S \\times V$-direcahability problem is to compute for every $s \\in S$, the set of all the vertices reachable from $s$ in $G$. Known naive algorithms for this problem either run a BFS/DFS separately from every source, and as a result require $O(m \\cdot n^{\\sigma})$ time, or compute the transitive closure of $G$ in $\\tilde O(n^{\\omega})$ time, where $\\omega < 2.371552\\ldots$ is the matrix multiplication exponent.","Hence, the current state-of-the-art bound for the problem on graphs with $m = \\Theta(n^{\\mu})$ edges in $\\tilde O(n^{\\min \\{\\mu +","\\sigma, \\omega \\}})$.","Our first contribution is an algorithm with running time $\\tilde O(n^{1 + \\tiny{\\frac{2}{3}} \\omega(\\sigma)})$ for this problem, where $\\omega(\\sigma)$ is the rectangular matrix multiplication exponent.","Using current state-of-the-art estimates on $\\omega(\\sigma)$, our exponent is better than $\\min \\{2 + \\sigma, \\omega \\}$ for $\\tilde \\sigma \\le \\sigma \\le 0.53$, where $1/3 <","\\tilde \\sigma < 0.3336$ is a universal constant.   ","Our second contribution is a sequence of algorithms $\\mathcal A_0, \\mathcal A_1, \\mathcal A_2, \\ldots$ for the $S \\times V$-direachability problem.","We argue that under a certain assumption that we introduce, for every $\\tilde \\sigma \\le \\sigma < 1$, there exists a sufficiently large index $k = k(\\sigma)$ so that $\\mathcal A_k$ improves upon the current state-of-the-art bounds for $S \\times V$-direachability with $|S| = n^{\\sigma}$, in the densest regime $\\mu =2$. We show that to prove this assumption, it is sufficient to devise an algorithm that computes a rectangular max-min matrix product roughly as efficiently as ordinary $(+, \\cdot)$ matrix product.   ","Our algorithms heavily exploit recent constructions of directed shortcuts by Kogan and Parter."],"url":"http://arxiv.org/abs/2401.05628v1"}
{"created":"2024-01-11 02:48:23","title":"Deterministic Near-Linear Time Minimum Cut in Weighted Graphs","abstract":"In 1996, Karger [Kar96] gave a startling randomized algorithm that finds a minimum-cut in a (weighted) graph in time $O(m\\log^3n)$ which he termed near-linear time meaning linear (in the size of the input) times a polylogarthmic factor. In this paper, we give the first deterministic algorithm which runs in near-linear time for weighted graphs.   Previously, the breakthrough results of Kawarabayashi and Thorup [KT19] gave a near-linear time algorithm for simple graphs. The main technique here is a clustering procedure that perfectly preserves minimum cuts. Recently, Li [Li21] gave an $m^{1+o(1)}$ deterministic minimum-cut algorithm for weighted graphs; this form of running time has been termed \"almost-linear''. Li uses almost-linear time deterministic expander decompositions which do not perfectly preserve minimum cuts, but he can use these clusterings to, in a sense, \"derandomize'' the methods of Karger.   In terms of techniques, we provide a structural theorem that says there exists a sparse clustering that preserves minimum cuts in a weighted graph with $o(1)$ error. In addition, we construct it deterministically in near linear time. This was done exactly for simple graphs in [KT19, HRW20] and with polylogarithmic error for weighted graphs in [Li21]. Extending the techniques in [KT19, HRW20] to weighted graphs presents significant challenges, and moreover, the algorithm can only polylogarithmically approximately preserve minimum cuts. A remaining challenge is to reduce the polylogarithmic-approximate clusterings to $1+o(1/\\log n)$-approximate so that they can be applied recursively as in [Li21] over $O(\\log n)$ many levels. This is an additional challenge that requires building on properties of tree-packings in the presence of a wide range of edge weights to, for example, find sources for local flow computations which identify minimum cuts that cross clusters.","sentences":["In 1996, Karger [Kar96] gave a startling randomized algorithm that finds a minimum-cut in a (weighted) graph in time $O(m\\log^3n)$ which he termed near-linear time meaning linear (in the size of the input) times a polylogarthmic factor.","In this paper, we give the first deterministic algorithm which runs in near-linear time for weighted graphs.   ","Previously, the breakthrough results of Kawarabayashi and Thorup [KT19] gave a near-linear time algorithm for simple graphs.","The main technique here is a clustering procedure that perfectly preserves minimum cuts.","Recently, Li","[Li21] gave an $m^{1+o(1)}$ deterministic minimum-cut algorithm for weighted graphs; this form of running time has been termed \"almost-linear''.","Li uses almost-linear time deterministic expander decompositions which do not perfectly preserve minimum cuts, but he can use these clusterings to, in a sense, \"derandomize'' the methods of Karger.   ","In terms of techniques, we provide a structural theorem that says there exists a sparse clustering that preserves minimum cuts in a weighted graph with $o(1)$ error.","In addition, we construct it deterministically in near linear time.","This was done exactly for simple graphs in [KT19, HRW20] and with polylogarithmic error for weighted graphs in [Li21].","Extending the techniques in [KT19, HRW20] to weighted graphs presents significant challenges, and moreover, the algorithm can only polylogarithmically approximately preserve minimum cuts.","A remaining challenge is to reduce the polylogarithmic-approximate clusterings to $1+o(1/\\log n)$-approximate so that they can be applied recursively as in [Li21] over $O(\\log n)$ many levels.","This is an additional challenge that requires building on properties of tree-packings in the presence of a wide range of edge weights to, for example, find sources for local flow computations which identify minimum cuts that cross clusters."],"url":"http://arxiv.org/abs/2401.05627v1"}
{"created":"2024-01-11 02:36:36","title":"A Composable Dynamic Sparse Dataflow Architecture for Efficient Event-based Vision Processing on FPGA","abstract":"Event-based vision represents a paradigm shift in how vision information is captured and processed. By only responding to dynamic intensity changes in the scene, event-based sensing produces far less data than conventional frame-based cameras, promising to springboard a new generation of high-speed, low-power machines for edge intelligence. However, processing such dynamically sparse input originated from event cameras efficiently in real time, particularly with complex deep neural networks (DNN), remains a formidable challenge. Existing solutions that employ GPUs and other frame-based DNN accelerators often struggle to efficiently process the dynamically sparse event data, missing the opportunities to improve processing efficiency with sparse data. To address this, we propose ESDA, a composable dynamic sparse dataflow architecture that allows customized DNN accelerators to be constructed rapidly on FPGAs for event-based vision tasks. ESDA is a modular system that is composed of a set of parametrizable modules for each network layer type. These modules share a uniform sparse token-feature interface and can be connected easily to compose an all-on-chip dataflow accelerator on FPGA for each network model. To fully exploit the intrinsic sparsity in event data, ESDA incorporates the use of submanifold sparse convolutions that largely enhance the activation sparsity throughout the layers while simplifying hardware implementation. Finally, a network architecture and hardware implementation co-optimizing framework that allows tradeoffs between accuracy and performance is also presented. Experimental results demonstrate that when compared with existing GPU and hardware-accelerated solutions, ESDA achieves substantial speedup and improvement in energy efficiency across different applications, and it allows much wider design space for real-world deployments.","sentences":["Event-based vision represents a paradigm shift in how vision information is captured and processed.","By only responding to dynamic intensity changes in the scene, event-based sensing produces far less data than conventional frame-based cameras, promising to springboard a new generation of high-speed, low-power machines for edge intelligence.","However, processing such dynamically sparse input originated from event cameras efficiently in real time, particularly with complex deep neural networks (DNN), remains a formidable challenge.","Existing solutions that employ GPUs and other frame-based DNN accelerators often struggle to efficiently process the dynamically sparse event data, missing the opportunities to improve processing efficiency with sparse data.","To address this, we propose ESDA, a composable dynamic sparse dataflow architecture that allows customized DNN accelerators to be constructed rapidly on FPGAs for event-based vision tasks.","ESDA is a modular system that is composed of a set of parametrizable modules for each network layer type.","These modules share a uniform sparse token-feature interface and can be connected easily to compose an all-on-chip dataflow accelerator on FPGA for each network model.","To fully exploit the intrinsic sparsity in event data, ESDA incorporates the use of submanifold sparse convolutions that largely enhance the activation sparsity throughout the layers while simplifying hardware implementation.","Finally, a network architecture and hardware implementation co-optimizing framework that allows tradeoffs between accuracy and performance is also presented.","Experimental results demonstrate that when compared with existing GPU and hardware-accelerated solutions, ESDA achieves substantial speedup and improvement in energy efficiency across different applications, and it allows much wider design space for real-world deployments."],"url":"http://arxiv.org/abs/2401.05626v1"}
{"created":"2024-01-11 01:52:25","title":"The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models","abstract":"In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.","sentences":["In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting.","We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy.","We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark.","CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance.","However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%.","Overall, CCoT leads to an average per-token cost reduction of 22.67%.","These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques.","In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs."],"url":"http://arxiv.org/abs/2401.05618v1"}
{"created":"2024-01-11 01:15:28","title":"Graph Q-Learning for Combinatorial Optimization","abstract":"Graph-structured data is ubiquitous throughout natural and social sciences, and Graph Neural Networks (GNNs) have recently been shown to be effective at solving prediction and inference problems on graph data. In this paper, we propose and demonstrate that GNNs can be applied to solve Combinatorial Optimization (CO) problems. CO concerns optimizing a function over a discrete solution space that is often intractably large. To learn to solve CO problems, we formulate the optimization process as a sequential decision making problem, where the return is related to how close the candidate solution is to optimality. We use a GNN to learn a policy to iteratively build increasingly promising candidate solutions. We present preliminary evidence that GNNs trained through Q-Learning can solve CO problems with performance approaching state-of-the-art heuristic-based solvers, using only a fraction of the parameters and training time.","sentences":["Graph-structured data is ubiquitous throughout natural and social sciences, and Graph Neural Networks (GNNs) have recently been shown to be effective at solving prediction and inference problems on graph data.","In this paper, we propose and demonstrate that GNNs can be applied to solve Combinatorial Optimization (CO) problems.","CO concerns optimizing a function over a discrete solution space that is often intractably large.","To learn to solve CO problems, we formulate the optimization process as a sequential decision making problem, where the return is related to how close the candidate solution is to optimality.","We use a GNN to learn a policy to iteratively build increasingly promising candidate solutions.","We present preliminary evidence that GNNs trained through Q-Learning can solve CO problems with performance approaching state-of-the-art heuristic-based solvers, using only a fraction of the parameters and training time."],"url":"http://arxiv.org/abs/2401.05610v1"}
{"created":"2024-01-11 00:03:36","title":"POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation","abstract":"Low-resource languages (LRLs) face challenges in supervised neural machine translation due to limited parallel data, prompting research into unsupervised methods. Unsupervised neural machine translation (UNMT) methods, including back-translation, transfer learning, and pivot-based translation, offer practical solutions for LRL translation, but they are hindered by issues like synthetic data noise, language bias, and error propagation, which can potentially be mitigated by Large Language Models (LLMs). LLMs have advanced NMT with in-context learning (ICL) and supervised fine-tuning methods, but insufficient training data results in poor performance in LRLs. We argue that LLMs can mitigate the linguistic noise with auxiliary languages to improve translations in LRLs. In this paper, we propose Probability-driven Meta-graph Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of multiple auxiliary languages to enhance LLMs' translation capabilities for LRLs. POMP involves constructing a directed acyclic meta-graph for each source language, from which we dynamically sample multiple paths to prompt LLMs to mitigate the linguistic noise and improve translations during training. We use the BLEURT metric to evaluate the translations and back-propagate rewards, estimated by scores, to update the probabilities of auxiliary languages in the paths. Our experiments show significant improvements in the translation quality of three LRLs, demonstrating the effectiveness of our approach.","sentences":["Low-resource languages (LRLs) face challenges in supervised neural machine translation due to limited parallel data, prompting research into unsupervised methods.","Unsupervised neural machine translation (UNMT) methods, including back-translation, transfer learning, and pivot-based translation, offer practical solutions for LRL translation, but they are hindered by issues like synthetic data noise, language bias, and error propagation, which can potentially be mitigated by Large Language Models (LLMs).","LLMs have advanced NMT with in-context learning (ICL) and supervised fine-tuning methods, but insufficient training data results in poor performance in LRLs.","We argue that LLMs can mitigate the linguistic noise with auxiliary languages to improve translations in LRLs.","In this paper, we propose Probability-driven Meta-graph Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of multiple auxiliary languages to enhance LLMs' translation capabilities for LRLs.","POMP involves constructing a directed acyclic meta-graph for each source language, from which we dynamically sample multiple paths to prompt LLMs to mitigate the linguistic noise and improve translations during training.","We use the BLEURT metric to evaluate the translations and back-propagate rewards, estimated by scores, to update the probabilities of auxiliary languages in the paths.","Our experiments show significant improvements in the translation quality of three LRLs, demonstrating the effectiveness of our approach."],"url":"http://arxiv.org/abs/2401.05596v1"}
{"created":"2024-01-10 23:05:23","title":"An Augmented Surprise-guided Sequential Learning Framework for Predicting the Melt Pool Geometry","abstract":"Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry, offering benefits like intricate design, minimal waste, rapid prototyping, material versatility, and customized solutions. However, its full industry adoption faces hurdles, particularly in achieving consistent product quality. A crucial aspect for MAM's success is understanding the relationship between process parameters and melt pool characteristics. Integrating Artificial Intelligence (AI) into MAM is essential. Traditional machine learning (ML) methods, while effective, depend on large datasets to capture complex relationships, a significant challenge in MAM due to the extensive time and resources required for dataset creation. Our study introduces a novel surprise-guided sequential learning framework, SurpriseAF-BO, signaling a significant shift in MAM. This framework uses an iterative, adaptive learning process, modeling the dynamics between process parameters and melt pool characteristics with limited data, a key benefit in MAM's cyber manufacturing context. Compared to traditional ML models, our sequential learning method shows enhanced predictive accuracy for melt pool dimensions. Further improving our approach, we integrated a Conditional Tabular Generative Adversarial Network (CTGAN) into our framework, forming the CT-SurpriseAF-BO. This produces synthetic data resembling real experimental data, improving learning effectiveness. This enhancement boosts predictive precision without requiring additional physical experiments. Our study demonstrates the power of advanced data-driven techniques in cyber manufacturing and the substantial impact of sequential AI and ML, particularly in overcoming MAM's traditional challenges.","sentences":["Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry, offering benefits like intricate design, minimal waste, rapid prototyping, material versatility, and customized solutions.","However, its full industry adoption faces hurdles, particularly in achieving consistent product quality.","A crucial aspect for MAM's success is understanding the relationship between process parameters and melt pool characteristics.","Integrating Artificial Intelligence (AI) into MAM is essential.","Traditional machine learning (ML) methods, while effective, depend on large datasets to capture complex relationships, a significant challenge in MAM due to the extensive time and resources required for dataset creation.","Our study introduces a novel surprise-guided sequential learning framework, SurpriseAF-BO, signaling a significant shift in MAM.","This framework uses an iterative, adaptive learning process, modeling the dynamics between process parameters and melt pool characteristics with limited data, a key benefit in MAM's cyber manufacturing context.","Compared to traditional ML models, our sequential learning method shows enhanced predictive accuracy for melt pool dimensions.","Further improving our approach, we integrated a Conditional Tabular Generative Adversarial Network (CTGAN) into our framework, forming the CT-SurpriseAF-BO.","This produces synthetic data resembling real experimental data, improving learning effectiveness.","This enhancement boosts predictive precision without requiring additional physical experiments.","Our study demonstrates the power of advanced data-driven techniques in cyber manufacturing and the substantial impact of sequential AI and ML, particularly in overcoming MAM's traditional challenges."],"url":"http://arxiv.org/abs/2401.05579v1"}
{"created":"2024-01-10 22:27:37","title":"Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms","abstract":"Self-supervised learning has become a popular way to pretrain a deep learning model and then transfer it to perform downstream tasks. However, most of these methods are developed on large-scale image datasets that contain natural objects with clear textures, outlines, and distinct color contrasts. It remains uncertain whether these methods are equally effective for medical imaging, where the regions of interest often blend subtly and indistinctly with the surrounding tissues. In this study, we propose an alternative method that uses contralateral mammograms to train a neural network to encode similar embeddings when a pair contains both normal images and different embeddings when a pair contains normal and abnormal images. Our approach leverages the natural symmetry of human body as weak labels to learn to distinguish abnormal lesions from background tissues in a fully unsupervised manner. Our findings suggest that it's feasible by incorporating soft labels derived from the Euclidean distances between the embeddings of the image pairs into the Siamese network loss. Our method demonstrates superior performance in mammogram patch classification compared to existing self-supervised learning methods. This approach not only leverages a vast amount of image data effectively but also minimizes reliance on costly labels, a significant advantage particularly in the field of medical imaging.","sentences":["Self-supervised learning has become a popular way to pretrain a deep learning model and then transfer it to perform downstream tasks.","However, most of these methods are developed on large-scale image datasets that contain natural objects with clear textures, outlines, and distinct color contrasts.","It remains uncertain whether these methods are equally effective for medical imaging, where the regions of interest often blend subtly and indistinctly with the surrounding tissues.","In this study, we propose an alternative method that uses contralateral mammograms to train a neural network to encode similar embeddings when a pair contains both normal images and different embeddings when a pair contains normal and abnormal images.","Our approach leverages the natural symmetry of human body as weak labels to learn to distinguish abnormal lesions from background tissues in a fully unsupervised manner.","Our findings suggest that it's feasible by incorporating soft labels derived from the Euclidean distances between the embeddings of the image pairs into the Siamese network loss.","Our method demonstrates superior performance in mammogram patch classification compared to existing self-supervised learning methods.","This approach not only leverages a vast amount of image data effectively but also minimizes reliance on costly labels, a significant advantage particularly in the field of medical imaging."],"url":"http://arxiv.org/abs/2401.05570v1"}
{"created":"2024-01-10 22:25:44","title":"SENet: Visual Detection of Online Social Engineering Attack Campaigns","abstract":"Social engineering (SE) aims at deceiving users into performing actions that may compromise their security and privacy. These threats exploit weaknesses in human's decision making processes by using tactics such as pretext, baiting, impersonation, etc. On the web, SE attacks include attack classes such as scareware, tech support scams, survey scams, sweepstakes, etc., which can result in sensitive data leaks, malware infections, and monetary loss. For instance, US consumers lose billions of dollars annually due to various SE attacks. Unfortunately, generic social engineering attacks remain understudied, compared to other important threats, such as software vulnerabilities and exploitation, network intrusions, malicious software, and phishing. The few existing technical studies that focus on social engineering are limited in scope and mostly focus on measurements rather than developing a generic defense. To fill this gap, we present SEShield, a framework for in-browser detection of social engineering attacks. SEShield consists of three main components: (i) a custom security crawler, called SECrawler, that is dedicated to scouting the web to collect examples of in-the-wild SE attacks; (ii) SENet, a deep learning-based image classifier trained on data collected by SECrawler that aims to detect the often glaring visual traits of SE attack pages; and (iii) SEGuard, a proof-of-concept extension that embeds SENet into the web browser and enables real-time SE attack detection. We perform an extensive evaluation of our system and show that SENet is able to detect new instances of SE attacks with a detection rate of up to 99.6% at 1% false positive, thus providing an effective first defense against SE attacks on the web.","sentences":["Social engineering (SE) aims at deceiving users into performing actions that may compromise their security and privacy.","These threats exploit weaknesses in human's decision making processes by using tactics such as pretext, baiting, impersonation, etc.","On the web, SE attacks include attack classes such as scareware, tech support scams, survey scams, sweepstakes, etc., which can result in sensitive data leaks, malware infections, and monetary loss.","For instance, US consumers lose billions of dollars annually due to various SE attacks.","Unfortunately, generic social engineering attacks remain understudied, compared to other important threats, such as software vulnerabilities and exploitation, network intrusions, malicious software, and phishing.","The few existing technical studies that focus on social engineering are limited in scope and mostly focus on measurements rather than developing a generic defense.","To fill this gap, we present SEShield, a framework for in-browser detection of social engineering attacks.","SEShield consists of three main components: (i) a custom security crawler, called SECrawler, that is dedicated to scouting the web to collect examples of in-the-wild SE attacks; (ii) SENet, a deep learning-based image classifier trained on data collected by SECrawler that aims to detect the often glaring visual traits of SE attack pages; and (iii) SEGuard, a proof-of-concept extension that embeds SENet into the web browser and enables real-time SE attack detection.","We perform an extensive evaluation of our system and show that SENet is able to detect new instances of SE attacks with a detection rate of up to 99.6% at 1% false positive, thus providing an effective first defense against SE attacks on the web."],"url":"http://arxiv.org/abs/2401.05569v1"}
{"created":"2024-01-10 22:07:40","title":"Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning","abstract":"Federated learning (FL) enables multiple participants to train a global machine learning model without sharing their private training data. Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating the server that aggregates local models from participants and then updates the global model. However, P2P FL is vulnerable to (i) honest-but-curious participants whose objective is to infer private training data of other participants, and (ii) Byzantine participants who can transmit arbitrarily manipulated local models to corrupt the learning process. P2P FL schemes that simultaneously guarantee Byzantine resilience and preserve privacy have been less studied. In this paper, we develop Brave, a protocol that ensures Byzantine Resilience And privacy-preserving property for P2P FL in the presence of both types of adversaries. We show that Brave preserves privacy by establishing that any honest-but-curious adversary cannot infer other participants' private data by observing their models. We further prove that Brave is Byzantine-resilient, which guarantees that all benign participants converge to an identical model that deviates from a global model trained without Byzantine adversaries by a bounded distance. We evaluate Brave against three state-of-the-art adversaries on a P2P FL for image classification tasks on benchmark datasets CIFAR10 and MNIST. Our results show that the global model learned with Brave in the presence of adversaries achieves comparable classification accuracy to a global model trained in the absence of any adversary.","sentences":["Federated learning (FL) enables multiple participants to train a global machine learning model without sharing their private training data.","Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating the server that aggregates local models from participants and then updates the global model.","However, P2P FL is vulnerable to (i) honest-but-curious participants whose objective is to infer private training data of other participants, and (ii) Byzantine participants who can transmit arbitrarily manipulated local models to corrupt the learning process.","P2P FL schemes that simultaneously guarantee Byzantine resilience and preserve privacy have been less studied.","In this paper, we develop Brave, a protocol that ensures Byzantine Resilience And privacy-preserving property for P2P FL in the presence of both types of adversaries.","We show that Brave preserves privacy by establishing that any honest-but-curious adversary cannot infer other participants' private data by observing their models.","We further prove that Brave is Byzantine-resilient, which guarantees that all benign participants converge to an identical model that deviates from a global model trained without Byzantine adversaries by a bounded distance.","We evaluate Brave against three state-of-the-art adversaries on a P2P FL for image classification tasks on benchmark datasets CIFAR10 and MNIST.","Our results show that the global model learned with Brave in the presence of adversaries achieves comparable classification accuracy to a global model trained in the absence of any adversary."],"url":"http://arxiv.org/abs/2401.05562v1"}
{"created":"2024-01-10 21:20:44","title":"X-HEEP: An Open-Source, Configurable and Extendible RISC-V Microcontroller for the Exploration of Ultra-Low-Power Edge Accelerators","abstract":"The field of edge computing has witnessed remarkable growth owing to the increasing demand for real-time processing of data in applications. However, challenges persist due to limitations in performance and power consumption. To overcome these challenges, heterogeneous architectures have emerged that combine host processors with specialized accelerators tailored to specific applications, leading to improved performance and reduced power consumption. However, most of the existing platforms lack the necessary configurability and extendability options for integrating custom accelerators. To overcome these limitations, we introduce in this paper the eXtendible Heterogeneous Energy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designed to natively support the integration of ultra-low-power edge accelerators. It provides customization options to match specific application requirements by exploring various core types, bus topologies, addressing modes, memory sizes, and peripherals. Moreover, the platform prioritizes energy efficiency by implementing low-power strategies, such as clock-gating and power-gating. We demonstrate the real-world applicability of X-HEEP by providing an integration example tailored for healthcare applications that includes a coarse-grained reconfigurable array (CGRA) and in-memory computing (IMC) accelerators. The resulting design, called HEEPocrates, has been implemented both in field programmable gate array (FPGA) on the Xilinx Zynq-7020 chip and in silicon with TSMC 65 nm low-power CMOS technology. We run a set of healthcare applications and measure their energy consumption to demonstrate the alignment of our chip with other state-of-the-art microcontrollers commonly adopted in this domain. Moreover, we showcase the energy benefit of 4.9 x gained by exploiting the integrated CGRA accelerator, compared to running on the host CPU.","sentences":["The field of edge computing has witnessed remarkable growth owing to the increasing demand for real-time processing of data in applications.","However, challenges persist due to limitations in performance and power consumption.","To overcome these challenges, heterogeneous architectures have emerged that combine host processors with specialized accelerators tailored to specific applications, leading to improved performance and reduced power consumption.","However, most of the existing platforms lack the necessary configurability and extendability options for integrating custom accelerators.","To overcome these limitations, we introduce in this paper the eXtendible Heterogeneous Energy-Efficient Platform (X-HEEP).","X-HEEP is an open-source platform designed to natively support the integration of ultra-low-power edge accelerators.","It provides customization options to match specific application requirements by exploring various core types, bus topologies, addressing modes, memory sizes, and peripherals.","Moreover, the platform prioritizes energy efficiency by implementing low-power strategies, such as clock-gating and power-gating.","We demonstrate the real-world applicability of X-HEEP by providing an integration example tailored for healthcare applications that includes a coarse-grained reconfigurable array (CGRA) and in-memory computing (IMC) accelerators.","The resulting design, called HEEPocrates, has been implemented both in field programmable gate array (FPGA) on the Xilinx Zynq-7020 chip and in silicon with TSMC 65 nm low-power CMOS technology.","We run a set of healthcare applications and measure their energy consumption to demonstrate the alignment of our chip with other state-of-the-art microcontrollers commonly adopted in this domain.","Moreover, we showcase the energy benefit of 4.9 x gained by exploiting the integrated CGRA accelerator, compared to running on the host CPU."],"url":"http://arxiv.org/abs/2401.05548v1"}
{"created":"2024-01-10 19:55:44","title":"VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition","abstract":"Transfer learning (TL) is an increasingly popular approach to training deep learning (DL) models that leverages the knowledge gained by training a foundation model on diverse, large-scale datasets for use on downstream tasks where less domain- or task-specific data is available. The literature is rich with TL techniques and applications; however, the bulk of the research makes use of deterministic DL models which are often uncalibrated and lack the ability to communicate a measure of epistemic (model) uncertainty in prediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models are often well-calibrated, provide access to epistemic uncertainty for a prediction, and are capable of achieving competitive predictive performance. In this study, we propose variational inference pre-trained audio neural networks (VI-PANNs). VI-PANNs are a variational inference variant of the popular ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio event detection dataset. We evaluate the quality of the resulting uncertainty when transferring knowledge from VI-PANNs to other downstream acoustic classification tasks using the ESC-50, UrbanSound8K, and DCASE2013 datasets. We demonstrate, for the first time, that it is possible to transfer calibrated uncertainty information along with knowledge from upstream tasks to enhance a model's capability to perform downstream tasks.","sentences":["Transfer learning (TL) is an increasingly popular approach to training deep learning (DL) models that leverages the knowledge gained by training a foundation model on diverse, large-scale datasets for use on downstream tasks where less domain- or task-specific data is available.","The literature is rich with TL techniques and applications; however, the bulk of the research makes use of deterministic DL models which are often uncalibrated and lack the ability to communicate a measure of epistemic (model) uncertainty in prediction.","Unlike their deterministic counterparts, Bayesian DL (BDL) models are often well-calibrated, provide access to epistemic uncertainty for a prediction, and are capable of achieving competitive predictive performance.","In this study, we propose variational inference pre-trained audio neural networks (VI-PANNs).","VI-PANNs are a variational inference variant of the popular ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio event detection dataset.","We evaluate the quality of the resulting uncertainty when transferring knowledge from VI-PANNs to other downstream acoustic classification tasks using the ESC-50, UrbanSound8K, and DCASE2013 datasets.","We demonstrate, for the first time, that it is possible to transfer calibrated uncertainty information along with knowledge from upstream tasks to enhance a model's capability to perform downstream tasks."],"url":"http://arxiv.org/abs/2401.05531v1"}
{"created":"2024-01-10 19:55:15","title":"Consensus Focus for Object Detection and minority classes","abstract":"Ensemble methods exploit the availability of a given number of classifiers or detectors trained in single or multiple source domains and tasks to address machine learning problems such as domain adaptation or multi-source transfer learning. Existing research measures the domain distance between the sources and the target dataset, trains multiple networks on the same data with different samples per class, or combines predictions from models trained under varied hyperparameters and settings. Their solutions enhanced the performance on small or tail categories but hurt the rest. To this end, we propose a modified consensus focus for semi-supervised and long-tailed object detection. We introduce a voting system based on source confidence that spots the contribution of each model in a consensus, lets the user choose the relevance of each class in the target label space so that it relaxes minority bounding boxes suppression, and combines multiple models' results without discarding the poisonous networks. Our tests on synthetic driving datasets retrieved higher confidence and more accurate bounding boxes than the NMS, soft-NMS, and WBF.","sentences":["Ensemble methods exploit the availability of a given number of classifiers or detectors trained in single or multiple source domains and tasks to address machine learning problems such as domain adaptation or multi-source transfer learning.","Existing research measures the domain distance between the sources and the target dataset, trains multiple networks on the same data with different samples per class, or combines predictions from models trained under varied hyperparameters and settings.","Their solutions enhanced the performance on small or tail categories but hurt the rest.","To this end, we propose a modified consensus focus for semi-supervised and long-tailed object detection.","We introduce a voting system based on source confidence that spots the contribution of each model in a consensus, lets the user choose the relevance of each class in the target label space so that it relaxes minority bounding boxes suppression, and combines multiple models' results without discarding the poisonous networks.","Our tests on synthetic driving datasets retrieved higher confidence and more accurate bounding boxes than the NMS, soft-NMS, and WBF."],"url":"http://arxiv.org/abs/2401.05530v1"}
{"created":"2024-01-10 19:09:52","title":"The Impact of Elicitation and Contrasting Narratives on Engagement, Recall and Attitude Change with News Articles Containing Data Visualization","abstract":"News articles containing data visualizations play an important role in informing the public on issues ranging from public health to politics. Recent research on the persuasive appeal of data visualizations suggests that prior attitudes can be notoriously difficult to change. Inspired by an NYT article, we designed two experiments to evaluate the impact of elicitation and contrasting narratives on attitude change, recall, and engagement. We hypothesized that eliciting prior beliefs leads to more elaborative thinking that ultimately results in higher attitude change, better recall, and engagement. Our findings revealed that visual elicitation leads to higher engagement in terms of feelings of surprise. While there is an overall attitude change across all experiment conditions, we did not observe a significant effect of belief elicitation on attitude change. With regard to recall error, while participants in the draw trend elicitation exhibited significantly lower recall error than participants in the categorize trend condition, we found no significant difference in recall error when comparing elicitation conditions to no elicitation. In a follow-up study, we added contrasting narratives with the purpose of making the main visualization (communicating data on the focal issue) appear strikingly different. Compared to the results of study 1, we found that contrasting narratives improved engagement in terms of surprise and interest but interestingly resulted in higher recall error and no significant change in attitude. We discuss the effects of elicitation and contrasting narratives in the context of topic involvement and the strengths of temporal trends encoded in the data visualization.","sentences":["News articles containing data visualizations play an important role in informing the public on issues ranging from public health to politics.","Recent research on the persuasive appeal of data visualizations suggests that prior attitudes can be notoriously difficult to change.","Inspired by an NYT article, we designed two experiments to evaluate the impact of elicitation and contrasting narratives on attitude change, recall, and engagement.","We hypothesized that eliciting prior beliefs leads to more elaborative thinking that ultimately results in higher attitude change, better recall, and engagement.","Our findings revealed that visual elicitation leads to higher engagement in terms of feelings of surprise.","While there is an overall attitude change across all experiment conditions, we did not observe a significant effect of belief elicitation on attitude change.","With regard to recall error, while participants in the draw trend elicitation exhibited significantly lower recall error than participants in the categorize trend condition, we found no significant difference in recall error when comparing elicitation conditions to no elicitation.","In a follow-up study, we added contrasting narratives with the purpose of making the main visualization (communicating data on the focal issue) appear strikingly different.","Compared to the results of study 1, we found that contrasting narratives improved engagement in terms of surprise and interest but interestingly resulted in higher recall error and no significant change in attitude.","We discuss the effects of elicitation and contrasting narratives in the context of topic involvement and the strengths of temporal trends encoded in the data visualization."],"url":"http://arxiv.org/abs/2401.05511v1"}
{"created":"2024-01-10 19:04:00","title":"InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks","abstract":"In this paper, we introduce \"InfiAgent-DABench\", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.","sentences":["In this paper, we introduce \"InfiAgent-DABench\", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks.","This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents.","We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated.","Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks.","In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets.","Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent."],"url":"http://arxiv.org/abs/2401.05507v1"}
{"created":"2024-01-10 19:01:05","title":"Diversity-aware clustering: Computational Complexity and Approximation Algorithms","abstract":"In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \\frac{2}{e}$, $1+\\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\\frac{2}{e}$ and $1+\\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improving the previous best known approximation ratio of factor $5$.","sentences":["In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups.","A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier.","We present parameterized approximation algorithms with approximation ratios $1+ \\frac{2}{e}$, $1+\\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively.","The approximation ratios are tight assuming Gap-ETH and FPT $\\neq$ W[2].","For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\\frac{2}{e}$ and $1+\\frac{8}{e}$, respectively.","For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improving the previous best known approximation ratio of factor $5$."],"url":"http://arxiv.org/abs/2401.05502v1"}
{"created":"2024-01-10 19:00:55","title":"Deflating the Chinese Balloon: Types of Twitter Bots in US-China balloon incident","abstract":"As digitalization increases, countries employ digital diplomacy, harnessing digital resources to project their desired image. Digital diplomacy also encompasses the interactivity of digital platforms, providing a trove of public opinion that diplomatic agents can collect. Social media bots actively participate in political events through influencing political communication and purporting coordinated narratives to influence human behavior. This article provides a methodology towards identifying three types of bots: General Bots, News Bots and Bridging Bots, then further identify these classes of bots on Twitter during a diplomatic incident involving the United States and China. Using a series of computational methods, this article examines the impact of bots on the topics disseminated, the influence and the use of information maneuvers of bots within the social communication network. Among others, our results observe that all three types of bots are present across the two countries; bots geotagged to the US are generally concerned with the balloon location while those geotagged to China discussed topics related to escalating tensions; and perform different extent of positive narrative and network information maneuvers.","sentences":["As digitalization increases, countries employ digital diplomacy, harnessing digital resources to project their desired image.","Digital diplomacy also encompasses the interactivity of digital platforms, providing a trove of public opinion that diplomatic agents can collect.","Social media bots actively participate in political events through influencing political communication and purporting coordinated narratives to influence human behavior.","This article provides a methodology towards identifying three types of bots: General Bots, News Bots and Bridging Bots, then further identify these classes of bots on Twitter during a diplomatic incident involving the United States and China.","Using a series of computational methods, this article examines the impact of bots on the topics disseminated, the influence and the use of information maneuvers of bots within the social communication network.","Among others, our results observe that all three types of bots are present across the two countries; bots geotagged to the US are generally concerned with the balloon location while those geotagged to China discussed topics related to escalating tensions; and perform different extent of positive narrative and network information maneuvers."],"url":"http://arxiv.org/abs/2401.05501v1"}
{"created":"2024-01-10 18:08:32","title":"The recursive scheme of clustering","abstract":"The problem of data clustering is one of the most important in data analysis. It can be problematic when dealing with experimental data characterized by measurement uncertainties and errors. Our paper proposes a recursive scheme for clustering data obtained in geographical (climatological) experiments. The discussion of results obtained by k-means and SOM methods with the developed recursive procedure is presented. We show that the clustering using the new approach gives more acceptable results when compared to experts assessments.","sentences":["The problem of data clustering is one of the most important in data analysis.","It can be problematic when dealing with experimental data characterized by measurement uncertainties and errors.","Our paper proposes a recursive scheme for clustering data obtained in geographical (climatological) experiments.","The discussion of results obtained by k-means and SOM methods with the developed recursive procedure is presented.","We show that the clustering using the new approach gives more acceptable results when compared to experts assessments."],"url":"http://arxiv.org/abs/2401.05479v1"}
{"created":"2024-01-10 18:04:12","title":"Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups","abstract":"Graph neural networks (GNN) are a powerful tool for combining imaging and non-imaging medical information for node classification tasks. Cross-network node classification extends GNN techniques to account for domain drift, allowing for node classification on an unlabeled target network. In this paper we present OTGCN, a powerful, novel approach to cross-network node classification. This approach leans on concepts from graph convolutional networks to harness insights from graph data structures while simultaneously applying strategies rooted in optimal transport to correct for the domain drift that can occur between samples from different data collection sites. This blended approach provides a practical solution for scenarios with many distinct forms of data collected across different locations and equipment. We demonstrate the effectiveness of this approach at classifying Autism Spectrum Disorder subjects using a blend of imaging and non-imaging data.","sentences":["Graph neural networks (GNN) are a powerful tool for combining imaging and non-imaging medical information for node classification tasks.","Cross-network node classification extends GNN techniques to account for domain drift, allowing for node classification on an unlabeled target network.","In this paper we present OTGCN, a powerful, novel approach to cross-network node classification.","This approach leans on concepts from graph convolutional networks to harness insights from graph data structures while simultaneously applying strategies rooted in optimal transport to correct for the domain drift that can occur between samples from different data collection sites.","This blended approach provides a practical solution for scenarios with many distinct forms of data collected across different locations and equipment.","We demonstrate the effectiveness of this approach at classifying Autism Spectrum Disorder subjects using a blend of imaging and non-imaging data."],"url":"http://arxiv.org/abs/2401.05478v1"}
