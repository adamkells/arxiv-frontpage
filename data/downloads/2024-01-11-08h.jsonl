{"created":"2024-01-10 18:59:51","title":"URHand: Universal Relightable Hands","abstract":"Existing photorealistic relightable hand models require extensive identity-specific observations in different views, poses, and illuminations, and face challenges in generalizing to natural illuminations and novel identities. To bridge this gap, we present URHand, the first universal relightable hand model that generalizes across viewpoints, poses, illuminations, and identities. Our model allows few-shot personalization using images captured with a mobile phone, and is ready to be photorealistically rendered under novel illuminations. To simplify the personalization process while retaining photorealism, we build a powerful universal relightable prior based on neural relighting from multi-view images of hands captured in a light stage with hundreds of identities. The key challenge is scaling the cross-identity training while maintaining personalized fidelity and sharp details without compromising generalization under natural illuminations. To this end, we propose a spatially varying linear lighting model as the neural renderer that takes physics-inspired shading as input feature. By removing non-linear activations and bias, our specifically designed lighting model explicitly keeps the linearity of light transport. This enables single-stage training from light-stage data while generalizing to real-time rendering under arbitrary continuous illuminations across diverse identities. In addition, we introduce the joint learning of a physically based model and our neural relighting model, which further improves fidelity and generalization. Extensive experiments show that our approach achieves superior performance over existing methods in terms of both quality and generalizability. We also demonstrate quick personalization of URHand from a short phone scan of an unseen identity.","sentences":["Existing photorealistic relightable hand models require extensive identity-specific observations in different views, poses, and illuminations, and face challenges in generalizing to natural illuminations and novel identities.","To bridge this gap, we present URHand, the first universal relightable hand model that generalizes across viewpoints, poses, illuminations, and identities.","Our model allows few-shot personalization using images captured with a mobile phone, and is ready to be photorealistically rendered under novel illuminations.","To simplify the personalization process while retaining photorealism, we build a powerful universal relightable prior based on neural relighting from multi-view images of hands captured in a light stage with hundreds of identities.","The key challenge is scaling the cross-identity training while maintaining personalized fidelity and sharp details without compromising generalization under natural illuminations.","To this end, we propose a spatially varying linear lighting model as the neural renderer that takes physics-inspired shading as input feature.","By removing non-linear activations and bias, our specifically designed lighting model explicitly keeps the linearity of light transport.","This enables single-stage training from light-stage data while generalizing to real-time rendering under arbitrary continuous illuminations across diverse identities.","In addition, we introduce the joint learning of a physically based model and our neural relighting model, which further improves fidelity and generalization.","Extensive experiments show that our approach achieves superior performance over existing methods in terms of both quality and generalizability.","We also demonstrate quick personalization of URHand from a short phone scan of an unseen identity."],"url":"http://arxiv.org/abs/2401.05334v1"}
{"created":"2024-01-10 18:54:13","title":"\\textit{SmartMME}: Implementation of Base Station Switching Off Strategy in ns-3","abstract":"In the landscape of next-generation cellular networks, a projected surge of over 12 billion subscriptions foreshadows a considerable upswing in the network's overall energy consumption. The proliferation of User Equipment (UE) drives this energy demand, urging 5G deployments to seek more energy-efficient methodologies. In this work, we propose SmartMME, as a pivotal solution aimed at optimizing Base Station (BS) energy usage. By harnessing and analyzing critical network states-such as UE connections, data traffic at individual UEs, and other pertinent metrics-our methodology intelligently orchestrates the BS's power states, making informed decisions on when to activate or deactivate the BS. This meticulous approach significantly curtails the network's overall energy consumption. In a bid to validate its efficiency, we seamlessly integrated our module into Network Simulator-3 (ns-3), conducting extensive testing to demonstrate its prowess in effectively managing and reducing net energy consumption. As advocates of collaborative progress, we've opted to open-source this module, inviting the engagement and feedback of the wider research community on GitHub.","sentences":["In the landscape of next-generation cellular networks, a projected surge of over 12 billion subscriptions foreshadows a considerable upswing in the network's overall energy consumption.","The proliferation of User Equipment (UE) drives this energy demand, urging 5G deployments to seek more energy-efficient methodologies.","In this work, we propose SmartMME, as a pivotal solution aimed at optimizing Base Station (BS) energy usage.","By harnessing and analyzing critical network states-such as UE connections, data traffic at individual UEs, and other pertinent metrics-our methodology intelligently orchestrates the BS's power states, making informed decisions on when to activate or deactivate the BS.","This meticulous approach significantly curtails the network's overall energy consumption.","In a bid to validate its efficiency, we seamlessly integrated our module into Network Simulator-3 (ns-3), conducting extensive testing to demonstrate its prowess in effectively managing and reducing net energy consumption.","As advocates of collaborative progress, we've opted to open-source this module, inviting the engagement and feedback of the wider research community on GitHub."],"url":"http://arxiv.org/abs/2401.05329v1"}
{"created":"2024-01-10 18:41:39","title":"Arrival Time Prediction for Autonomous Shuttle Services in the Real World: Evidence from Five Cities","abstract":"Urban mobility is on the cusp of transformation with the emergence of shared, connected, and cooperative automated vehicles. Yet, for them to be accepted by customers, trust in their punctuality is vital. Many pilot initiatives operate without a fixed schedule, thus enhancing the importance of reliable arrival time (AT) predictions. This study presents an AT prediction system for autonomous shuttles, utilizing separate models for dwell and running time predictions, validated on real-world data from five cities. Alongside established methods such as XGBoost, we explore the benefits of integrating spatial data using graph neural networks (GNN). To accurately handle the case of a shuttle bypassing a stop, we propose a hierarchical model combining a random forest classifier and a GNN. The results for the final AT prediction are promising, showing low errors even when predicting several stops ahead. Yet, no single model emerges as universally superior, and we provide insights into the characteristics of pilot sites that influence the model selection process. Finally, we identify dwell time prediction as the key determinant in overall AT prediction accuracy when autonomous shuttles are deployed in low-traffic areas or under regulatory speed limits. This research provides insights into the current state of autonomous public transport prediction models and paves the way for more data-informed decision-making as the field advances.","sentences":["Urban mobility is on the cusp of transformation with the emergence of shared, connected, and cooperative automated vehicles.","Yet, for them to be accepted by customers, trust in their punctuality is vital.","Many pilot initiatives operate without a fixed schedule, thus enhancing the importance of reliable arrival time (AT) predictions.","This study presents an AT prediction system for autonomous shuttles, utilizing separate models for dwell and running time predictions, validated on real-world data from five cities.","Alongside established methods such as XGBoost, we explore the benefits of integrating spatial data using graph neural networks (GNN).","To accurately handle the case of a shuttle bypassing a stop, we propose a hierarchical model combining a random forest classifier and a GNN.","The results for the final AT prediction are promising, showing low errors even when predicting several stops ahead.","Yet, no single model emerges as universally superior, and we provide insights into the characteristics of pilot sites that influence the model selection process.","Finally, we identify dwell time prediction as the key determinant in overall AT prediction accuracy when autonomous shuttles are deployed in low-traffic areas or under regulatory speed limits.","This research provides insights into the current state of autonomous public transport prediction models and paves the way for more data-informed decision-making as the field advances."],"url":"http://arxiv.org/abs/2401.05322v1"}
{"created":"2024-01-10 18:37:59","title":"Leveraging Print Debugging to Improve Code Generation in Large Language Models","abstract":"Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal. To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a \"print debugging\" method, which involves inserting print statements to trace and analysing logs for fixing the bug. We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system. Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%.","sentences":["Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal.","To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a \"print debugging\" method, which involves inserting print statements to trace and analysing logs for fixing the bug.","We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system.","Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%."],"url":"http://arxiv.org/abs/2401.05319v1"}
{"created":"2024-01-10 18:22:00","title":"Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL Networks","abstract":"The deployment of federated learning (FL) within vertical heterogeneous networks, such as those enabled by high-altitude platform station (HAPS), offers the opportunity to engage a wide array of clients, each endowed with distinct communication and computational capabilities. This diversity not only enhances the training accuracy of FL models but also hastens their convergence. Yet, applying FL in these expansive networks presents notable challenges, particularly the significant non-IIDness in client data distributions. Such data heterogeneity often results in slower convergence rates and reduced effectiveness in model training performance. Our study introduces a client selection strategy tailored to address this issue, leveraging user network traffic behaviour. This strategy involves the prediction and classification of clients based on their network usage patterns while prioritizing user privacy. By strategically selecting clients whose data exhibit similar patterns for participation in FL training, our approach fosters a more uniform and representative data distribution across the network. Our simulations demonstrate that this targeted client selection methodology significantly reduces the training loss of FL models in HAPS networks, thereby effectively tackling a crucial challenge in implementing large-scale FL systems.","sentences":["The deployment of federated learning (FL) within vertical heterogeneous networks, such as those enabled by high-altitude platform station (HAPS), offers the opportunity to engage a wide array of clients, each endowed with distinct communication and computational capabilities.","This diversity not only enhances the training accuracy of FL models but also hastens their convergence.","Yet, applying FL in these expansive networks presents notable challenges, particularly the significant non-IIDness in client data distributions.","Such data heterogeneity often results in slower convergence rates and reduced effectiveness in model training performance.","Our study introduces a client selection strategy tailored to address this issue, leveraging user network traffic behaviour.","This strategy involves the prediction and classification of clients based on their network usage patterns while prioritizing user privacy.","By strategically selecting clients whose data exhibit similar patterns for participation in FL training, our approach fosters a more uniform and representative data distribution across the network.","Our simulations demonstrate that this targeted client selection methodology significantly reduces the training loss of FL models in HAPS networks, thereby effectively tackling a crucial challenge in implementing large-scale FL systems."],"url":"http://arxiv.org/abs/2401.05308v1"}
{"created":"2024-01-10 18:06:27","title":"I am a Strange Dataset: Metalinguistic Tests for Language Models","abstract":"Statements involving metalinguistic self-reference (\"This paper has six sections.\") are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present \"I am a Strange Dataset\", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like \"The penultimate word in this sentence is\" (where a correct continuation is \"is\"). In verification, models judge the truth of statements like \"The penultimate word in this sentence is sentence.\" (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range. The dataset and evaluation toolkit are available at https://github.com/TristanThrush/i-am-a-strange-dataset.","sentences":["Statements involving metalinguistic self-reference (\"This paper has six sections.\") are prevalent in many domains.","Can large language models (LLMs) handle such language?","In this paper, we present \"I am a Strange Dataset\", a new dataset for addressing this question.","There are two subtasks: generation and verification.","In generation, models continue statements like \"The penultimate word in this sentence is\" (where a correct continuation is \"is\").","In verification, models judge the truth of statements like \"The penultimate word in this sentence is sentence.\"","(false).","We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all.","The dataset is hand-crafted by experts and validated by non-expert annotators.","We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs.","All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale.","GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range.","The dataset and evaluation toolkit are available at https://github.com/TristanThrush/i-am-a-strange-dataset."],"url":"http://arxiv.org/abs/2401.05300v1"}
{"created":"2024-01-10 16:57:24","title":"AUTOACT: Automatic Agent Learning from Scratch via Self-Planning","abstract":"Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent. Code will be available at https://github.com/zjunlp/AutoAct.","sentences":["Language agents have achieved considerable performance on various complex tasks.","Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions.","To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4).","Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models.","Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task.","We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines.","We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent.","Code will be available at https://github.com/zjunlp/AutoAct."],"url":"http://arxiv.org/abs/2401.05268v1"}
{"created":"2024-01-10 16:27:30","title":"ReACT: Reinforcement Learning for Controller Parametrization using B-Spline Geometries","abstract":"Robust and performant controllers are essential for industrial applications. However, deriving controller parameters for complex and nonlinear systems is challenging and time-consuming. To facilitate automatic controller parametrization, this work presents a novel approach using deep reinforcement learning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the control of parameter-variant systems, a class of systems with complex behavior which depends on the operating conditions. For this system class, gain-scheduling control structures are widely used in applications across industries due to well-known design principles. Facilitating the expensive controller parametrization task regarding these control structures, we deploy an DRL agent. Based on control system observations, the agent autonomously decides how to adapt the controller parameters. We make the adaptation process more efficient by introducing BSGs to map the controller parameters which may depend on numerous operating conditions. To preprocess time-series data and extract a fixed-length feature vector, we use a long short-term memory (LSTM) neural networks. Furthermore, this work contributes actor regularizations that are relevant to real-world environments which differ from training. Accordingly, we apply dropout layer normalization to the actor and critic networks of the truncated quantile critic (TQC) algorithm. To show our approach's working principle and effectiveness, we train and evaluate the DRL agent on the parametrization task of an industrial control structure with parameter lookup tables.","sentences":["Robust and performant controllers are essential for industrial applications.","However, deriving controller parameters for complex and nonlinear systems is challenging and time-consuming.","To facilitate automatic controller parametrization, this work presents a novel approach using deep reinforcement learning (DRL) with N-dimensional B-spline geometries (BSGs).","We focus on the control of parameter-variant systems, a class of systems with complex behavior which depends on the operating conditions.","For this system class, gain-scheduling control structures are widely used in applications across industries due to well-known design principles.","Facilitating the expensive controller parametrization task regarding these control structures, we deploy an DRL agent.","Based on control system observations, the agent autonomously decides how to adapt the controller parameters.","We make the adaptation process more efficient by introducing BSGs to map the controller parameters which may depend on numerous operating conditions.","To preprocess time-series data and extract a fixed-length feature vector, we use a long short-term memory (LSTM) neural networks.","Furthermore, this work contributes actor regularizations that are relevant to real-world environments which differ from training.","Accordingly, we apply dropout layer normalization to the actor and critic networks of the truncated quantile critic (TQC) algorithm.","To show our approach's working principle and effectiveness, we train and evaluate the DRL agent on the parametrization task of an industrial control structure with parameter lookup tables."],"url":"http://arxiv.org/abs/2401.05251v1"}
{"created":"2024-01-10 16:21:18","title":"CASA: Causality-driven Argument Sufficiency Assessment","abstract":"The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion. To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event. Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments. Code and data are available at https://github.com/xxxiaol/CASA.","sentences":["The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion.","To tackle this task, existing works often train a classifier on data annotated by humans.","However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria.","Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework.","PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent.","To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event.","Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments.","We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments.","Code and data are available at https://github.com/xxxiaol/CASA."],"url":"http://arxiv.org/abs/2401.05249v1"}
{"created":"2024-01-10 16:13:21","title":"Decoupling Decision-Making in Fraud Prevention through Classifier Calibration for Business Logic Action","abstract":"Machine learning models typically focus on specific targets like creating classifiers, often based on known population feature distributions in a business context. However, models calculating individual features adapt over time to improve precision, introducing the concept of decoupling: shifting from point evaluation to data distribution. We use calibration strategies as strategy for decoupling machine learning (ML) classifiers from score-based actions within business logic frameworks. To evaluate these strategies, we perform a comparative analysis using a real-world business scenario and multiple ML models. Our findings highlight the trade-offs and performance implications of the approach, offering valuable insights for practitioners seeking to optimize their decoupling efforts. In particular, the Isotonic and Beta calibration methods stand out for scenarios in which there is shift between training and testing data.","sentences":["Machine learning models typically focus on specific targets like creating classifiers, often based on known population feature distributions in a business context.","However, models calculating individual features adapt over time to improve precision, introducing the concept of decoupling: shifting from point evaluation to data distribution.","We use calibration strategies as strategy for decoupling machine learning (ML) classifiers from score-based actions within business logic frameworks.","To evaluate these strategies, we perform a comparative analysis using a real-world business scenario and multiple ML models.","Our findings highlight the trade-offs and performance implications of the approach, offering valuable insights for practitioners seeking to optimize their decoupling efforts.","In particular, the Isotonic and Beta calibration methods stand out for scenarios in which there is shift between training and testing data."],"url":"http://arxiv.org/abs/2401.05240v1"}
{"created":"2024-01-10 16:01:08","title":"Taming \"data-hungry\" reinforcement learning? Stability in continuous state-action spaces","abstract":"We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures. We argue that these properties are satisfied in many continuous state-action Markov decision processes, and demonstrate how they arise naturally when using linear function approximation methods. Our analysis offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL, and highlights the connection between off-line RL and transfer learning.","sentences":["We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings.","Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures.","We argue that these properties are satisfied in many continuous state-action Markov decision processes, and demonstrate how they arise naturally when using linear function approximation methods.","Our analysis offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL, and highlights the connection between off-line RL and transfer learning."],"url":"http://arxiv.org/abs/2401.05233v1"}
{"created":"2024-01-10 15:38:00","title":"Distributed Monitoring for Data Distribution Shifts in Edge-ML Fraud Detection","abstract":"The digital era has seen a marked increase in financial fraud. edge ML emerged as a promising solution for smartphone payment services fraud detection, enabling the deployment of ML models directly on edge devices. This approach enables a more personalized real-time fraud detection. However, a significant gap in current research is the lack of a robust system for monitoring data distribution shifts in these distributed edge ML applications. Our work bridges this gap by introducing a novel open-source framework designed for continuous monitoring of data distribution shifts on a network of edge devices. Our system includes an innovative calculation of the Kolmogorov-Smirnov (KS) test over a distributed network of edge devices, enabling efficient and accurate monitoring of users behavior shifts. We comprehensively evaluate the proposed framework employing both real-world and synthetic financial transaction datasets and demonstrate the framework's effectiveness.","sentences":["The digital era has seen a marked increase in financial fraud.","edge ML emerged as a promising solution for smartphone payment services fraud detection, enabling the deployment of ML models directly on edge devices.","This approach enables a more personalized real-time fraud detection.","However, a significant gap in current research is the lack of a robust system for monitoring data distribution shifts in these distributed edge ML applications.","Our work bridges this gap by introducing a novel open-source framework designed for continuous monitoring of data distribution shifts on a network of edge devices.","Our system includes an innovative calculation of the Kolmogorov-Smirnov (KS) test over a distributed network of edge devices, enabling efficient and accurate monitoring of users behavior shifts.","We comprehensively evaluate the proposed framework employing both real-world and synthetic financial transaction datasets and demonstrate the framework's effectiveness."],"url":"http://arxiv.org/abs/2401.05219v1"}
{"created":"2024-01-10 15:34:42","title":"Invariant Causal Prediction with Locally Linear Models","abstract":"We consider the task of identifying the causal parents of a target variable among a set of candidate variables from observational data. Our main assumption is that the candidate variables are observed in different environments which may, for example, correspond to different settings of a machine or different time intervals in a dynamical process. Under certain assumptions different environments can be regarded as interventions on the observed system. We assume a linear relationship between target and covariates, which can be different in each environment with the only restriction that the causal structure is invariant across environments. This is an extension of the ICP ($\\textbf{I}$nvariant $\\textbf{C}$ausal $\\textbf{P}$rediction) principle by Peters et al. [2016], who assumed a fixed linear relationship across all environments. Within our proposed setting we provide sufficient conditions for identifiability of the causal parents and introduce a practical method called LoLICaP ($\\textbf{Lo}$cally $\\textbf{L}$inear $\\textbf{I}$nvariant $\\textbf{Ca}$usal $\\textbf{P}$rediction), which is based on a hypothesis test for parent identification using a ratio of minimum and maximum statistics. We then show in a simplified setting that the statistical power of LoLICaP converges exponentially fast in the sample size, and finally we analyze the behavior of LoLICaP experimentally in more general settings.","sentences":["We consider the task of identifying the causal parents of a target variable among a set of candidate variables from observational data.","Our main assumption is that the candidate variables are observed in different environments which may, for example, correspond to different settings of a machine or different time intervals in a dynamical process.","Under certain assumptions different environments can be regarded as interventions on the observed system.","We assume a linear relationship between target and covariates, which can be different in each environment with the only restriction that the causal structure is invariant across environments.","This is an extension of the ICP ($\\textbf{I}$nvariant $\\textbf{C}$ausal $\\textbf{P}$rediction) principle by Peters et al.","[2016], who assumed a fixed linear relationship across all environments.","Within our proposed setting we provide sufficient conditions for identifiability of the causal parents and introduce a practical method called LoLICaP ($\\textbf{Lo}$cally $\\textbf{L}$inear $\\textbf{I}$nvariant $\\textbf{Ca}$usal $\\textbf{P}$rediction), which is based on a hypothesis test for parent identification using a ratio of minimum and maximum statistics.","We then show in a simplified setting that the statistical power of LoLICaP converges exponentially fast in the sample size, and finally we analyze the behavior of LoLICaP experimentally in more general settings."],"url":"http://arxiv.org/abs/2401.05218v1"}
{"created":"2024-01-10 14:53:18","title":"Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking","abstract":"Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBeluga2 trailing closely, presenting an attractive option given its data privacy and customization benefits. Overall, this work offers preliminary insights for factories considering using LLM-tools for knowledge management.","sentences":["Managing knowledge efficiently is crucial for organizational success.","In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators.","In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation.","The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge.","To assess its effectiveness, we conducted an evaluation in a factory setting.","The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues.","However, the study also highlighted a preference for learning from a human expert when such an option is available.","Furthermore, we benchmarked several closed and open-sourced LLMs for this system.","GPT-4 consistently outperformed its counterparts, with open-source models like StableBeluga2 trailing closely, presenting an attractive option given its data privacy and customization benefits.","Overall, this work offers preliminary insights for factories considering using LLM-tools for knowledge management."],"url":"http://arxiv.org/abs/2401.05200v1"}
{"created":"2024-01-10 14:40:23","title":"Experiment Planning with Function Approximation","abstract":"We study the problem of experiment planning with function approximation in contextual bandit problems. In settings where there is a significant overhead to deploying adaptive algorithms -- for example, when the execution of the data collection policies is required to be distributed, or a human in the loop is needed to implement these policies -- producing in advance a set of policies for data collection is paramount. We study the setting where a large dataset of contexts but not rewards is available and may be used by the learner to design an effective data collection strategy. Although when rewards are linear this problem has been well studied, results are still missing for more complex reward models. In this work we propose two experiment planning strategies compatible with function approximation. The first is an eluder planning and sampling procedure that can recover optimality guarantees depending on the eluder dimension of the reward function class. For the second, we show that a uniform sampler achieves competitive optimality rates in the setting where the number of actions is small. We finalize our results introducing a statistical gap fleshing out the fundamental differences between planning and adaptive learning and provide results for planning with model selection.","sentences":["We study the problem of experiment planning with function approximation in contextual bandit problems.","In settings where there is a significant overhead to deploying adaptive algorithms -- for example, when the execution of the data collection policies is required to be distributed, or a human in the loop is needed to implement these policies -- producing in advance a set of policies for data collection is paramount.","We study the setting where a large dataset of contexts but not rewards is available and may be used by the learner to design an effective data collection strategy.","Although when rewards are linear this problem has been well studied, results are still missing for more complex reward models.","In this work we propose two experiment planning strategies compatible with function approximation.","The first is an eluder planning and sampling procedure that can recover optimality guarantees depending on the eluder dimension of the reward function class.","For the second, we show that a uniform sampler achieves competitive optimality rates in the setting where the number of actions is small.","We finalize our results introducing a statistical gap fleshing out the fundamental differences between planning and adaptive learning and provide results for planning with model selection."],"url":"http://arxiv.org/abs/2401.05193v1"}
{"created":"2024-01-10 14:38:46","title":"Divide and Conquer for Large Language Models Reasoning","abstract":"Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed strategy significantly boosts the models' reasoning abilities across nine datasets involving arithmetic, commonsense, and logic tasks. For instance, compared to baseline, we make a striking improvement on low confidence subsets of 8.72\\% for AQuA, 15.07\\% for ARC Challenge and 7.71\\% for RiddleSense. In addition, through extensive analysis on length of rationale and number of options, we verify that longer reasoning paths in PKR could prevent models from referring infer-harmful shortcuts, and also find that removing irrelevant choices in FCR would substantially avoid models' confusion. The code is at \\url{https://github.com/AiMijie/Divide-and-Conquer}","sentences":["Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs).","However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones.","To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning.","First, we divide questions into different subsets based on the statistical confidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants.","Our experiments demonstrate that this proposed strategy significantly boosts the models' reasoning abilities across nine datasets involving arithmetic, commonsense, and logic tasks.","For instance, compared to baseline, we make a striking improvement on low confidence subsets of 8.72\\% for AQuA, 15.07\\% for ARC Challenge and 7.71\\% for RiddleSense.","In addition, through extensive analysis on length of rationale and number of options, we verify that longer reasoning paths in PKR could prevent models from referring infer-harmful shortcuts, and also find that removing irrelevant choices in FCR would substantially avoid models' confusion.","The code is at \\url{https://github.com/AiMijie/Divide-and-Conquer}"],"url":"http://arxiv.org/abs/2401.05190v1"}
{"created":"2024-01-10 14:06:50","title":"Multivariate Extreme Value Theory Based Rate Selection for Ultra-Reliable Communications","abstract":"Diversity schemes play a vital role in improving the performance of ultra-reliable communication systems by transmitting over two or more communication channels to combat fading and co-channel interference. Determining an appropriate transmission strategy that satisfies ultra-reliability constraint necessitates derivation of statistics of channel in ultra-reliable region and, subsequently, integration of these statistics into rate selection while incorporating a confidence interval to account for potential uncertainties that may arise during estimation. In this paper, we propose a novel framework for ultra-reliable real-time transmission considering both spatial diversities and ultra-reliable channel statistics based on multivariate extreme value theory. First, tail distribution of joint received power sequences obtained from different receivers is modeled while incorporating inter-relations of extreme events occurring rarely based on Poisson point process approach in MEVT. The optimum transmission strategies are then developed by determining optimum transmission rate based on estimated joint tail distribution and incorporating confidence intervals into estimations to cope with the availability of limited data. Finally, system reliability is assessed by utilizing outage probability metric. Through analysis of data obtained from the engine compartment of Fiat Linea, our study showcases the effectiveness of proposed methodology in surpassing traditional extrapolation-based approaches. This innovative method not only achieves a higher transmission rate, but also effectively addresses stringent requirements of ultra-reliability. The findings indicate that proposed rate selection framework offers a viable solution for achieving a desired target error probability by employing a higher transmission rate and reducing the amount of training data compared to conventional rate selection methods.","sentences":["Diversity schemes play a vital role in improving the performance of ultra-reliable communication systems by transmitting over two or more communication channels to combat fading and co-channel interference.","Determining an appropriate transmission strategy that satisfies ultra-reliability constraint necessitates derivation of statistics of channel in ultra-reliable region and, subsequently, integration of these statistics into rate selection while incorporating a confidence interval to account for potential uncertainties that may arise during estimation.","In this paper, we propose a novel framework for ultra-reliable real-time transmission considering both spatial diversities and ultra-reliable channel statistics based on multivariate extreme value theory.","First, tail distribution of joint received power sequences obtained from different receivers is modeled while incorporating inter-relations of extreme events occurring rarely based on Poisson point process approach in MEVT.","The optimum transmission strategies are then developed by determining optimum transmission rate based on estimated joint tail distribution and incorporating confidence intervals into estimations to cope with the availability of limited data.","Finally, system reliability is assessed by utilizing outage probability metric.","Through analysis of data obtained from the engine compartment of Fiat Linea, our study showcases the effectiveness of proposed methodology in surpassing traditional extrapolation-based approaches.","This innovative method not only achieves a higher transmission rate, but also effectively addresses stringent requirements of ultra-reliability.","The findings indicate that proposed rate selection framework offers a viable solution for achieving a desired target error probability by employing a higher transmission rate and reducing the amount of training data compared to conventional rate selection methods."],"url":"http://arxiv.org/abs/2401.05171v1"}
{"created":"2024-01-10 14:03:05","title":"CLIP-guided Source-free Object Detection in Aerial Images","abstract":"Domain adaptation is crucial in aerial imagery, as the visual representation of these images can significantly vary based on factors such as geographic location, time, and weather conditions. Additionally, high-resolution aerial images often require substantial storage space and may not be readily accessible to the public. To address these challenges, we propose a novel Source-Free Object Detection (SFOD) method. Specifically, our approach is built upon a self-training framework; however, self-training can lead to inaccurate learning in the absence of labeled training data. To address this issue, we further integrate Contrastive Language-Image Pre-training (CLIP) to guide the generation of pseudo-labels, termed CLIP-guided Aggregation. By leveraging CLIP's zero-shot classification capability, we use it to aggregate scores with the original predicted bounding boxes, enabling us to obtain refined scores for the pseudo-labels. To validate the effectiveness of our method, we constructed two new datasets from different domains based on the DIOR dataset, named DIOR-C and DIOR-Cloudy. Experiments demonstrate that our method outperforms other comparative algorithms.","sentences":["Domain adaptation is crucial in aerial imagery, as the visual representation of these images can significantly vary based on factors such as geographic location, time, and weather conditions.","Additionally, high-resolution aerial images often require substantial storage space and may not be readily accessible to the public.","To address these challenges, we propose a novel Source-Free Object Detection (SFOD) method.","Specifically, our approach is built upon a self-training framework; however, self-training can lead to inaccurate learning in the absence of labeled training data.","To address this issue, we further integrate Contrastive Language-Image Pre-training (CLIP) to guide the generation of pseudo-labels, termed CLIP-guided Aggregation.","By leveraging CLIP's zero-shot classification capability, we use it to aggregate scores with the original predicted bounding boxes, enabling us to obtain refined scores for the pseudo-labels.","To validate the effectiveness of our method, we constructed two new datasets from different domains based on the DIOR dataset, named DIOR-C and DIOR-Cloudy.","Experiments demonstrate that our method outperforms other comparative algorithms."],"url":"http://arxiv.org/abs/2401.05168v1"}
{"created":"2024-01-10 14:02:45","title":"Watermark Text Pattern Spotting in Document Images","abstract":"Watermark text spotting in document images can offer access to an often unexplored source of information, providing crucial evidence about a record's scope, audience and sometimes even authenticity. Stemming from the problem of text spotting, detecting and understanding watermarks in documents inherits the same hardships - in the wild, writing can come in various fonts, sizes and forms, making generic recognition a very difficult problem. To address the lack of resources in this field and propel further research, we propose a novel benchmark (K-Watermark) containing 65,447 data samples generated using Wrender, a watermark text patterns rendering procedure. A validity study using humans raters yields an authenticity score of 0.51 against pre-generated watermarked documents. To prove the usefulness of the dataset and rendering technique, we developed an end-to-end solution (Wextract) for detecting the bounding box instances of watermark text, while predicting the depicted text. To deal with this specific task, we introduce a variance minimization loss and a hierarchical self-attention mechanism. To the best of our knowledge, we are the first to propose an evaluation benchmark and a complete solution for retrieving watermarks from documents surpassing baselines by 5 AP points in detection and 4 points in character accuracy.","sentences":["Watermark text spotting in document images can offer access to an often unexplored source of information, providing crucial evidence about a record's scope, audience and sometimes even authenticity.","Stemming from the problem of text spotting, detecting and understanding watermarks in documents inherits the same hardships - in the wild, writing can come in various fonts, sizes and forms, making generic recognition a very difficult problem.","To address the lack of resources in this field and propel further research, we propose a novel benchmark (K-Watermark) containing 65,447 data samples generated using Wrender, a watermark text patterns rendering procedure.","A validity study using humans raters yields an authenticity score of 0.51 against pre-generated watermarked documents.","To prove the usefulness of the dataset and rendering technique, we developed an end-to-end solution (Wextract) for detecting the bounding box instances of watermark text, while predicting the depicted text.","To deal with this specific task, we introduce a variance minimization loss and a hierarchical self-attention mechanism.","To the best of our knowledge, we are the first to propose an evaluation benchmark and a complete solution for retrieving watermarks from documents surpassing baselines by 5 AP points in detection and 4 points in character accuracy."],"url":"http://arxiv.org/abs/2401.05167v1"}
{"created":"2024-01-10 13:56:40","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA","abstract":"Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision field task data to be applied to VLP. Experiments show that our method achieves excellent results with fewer multimodal datasets and demonstrates the advantages of generative VQA models. The code and model weights will be released upon the paper's acceptance.","sentences":["Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance.","However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios.","Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking.","In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks.","Unlike existing methods, we treat medical VQA as a generative task.","We unify the text encoder and multimodal encoder and align image-text features through multi-task learning.","Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision field task data to be applied to VLP.","Experiments show that our method achieves excellent results with fewer multimodal datasets and demonstrates the advantages of generative VQA models.","The code and model weights will be released upon the paper's acceptance."],"url":"http://arxiv.org/abs/2401.05163v1"}
{"created":"2024-01-10 13:46:03","title":"Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN","abstract":"This study explores the utilization of Dermatoscopic synthetic data generated through stable diffusion models as a strategy for enhancing the robustness of machine learning model training. Synthetic data generation plays a pivotal role in mitigating challenges associated with limited labeled datasets, thereby facilitating more effective model training. In this context, we aim to incorporate enhanced data transformation techniques by extending the recent success of few-shot learning and a small amount of data representation in text-to-image latent diffusion models. The optimally tuned model is further used for rendering high-quality skin lesion synthetic data with diverse and realistic characteristics, providing a valuable supplement and diversity to the existing training data. We investigate the impact of incorporating newly generated synthetic data into the training pipeline of state-of-art machine learning models, assessing its effectiveness in enhancing model performance and generalization to unseen real-world data. Our experimental results demonstrate the efficacy of the synthetic data generated through stable diffusion models helps in improving the robustness and adaptability of end-to-end CNN and vision transformer models on two different real-world skin lesion datasets.","sentences":["This study explores the utilization of Dermatoscopic synthetic data generated through stable diffusion models as a strategy for enhancing the robustness of machine learning model training.","Synthetic data generation plays a pivotal role in mitigating challenges associated with limited labeled datasets, thereby facilitating more effective model training.","In this context, we aim to incorporate enhanced data transformation techniques by extending the recent success of few-shot learning and a small amount of data representation in text-to-image latent diffusion models.","The optimally tuned model is further used for rendering high-quality skin lesion synthetic data with diverse and realistic characteristics, providing a valuable supplement and diversity to the existing training data.","We investigate the impact of incorporating newly generated synthetic data into the training pipeline of state-of-art machine learning models, assessing its effectiveness in enhancing model performance and generalization to unseen real-world data.","Our experimental results demonstrate the efficacy of the synthetic data generated through stable diffusion models helps in improving the robustness and adaptability of end-to-end CNN and vision transformer models on two different real-world skin lesion datasets."],"url":"http://arxiv.org/abs/2401.05159v1"}
{"created":"2024-01-10 13:43:06","title":"Toward distortion-aware change detection in realistic scenarios","abstract":"In the conventional change detection (CD) pipeline, two manually registered and labeled remote sensing datasets serve as the input of the model for training and prediction. However, in realistic scenarios, data from different periods or sensors could fail to be aligned as a result of various coordinate systems. Geometric distortion caused by coordinate shifting remains a thorny issue for CD algorithms. In this paper, we propose a reusable self-supervised framework for bitemporal geometric distortion in CD tasks. The whole framework is composed of Pretext Representation Pre-training, Bitemporal Image Alignment, and Down-stream Decoder Fine-Tuning. With only single-stage pre-training, the key components of the framework can be reused for assistance in the bitemporal image alignment, while simultaneously enhancing the performance of the CD decoder. Experimental results in 2 large-scale realistic scenarios demonstrate that our proposed method can alleviate the bitemporal geometric distortion in CD tasks.","sentences":["In the conventional change detection (CD) pipeline, two manually registered and labeled remote sensing datasets serve as the input of the model for training and prediction.","However, in realistic scenarios, data from different periods or sensors could fail to be aligned as a result of various coordinate systems.","Geometric distortion caused by coordinate shifting remains a thorny issue for CD algorithms.","In this paper, we propose a reusable self-supervised framework for bitemporal geometric distortion in CD tasks.","The whole framework is composed of Pretext Representation Pre-training, Bitemporal Image Alignment, and Down-stream Decoder Fine-Tuning.","With only single-stage pre-training, the key components of the framework can be reused for assistance in the bitemporal image alignment, while simultaneously enhancing the performance of the CD decoder.","Experimental results in 2 large-scale realistic scenarios demonstrate that our proposed method can alleviate the bitemporal geometric distortion in CD tasks."],"url":"http://arxiv.org/abs/2401.05157v1"}
{"created":"2024-01-10 13:32:01","title":"Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational Collaborative SLAM","abstract":"Collaborative Simultaneous Localization and Mapping (CSLAM) is critical to enable multiple robots to operate in complex environments. Most CSLAM techniques rely on raw sensor measurement or low-level features such as keyframe descriptors, which can lead to wrong loop closures due to the lack of deep understanding of the environment. Moreover, the exchange of these measurements and low-level features among the robots requires the transmission of a significant amount of data, which limits the scalability of the system. To overcome these limitations, we present Multi S-Graphs, a decentralized CSLAM system that utilizes high-level semantic-relational information embedded in the four-layered hierarchical and optimizable situational graphs for cooperative map generation and localization while minimizing the information exchanged between the robots. To support this, we present a novel room-based descriptor which, along with its connected walls, is used to perform inter-robot loop closures, addressing the challenges of multi-robot kidnapped problem initialization. Multiple experiments in simulated and real environments validate the improvement in accuracy and robustness of the proposed approach while reducing the amount of data exchanged between robots compared to other state-of-the-art approaches.   Software available within a docker image: https://github.com/snt-arg/multi_s_graphs_docker","sentences":["Collaborative Simultaneous Localization and Mapping (CSLAM) is critical to enable multiple robots to operate in complex environments.","Most CSLAM techniques rely on raw sensor measurement or low-level features such as keyframe descriptors, which can lead to wrong loop closures due to the lack of deep understanding of the environment.","Moreover, the exchange of these measurements and low-level features among the robots requires the transmission of a significant amount of data, which limits the scalability of the system.","To overcome these limitations, we present Multi S-Graphs, a decentralized CSLAM system that utilizes high-level semantic-relational information embedded in the four-layered hierarchical and optimizable situational graphs for cooperative map generation and localization while minimizing the information exchanged between the robots.","To support this, we present a novel room-based descriptor which, along with its connected walls, is used to perform inter-robot loop closures, addressing the challenges of multi-robot kidnapped problem initialization.","Multiple experiments in simulated and real environments validate the improvement in accuracy and robustness of the proposed approach while reducing the amount of data exchanged between robots compared to other state-of-the-art approaches.   ","Software available within a docker image: https://github.com/snt-arg/multi_s_graphs_docker"],"url":"http://arxiv.org/abs/2401.05152v1"}
{"created":"2024-01-10 13:28:37","title":"On the Influence of Reading Sequences on Knowledge Gain during Web Search","abstract":"Nowadays, learning increasingly involves the usage of search engines and web resources. The related interdisciplinary research field search as learning aims to understand how people learn on the web. Previous work has investigated several feature classes to predict, for instance, the expected knowledge gain during web search. Therein, eye-tracking features have not been extensively studied so far. In this paper, we extend a previously used reading model from a line-based one to one that can detect reading sequences across multiple lines. We use publicly available study data from a web-based learning task to examine the relationship between our feature set and the participants' test scores. Our findings demonstrate that learners with higher knowledge gain spent significantly more time reading, and processing more words in total. We also find evidence that faster reading at the expense of more backward regressions may be an indicator of better web-based learning. We make our code publicly available at https://github.com/TIBHannover/reading_web_search.","sentences":["Nowadays, learning increasingly involves the usage of search engines and web resources.","The related interdisciplinary research field search as learning aims to understand how people learn on the web.","Previous work has investigated several feature classes to predict, for instance, the expected knowledge gain during web search.","Therein, eye-tracking features have not been extensively studied so far.","In this paper, we extend a previously used reading model from a line-based one to one that can detect reading sequences across multiple lines.","We use publicly available study data from a web-based learning task to examine the relationship between our feature set and the participants' test scores.","Our findings demonstrate that learners with higher knowledge gain spent significantly more time reading, and processing more words in total.","We also find evidence that faster reading at the expense of more backward regressions may be an indicator of better web-based learning.","We make our code publicly available at https://github.com/TIBHannover/reading_web_search."],"url":"http://arxiv.org/abs/2401.05148v1"}
{"created":"2024-01-10 13:26:19","title":"Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics","abstract":"Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally. Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally. While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear. In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples. Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired \"good\" knowledge. This highlights the necessity for novel Federated Unlearning (FU) algorithms, which can efficiently remove specific clients' contributions without full model retraining. This survey provides background concepts, empirical evidence, and practical guidelines to design/implement efficient FU schemes. Our study includes a detailed analysis of the metrics for evaluating unlearning in FL and presents an in-depth literature review categorizing state-of-the-art FU contributions under a novel taxonomy. Finally, we outline the most relevant and still open technical challenges, by identifying the most promising research directions in the field.","sentences":["Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally.","Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally.","While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear.","In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples.","Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired \"good\" knowledge.","This highlights the necessity for novel Federated Unlearning (FU) algorithms, which can efficiently remove specific clients' contributions without full model retraining.","This survey provides background concepts, empirical evidence, and practical guidelines to design/implement efficient FU schemes.","Our study includes a detailed analysis of the metrics for evaluating unlearning in FL and presents an in-depth literature review categorizing state-of-the-art FU contributions under a novel taxonomy.","Finally, we outline the most relevant and still open technical challenges, by identifying the most promising research directions in the field."],"url":"http://arxiv.org/abs/2401.05146v1"}
{"created":"2024-01-10 13:25:49","title":"Machine Learning to Promote Translational Research: Predicting Patent and Clinical Trial Inclusion in Dementia Research","abstract":"Projected to impact 1.6 million people in the UK by 2040 and costing {\\pounds}25 billion annually, dementia presents a growing challenge to society. This study, a pioneering effort to predict the translational potential of dementia research using machine learning, hopes to address the slow translation of fundamental discoveries into practical applications despite dementia's significant societal and economic impact. We used the Dimensions database to extract data from 43,091 UK dementia research publications between the years 1990-2023, specifically metadata (authors, publication year etc.), concepts mentioned in the paper, and the paper abstract. To prepare the data for machine learning we applied methods such as one hot encoding and/or word embeddings. We trained a CatBoost Classifier to predict if a publication will be cited in a future patent or clinical trial. We trained several model variations. The model combining metadata, concept, and abstract embeddings yielded the highest performance: for patent predictions, an Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.84 and 77.17% accuracy; for clinical trial predictions, an AUROC of 0.81 and 75.11% accuracy. The results demonstrate that integrating machine learning within current research methodologies can uncover overlooked publications, expediting the identification of promising research and potentially transforming dementia research by predicting real-world impact and guiding translational strategies.","sentences":["Projected to impact 1.6 million people in the UK by 2040 and costing {\\pounds}25 billion annually, dementia presents a growing challenge to society.","This study, a pioneering effort to predict the translational potential of dementia research using machine learning, hopes to address the slow translation of fundamental discoveries into practical applications despite dementia's significant societal and economic impact.","We used the Dimensions database to extract data from 43,091 UK dementia research publications between the years 1990-2023, specifically metadata (authors, publication year etc.), concepts mentioned in the paper, and the paper abstract.","To prepare the data for machine learning we applied methods such as one hot encoding and/or word embeddings.","We trained a CatBoost Classifier to predict if a publication will be cited in a future patent or clinical trial.","We trained several model variations.","The model combining metadata, concept, and abstract embeddings yielded the highest performance: for patent predictions, an Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.84 and 77.17% accuracy; for clinical trial predictions, an AUROC of 0.81 and 75.11% accuracy.","The results demonstrate that integrating machine learning within current research methodologies can uncover overlooked publications, expediting the identification of promising research and potentially transforming dementia research by predicting real-world impact and guiding translational strategies."],"url":"http://arxiv.org/abs/2401.05145v1"}
{"created":"2024-01-10 12:27:18","title":"Unpacking Human-AI interactions: From interaction primitives to a design space","abstract":"This paper aims to develop a semi-formal design space for Human-AI interactions, by building a set of interaction primitives which specify the communication between users and AI systems during their interaction. We show how these primitives can be combined into a set of interaction patterns which can provide an abstract specification for exchanging messages between humans and AI/ML models to carry out purposeful interactions. The motivation behind this is twofold: firstly, to provide a compact generalisation of existing practices, that highlights the similarities and differences between systems in terms of their interaction behaviours; and secondly, to support the creation of new systems, in particular by opening the space of possibilities for interactions with models. We present a short literature review on frameworks, guidelines and taxonomies related to the design and implementation of HAI interactions, including human-in-the-loop, explainable AI, as well as hybrid intelligence and collaborative learning approaches. From the literature review, we define a vocabulary for describing information exchanges in terms of providing and requesting particular model-specific data types. Based on this vocabulary, a message passing model for interactions between humans and models is presented, which we demonstrate can account for existing systems and approaches. Finally, we build this into design patterns as mid-level constructs that capture common interactional structures. We discuss how this approach can be used towards a design space for Human-AI interactions that creates new possibilities for designs as well as keeping track of implementation issues and concerns.","sentences":["This paper aims to develop a semi-formal design space for Human-AI interactions, by building a set of interaction primitives which specify the communication between users and AI systems during their interaction.","We show how these primitives can be combined into a set of interaction patterns which can provide an abstract specification for exchanging messages between humans and AI/ML models to carry out purposeful interactions.","The motivation behind this is twofold: firstly, to provide a compact generalisation of existing practices, that highlights the similarities and differences between systems in terms of their interaction behaviours; and secondly, to support the creation of new systems, in particular by opening the space of possibilities for interactions with models.","We present a short literature review on frameworks, guidelines and taxonomies related to the design and implementation of HAI interactions, including human-in-the-loop, explainable AI, as well as hybrid intelligence and collaborative learning approaches.","From the literature review, we define a vocabulary for describing information exchanges in terms of providing and requesting particular model-specific data types.","Based on this vocabulary, a message passing model for interactions between humans and models is presented, which we demonstrate can account for existing systems and approaches.","Finally, we build this into design patterns as mid-level constructs that capture common interactional structures.","We discuss how this approach can be used towards a design space for Human-AI interactions that creates new possibilities for designs as well as keeping track of implementation issues and concerns."],"url":"http://arxiv.org/abs/2401.05115v1"}
{"created":"2024-01-10 12:22:26","title":"Finding XPath Bugs in XML Document Processors via Differential Testing","abstract":"Extensible Markup Language (XML) is a widely used file format for data storage and transmission. Many XML processors support XPath, a query language that enables the extraction of elements from XML documents. These systems can be affected by logic bugs, which are bugs that cause the processor to return incorrect results. In order to tackle such bugs, we propose a new approach, which we realized as a system called XPress. As a test oracle, XPress relies on differential testing, which compares the results of multiple systems on the same test input, and identifies bugs through discrepancies in their outputs. As test inputs, XPress generates both XML documents and XPath queries. Aiming to generate meaningful queries that compute non-empty results, XPress selects a so-called targeted node to guide the XPath expression generation process. Using the targeted node, XPress generates XPath expressions that reference existing context related to the targeted node, such as its tag name and attributes, while also guaranteeing that a predicate evaluates to true before further expanding the query. We tested our approach on six mature XML processors, BaseX, eXist-DB, Saxon, PostgreSQL, libXML2, and a commercial database system. In total, we have found 20 unique bugs in these systems, of which 25 have been verified by the developers, and 12 of which have been fixed. XPress is efficient, as it finds 12 unique bugs in BaseX in 24 hours, which is 2x as fast as naive random generation. We expect that the effectiveness and simplicity of our approach will help to improve the robustness of many XML processors.","sentences":["Extensible Markup Language (XML) is a widely used file format for data storage and transmission.","Many XML processors support XPath, a query language that enables the extraction of elements from XML documents.","These systems can be affected by logic bugs, which are bugs that cause the processor to return incorrect results.","In order to tackle such bugs, we propose a new approach, which we realized as a system called XPress.","As a test oracle, XPress relies on differential testing, which compares the results of multiple systems on the same test input, and identifies bugs through discrepancies in their outputs.","As test inputs, XPress generates both XML documents and XPath queries.","Aiming to generate meaningful queries that compute non-empty results, XPress selects a so-called targeted node to guide the XPath expression generation process.","Using the targeted node, XPress generates XPath expressions that reference existing context related to the targeted node, such as its tag name and attributes, while also guaranteeing that a predicate evaluates to true before further expanding the query.","We tested our approach on six mature XML processors, BaseX, eXist-DB, Saxon, PostgreSQL, libXML2, and a commercial database system.","In total, we have found 20 unique bugs in these systems, of which 25 have been verified by the developers, and 12 of which have been fixed.","XPress is efficient, as it finds 12 unique bugs in BaseX in 24 hours, which is 2x as fast as naive random generation.","We expect that the effectiveness and simplicity of our approach will help to improve the robustness of many XML processors."],"url":"http://arxiv.org/abs/2401.05112v1"}
{"created":"2024-01-10 11:55:58","title":"SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image","abstract":"With recent advancements in aerospace technology, the volume of unlabeled remote sensing image (RSI) data has increased dramatically. Effectively leveraging this data through self-supervised learning (SSL) is vital in the field of remote sensing. However, current methodologies, particularly contrastive learning (CL), a leading SSL method, encounter specific challenges in this domain. Firstly, CL often mistakenly identifies geographically adjacent samples with similar semantic content as negative pairs, leading to confusion during model training. Secondly, as an instance-level discriminative task, it tends to neglect the essential fine-grained features and complex details inherent in unstructured RSIs. To overcome these obstacles, we introduce SwiMDiff, a novel self-supervised pre-training framework designed for RSIs. SwiMDiff employs a scene-wide matching approach that effectively recalibrates labels to recognize data from the same scene as false negatives. This adjustment makes CL more applicable to the nuances of remote sensing. Additionally, SwiMDiff seamlessly integrates CL with a diffusion model. Through the implementation of pixel-level diffusion constraints, we enhance the encoder's ability to capture both the global semantic information and the fine-grained features of the images more comprehensively. Our proposed framework significantly enriches the information available for downstream tasks in remote sensing. Demonstrating exceptional performance in change detection and land-cover classification tasks, SwiMDiff proves its substantial utility and value in the field of remote sensing.","sentences":["With recent advancements in aerospace technology, the volume of unlabeled remote sensing image (RSI) data has increased dramatically.","Effectively leveraging this data through self-supervised learning (SSL) is vital in the field of remote sensing.","However, current methodologies, particularly contrastive learning (CL), a leading SSL method, encounter specific challenges in this domain.","Firstly, CL often mistakenly identifies geographically adjacent samples with similar semantic content as negative pairs, leading to confusion during model training.","Secondly, as an instance-level discriminative task, it tends to neglect the essential fine-grained features and complex details inherent in unstructured RSIs.","To overcome these obstacles, we introduce SwiMDiff, a novel self-supervised pre-training framework designed for RSIs.","SwiMDiff employs a scene-wide matching approach that effectively recalibrates labels to recognize data from the same scene as false negatives.","This adjustment makes CL more applicable to the nuances of remote sensing.","Additionally, SwiMDiff seamlessly integrates CL with a diffusion model.","Through the implementation of pixel-level diffusion constraints, we enhance the encoder's ability to capture both the global semantic information and the fine-grained features of the images more comprehensively.","Our proposed framework significantly enriches the information available for downstream tasks in remote sensing.","Demonstrating exceptional performance in change detection and land-cover classification tasks, SwiMDiff proves its substantial utility and value in the field of remote sensing."],"url":"http://arxiv.org/abs/2401.05093v1"}
{"created":"2024-01-10 11:41:29","title":"Parameterized Algorithms for Minimum Sum Vertex Cover","abstract":"Minimum sum vertex cover of an $n$-vertex graph $G$ is a bijection $\\phi : V(G) \\to [n]$ that minimizes the cost $\\sum_{\\{u,v\\} \\in E(G)} \\min \\{\\phi(u), \\phi(v) \\}$. Finding a minimum sum vertex cover of a graph (the MSVC problem) is NP-hard. MSVC is studied well in the realm of approximation algorithms. The best-known approximation factor in polynomial time for the problem is $16/9$ [Bansal, Batra, Farhadi, and Tetali, SODA 2021]. Recently, Stankovic [APPROX/RANDOM 2022] proved that achieving an approximation ratio better than $1.014$ for MSVC is NP-hard, assuming the Unique Games Conjecture. We study the MSVC problem from the perspective of parameterized algorithms. The parameters we consider are the size of a minimum vertex cover and the size of a minimum clique modulator of the input graph. We obtain the following results.   1. MSVC can be solved in $2^{2^{O(k)}} n^{O(1)}$ time, where $k$ is the size of a minimum vertex cover.   2. MSVC can be solved in $f(k)\\cdot n^{O(1)}$ time for some computable function $f$, where $k$ is the size of a minimum clique modulator.","sentences":["Minimum sum vertex cover of an $n$-vertex graph $G$ is a bijection $\\phi : V(G)","\\to","[n]$ that minimizes the cost $\\sum_{\\{u,v\\} \\in E(G)} \\min \\{\\phi(u), \\phi(v) \\}$. Finding a minimum sum vertex cover of a graph (the MSVC problem) is NP-hard.","MSVC is studied well in the realm of approximation algorithms.","The best-known approximation factor in polynomial time for the problem is $16/9$ [Bansal, Batra, Farhadi, and Tetali, SODA 2021].","Recently, Stankovic [APPROX/RANDOM 2022] proved that achieving an approximation ratio better than $1.014$ for MSVC is NP-hard, assuming the Unique Games Conjecture.","We study the MSVC problem from the perspective of parameterized algorithms.","The parameters we consider are the size of a minimum vertex cover and the size of a minimum clique modulator of the input graph.","We obtain the following results.   ","1. MSVC can be solved in $2^{2^{O(k)}} n^{O(1)}$ time, where $k$ is the size of a minimum vertex cover.   ","2. MSVC can be solved in $f(k)\\cdot n^{O(1)}$ time for some computable function $f$, where $k$ is the size of a minimum clique modulator."],"url":"http://arxiv.org/abs/2401.05085v1"}
{"created":"2024-01-10 11:40:26","title":"Discrete-Time Stress Matrix-Based Formation Control of General Linear Multi-Agent Systems","abstract":"This paper considers the distributed leader-follower stress-matrix-based affine formation control problem of discrete-time linear multi-agent systems with static and dynamic leaders. In leader-follower multi-agent formation control, the aim is to drive a set of agents comprising leaders and followers to form any desired geometric pattern and simultaneously execute any required manoeuvre by controlling only a few agents denoted as leaders. Existing works in literature are mostly limited to the cases where the agents' inter-agent communications are either in the continuous-time settings or the sampled-data cases where the leaders are constrained to constant (or zero) velocities or accelerations. Here, we relax these constraints and study the discrete-time cases where the leaders can have stationary or time-varying velocities. We propose control laws in the study of different situations and provide some sufficient conditions to guarantee the overall system stability. Simulation study is used to demonstrate the efficacy of our proposed control laws.","sentences":["This paper considers the distributed leader-follower stress-matrix-based affine formation control problem of discrete-time linear multi-agent systems with static and dynamic leaders.","In leader-follower multi-agent formation control, the aim is to drive a set of agents comprising leaders and followers to form any desired geometric pattern and simultaneously execute any required manoeuvre by controlling only a few agents denoted as leaders.","Existing works in literature are mostly limited to the cases where the agents' inter-agent communications are either in the continuous-time settings or the sampled-data cases where the leaders are constrained to constant (or zero) velocities or accelerations.","Here, we relax these constraints and study the discrete-time cases where the leaders can have stationary or time-varying velocities.","We propose control laws in the study of different situations and provide some sufficient conditions to guarantee the overall system stability.","Simulation study is used to demonstrate the efficacy of our proposed control laws."],"url":"http://arxiv.org/abs/2401.05083v1"}
{"created":"2024-01-10 11:07:32","title":"Hierarchical Classification of Transversal Skills in Job Ads Based on Sentence Embeddings","abstract":"This paper proposes a classification framework aimed at identifying correlations between job ad requirements and transversal skill sets, with a focus on predicting the necessary skills for individual job descriptions using a deep learning model. The approach involves data collection, preprocessing, and labeling using ESCO (European Skills, Competences, and Occupations) taxonomy. Hierarchical classification and multi-label strategies are used for skill identification, while augmentation techniques address data imbalance, enhancing model robustness. A comparison between results obtained with English-specific and multi-language sentence embedding models reveals close accuracy. The experimental case studies detail neural network configurations, hyperparameters, and cross-validation results, highlighting the efficacy of the hierarchical approach and the suitability of the multi-language model for the diverse European job market. Thus, a new approach is proposed for the hierarchical classification of transversal skills from job ads.","sentences":["This paper proposes a classification framework aimed at identifying correlations between job ad requirements and transversal skill sets, with a focus on predicting the necessary skills for individual job descriptions using a deep learning model.","The approach involves data collection, preprocessing, and labeling using ESCO (European Skills, Competences, and Occupations) taxonomy.","Hierarchical classification and multi-label strategies are used for skill identification, while augmentation techniques address data imbalance, enhancing model robustness.","A comparison between results obtained with English-specific and multi-language sentence embedding models reveals close accuracy.","The experimental case studies detail neural network configurations, hyperparameters, and cross-validation results, highlighting the efficacy of the hierarchical approach and the suitability of the multi-language model for the diverse European job market.","Thus, a new approach is proposed for the hierarchical classification of transversal skills from job ads."],"url":"http://arxiv.org/abs/2401.05073v1"}
{"created":"2024-01-10 10:57:12","title":"MISS: Multiclass Interpretable Scoring Systems","abstract":"In this work, we present a novel, machine-learning approach for constructing Multiclass Interpretable Scoring Systems (MISS) - a fully data-driven methodology for generating single, sparse, and user-friendly scoring systems for multiclass classification problems. Scoring systems are commonly utilized as decision support models in healthcare, criminal justice, and other domains where interpretability of predictions and ease of use are crucial. Prior methods for data-driven scoring, such as SLIM (Supersparse Linear Integer Model), were limited to binary classification tasks and extensions to multiclass domains were primarily accomplished via one-versus-all-type techniques. The scores produced by our method can be easily transformed into class probabilities via the softmax function. We demonstrate techniques for dimensionality reduction and heuristics that enhance the training efficiency and decrease the optimality gap, a measure that can certify the optimality of the model. Our approach has been extensively evaluated on datasets from various domains, and the results indicate that it is competitive with other machine learning models in terms of classification performance metrics and provides well-calibrated class probabilities.","sentences":["In this work, we present a novel, machine-learning approach for constructing Multiclass Interpretable Scoring Systems (MISS) - a fully data-driven methodology for generating single, sparse, and user-friendly scoring systems for multiclass classification problems.","Scoring systems are commonly utilized as decision support models in healthcare, criminal justice, and other domains where interpretability of predictions and ease of use are crucial.","Prior methods for data-driven scoring, such as SLIM (Supersparse Linear Integer Model), were limited to binary classification tasks and extensions to multiclass domains were primarily accomplished via one-versus-all-type techniques.","The scores produced by our method can be easily transformed into class probabilities via the softmax function.","We demonstrate techniques for dimensionality reduction and heuristics that enhance the training efficiency and decrease the optimality gap, a measure that can certify the optimality of the model.","Our approach has been extensively evaluated on datasets from various domains, and the results indicate that it is competitive with other machine learning models in terms of classification performance metrics and provides well-calibrated class probabilities."],"url":"http://arxiv.org/abs/2401.05069v1"}
{"created":"2024-01-10 10:41:38","title":"Singer Identity Representation Learning using Self-Supervised Techniques","abstract":"Significant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and trained models to facilitate further research on singing voice and related areas.","sentences":["Significant strides have been made in creating voice identity representations using speech data.","However, the same level of progress has not been achieved for singing voices.","To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis.","We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations.","We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization.","Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz.","We release our code and trained models to facilitate further research on singing voice and related areas."],"url":"http://arxiv.org/abs/2401.05064v1"}
{"created":"2024-01-10 09:56:26","title":"Accelerating Maximal Biclique Enumeration on GPUs","abstract":"Maximal Biclique Enumeration (MBE) holds critical importance in graph theory with applications extending across fields such as bioinformatics, social networks, and recommendation systems. However, its computational complexity presents barriers for efficiently scaling to large graphs. To address these challenges, we introduce cuMBE, a GPU-optimized parallel algorithm for MBE. Utilizing a unique data structure, called compact array, cuMBE eradicates the need for recursion, thereby significantly minimizing dynamic memory requirements and computational overhead. The algorithm utilizes a hybrid parallelism approach, in which GPU thread blocks handle coarse-grained tasks associated with part of the search process. Besides, we implement three fine-grained optimizations within each thread block to enhance performance. Further, we integrate a work-stealing mechanism to mitigate workload imbalances among thread blocks. Our experiments reveal that cuMBE achieves an geometric mean speedup of 4.02x and 4.13x compared to the state-of-the-art serial algorithm and parallel CPU-based algorithm on both common and real-world datasets, respectively.","sentences":["Maximal Biclique Enumeration (MBE) holds critical importance in graph theory with applications extending across fields such as bioinformatics, social networks, and recommendation systems.","However, its computational complexity presents barriers for efficiently scaling to large graphs.","To address these challenges, we introduce cuMBE, a GPU-optimized parallel algorithm for MBE.","Utilizing a unique data structure, called compact array, cuMBE eradicates the need for recursion, thereby significantly minimizing dynamic memory requirements and computational overhead.","The algorithm utilizes a hybrid parallelism approach, in which GPU thread blocks handle coarse-grained tasks associated with part of the search process.","Besides, we implement three fine-grained optimizations within each thread block to enhance performance.","Further, we integrate a work-stealing mechanism to mitigate workload imbalances among thread blocks.","Our experiments reveal that cuMBE achieves an geometric mean speedup of 4.02x and 4.13x compared to the state-of-the-art serial algorithm and parallel CPU-based algorithm on both common and real-world datasets, respectively."],"url":"http://arxiv.org/abs/2401.05039v1"}
{"created":"2024-01-10 09:49:10","title":"Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk","abstract":"Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via \"self-talk\" of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.","sentences":["Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging.","Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate.","Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions.","Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles.","This approach generates a training data via \"self-talk\" of LLMs that can be refined and utilized for supervised fine-tuning.","We introduce an automated way to measure the (partial) success of a dialogue.","This metric is used to filter the generated conversational data that is fed back in LLM for training.","Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results.","In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data."],"url":"http://arxiv.org/abs/2401.05033v1"}
{"created":"2024-01-10 09:15:50","title":"AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction","abstract":"To achieve seamless collaboration between robots and humans in a shared environment, accurately predicting future human movements is essential. Human motion prediction has traditionally been approached as a sequence prediction problem, leveraging historical human motion data to estimate future poses. Beginning with vanilla recurrent networks, the research community has investigated a variety of methods for learning human motion dynamics, encompassing graph-based and generative approaches. Despite these efforts, achieving accurate long-term predictions continues to be a significant challenge. In this regard, we present the Adversarial Motion Transformer (AdvMT), a novel model that integrates a transformer-based motion encoder and a temporal continuity discriminator. This combination effectively captures spatial and temporal dependencies simultaneously within frames. With adversarial training, our method effectively reduces the unwanted artifacts in predictions, thereby ensuring the learning of more realistic and fluid human motions. The evaluation results indicate that AdvMT greatly enhances the accuracy of long-term predictions while also delivering robust short-term predictions","sentences":["To achieve seamless collaboration between robots and humans in a shared environment, accurately predicting future human movements is essential.","Human motion prediction has traditionally been approached as a sequence prediction problem, leveraging historical human motion data to estimate future poses.","Beginning with vanilla recurrent networks, the research community has investigated a variety of methods for learning human motion dynamics, encompassing graph-based and generative approaches.","Despite these efforts, achieving accurate long-term predictions continues to be a significant challenge.","In this regard, we present the Adversarial Motion Transformer (AdvMT), a novel model that integrates a transformer-based motion encoder and a temporal continuity discriminator.","This combination effectively captures spatial and temporal dependencies simultaneously within frames.","With adversarial training, our method effectively reduces the unwanted artifacts in predictions, thereby ensuring the learning of more realistic and fluid human motions.","The evaluation results indicate that AdvMT greatly enhances the accuracy of long-term predictions while also delivering robust short-term predictions"],"url":"http://arxiv.org/abs/2401.05018v1"}
{"created":"2024-01-10 09:02:24","title":"Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential of Task-Irrelevant Data","abstract":"Source-free cross-modal knowledge transfer is a crucial yet challenging task, which aims to transfer knowledge from one source modality (e.g., RGB) to the target modality (e.g., depth or infrared) with no access to the task-relevant (TR) source data due to memory and privacy concerns. A recent attempt leverages the paired task-irrelevant (TI) data and directly matches the features from them to eliminate the modality gap. However, it ignores a pivotal clue that the paired TI data could be utilized to effectively estimate the source data distribution and better facilitate knowledge transfer to the target modality. To this end, we propose a novel yet concise framework to unlock the potential of paired TI data for enhancing source-free cross-modal knowledge transfer. Our work is buttressed by two key technical components. Firstly, to better estimate the source data distribution, we introduce a Task-irrelevant data-Guided Modality Bridging (TGMB) module. It translates the target modality data (e.g., infrared) into the source-like RGB images based on paired TI data and the guidance of the available source model to alleviate two key gaps: 1) inter-modality gap between the paired TI data; 2) intra-modality gap between TI and TR target data. We then propose a Task-irrelevant data-Guided Knowledge Transfer (TGKT) module that transfers knowledge from the source model to the target model by leveraging the paired TI data. Notably, due to the unavailability of labels for the TR target data and its less reliable prediction from the source model, our TGKT model incorporates a self-supervised pseudo-labeling approach to enable the target model to learn from its predictions. Extensive experiments show that our method achieves state-of-the-art performance on three datasets (RGB-to-depth and RGB-to-infrared).","sentences":["Source-free cross-modal knowledge transfer is a crucial yet challenging task, which aims to transfer knowledge from one source modality (e.g., RGB) to the target modality (e.g., depth or infrared) with no access to the task-relevant (TR) source data due to memory and privacy concerns.","A recent attempt leverages the paired task-irrelevant (TI) data and directly matches the features from them to eliminate the modality gap.","However, it ignores a pivotal clue that the paired TI data could be utilized to effectively estimate the source data distribution and better facilitate knowledge transfer to the target modality.","To this end, we propose a novel yet concise framework to unlock the potential of paired TI data for enhancing source-free cross-modal knowledge transfer.","Our work is buttressed by two key technical components.","Firstly, to better estimate the source data distribution, we introduce a Task-irrelevant data-Guided Modality Bridging (TGMB) module.","It translates the target modality data (e.g., infrared) into the source-like RGB images based on paired TI data and the guidance of the available source model to alleviate two key gaps: 1) inter-modality gap between the paired TI data; 2) intra-modality gap between TI and TR target data.","We then propose a Task-irrelevant data-Guided Knowledge Transfer (TGKT) module that transfers knowledge from the source model to the target model by leveraging the paired TI data.","Notably, due to the unavailability of labels for the TR target data and its less reliable prediction from the source model, our TGKT model incorporates a self-supervised pseudo-labeling approach to enable the target model to learn from its predictions.","Extensive experiments show that our method achieves state-of-the-art performance on three datasets (RGB-to-depth and RGB-to-infrared)."],"url":"http://arxiv.org/abs/2401.05014v1"}
{"created":"2024-01-10 08:56:07","title":"Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object Detection","abstract":"Semi-supervised 3D object detection is a promising yet under-explored direction to reduce data annotation costs, especially for cluttered indoor scenes. A few prior works, such as SESS and 3DIoUMatch, attempt to solve this task by utilizing a teacher model to generate pseudo-labels for unlabeled samples. However, the availability of unlabeled samples in the 3D domain is relatively limited compared to its 2D counterpart due to the greater effort required to collect 3D data. Moreover, the loose consistency regularization in SESS and restricted pseudo-label selection strategy in 3DIoUMatch lead to either low-quality supervision or a limited amount of pseudo labels. To address these issues, we present a novel Dual-Perspective Knowledge Enrichment approach named DPKE for semi-supervised 3D object detection. Our DPKE enriches the knowledge of limited training data, particularly unlabeled data, from two perspectives: data-perspective and feature-perspective. Specifically, from the data-perspective, we propose a class-probabilistic data augmentation method that augments the input data with additional instances based on the varying distribution of class probabilities. Our DPKE achieves feature-perspective knowledge enrichment by designing a geometry-aware feature matching method that regularizes feature-level similarity between object proposals from the student and teacher models. Extensive experiments on the two benchmark datasets demonstrate that our DPKE achieves superior performance over existing state-of-the-art approaches under various label ratio conditions. The source code will be made available to the public.","sentences":["Semi-supervised 3D object detection is a promising yet under-explored direction to reduce data annotation costs, especially for cluttered indoor scenes.","A few prior works, such as SESS and 3DIoUMatch, attempt to solve this task by utilizing a teacher model to generate pseudo-labels for unlabeled samples.","However, the availability of unlabeled samples in the 3D domain is relatively limited compared to its 2D counterpart due to the greater effort required to collect 3D data.","Moreover, the loose consistency regularization in SESS and restricted pseudo-label selection strategy in 3DIoUMatch lead to either low-quality supervision or a limited amount of pseudo labels.","To address these issues, we present a novel Dual-Perspective Knowledge Enrichment approach named DPKE for semi-supervised 3D object detection.","Our DPKE enriches the knowledge of limited training data, particularly unlabeled data, from two perspectives: data-perspective and feature-perspective.","Specifically, from the data-perspective, we propose a class-probabilistic data augmentation method that augments the input data with additional instances based on the varying distribution of class probabilities.","Our DPKE achieves feature-perspective knowledge enrichment by designing a geometry-aware feature matching method that regularizes feature-level similarity between object proposals from the student and teacher models.","Extensive experiments on the two benchmark datasets demonstrate that our DPKE achieves superior performance over existing state-of-the-art approaches under various label ratio conditions.","The source code will be made available to the public."],"url":"http://arxiv.org/abs/2401.05011v1"}
{"created":"2024-01-10 08:28:36","title":"Distributed Experimental Design Networks","abstract":"As edge computing capabilities increase, model learning deployments in diverse edge environments have emerged. In experimental design networks, introduced recently, network routing and rate allocation are designed to aid the transfer of data from sensors to heterogeneous learners. We design efficient experimental design network algorithms that are (a) distributed and (b) use multicast transmissions. This setting poses significant challenges as classic decentralization approaches often operate on (strictly) concave objectives under differentiable constraints. In contrast, the problem we study here has a non-convex, continuous DR-submodular objective, while multicast transmissions naturally result in non-differentiable constraints. From a technical standpoint, we propose a distributed Frank-Wolfe and a distributed projected gradient ascent algorithm that, coupled with a relaxation of non-differentiable constraints, yield allocations within a $1-1/e$ factor from the optimal. Numerical evaluations show that our proposed algorithms outperform competitors with respect to model learning quality.","sentences":["As edge computing capabilities increase, model learning deployments in diverse edge environments have emerged.","In experimental design networks, introduced recently, network routing and rate allocation are designed to aid the transfer of data from sensors to heterogeneous learners.","We design efficient experimental design network algorithms that are (a) distributed and (b) use multicast transmissions.","This setting poses significant challenges as classic decentralization approaches often operate on (strictly) concave objectives under differentiable constraints.","In contrast, the problem we study here has a non-convex, continuous DR-submodular objective, while multicast transmissions naturally result in non-differentiable constraints.","From a technical standpoint, we propose a distributed Frank-Wolfe and a distributed projected gradient ascent algorithm that, coupled with a relaxation of non-differentiable constraints, yield allocations within a $1-1/e$ factor from the optimal.","Numerical evaluations show that our proposed algorithms outperform competitors with respect to model learning quality."],"url":"http://arxiv.org/abs/2401.04996v1"}
{"created":"2024-01-10 08:10:08","title":"Optimising Graph Representation for Hardware Implementation of Graph Convolutional Networks for Event-based Vision","abstract":"Event-based vision is an emerging research field involving processing data generated by Dynamic Vision Sensors (neuromorphic cameras). One of the latest proposals in this area are Graph Convolutional Networks (GCNs), which allow to process events in its original sparse form while maintaining high detection and classification performance. In this paper, we present the hardware implementation of a~graph generation process from an event camera data stream, taking into account both the advantages and limitations of FPGAs. We propose various ways to simplify the graph representation and use scaling and quantisation of values. We consider both undirected and directed graphs that enable the use of PointNet convolution. The results obtained show that by appropriately modifying the graph representation, it is possible to create a~hardware module for graph generation. Moreover, the proposed modifications have no significant impact on object detection performance, only 0.08% mAP less for the base model and the N-Caltech data set.Finally, we describe the proposed hardware architecture of the graph generation module.","sentences":["Event-based vision is an emerging research field involving processing data generated by Dynamic Vision Sensors (neuromorphic cameras).","One of the latest proposals in this area are Graph Convolutional Networks (GCNs), which allow to process events in its original sparse form while maintaining high detection and classification performance.","In this paper, we present the hardware implementation of a~graph generation process from an event camera data stream, taking into account both the advantages and limitations of FPGAs.","We propose various ways to simplify the graph representation and use scaling and quantisation of values.","We consider both undirected and directed graphs that enable the use of PointNet convolution.","The results obtained show that by appropriately modifying the graph representation, it is possible to create a~hardware module for graph generation.","Moreover, the proposed modifications have no significant impact on object detection performance, only 0.08% mAP less for the base model and the N-Caltech data set.","Finally, we describe the proposed hardware architecture of the graph generation module."],"url":"http://arxiv.org/abs/2401.04988v1"}
{"created":"2024-01-10 08:02:38","title":"Structure-Preserving Physics-Informed Neural Networks With Energy or Lyapunov Structure","abstract":"Recently, there has been growing interest in using physics-informed neural networks (PINNs) to solve differential equations. However, the preservation of structure, such as energy and stability, in a suitable manner has yet to be established. This limitation could be a potential reason why the learning process for PINNs is not always efficient and the numerical results may suggest nonphysical behavior. Besides, there is little research on their applications on downstream tasks. To address these issues, we propose structure-preserving PINNs to improve their performance and broaden their applications for downstream tasks. Firstly, by leveraging prior knowledge about the physical system, a structure-preserving loss function is designed to assist the PINN in learning the underlying structure. Secondly, a framework that utilizes structure-preserving PINN for robust image recognition is proposed. Here, preserving the Lyapunov structure of the underlying system ensures the stability of the system. Experimental results demonstrate that the proposed method improves the numerical accuracy of PINNs for partial differential equations. Furthermore, the robustness of the model against adversarial perturbations in image data is enhanced.","sentences":["Recently, there has been growing interest in using physics-informed neural networks (PINNs) to solve differential equations.","However, the preservation of structure, such as energy and stability, in a suitable manner has yet to be established.","This limitation could be a potential reason why the learning process for PINNs is not always efficient and the numerical results may suggest nonphysical behavior.","Besides, there is little research on their applications on downstream tasks.","To address these issues, we propose structure-preserving PINNs to improve their performance and broaden their applications for downstream tasks.","Firstly, by leveraging prior knowledge about the physical system, a structure-preserving loss function is designed to assist the PINN in learning the underlying structure.","Secondly, a framework that utilizes structure-preserving PINN for robust image recognition is proposed.","Here, preserving the Lyapunov structure of the underlying system ensures the stability of the system.","Experimental results demonstrate that the proposed method improves the numerical accuracy of PINNs for partial differential equations.","Furthermore, the robustness of the model against adversarial perturbations in image data is enhanced."],"url":"http://arxiv.org/abs/2401.04986v1"}
{"created":"2024-01-10 07:58:44","title":"MGNet: Learning Correspondences via Multiple Graphs","abstract":"Learning correspondences aims to find correct correspondences (inliers) from the initial correspondence set with an uneven correspondence distribution and a low inlier rate, which can be regarded as graph data. Recent advances usually use graph neural networks (GNNs) to build a single type of graph or simply stack local graphs into the global one to complete the task. But they ignore the complementary relationship between different types of graphs, which can effectively capture potential relationships among sparse correspondences. To address this problem, we propose MGNet to effectively combine multiple complementary graphs. To obtain information integrating implicit and explicit local graphs, we construct local graphs from implicit and explicit aspects and combine them effectively, which is used to build a global graph. Moreover, we propose Graph~Soft~Degree~Attention (GSDA) to make full use of all sparse correspondence information at once in the global graph, which can capture and amplify discriminative features. Extensive experiments demonstrate that MGNet outperforms state-of-the-art methods in different visual tasks. The code is provided in https://github.com/DAILUANYUAN/MGNet-2024AAAI.","sentences":["Learning correspondences aims to find correct correspondences (inliers) from the initial correspondence set with an uneven correspondence distribution and a low inlier rate, which can be regarded as graph data.","Recent advances usually use graph neural networks (GNNs) to build a single type of graph or simply stack local graphs into the global one to complete the task.","But they ignore the complementary relationship between different types of graphs, which can effectively capture potential relationships among sparse correspondences.","To address this problem, we propose MGNet to effectively combine multiple complementary graphs.","To obtain information integrating implicit and explicit local graphs, we construct local graphs from implicit and explicit aspects and combine them effectively, which is used to build a global graph.","Moreover, we propose Graph~Soft~Degree~Attention (GSDA) to make full use of all sparse correspondence information at once in the global graph, which can capture and amplify discriminative features.","Extensive experiments demonstrate that MGNet outperforms state-of-the-art methods in different visual tasks.","The code is provided in https://github.com/DAILUANYUAN/MGNet-2024AAAI."],"url":"http://arxiv.org/abs/2401.04984v1"}
{"created":"2024-01-10 07:51:02","title":"Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series","abstract":"To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significantly advances irregular time series analysis, introducing innovative techniques and offering a versatile tool for diverse practical applications.","sentences":["To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method.","While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form.","Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden.","Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics.","Our research presents an advanced framework that excels in both classification and interpolation tasks.","At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks.","Empirical analysis demonstrates that our method significantly outperforms existing models.","This work significantly advances irregular time series analysis, introducing innovative techniques and offering a versatile tool for diverse practical applications."],"url":"http://arxiv.org/abs/2401.04979v1"}
{"created":"2024-01-10 07:42:55","title":"HaltingVT: Adaptive Token Halting Transformer for Efficient Video Recognition","abstract":"Action recognition in videos poses a challenge due to its high computational cost, especially for Joint Space-Time video transformers (Joint VT). Despite their effectiveness, the excessive number of tokens in such architectures significantly limits their efficiency. In this paper, we propose HaltingVT, an efficient video transformer adaptively removing redundant video patch tokens, which is primarily composed of a Joint VT and a Glimpser module. Specifically, HaltingVT applies data-adaptive token reduction at each layer, resulting in a significant reduction in the overall computational cost. Besides, the Glimpser module quickly removes redundant tokens in shallow transformer layers, which may even be misleading for video recognition tasks based on our observations. To further encourage HaltingVT to focus on the key motion-related information in videos, we design an effective Motion Loss during training. HaltingVT acquires video analysis capabilities and token halting compression strategies simultaneously in a unified training process, without requiring additional training procedures or sub-networks. On the Mini-Kinetics dataset, we achieved 75.0% top-1 ACC with 24.2 GFLOPs, as well as 67.2% top-1 ACC with an extremely low 9.9 GFLOPs. The code is available at https://github.com/dun-research/HaltingVT.","sentences":["Action recognition in videos poses a challenge due to its high computational cost, especially for Joint Space-Time video transformers (Joint VT).","Despite their effectiveness, the excessive number of tokens in such architectures significantly limits their efficiency.","In this paper, we propose HaltingVT, an efficient video transformer adaptively removing redundant video patch tokens, which is primarily composed of a Joint VT and a Glimpser module.","Specifically, HaltingVT applies data-adaptive token reduction at each layer, resulting in a significant reduction in the overall computational cost.","Besides, the Glimpser module quickly removes redundant tokens in shallow transformer layers, which may even be misleading for video recognition tasks based on our observations.","To further encourage HaltingVT to focus on the key motion-related information in videos, we design an effective Motion Loss during training.","HaltingVT acquires video analysis capabilities and token halting compression strategies simultaneously in a unified training process, without requiring additional training procedures or sub-networks.","On the Mini-Kinetics dataset, we achieved 75.0% top-1 ACC with 24.2 GFLOPs, as well as 67.2% top-1 ACC with an extremely low 9.9 GFLOPs.","The code is available at https://github.com/dun-research/HaltingVT."],"url":"http://arxiv.org/abs/2401.04975v1"}
{"created":"2024-01-10 07:33:32","title":"Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation","abstract":"Machine translation often suffers from biased data and algorithms that can lead to unacceptable errors in system output. While bias in gender norms has been investigated, less is known about whether MT systems encode bias about social relationships, e.g. sentences such as \"the lawyer kissed her wife.\" We investigate the degree of bias against same-gender relationships in MT systems, using generated template sentences drawn from several noun-gender languages (e.g. Spanish). We find that three popular MT services consistently fail to accurately translate sentences concerning relationships between nouns of the same gender. The error rate varies considerably based on the context, e.g. same-gender sentences referencing high female-representation occupations are translated with lower accuracy. We provide this work as a case study in the evaluation of intrinsic bias in NLP systems, with respect to social relationships.","sentences":["Machine translation often suffers from biased data and algorithms that can lead to unacceptable errors in system output.","While bias in gender norms has been investigated, less is known about whether MT systems encode bias about social relationships, e.g. sentences such as \"the lawyer kissed her wife.\"","We investigate the degree of bias against same-gender relationships in MT systems, using generated template sentences drawn from several noun-gender languages (e.g. Spanish).","We find that three popular MT services consistently fail to accurately translate sentences concerning relationships between nouns of the same gender.","The error rate varies considerably based on the context, e.g. same-gender sentences referencing high female-representation occupations are translated with lower accuracy.","We provide this work as a case study in the evaluation of intrinsic bias in NLP systems, with respect to social relationships."],"url":"http://arxiv.org/abs/2401.04972v1"}
{"created":"2024-01-10 07:00:07","title":"Why Change Your Controller When You Can Change Your Planner: Drag-Aware Trajectory Generation for Quadrotor Systems","abstract":"Motivated by the increasing use of quadrotors for payload delivery, we consider a joint trajectory generation and feedback control design problem for a quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag forces from carried payloads can lead to catastrophic outcomes. Prior work model aerodynamic effects as residual dynamics or external disturbances in the control problem leading to a reactive policy that could be catastrophic. Moreover, redesigning controllers and tuning control gains on hardware platforms is a laborious effort. In this paper, we argue that adapting the trajectory generation component keeping the controller fixed can improve trajectory tracking for quadrotor systems experiencing drag forces. To achieve this, we formulate a drag-aware planning problem by applying a suitable relaxation to an optimal quadrotor control problem, introducing a tracking cost function which measures the ability of a controller to follow a reference trajectory. This tracking cost function acts as a regularizer in trajectory generation and is learned from data obtained from simulation. Our experiments in both simulation and on the Crazyflie hardware platform show that changing the planner reduces tracking error by as much as 83%. Evaluation on hardware demonstrates that our planned path, as opposed to a baseline, avoids controller saturation and catastrophic outcomes during aggressive maneuvers.","sentences":["Motivated by the increasing use of quadrotors for payload delivery, we consider a joint trajectory generation and feedback control design problem for a quadrotor experiencing aerodynamic wrenches.","Unmodeled aerodynamic drag forces from carried payloads can lead to catastrophic outcomes.","Prior work model aerodynamic effects as residual dynamics or external disturbances in the control problem leading to a reactive policy that could be catastrophic.","Moreover, redesigning controllers and tuning control gains on hardware platforms is a laborious effort.","In this paper, we argue that adapting the trajectory generation component keeping the controller fixed can improve trajectory tracking for quadrotor systems experiencing drag forces.","To achieve this, we formulate a drag-aware planning problem by applying a suitable relaxation to an optimal quadrotor control problem, introducing a tracking cost function which measures the ability of a controller to follow a reference trajectory.","This tracking cost function acts as a regularizer in trajectory generation and is learned from data obtained from simulation.","Our experiments in both simulation and on the Crazyflie hardware platform show that changing the planner reduces tracking error by as much as 83%.","Evaluation on hardware demonstrates that our planned path, as opposed to a baseline, avoids controller saturation and catastrophic outcomes during aggressive maneuvers."],"url":"http://arxiv.org/abs/2401.04960v1"}
{"created":"2024-01-10 06:45:37","title":"EmMixformer: Mix transformer for eye movement recognition","abstract":"Eye movement (EM) is a new highly secure biometric behavioral modality that has received increasing attention in recent years. Although deep neural networks, such as convolutional neural network (CNN), have recently achieved promising performance, current solutions fail to capture local and global temporal dependencies within eye movement data. To overcome this problem, we propose in this paper a mixed transformer termed EmMixformer to extract time and frequency domain information for eye movement recognition. To this end, we propose a mixed block consisting of three modules, transformer, attention Long short-term memory (attention LSTM), and Fourier transformer. We are the first to attempt leveraging transformer to learn long temporal dependencies within eye movement. Second, we incorporate the attention mechanism into LSTM to propose attention LSTM with the aim to learn short temporal dependencies. Third, we perform self attention in the frequency domain to learn global features. As the three modules provide complementary feature representations in terms of local and global dependencies, the proposed EmMixformer is capable of improving recognition accuracy. The experimental results on our eye movement dataset and two public eye movement datasets show that the proposed EmMixformer outperforms the state of the art by achieving the lowest verification error.","sentences":["Eye movement (EM) is a new highly secure biometric behavioral modality that has received increasing attention in recent years.","Although deep neural networks, such as convolutional neural network (CNN), have recently achieved promising performance, current solutions fail to capture local and global temporal dependencies within eye movement data.","To overcome this problem, we propose in this paper a mixed transformer termed EmMixformer to extract time and frequency domain information for eye movement recognition.","To this end, we propose a mixed block consisting of three modules, transformer, attention Long short-term memory (attention LSTM), and Fourier transformer.","We are the first to attempt leveraging transformer to learn long temporal dependencies within eye movement.","Second, we incorporate the attention mechanism into LSTM to propose attention LSTM with the aim to learn short temporal dependencies.","Third, we perform self attention in the frequency domain to learn global features.","As the three modules provide complementary feature representations in terms of local and global dependencies, the proposed EmMixformer is capable of improving recognition accuracy.","The experimental results on our eye movement dataset and two public eye movement datasets show that the proposed EmMixformer outperforms the state of the art by achieving the lowest verification error."],"url":"http://arxiv.org/abs/2401.04956v1"}
{"created":"2024-01-10 05:38:48","title":"Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics","abstract":"In the past several years, road anomaly segmentation is actively explored in the academia and drawing growing attention in the industry. The rationale behind is straightforward: if the autonomous car can brake before hitting an anomalous object, safety is promoted. However, this rationale naturally calls for a temporally informed setting while existing methods and benchmarks are designed in an unrealistic frame-wise manner. To bridge this gap, we contribute the first video anomaly segmentation dataset for autonomous driving. Since placing various anomalous objects on busy roads and annotating them in every frame are dangerous and expensive, we resort to synthetic data. To improve the relevance of this synthetic dataset to real-world applications, we train a generative adversarial network conditioned on rendering G-buffers for photorealism enhancement. Our dataset consists of 120,000 high-resolution frames at a 60 FPS framerate, as recorded in 7 different towns. As an initial benchmarking, we provide baselines using latest supervised and unsupervised road anomaly segmentation methods. Apart from conventional ones, we focus on two new metrics: temporal consistency and latencyaware streaming accuracy. We believe the latter is valuable as it measures whether an anomaly segmentation algorithm can truly prevent a car from crashing in a temporally informed setting.","sentences":["In the past several years, road anomaly segmentation is actively explored in the academia and drawing growing attention in the industry.","The rationale behind is straightforward: if the autonomous car can brake before hitting an anomalous object, safety is promoted.","However, this rationale naturally calls for a temporally informed setting while existing methods and benchmarks are designed in an unrealistic frame-wise manner.","To bridge this gap, we contribute the first video anomaly segmentation dataset for autonomous driving.","Since placing various anomalous objects on busy roads and annotating them in every frame are dangerous and expensive, we resort to synthetic data.","To improve the relevance of this synthetic dataset to real-world applications, we train a generative adversarial network conditioned on rendering G-buffers for photorealism enhancement.","Our dataset consists of 120,000 high-resolution frames at a 60 FPS framerate, as recorded in 7 different towns.","As an initial benchmarking, we provide baselines using latest supervised and unsupervised road anomaly segmentation methods.","Apart from conventional ones, we focus on two new metrics: temporal consistency and latencyaware streaming accuracy.","We believe the latter is valuable as it measures whether an anomaly segmentation algorithm can truly prevent a car from crashing in a temporally informed setting."],"url":"http://arxiv.org/abs/2401.04942v1"}
{"created":"2024-01-10 04:58:17","title":"Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks","abstract":"Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network classifier to determine membership. The experiment results show that LDC-MIA can improve TPR at low FPR by up to 4x compared to the other difficulty calibration based MIAs. It also has the highest Area Under ROC curve (AUC) across all datasets. Our method's cost is comparable with most of the existing MIAs, but is orders of magnitude more efficient than one of the state-of-the-art methods, LiRA, while achieving similar performance.","sentences":["Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance.","However, using sensitive data to train these models raises concerns about privacy and security.","One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset.","While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%).","This is a crucial factor to consider for an MIA to be practically useful in real-world settings.","In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs.","Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network classifier to determine membership.","The experiment results show that LDC-MIA can improve TPR at low FPR by up to 4x compared to the other difficulty calibration based MIAs.","It also has the highest Area Under ROC curve (AUC) across all datasets.","Our method's cost is comparable with most of the existing MIAs, but is orders of magnitude more efficient than one of the state-of-the-art methods, LiRA, while achieving similar performance."],"url":"http://arxiv.org/abs/2401.04929v1"}
{"created":"2024-01-10 04:55:24","title":"Relaxed Contrastive Learning for Federated Learning","abstract":"We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\\\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge margins on the standard benchmarks through extensive experimental results.","sentences":["We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning.","We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations.","In addition, we show that a na\\\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains.","To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class.","This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements.","Our framework outperforms all existing federated learning approaches by huge margins on the standard benchmarks through extensive experimental results."],"url":"http://arxiv.org/abs/2401.04928v1"}
{"created":"2024-01-10 04:18:02","title":"Inconsistency-Based Data-Centric Active Open-Set Annotation","abstract":"Active learning is a commonly used approach that reduces the labeling effort required to train deep neural networks. However, the effectiveness of current active learning methods is limited by their closed-world assumptions, which assume that all data in the unlabeled pool comes from a set of predefined known classes. This assumption is often not valid in practical situations, as there may be unknown classes in the unlabeled data, leading to the active open-set annotation problem. The presence of unknown classes in the data can significantly impact the performance of existing active learning methods due to the uncertainty they introduce. To address this issue, we propose a novel data-centric active learning method called NEAT that actively annotates open-set data. NEAT is designed to label known classes data from a pool of both known and unknown classes unlabeled data. It utilizes the clusterability of labels to identify the known classes from the unlabeled pool and selects informative samples from those classes based on a consistency criterion that measures inconsistencies between model predictions and local feature distribution. Unlike the recently proposed learning-centric method for the same problem, NEAT is much more computationally efficient and is a data-centric active open-set annotation method. Our experiments demonstrate that NEAT achieves significantly better performance than state-of-the-art active learning methods for active open-set annotation.","sentences":["Active learning is a commonly used approach that reduces the labeling effort required to train deep neural networks.","However, the effectiveness of current active learning methods is limited by their closed-world assumptions, which assume that all data in the unlabeled pool comes from a set of predefined known classes.","This assumption is often not valid in practical situations, as there may be unknown classes in the unlabeled data, leading to the active open-set annotation problem.","The presence of unknown classes in the data can significantly impact the performance of existing active learning methods due to the uncertainty they introduce.","To address this issue, we propose a novel data-centric active learning method called NEAT that actively annotates open-set data.","NEAT is designed to label known classes data from a pool of both known and unknown classes unlabeled data.","It utilizes the clusterability of labels to identify the known classes from the unlabeled pool and selects informative samples from those classes based on a consistency criterion that measures inconsistencies between model predictions and local feature distribution.","Unlike the recently proposed learning-centric method for the same problem, NEAT is much more computationally efficient and is a data-centric active open-set annotation method.","Our experiments demonstrate that NEAT achieves significantly better performance than state-of-the-art active learning methods for active open-set annotation."],"url":"http://arxiv.org/abs/2401.04923v1"}
{"created":"2024-01-10 04:00:37","title":"Digital Retina for IoV Towards 6G: Architecture, Opportunities, and Challenges","abstract":"Vehicles are no longer isolated entities in traffic environments, thanks to the development of IoV powered by 5G networks and their evolution into 6G. However, it is not enough for vehicles in a highly dynamic and complex traffic environment to make reliable and efficient decisions. As a result, this paper proposes a cloud-edge-end computing system with multi-streams for IoV, referred to as Vehicular Digital Retina (VDR). Local computing and edge computing are effectively integrated in the VDR system through the aid of vehicle-to-everything (V2X) networks, resulting in a heterogeneous computing environment that improves vehicles' perception and decision-making abilities with collaborative strategies. Once the system framework is presented, various important functions in the VDR system are explained in detail, including V2X-aided collaborative perception, V2X-aided stream sharing for collaborative learning, and V2X-aided secured collaboration. All of them enable the development of efficient mechanisms of data sharing and information interaction with high security for collaborative intelligent driving. We also present a case study with simulation results to demonstrate the effectiveness of the proposed VDR system.","sentences":["Vehicles are no longer isolated entities in traffic environments, thanks to the development of IoV powered by 5G networks and their evolution into 6G.","However, it is not enough for vehicles in a highly dynamic and complex traffic environment to make reliable and efficient decisions.","As a result, this paper proposes a cloud-edge-end computing system with multi-streams for IoV, referred to as Vehicular Digital Retina (VDR).","Local computing and edge computing are effectively integrated in the VDR system through the aid of vehicle-to-everything (V2X) networks, resulting in a heterogeneous computing environment that improves vehicles' perception and decision-making abilities with collaborative strategies.","Once the system framework is presented, various important functions in the VDR system are explained in detail, including V2X-aided collaborative perception, V2X-aided stream sharing for collaborative learning, and V2X-aided secured collaboration.","All of them enable the development of efficient mechanisms of data sharing and information interaction with high security for collaborative intelligent driving.","We also present a case study with simulation results to demonstrate the effectiveness of the proposed VDR system."],"url":"http://arxiv.org/abs/2401.04916v1"}
{"created":"2024-01-10 03:43:50","title":"DualVAE: Dual Disentangled Variational AutoEncoder for Recommendation","abstract":"Learning precise representations of users and items to fit observed interaction data is the fundamental task of collaborative filtering. Existing studies usually infer entangled representations to fit such interaction data, neglecting to model the diverse matching relationships between users and items behind their interactions, leading to limited performance and weak interpretability. To address this problem, we propose a Dual Disentangled Variational AutoEncoder (DualVAE) for collaborative recommendation, which combines disentangled representation learning with variational inference to facilitate the generation of implicit interaction data. Specifically, we first implement the disentangling concept by unifying an attention-aware dual disentanglement and disentangled variational autoencoder to infer the disentangled latent representations of users and items. Further, to encourage the correspondence and independence of disentangled representations of users and items, we design a neighborhood-enhanced representation constraint with a customized contrastive mechanism to improve the representation quality. Extensive experiments on three real-world benchmarks show that our proposed model significantly outperforms several recent state-of-the-art baselines. Further empirical experimental results also illustrate the interpretability of the disentangled representations learned by DualVAE.","sentences":["Learning precise representations of users and items to fit observed interaction data is the fundamental task of collaborative filtering.","Existing studies usually infer entangled representations to fit such interaction data, neglecting to model the diverse matching relationships between users and items behind their interactions, leading to limited performance and weak interpretability.","To address this problem, we propose a Dual Disentangled Variational AutoEncoder (DualVAE) for collaborative recommendation, which combines disentangled representation learning with variational inference to facilitate the generation of implicit interaction data.","Specifically, we first implement the disentangling concept by unifying an attention-aware dual disentanglement and disentangled variational autoencoder to infer the disentangled latent representations of users and items.","Further, to encourage the correspondence and independence of disentangled representations of users and items, we design a neighborhood-enhanced representation constraint with a customized contrastive mechanism to improve the representation quality.","Extensive experiments on three real-world benchmarks show that our proposed model significantly outperforms several recent state-of-the-art baselines.","Further empirical experimental results also illustrate the interpretability of the disentangled representations learned by DualVAE."],"url":"http://arxiv.org/abs/2401.04914v1"}
{"created":"2024-01-10 03:38:37","title":"A Formula for the I/O Cost of Linear Repair Schemes and Application to Reed-Solomon Codes","abstract":"Node repair is a crucial problem in erasure-code-based distributed storage systems. An important metric for repair efficiency is the I/O cost which equals the total amount of data accessed at helper nodes to repair a failed node. In this work, a general formula for computing the I/O cost of linear repair schemes is derived from a new perspective, i.e., by investigating the Hamming weight of a related linear space. Applying the formula to Reed-Solomon (RS) codes, we obtain lower bounds on the I/O cost for full-length RS codes with two and three parities. Furthermore, we build linear repair schemes for the RS codes with improved I/O cost. For full-length RS codes with two parities, our scheme meets the lower bound on the I/O cost.","sentences":["Node repair is a crucial problem in erasure-code-based distributed storage systems.","An important metric for repair efficiency is the I/O cost which equals the total amount of data accessed at helper nodes to repair a failed node.","In this work, a general formula for computing the I/O cost of linear repair schemes is derived from a new perspective, i.e., by investigating the Hamming weight of a related linear space.","Applying the formula to Reed-Solomon (RS) codes, we obtain lower bounds on the I/O cost for full-length RS codes with two and three parities.","Furthermore, we build linear repair schemes for the RS codes with improved I/O cost.","For full-length RS codes with two parities, our scheme meets the lower bound on the I/O cost."],"url":"http://arxiv.org/abs/2401.04912v1"}
{"created":"2024-01-10 03:26:13","title":"On Achieving High-Fidelity Grant-free Non-Orthogonal Multiple Access","abstract":"Grant-free access (GFA) has been envisioned to play an active role in massive Machine Type Communication (mMTC) under 5G and Beyond mobile systems, which targets at achieving significant reduction of signaling overhead and access latency in the presence of sporadic traffic and small-size data. The paper focuses on a novel K-repetition GFA (K-GFA) scheme by incorporating Reed-Solomon (RS) code with the contention resolution diversity slotted ALOHA (CRDSA), aiming to achieve high-reliability and low-latency access in the presence of massive uncoordinated MTC devices (MTCDs). We firstly defines a MAC layer transmission structure at each MTCD for supporting message-level RS coding on a data message of $Q$ packets, where a RS code of $KQ$ packets is generated and sent in a super time frame (STF) that is composed of $Q$ time frames. The access point (AP) can recover the original $Q$ packets of the data message if at least $Q$ out of the $KQ$ packets of the RS code are successfully received. The AP buffers the received MTCD signals of each resource block (RB) within an STF and exercises the CRDSA based multi-user detection (MUD) by exploring signal-level inter-RB correlation via iterative interference cancellation (IIC). With the proposed CRDSA based K-GFA scheme, we provide the complexity analysis, and derive a closed-form analytical model on the access probability for each MTCD as well as its simplified approximate form. Extensive numerical experiments are conducted to validate its effectiveness on the proposed CRDSA based K-GFA scheme and gain deep understanding on its performance regarding various key operational parameters.","sentences":["Grant-free access (GFA) has been envisioned to play an active role in massive Machine Type Communication (mMTC) under 5G and Beyond mobile systems, which targets at achieving significant reduction of signaling overhead and access latency in the presence of sporadic traffic and small-size data.","The paper focuses on a novel K-repetition GFA (K-GFA) scheme by incorporating Reed-Solomon (RS) code with the contention resolution diversity slotted ALOHA (CRDSA), aiming to achieve high-reliability and low-latency access in the presence of massive uncoordinated MTC devices (MTCDs).","We firstly defines a MAC layer transmission structure at each MTCD for supporting message-level RS coding on a data message of $Q$ packets, where a RS code of $KQ$ packets is generated and sent in a super time frame (STF) that is composed of $Q$ time frames.","The access point (AP) can recover the original $Q$ packets of the data message if at least $Q$ out of the $KQ$ packets of the RS code are successfully received.","The AP buffers the received MTCD signals of each resource block (RB) within an STF and exercises the CRDSA based multi-user detection (MUD) by exploring signal-level inter-RB correlation via iterative interference cancellation (IIC).","With the proposed CRDSA based K-GFA scheme, we provide the complexity analysis, and derive a closed-form analytical model on the access probability for each MTCD as well as its simplified approximate form.","Extensive numerical experiments are conducted to validate its effectiveness on the proposed CRDSA based K-GFA scheme and gain deep understanding on its performance regarding various key operational parameters."],"url":"http://arxiv.org/abs/2401.04908v1"}
{"created":"2024-01-10 03:11:21","title":"SnapCap: Efficient Snapshot Compressive Video Captioning","abstract":"Video Captioning (VC) is a challenging multi-modal task since it requires describing the scene in language by understanding various and complex videos. For machines, the traditional VC follows the \"imaging-compression-decoding-and-then-captioning\" pipeline, where compression is pivot for storage and transmission. However, in such a pipeline, some potential shortcomings are inevitable, i.e., information redundancy resulting in low efficiency and information loss during the sampling process for captioning. To address these problems, in this paper, we propose a novel VC pipeline to generate captions directly from the compressed measurement, which can be captured by a snapshot compressive sensing camera and we dub our model SnapCap. To be more specific, benefiting from the signal simulation, we have access to obtain abundant measurement-video-annotation data pairs for our model. Besides, to better extract language-related visual representations from the compressed measurement, we propose to distill the knowledge from videos via a pre-trained CLIP with plentiful language-vision associations to guide the learning of our SnapCap. To demonstrate the effectiveness of SnapCap, we conduct experiments on two widely-used VC datasets. Both the qualitative and quantitative results verify the superiority of our pipeline over conventional VC pipelines. In particular, compared to the \"caption-after-reconstruction\" methods, our SnapCap can run at least 3$\\times$ faster, and achieve better caption results.","sentences":["Video Captioning (VC) is a challenging multi-modal task since it requires describing the scene in language by understanding various and complex videos.","For machines, the traditional VC follows the \"imaging-compression-decoding-and-then-captioning\" pipeline, where compression is pivot for storage and transmission.","However, in such a pipeline, some potential shortcomings are inevitable, i.e., information redundancy resulting in low efficiency and information loss during the sampling process for captioning.","To address these problems, in this paper, we propose a novel VC pipeline to generate captions directly from the compressed measurement, which can be captured by a snapshot compressive sensing camera and we dub our model SnapCap.","To be more specific, benefiting from the signal simulation, we have access to obtain abundant measurement-video-annotation data pairs for our model.","Besides, to better extract language-related visual representations from the compressed measurement, we propose to distill the knowledge from videos via a pre-trained CLIP with plentiful language-vision associations to guide the learning of our SnapCap.","To demonstrate the effectiveness of SnapCap, we conduct experiments on two widely-used VC datasets.","Both the qualitative and quantitative results verify the superiority of our pipeline over conventional VC pipelines.","In particular, compared to the \"caption-after-reconstruction\" methods, our SnapCap can run at least 3$\\times$ faster, and achieve better caption results."],"url":"http://arxiv.org/abs/2401.04903v1"}
{"created":"2024-01-10 02:59:49","title":"ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain","abstract":"Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis. Addressing these concerns, this paper introduces ANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes \\textit{Keypoint} categorization standard for the first time, each question in ANGO can correspond to multiple keypoints, effectively enhancing interpretability of evaluation results. Base on performance of real humans, we build a quantifiable question difficulty standard and divide ANGO questions into 9 difficulty levels, which provide more precise guidance for model training. To minimize data leakage impact and fully leverage ANGO's innovative features, we have engineered exclusive sampling strategies and a new evaluation framework that support swift testset iteration. Our experiments demonstrate that ANGO poses a stronger challenge to models and reveals more details in evaluation result compared to existing benchmarks.","sentences":["Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis.","Addressing these concerns, this paper introduces ANGO, a Chinese multi-choice question evaluation benchmark.","ANGO proposes \\textit{Keypoint} categorization standard for the first time, each question in ANGO can correspond to multiple keypoints, effectively enhancing interpretability of evaluation results.","Base on performance of real humans, we build a quantifiable question difficulty standard and divide ANGO questions into 9 difficulty levels, which provide more precise guidance for model training.","To minimize data leakage impact and fully leverage ANGO's innovative features, we have engineered exclusive sampling strategies and a new evaluation framework that support swift testset iteration.","Our experiments demonstrate that ANGO poses a stronger challenge to models and reveals more details in evaluation result compared to existing benchmarks."],"url":"http://arxiv.org/abs/2401.04898v1"}
{"created":"2024-01-10 00:39:03","title":"Modality-Aware Representation Learning for Zero-shot Sketch-based Image Retrieval","abstract":"Zero-shot learning offers an efficient solution for a machine learning model to treat unseen categories, avoiding exhaustive data collection. Zero-shot Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it is hard and costly to collect paired sketch-photo samples. We propose a novel framework that indirectly aligns sketches and photos by contrasting them through texts, removing the necessity of access to sketch-photo pairs. With an explicit modality encoding learned from data, our approach disentangles modality-agnostic semantics from modality-specific information, bridging the modality gap and enabling effective cross-modal content retrieval within a joint latent space. From comprehensive experiments, we verify the efficacy of the proposed model on ZS-SBIR, and it can be also applied to generalized and fine-grained settings.","sentences":["Zero-shot learning offers an efficient solution for a machine learning model to treat unseen categories, avoiding exhaustive data collection.","Zero-shot Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it is hard and costly to collect paired sketch-photo samples.","We propose a novel framework that indirectly aligns sketches and photos by contrasting them through texts, removing the necessity of access to sketch-photo pairs.","With an explicit modality encoding learned from data, our approach disentangles modality-agnostic semantics from modality-specific information, bridging the modality gap and enabling effective cross-modal content retrieval within a joint latent space.","From comprehensive experiments, we verify the efficacy of the proposed model on ZS-SBIR, and it can be also applied to generalized and fine-grained settings."],"url":"http://arxiv.org/abs/2401.04860v1"}
{"created":"2024-01-10 00:25:57","title":"Transportation Market Rate Forecast Using Signature Transform","abstract":"Currently, Amazon relies on third parties for transportation marketplace rate forecasts, despite the poor quality and lack of interpretability of these forecasts. While transportation marketplace rates are typically very challenging to forecast accurately, we have developed a novel signature-based statistical technique to address these challenges and built a predictive and adaptive model to forecast marketplace rates. This novel technique is based on two key properties of the signature transform. The first is its universal nonlinearity which linearizes the feature space and hence translates the forecasting problem into a linear regression analysis; the second is the signature kernel which allows for comparing computationally efficiently similarities between time series data. Combined, these properties allow for efficient feature generation and more precise identification of seasonality and regime switching in the forecasting process. Preliminary result by the model shows that this new technique leads to far superior forecast accuracy versus commercially available industry models with better interpretability, even during the period of Covid-19 and with the sudden onset of the Ukraine war.","sentences":["Currently, Amazon relies on third parties for transportation marketplace rate forecasts, despite the poor quality and lack of interpretability of these forecasts.","While transportation marketplace rates are typically very challenging to forecast accurately, we have developed a novel signature-based statistical technique to address these challenges and built a predictive and adaptive model to forecast marketplace rates.","This novel technique is based on two key properties of the signature transform.","The first is its universal nonlinearity which linearizes the feature space and hence translates the forecasting problem into a linear regression analysis; the second is the signature kernel which allows for comparing computationally efficiently similarities between time series data.","Combined, these properties allow for efficient feature generation and more precise identification of seasonality and regime switching in the forecasting process.","Preliminary result by the model shows that this new technique leads to far superior forecast accuracy versus commercially available industry models with better interpretability, even during the period of Covid-19 and with the sudden onset of the Ukraine war."],"url":"http://arxiv.org/abs/2401.04857v1"}
{"created":"2024-01-10 00:17:36","title":"A Good Score Does not Lead to A Good Generative Model","abstract":"Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. The method enjoys empirical success and is supported by rigorous theoretical convergence properties. In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model. We provide a counter-example in this paper. Through the sample complexity argument, we provide one specific setting where the score function is learned well. Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation. The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate.","sentences":["Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions.","The method enjoys empirical success and is supported by rigorous theoretical convergence properties.","In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model.","We provide a counter-example in this paper.","Through the sample complexity argument, we provide one specific setting where the score function is learned well.","Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation.","The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate."],"url":"http://arxiv.org/abs/2401.04856v1"}
{"created":"2024-01-09 23:52:32","title":"Entity Recognition from Colloquial Text","abstract":"Extraction of concepts and entities of interest from non-formal texts such as social media posts and informal communication is an important capability for decision support systems in many domains, including healthcare, customer relationship management, and others. Despite the recent advances in training large language models for a variety of natural language processing tasks, the developed models and techniques have mainly focused on formal texts and do not perform as well on colloquial data, which is characterized by a number of distinct challenges. In our research, we focus on the healthcare domain and investigate the problem of symptom recognition from colloquial texts by designing and evaluating several training strategies for BERT-based model fine-tuning. These strategies are distinguished by the choice of the base model, the training corpora, and application of term perturbations in the training data. The best-performing models trained using these strategies outperform the state-of-the-art specialized symptom recognizer by a large margin. Through a series of experiments, we have found specific patterns of model behavior associated with the training strategies we designed. We present design principles for training strategies for effective entity recognition in colloquial texts based on our findings.","sentences":["Extraction of concepts and entities of interest from non-formal texts such as social media posts and informal communication is an important capability for decision support systems in many domains, including healthcare, customer relationship management, and others.","Despite the recent advances in training large language models for a variety of natural language processing tasks, the developed models and techniques have mainly focused on formal texts and do not perform as well on colloquial data, which is characterized by a number of distinct challenges.","In our research, we focus on the healthcare domain and investigate the problem of symptom recognition from colloquial texts by designing and evaluating several training strategies for BERT-based model fine-tuning.","These strategies are distinguished by the choice of the base model, the training corpora, and application of term perturbations in the training data.","The best-performing models trained using these strategies outperform the state-of-the-art specialized symptom recognizer by a large margin.","Through a series of experiments, we have found specific patterns of model behavior associated with the training strategies we designed.","We present design principles for training strategies for effective entity recognition in colloquial texts based on our findings."],"url":"http://arxiv.org/abs/2401.04853v1"}
{"created":"2024-01-09 23:48:54","title":"Answer Retrieval in Legal Community Question Answering","abstract":"The task of answer retrieval in the legal domain aims to help users to seek relevant legal advice from massive amounts of professional responses. Two main challenges hinder applying existing answer retrieval approaches in other domains to the legal domain: (1) a huge knowledge gap between lawyers and non-professionals; and (2) a mix of informal and formal content on legal QA websites. To tackle these challenges, we propose CE_FS, a novel cross-encoder (CE) re-ranker based on the fine-grained structured inputs. CE_FS uses additional structured information in the CQA data to improve the effectiveness of cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-world benchmark dataset for evaluating answer retrieval in the legal domain. Experiments conducted on LegalQA show that our proposed method significantly outperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novel finding is that adding the question tags of each question besides the question description and title into the input of cross-encoder re-rankers structurally boosts the rankers' effectiveness. While we study our proposed method in the legal domain, we believe that our method can be applied in similar applications in other domains.","sentences":["The task of answer retrieval in the legal domain aims to help users to seek relevant legal advice from massive amounts of professional responses.","Two main challenges hinder applying existing answer retrieval approaches in other domains to the legal domain: (1) a huge knowledge gap between lawyers and non-professionals; and (2) a mix of informal and formal content on legal QA websites.","To tackle these challenges, we propose CE_FS, a novel cross-encoder (CE) re-ranker based on the fine-grained structured inputs.","CE_FS uses additional structured information in the CQA data to improve the effectiveness of cross-encoder re-rankers.","Furthermore, we propose LegalQA: a real-world benchmark dataset for evaluating answer retrieval in the legal domain.","Experiments conducted on LegalQA show that our proposed method significantly outperforms strong cross-encoder re-rankers fine-tuned on MS MARCO.","Our novel finding is that adding the question tags of each question besides the question description and title into the input of cross-encoder re-rankers structurally boosts the rankers' effectiveness.","While we study our proposed method in the legal domain, we believe that our method can be applied in similar applications in other domains."],"url":"http://arxiv.org/abs/2401.04852v1"}
{"created":"2024-01-09 23:45:00","title":"FairQ: Fair and Fast Rate Allocation in Data Centers","abstract":"The peculiar congestion patterns in data centers are caused by the bursty and composite nature of traffic, the small bandwidth-delay product, and the tiny switch buffers. It is not practical to modify TCP to adapt to data centers, especially in public clouds where multiple congestion control protocols coexist. In this work, we design a switch-based method to address such congestion issues; our approach does not require any modification to TCP, which enables easy and seamless deployment in public data centers via switch software update. We first present a simple analysis to demonstrate the stability and effectiveness of the scheme, and then we discuss a hardware NetFPGA switch-based prototype. The experimental results from real deployments in a small testbed cluster show the effectiveness of our approach.","sentences":["The peculiar congestion patterns in data centers are caused by the bursty and composite nature of traffic, the small bandwidth-delay product, and the tiny switch buffers.","It is not practical to modify TCP to adapt to data centers, especially in public clouds where multiple congestion control protocols coexist.","In this work, we design a switch-based method to address such congestion issues; our approach does not require any modification to TCP, which enables easy and seamless deployment in public data centers via switch software update.","We first present a simple analysis to demonstrate the stability and effectiveness of the scheme, and then we discuss a hardware NetFPGA switch-based prototype.","The experimental results from real deployments in a small testbed cluster show the effectiveness of our approach."],"url":"http://arxiv.org/abs/2401.04850v1"}
{"created":"2024-01-09 22:00:15","title":"CoNST: Code Generator for Sparse Tensor Networks","abstract":"Sparse tensor networks are commonly used to represent contractions over sparse tensors. Tensor contractions are higher-order analogs of matrix multiplication. Tensor networks arise commonly in many domains of scientific computing and data science. After a transformation into a tree of binary contractions, the network is implemented as a sequence of individual contractions. Several critical aspects must be considered in the generation of efficient code for a contraction tree, including sparse tensor layout mode order, loop fusion to reduce intermediate tensors, and the interdependence of loop order, mode order, and contraction order. We propose CoNST, a novel approach that considers these factors in an integrated manner using a single formulation. Our approach creates a constraint system that encodes these decisions and their interdependence, while aiming to produce reduced-order intermediate tensors via fusion. The constraint system is solved by the Z3 SMT solver and the result is used to create the desired fused loop structure and tensor mode layouts for the entire contraction tree. This structure is lowered to the IR of the TACO compiler, which is then used to generate executable code. Our experimental evaluation demonstrates very significant (sometimes orders of magnitude) performance improvements over current state-of-the-art sparse tensor compiler/library alternatives.","sentences":["Sparse tensor networks are commonly used to represent contractions over sparse tensors.","Tensor contractions are higher-order analogs of matrix multiplication.","Tensor networks arise commonly in many domains of scientific computing and data science.","After a transformation into a tree of binary contractions, the network is implemented as a sequence of individual contractions.","Several critical aspects must be considered in the generation of efficient code for a contraction tree, including sparse tensor layout mode order, loop fusion to reduce intermediate tensors, and the interdependence of loop order, mode order, and contraction order.","We propose CoNST, a novel approach that considers these factors in an integrated manner using a single formulation.","Our approach creates a constraint system that encodes these decisions and their interdependence, while aiming to produce reduced-order intermediate tensors via fusion.","The constraint system is solved by the Z3 SMT solver and the result is used to create the desired fused loop structure and tensor mode layouts for the entire contraction tree.","This structure is lowered to the IR of the TACO compiler, which is then used to generate executable code.","Our experimental evaluation demonstrates very significant (sometimes orders of magnitude) performance improvements over current state-of-the-art sparse tensor compiler/library alternatives."],"url":"http://arxiv.org/abs/2401.04836v1"}
{"created":"2024-01-09 21:26:00","title":"The site linkage spectrum of data arrays","abstract":"A new perspective is introduced regarding the analysis of Multiple Sequence Alignments (MSA), representing aligned data defined over a finite alphabet of symbols. The framework is designed to produce a block decomposition of an MSA, where each block is comprised of sequences exhibiting a certain site-coherence. The key component of this framework is an information theoretical potential defined on pairs of sites (links) within the MSA. This potential quantifies the expected drop in variation of information between the two constituent sites, where the expectation is taken with respect to all possible sub-alignments, obtained by removing a finite, fixed collection of rows. It is proved that the potential is zero for linked sites representing columns, whose symbols are in bijective correspondence and it is strictly positive, otherwise. It is furthermore shown that the potential assumes its unique minimum for links at which each symbol pair appears with the same multiplicity. Finally, an application is presented regarding anomaly detection in an MSA, composed of inverse fold solutions of a fixed tRNA secondary structure, where the anomalies are represented by inverse fold solutions of a different RNA structure.","sentences":["A new perspective is introduced regarding the analysis of Multiple Sequence Alignments (MSA), representing aligned data defined over a finite alphabet of symbols.","The framework is designed to produce a block decomposition of an MSA, where each block is comprised of sequences exhibiting a certain site-coherence.","The key component of this framework is an information theoretical potential defined on pairs of sites (links) within the MSA.","This potential quantifies the expected drop in variation of information between the two constituent sites, where the expectation is taken with respect to all possible sub-alignments, obtained by removing a finite, fixed collection of rows.","It is proved that the potential is zero for linked sites representing columns, whose symbols are in bijective correspondence and it is strictly positive, otherwise.","It is furthermore shown that the potential assumes its unique minimum for links at which each symbol pair appears with the same multiplicity.","Finally, an application is presented regarding anomaly detection in an MSA, composed of inverse fold solutions of a fixed tRNA secondary structure, where the anomalies are represented by inverse fold solutions of a different RNA structure."],"url":"http://arxiv.org/abs/2401.04827v1"}
{"created":"2024-01-09 21:09:07","title":"MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer","abstract":"Transformer-based pre-trained language models (PLMs) have achieved remarkable performance in various natural language processing (NLP) tasks. However, pre-training such models can take considerable resources that are almost only available to high-resource languages. On the contrary, static word embeddings are easier to train in terms of computing resources and the amount of data required. In this paper, we introduce MoSECroT Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task that is especially relevant to low-resource languages for which static word embeddings are available. To tackle the task, we present the first framework that leverages relative representations to construct a common space for the embeddings of a source language PLM and the static word embeddings of a target language. In this way, we can train the PLM on source-language training data and perform zero-shot transfer to the target language by simply swapping the embedding layer. However, through extensive experiments on two classification datasets, we show that although our proposed framework is competitive with weak baselines when addressing MoSECroT, it fails to achieve competitive results compared with some strong baselines. In this paper, we attempt to explain this negative result and provide several thoughts on possible improvement.","sentences":["Transformer-based pre-trained language models (PLMs) have achieved remarkable performance in various natural language processing (NLP) tasks.","However, pre-training such models can take considerable resources that are almost only available to high-resource languages.","On the contrary, static word embeddings are easier to train in terms of computing resources and the amount of data required.","In this paper, we introduce MoSECroT Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task that is especially relevant to low-resource languages for which static word embeddings are available.","To tackle the task, we present the first framework that leverages relative representations to construct a common space for the embeddings of a source language PLM and the static word embeddings of a target language.","In this way, we can train the PLM on source-language training data and perform zero-shot transfer to the target language by simply swapping the embedding layer.","However, through extensive experiments on two classification datasets, we show that although our proposed framework is competitive with weak baselines when addressing MoSECroT, it fails to achieve competitive results compared with some strong baselines.","In this paper, we attempt to explain this negative result and provide several thoughts on possible improvement."],"url":"http://arxiv.org/abs/2401.04821v1"}
{"created":"2024-01-09 21:08:13","title":"Phishing Website Detection through Multi-Model Analysis of HTML Content","abstract":"The way we communicate and work has changed significantly with the rise of the Internet. While it has opened up new opportunities, it has also brought about an increase in cyber threats. One common and serious threat is phishing, where cybercriminals employ deceptive methods to steal sensitive information.This study addresses the pressing issue of phishing by introducing an advanced detection model that meticulously focuses on HTML content. Our proposed approach integrates a specialized Multi-Layer Perceptron (MLP) model for structured tabular data and two pretrained Natural Language Processing (NLP) models for analyzing textual features such as page titles and content. The embeddings from these models are harmoniously combined through a novel fusion process. The resulting fused embeddings are then input into a linear classifier. Recognizing the scarcity of recent datasets for comprehensive phishing research, our contribution extends to the creation of an up-to-date dataset, which we openly share with the community. The dataset is meticulously curated to reflect real-life phishing conditions, ensuring relevance and applicability. The research findings highlight the effectiveness of the proposed approach, with the CANINE demonstrating superior performance in analyzing page titles and the RoBERTa excelling in evaluating page content. The fusion of two NLP and one MLP model,termed MultiText-LP, achieves impressive results, yielding a 96.80 F1 score and a 97.18 accuracy score on our research dataset. Furthermore, our approach outperforms existing methods on the CatchPhish HTML dataset, showcasing its efficacies.","sentences":["The way we communicate and work has changed significantly with the rise of the Internet.","While it has opened up new opportunities, it has also brought about an increase in cyber threats.","One common and serious threat is phishing, where cybercriminals employ deceptive methods to steal sensitive information.","This study addresses the pressing issue of phishing by introducing an advanced detection model that meticulously focuses on HTML content.","Our proposed approach integrates a specialized Multi-Layer Perceptron (MLP) model for structured tabular data and two pretrained Natural Language Processing (NLP) models for analyzing textual features such as page titles and content.","The embeddings from these models are harmoniously combined through a novel fusion process.","The resulting fused embeddings are then input into a linear classifier.","Recognizing the scarcity of recent datasets for comprehensive phishing research, our contribution extends to the creation of an up-to-date dataset, which we openly share with the community.","The dataset is meticulously curated to reflect real-life phishing conditions, ensuring relevance and applicability.","The research findings highlight the effectiveness of the proposed approach, with the CANINE demonstrating superior performance in analyzing page titles and the RoBERTa excelling in evaluating page content.","The fusion of two NLP and one MLP model,termed MultiText-LP, achieves impressive results, yielding a 96.80 F1 score and a 97.18 accuracy score on our research dataset.","Furthermore, our approach outperforms existing methods on the CatchPhish HTML dataset, showcasing its efficacies."],"url":"http://arxiv.org/abs/2401.04820v1"}
{"created":"2024-01-09 19:38:59","title":"First 100 days of pandemic; an interplay of pharmaceutical, behavioral and digital interventions -- A study using agent based modeling","abstract":"Pandemics, notably the recent COVID-19 outbreak, have impacted both public health and the global economy. A profound understanding of disease progression and efficient response strategies is thus needed to prepare for potential future outbreaks. In this paper, we emphasize the potential of Agent-Based Models (ABM) in capturing complex infection dynamics and understanding the impact of interventions. We simulate realistic pharmaceutical, behavioral, and digital interventions that mirror challenges in real-world policy adoption and suggest a holistic combination of these interventions for pandemic response. Using these simulations, we study the trends of emergent behavior on a large-scale population based on real-world socio-demographic and geo-census data from Kings County in Washington. Our analysis reveals the pivotal role of the initial 100 days in dictating a pandemic's course, emphasizing the importance of quick decision-making and efficient policy development. Further, we highlight that investing in behavioral and digital interventions can reduce the burden on pharmaceutical interventions by reducing the total number of infections and hospitalizations, and by delaying the pandemic's peak. We also infer that allocating the same amount of dollars towards extensive testing with contact tracing and self-quarantine offers greater cost efficiency compared to spending the entire budget on vaccinations.","sentences":["Pandemics, notably the recent COVID-19 outbreak, have impacted both public health and the global economy.","A profound understanding of disease progression and efficient response strategies is thus needed to prepare for potential future outbreaks.","In this paper, we emphasize the potential of Agent-Based Models (ABM) in capturing complex infection dynamics and understanding the impact of interventions.","We simulate realistic pharmaceutical, behavioral, and digital interventions that mirror challenges in real-world policy adoption and suggest a holistic combination of these interventions for pandemic response.","Using these simulations, we study the trends of emergent behavior on a large-scale population based on real-world socio-demographic and geo-census data from Kings County in Washington.","Our analysis reveals the pivotal role of the initial 100 days in dictating a pandemic's course, emphasizing the importance of quick decision-making and efficient policy development.","Further, we highlight that investing in behavioral and digital interventions can reduce the burden on pharmaceutical interventions by reducing the total number of infections and hospitalizations, and by delaying the pandemic's peak.","We also infer that allocating the same amount of dollars towards extensive testing with contact tracing and self-quarantine offers greater cost efficiency compared to spending the entire budget on vaccinations."],"url":"http://arxiv.org/abs/2401.04795v1"}
{"created":"2024-01-09 19:01:32","title":"Network Layout Algorithm with Covariate Smoothing","abstract":"Network science explores intricate connections among objects, employed in diverse domains like social interactions, fraud detection, and disease spread. Visualization of networks facilitates conceptualizing research questions and forming scientific hypotheses. Networks, as mathematical high-dimensional objects, require dimensionality reduction for (planar) visualization. Visualizing empirical networks present additional challenges. They often contain false positive (spurious) and false negative (missing) edges. Traditional visualization methods don't account for errors in observation, potentially biasing interpretations. Moreover, contemporary network data includes rich nodal attributes. However, traditional methods neglect these attributes when computing node locations. Our visualization approach aims to leverage nodal attribute richness to compensate for network data limitations. We employ a statistical model estimating the probability of edge connections between nodes based on their covariates. We enhance the Fruchterman-Reingold algorithm to incorporate estimated dyad connection probabilities, allowing practitioners to balance reliance on observed versus estimated edges. We explore optimal smoothing levels, offering a natural way to include relevant nodal information in layouts. Results demonstrate the effectiveness of our method in achieving robust network visualization, providing insights for improved analysis.","sentences":["Network science explores intricate connections among objects, employed in diverse domains like social interactions, fraud detection, and disease spread.","Visualization of networks facilitates conceptualizing research questions and forming scientific hypotheses.","Networks, as mathematical high-dimensional objects, require dimensionality reduction for (planar) visualization.","Visualizing empirical networks present additional challenges.","They often contain false positive (spurious) and false negative (missing) edges.","Traditional visualization methods don't account for errors in observation, potentially biasing interpretations.","Moreover, contemporary network data includes rich nodal attributes.","However, traditional methods neglect these attributes when computing node locations.","Our visualization approach aims to leverage nodal attribute richness to compensate for network data limitations.","We employ a statistical model estimating the probability of edge connections between nodes based on their covariates.","We enhance the Fruchterman-Reingold algorithm to incorporate estimated dyad connection probabilities, allowing practitioners to balance reliance on observed versus estimated edges.","We explore optimal smoothing levels, offering a natural way to include relevant nodal information in layouts.","Results demonstrate the effectiveness of our method in achieving robust network visualization, providing insights for improved analysis."],"url":"http://arxiv.org/abs/2401.04771v1"}
{"created":"2024-01-09 18:58:40","title":"Revisiting Adversarial Training at Scale","abstract":"The machine learning community has witnessed a drastic change in the training pipeline, pivoted by those ''foundation models'' with unprecedented scales. However, the field of adversarial training is lagging behind, predominantly centered around small model sizes like ResNet-50, and tiny and low-resolution datasets like CIFAR-10. To bridge this transformation gap, this paper provides a modern re-examination with adversarial training, investigating its potential benefits when applied at scale. Additionally, we introduce an efficient and effective training strategy to enable adversarial training with giant models and web-scale data at an affordable computing cost. We denote this newly introduced framework as AdvXL.   Empirical results demonstrate that AdvXL establishes new state-of-the-art robust accuracy records under AutoAttack on ImageNet-1K. For example, by training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and $l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively. This achievement posits AdvXL as a pioneering approach, charting a new trajectory for the efficient training of robust visual representations at significantly larger scales. Our code is available at https://github.com/UCSC-VLAA/AdvXL.","sentences":["The machine learning community has witnessed a drastic change in the training pipeline, pivoted by those ''foundation models'' with unprecedented scales.","However, the field of adversarial training is lagging behind, predominantly centered around small model sizes like ResNet-50, and tiny and low-resolution datasets like CIFAR-10.","To bridge this transformation gap, this paper provides a modern re-examination with adversarial training, investigating its potential benefits when applied at scale.","Additionally, we introduce an efficient and effective training strategy to enable adversarial training with giant models and web-scale data at an affordable computing cost.","We denote this newly introduced framework as AdvXL.   ","Empirical results demonstrate that AdvXL establishes new state-of-the-art robust accuracy records under AutoAttack on ImageNet-1K. For example, by training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and $l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively.","This achievement posits AdvXL as a pioneering approach, charting a new trajectory for the efficient training of robust visual representations at significantly larger scales.","Our code is available at https://github.com/UCSC-VLAA/AdvXL."],"url":"http://arxiv.org/abs/2401.04727v1"}
{"created":"2024-01-09 18:46:59","title":"Low-resource finetuning of foundation models beats state-of-the-art in histopathology","abstract":"To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models.","sentences":["To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning.","The performance of this workflow strongly depends on the quality of the extracted features.","Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks.","In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data.","We evaluate the models in two settings: slide-level classification and patch-level classification.","We show that foundation models are a strong baseline.","Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology.","These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset.","This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor.","We publish all code used for training and evaluation as well as the finetuned models."],"url":"http://arxiv.org/abs/2401.04720v1"}
{"created":"2024-01-09 18:40:52","title":"Low-Resource Vision Challenges for Foundation Models","abstract":"Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for machine learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we strive to address this gap and explore the challenges of low-resource image tasks with vision foundation models. Thus, we first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share the three challenges of data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we propose to i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on the three low-resource data sources in our benchmark demonstrate our proposals already provide a better baseline than common transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project website: https://xiaobai1217.github.io/Low-Resource-Vision/.","sentences":["Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for machine learning at scale.","However, low-resource problems are under-explored in computer vision.","In this paper, we strive to address this gap and explore the challenges of low-resource image tasks with vision foundation models.","Thus, we first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings.","These low-resource settings all share the three challenges of data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest.","While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks.","To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge.","Specifically, we propose to i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains.","Experiments on the three low-resource data sources in our benchmark demonstrate our proposals already provide a better baseline than common transfer learning, data augmentation, and fine-grained methods.","This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation.","Project website: https://xiaobai1217.github.io/Low-Resource-Vision/."],"url":"http://arxiv.org/abs/2401.04716v2"}
{"created":"2024-01-09 18:39:42","title":"Bin Packing under Random-Order: Breaking the Barrier of 3/2","abstract":"Best-Fit is one of the most prominent and practically used algorithms for the bin packing problem, where a set of items with associated sizes needs to be packed in the minimum number of unit-capacity bins. Kenyon [SODA '96] studied online bin packing under random-order arrival, where the adversary chooses the list of items, but the items arrive one by one according to an arrival order drawn uniformly randomly from the set of all permutations of the items. Kenyon's seminal result established an upper bound of $1.5$ and a lower bound of $1.08$ on the random-order ratio of Best-Fit, and it was conjectured that the true ratio is $\\approx 1.15$. The conjecture, if true, will also imply that Best-Fit (on randomly permuted input) has the best performance guarantee among all the widely-used simple algorithms for (offline) bin packing. This conjecture has remained one of the major open problems in the area, as highlighted in the recent survey on random-order models by Gupta and Singla [Beyond the Worst-Case Analysis of Algorithms '20]. Recently, Albers et al. [Algorithmica '21] improved the upper bound to $1.25$ for the special case when all the item sizes are greater than $1/3$, and they improve the lower bound to $1.1$. Ayyadevara et al. [ICALP '22] obtained an improved result for the special case when all the item sizes lie in $(1/4, 1/2]$, which corresponds to the $3$-partition problem. The upper bound of $3/2$ for the general case, however, has remained unimproved.   In this paper, we make the first progress towards the conjecture, by showing that Best-Fit achieves a random-order ratio of at most $1.5 - \\varepsilon$, for a small constant $\\varepsilon>0$. Furthermore, we establish an improved lower bound of $1.144$ on the random-order ratio of Best-Fit, nearly reaching the conjectured ratio.","sentences":["Best-Fit is one of the most prominent and practically used algorithms for the bin packing problem, where a set of items with associated sizes needs to be packed in the minimum number of unit-capacity bins.","Kenyon [SODA '96] studied online bin packing under random-order arrival, where the adversary chooses the list of items, but the items arrive one by one according to an arrival order drawn uniformly randomly from the set of all permutations of the items.","Kenyon's seminal result established an upper bound of $1.5$ and a lower bound of $1.08$ on the random-order ratio of Best-Fit, and it was conjectured that the true ratio is $\\approx 1.15$.","The conjecture, if true, will also imply that Best-Fit (on randomly permuted input) has the best performance guarantee among all the widely-used simple algorithms for (offline) bin packing.","This conjecture has remained one of the major open problems in the area, as highlighted in the recent survey on random-order models by Gupta and Singla","[Beyond the Worst-Case Analysis of Algorithms '20].","Recently, Albers et al.","[Algorithmica '21] improved the upper bound to $1.25$ for the special case when all the item sizes are greater than $1/3$, and they improve the lower bound to $1.1$. Ayyadevara et al.","[ICALP '22] obtained an improved result for the special case when all the item sizes lie in $(1/4, 1/2]$, which corresponds to the $3$-partition problem.","The upper bound of $3/2$ for the general case, however, has remained unimproved.   ","In this paper, we make the first progress towards the conjecture, by showing that Best-Fit achieves a random-order ratio of at most $1.5 - \\varepsilon$, for a small constant $\\varepsilon>0$. Furthermore, we establish an improved lower bound of $1.144$ on the random-order ratio of Best-Fit, nearly reaching the conjectured ratio."],"url":"http://arxiv.org/abs/2401.04714v1"}
{"created":"2024-01-09 18:04:18","title":"HiRace: Accurate and Fast Source-Level Race Checking of GPU Programs","abstract":"Data races are egregious parallel programming bugs on CPUs. They are even worse on GPUs due to the hierarchical thread and memory structure, which makes it possible to write code that is correctly synchronized within a thread group while not being correct across groups. Thus far, all major data-race checkers for GPUs suffer from at least one of the following problems: they do not check races in global memory, do not work on recent GPUs, scale poorly, have not been extensively tested, miss simple data races, or are not dependable without detailed knowledge of the compiler.   Our new data-race detection tool, HiRace, overcomes these limitations. Its key novelty is an innovative parallel finite-state machine that condenses an arbitrarily long access history into a constant-length state, thus allowing it to handle large and long-running programs. HiRace is a dynamic tool that checks for thread-group shared memory and global device memory races. It utilizes source-code instrumentation, thus avoiding driver, compiler, and hardware dependencies. We evaluate it on a modern calibrated data-race benchmark suite. On the 580 tested CUDA kernels, 346 of which contain data races, HiRace finds races missed by other tools without false alarms and is more than 10 times faster on average than the current state of the art, while incurring only half the memory overhead.","sentences":["Data races are egregious parallel programming bugs on CPUs.","They are even worse on GPUs due to the hierarchical thread and memory structure, which makes it possible to write code that is correctly synchronized within a thread group while not being correct across groups.","Thus far, all major data-race checkers for GPUs suffer from at least one of the following problems: they do not check races in global memory, do not work on recent GPUs, scale poorly, have not been extensively tested, miss simple data races, or are not dependable without detailed knowledge of the compiler.   ","Our new data-race detection tool, HiRace, overcomes these limitations.","Its key novelty is an innovative parallel finite-state machine that condenses an arbitrarily long access history into a constant-length state, thus allowing it to handle large and long-running programs.","HiRace is a dynamic tool that checks for thread-group shared memory and global device memory races.","It utilizes source-code instrumentation, thus avoiding driver, compiler, and hardware dependencies.","We evaluate it on a modern calibrated data-race benchmark suite.","On the 580 tested CUDA kernels, 346 of which contain data races, HiRace finds races missed by other tools without false alarms and is more than 10 times faster on average than the current state of the art, while incurring only half the memory overhead."],"url":"http://arxiv.org/abs/2401.04701v1"}
{"created":"2024-01-09 17:39:45","title":"Comparative Evaluation of Animated Scatter Plot Transitions","abstract":"Scatter plots are popular for displaying 2D data, but in practice, many data sets have more than two dimensions. For the analysis of such multivariate data, it is often necessary to switch between scatter plots of different dimension pairs, e.g., in a scatter plot matrix (SPLOM). Alternative approaches include a \"grand tour\" for an overview of the entire data set or creating artificial axes from dimensionality reduction (DR). A cross-cutting concern in all techniques is the ability of viewers to find correspondence between data points in different views. Previous work proposed animations to preserve the mental map between view changes and to trace points as well as clusters between scatter plots of the same underlying data set. In this paper, we evaluate a variety of spline- and rotation-based view transitions in a crowdsourced user study focusing on ecological validity. Using the study results, we assess each animation's suitability for tracing points and clusters across view changes. We evaluate whether the order of horizontal and vertical rotation is relevant for task accuracy. The results show that rotations with an orthographic camera or staged expansion of a depth axis significantly outperform all other animation techniques for the traceability of individual points. Further, we provide a ranking of the animated transition techniques for traceability of individual points. However, we could not find any significant differences for the traceability of clusters. Furthermore, we identified differences by animation direction that could guide further studies to determine potential confounds for these differences. We publish the study data for reuse and provide the animation framework as a D3.js plug-in.","sentences":["Scatter plots are popular for displaying 2D data, but in practice, many data sets have more than two dimensions.","For the analysis of such multivariate data, it is often necessary to switch between scatter plots of different dimension pairs, e.g., in a scatter plot matrix (SPLOM).","Alternative approaches include a \"grand tour\" for an overview of the entire data set or creating artificial axes from dimensionality reduction (DR).","A cross-cutting concern in all techniques is the ability of viewers to find correspondence between data points in different views.","Previous work proposed animations to preserve the mental map between view changes and to trace points as well as clusters between scatter plots of the same underlying data set.","In this paper, we evaluate a variety of spline- and rotation-based view transitions in a crowdsourced user study focusing on ecological validity.","Using the study results, we assess each animation's suitability for tracing points and clusters across view changes.","We evaluate whether the order of horizontal and vertical rotation is relevant for task accuracy.","The results show that rotations with an orthographic camera or staged expansion of a depth axis significantly outperform all other animation techniques for the traceability of individual points.","Further, we provide a ranking of the animated transition techniques for traceability of individual points.","However, we could not find any significant differences for the traceability of clusters.","Furthermore, we identified differences by animation direction that could guide further studies to determine potential confounds for these differences.","We publish the study data for reuse and provide the animation framework as a D3.js plug-in."],"url":"http://arxiv.org/abs/2401.04692v1"}
{"created":"2024-01-09 17:38:19","title":"AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale","abstract":"Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution. We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island. Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. The highest level of threat is found at Madagascar and the neighbouring islands. In Sumatra, we found good correspondence of protected areas with our indicators, but supplementing current IUCN assessments with status predictions results in alarming levels of species threat across the island. Recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale. As an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels.","sentences":["Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk.","We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales.","We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution.","We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage.","We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island.","Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales.","The highest level of threat is found at Madagascar and the neighbouring islands.","In Sumatra, we found good correspondence of protected areas with our indicators, but supplementing current IUCN assessments with status predictions results in alarming levels of species threat across the island.","Recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale.","As an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels."],"url":"http://arxiv.org/abs/2401.04691v1"}
{"created":"2024-01-09 17:15:47","title":"Mixture of multilayer stochastic block models for multiview clustering","abstract":"In this work, we propose an original method for aggregating multiple clustering coming from different sources of information. Each partition is encoded by a co-membership matrix between observations. Our approach uses a mixture of multilayer Stochastic Block Models (SBM) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components. The identifiability of the model parameters is established and a variational Bayesian EM algorithm is proposed for the estimation of these parameters. The Bayesian framework allows for selecting an optimal number of clusters and components. The proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks. Finally, the method is utilized to analyze global food trading networks, leading to structures of interest.","sentences":["In this work, we propose an original method for aggregating multiple clustering coming from different sources of information.","Each partition is encoded by a co-membership matrix between observations.","Our approach uses a mixture of multilayer Stochastic Block Models (SBM) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components.","The identifiability of the model parameters is established and a variational Bayesian EM algorithm is proposed for the estimation of these parameters.","The Bayesian framework allows for selecting an optimal number of clusters and components.","The proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks.","Finally, the method is utilized to analyze global food trading networks, leading to structures of interest."],"url":"http://arxiv.org/abs/2401.04682v1"}
{"created":"2024-01-09 16:52:57","title":"Transfer-Learning-Based Autotuning Using Gaussian Copula","abstract":"As diverse high-performance computing (HPC) systems are built, many opportunities arise for applications to solve larger problems than ever before. Given the significantly increased complexity of these HPC systems and application tuning, empirical performance tuning, such as autotuning, has emerged as a promising approach in recent years. Despite its effectiveness, autotuning is often a computationally expensive approach. Transfer learning (TL)-based autotuning seeks to address this issue by leveraging the data from prior tuning. Current TL methods for autotuning spend significant time modeling the relationship between parameter configurations and performance, which is ineffective for few-shot (that is, few empirical evaluations) tuning on new tasks. We introduce the first generative TL-based autotuning approach based on the Gaussian copula (GC) to model the high-performing regions of the search space from prior data and then generate high-performing configurations for new tasks. This allows a sampling-based approach that maximizes few-shot performance and provides the first probabilistic estimation of the few-shot budget for effective TL-based autotuning. We compare our generative TL approach with state-of-the-art autotuning techniques on several benchmarks. We find that the GC is capable of achieving 64.37% of peak few-shot performance in its first evaluation. Furthermore, the GC model can determine a few-shot transfer budget that yields up to 33.39$\\times$ speedup, a dramatic improvement over the 20.58$\\times$ speedup using prior techniques.","sentences":["As diverse high-performance computing (HPC) systems are built, many opportunities arise for applications to solve larger problems than ever before.","Given the significantly increased complexity of these HPC systems and application tuning, empirical performance tuning, such as autotuning, has emerged as a promising approach in recent years.","Despite its effectiveness, autotuning is often a computationally expensive approach.","Transfer learning (TL)-based autotuning seeks to address this issue by leveraging the data from prior tuning.","Current TL methods for autotuning spend significant time modeling the relationship between parameter configurations and performance, which is ineffective for few-shot (that is, few empirical evaluations) tuning on new tasks.","We introduce the first generative TL-based autotuning approach based on the Gaussian copula (GC) to model the high-performing regions of the search space from prior data and then generate high-performing configurations for new tasks.","This allows a sampling-based approach that maximizes few-shot performance and provides the first probabilistic estimation of the few-shot budget for effective TL-based autotuning.","We compare our generative TL approach with state-of-the-art autotuning techniques on several benchmarks.","We find that the GC is capable of achieving 64.37% of peak few-shot performance in its first evaluation.","Furthermore, the GC model can determine a few-shot transfer budget that yields up to 33.39$\\times$ speedup, a dramatic improvement over the 20.58$\\times$ speedup using prior techniques."],"url":"http://arxiv.org/abs/2401.04669v1"}
{"created":"2024-01-09 16:48:11","title":"Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset","abstract":"As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset. From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large.","sentences":["As the most basic application and implementation of deep learning, image classification has grown in popularity.","Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models.","The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards.","A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions.","Hyper-parameters are changed to gain the best result from a model.","By applying this approach, we have got higher accuracy without major changes in the training model.","To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090.","The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset.","From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large."],"url":"http://arxiv.org/abs/2401.04666v1"}
{"created":"2024-01-09 16:16:32","title":"A novel framework for generalization of deep hidden physics models","abstract":"Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources. Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics. However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable. In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains. We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configuration.","sentences":["Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources.","Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics.","However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable.","In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains.","We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configuration."],"url":"http://arxiv.org/abs/2401.04648v1"}
{"created":"2024-01-09 16:05:47","title":"Applying Large Language Models API to Issue Classification Problem","abstract":"Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately label and prioritize issue reports with a reduced training dataset. By reducing reliance on massive data requirements and focusing on few-shot fine-tuning, our methodology offers a more accessible and efficient solution for issue prioritization in software engineering. Our model predicted issue types in individual projects up to 93.2% in precision, 95% in recall, and 89.3% in F1-score.","sentences":["Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly.","However, the manual classification of issue reports for prioritization is laborious and lacks scalability.","Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training.","This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets.","Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task.","By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability.","In our research, we have developed a reliable GPT-based approach to accurately label and prioritize issue reports with a reduced training dataset.","By reducing reliance on massive data requirements and focusing on few-shot fine-tuning, our methodology offers a more accessible and efficient solution for issue prioritization in software engineering.","Our model predicted issue types in individual projects up to 93.2% in precision, 95% in recall, and 89.3% in F1-score."],"url":"http://arxiv.org/abs/2401.04637v1"}
{"created":"2024-01-09 15:59:43","title":"Hypercomplex neural network in time series forecasting of stock data","abstract":"The three classes of architectures for time series prediction were tested. They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras. The input was four related Stock Market time series, and the prediction of one of them is expected. The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class. The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters. Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures. Moreover, the order of the input time series has an impact on effectively.","sentences":["The three classes of architectures for time series prediction were tested.","They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras.","The input was four related Stock Market time series, and the prediction of one of them is expected.","The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class.","The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters.","Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures.","Moreover, the order of the input time series has an impact on effectively."],"url":"http://arxiv.org/abs/2401.04632v1"}
{"created":"2024-01-09 15:56:43","title":"Multi-Neuron Representations of Hierarchical Concepts in Spiking Neural Networks","abstract":"We describe how hierarchical concepts can be represented in three types of layered neural networks. The aim is to support recognition of the concepts when partial information about the concepts is presented, and also when some of the neurons in the network might fail. Our failure model involves initial random failures. The three types of networks are: feed-forward networks with high connectivity, feed-forward networks with low connectivity, and layered networks with low connectivity and with both forward edges and \"lateral\" edges within layers. In order to achieve fault-tolerance, the representations all use multiple representative neurons for each concept. We show how recognition can work in all three of these settings, and quantify how the probability of correct recognition depends on several parameters, including the number of representatives and the neuron failure probability. We also discuss how these representations might be learned, in all three types of networks. For the feed-forward networks, the learning algorithms are similar to ones used in [4], whereas for networks with lateral edges, the algorithms are generally inspired by work on the assembly calculus [3, 6, 7].","sentences":["We describe how hierarchical concepts can be represented in three types of layered neural networks.","The aim is to support recognition of the concepts when partial information about the concepts is presented, and also when some of the neurons in the network might fail.","Our failure model involves initial random failures.","The three types of networks are: feed-forward networks with high connectivity, feed-forward networks with low connectivity, and layered networks with low connectivity and with both forward edges and \"lateral\" edges within layers.","In order to achieve fault-tolerance, the representations all use multiple representative neurons for each concept.","We show how recognition can work in all three of these settings, and quantify how the probability of correct recognition depends on several parameters, including the number of representatives and the neuron failure probability.","We also discuss how these representations might be learned, in all three types of networks.","For the feed-forward networks, the learning algorithms are similar to ones used in [4], whereas for networks with lateral edges, the algorithms are generally inspired by work on the assembly calculus [3, 6, 7]."],"url":"http://arxiv.org/abs/2401.04628v1"}
{"created":"2024-01-09 15:55:08","title":"A Novel OMNeT++-based Simulation Tool for Vehicular Cloud Computing in ETSI MEC-compliant 5G Environments","abstract":"Vehicular cloud computing is gaining popularity thanks to the rapid advancements in next generation wireless communication networks. Similarly, Edge Computing, along with its standard proposals such as European Telecommunications Standards Institute (ETSI) Multi-access Edge Computing (MEC), will play a vital role in these scenarios, by enabling the execution of cloud-based services at the edge of the network. Together, these solutions have the potential to create real micro-datacenters at the network edge, favoring several benefits like minimal latency, real-time data processing, and data locality. However, the research community has not yet the opportunity to use integrated simulation frameworks for the easy testing of applications that exploit both the vehicular cloud paradigm and MEC-compliant 5G deployment environments. In this paper, we present our simulation tool as a platform for researchers and engineers to design, test, and enhance applications utilizing the concepts of vehicular and edge cloud. Our platform significantly extends OMNet++ and Simu5G, and implements our ETSI MEC-compliant architecture that leverages resources provided by far-edge nodes. In addition, the paper analyzes and reports performance results for our simulation platform, as well as provides a use case where our simulator is used to support the design, test, and validation of an algorithm to distribute MEC application components on vehicular cloud resources.","sentences":["Vehicular cloud computing is gaining popularity thanks to the rapid advancements in next generation wireless communication networks.","Similarly, Edge Computing, along with its standard proposals such as European Telecommunications Standards Institute (ETSI) Multi-access Edge Computing (MEC), will play a vital role in these scenarios, by enabling the execution of cloud-based services at the edge of the network.","Together, these solutions have the potential to create real micro-datacenters at the network edge, favoring several benefits like minimal latency, real-time data processing, and data locality.","However, the research community has not yet the opportunity to use integrated simulation frameworks for the easy testing of applications that exploit both the vehicular cloud paradigm and MEC-compliant 5G deployment environments.","In this paper, we present our simulation tool as a platform for researchers and engineers to design, test, and enhance applications utilizing the concepts of vehicular and edge cloud.","Our platform significantly extends OMNet++ and Simu5G, and implements our ETSI MEC-compliant architecture that leverages resources provided by far-edge nodes.","In addition, the paper analyzes and reports performance results for our simulation platform, as well as provides a use case where our simulator is used to support the design, test, and validation of an algorithm to distribute MEC application components on vehicular cloud resources."],"url":"http://arxiv.org/abs/2401.04626v1"}
{"created":"2024-01-09 15:46:38","title":"DebugBench: Evaluating Debugging Capability of Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.","sentences":["Large Language Models (LLMs) have demonstrated exceptional coding capability.","However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored.","Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs.","To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances.","It covers four major bug categories and 18 minor types in C++, Java, and Python.","To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks.","We evaluate two commercial and three open-source models in a zero-shot scenario.","We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful.","As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models.","These findings will benefit the development of LLMs in debugging."],"url":"http://arxiv.org/abs/2401.04621v1"}
{"created":"2024-01-09 15:28:29","title":"Distribution-Free Conformal Joint Prediction Regions for Neural Marked Temporal Point Processes","abstract":"Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields. Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark. However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty. This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction. A primary objective is to generate a distribution-free joint prediction region for the arrival time and mark, with a finite-sample marginal coverage guarantee. A key challenge is to handle both a strictly positive, continuous response and a categorical response, without distributional assumptions. We first consider a simple but overly conservative approach that combines individual prediction regions for the event arrival time and mark. Then, we introduce a more effective method based on bivariate highest density regions derived from the joint predictive density of event arrival time and mark. By leveraging the dependencies between these two variables, this method exclude unlikely combinations of the two, resulting in sharper prediction regions while still attaining the pre-specified coverage level. We also explore the generation of individual univariate prediction regions for arrival times and marks through conformal regression and classification techniques. Moreover, we investigate the stronger notion of conditional coverage. Finally, through extensive experimentation on both simulated and real-world datasets, we assess the validity and efficiency of these methods.","sentences":["Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields.","Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark.","However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty.","This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction.","A primary objective is to generate a distribution-free joint prediction region for the arrival time and mark, with a finite-sample marginal coverage guarantee.","A key challenge is to handle both a strictly positive, continuous response and a categorical response, without distributional assumptions.","We first consider a simple but overly conservative approach that combines individual prediction regions for the event arrival time and mark.","Then, we introduce a more effective method based on bivariate highest density regions derived from the joint predictive density of event arrival time and mark.","By leveraging the dependencies between these two variables, this method exclude unlikely combinations of the two, resulting in sharper prediction regions while still attaining the pre-specified coverage level.","We also explore the generation of individual univariate prediction regions for arrival times and marks through conformal regression and classification techniques.","Moreover, we investigate the stronger notion of conditional coverage.","Finally, through extensive experimentation on both simulated and real-world datasets, we assess the validity and efficiency of these methods."],"url":"http://arxiv.org/abs/2401.04612v1"}
{"created":"2024-01-09 14:58:34","title":"A Multi-Modal Approach Based on Large Vision Model for Close-Range Underwater Target Localization","abstract":"Underwater target localization uses real-time sensory measurements to estimate the position of underwater objects of interest, providing critical feedback information for underwater robots. While acoustic sensing is the most acknowledged method in underwater robots and possibly the only effective approach for long-range underwater target localization, such a sensing modality generally suffers from low resolution, high cost and high energy consumption, thus leading to a mediocre performance when applied to close-range underwater target localization. On the other hand, optical sensing has attracted increasing attention in the underwater robotics community for its advantages of high resolution and low cost, holding a great potential particularly in close-range underwater target localization. However, most existing studies in underwater optical sensing are restricted to specific types of targets due to the limited training data available. In addition, these studies typically focus on the design of estimation algorithms and ignore the influence of illumination conditions on the sensing performance, thus hindering wider applications in the real world. To address the aforementioned issues, this paper proposes a novel target localization method that assimilates both optical and acoustic sensory measurements to estimate the 3D positions of close-range underwater targets. A test platform with controllable illumination conditions is designed and developed to experimentally investigate the proposed multi-modal sensing approach. A large vision model is applied to process the optical imaging measurements, eliminating the requirement for training data acquisition, thus significantly expanding the scope of potential applications. Extensive experiments are conducted, the results of which validate the effectiveness of the proposed underwater target localization method.","sentences":["Underwater target localization uses real-time sensory measurements to estimate the position of underwater objects of interest, providing critical feedback information for underwater robots.","While acoustic sensing is the most acknowledged method in underwater robots and possibly the only effective approach for long-range underwater target localization, such a sensing modality generally suffers from low resolution, high cost and high energy consumption, thus leading to a mediocre performance when applied to close-range underwater target localization.","On the other hand, optical sensing has attracted increasing attention in the underwater robotics community for its advantages of high resolution and low cost, holding a great potential particularly in close-range underwater target localization.","However, most existing studies in underwater optical sensing are restricted to specific types of targets due to the limited training data available.","In addition, these studies typically focus on the design of estimation algorithms and ignore the influence of illumination conditions on the sensing performance, thus hindering wider applications in the real world.","To address the aforementioned issues, this paper proposes a novel target localization method that assimilates both optical and acoustic sensory measurements to estimate the 3D positions of close-range underwater targets.","A test platform with controllable illumination conditions is designed and developed to experimentally investigate the proposed multi-modal sensing approach.","A large vision model is applied to process the optical imaging measurements, eliminating the requirement for training data acquisition, thus significantly expanding the scope of potential applications.","Extensive experiments are conducted, the results of which validate the effectiveness of the proposed underwater target localization method."],"url":"http://arxiv.org/abs/2401.04595v1"}
{"created":"2024-01-09 14:50:04","title":"An Assessment on Comprehending Mental Health through Large Language Models","abstract":"Mental health challenges pose considerable global burdens on individuals and communities. Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime. On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health. On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language. This study presents an initial evaluation of large language models in addressing this gap. Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models. Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models.","sentences":["Mental health challenges pose considerable global burdens on individuals and communities.","Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime.","On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health.","On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language.","This study presents an initial evaluation of large language models in addressing this gap.","Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models.","Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models."],"url":"http://arxiv.org/abs/2401.04592v1"}
{"created":"2024-01-09 14:32:24","title":"Effective pruning of web-scale datasets based on complexity of concept clusters","abstract":"Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models. Today's most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most prototypical samples. We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept. Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training. By filtering from the LAION dataset, we find that training on a smaller set of high-quality data can lead to higher performance with significantly lower training costs. More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot accuracy by 1.1p.p. while only using 27.7% of the data and training compute. Despite a strong reduction in training cost, we also see improvements on ImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks.","sentences":["Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training.","In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models.","Today's most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most prototypical samples.","We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept.","Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training.","By filtering from the LAION dataset, we find that training on a smaller set of high-quality data can lead to higher performance with significantly lower training costs.","More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot accuracy by 1.1p.p.","while only using 27.7% of the data and training compute.","Despite a strong reduction in training cost, we also see improvements on ImageNet dist.","shifts, retrieval tasks and VTAB.","On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks."],"url":"http://arxiv.org/abs/2401.04578v1"}
{"created":"2024-01-09 14:24:29","title":"Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding","abstract":"Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds. Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize. Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer.","sentences":["Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes.","This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices.","Therefore, we seek more efficient ways to collect and annotate images.","Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity.","For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency.","We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites.","When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds.","Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize.","Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer."],"url":"http://arxiv.org/abs/2401.04575v1"}
