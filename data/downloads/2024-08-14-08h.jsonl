{"created":"2024-08-13 17:57:14","title":"Fingerspelling within Sign Language Translation","abstract":"Fingerspelling poses challenges for sign language processing due to its high-frequency motion and use for open-vocabulary terms. While prior work has studied fingerspelling recognition, there has been little attention to evaluating how well sign language translation models understand fingerspelling in the context of entire sentences -- and improving this capability. We manually annotate instances of fingerspelling within FLEURS-ASL and use them to evaluate the effect of two simple measures to improve fingerspelling recognition within American Sign Language to English translation: 1) use a model family (ByT5) with character- rather than subword-level tokenization, and 2) mix fingerspelling recognition data into the translation training mixture. We find that 1) substantially improves understanding of fingerspelling (and therefore translation quality overall), but the effect of 2) is mixed.","sentences":["Fingerspelling poses challenges for sign language processing due to its high-frequency motion and use for open-vocabulary terms.","While prior work has studied fingerspelling recognition, there has been little attention to evaluating how well sign language translation models understand fingerspelling in the context of entire sentences -- and improving this capability.","We manually annotate instances of fingerspelling within FLEURS-ASL and use them to evaluate the effect of two simple measures to improve fingerspelling recognition within American Sign Language to English translation: 1) use a model family (ByT5) with character- rather than subword-level tokenization, and 2) mix fingerspelling recognition data into the translation training mixture.","We find that 1) substantially improves understanding of fingerspelling (and therefore translation quality overall), but the effect of 2) is mixed."],"url":"http://arxiv.org/abs/2408.07065v1"}
{"created":"2024-08-13 17:56:51","title":"HADRON: Human-friendly Control and Artificial Intelligence for Military Drone Operations","abstract":"As drones are getting more and more entangled in our society, more untrained users require the capability to operate them. This scenario is to be achieved through the development of artificial intelligence capabilities assisting the human operator in controlling the Unmanned Aerial System (UAS) and processing the sensor data, thereby alleviating the need for extensive operator training. This paper presents the HADRON project that seeks to develop and test multiple novel technologies to enable human-friendly control of drone swarms. This project is divided into three main parts. The first part consists of the integration of different technologies for the intuitive control of drones, focusing on novice or inexperienced pilots and operators. The second part focuses on the development of a multi-drone system that will be controlled from a command and control station, in which an expert pilot can supervise the operations of the multiple drones. The third part of the project will focus on reducing the cognitive load on human operators, whether they are novice or expert pilots. For this, we will develop AI tools that will assist drone operators with semi-automated real-time data processing.","sentences":["As drones are getting more and more entangled in our society, more untrained users require the capability to operate them.","This scenario is to be achieved through the development of artificial intelligence capabilities assisting the human operator in controlling the Unmanned Aerial System (UAS) and processing the sensor data, thereby alleviating the need for extensive operator training.","This paper presents the HADRON project that seeks to develop and test multiple novel technologies to enable human-friendly control of drone swarms.","This project is divided into three main parts.","The first part consists of the integration of different technologies for the intuitive control of drones, focusing on novice or inexperienced pilots and operators.","The second part focuses on the development of a multi-drone system that will be controlled from a command and control station, in which an expert pilot can supervise the operations of the multiple drones.","The third part of the project will focus on reducing the cognitive load on human operators, whether they are novice or expert pilots.","For this, we will develop AI tools that will assist drone operators with semi-automated real-time data processing."],"url":"http://arxiv.org/abs/2408.07063v1"}
{"created":"2024-08-13 17:46:12","title":"LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs","abstract":"Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words. Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT). In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets. To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words. By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality. We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities. Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models. In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability. Our code & models are at: https://github.com/THUDM/LongWriter.","sentences":["Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words.","Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT).","In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets.","To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words.","Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words.","By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality.","We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities.","Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models.","In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability.","Our code & models are at: https://github.com/THUDM/LongWriter."],"url":"http://arxiv.org/abs/2408.07055v1"}
{"created":"2024-08-13 17:37:40","title":"PSM: Learning Probabilistic Embeddings for Multi-scale Zero-Shot Soundscape Mapping","abstract":"A soundscape is defined by the acoustic environment a person perceives at a location. In this work, we propose a framework for mapping soundscapes across the Earth. Since soundscapes involve sound distributions that span varying spatial scales, we represent locations with multi-scale satellite imagery and learn a joint representation among this imagery, audio, and text. To capture the inherent uncertainty in the soundscape of a location, we design the representation space to be probabilistic. We also fuse ubiquitous metadata (including geolocation, time, and data source) to enable learning of spatially and temporally dynamic representations of soundscapes. We demonstrate the utility of our framework by creating large-scale soundscape maps integrating both audio and text with temporal control. To facilitate future research on this task, we also introduce a large-scale dataset, GeoSound, containing over $300k$ geotagged audio samples paired with both low- and high-resolution satellite imagery. We demonstrate that our method outperforms the existing state-of-the-art on both GeoSound and the existing SoundingEarth dataset. Our dataset and code is available at https://github.com/mvrl/PSM.","sentences":["A soundscape is defined by the acoustic environment a person perceives at a location.","In this work, we propose a framework for mapping soundscapes across the Earth.","Since soundscapes involve sound distributions that span varying spatial scales, we represent locations with multi-scale satellite imagery and learn a joint representation among this imagery, audio, and text.","To capture the inherent uncertainty in the soundscape of a location, we design the representation space to be probabilistic.","We also fuse ubiquitous metadata (including geolocation, time, and data source) to enable learning of spatially and temporally dynamic representations of soundscapes.","We demonstrate the utility of our framework by creating large-scale soundscape maps integrating both audio and text with temporal control.","To facilitate future research on this task, we also introduce a large-scale dataset, GeoSound, containing over $300k$ geotagged audio samples paired with both low- and high-resolution satellite imagery.","We demonstrate that our method outperforms the existing state-of-the-art on both GeoSound and the existing SoundingEarth dataset.","Our dataset and code is available at https://github.com/mvrl/PSM."],"url":"http://arxiv.org/abs/2408.07050v1"}
{"created":"2024-08-13 17:20:52","title":"TableGuard -- Securing Structured & Unstructured Data","abstract":"With the increasing demand for data sharing across platforms and organizations, ensuring the privacy and security of sensitive information has become a critical challenge. This paper introduces \"TableGuard\". An innovative approach to data obfuscation tailored for relational databases. Building on the principles and techniques developed in prior work on context-sensitive obfuscation, TableGuard applies these methods to ensure that API calls return only obfuscated data, thereby safeguarding privacy when sharing data with third parties. TableGuard leverages advanced context-sensitive obfuscation techniques to replace sensitive data elements with contextually appropriate alternatives. By maintaining the relational integrity and coherence of the data, our approach mitigates the risks of cognitive dissonance and data leakage. We demonstrate the implementation of TableGuard using a BERT based transformer model, which identifies and obfuscates sensitive entities within relational tables. Our evaluation shows that TableGuard effectively balances privacy protection with data utility, minimizing information loss while ensuring that the obfuscated data remains functionally useful for downstream applications. The results highlight the importance of domain-specific obfuscation strategies and the role of context length in preserving data integrity. The implications of this research are significant for organizations that need to share data securely with external parties. TableGuard offers a robust framework for implementing privacy-preserving data sharing mechanisms, thereby contributing to the broader field of data privacy and security.","sentences":["With the increasing demand for data sharing across platforms and organizations, ensuring the privacy and security of sensitive information has become a critical challenge.","This paper introduces \"TableGuard\".","An innovative approach to data obfuscation tailored for relational databases.","Building on the principles and techniques developed in prior work on context-sensitive obfuscation, TableGuard applies these methods to ensure that API calls return only obfuscated data, thereby safeguarding privacy when sharing data with third parties.","TableGuard leverages advanced context-sensitive obfuscation techniques to replace sensitive data elements with contextually appropriate alternatives.","By maintaining the relational integrity and coherence of the data, our approach mitigates the risks of cognitive dissonance and data leakage.","We demonstrate the implementation of TableGuard using a BERT based transformer model, which identifies and obfuscates sensitive entities within relational tables.","Our evaluation shows that TableGuard effectively balances privacy protection with data utility, minimizing information loss while ensuring that the obfuscated data remains functionally useful for downstream applications.","The results highlight the importance of domain-specific obfuscation strategies and the role of context length in preserving data integrity.","The implications of this research are significant for organizations that need to share data securely with external parties.","TableGuard offers a robust framework for implementing privacy-preserving data sharing mechanisms, thereby contributing to the broader field of data privacy and security."],"url":"http://arxiv.org/abs/2408.07045v1"}
{"created":"2024-08-13 16:36:33","title":"Improved Counting under Continual Observation with Pure Differential Privacy","abstract":"Counting under continual observation is a well-studied problem in the area of differential privacy. Given a stream of updates $x_1,x_2,\\dots,x_T \\in \\{0,1\\}$ the problem is to continuously release estimates of the prefix sums $\\sum_{i=1}^t x_i$ for $t=1,\\dots,T$ while protecting each input $x_i$ in the stream with differential privacy. Recently, significant leaps have been made in our understanding of this problem under $\\textit{approximate}$ differential privacy, aka. $(\\varepsilon,\\delta)$$\\textit{-differential privacy}$. However, for the classical case of $\\varepsilon$-differential privacy, we are not aware of any improvement in mean squared error since the work of Honaker (TPDP 2015). In this paper we present such an improvement, reducing the mean squared error by a factor of about 4, asymptotically. The key technique is a new generalization of the binary tree mechanism that uses a $k$-ary number system with $\\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our mechanism improves the mean squared error over all 'optimal' $(\\varepsilon,\\delta)$-differentially private factorization mechanisms based on Gaussian noise whenever $\\delta$ is sufficiently small. Specifically, using $k=19$ we get an asymptotic improvement over the bound given in the work by Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\\delta = O(T^{-0.92})$.","sentences":["Counting under continual observation is a well-studied problem in the area of differential privacy.","Given a stream of updates $x_1,x_2,\\dots,x_T","\\in \\{0,1\\}$ the problem is to continuously release estimates of the prefix sums $\\sum_{i=1}^t x_i$ for $t=1,\\dots,T$ while protecting each input $x_i$ in the stream with differential privacy.","Recently, significant leaps have been made in our understanding of this problem under $\\textit{approximate}$ differential privacy, aka.","$(\\varepsilon,\\delta)$$\\textit{-differential privacy}$.","However, for the classical case of $\\varepsilon$-differential privacy, we are not aware of any improvement in mean squared error since the work of Honaker (TPDP 2015).","In this paper we present such an improvement, reducing the mean squared error by a factor of about 4, asymptotically.","The key technique is a new generalization of the binary tree mechanism that uses a $k$-ary number system with $\\textit{negative digits}$ to improve the privacy-accuracy trade-off.","Our mechanism improves the mean squared error over all 'optimal' $(\\varepsilon,\\delta)$-differentially private factorization mechanisms based on Gaussian noise whenever $\\delta$ is sufficiently small.","Specifically, using $k=19$ we get an asymptotic improvement over the bound given in the work by Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\\delta = O(T^{-0.92})$."],"url":"http://arxiv.org/abs/2408.07021v1"}
{"created":"2024-08-13 16:35:22","title":"A $5/4$ Approximation for Two-Edge-Connectivity","abstract":"The $2$-Edge Connected Spanning Subgraph problem (2ECSS) is among the most basic survivable network design problems: given an undirected unweighted graph, find a subgraph with the minimum number of edges which is 2-edge-connected (i.e., it remains connected after the removal of any single edge). This NP-hard problem is well-studied in terms of approximation algorithms. The current-best approximation factor for 2ECSS is $1.3+\\varepsilon$ for any constant $\\varepsilon >0$ [Garg, Grandoni, Jabal-Ameli'23; Kobayashi,Noguchi'23]. In this paper we present a much simpler $9/7$ approximation algorithm, and a more complex $5/4$ one. Our algorithms are also faster: their running time is $n^{O(1)}$ instead of $n^{O(1/\\varepsilon)}$.","sentences":["The $2$-Edge Connected Spanning Subgraph problem (2ECSS) is among the most basic survivable network design problems: given an undirected unweighted graph, find a subgraph with the minimum number of edges which is 2-edge-connected (i.e., it remains connected after the removal of any single edge).","This NP-hard problem is well-studied in terms of approximation algorithms.","The current-best approximation factor for 2ECSS is $1.3+\\varepsilon$ for any constant $\\varepsilon >0$","[Garg, Grandoni, Jabal-Ameli'23; Kobayashi,Noguchi'23].","In this paper we present a much simpler $9/7$ approximation algorithm, and a more complex $5/4$ one.","Our algorithms are also faster: their running time is $n^{O(1)}$ instead of $n^{O(1/\\varepsilon)}$."],"url":"http://arxiv.org/abs/2408.07019v1"}
{"created":"2024-08-13 16:30:36","title":"Defining and Measuring Disentanglement for non-Independent Factors of Variation","abstract":"Representation learning is an approach that allows to discover and extract the factors of variation from the data. Intuitively, a representation is said to be disentangled if it separates the different factors of variation in a way that is understandable to humans. Definitions of disentanglement and metrics to measure it usually assume that the factors of variation are independent of each other. However, this is generally false in the real world, which limits the use of these definitions and metrics to very specific and unrealistic scenarios. In this paper we give a definition of disentanglement based on information theory that is also valid when the factors of variation are not independent. Furthermore, we relate this definition to the Information Bottleneck Method. Finally, we propose a method to measure the degree of disentanglement from the given definition that works when the factors of variation are not independent. We show through different experiments that the method proposed in this paper correctly measures disentanglement with non-independent factors of variation, while other methods fail in this scenario.","sentences":["Representation learning is an approach that allows to discover and extract the factors of variation from the data.","Intuitively, a representation is said to be disentangled if it separates the different factors of variation in a way that is understandable to humans.","Definitions of disentanglement and metrics to measure it usually assume that the factors of variation are independent of each other.","However, this is generally false in the real world, which limits the use of these definitions and metrics to very specific and unrealistic scenarios.","In this paper we give a definition of disentanglement based on information theory that is also valid when the factors of variation are not independent.","Furthermore, we relate this definition to the Information Bottleneck Method.","Finally, we propose a method to measure the degree of disentanglement from the given definition that works when the factors of variation are not independent.","We show through different experiments that the method proposed in this paper correctly measures disentanglement with non-independent factors of variation, while other methods fail in this scenario."],"url":"http://arxiv.org/abs/2408.07016v1"}
{"created":"2024-08-13 16:08:37","title":"Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models","abstract":"Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience. Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services. However, the privacy consequences associated with these services and their third-party plugins are not well understood. Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins. In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services. Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services. At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier. We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.","sentences":["Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience.","Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services.","However, the privacy consequences associated with these services and their third-party plugins are not well understood.","Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins.","In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services.","Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services.","At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier.","We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively."],"url":"http://arxiv.org/abs/2408.07004v1"}
{"created":"2024-08-13 16:00:30","title":"Faster Private Minimum Spanning Trees","abstract":"Motivated by applications in clustering and synthetic data generation, we consider the problem of releasing a minimum spanning tree (MST) under edge-weight differential privacy constraints where a graph topology $G=(V,E)$ with $n$ vertices and $m$ edges is public, the weight matrix $\\vec{W}\\in \\mathbb{R}^{n \\times n}$ is private, and we wish to release an approximate MST under $\\rho$-zero-concentrated differential privacy. Weight matrices are considered neighboring if they differ by at most $\\Delta_\\infty$ in each entry, i.e., we consider an $\\ell_\\infty$ neighboring relationship. Existing private MST algorithms either add noise to each entry in $\\vec{W}$ and estimate the MST by post-processing or add noise to weights in-place during the execution of a specific MST algorithm. Using the post-processing approach with an efficient MST algorithm takes $O(n^2)$ time on dense graphs but results in an additive error on the weight of the MST of magnitude $O(n^2\\log n)$. In-place algorithms give asymptotically better utility, but the running time of existing in-place algorithms is $O(n^3)$ for dense graphs. Our main result is a new differentially private MST algorithm that matches the utility of existing in-place methods while running in time $O(m + n^{3/2}\\log n)$ for fixed privacy parameter $\\rho$. The technical core of our algorithm is an efficient sublinear time simulation of Report-Noisy-Max that works by discretizing all edge weights to a multiple of $\\Delta_\\infty$ and forming groups of edges with identical weights. Specifically, we present a data structure that allows us to sample a noisy minimum weight edge among at most $O(n^2)$ cut edges in $O(\\sqrt{n} \\log n)$ time. Experimental evaluations support our claims that our algorithm significantly improves previous algorithms either in utility or running time.","sentences":["Motivated by applications in clustering and synthetic data generation, we consider the problem of releasing a minimum spanning tree (MST) under edge-weight differential privacy constraints where a graph topology $G=(V,E)$ with $n$ vertices and $m$ edges is public, the weight matrix $\\vec{W}\\in \\mathbb{R}^{n \\times n}$ is private, and we wish to release an approximate MST under $\\rho$-zero-concentrated differential privacy.","Weight matrices are considered neighboring if they differ by at most $\\Delta_\\infty$ in each entry, i.e., we consider an $\\ell_\\infty$ neighboring relationship.","Existing private MST algorithms either add noise to each entry in $\\vec{W}$ and estimate the MST by post-processing or add noise to weights in-place during the execution of a specific MST algorithm.","Using the post-processing approach with an efficient MST algorithm takes $O(n^2)$ time on dense graphs but results in an additive error on the weight of the MST of magnitude $O(n^2\\log n)$.","In-place algorithms give asymptotically better utility, but the running time of existing in-place algorithms is $O(n^3)$ for dense graphs.","Our main result is a new differentially private MST algorithm that matches the utility of existing in-place methods while running in time $O(m + n^{3/2}\\log n)$ for fixed privacy parameter $\\rho$. The technical core of our algorithm is an efficient sublinear time simulation of Report-Noisy-Max that works by discretizing all edge weights to a multiple of $\\Delta_\\infty$ and forming groups of edges with identical weights.","Specifically, we present a data structure that allows us to sample a noisy minimum weight edge among at most $O(n^2)$ cut edges in $O(\\sqrt{n} \\log n)$ time.","Experimental evaluations support our claims that our algorithm significantly improves previous algorithms either in utility or running time."],"url":"http://arxiv.org/abs/2408.06997v1"}
{"created":"2024-08-13 15:56:42","title":"Blessing of Dimensionality for Approximating Sobolev Classes on Manifolds","abstract":"The manifold hypothesis says that natural high-dimensional data is actually supported on or around a low-dimensional manifold. Recent success of statistical and learning-based methods empirically supports this hypothesis, due to outperforming classical statistical intuition in very high dimensions. A natural step for analysis is thus to assume the manifold hypothesis and derive bounds that are independent of any embedding space. Theoretical implications in this direction have recently been explored in terms of generalization of ReLU networks and convergence of Langevin methods. We complement existing results by providing theoretical statistical complexity results, which directly relates to generalization properties. In particular, we demonstrate that the statistical complexity required to approximate a class of bounded Sobolev functions on a compact manifold is bounded from below, and moreover that this bound is dependent only on the intrinsic properties of the manifold. These provide complementary bounds for existing approximation results for ReLU networks on manifolds, which give upper bounds on generalization capacity.","sentences":["The manifold hypothesis says that natural high-dimensional data is actually supported on or around a low-dimensional manifold.","Recent success of statistical and learning-based methods empirically supports this hypothesis, due to outperforming classical statistical intuition in very high dimensions.","A natural step for analysis is thus to assume the manifold hypothesis and derive bounds that are independent of any embedding space.","Theoretical implications in this direction have recently been explored in terms of generalization of ReLU networks and convergence of Langevin methods.","We complement existing results by providing theoretical statistical complexity results, which directly relates to generalization properties.","In particular, we demonstrate that the statistical complexity required to approximate a class of bounded Sobolev functions on a compact manifold is bounded from below, and moreover that this bound is dependent only on the intrinsic properties of the manifold.","These provide complementary bounds for existing approximation results for ReLU networks on manifolds, which give upper bounds on generalization capacity."],"url":"http://arxiv.org/abs/2408.06996v1"}
{"created":"2024-08-13 15:47:10","title":"Catamorphic Abstractions for Constrained Horn Clause Satisfiability","abstract":"Catamorphisms are functions that are recursively defined on list and trees and, in general, on Algebraic Data Types (ADTs), and are often used to compute suitable abstractions of programs that manipulate ADTs. Examples of catamorphisms include functions that compute size of lists, orderedness of lists, and height of trees. It is well known that program properties specified through catamorphisms can be proved by showing the satisfiability of suitable sets of Constrained Horn Clauses (CHCs). We address the problem of checking the satisfiability of those sets of CHCs, and we propose a method for transforming sets of CHCs into equisatisfiable sets where catamorphisms are no longer present. As a consequence, clauses with catamorphisms can be handled without extending the satisfiability algorithms used by existing CHC solvers. Through an experimental evaluation on a non-trivial benchmark consisting of many list and tree processing algorithms expressed as sets of CHCs, we show that our technique is indeed effective and significantly enhances the performance of state-of-the-art CHC solvers.","sentences":["Catamorphisms are functions that are recursively defined on list and trees and, in general, on Algebraic Data Types (ADTs), and are often used to compute suitable abstractions of programs that manipulate ADTs.","Examples of catamorphisms include functions that compute size of lists, orderedness of lists, and height of trees.","It is well known that program properties specified through catamorphisms can be proved by showing the satisfiability of suitable sets of Constrained Horn Clauses (CHCs).","We address the problem of checking the satisfiability of those sets of CHCs, and we propose a method for transforming sets of CHCs into equisatisfiable sets where catamorphisms are no longer present.","As a consequence, clauses with catamorphisms can be handled without extending the satisfiability algorithms used by existing CHC solvers.","Through an experimental evaluation on a non-trivial benchmark consisting of many list and tree processing algorithms expressed as sets of CHCs, we show that our technique is indeed effective and significantly enhances the performance of state-of-the-art CHC solvers."],"url":"http://arxiv.org/abs/2408.06988v1"}
{"created":"2024-08-13 15:27:43","title":"Prompt-Based Segmentation at Multiple Resolutions and Lighting Conditions using Segment Anything Model 2","abstract":"This paper provides insight into the effectiveness of zero-shot, prompt-based, Segment Anything Model (SAM), and its updated version, SAM 2, and the non-promptable, conventional convolutional network (CNN), in segmenting solar panels, in RGB aerial imagery, across lighting conditions, spatial resolutions, and prompt strategies. SAM 2 demonstrates improvements over SAM, particularly in sub-optimal lighting conditions when prompted by points. Both SAMs, prompted by user-box, outperformed CNN, in all scenarios. Additionally, YOLOv9 prompting outperformed user points prompting. In high-resolution imagery, both in optimal and sub-optimal lighting conditions, Eff-UNet outperformed both SAM models prompted by YOLOv9 boxes, positioning Eff-UNet as the appropriate model for automatic segmentation in high-resolution data. In low-resolution data, user box prompts were found crucial to achieve a reasonable performance. This paper provides details on strengths and limitations of each model and outlines robustness of user prompted image segmentation models in inconsistent resolution and lighting conditions of remotely sensed data.","sentences":["This paper provides insight into the effectiveness of zero-shot, prompt-based, Segment Anything Model (SAM), and its updated version, SAM 2, and the non-promptable, conventional convolutional network (CNN), in segmenting solar panels, in RGB aerial imagery, across lighting conditions, spatial resolutions, and prompt strategies.","SAM 2 demonstrates improvements over SAM, particularly in sub-optimal lighting conditions when prompted by points.","Both SAMs, prompted by user-box, outperformed CNN, in all scenarios.","Additionally, YOLOv9 prompting outperformed user points prompting.","In high-resolution imagery, both in optimal and sub-optimal lighting conditions, Eff-UNet outperformed both SAM models prompted by YOLOv9 boxes, positioning Eff-UNet as the appropriate model for automatic segmentation in high-resolution data.","In low-resolution data, user box prompts were found crucial to achieve a reasonable performance.","This paper provides details on strengths and limitations of each model and outlines robustness of user prompted image segmentation models in inconsistent resolution and lighting conditions of remotely sensed data."],"url":"http://arxiv.org/abs/2408.06970v1"}
{"created":"2024-08-13 15:21:46","title":"DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs","abstract":"Dynamic graph learning aims to uncover evolutionary laws in real-world systems, enabling accurate social recommendation (link prediction) or early detection of cancer cells (classification). Inspired by the success of state space models, e.g., Mamba, for efficiently capturing long-term dependencies in language modeling, we propose DyG-Mamba, a new continuous state space model (SSM) for dynamic graph learning. Specifically, we first found that using inputs as control signals for SSM is not suitable for continuous-time dynamic network data with irregular sampling intervals, resulting in models being insensitive to time information and lacking generalization properties. Drawing inspiration from the Ebbinghaus forgetting curve, which suggests that memory of past events is strongly correlated with time intervals rather than specific details of the events themselves, we directly utilize irregular time spans as control signals for SSM to achieve significant robustness and generalization. Through exhaustive experiments on 12 datasets for dynamic link prediction and dynamic node classification tasks, we found that DyG-Mamba achieves state-of-the-art performance on most of the datasets, while also demonstrating significantly improved computation and memory efficiency.","sentences":["Dynamic graph learning aims to uncover evolutionary laws in real-world systems, enabling accurate social recommendation (link prediction) or early detection of cancer cells (classification).","Inspired by the success of state space models, e.g., Mamba, for efficiently capturing long-term dependencies in language modeling, we propose DyG-Mamba, a new continuous state space model (SSM) for dynamic graph learning.","Specifically, we first found that using inputs as control signals for SSM is not suitable for continuous-time dynamic network data with irregular sampling intervals, resulting in models being insensitive to time information and lacking generalization properties.","Drawing inspiration from the Ebbinghaus forgetting curve, which suggests that memory of past events is strongly correlated with time intervals rather than specific details of the events themselves, we directly utilize irregular time spans as control signals for SSM to achieve significant robustness and generalization.","Through exhaustive experiments on 12 datasets for dynamic link prediction and dynamic node classification tasks, we found that DyG-Mamba achieves state-of-the-art performance on most of the datasets, while also demonstrating significantly improved computation and memory efficiency."],"url":"http://arxiv.org/abs/2408.06966v1"}
{"created":"2024-08-13 15:17:03","title":"Measuring User Understanding in Dialogue-based XAI Systems","abstract":"The field of eXplainable Artificial Intelligence (XAI) is increasingly recognizing the need to personalize and/or interactively adapt the explanation to better reflect users' explanation needs. While dialogue-based approaches to XAI have been proposed recently, the state-of-the-art in XAI is still characterized by what we call one-shot, non-personalized and one-way explanations. In contrast, dialogue-based systems that can adapt explanations through interaction with a user promise to be superior to GUI-based or dashboard explanations as they offer a more intuitive way of requesting information. In general, while interactive XAI systems are often evaluated in terms of user satisfaction, there are limited studies that access user's objective model understanding. This is in particular the case for dialogue-based XAI approaches. In this paper, we close this gap by carrying out controlled experiments within a dialogue framework in which we measure understanding of users in three phases by asking them to simulate the predictions of the model they are learning about. By this, we can quantify the level of (improved) understanding w.r.t. how the model works, comparing the state prior, and after the interaction. We further analyze the data to reveal patterns of how the interaction between groups with high vs. low understanding gain differ. Overall, our work thus contributes to our understanding about the effectiveness of XAI approaches.","sentences":["The field of eXplainable Artificial Intelligence (XAI) is increasingly recognizing the need to personalize and/or interactively adapt the explanation to better reflect users' explanation needs.","While dialogue-based approaches to XAI have been proposed recently, the state-of-the-art in XAI is still characterized by what we call one-shot, non-personalized and one-way explanations.","In contrast, dialogue-based systems that can adapt explanations through interaction with a user promise to be superior to GUI-based or dashboard explanations as they offer a more intuitive way of requesting information.","In general, while interactive XAI systems are often evaluated in terms of user satisfaction, there are limited studies that access user's objective model understanding.","This is in particular the case for dialogue-based XAI approaches.","In this paper, we close this gap by carrying out controlled experiments within a dialogue framework in which we measure understanding of users in three phases by asking them to simulate the predictions of the model they are learning about.","By this, we can quantify the level of (improved) understanding w.r.t.","how the model works, comparing the state prior, and after the interaction.","We further analyze the data to reveal patterns of how the interaction between groups with high vs. low understanding gain differ.","Overall, our work thus contributes to our understanding about the effectiveness of XAI approaches."],"url":"http://arxiv.org/abs/2408.06960v1"}
{"created":"2024-08-13 15:15:37","title":"AuToMATo: A Parameter-Free Persistence-Based Clustering Algorithm","abstract":"We present AuToMATo, a novel parameter-free clustering algorithm based on persistent homology. AuToMATo combines the existing ToMATo clustering algorithm with a bootstrapping procedure in order to separate significant peaks of an estimated density function from non-significant ones. We perform a thorough comparison of AuToMATo against many other state-of-the-art clustering algorithms. We find that not only that AuToMATo compares favorably against other parameter-free clustering algorithms, but in many instances also significantly outperforms even the best selection of parameters for other algorithms. AuToMATo is motivated by applications in topological data analysis, in particular the Mapper algorithm, where it is desirable to work with a parameter-free clustering algorithm. Indeed, we provide evidence that AuToMATo performs well when used with Mapper. Finally, we provide an open-source implementation of AuToMATo in Python that is fully compatible with the standardscikit-learn architecture.","sentences":["We present AuToMATo, a novel parameter-free clustering algorithm based on persistent homology.","AuToMATo combines the existing ToMATo clustering algorithm with a bootstrapping procedure in order to separate significant peaks of an estimated density function from non-significant ones.","We perform a thorough comparison of AuToMATo against many other state-of-the-art clustering algorithms.","We find that not only that AuToMATo compares favorably against other parameter-free clustering algorithms, but in many instances also significantly outperforms even the best selection of parameters for other algorithms.","AuToMATo is motivated by applications in topological data analysis, in particular the Mapper algorithm, where it is desirable to work with a parameter-free clustering algorithm.","Indeed, we provide evidence that AuToMATo performs well when used with Mapper.","Finally, we provide an open-source implementation of AuToMATo in Python that is fully compatible with the standardscikit-learn architecture."],"url":"http://arxiv.org/abs/2408.06958v1"}
{"created":"2024-08-13 15:15:10","title":"Rural Handover Parameter Tuning to Achieve End to End Latency Requirements of Future Railway Mobile Communication Systems","abstract":"GSM-R (GSM for Railways) is a 2G-based standardized ground-to-train communications system that enabled interoperability across different countries. However, as a 2G-based system, it is nearing its lifetime and therefore, it will be replaced with 5G-based Future Railway Mobile Communications System (FRMCS). FRMCS is expected to bring in new use cases that demand low latency and high reliability. However, from a mobility perspective, it is not clear how the low latency and high reliability will be achieved. This paper investigates the effect of handover procedure on latency and reliability and analyzes which use cases of FRMCS can be satisfied using baseline handover. We also sweep through different handover parameter configurations and analyze their effect on mobility performance. Then, we analyze the effect of mobility performance on packet latency and reliability. Our results show that, with baseline handover, Standard Data Communications Scenario is met and optimizing for baseline handover performance can reduce latency by up to 18.5%, indicating that optimizing for mobility performance is crucial in FRMCS.","sentences":["GSM-R (GSM for Railways) is a 2G-based standardized ground-to-train communications system that enabled interoperability across different countries.","However, as a 2G-based system, it is nearing its lifetime and therefore, it will be replaced with 5G-based Future Railway Mobile Communications System (FRMCS).","FRMCS is expected to bring in new use cases that demand low latency and high reliability.","However, from a mobility perspective, it is not clear how the low latency and high reliability will be achieved.","This paper investigates the effect of handover procedure on latency and reliability and analyzes which use cases of FRMCS can be satisfied using baseline handover.","We also sweep through different handover parameter configurations and analyze their effect on mobility performance.","Then, we analyze the effect of mobility performance on packet latency and reliability.","Our results show that, with baseline handover, Standard Data Communications Scenario is met and optimizing for baseline handover performance can reduce latency by up to 18.5%, indicating that optimizing for mobility performance is crucial in FRMCS."],"url":"http://arxiv.org/abs/2408.06957v1"}
{"created":"2024-08-13 15:13:21","title":"Neural Speech and Audio Coding","abstract":"This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.","sentences":["This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems.","It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods.","The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements.","Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks.","Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs.","Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques."],"url":"http://arxiv.org/abs/2408.06954v1"}
{"created":"2024-08-13 15:01:33","title":"Towards Holistic Disease Risk Prediction using Small Language Models","abstract":"Data in the healthcare domain arise from a variety of sources and modalities, such as x-ray images, continuous measurements, and clinical notes. Medical practitioners integrate these diverse data types daily to make informed and accurate decisions. With recent advancements in language models capable of handling multimodal data, it is a logical progression to apply these models to the healthcare sector. In this work, we introduce a framework that connects small language models to multiple data sources, aiming to predict the risk of various diseases simultaneously. Our experiments encompass 12 different tasks within a multitask learning setup. Although our approach does not surpass state-of-the-art methods specialized for single tasks, it demonstrates competitive performance and underscores the potential of small language models for multimodal reasoning in healthcare.","sentences":["Data in the healthcare domain arise from a variety of sources and modalities, such as x-ray images, continuous measurements, and clinical notes.","Medical practitioners integrate these diverse data types daily to make informed and accurate decisions.","With recent advancements in language models capable of handling multimodal data, it is a logical progression to apply these models to the healthcare sector.","In this work, we introduce a framework that connects small language models to multiple data sources, aiming to predict the risk of various diseases simultaneously.","Our experiments encompass 12 different tasks within a multitask learning setup.","Although our approach does not surpass state-of-the-art methods specialized for single tasks, it demonstrates competitive performance and underscores the potential of small language models for multimodal reasoning in healthcare."],"url":"http://arxiv.org/abs/2408.06943v1"}
{"created":"2024-08-13 15:01:03","title":"Speech-based Mark for Data Sonification","abstract":"Sonification serves as a powerful tool for data accessibility, especially for people with vision loss. Among various modalities, speech is a familiar means of communication similar to the role of text in visualization. However, speech-based sonification is underexplored. We introduce SpeechTone, a novel speech-based mark for data sonification and extension to the existing Erie declarative grammar for sonification. It encodes data into speech attributes such as pitch, speed, voice and speech content. We demonstrate the efficacy of SpeechTone through three examples.","sentences":["Sonification serves as a powerful tool for data accessibility, especially for people with vision loss.","Among various modalities, speech is a familiar means of communication similar to the role of text in visualization.","However, speech-based sonification is underexplored.","We introduce SpeechTone, a novel speech-based mark for data sonification and extension to the existing Erie declarative grammar for sonification.","It encodes data into speech attributes such as pitch, speed, voice and speech content.","We demonstrate the efficacy of SpeechTone through three examples."],"url":"http://arxiv.org/abs/2408.06942v1"}
{"created":"2024-08-13 14:34:59","title":"The advantages of context specific language models: the case of the Erasmian Language Model","abstract":"The current trend to improve language model performance seems to be based on scaling up with the number of parameters (e.g. the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the model. However this comes at significant costs in terms of computational resources and energy costs that compromise the sustainability of AI solutions, as well as risk relating to privacy and misuse. In this paper we present the Erasmian Language Model (ELM) a small context specific, 900 million parameter model, pre-trained and fine-tuned by and for Erasmus University Rotterdam. We show how the model performs adequately in a classroom context for essay writing, and how it achieves superior performance in subjects that are part of its context. This has implications for a wide range of institutions and organizations, showing that context specific language models may be a viable alternative for resource constrained, privacy sensitive use cases.","sentences":["The current trend to improve language model performance seems to be based on scaling up with the number of parameters (e.g. the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the model.","However this comes at significant costs in terms of computational resources and energy costs that compromise the sustainability of AI solutions, as well as risk relating to privacy and misuse.","In this paper we present the Erasmian Language Model (ELM) a small context specific, 900 million parameter model, pre-trained and fine-tuned by and for Erasmus University Rotterdam.","We show how the model performs adequately in a classroom context for essay writing, and how it achieves superior performance in subjects that are part of its context.","This has implications for a wide range of institutions and organizations, showing that context specific language models may be a viable alternative for resource constrained, privacy sensitive use cases."],"url":"http://arxiv.org/abs/2408.06931v1"}
{"created":"2024-08-13 14:33:32","title":"Diagnosis extraction from unstructured Dutch echocardiogram reports using span- and document-level characteristic classification","abstract":"Clinical machine learning research and AI driven clinical decision support models rely on clinically accurate labels. Manually extracting these labels with the help of clinical specialists is often time-consuming and expensive. This study tests the feasibility of automatic span- and document-level diagnosis extraction from unstructured Dutch echocardiogram reports.   We included 115,692 unstructured echocardiogram reports from the UMCU a large university hospital in the Netherlands. A randomly selected subset was manually annotated for the occurrence and severity of eleven commonly described cardiac characteristics. We developed and tested several automatic labelling techniques at both span and document levels, using weighted and macro F1-score, precision, and recall for performance evaluation. We compared the performance of span labelling against document labelling methods, which included both direct document classifiers and indirect document classifiers that rely on span classification results.   The SpanCategorizer and MedRoBERTa.nl models outperformed all other span and document classifiers, respectively. The weighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in SpanCategorizer and 0.96 to 0.98 in MedRoBERTa.nl. Direct document classification was superior to indirect document classification using span classifiers. SetFit achieved competitive document classification performance using only 10\\% of the training data. Utilizing a reduced label set yielded near-perfect document classification results.   We recommend using our published SpanCategorizer and MedRoBERTa.nl models for span- and document-level diagnosis extraction from Dutch echocardiography reports. For settings with limited training data, SetFit may be a promising alternative for document classification.","sentences":["Clinical machine learning research and AI driven clinical decision support models rely on clinically accurate labels.","Manually extracting these labels with the help of clinical specialists is often time-consuming and expensive.","This study tests the feasibility of automatic span- and document-level diagnosis extraction from unstructured Dutch echocardiogram reports.   ","We included 115,692 unstructured echocardiogram reports from the UMCU a large university hospital in the Netherlands.","A randomly selected subset was manually annotated for the occurrence and severity of eleven commonly described cardiac characteristics.","We developed and tested several automatic labelling techniques at both span and document levels, using weighted and macro F1-score, precision, and recall for performance evaluation.","We compared the performance of span labelling against document labelling methods, which included both direct document classifiers and indirect document classifiers that rely on span classification results.   ","The SpanCategorizer and MedRoBERTa.nl models outperformed all other span and document classifiers, respectively.","The weighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in SpanCategorizer and 0.96 to 0.98 in MedRoBERTa.nl.","Direct document classification was superior to indirect document classification using span classifiers.","SetFit achieved competitive document classification performance using only 10\\% of the training data.","Utilizing a reduced label set yielded near-perfect document classification results.   ","We recommend using our published SpanCategorizer and MedRoBERTa.nl models for span- and document-level diagnosis extraction from Dutch echocardiography reports.","For settings with limited training data, SetFit may be a promising alternative for document classification."],"url":"http://arxiv.org/abs/2408.06930v1"}
{"created":"2024-08-13 14:29:00","title":"Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator","abstract":"Dataset distillation has emerged as a technique aiming to condense informative features from large, natural datasets into a compact and synthetic form. While recent advancements have refined this technique, its performance is bottlenecked by the prevailing class-specific synthesis paradigm. Under this paradigm, synthetic data is optimized exclusively for a pre-assigned one-hot label, creating an implicit class barrier in feature condensation. This leads to inefficient utilization of the distillation budget and oversight of inter-class feature distributions, which ultimately limits the effectiveness and efficiency, as demonstrated in our analysis.   To overcome these constraints, this paper presents the Inter-class Feature Compensator (INFER), an innovative distillation approach that transcends the class-specific data-label framework widely utilized in current dataset distillation methods. Specifically, INFER leverages a Universal Feature Compensator (UFC) to enhance feature integration across classes, enabling the generation of multiple additional synthetic instances from a single UFC input. This significantly improves the efficiency of the distillation budget.   Moreover, INFER enriches inter-class interactions during the distillation, thereby enhancing the effectiveness and generalizability of the distilled data. By allowing for the linear interpolation of labels similar to those in the original dataset, INFER meticulously optimizes the synthetic data and dramatically reduces the size of soft labels in the synthetic dataset to almost zero, establishing a new benchmark for efficiency and effectiveness in dataset distillation.","sentences":["Dataset distillation has emerged as a technique aiming to condense informative features from large, natural datasets into a compact and synthetic form.","While recent advancements have refined this technique, its performance is bottlenecked by the prevailing class-specific synthesis paradigm.","Under this paradigm, synthetic data is optimized exclusively for a pre-assigned one-hot label, creating an implicit class barrier in feature condensation.","This leads to inefficient utilization of the distillation budget and oversight of inter-class feature distributions, which ultimately limits the effectiveness and efficiency, as demonstrated in our analysis.   ","To overcome these constraints, this paper presents the Inter-class Feature Compensator (INFER), an innovative distillation approach that transcends the class-specific data-label framework widely utilized in current dataset distillation methods.","Specifically, INFER leverages a Universal Feature Compensator (UFC) to enhance feature integration across classes, enabling the generation of multiple additional synthetic instances from a single UFC input.","This significantly improves the efficiency of the distillation budget.   ","Moreover, INFER enriches inter-class interactions during the distillation, thereby enhancing the effectiveness and generalizability of the distilled data.","By allowing for the linear interpolation of labels similar to those in the original dataset, INFER meticulously optimizes the synthetic data and dramatically reduces the size of soft labels in the synthetic dataset to almost zero, establishing a new benchmark for efficiency and effectiveness in dataset distillation."],"url":"http://arxiv.org/abs/2408.06927v1"}
{"created":"2024-08-13 14:26:30","title":"SceneGPT: A Language Model for 3D Scene Understanding","abstract":"Building models that can understand and reason about 3D scenes is difficult owing to the lack of data sources for 3D supervised training and large-scale training regimes. In this work we ask - How can the knowledge in a pre-trained language model be leveraged for 3D scene understanding without any 3D pre-training. The aim of this work is to establish whether pre-trained LLMs possess priors/knowledge required for reasoning in 3D space and how can we prompt them such that they can be used for general purpose spatial reasoning and object understanding in 3D. To this end, we present SceneGPT, an LLM based scene understanding system which can perform 3D spatial reasoning without training or explicit 3D supervision. The key components of our framework are - 1) a 3D scene graph, that serves as scene representation, encoding the objects in the scene and their spatial relationships 2) a pre-trained LLM that can be adapted with in context learning for 3D spatial reasoning. We evaluate our framework qualitatively on object and scene understanding tasks including object semantics, physical properties and affordances (object-level) and spatial understanding (scene-level).","sentences":["Building models that can understand and reason about 3D scenes is difficult owing to the lack of data sources for 3D supervised training and large-scale training regimes.","In this work we ask - How can the knowledge in a pre-trained language model be leveraged for 3D scene understanding without any 3D pre-training.","The aim of this work is to establish whether pre-trained LLMs possess priors/knowledge required for reasoning in 3D space and how can we prompt them such that they can be used for general purpose spatial reasoning and object understanding in 3D. To this end, we present SceneGPT, an LLM based scene understanding system which can perform 3D spatial reasoning without training or explicit 3D supervision.","The key components of our framework are - 1) a 3D scene graph, that serves as scene representation, encoding the objects in the scene and their spatial relationships 2) a pre-trained LLM that can be adapted with in context learning for 3D spatial reasoning.","We evaluate our framework qualitatively on object and scene understanding tasks including object semantics, physical properties and affordances (object-level) and spatial understanding (scene-level)."],"url":"http://arxiv.org/abs/2408.06926v1"}
{"created":"2024-08-13 14:18:50","title":"Engineering Hypergraph $b$-Matching Algorithms","abstract":"Recently, researchers have extended the concept of matchings to the more general problem of finding $b$-matchings in hypergraphs broadening the scope of potential applications and challenges. The concept of $b$-matchings, where $b$ is a function that assigns positive integers to the vertices of the graph, is a natural extension of matchings in graphs, where each vertex $v$ is allowed to be matched to up to $b(v)$ edges, rather than just one. The weighted $b$-matching problem then seeks to select a subset of the hyperedges that fulfills the constraint and maximizes the weight.   In this work, we engineer novel algorithms for this generalized problem. More precisely, we introduce exact data reductions for the problem as well as a novel greedy initial solution and local search algorithms. These data reductions allow us to significantly shrink the input size. This is done by either determining if a hyperedge is guaranteed to be in an optimum $b$-matching and thus can be added to our solution or if it can be safely ignored. Our iterated local search algorithm provides a framework for finding suitable improvement swaps of edges.   Experiments on a wide range of real-world hypergraphs show that our new set of data reductions are highly practical, and our initial solutions are competitive for graphs and hypergraphs as well.","sentences":["Recently, researchers have extended the concept of matchings to the more general problem of finding $b$-matchings in hypergraphs broadening the scope of potential applications and challenges.","The concept of $b$-matchings, where $b$ is a function that assigns positive integers to the vertices of the graph, is a natural extension of matchings in graphs, where each vertex $v$ is allowed to be matched to up to $b(v)$ edges, rather than just one.","The weighted $b$-matching problem then seeks to select a subset of the hyperedges that fulfills the constraint and maximizes the weight.   ","In this work, we engineer novel algorithms for this generalized problem.","More precisely, we introduce exact data reductions for the problem as well as a novel greedy initial solution and local search algorithms.","These data reductions allow us to significantly shrink the input size.","This is done by either determining if a hyperedge is guaranteed to be in an optimum $b$-matching and thus can be added to our solution or if it can be safely ignored.","Our iterated local search algorithm provides a framework for finding suitable improvement swaps of edges.   ","Experiments on a wide range of real-world hypergraphs show that our new set of data reductions are highly practical, and our initial solutions are competitive for graphs and hypergraphs as well."],"url":"http://arxiv.org/abs/2408.06924v1"}
{"created":"2024-08-13 14:15:15","title":"Temporal Variability and Multi-Viewed Self-Supervised Representations to Tackle the ASVspoof5 Deepfake Challenge","abstract":"ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest global audio security challenges. It aims to advance the development of countermeasure (CM) to discriminate bonafide and spoofed speech utterances. In this paper, we focus on addressing the problem of open-domain audio deepfake detection, which corresponds directly to the ASVspoof5 Track1 open condition. At first, we comprehensively investigate various CM on ASVspoof5, including data expansion, data augmentation, and self-supervised learning (SSL) features. Due to the high-frequency gaps characteristic of the ASVspoof5 dataset, we introduce Frequency Mask, a data augmentation method that masks specific frequency bands to improve CM robustness. Combining various scale of temporal information with multiple SSL features, our experiments achieved a minDCF of 0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set.","sentences":["ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest global audio security challenges.","It aims to advance the development of countermeasure (CM) to discriminate bonafide and spoofed speech utterances.","In this paper, we focus on addressing the problem of open-domain audio deepfake detection, which corresponds directly to the ASVspoof5","Track1 open condition.","At first, we comprehensively investigate various CM on ASVspoof5, including data expansion, data augmentation, and self-supervised learning (SSL) features.","Due to the high-frequency gaps characteristic of the ASVspoof5 dataset, we introduce Frequency Mask, a data augmentation method that masks specific frequency bands to improve CM robustness.","Combining various scale of temporal information with multiple SSL features, our experiments achieved a minDCF of 0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set."],"url":"http://arxiv.org/abs/2408.06922v1"}
{"created":"2024-08-13 13:56:17","title":"Heterogeneity: An Open Challenge for Federated On-board Machine Learning","abstract":"The design of satellite missions is currently undergoing a paradigm shift from the historical approach of individualised monolithic satellites towards distributed mission configurations, consisting of multiple small satellites. With a rapidly growing number of such satellites now deployed in orbit, each collecting large amounts of data, interest in on-board orbital edge computing is rising. Federated Learning is a promising distributed computing approach in this context, allowing multiple satellites to collaborate efficiently in training on-board machine learning models. Though recent works on the use of Federated Learning in orbital edge computing have focused largely on homogeneous satellite constellations, Federated Learning could also be employed to allow heterogeneous satellites to form ad-hoc collaborations, e.g. in the case of communications satellites operated by different providers. Such an application presents additional challenges to the Federated Learning paradigm, arising largely from the heterogeneity of such a system. In this position paper, we offer a systematic review of these challenges in the context of the cross-provider use case, giving a brief overview of the state-of-the-art for each, and providing an entry point for deeper exploration of each issue.","sentences":["The design of satellite missions is currently undergoing a paradigm shift from the historical approach of individualised monolithic satellites towards distributed mission configurations, consisting of multiple small satellites.","With a rapidly growing number of such satellites now deployed in orbit, each collecting large amounts of data, interest in on-board orbital edge computing is rising.","Federated Learning is a promising distributed computing approach in this context, allowing multiple satellites to collaborate efficiently in training on-board machine learning models.","Though recent works on the use of Federated Learning in orbital edge computing have focused largely on homogeneous satellite constellations, Federated Learning could also be employed to allow heterogeneous satellites to form ad-hoc collaborations, e.g. in the case of communications satellites operated by different providers.","Such an application presents additional challenges to the Federated Learning paradigm, arising largely from the heterogeneity of such a system.","In this position paper, we offer a systematic review of these challenges in the context of the cross-provider use case, giving a brief overview of the state-of-the-art for each, and providing an entry point for deeper exploration of each issue."],"url":"http://arxiv.org/abs/2408.06903v1"}
{"created":"2024-08-13 13:50:49","title":"Entendre, a Social Bot Detection Tool for Niche, Fringe, and Extreme Social Media","abstract":"Social bots-automated accounts that generate and spread content on social media-are exploiting vulnerabilities in these platforms to manipulate public perception and disseminate disinformation. This has prompted the development of public bot detection services; however, most of these services focus primarily on Twitter, leaving niche platforms vulnerable. Fringe social media platforms such as Parler, Gab, and Gettr often have minimal moderation, which facilitates the spread of hate speech and misinformation. To address this gap, we introduce Entendre, an open-access, scalable, and platform-agnostic bot detection framework. Entendre can process a labeled dataset from any social platform to produce a tailored bot detection model using a random forest classification approach, ensuring robust social bot detection. We exploit the idea that most social platforms share a generic template, where users can post content, approve content, and provide a bio (common data features). By emphasizing general data features over platform-specific ones, Entendre offers rapid extensibility at the expense of some accuracy. To demonstrate Entendre's effectiveness, we used it to explore the presence of bots among accounts posting racist content on the now-defunct right-wing platform Parler. We examined 233,000 posts from 38,379 unique users and found that 1,916 unique users (4.99%) exhibited bot-like behavior. Visualization techniques further revealed that these bots significantly impacted the network, amplifying influential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt). These preliminary findings underscore the need for tools like Entendre to monitor and assess bot activity across diverse platforms.","sentences":["Social bots-automated accounts that generate and spread content on social media-are exploiting vulnerabilities in these platforms to manipulate public perception and disseminate disinformation.","This has prompted the development of public bot detection services; however, most of these services focus primarily on Twitter, leaving niche platforms vulnerable.","Fringe social media platforms such as Parler, Gab, and Gettr often have minimal moderation, which facilitates the spread of hate speech and misinformation.","To address this gap, we introduce Entendre, an open-access, scalable, and platform-agnostic bot detection framework.","Entendre can process a labeled dataset from any social platform to produce a tailored bot detection model using a random forest classification approach, ensuring robust social bot detection.","We exploit the idea that most social platforms share a generic template, where users can post content, approve content, and provide a bio (common data features).","By emphasizing general data features over platform-specific ones, Entendre offers rapid extensibility at the expense of some accuracy.","To demonstrate Entendre's effectiveness, we used it to explore the presence of bots among accounts posting racist content on the now-defunct right-wing platform Parler.","We examined 233,000 posts from 38,379 unique users and found that 1,916 unique users (4.99%) exhibited bot-like behavior.","Visualization techniques further revealed that these bots significantly impacted the network, amplifying influential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt).","These preliminary findings underscore the need for tools like Entendre to monitor and assess bot activity across diverse platforms."],"url":"http://arxiv.org/abs/2408.06900v1"}
{"created":"2024-08-13 13:36:48","title":"BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning","abstract":"Developing models with robust group fairness properties is paramount, particularly in ethically sensitive domains such as medical diagnosis. Recent approaches to achieving fairness in machine learning require a substantial amount of training data and depend on model retraining, which may not be practical in real-world scenarios. To mitigate these challenges, we propose Bias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method that enhances the fairness of a trained model in significantly fewer epochs without requiring access to the original training data. BMFT produces a mask over model parameters, which efficiently identifies the weights contributing the most towards biased predictions. Furthermore, we propose a two-step debiasing strategy, wherein the feature extractor undergoes initial fine-tuning on the identified bias-influenced weights, succeeded by a fine-tuning phase on a reinitialised classification layer to uphold discriminative performance. Extensive experiments across four dermatological datasets and two sensitive attributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA) techniques in both diagnostic accuracy and fairness metrics. Our findings underscore the efficacy and robustness of BMFT in advancing fairness across various out-of-distribution (OOD) settings. Our code is available at: https://github.com/vios-s/BMFT","sentences":["Developing models with robust group fairness properties is paramount, particularly in ethically sensitive domains such as medical diagnosis.","Recent approaches to achieving fairness in machine learning require a substantial amount of training data and depend on model retraining, which may not be practical in real-world scenarios.","To mitigate these challenges, we propose Bias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method that enhances the fairness of a trained model in significantly fewer epochs without requiring access to the original training data.","BMFT produces a mask over model parameters, which efficiently identifies the weights contributing the most towards biased predictions.","Furthermore, we propose a two-step debiasing strategy, wherein the feature extractor undergoes initial fine-tuning on the identified bias-influenced weights, succeeded by a fine-tuning phase on a reinitialised classification layer to uphold discriminative performance.","Extensive experiments across four dermatological datasets and two sensitive attributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA) techniques in both diagnostic accuracy and fairness metrics.","Our findings underscore the efficacy and robustness of BMFT in advancing fairness across various out-of-distribution (OOD) settings.","Our code is available at: https://github.com/vios-s/BMFT"],"url":"http://arxiv.org/abs/2408.06890v1"}
{"created":"2024-08-13 13:29:57","title":"Diffusion Model for Slate Recommendation","abstract":"Slate recommendation is a technique commonly used on streaming platforms and e-commerce sites to present multiple items together. A significant challenge with slate recommendation is managing the complex combinatorial choice space. Traditional methods often simplify this problem by assuming users engage with only one item at a time. However, this simplification does not reflect the reality, as users often interact with multiple items simultaneously. In this paper, we address the general slate recommendation problem, which accounts for simultaneous engagement with multiple items. We propose a generative approach using Diffusion Models, leveraging their ability to learn structures in high-dimensional data. Our model generates high-quality slates that maximize user satisfaction by overcoming the challenges of the combinatorial choice space. Furthermore, our approach enhances the diversity of recommendations. Extensive offline evaluations on applications such as music playlist generation and e-commerce bundle recommendations show that our model outperforms state-of-the-art baselines in both relevance and diversity.","sentences":["Slate recommendation is a technique commonly used on streaming platforms and e-commerce sites to present multiple items together.","A significant challenge with slate recommendation is managing the complex combinatorial choice space.","Traditional methods often simplify this problem by assuming users engage with only one item at a time.","However, this simplification does not reflect the reality, as users often interact with multiple items simultaneously.","In this paper, we address the general slate recommendation problem, which accounts for simultaneous engagement with multiple items.","We propose a generative approach using Diffusion Models, leveraging their ability to learn structures in high-dimensional data.","Our model generates high-quality slates that maximize user satisfaction by overcoming the challenges of the combinatorial choice space.","Furthermore, our approach enhances the diversity of recommendations.","Extensive offline evaluations on applications such as music playlist generation and e-commerce bundle recommendations show that our model outperforms state-of-the-art baselines in both relevance and diversity."],"url":"http://arxiv.org/abs/2408.06883v1"}
{"created":"2024-08-13 13:26:38","title":"Architecture Specific Generation of Large Scale Lattice Boltzmann Methods for Sparse Complex Geometries","abstract":"We implement and analyse a sparse / indirect-addressing data structure for the Lattice Boltzmann Method to support efficient compute kernels for fluid dynamics problems with a high number of non-fluid nodes in the domain, such as in porous media flows. The data structure is integrated into a code generation pipeline to enable sparse Lattice Boltzmann Methods with a variety of stencils and collision operators and to generate efficient code for kernels for CPU as well as for AMD and NVIDIA accelerator cards. We optimize these sparse kernels with an in-place streaming pattern to save memory accesses and memory consumption and we implement a communication hiding technique to prove scalability. We present single GPU performance results with up to 99% of maximal bandwidth utilization. We integrate the optimized generated kernels in the high performance framework WALBERLA and achieve a scaling efficiency of at least 82% on up to 1024 NVIDIA A100 GPUs and up to 4096 AMD MI250X GPUs on modern HPC systems. Further, we set up three different applications to test the sparse data structure for realistic demonstrator problems. We show performance results for flow through porous media, free flow over a particle bed, and blood flow in a coronary artery. We achieve a maximal performance speed-up of 2 and a significantly reduced memory consumption by up to 75% with the sparse / indirect-addressing data structure compared to the direct-addressing data structure for these applications.","sentences":["We implement and analyse a sparse / indirect-addressing data structure for the Lattice Boltzmann Method to support efficient compute kernels for fluid dynamics problems with a high number of non-fluid nodes in the domain, such as in porous media flows.","The data structure is integrated into a code generation pipeline to enable sparse Lattice Boltzmann Methods with a variety of stencils and collision operators and to generate efficient code for kernels for CPU as well as for AMD and NVIDIA accelerator cards.","We optimize these sparse kernels with an in-place streaming pattern to save memory accesses and memory consumption and we implement a communication hiding technique to prove scalability.","We present single GPU performance results with up to 99% of maximal bandwidth utilization.","We integrate the optimized generated kernels in the high performance framework WALBERLA and achieve a scaling efficiency of at least 82% on up to 1024 NVIDIA A100 GPUs and up to 4096 AMD MI250X GPUs on modern HPC systems.","Further, we set up three different applications to test the sparse data structure for realistic demonstrator problems.","We show performance results for flow through porous media, free flow over a particle bed, and blood flow in a coronary artery.","We achieve a maximal performance speed-up of 2 and a significantly reduced memory consumption by up to 75% with the sparse / indirect-addressing data structure compared to the direct-addressing data structure for these applications."],"url":"http://arxiv.org/abs/2408.06880v1"}
{"created":"2024-08-13 13:11:56","title":"Advancing Interactive Explainable AI via Belief Change Theory","abstract":"As AI models become ever more complex and intertwined in humans' daily lives, greater levels of interactivity of explainable AI (XAI) methods are needed. In this paper, we propose the use of belief change theory as a formal foundation for operators that model the incorporation of new information, i.e. user feedback in interactive XAI, to logical representations of data-driven classifiers. We argue that this type of formalisation provides a framework and a methodology to develop interactive explanations in a principled manner, providing warranted behaviour and favouring transparency and accountability of such interactions. Concretely, we first define a novel, logic-based formalism to represent explanatory information shared between humans and machines. We then consider real world scenarios for interactive XAI, with different prioritisations of new and existing knowledge, where our formalism may be instantiated. Finally, we analyse a core set of belief change postulates, discussing their suitability for our real world settings and pointing to particular challenges that may require the relaxation or reinterpretation of some of the theoretical assumptions underlying existing operators.","sentences":["As AI models become ever more complex and intertwined in humans' daily lives, greater levels of interactivity of explainable AI (XAI) methods are needed.","In this paper, we propose the use of belief change theory as a formal foundation for operators that model the incorporation of new information, i.e. user feedback in interactive XAI, to logical representations of data-driven classifiers.","We argue that this type of formalisation provides a framework and a methodology to develop interactive explanations in a principled manner, providing warranted behaviour and favouring transparency and accountability of such interactions.","Concretely, we first define a novel, logic-based formalism to represent explanatory information shared between humans and machines.","We then consider real world scenarios for interactive XAI, with different prioritisations of new and existing knowledge, where our formalism may be instantiated.","Finally, we analyse a core set of belief change postulates, discussing their suitability for our real world settings and pointing to particular challenges that may require the relaxation or reinterpretation of some of the theoretical assumptions underlying existing operators."],"url":"http://arxiv.org/abs/2408.06875v1"}
{"created":"2024-08-13 13:11:53","title":"Leveraging Language Models for Emotion and Behavior Analysis in Education","abstract":"The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences. Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues. This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students. Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution. We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting. Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding. This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis.","sentences":["The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences.","Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues.","This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students.","Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution.","We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting.","Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding.","This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis."],"url":"http://arxiv.org/abs/2408.06874v1"}
{"created":"2024-08-13 13:10:03","title":"Generative AI Tools in Academic Research: Applications and Implications for Qualitative and Quantitative Research Methodologies","abstract":"This study examines the impact of Generative Artificial Intelligence (GenAI) on academic research, focusing on its application to qualitative and quantitative data analysis. As GenAI tools evolve rapidly, they offer new possibilities for enhancing research productivity and democratising complex analytical processes. However, their integration into academic practice raises significant questions regarding research integrity and security, authorship, and the changing nature of scholarly work. Through an examination of current capabilities and potential future applications, this study provides insights into how researchers may utilise GenAI tools responsibly and ethically.   We present case studies that demonstrate the application of GenAI in various research methodologies, discuss the challenges of replicability and consistency in AI-assisted research, and consider the ethical implications of increased AI integration in academia. This study explores both qualitative and quantitative applications of GenAI, highlighting tools for transcription, coding, thematic analysis, visual analytics, and statistical analysis. By addressing these issues, we aim to contribute to the ongoing discourse on the role of AI in shaping the future of academic research and provide guidance for researchers exploring the rapidly evolving landscape of AI-assisted research tools and research.","sentences":["This study examines the impact of Generative Artificial Intelligence (GenAI) on academic research, focusing on its application to qualitative and quantitative data analysis.","As GenAI tools evolve rapidly, they offer new possibilities for enhancing research productivity and democratising complex analytical processes.","However, their integration into academic practice raises significant questions regarding research integrity and security, authorship, and the changing nature of scholarly work.","Through an examination of current capabilities and potential future applications, this study provides insights into how researchers may utilise GenAI tools responsibly and ethically.   ","We present case studies that demonstrate the application of GenAI in various research methodologies, discuss the challenges of replicability and consistency in AI-assisted research, and consider the ethical implications of increased AI integration in academia.","This study explores both qualitative and quantitative applications of GenAI, highlighting tools for transcription, coding, thematic analysis, visual analytics, and statistical analysis.","By addressing these issues, we aim to contribute to the ongoing discourse on the role of AI in shaping the future of academic research and provide guidance for researchers exploring the rapidly evolving landscape of AI-assisted research tools and research."],"url":"http://arxiv.org/abs/2408.06872v1"}
{"created":"2024-08-13 13:06:50","title":"A Comprehensive Survey on Synthetic Infrared Image synthesis","abstract":"Synthetic infrared (IR) scene and target generation is an important computer vision problem as it allows the generation of realistic IR images and targets for training and testing of various applications, such as remote sensing, surveillance, and target recognition. It also helps reduce the cost and risk associated with collecting real-world IR data. This survey paper aims to provide a comprehensive overview of the conventional mathematical modelling-based methods and deep learning-based methods used for generating synthetic IR scenes and targets. The paper discusses the importance of synthetic IR scene and target generation and briefly covers the mathematics of blackbody and grey body radiations, as well as IR image-capturing methods. The potential use cases of synthetic IR scenes and target generation are also described, highlighting the significance of these techniques in various fields. Additionally, the paper explores possible new ways of developing new techniques to enhance the efficiency and effectiveness of synthetic IR scenes and target generation while highlighting the need for further research to advance this field.","sentences":["Synthetic infrared (IR) scene and target generation is an important computer vision problem as it allows the generation of realistic IR images and targets for training and testing of various applications, such as remote sensing, surveillance, and target recognition.","It also helps reduce the cost and risk associated with collecting real-world IR data.","This survey paper aims to provide a comprehensive overview of the conventional mathematical modelling-based methods and deep learning-based methods used for generating synthetic IR scenes and targets.","The paper discusses the importance of synthetic IR scene and target generation and briefly covers the mathematics of blackbody and grey body radiations, as well as IR image-capturing methods.","The potential use cases of synthetic IR scenes and target generation are also described, highlighting the significance of these techniques in various fields.","Additionally, the paper explores possible new ways of developing new techniques to enhance the efficiency and effectiveness of synthetic IR scenes and target generation while highlighting the need for further research to advance this field."],"url":"http://arxiv.org/abs/2408.06868v1"}
{"created":"2024-08-13 12:31:03","title":"Better Gaussian Mechanism using Correlated Noise","abstract":"We present a simple variant of the Gaussian mechanism for answering differentially private queries when the sensitivity space has a certain common structure. Our motivating problem is the fundamental task of answering $d$ counting queries under the add/remove neighboring relation. The standard Gaussian mechanism solves this task by adding noise distributed as a Gaussian with variance scaled by $d$ independently to each count. We show that adding a random variable distributed as a Gaussian with variance scaled by $(\\sqrt{d} + 1)/4$ to all counts allows us to reduce the variance of the independent Gaussian noise samples to scale only with $(d + \\sqrt{d})/4$. The total noise added to each counting query follows a Gaussian distribution with standard deviation scaled by $(\\sqrt{d} + 1)/2$ rather than $\\sqrt{d}$. The central idea of our mechanism is simple and the technique is flexible. We show that applying our technique to another problem gives similar improvements over the standard Gaussian mechanism.","sentences":["We present a simple variant of the Gaussian mechanism for answering differentially private queries when the sensitivity space has a certain common structure.","Our motivating problem is the fundamental task of answering $d$ counting queries under the add/remove neighboring relation.","The standard Gaussian mechanism solves this task by adding noise distributed as a Gaussian with variance scaled by $d$ independently to each count.","We show that adding a random variable distributed as a Gaussian with variance scaled by $(\\sqrt{d} + 1)/4$ to all counts allows us to reduce the variance of the independent Gaussian noise samples to scale only with $(d + \\sqrt{d})/4$.","The total noise added to each counting query follows a Gaussian distribution with standard deviation scaled by $(\\sqrt{d} + 1)/2$ rather than $\\sqrt{d}$. The central idea of our mechanism is simple and the technique is flexible.","We show that applying our technique to another problem gives similar improvements over the standard Gaussian mechanism."],"url":"http://arxiv.org/abs/2408.06853v1"}
{"created":"2024-08-13 12:22:26","title":"Causal Agent based on Large Language Model","abstract":"Large language models (LLMs) have achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLMs to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLMs' ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLMs excel in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. This lack of causal reasoning capability limits the development of LLMs. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tools module, the causal agent applies causal methods to align tabular data with natural language. In the reasoning module, the causal agent employs the ReAct framework to perform reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for these four levels of issues and tested the causal agent on the datasets. Our methodology demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80%. For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/Kairong-Han/Causal_Agent.","sentences":["Large language models (LLMs) have achieved significant success across various domains.","However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLMs to comprehend and use them effectively.","Causal methods are not easily conveyed through natural language, which hinders LLMs' ability to apply them accurately.","Additionally, causal datasets are typically tabular, while LLMs excel in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data.","This lack of causal reasoning capability limits the development of LLMs.","To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems.","The causal agent comprises tools, memory, and reasoning modules.","In the tools module, the causal agent applies causal methods to align tabular data with natural language.","In the reasoning module, the causal agent employs the ReAct framework to perform reasoning through multiple iterations with the tools.","In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs.","To verify the causal ability of the causal agent, we established a benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level.","We generated a test dataset of 1.3K using ChatGPT-3.5 for these four levels of issues and tested the causal agent on the datasets.","Our methodology demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80%.","For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/Kairong-Han/Causal_Agent."],"url":"http://arxiv.org/abs/2408.06849v1"}
{"created":"2024-08-13 12:19:02","title":"AI Research is not Magic, it has to be Reproducible and Responsible: Challenges in the AI field from the Perspective of its PhD Students","abstract":"With the goal of uncovering the challenges faced by European AI students during their research endeavors, we surveyed 28 AI doctoral candidates from 13 European countries. The outcomes underscore challenges in three key areas: (1) the findability and quality of AI resources such as datasets, models, and experiments; (2) the difficulties in replicating the experiments in AI papers; (3) and the lack of trustworthiness and interdisciplinarity. From our findings, it appears that although early stage AI researchers generally tend to share their AI resources, they lack motivation or knowledge to engage more in dataset and code preparation and curation, and ethical assessments, and are not used to cooperate with well-versed experts in application domains. Furthermore, we examine existing practices in data governance and reproducibility both in computer science and in artificial intelligence. For instance, only a minority of venues actively promote reproducibility initiatives such as reproducibility evaluations.   Critically, there is need for immediate adoption of responsible and reproducible AI research practices, crucial for society at large, and essential for the AI research community in particular. This paper proposes a combination of social and technical recommendations to overcome the identified challenges. Socially, we propose the general adoption of reproducibility initiatives in AI conferences and journals, as well as improved interdisciplinary collaboration, especially in data governance practices. On the technical front, we call for enhanced tools to better support versioning control of datasets and code, and a computing infrastructure that facilitates the sharing and discovery of AI resources, as well as the sharing, execution, and verification of experiments.","sentences":["With the goal of uncovering the challenges faced by European AI students during their research endeavors, we surveyed 28 AI doctoral candidates from 13 European countries.","The outcomes underscore challenges in three key areas: (1) the findability and quality of AI resources such as datasets, models, and experiments; (2) the difficulties in replicating the experiments in AI papers; (3) and the lack of trustworthiness and interdisciplinarity.","From our findings, it appears that although early stage AI researchers generally tend to share their AI resources, they lack motivation or knowledge to engage more in dataset and code preparation and curation, and ethical assessments, and are not used to cooperate with well-versed experts in application domains.","Furthermore, we examine existing practices in data governance and reproducibility both in computer science and in artificial intelligence.","For instance, only a minority of venues actively promote reproducibility initiatives such as reproducibility evaluations.   ","Critically, there is need for immediate adoption of responsible and reproducible AI research practices, crucial for society at large, and essential for the AI research community in particular.","This paper proposes a combination of social and technical recommendations to overcome the identified challenges.","Socially, we propose the general adoption of reproducibility initiatives in AI conferences and journals, as well as improved interdisciplinary collaboration, especially in data governance practices.","On the technical front, we call for enhanced tools to better support versioning control of datasets and code, and a computing infrastructure that facilitates the sharing and discovery of AI resources, as well as the sharing, execution, and verification of experiments."],"url":"http://arxiv.org/abs/2408.06847v1"}
{"created":"2024-08-13 11:54:18","title":"How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study on Bar Charts with Varying Layouts","abstract":"Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways? Graphical perception literature has shown that human chart takeaways are sensitive to visualization design choices, such as spatial layouts. In this work, we examine the extent to which LLMs exhibit such sensitivity when generating takeaways, using bar charts with varying spatial layouts as a case study. We conducted three experiments and tested four common bar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. In Experiment 1, we identified the optimal configurations to generate meaningful chart takeaways by testing four LLMs, two temperature settings, nine chart specifications, and two prompting strategies. We found that even state-of-the-art LLMs struggled to generate semantically diverse and factually accurate takeaways. In Experiment 2, we used the optimal configurations to generate 30 chart takeaways each for eight visualizations across four layouts and two datasets in both zero-shot and one-shot settings. Compared to human takeaways, we found that the takeaways LLMs generated often did not match the types of comparisons made by humans. In Experiment 3, we examined the effect of chart context and data on LLM takeaways. We found that LLMs, unlike humans, exhibited variation in takeaway comparison types for different bar charts using the same bar layout. Overall, our case study evaluates the ability of LLMs to emulate human interpretations of data and points to challenges and opportunities in using LLMs to predict human chart takeaways.","sentences":["Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways?","Graphical perception literature has shown that human chart takeaways are sensitive to visualization design choices, such as spatial layouts.","In this work, we examine the extent to which LLMs exhibit such sensitivity when generating takeaways, using bar charts with varying spatial layouts as a case study.","We conducted three experiments and tested four common bar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked.","In Experiment 1, we identified the optimal configurations to generate meaningful chart takeaways by testing four LLMs, two temperature settings, nine chart specifications, and two prompting strategies.","We found that even state-of-the-art LLMs struggled to generate semantically diverse and factually accurate takeaways.","In Experiment 2, we used the optimal configurations to generate 30 chart takeaways each for eight visualizations across four layouts and two datasets in both zero-shot and one-shot settings.","Compared to human takeaways, we found that the takeaways LLMs generated often did not match the types of comparisons made by humans.","In Experiment 3, we examined the effect of chart context and data on LLM takeaways.","We found that LLMs, unlike humans, exhibited variation in takeaway comparison types for different bar charts using the same bar layout.","Overall, our case study evaluates the ability of LLMs to emulate human interpretations of data and points to challenges and opportunities in using LLMs to predict human chart takeaways."],"url":"http://arxiv.org/abs/2408.06837v1"}
{"created":"2024-08-13 11:46:32","title":"FlatFusion: Delving into Details of Sparse Transformer-based Camera-LiDAR Fusion for Autonomous Driving","abstract":"The integration of data from diverse sensor modalities (e.g., camera and LiDAR) constitutes a prevalent methodology within the ambit of autonomous driving scenarios. Recent advancements in efficient point cloud transformers have underscored the efficacy of integrating information in sparse formats. When it comes to fusion, since image patches are dense in pixel space with ambiguous depth, it necessitates additional design considerations for effective fusion. In this paper, we conduct a comprehensive exploration of design choices for Transformer-based sparse cameraLiDAR fusion. This investigation encompasses strategies for image-to-3D and LiDAR-to-2D mapping, attention neighbor grouping, single modal tokenizer, and micro-structure of Transformer. By amalgamating the most effective principles uncovered through our investigation, we introduce FlatFusion, a carefully designed framework for sparse camera-LiDAR fusion. Notably, FlatFusion significantly outperforms state-of-the-art sparse Transformer-based methods, including UniTR, CMT, and SparseFusion, achieving 73.7 NDS on the nuScenes validation set with 10.1 FPS with PyTorch.","sentences":["The integration of data from diverse sensor modalities (e.g., camera and LiDAR) constitutes a prevalent methodology within the ambit of autonomous driving scenarios.","Recent advancements in efficient point cloud transformers have underscored the efficacy of integrating information in sparse formats.","When it comes to fusion, since image patches are dense in pixel space with ambiguous depth, it necessitates additional design considerations for effective fusion.","In this paper, we conduct a comprehensive exploration of design choices for Transformer-based sparse cameraLiDAR fusion.","This investigation encompasses strategies for image-to-3D and LiDAR-to-2D mapping, attention neighbor grouping, single modal tokenizer, and micro-structure of Transformer.","By amalgamating the most effective principles uncovered through our investigation, we introduce FlatFusion, a carefully designed framework for sparse camera-LiDAR fusion.","Notably, FlatFusion significantly outperforms state-of-the-art sparse Transformer-based methods, including UniTR, CMT, and SparseFusion, achieving 73.7 NDS on the nuScenes validation set with 10.1 FPS with PyTorch."],"url":"http://arxiv.org/abs/2408.06832v1"}
{"created":"2024-08-13 11:34:28","title":"Membership Inference Attack Against Masked Image Modeling","abstract":"Masked Image Modeling (MIM) has achieved significant success in the realm of self-supervised learning (SSL) for visual recognition. The image encoder pre-trained through MIM, involving the masking and subsequent reconstruction of input images, attains state-of-the-art performance in various downstream vision tasks. However, most existing works focus on improving the performance of MIM.In this work, we take a different angle by studying the pre-training data privacy of MIM. Specifically, we propose the first membership inference attack against image encoders pre-trained by MIM, which aims to determine whether an image is part of the MIM pre-training dataset. The key design is to simulate the pre-training paradigm of MIM, i.e., image masking and subsequent reconstruction, and then obtain reconstruction errors. These reconstruction errors can serve as membership signals for achieving attack goals, as the encoder is more capable of reconstructing the input image in its training set with lower errors. Extensive evaluations are conducted on three model architectures and three benchmark datasets. Empirical results show that our attack outperforms baseline methods. Additionally, we undertake intricate ablation studies to analyze multiple factors that could influence the performance of the attack.","sentences":["Masked Image Modeling (MIM) has achieved significant success in the realm of self-supervised learning (SSL) for visual recognition.","The image encoder pre-trained through MIM, involving the masking and subsequent reconstruction of input images, attains state-of-the-art performance in various downstream vision tasks.","However, most existing works focus on improving the performance of MIM.In this work, we take a different angle by studying the pre-training data privacy of MIM.","Specifically, we propose the first membership inference attack against image encoders pre-trained by MIM, which aims to determine whether an image is part of the MIM pre-training dataset.","The key design is to simulate the pre-training paradigm of MIM, i.e., image masking and subsequent reconstruction, and then obtain reconstruction errors.","These reconstruction errors can serve as membership signals for achieving attack goals, as the encoder is more capable of reconstructing the input image in its training set with lower errors.","Extensive evaluations are conducted on three model architectures and three benchmark datasets.","Empirical results show that our attack outperforms baseline methods.","Additionally, we undertake intricate ablation studies to analyze multiple factors that could influence the performance of the attack."],"url":"http://arxiv.org/abs/2408.06825v1"}
{"created":"2024-08-13 11:29:30","title":"CRISP: Confidentiality, Rollback, and Integrity Storage Protection for Confidential Cloud-Native Computing","abstract":"Trusted execution environments (TEEs) protect the integrity and confidentiality of running code and its associated data. Nevertheless, TEEs' integrity protection does not extend to the state saved on disk. Furthermore, modern cloud-native applications heavily rely on orchestration (e.g., through systems such as Kubernetes) and, thus, have their services frequently restarted. During restarts, attackers can revert the state of confidential services to a previous version that may aid their malicious intent. This paper presents CRISP, a rollback protection mechanism that uses an existing runtime for Intel SGX and transparently prevents rollback. Our approach can constrain the attack window to a fixed and short period or give developers the tools to avoid the vulnerability window altogether. Finally, experiments show that applying CRISP in a critical stateful cloud-native application may incur a resource increase but only a minor performance penalty.","sentences":["Trusted execution environments (TEEs) protect the integrity and confidentiality of running code and its associated data.","Nevertheless, TEEs' integrity protection does not extend to the state saved on disk.","Furthermore, modern cloud-native applications heavily rely on orchestration (e.g., through systems such as Kubernetes) and, thus, have their services frequently restarted.","During restarts, attackers can revert the state of confidential services to a previous version that may aid their malicious intent.","This paper presents CRISP, a rollback protection mechanism that uses an existing runtime for Intel SGX and transparently prevents rollback.","Our approach can constrain the attack window to a fixed and short period or give developers the tools to avoid the vulnerability window altogether.","Finally, experiments show that applying CRISP in a critical stateful cloud-native application may incur a resource increase but only a minor performance penalty."],"url":"http://arxiv.org/abs/2408.06822v1"}
{"created":"2024-08-13 11:25:22","title":"Enhancing Multiview Synergy: Robust Learning by Exploiting the Wave Loss Function with Consensus and Complementarity Principles","abstract":"Multiview learning (MvL) is an advancing domain in machine learning, leveraging multiple data perspectives to enhance model performance through view-consistency and view-discrepancy. Despite numerous successful multiview-based SVM models, existing frameworks predominantly focus on the consensus principle, often overlooking the complementarity principle. Furthermore, they exhibit limited robustness against noisy, error-prone, and view-inconsistent samples, prevalent in multiview datasets. To tackle the aforementioned limitations, this paper introduces Wave-MvSVM, a novel multiview support vector machine framework leveraging the wave loss (W-loss) function, specifically designed to harness both consensus and complementarity principles. Unlike traditional approaches that often overlook the complementary information among different views, the proposed Wave-MvSVM ensures a more comprehensive and resilient learning process by integrating both principles effectively. The W-loss function, characterized by its smoothness, asymmetry, and bounded nature, is particularly effective in mitigating the adverse effects of noisy and outlier data, thereby enhancing model stability. Theoretically, the W-loss function also exhibits a crucial classification-calibrated property, further boosting its effectiveness. Wave-MvSVM employs a between-view co-regularization term to enforce view consistency and utilizes an adaptive combination weight strategy to maximize the discriminative power of each view. The optimization problem is efficiently solved using a combination of GD and the ADMM, ensuring reliable convergence to optimal solutions. Theoretical analyses, grounded in Rademacher complexity, validate the generalization capabilities of the Wave-MvSVM model. Extensive empirical evaluations across diverse datasets demonstrate the superior performance of Wave-MvSVM in comparison to existing benchmark models.","sentences":["Multiview learning (MvL) is an advancing domain in machine learning, leveraging multiple data perspectives to enhance model performance through view-consistency and view-discrepancy.","Despite numerous successful multiview-based SVM models, existing frameworks predominantly focus on the consensus principle, often overlooking the complementarity principle.","Furthermore, they exhibit limited robustness against noisy, error-prone, and view-inconsistent samples, prevalent in multiview datasets.","To tackle the aforementioned limitations, this paper introduces Wave-MvSVM, a novel multiview support vector machine framework leveraging the wave loss (W-loss) function, specifically designed to harness both consensus and complementarity principles.","Unlike traditional approaches that often overlook the complementary information among different views, the proposed Wave-MvSVM ensures a more comprehensive and resilient learning process by integrating both principles effectively.","The W-loss function, characterized by its smoothness, asymmetry, and bounded nature, is particularly effective in mitigating the adverse effects of noisy and outlier data, thereby enhancing model stability.","Theoretically, the W-loss function also exhibits a crucial classification-calibrated property, further boosting its effectiveness.","Wave-MvSVM employs a between-view co-regularization term to enforce view consistency and utilizes an adaptive combination weight strategy to maximize the discriminative power of each view.","The optimization problem is efficiently solved using a combination of GD and the ADMM, ensuring reliable convergence to optimal solutions.","Theoretical analyses, grounded in Rademacher complexity, validate the generalization capabilities of the Wave-MvSVM model.","Extensive empirical evaluations across diverse datasets demonstrate the superior performance of Wave-MvSVM in comparison to existing benchmark models."],"url":"http://arxiv.org/abs/2408.06819v1"}
{"created":"2024-08-13 11:17:31","title":"MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty","abstract":"Although large language models (LLMs) are capable of performing various tasks, they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on questions requiring a single clear answer, ignoring the existence of data uncertainty that arises from irreducible randomness. Instead, these methods only consider model uncertainty, which arises from a lack of knowledge. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that entropy and consistency-based methods estimate the model uncertainty well even under data uncertainty, while other methods for white- and black-box LLMs struggle depending on the tasks. Additionally, methods designed for white-box LLMs suffer from overconfidence in reasoning tasks compared to simple knowledge queries. We believe our observations will pave the way for future work on uncertainty quantification in realistic setting.","sentences":["Although large language models (LLMs) are capable of performing various tasks, they still suffer from producing plausible but incorrect responses.","To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not.","However, most uncertainty quantification methods have been evaluated on questions requiring a single clear answer, ignoring the existence of data uncertainty that arises from irreducible randomness.","Instead, these methods only consider model uncertainty, which arises from a lack of knowledge.","In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty.","Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs.","Our findings show that entropy and consistency-based methods estimate the model uncertainty well even under data uncertainty, while other methods for white- and black-box LLMs struggle depending on the tasks.","Additionally, methods designed for white-box LLMs suffer from overconfidence in reasoning tasks compared to simple knowledge queries.","We believe our observations will pave the way for future work on uncertainty quantification in realistic setting."],"url":"http://arxiv.org/abs/2408.06816v1"}
{"created":"2024-08-13 11:00:51","title":"Oracle Bone Script Similiar Character Screening Approach Based on Simsiam Contrastive Learning and Supervised Learning","abstract":"This project proposes a new method that uses fuzzy comprehensive evaluation method to integrate ResNet-50 self-supervised and RepVGG supervised learning. The source image dataset HWOBC oracle is taken as input, the target image is selected, and finally the most similar image is output in turn without any manual intervention. The same feature encoding method is not used for images of different modalities. Before the model training, the image data is preprocessed, and the image is enhanced by random rotation processing, self-square graph equalization theory algorithm, and gamma transform, which effectively enhances the key feature learning. Finally, the fuzzy comprehensive evaluation method is used to combine the results of supervised training and unsupervised training, which can better solve the \"most similar\" problem that is difficult to quantify. At present, there are many unknown oracle-bone inscriptions waiting for us to crack. Contacting with the glyphs can provide new ideas for cracking.","sentences":["This project proposes a new method that uses fuzzy comprehensive evaluation method to integrate ResNet-50 self-supervised and RepVGG supervised learning.","The source image dataset HWOBC oracle is taken as input, the target image is selected, and finally the most similar image is output in turn without any manual intervention.","The same feature encoding method is not used for images of different modalities.","Before the model training, the image data is preprocessed, and the image is enhanced by random rotation processing, self-square graph equalization theory algorithm, and gamma transform, which effectively enhances the key feature learning.","Finally, the fuzzy comprehensive evaluation method is used to combine the results of supervised training and unsupervised training, which can better solve the \"most similar\" problem that is difficult to quantify.","At present, there are many unknown oracle-bone inscriptions waiting for us to crack.","Contacting with the glyphs can provide new ideas for cracking."],"url":"http://arxiv.org/abs/2408.06811v1"}
{"created":"2024-08-13 10:15:55","title":"Unlock the Power of Frozen LLMs in Knowledge Graph Completion","abstract":"Classical knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large corpora with powerful context modeling, which is ideal for mitigating the limitations of previous methods. Directly fine-tuning LLMs offers great capability but comes at the cost of huge time and memory consumption, while utilizing frozen LLMs yields suboptimal results. In this work, we aim to leverage LLMs for KGC effectively and efficiently. We capture the context-aware hidden states of knowledge triples by employing prompts to stimulate the intermediate layers of LLMs. We then train a data-efficient classifier on these hidden states to harness the inherent capabilities of frozen LLMs in KGC. We also generate entity descriptions with subgraph sampling on KGs, reducing the ambiguity of triplets and enriching the knowledge representation. Extensive experiments on standard benchmarks showcase the efficiency and effectiveness of our approach. We outperform classical KGC methods on most datasets and match the performance of fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU memory efficiency by \\textbf{$188\\times$} and speed up training+inference by \\textbf{$13.48\\times$}.","sentences":["Classical knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs).","Large Language Models (LLMs) learn extensive knowledge from large corpora with powerful context modeling, which is ideal for mitigating the limitations of previous methods.","Directly fine-tuning LLMs offers great capability but comes at the cost of huge time and memory consumption, while utilizing frozen LLMs yields suboptimal results.","In this work, we aim to leverage LLMs for KGC effectively and efficiently.","We capture the context-aware hidden states of knowledge triples by employing prompts to stimulate the intermediate layers of LLMs.","We then train a data-efficient classifier on these hidden states to harness the inherent capabilities of frozen LLMs in KGC.","We also generate entity descriptions with subgraph sampling on KGs, reducing the ambiguity of triplets and enriching the knowledge representation.","Extensive experiments on standard benchmarks showcase the efficiency and effectiveness of our approach.","We outperform classical KGC methods on most datasets and match the performance of fine-tuned LLMs.","Additionally, compared to fine-tuned LLMs, we boost GPU memory efficiency by \\textbf{$188\\times$} and speed up training+inference by \\textbf{$13.48\\times$}."],"url":"http://arxiv.org/abs/2408.06787v1"}
{"created":"2024-08-13 10:06:53","title":"Do Vision-Language Foundational models show Robust Visual Perception?","abstract":"Recent advances in vision-language foundational models have enabled development of systems that can perform visual understanding and reasoning tasks. However, it is unclear if these models are robust to distribution shifts, and how their performance and generalization capabilities vary under changes in data distribution. In this project we strive to answer the question \"Are vision-language foundational models robust to distribution shifts like human perception?\" Specifically, we consider a diverse range of vision-language models and compare how the performance of these systems is affected by corruption based distribution shifts (such as \\textit{motion blur, fog, snow, gaussian noise}) commonly found in practical real-world scenarios. We analyse the generalization capabilities qualitatively and quantitatively on zero-shot image classification task under aforementioned distribution shifts. Our code will be avaible at \\url{https://github.com/shivam-chandhok/CPSC-540-Project}","sentences":["Recent advances in vision-language foundational models have enabled development of systems that can perform visual understanding and reasoning tasks.","However, it is unclear if these models are robust to distribution shifts, and how their performance and generalization capabilities vary under changes in data distribution.","In this project we strive to answer the question \"Are vision-language foundational models robust to distribution shifts like human perception?\"","Specifically, we consider a diverse range of vision-language models and compare how the performance of these systems is affected by corruption based distribution shifts (such as \\textit{motion blur, fog, snow, gaussian noise}) commonly found in practical real-world scenarios.","We analyse the generalization capabilities qualitatively and quantitatively on zero-shot image classification task under aforementioned distribution shifts.","Our code will be avaible at \\url{https://github.com/shivam-chandhok/CPSC-540-Project}"],"url":"http://arxiv.org/abs/2408.06781v1"}
{"created":"2024-08-13 10:05:20","title":"ED$^4$: Explicit Data-level Debiasing for Deepfake Detection","abstract":"Learning intrinsic bias from limited data has been considered the main reason for the failure of deepfake detection with generalizability. Apart from the discovered content and specific-forgery bias, we reveal a novel spatial bias, where detectors inertly anticipate observing structural forgery clues appearing at the image center, also can lead to the poor generalization of existing methods. We present ED$^4$, a simple and effective strategy, to address aforementioned biases explicitly at the data level in a unified framework rather than implicit disentanglement via network design. In particular, we develop ClockMix to produce facial structure preserved mixtures with arbitrary samples, which allows the detector to learn from an exponentially extended data distribution with much more diverse identities, backgrounds, local manipulation traces, and the co-occurrence of multiple forgery artifacts. We further propose the Adversarial Spatial Consistency Module (AdvSCM) to prevent extracting features with spatial bias, which adversarially generates spatial-inconsistent images and constrains their extracted feature to be consistent. As a model-agnostic debiasing strategy, ED$^4$ is plug-and-play: it can be integrated with various deepfake detectors to obtain significant benefits. We conduct extensive experiments to demonstrate its effectiveness and superiority over existing deepfake detection approaches.","sentences":["Learning intrinsic bias from limited data has been considered the main reason for the failure of deepfake detection with generalizability.","Apart from the discovered content and specific-forgery bias, we reveal a novel spatial bias, where detectors inertly anticipate observing structural forgery clues appearing at the image center, also can lead to the poor generalization of existing methods.","We present ED$^4$, a simple and effective strategy, to address aforementioned biases explicitly at the data level in a unified framework rather than implicit disentanglement via network design.","In particular, we develop ClockMix to produce facial structure preserved mixtures with arbitrary samples, which allows the detector to learn from an exponentially extended data distribution with much more diverse identities, backgrounds, local manipulation traces, and the co-occurrence of multiple forgery artifacts.","We further propose the Adversarial Spatial Consistency Module (AdvSCM) to prevent extracting features with spatial bias, which adversarially generates spatial-inconsistent images and constrains their extracted feature to be consistent.","As a model-agnostic debiasing strategy, ED$^4$ is plug-and-play: it can be integrated with various deepfake detectors to obtain significant benefits.","We conduct extensive experiments to demonstrate its effectiveness and superiority over existing deepfake detection approaches."],"url":"http://arxiv.org/abs/2408.06779v1"}
{"created":"2024-08-13 09:55:38","title":"Exploring Domain Shift on Radar-Based 3D Object Detection Amidst Diverse Environmental Conditions","abstract":"The rapid evolution of deep learning and its integration with autonomous driving systems have led to substantial advancements in 3D perception using multimodal sensors. Notably, radar sensors show greater robustness compared to cameras and lidar under adverse weather and varying illumination conditions. This study delves into the often-overlooked yet crucial issue of domain shift in 4D radar-based object detection, examining how varying environmental conditions, such as different weather patterns and road types, impact 3D object detection performance. Our findings highlight distinct domain shifts across various weather scenarios, revealing unique dataset sensitivities that underscore the critical role of radar point cloud generation. Additionally, we demonstrate that transitioning between different road types, especially from highways to urban settings, introduces notable domain shifts, emphasizing the necessity for diverse data collection across varied road environments. To the best of our knowledge, this is the first comprehensive analysis of domain shift effects on 4D radar-based object detection. We believe this empirical study contributes to understanding the complex nature of domain shifts in radar data and suggests paths forward for data collection strategy in the face of environmental variability.","sentences":["The rapid evolution of deep learning and its integration with autonomous driving systems have led to substantial advancements in 3D perception using multimodal sensors.","Notably, radar sensors show greater robustness compared to cameras and lidar under adverse weather and varying illumination conditions.","This study delves into the often-overlooked yet crucial issue of domain shift in 4D radar-based object detection, examining how varying environmental conditions, such as different weather patterns and road types, impact 3D object detection performance.","Our findings highlight distinct domain shifts across various weather scenarios, revealing unique dataset sensitivities that underscore the critical role of radar point cloud generation.","Additionally, we demonstrate that transitioning between different road types, especially from highways to urban settings, introduces notable domain shifts, emphasizing the necessity for diverse data collection across varied road environments.","To the best of our knowledge, this is the first comprehensive analysis of domain shift effects on 4D radar-based object detection.","We believe this empirical study contributes to understanding the complex nature of domain shifts in radar data and suggests paths forward for data collection strategy in the face of environmental variability."],"url":"http://arxiv.org/abs/2408.06772v1"}
{"created":"2024-08-13 09:37:26","title":"Cross-View Geolocalization and Disaster Mapping with Street-View and VHR Satellite Imagery: A Case Study of Hurricane IAN","abstract":"Nature disasters play a key role in shaping human-urban infrastructure interactions. Effective and efficient response to natural disasters is essential for building resilience and a sustainable urban environment. Two types of information are usually the most necessary and difficult to gather in disaster response. The first information is about disaster damage perception, which shows how badly people think that urban infrastructure has been damaged. The second information is geolocation awareness, which means how people whereabouts are made available. In this paper, we proposed a novel disaster mapping framework, namely CVDisaster, aiming at simultaneously addressing geolocalization and damage perception estimation using cross-view Street-View Imagery (SVI) and Very High-Resolution satellite imagery. CVDisaster consists of two cross-view models, where CVDisaster-Geoloc refers to a cross-view geolocalization model based on a contrastive learning objective with a Siamese ConvNeXt image encoder, and CVDisaster-Est is a cross-view classification model based on a Couple Global Context Vision Transformer (CGCViT). Taking Hurricane IAN as a case study, we evaluate the CVDisaster framework by creating a novel cross-view dataset (CVIAN) and conducting extensive experiments. As a result, we show that CVDisaster can achieve highly competitive performance (over 80% for geolocalization and 75% for damage perception estimation) with even limited fine-tuning efforts, which largely motivates future cross-view models and applications within a broader GeoAI research community. The data and code are publicly available at: https://github.com/tum-bgd/CVDisaster.","sentences":["Nature disasters play a key role in shaping human-urban infrastructure interactions.","Effective and efficient response to natural disasters is essential for building resilience and a sustainable urban environment.","Two types of information are usually the most necessary and difficult to gather in disaster response.","The first information is about disaster damage perception, which shows how badly people think that urban infrastructure has been damaged.","The second information is geolocation awareness, which means how people whereabouts are made available.","In this paper, we proposed a novel disaster mapping framework, namely CVDisaster, aiming at simultaneously addressing geolocalization and damage perception estimation using cross-view Street-View Imagery (SVI) and Very High-Resolution satellite imagery.","CVDisaster consists of two cross-view models, where CVDisaster-Geoloc refers to a cross-view geolocalization model based on a contrastive learning objective with a Siamese ConvNeXt image encoder, and CVDisaster-Est is a cross-view classification model based on a Couple Global Context Vision Transformer (CGCViT).","Taking Hurricane IAN as a case study, we evaluate the CVDisaster framework by creating a novel cross-view dataset (CVIAN) and conducting extensive experiments.","As a result, we show that CVDisaster can achieve highly competitive performance (over 80% for geolocalization and 75% for damage perception estimation) with even limited fine-tuning efforts, which largely motivates future cross-view models and applications within a broader GeoAI research community.","The data and code are publicly available at: https://github.com/tum-bgd/CVDisaster."],"url":"http://arxiv.org/abs/2408.06761v1"}
{"created":"2024-08-13 09:19:59","title":"Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies","abstract":"Existing methods on audio-visual deepfake detection mainly focus on high-level features for modeling inconsistencies between audio and visual data. As a result, these approaches usually overlook finer audio-visual artifacts, which are inherent to deepfakes. Herein, we propose the introduction of fine-grained mechanisms for detecting subtle artifacts in both spatial and temporal domains. First, we introduce a local audio-visual model capable of capturing small spatial regions that are prone to inconsistencies with audio. For that purpose, a fine-grained mechanism based on a spatially-local distance coupled with an attention module is adopted. Second, we introduce a temporally-local pseudo-fake augmentation to include samples incorporating subtle temporal inconsistencies in our training set. Experiments on the DFDC and the FakeAVCeleb datasets demonstrate the superiority of the proposed method in terms of generalization as compared to the state-of-the-art under both in-dataset and cross-dataset settings.","sentences":["Existing methods on audio-visual deepfake detection mainly focus on high-level features for modeling inconsistencies between audio and visual data.","As a result, these approaches usually overlook finer audio-visual artifacts, which are inherent to deepfakes.","Herein, we propose the introduction of fine-grained mechanisms for detecting subtle artifacts in both spatial and temporal domains.","First, we introduce a local audio-visual model capable of capturing small spatial regions that are prone to inconsistencies with audio.","For that purpose, a fine-grained mechanism based on a spatially-local distance coupled with an attention module is adopted.","Second, we introduce a temporally-local pseudo-fake augmentation to include samples incorporating subtle temporal inconsistencies in our training set.","Experiments on the DFDC and the FakeAVCeleb datasets demonstrate the superiority of the proposed method in terms of generalization as compared to the state-of-the-art under both in-dataset and cross-dataset settings."],"url":"http://arxiv.org/abs/2408.06753v1"}
{"created":"2024-08-13 09:04:47","title":"Class-aware and Augmentation-free Contrastive Learning from Label Proportion","abstract":"Learning from Label Proportion (LLP) is a weakly supervised learning scenario in which training data is organized into predefined bags of instances, disclosing only the class label proportions per bag. This paradigm is essential for user modeling and personalization, where user privacy is paramount, offering insights into user preferences without revealing individual data. LLP faces a unique difficulty: the misalignment between bag-level supervision and the objective of instance-level prediction, primarily due to the inherent ambiguity in label proportion matching. Previous studies have demonstrated deep representation learning can generate auxiliary signals to promote the supervision level in the image domain. However, applying these techniques to tabular data presents significant challenges: 1) they rely heavily on label-invariant augmentation to establish multi-view, which is not feasible with the heterogeneous nature of tabular datasets, and 2) tabular datasets often lack sufficient semantics for perfect class distinction, making them prone to suboptimality caused by the inherent ambiguity of label proportion matching.   To address these challenges, we propose an augmentation-free contrastive framework TabLLP-BDC that introduces class-aware supervision (explicitly aware of class differences) at the instance level. Our solution features a two-stage Bag Difference Contrastive (BDC) learning mechanism that establishes robust class-aware instance-level supervision by disassembling the nuance between bag label proportions, without relying on augmentations. Concurrently, our model presents a pioneering multi-task pretraining pipeline tailored for tabular-based LLP, capturing intrinsic tabular feature correlations in alignment with label proportion distribution. Extensive experiments demonstrate that TabLLP-BDC achieves state-of-the-art performance for LLP in the tabular domain.","sentences":["Learning from Label Proportion (LLP) is a weakly supervised learning scenario in which training data is organized into predefined bags of instances, disclosing only the class label proportions per bag.","This paradigm is essential for user modeling and personalization, where user privacy is paramount, offering insights into user preferences without revealing individual data.","LLP faces a unique difficulty: the misalignment between bag-level supervision and the objective of instance-level prediction, primarily due to the inherent ambiguity in label proportion matching.","Previous studies have demonstrated deep representation learning can generate auxiliary signals to promote the supervision level in the image domain.","However, applying these techniques to tabular data presents significant challenges: 1) they rely heavily on label-invariant augmentation to establish multi-view, which is not feasible with the heterogeneous nature of tabular datasets, and 2) tabular datasets often lack sufficient semantics for perfect class distinction, making them prone to suboptimality caused by the inherent ambiguity of label proportion matching.   ","To address these challenges, we propose an augmentation-free contrastive framework TabLLP-BDC that introduces class-aware supervision (explicitly aware of class differences) at the instance level.","Our solution features a two-stage Bag Difference Contrastive (BDC) learning mechanism that establishes robust class-aware instance-level supervision by disassembling the nuance between bag label proportions, without relying on augmentations.","Concurrently, our model presents a pioneering multi-task pretraining pipeline tailored for tabular-based LLP, capturing intrinsic tabular feature correlations in alignment with label proportion distribution.","Extensive experiments demonstrate that TabLLP-BDC achieves state-of-the-art performance for LLP in the tabular domain."],"url":"http://arxiv.org/abs/2408.06743v1"}
{"created":"2024-08-13 09:03:00","title":"Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to Tail","abstract":"Current out-of-distribution (OOD) detection methods typically assume balanced in-distribution (ID) data, while most real-world data follow a long-tailed distribution. Previous approaches to long-tailed OOD detection often involve balancing the ID data by reducing the semantics of head classes. However, this reduction can severely affect the classification accuracy of ID data. The main challenge of this task lies in the severe lack of features for tail classes, leading to confusion with OOD data. To tackle this issue, we introduce a novel Prioritizing Attention to Tail (PATT) method using augmentation instead of reduction. Our main intuition involves using a mixture of von Mises-Fisher (vMF) distributions to model the ID data and a temperature scaling module to boost the confidence of ID data. This enables us to generate infinite contrastive pairs, implicitly enhancing the semantics of ID classes while promoting differentiation between ID and OOD data. To further strengthen the detection of OOD data without compromising the classification performance of ID data, we propose feature calibration during the inference phase. By extracting an attention weight from the training set that prioritizes the tail classes and reduces the confidence in OOD data, we improve the OOD detection capability. Extensive experiments verified that our method outperforms the current state-of-the-art methods on various benchmarks.","sentences":["Current out-of-distribution (OOD) detection methods typically assume balanced in-distribution (ID) data, while most real-world data follow a long-tailed distribution.","Previous approaches to long-tailed OOD detection often involve balancing the ID data by reducing the semantics of head classes.","However, this reduction can severely affect the classification accuracy of ID data.","The main challenge of this task lies in the severe lack of features for tail classes, leading to confusion with OOD data.","To tackle this issue, we introduce a novel Prioritizing Attention to Tail (PATT) method using augmentation instead of reduction.","Our main intuition involves using a mixture of von Mises-Fisher (vMF) distributions to model the ID data and a temperature scaling module to boost the confidence of ID data.","This enables us to generate infinite contrastive pairs, implicitly enhancing the semantics of ID classes while promoting differentiation between ID and OOD data.","To further strengthen the detection of OOD data without compromising the classification performance of ID data, we propose feature calibration during the inference phase.","By extracting an attention weight from the training set that prioritizes the tail classes and reduces the confidence in OOD data, we improve the OOD detection capability.","Extensive experiments verified that our method outperforms the current state-of-the-art methods on various benchmarks."],"url":"http://arxiv.org/abs/2408.06742v1"}
{"created":"2024-08-13 09:01:12","title":"Improving Synthetic Image Detection Towards Generalization: An Image Transformation Perspective","abstract":"With recent generative models facilitating photo-realistic image synthesis, the proliferation of synthetic images has also engendered certain negative impacts on social platforms, thereby raising an urgent imperative to develop effective detectors. Current synthetic image detection (SID) pipelines are primarily dedicated to crafting universal artifact features, accompanied by an oversight about SID training paradigm. In this paper, we re-examine the SID problem and identify two prevalent biases in current training paradigms, i.e., weakened artifact features and overfitted artifact features. Meanwhile, we discover that the imaging mechanism of synthetic images contributes to heightened local correlations among pixels, suggesting that detectors should be equipped with local awareness. In this light, we propose SAFE, a lightweight and effective detector with three simple image transformations. Firstly, for weakened artifact features, we substitute the down-sampling operator with the crop operator in image pre-processing to help circumvent artifact distortion. Secondly, for overfitted artifact features, we include ColorJitter and RandomRotation as additional data augmentations, to help alleviate irrelevant biases from color discrepancies and semantic differences in limited training samples. Thirdly, for local awareness, we propose a patch-based random masking strategy tailored for SID, forcing the detector to focus on local regions at training. Comparative experiments are conducted on an open-world dataset, comprising synthetic images generated by 26 distinct generative models. Our pipeline achieves a new state-of-the-art performance, with remarkable improvements of 4.5% in accuracy and 2.9% in average precision against existing methods.","sentences":["With recent generative models facilitating photo-realistic image synthesis, the proliferation of synthetic images has also engendered certain negative impacts on social platforms, thereby raising an urgent imperative to develop effective detectors.","Current synthetic image detection (SID) pipelines are primarily dedicated to crafting universal artifact features, accompanied by an oversight about SID training paradigm.","In this paper, we re-examine the SID problem and identify two prevalent biases in current training paradigms, i.e., weakened artifact features and overfitted artifact features.","Meanwhile, we discover that the imaging mechanism of synthetic images contributes to heightened local correlations among pixels, suggesting that detectors should be equipped with local awareness.","In this light, we propose SAFE, a lightweight and effective detector with three simple image transformations.","Firstly, for weakened artifact features, we substitute the down-sampling operator with the crop operator in image pre-processing to help circumvent artifact distortion.","Secondly, for overfitted artifact features, we include ColorJitter and RandomRotation as additional data augmentations, to help alleviate irrelevant biases from color discrepancies and semantic differences in limited training samples.","Thirdly, for local awareness, we propose a patch-based random masking strategy tailored for SID, forcing the detector to focus on local regions at training.","Comparative experiments are conducted on an open-world dataset, comprising synthetic images generated by 26 distinct generative models.","Our pipeline achieves a new state-of-the-art performance, with remarkable improvements of 4.5% in accuracy and 2.9% in average precision against existing methods."],"url":"http://arxiv.org/abs/2408.06741v1"}
{"created":"2024-08-13 08:50:35","title":"Grasping by Hanging: a Learning-Free Grasping Detection Method for Previously Unseen Objects","abstract":"This paper proposes a novel learning-free three-stage method that predicts grasping poses, enabling robots to pick up and transfer previously unseen objects. Our method first identifies potential structures that can afford the action of hanging by analyzing the hanging mechanics and geometric properties. Then 6D poses are detected for a parallel gripper retrofitted with an extending bar, which when closed forms loops to hook each hangable structure. Finally, an evaluation policy qualities and rank grasp candidates for execution attempts. Compared to the traditional physical model-based and deep learning-based methods, our approach is closer to the human natural action of grasping unknown objects. And it also eliminates the need for a vast amount of training data. To evaluate the effectiveness of the proposed method, we conducted experiments with a real robot. Experimental results indicate that the grasping accuracy and stability are significantly higher than the state-of-the-art learning-based method, especially for thin and flat objects.","sentences":["This paper proposes a novel learning-free three-stage method that predicts grasping poses, enabling robots to pick up and transfer previously unseen objects.","Our method first identifies potential structures that can afford the action of hanging by analyzing the hanging mechanics and geometric properties.","Then 6D poses are detected for a parallel gripper retrofitted with an extending bar, which when closed forms loops to hook each hangable structure.","Finally, an evaluation policy qualities and rank grasp candidates for execution attempts.","Compared to the traditional physical model-based and deep learning-based methods, our approach is closer to the human natural action of grasping unknown objects.","And it also eliminates the need for a vast amount of training data.","To evaluate the effectiveness of the proposed method, we conducted experiments with a real robot.","Experimental results indicate that the grasping accuracy and stability are significantly higher than the state-of-the-art learning-based method, especially for thin and flat objects."],"url":"http://arxiv.org/abs/2408.06734v1"}
{"created":"2024-08-13 08:32:06","title":"Adaptive Data Quality Scoring Operations Framework using Drift-Aware Mechanism for Industrial Applications","abstract":"Within data-driven artificial intelligence (AI) systems for industrial applications, ensuring the reliability of the incoming data streams is an integral part of trustworthy decision-making. An approach to assess data validity is data quality scoring, which assigns a score to each data point or stream based on various quality dimensions. However, certain dimensions exhibit dynamic qualities, which require adaptation on the basis of the system's current conditions. Existing methods often overlook this aspect, making them inefficient in dynamic production environments. In this paper, we introduce the Adaptive Data Quality Scoring Operations Framework, a novel framework developed to address the challenges posed by dynamic quality dimensions in industrial data streams. The framework introduces an innovative approach by integrating a dynamic change detector mechanism that actively monitors and adapts to changes in data quality, ensuring the relevance of quality scores. We evaluate the proposed framework performance in a real-world industrial use case. The experimental results reveal high predictive performance and efficient processing time, highlighting its effectiveness in practical quality-driven AI applications.","sentences":["Within data-driven artificial intelligence (AI) systems for industrial applications, ensuring the reliability of the incoming data streams is an integral part of trustworthy decision-making.","An approach to assess data validity is data quality scoring, which assigns a score to each data point or stream based on various quality dimensions.","However, certain dimensions exhibit dynamic qualities, which require adaptation on the basis of the system's current conditions.","Existing methods often overlook this aspect, making them inefficient in dynamic production environments.","In this paper, we introduce the Adaptive Data Quality Scoring Operations Framework, a novel framework developed to address the challenges posed by dynamic quality dimensions in industrial data streams.","The framework introduces an innovative approach by integrating a dynamic change detector mechanism that actively monitors and adapts to changes in data quality, ensuring the relevance of quality scores.","We evaluate the proposed framework performance in a real-world industrial use case.","The experimental results reveal high predictive performance and efficient processing time, highlighting its effectiveness in practical quality-driven AI applications."],"url":"http://arxiv.org/abs/2408.06724v1"}
{"created":"2024-08-13 08:30:52","title":"Sustaining Maintenance Labor for Healthy Open Source Software Projects through Human Infrastructure: A Maintainer Perspective","abstract":"Background: Open Source Software (OSS) fuels our global digital infrastructure but is commonly maintained by small groups of people whose time and labor represent a depletable resource. For the OSS projects to stay sustainable, i.e., viable and maintained over time without interruption or weakening, maintenance labor requires an underlying infrastructure to be supported and secured. Aims: Using the construct of human infrastructure, our study aims to investigate how maintenance labor can be supported and secured to enable the creation and maintenance of sustainable OSS projects, viewed from the maintainers' perspective. Method: In our exploration, we interviewed ten maintainers from nine well-adopted OSS projects. We coded the data in two steps using investigator-triangulation. Results: We constructed a framework of infrastructure design that provide insight for OSS projects in the design of their human infrastructure. The framework specifically highlight the importance of human factors, e.g., securing a work-life balance and proactively managing social pressure, toxicity, and diversity. We also note both differences and overlaps in how the infrastructure needs to support and secure maintenance labor from maintainers and the wider OSS community, respectively. Funding is specifically highlighted as an important enabler for both types of resources. Conclusions: The study contributes to the qualitative understanding of the importance, sensitivity, and risk for depletion of the maintenance labor required to build and maintain healthy OSS projects. Human infrastructure is pivotal in ensuring that maintenance labor is sustainable, and by extension the OSS projects on which we all depend.","sentences":["Background: Open Source Software (OSS) fuels our global digital infrastructure but is commonly maintained by small groups of people whose time and labor represent a depletable resource.","For the OSS projects to stay sustainable, i.e., viable and maintained over time without interruption or weakening, maintenance labor requires an underlying infrastructure to be supported and secured.","Aims:","Using the construct of human infrastructure, our study aims to investigate how maintenance labor can be supported and secured to enable the creation and maintenance of sustainable OSS projects, viewed from the maintainers' perspective.","Method:","In our exploration, we interviewed ten maintainers from nine well-adopted OSS projects.","We coded the data in two steps using investigator-triangulation.","Results:","We constructed a framework of infrastructure design that provide insight for OSS projects in the design of their human infrastructure.","The framework specifically highlight the importance of human factors, e.g., securing a work-life balance and proactively managing social pressure, toxicity, and diversity.","We also note both differences and overlaps in how the infrastructure needs to support and secure maintenance labor from maintainers and the wider OSS community, respectively.","Funding is specifically highlighted as an important enabler for both types of resources.","Conclusions: The study contributes to the qualitative understanding of the importance, sensitivity, and risk for depletion of the maintenance labor required to build and maintain healthy OSS projects.","Human infrastructure is pivotal in ensuring that maintenance labor is sustainable, and by extension the OSS projects on which we all depend."],"url":"http://arxiv.org/abs/2408.06723v1"}
{"created":"2024-08-13 08:24:52","title":"Multimodal Analysis of White Blood Cell Differentiation in Acute Myeloid Leukemia Patients using a \u03b2-Variational Autoencoder","abstract":"Biomedical imaging and RNA sequencing with single-cell resolution improves our understanding of white blood cell diseases like leukemia. By combining morphological and transcriptomic data, we can gain insights into cellular functions and trajectoriess involved in blood cell differentiation. However, existing methodologies struggle with integrating morphological and transcriptomic data, leaving a significant research gap in comprehensively understanding the dynamics of cell differentiation. Here, we introduce an unsupervised method that explores and reconstructs these two modalities and uncovers the relationship between different subtypes of white blood cells from human peripheral blood smears in terms of morphology and their corresponding transcriptome. Our method is based on a beta-variational autoencoder (\\beta-VAE) with a customized loss function, incorporating a R-CNN architecture to distinguish single-cell from background and to minimize any interference from artifacts. This implementation of \\beta-VAE shows good reconstruction capability along with continuous latent embeddings, while maintaining clear differentiation between single-cell classes. Our novel approach is especially helpful to uncover the correlation of two latent features in complex biological processes such as formation of granules in the cell (granulopoiesis) with gene expression patterns. It thus provides a unique tool to improve the understanding of white blood cell maturation for biomedicine and diagnostics.","sentences":["Biomedical imaging and RNA sequencing with single-cell resolution improves our understanding of white blood cell diseases like leukemia.","By combining morphological and transcriptomic data, we can gain insights into cellular functions and trajectoriess involved in blood cell differentiation.","However, existing methodologies struggle with integrating morphological and transcriptomic data, leaving a significant research gap in comprehensively understanding the dynamics of cell differentiation.","Here, we introduce an unsupervised method that explores and reconstructs these two modalities and uncovers the relationship between different subtypes of white blood cells from human peripheral blood smears in terms of morphology and their corresponding transcriptome.","Our method is based on a beta-variational autoencoder (\\beta-VAE) with a customized loss function, incorporating a R-CNN architecture to distinguish single-cell from background and to minimize any interference from artifacts.","This implementation of \\beta-VAE shows good reconstruction capability along with continuous latent embeddings, while maintaining clear differentiation between single-cell classes.","Our novel approach is especially helpful to uncover the correlation of two latent features in complex biological processes such as formation of granules in the cell (granulopoiesis) with gene expression patterns.","It thus provides a unique tool to improve the understanding of white blood cell maturation for biomedicine and diagnostics."],"url":"http://arxiv.org/abs/2408.06720v1"}
{"created":"2024-08-13 08:22:01","title":"Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models","abstract":"Graph Neural Networks (GNNs), like other neural networks, have shown remarkable success but are hampered by the complexity of their architecture designs, which heavily depend on specific data and tasks. Traditionally, designing proper architectures involves trial and error, which requires intensive manual effort to optimize various components. To reduce human workload, researchers try to develop automated algorithms to design GNNs. However, both experts and automated algorithms suffer from two major issues in designing GNNs: 1) the substantial computational resources expended in repeatedly trying candidate GNN architectures until a feasible design is achieved, and 2) the intricate and prolonged processes required for humans or algorithms to accumulate knowledge of the interrelationship between graphs, GNNs, and performance.   To further enhance the automation of GNN architecture design, we propose a computation-friendly way to empower Large Language Models (LLMs) with specialized knowledge in designing GNNs, thereby drastically shortening the computational overhead and development cycle of designing GNN architectures. Our framework begins by establishing a knowledge retrieval pipeline that comprehends the intercorrelations between graphs, GNNs, and performance. This pipeline converts past model design experiences into structured knowledge for LLM reference, allowing it to quickly suggest initial model proposals. Subsequently, we introduce a knowledge-driven search strategy that emulates the exploration-exploitation process of human experts, enabling quick refinement of initial proposals within a promising scope. Extensive experiments demonstrate that our framework can efficiently deliver promising (e.g., Top-5.77%) initial model proposals for unseen datasets within seconds and without any prior training and achieve outstanding search performance in a few iterations.","sentences":["Graph Neural Networks (GNNs), like other neural networks, have shown remarkable success but are hampered by the complexity of their architecture designs, which heavily depend on specific data and tasks.","Traditionally, designing proper architectures involves trial and error, which requires intensive manual effort to optimize various components.","To reduce human workload, researchers try to develop automated algorithms to design GNNs.","However, both experts and automated algorithms suffer from two major issues in designing GNNs: 1) the substantial computational resources expended in repeatedly trying candidate GNN architectures until a feasible design is achieved, and 2) the intricate and prolonged processes required for humans or algorithms to accumulate knowledge of the interrelationship between graphs, GNNs, and performance.   ","To further enhance the automation of GNN architecture design, we propose a computation-friendly way to empower Large Language Models (LLMs) with specialized knowledge in designing GNNs, thereby drastically shortening the computational overhead and development cycle of designing GNN architectures.","Our framework begins by establishing a knowledge retrieval pipeline that comprehends the intercorrelations between graphs, GNNs, and performance.","This pipeline converts past model design experiences into structured knowledge for LLM reference, allowing it to quickly suggest initial model proposals.","Subsequently, we introduce a knowledge-driven search strategy that emulates the exploration-exploitation process of human experts, enabling quick refinement of initial proposals within a promising scope.","Extensive experiments demonstrate that our framework can efficiently deliver promising (e.g., Top-5.77%) initial model proposals for unseen datasets within seconds and without any prior training and achieve outstanding search performance in a few iterations."],"url":"http://arxiv.org/abs/2408.06717v1"}
{"created":"2024-08-13 08:09:05","title":"Variational Learning of Gaussian Process Latent Variable Models through Stochastic Gradient Annealed Importance Sampling","abstract":"Gaussian Process Latent Variable Models (GPLVMs) have become increasingly popular for unsupervised tasks such as dimensionality reduction and missing data recovery due to their flexibility and non-linear nature. An importance-weighted version of the Bayesian GPLVMs has been proposed to obtain a tighter variational bound. However, this version of the approach is primarily limited to analyzing simple data structures, as the generation of an effective proposal distribution can become quite challenging in high-dimensional spaces or with complex data sets. In this work, we propose an Annealed Importance Sampling (AIS) approach to address these issues. By transforming the posterior into a sequence of intermediate distributions using annealing, we combine the strengths of Sequential Monte Carlo samplers and VI to explore a wider range of posterior distributions and gradually approach the target distribution. We further propose an efficient algorithm by reparameterizing all variables in the evidence lower bound (ELBO). Experimental results on both toy and image datasets demonstrate that our method outperforms state-of-the-art methods in terms of tighter variational bounds, higher log-likelihoods, and more robust convergence.","sentences":["Gaussian Process Latent Variable Models (GPLVMs) have become increasingly popular for unsupervised tasks such as dimensionality reduction and missing data recovery due to their flexibility and non-linear nature.","An importance-weighted version of the Bayesian GPLVMs has been proposed to obtain a tighter variational bound.","However, this version of the approach is primarily limited to analyzing simple data structures, as the generation of an effective proposal distribution can become quite challenging in high-dimensional spaces or with complex data sets.","In this work, we propose an Annealed Importance Sampling (AIS) approach to address these issues.","By transforming the posterior into a sequence of intermediate distributions using annealing, we combine the strengths of Sequential Monte Carlo samplers and VI to explore a wider range of posterior distributions and gradually approach the target distribution.","We further propose an efficient algorithm by reparameterizing all variables in the evidence lower bound (ELBO).","Experimental results on both toy and image datasets demonstrate that our method outperforms state-of-the-art methods in terms of tighter variational bounds, higher log-likelihoods, and more robust convergence."],"url":"http://arxiv.org/abs/2408.06710v1"}
{"created":"2024-08-13 07:56:21","title":"DiffSG: A Generative Solver for Network Optimization with Diffusion Model","abstract":"Diffusion generative models, famous for their performance in image generation, are popular in various cross-domain applications. However, their use in the communication community has been mostly limited to auxiliary tasks like data modeling and feature extraction. These models hold greater promise for fundamental problems in network optimization compared to traditional machine learning methods. Discriminative deep learning often falls short due to its single-step input-output mapping and lack of global awareness of the solution space, especially given the complexity of network optimization's objective functions. In contrast, diffusion generative models can consider a broader range of solutions and exhibit stronger generalization by learning parameters that describe the distribution of the underlying solution space, with higher probabilities assigned to better solutions. We propose a new framework Diffusion Model-based Solution Generation (DiffSG), which leverages the intrinsic distribution learning capabilities of diffusion generative models to learn high-quality solution distributions based on given inputs. The optimal solution within this distribution is highly probable, allowing it to be effectively reached through repeated sampling. We validate the performance of DiffSG on several typical network optimization problems, including mixed-integer non-linear programming, convex optimization, and hierarchical non-convex optimization. Our results show that DiffSG outperforms existing baselines. In summary, we demonstrate the potential of diffusion generative models in tackling complex network optimization problems and outline a promising path for their broader application in the communication community.","sentences":["Diffusion generative models, famous for their performance in image generation, are popular in various cross-domain applications.","However, their use in the communication community has been mostly limited to auxiliary tasks like data modeling and feature extraction.","These models hold greater promise for fundamental problems in network optimization compared to traditional machine learning methods.","Discriminative deep learning often falls short due to its single-step input-output mapping and lack of global awareness of the solution space, especially given the complexity of network optimization's objective functions.","In contrast, diffusion generative models can consider a broader range of solutions and exhibit stronger generalization by learning parameters that describe the distribution of the underlying solution space, with higher probabilities assigned to better solutions.","We propose a new framework Diffusion Model-based Solution Generation (DiffSG), which leverages the intrinsic distribution learning capabilities of diffusion generative models to learn high-quality solution distributions based on given inputs.","The optimal solution within this distribution is highly probable, allowing it to be effectively reached through repeated sampling.","We validate the performance of DiffSG on several typical network optimization problems, including mixed-integer non-linear programming, convex optimization, and hierarchical non-convex optimization.","Our results show that DiffSG outperforms existing baselines.","In summary, we demonstrate the potential of diffusion generative models in tackling complex network optimization problems and outline a promising path for their broader application in the communication community."],"url":"http://arxiv.org/abs/2408.06701v1"}
{"created":"2024-08-13 07:24:53","title":"Faster Lattice Basis Computation -- The Generalization of the Euclidean Algorithm","abstract":"The Euclidean algorithm the oldest algorithms known to mankind. Given two integral numbers $a_1$ and $a_2$, it computes the greatest common divisor (gcd) of $a_1$ and $a_2$ in a very elegant way. From a lattice perspective, it computes a basis of the sum of two one-dimensional lattices $a_1 \\mathbb{Z}$ and $a_2 \\mathbb{Z}$ as $\\gcd(a_1,a_2) \\mathbb{Z} = a_1 \\mathbb{Z} + a_2 \\mathbb{Z}$. In this paper, we show that the classical Euclidean algorithm can be adapted in a very natural way to compute a basis of a general lattice $L (A_1, \\ldots , A_n)$ given vectors $A_1, \\ldots , A_n \\in \\mathbb{Z}^d$ with $n> \\mathrm{rank}(a_1, \\ldots ,a_d)$. Similar to the Euclidean algorithm, our algorithm is very easy to describe and implement and can be written within 12 lines of pseudocode.   Our generalized version of the Euclidean algorithm allows for several degrees of freedom in the pivoting process. Hence, in a second step, we show that this freedom can be exploited to make the algorithm perform more efficiently. As our main result, we obtain an algorithm to compute a lattice basis for given vectors $A_1, \\ldots , A_n \\in \\mathbb{Z}^d$ in time (counting bit operations) $LS + \\tilde O ((n-d)d^2 \\cdot \\log(||A||)$, where $LS$ is the time required to obtain the exact fractional solution of a certain system of linear equalities. The analysis of the running time of our algorithms relies on fundamental statements on the fractionality of solutions of linear systems of equations.   So far, the fastest algorithm for lattice basis computation was due to Storjohann and Labhan [SL96] having a running time of $\\tilde O (nd^\\omega\\log ||A||)$. For current upper bounds of $LS$, our algorithm has a running time improvement of a factor of at least $d^{0.12}$ over [SL96]. Our algorithm is therefore the first general algorithmic improvement to this classical problem in nearly 30 years.","sentences":["The Euclidean algorithm the oldest algorithms known to mankind.","Given two integral numbers $a_1$ and $a_2$, it computes the greatest common divisor (gcd) of $a_1$ and $a_2$ in a very elegant way.","From a lattice perspective, it computes a basis of the sum of two one-dimensional lattices $a_1 \\mathbb{Z}$ and $a_2 \\mathbb{Z}$ as $\\gcd(a_1,a_2) \\mathbb{Z} = a_1 \\mathbb{Z} + a_2 \\mathbb{Z}$.","In this paper, we show that the classical Euclidean algorithm can be adapted in a very natural way to compute a basis of a general lattice $L (A_1, \\ldots , A_n)$ given vectors $A_1, \\ldots , A_n \\in \\mathbb{Z}^d$ with $n> \\mathrm{rank}(a_1, \\ldots ,a_d)$. Similar to the Euclidean algorithm, our algorithm is very easy to describe and implement and can be written within 12 lines of pseudocode.   ","Our generalized version of the Euclidean algorithm allows for several degrees of freedom in the pivoting process.","Hence, in a second step, we show that this freedom can be exploited to make the algorithm perform more efficiently.","As our main result, we obtain an algorithm to compute a lattice basis for given vectors $A_1, \\ldots , A_n \\in \\mathbb{Z}^d$ in time (counting bit operations) $LS","+","\\tilde O ((n-d)d^2 \\cdot \\log(||A||)$, where $LS$ is the time required to obtain the exact fractional solution of a certain system of linear equalities.","The analysis of the running time of our algorithms relies on fundamental statements on the fractionality of solutions of linear systems of equations.   ","So far, the fastest algorithm for lattice basis computation was due to Storjohann and Labhan","[SL96] having a running time of $\\tilde O (nd^\\omega\\log ||A||)$.","For current upper bounds of $LS$, our algorithm has a running time improvement of a factor of at least $d^{0.12}$ over [SL96].","Our algorithm is therefore the first general algorithmic improvement to this classical problem in nearly 30 years."],"url":"http://arxiv.org/abs/2408.06685v1"}
{"created":"2024-08-13 07:08:54","title":"Case-based Explainability for Random Forest: Prototypes, Critics, Counter-factuals and Semi-factuals","abstract":"The explainability of black-box machine learning algorithms, commonly known as Explainable Artificial Intelligence (XAI), has become crucial for financial and other regulated industrial applications due to regulatory requirements and the need for transparency in business practices. Among the various paradigms of XAI, Explainable Case-Based Reasoning (XCBR) stands out as a pragmatic approach that elucidates the output of a model by referencing actual examples from the data used to train or test the model. Despite its potential, XCBR has been relatively underexplored for many algorithms such as tree-based models until recently. We start by observing that most XCBR methods are defined based on the distance metric learned by the algorithm. By utilizing a recently proposed technique to extract the distance metric learned by Random Forests (RFs), which is both geometry- and accuracy-preserving, we investigate various XCBR methods. These methods amount to identify special points from the training datasets, such as prototypes, critics, counter-factuals, and semi-factuals, to explain the predictions for a given query of the RF. We evaluate these special points using various evaluation metrics to assess their explanatory power and effectiveness.","sentences":["The explainability of black-box machine learning algorithms, commonly known as Explainable Artificial Intelligence (XAI), has become crucial for financial and other regulated industrial applications due to regulatory requirements and the need for transparency in business practices.","Among the various paradigms of XAI, Explainable Case-Based Reasoning (XCBR) stands out as a pragmatic approach that elucidates the output of a model by referencing actual examples from the data used to train or test the model.","Despite its potential, XCBR has been relatively underexplored for many algorithms such as tree-based models until recently.","We start by observing that most XCBR methods are defined based on the distance metric learned by the algorithm.","By utilizing a recently proposed technique to extract the distance metric learned by Random Forests (RFs), which is both geometry- and accuracy-preserving, we investigate various XCBR methods.","These methods amount to identify special points from the training datasets, such as prototypes, critics, counter-factuals, and semi-factuals, to explain the predictions for a given query of the RF.","We evaluate these special points using various evaluation metrics to assess their explanatory power and effectiveness."],"url":"http://arxiv.org/abs/2408.06679v1"}
{"created":"2024-08-13 06:55:54","title":"Latin Treebanks in Review: An Evaluation of Morphological Tagging Across Time","abstract":"Existing Latin treebanks draw from Latin's long written tradition, spanning 17 centuries and a variety of cultures. Recent efforts have begun to harmonize these treebanks' annotations to better train and evaluate morphological taggers. However, the heterogeneity of these treebanks must be carefully considered to build effective and reliable data. In this work, we review existing Latin treebanks to identify the texts they draw from, identify their overlap, and document their coverage across time and genre. We additionally design automated conversions of their morphological feature annotations into the conventions of standard Latin grammar. From this, we build new time-period data splits that draw from the existing treebanks which we use to perform a broad cross-time analysis for POS and morphological feature tagging. We find that BERT-based taggers outperform existing taggers while also being more robust to cross-domain shifts.","sentences":["Existing Latin treebanks draw from Latin's long written tradition, spanning 17 centuries and a variety of cultures.","Recent efforts have begun to harmonize these treebanks' annotations to better train and evaluate morphological taggers.","However, the heterogeneity of these treebanks must be carefully considered to build effective and reliable data.","In this work, we review existing Latin treebanks to identify the texts they draw from, identify their overlap, and document their coverage across time and genre.","We additionally design automated conversions of their morphological feature annotations into the conventions of standard Latin grammar.","From this, we build new time-period data splits that draw from the existing treebanks which we use to perform a broad cross-time analysis for POS and morphological feature tagging.","We find that BERT-based taggers outperform existing taggers while also being more robust to cross-domain shifts."],"url":"http://arxiv.org/abs/2408.06675v1"}
{"created":"2024-08-13 06:47:59","title":"Leveraging Priors via Diffusion Bridge for Time Series Generation","abstract":"Time series generation is widely used in real-world applications such as simulation, data augmentation, and hypothesis test techniques. Recently, diffusion models have emerged as the de facto approach for time series generation, emphasizing diverse synthesis scenarios based on historical or correlated time series data streams. Since time series have unique characteristics, such as fixed time order and data scaling, standard Gaussian prior might be ill-suited for general time series generation. In this paper, we exploit the usage of diverse prior distributions for synthesis. Then, we propose TimeBridge, a framework that enables flexible synthesis by leveraging diffusion bridges to learn the transport between chosen prior and data distributions. Our model covers a wide range of scenarios in time series diffusion models, which leverages (i) data- and time-dependent priors for unconditional synthesis, and (ii) data-scale preserving synthesis with a constraint as a prior for conditional generation. Experimentally, our model achieves state-of-the-art performance in both unconditional and conditional time series generation tasks.","sentences":["Time series generation is widely used in real-world applications such as simulation, data augmentation, and hypothesis test techniques.","Recently, diffusion models have emerged as the de facto approach for time series generation, emphasizing diverse synthesis scenarios based on historical or correlated time series data streams.","Since time series have unique characteristics, such as fixed time order and data scaling, standard Gaussian prior might be ill-suited for general time series generation.","In this paper, we exploit the usage of diverse prior distributions for synthesis.","Then, we propose TimeBridge, a framework that enables flexible synthesis by leveraging diffusion bridges to learn the transport between chosen prior and data distributions.","Our model covers a wide range of scenarios in time series diffusion models, which leverages (i) data- and time-dependent priors for unconditional synthesis, and (ii) data-scale preserving synthesis with a constraint as a prior for conditional generation.","Experimentally, our model achieves state-of-the-art performance in both unconditional and conditional time series generation tasks."],"url":"http://arxiv.org/abs/2408.06672v1"}
{"created":"2024-08-13 05:53:46","title":"Hierarchical Structured Neural Network for Retrieval","abstract":"Embedding Based Retrieval (EBR) is a crucial component of the retrieval stage in (Ads) Recommendation System that utilizes Two Tower or Siamese Networks to learn embeddings for both users and items (ads). It then employs an Approximate Nearest Neighbor Search (ANN) to efficiently retrieve the most relevant ads for a specific user. Despite the recent rise to popularity in the industry, they have a couple of limitations. Firstly, Two Tower model architecture uses a single dot product interaction which despite their efficiency fail to capture the data distribution in practice. Secondly, the centroid representation and cluster assignment, which are components of ANN, occur after the training process has been completed. As a result, they do not take into account the optimization criteria used for retrieval model. In this paper, we present Hierarchical Structured Neural Network (HSNN), a deployed jointly optimized hierarchical clustering and neural network model that can take advantage of sophisticated interactions and model architectures that are more common in the ranking stages while maintaining a sub-linear inference cost. We achieve 6.5% improvement in offline evaluation and also demonstrate 1.22% online gains through A/B experiments. HSNN has been successfully deployed into the Ads Recommendation system and is currently handling major portion of the traffic. The paper shares our experience in developing this system, dealing with challenges like freshness, volatility, cold start recommendations, cluster collapse and lessons deploying the model in a large scale retrieval production system.","sentences":["Embedding Based Retrieval (EBR) is a crucial component of the retrieval stage in (Ads) Recommendation System that utilizes Two Tower or Siamese Networks to learn embeddings for both users and items (ads).","It then employs an Approximate Nearest Neighbor Search (ANN) to efficiently retrieve the most relevant ads for a specific user.","Despite the recent rise to popularity in the industry, they have a couple of limitations.","Firstly, Two Tower model architecture uses a single dot product interaction which despite their efficiency fail to capture the data distribution in practice.","Secondly, the centroid representation and cluster assignment, which are components of ANN, occur after the training process has been completed.","As a result, they do not take into account the optimization criteria used for retrieval model.","In this paper, we present Hierarchical Structured Neural Network (HSNN), a deployed jointly optimized hierarchical clustering and neural network model that can take advantage of sophisticated interactions and model architectures that are more common in the ranking stages while maintaining a sub-linear inference cost.","We achieve 6.5% improvement in offline evaluation and also demonstrate 1.22% online gains through A/B experiments.","HSNN has been successfully deployed into the Ads Recommendation system and is currently handling major portion of the traffic.","The paper shares our experience in developing this system, dealing with challenges like freshness, volatility, cold start recommendations, cluster collapse and lessons deploying the model in a large scale retrieval production system."],"url":"http://arxiv.org/abs/2408.06653v1"}
{"created":"2024-08-13 05:37:18","title":"A Miniature Vision-Based Localization System for Indoor Blimps","abstract":"With increasing attention paid to blimp research, I hope to build an indoor blimp to interact with humans. To begin with, I propose developing a visual localization system to enable blimps to localize themselves in an indoor environment autonomously. This system initially reconstructs an indoor environment by employing Structure from Motion with Superpoint visual features. Next, with the previously built sparse point cloud map, the system generates camera poses by continuously employing pose estimation on matched visual features observed from the map. In this project, the blimp only serves as a reference mobile platform that constrains the weight of the perception system. The perception system contains one monocular camera and a WiFi adaptor to capture and transmit visual data to a ground PC station where the algorithms will be executed. The success of this project will transform remote-controlled indoor blimps into autonomous indoor blimps, which can be utilized for applications such as surveillance, advertisement, and indoor mapping.","sentences":["With increasing attention paid to blimp research, I hope to build an indoor blimp to interact with humans.","To begin with, I propose developing a visual localization system to enable blimps to localize themselves in an indoor environment autonomously.","This system initially reconstructs an indoor environment by employing Structure from Motion with Superpoint visual features.","Next, with the previously built sparse point cloud map, the system generates camera poses by continuously employing pose estimation on matched visual features observed from the map.","In this project, the blimp only serves as a reference mobile platform that constrains the weight of the perception system.","The perception system contains one monocular camera and a WiFi adaptor to capture and transmit visual data to a ground PC station where the algorithms will be executed.","The success of this project will transform remote-controlled indoor blimps into autonomous indoor blimps, which can be utilized for applications such as surveillance, advertisement, and indoor mapping."],"url":"http://arxiv.org/abs/2408.06648v1"}
{"created":"2024-08-13 04:33:23","title":"Fast Information Streaming Handler (FisH): A Unified Seismic Neural Network for Single Station Real-Time Earthquake Early Warning","abstract":"Existing EEW approaches often treat phase picking, location estimation, and magnitude estimation as separate tasks, lacking a unified framework. Additionally, most deep learning models in seismology rely on full three-component waveforms and are not suitable for real-time streaming data. To address these limitations, we propose a novel unified seismic neural network called Fast Information Streaming Handler (FisH). FisH is designed to process real-time streaming seismic data and generate simultaneous results for phase picking, location estimation, and magnitude estimation in an end-to-end fashion. By integrating these tasks within a single model, FisH simplifies the overall process and leverages the nonlinear relationships between tasks for improved performance. The FisH model utilizes RetNet as its backbone, enabling parallel processing during training and recurrent handling during inference. This capability makes FisH suitable for real-time applications, reducing latency in EEW systems. Extensive experiments conducted on the STEAD benchmark dataset provide strong validation for the effectiveness of our proposed FisH model. The results demonstrate that FisH achieves impressive performance across multiple seismic event detection and characterization tasks. Specifically, it achieves an F1 score of 0.99/0.96. Also, FisH demonstrates precise earthquake location estimation, with location error of only 6.0km, a distance error of 2.6km, and a back-azimuth error of 19{\\deg}. The model also exhibits accurate earthquake magnitude estimation, with a magnitude error of just 0.14. Additionally, FisH is capable of generating real-time estimations, providing location and magnitude estimations with a location error of 8.06km and a magnitude error of 0.18 within a mere 3 seconds after the P-wave arrives.","sentences":["Existing EEW approaches often treat phase picking, location estimation, and magnitude estimation as separate tasks, lacking a unified framework.","Additionally, most deep learning models in seismology rely on full three-component waveforms and are not suitable for real-time streaming data.","To address these limitations, we propose a novel unified seismic neural network called Fast Information Streaming Handler (FisH).","FisH is designed to process real-time streaming seismic data and generate simultaneous results for phase picking, location estimation, and magnitude estimation in an end-to-end fashion.","By integrating these tasks within a single model, FisH simplifies the overall process and leverages the nonlinear relationships between tasks for improved performance.","The FisH model utilizes RetNet as its backbone, enabling parallel processing during training and recurrent handling during inference.","This capability makes FisH suitable for real-time applications, reducing latency in EEW systems.","Extensive experiments conducted on the STEAD benchmark dataset provide strong validation for the effectiveness of our proposed FisH model.","The results demonstrate that FisH achieves impressive performance across multiple seismic event detection and characterization tasks.","Specifically, it achieves an F1 score of 0.99/0.96.","Also, FisH demonstrates precise earthquake location estimation, with location error of only 6.0km, a distance error of 2.6km, and a back-azimuth error of 19{\\deg}.","The model also exhibits accurate earthquake magnitude estimation, with a magnitude error of just 0.14.","Additionally, FisH is capable of generating real-time estimations, providing location and magnitude estimations with a location error of 8.06km and a magnitude error of 0.18 within a mere 3 seconds after the P-wave arrives."],"url":"http://arxiv.org/abs/2408.06629v1"}
{"created":"2024-08-13 04:18:32","title":"Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, training LLMs on human-written text entails significant risk of privacy and copyright violations, which demands an efficient machine unlearning framework to remove knowledge of sensitive data without retraining the model from scratch. While Gradient Ascent (GA) is widely used for unlearning by reducing the likelihood of generating unwanted information, the unboundedness of increasing the cross-entropy loss causes not only unstable optimization, but also catastrophic forgetting of knowledge that needs to be retained. We also discover its joint application under low-rank adaptation results in significantly suboptimal computational cost vs. generative performance trade-offs. In light of this limitation, we propose two novel techniques for robust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge loss that suppresses unwanted tokens by increasing the probability of the next most likely token, thereby retaining fluency and structure in language generation. We also propose to initialize low-rank adapter weights based on Fisher-weighted low-rank approximation, which induces faster unlearning and better knowledge retention by allowing model updates to be focused on parameters that are important in generating textual data we wish to remove.","sentences":["Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora.","However, training LLMs on human-written text entails significant risk of privacy and copyright violations, which demands an efficient machine unlearning framework to remove knowledge of sensitive data without retraining the model from scratch.","While Gradient Ascent (GA) is widely used for unlearning by reducing the likelihood of generating unwanted information, the unboundedness of increasing the cross-entropy loss causes not only unstable optimization, but also catastrophic forgetting of knowledge that needs to be retained.","We also discover its joint application under low-rank adaptation results in significantly suboptimal computational cost vs. generative performance trade-offs.","In light of this limitation, we propose two novel techniques for robust and cost-efficient unlearning on LLMs.","We first design an Inverted Hinge loss that suppresses unwanted tokens by increasing the probability of the next most likely token, thereby retaining fluency and structure in language generation.","We also propose to initialize low-rank adapter weights based on Fisher-weighted low-rank approximation, which induces faster unlearning and better knowledge retention by allowing model updates to be focused on parameters that are important in generating textual data we wish to remove."],"url":"http://arxiv.org/abs/2408.06621v1"}
{"created":"2024-08-13 04:18:32","title":"ActPrompt: In-Domain Feature Adaptation via Action Cues for Video Temporal Grounding","abstract":"Video temporal grounding is an emerging topic aiming to identify specific clips within videos. In addition to pre-trained video models, contemporary methods utilize pre-trained vision-language models (VLM) to capture detailed characteristics of diverse scenes and objects from video frames. However, as pre-trained on images, VLM may struggle to distinguish action-sensitive patterns from static objects, making it necessary to adapt them to specific data domains for effective feature representation over temporal grounding. We address two primary challenges to achieve this goal. Specifically, to mitigate high adaptation costs, we propose an efficient preliminary in-domain fine-tuning paradigm for feature adaptation, where downstream-adaptive features are learned through several pretext tasks. Furthermore, to integrate action-sensitive information into VLM, we introduce Action-Cue-Injected Temporal Prompt Learning (ActPrompt), which injects action cues into the image encoder of VLM for better discovering action-sensitive patterns. Extensive experiments demonstrate that ActPrompt is an off-the-shelf training framework that can be effectively applied to various SOTA methods, resulting in notable improvements. The complete code used in this study is provided in the supplementary materials.","sentences":["Video temporal grounding is an emerging topic aiming to identify specific clips within videos.","In addition to pre-trained video models, contemporary methods utilize pre-trained vision-language models (VLM) to capture detailed characteristics of diverse scenes and objects from video frames.","However, as pre-trained on images, VLM may struggle to distinguish action-sensitive patterns from static objects, making it necessary to adapt them to specific data domains for effective feature representation over temporal grounding.","We address two primary challenges to achieve this goal.","Specifically, to mitigate high adaptation costs, we propose an efficient preliminary in-domain fine-tuning paradigm for feature adaptation, where downstream-adaptive features are learned through several pretext tasks.","Furthermore, to integrate action-sensitive information into VLM, we introduce Action-Cue-Injected Temporal Prompt Learning (ActPrompt), which injects action cues into the image encoder of VLM for better discovering action-sensitive patterns.","Extensive experiments demonstrate that ActPrompt is an off-the-shelf training framework that can be effectively applied to various SOTA methods, resulting in notable improvements.","The complete code used in this study is provided in the supplementary materials."],"url":"http://arxiv.org/abs/2408.06622v1"}
{"created":"2024-08-13 03:37:13","title":"MV-DETR: Multi-modality indoor object detection by Multi-View DEtecton TRansformers","abstract":"We introduce a novel MV-DETR pipeline which is effective while efficient transformer based detection method. Given input RGBD data, we notice that there are super strong pretraining weights for RGB data while less effective works for depth related data. First and foremost , we argue that geometry and texture cues are both of vital importance while could be encoded separately. Secondly, we find that visual texture feature is relatively hard to extract compared with geometry feature in 3d space. Unfortunately, single RGBD dataset with thousands of data is not enough for training an discriminating filter for visual texture feature extraction. Last but certainly not the least, we designed a lightweight VG module consists of a visual textual encoder, a geometry encoder and a VG connector. Compared with previous state of the art works like V-DETR, gains from pretrained visual encoder could be seen. Extensive experiments on ScanNetV2 dataset shows the effectiveness of our method. It is worth mentioned that our method achieve 78\\% AP which create new state of the art on ScanNetv2 benchmark.","sentences":["We introduce a novel MV-DETR pipeline which is effective while efficient transformer based detection method.","Given input RGBD data, we notice that there are super strong pretraining weights for RGB data while less effective works for depth related data.","First and foremost , we argue that geometry and texture cues are both of vital importance while could be encoded separately.","Secondly, we find that visual texture feature is relatively hard to extract compared with geometry feature in 3d space.","Unfortunately, single RGBD dataset with thousands of data is not enough for training an discriminating filter for visual texture feature extraction.","Last but certainly not the least, we designed a lightweight VG module consists of a visual textual encoder, a geometry encoder and a VG connector.","Compared with previous state of the art works like V-DETR, gains from pretrained visual encoder could be seen.","Extensive experiments on ScanNetV2 dataset shows the effectiveness of our method.","It is worth mentioned that our method achieve 78\\% AP which create new state of the art on ScanNetv2 benchmark."],"url":"http://arxiv.org/abs/2408.06604v1"}
{"created":"2024-08-13 03:33:41","title":"HiRegEx: Interactive Visual Query and Exploration of Multivariate Hierarchical Data","abstract":"When using exploratory visual analysis to examine multivariate hierarchical data, users often need to query data to narrow down the scope of analysis. However, formulating effective query expressions remains a challenge for multivariate hierarchical data, particularly when datasets become very large. To address this issue, we develop a declarative grammar, HiRegEx (Hierarchical data Regular Expression), for querying and exploring multivariate hierarchical data. Rooted in the extended multi-level task topology framework for tree visualizations (e-MLTT), HiRegEx delineates three query targets (node, path, and subtree) and two aspects for querying these targets (features and positions), and uses operators developed based on classical regular expressions for query construction. Based on the HiRegEx grammar, we develop an exploratory framework for querying and exploring multivariate hierarchical data and integrate it into the TreeQueryER prototype system. The exploratory framework includes three major components: top-down pattern specification, bottom-up data-driven inquiry, and context-creation data overview. We validate the expressiveness of HiRegEx with the tasks from the e-MLTT framework and showcase the utility and effectiveness of TreeQueryER system through a case study involving expert users in the analysis of a citation tree dataset.","sentences":["When using exploratory visual analysis to examine multivariate hierarchical data, users often need to query data to narrow down the scope of analysis.","However, formulating effective query expressions remains a challenge for multivariate hierarchical data, particularly when datasets become very large.","To address this issue, we develop a declarative grammar, HiRegEx (Hierarchical data Regular Expression), for querying and exploring multivariate hierarchical data.","Rooted in the extended multi-level task topology framework for tree visualizations (e-MLTT), HiRegEx delineates three query targets (node, path, and subtree) and two aspects for querying these targets (features and positions), and uses operators developed based on classical regular expressions for query construction.","Based on the HiRegEx grammar, we develop an exploratory framework for querying and exploring multivariate hierarchical data and integrate it into the TreeQueryER prototype system.","The exploratory framework includes three major components: top-down pattern specification, bottom-up data-driven inquiry, and context-creation data overview.","We validate the expressiveness of HiRegEx with the tasks from the e-MLTT framework and showcase the utility and effectiveness of TreeQueryER system through a case study involving expert users in the analysis of a citation tree dataset."],"url":"http://arxiv.org/abs/2408.06601v1"}
{"created":"2024-08-13 03:25:49","title":"A Perspective on Large Language Models, Intelligent Machines, and Knowledge Acquisition","abstract":"Large Language Models (LLMs) are known for their remarkable ability to generate synthesized 'knowledge', such as text documents, music, images, etc. However, there is a huge gap between LLM's and human capabilities for understanding abstract concepts and reasoning. We discuss these issues in a larger philosophical context of human knowledge acquisition and the Turing test. In addition, we illustrate the limitations of LLMs by analyzing GPT-4 responses to questions ranging from science and math to common sense reasoning. These examples show that GPT-4 can often imitate human reasoning, even though it lacks understanding. However, LLM responses are synthesized from a large LLM model trained on all available data. In contrast, human understanding is based on a small number of abstract concepts. Based on this distinction, we discuss the impact of LLMs on acquisition of human knowledge and education.","sentences":["Large Language Models (LLMs) are known for their remarkable ability to generate synthesized 'knowledge', such as text documents, music, images, etc.","However, there is a huge gap between LLM's and human capabilities for understanding abstract concepts and reasoning.","We discuss these issues in a larger philosophical context of human knowledge acquisition and the Turing test.","In addition, we illustrate the limitations of LLMs by analyzing GPT-4 responses to questions ranging from science and math to common sense reasoning.","These examples show that GPT-4 can often imitate human reasoning, even though it lacks understanding.","However, LLM responses are synthesized from a large LLM model trained on all available data.","In contrast, human understanding is based on a small number of abstract concepts.","Based on this distinction, we discuss the impact of LLMs on acquisition of human knowledge and education."],"url":"http://arxiv.org/abs/2408.06598v1"}
{"created":"2024-08-13 02:43:19","title":"Biomedical Event Extraction via Structure-aware Generation","abstract":"Biomedical Event Extraction (BEE) is a critical task that involves modeling complex relationships between fine-grained entities in biomedical text data. However, most existing BEE models rely on classification methods that neglect the label semantics and argument dependency structure within the data. To address these limitations, we propose GenBEE, a generative model enhanced with a structure-aware prefix for biomedical event extraction. GenBEE constructs event prompts that leverage knowledge distilled from large language models (LLMs), thereby incorporating both label semantics and argument dependency relationships. Additionally, GenBEE introduces a structural prefix learning module that generates structure-aware prefixes with structural prompts, enriching the generation process with structural features. Extensive experiments on three benchmark datasets demonstrate the effectiveness of GenBEE and it achieves state-of-the-art performance on the MLEE and GE11 datasets. Furthermore, our analysis shows that the structural prefixes effectively bridge the gap between structural prompts and the representation space of generative models, enabling better integration of event structural information.","sentences":["Biomedical Event Extraction (BEE) is a critical task that involves modeling complex relationships between fine-grained entities in biomedical text data.","However, most existing BEE models rely on classification methods that neglect the label semantics and argument dependency structure within the data.","To address these limitations, we propose GenBEE, a generative model enhanced with a structure-aware prefix for biomedical event extraction.","GenBEE constructs event prompts that leverage knowledge distilled from large language models (LLMs), thereby incorporating both label semantics and argument dependency relationships.","Additionally, GenBEE introduces a structural prefix learning module that generates structure-aware prefixes with structural prompts, enriching the generation process with structural features.","Extensive experiments on three benchmark datasets demonstrate the effectiveness of GenBEE and it achieves state-of-the-art performance on the MLEE and GE11 datasets.","Furthermore, our analysis shows that the structural prefixes effectively bridge the gap between structural prompts and the representation space of generative models, enabling better integration of event structural information."],"url":"http://arxiv.org/abs/2408.06583v1"}
{"created":"2024-08-13 02:25:16","title":"CTISum: A New Benchmark Dataset For Cyber Threat Intelligence Summarization","abstract":"Cyber Threat Intelligence (CTI) summarization task requires the system to generate concise and accurate highlights from raw intelligence data, which plays an important role in providing decision-makers with crucial information to quickly detect and respond to cyber threats in the cybersecurity domain. However, efficient techniques for summarizing CTI reports, including facts, analytical insights, attack processes, etc., have largely been unexplored, primarily due to the lack of available dataset. To this end, we present CTISum, a new benchmark for CTI summarization task. Considering the importance of attack process, a novel fine-grained subtask of attack process summarization is proposed to enable defenders to assess risk, identify security gaps, vulnerabilities, and so on. Specifically, we first design a multi-stage annotation pipeline to gather and annotate the CTI data, and then benchmark the CTISum with a collection of extractive and abstractive summarization methods. Experimental results show that current state-of-the-art models exhibit limitations when applied to CTISum, underscoring the fact that automatically producing concise summaries of CTI reports remains an open research challenge.","sentences":["Cyber Threat Intelligence (CTI) summarization task requires the system to generate concise and accurate highlights from raw intelligence data, which plays an important role in providing decision-makers with crucial information to quickly detect and respond to cyber threats in the cybersecurity domain.","However, efficient techniques for summarizing CTI reports, including facts, analytical insights, attack processes, etc., have largely been unexplored, primarily due to the lack of available dataset.","To this end, we present CTISum, a new benchmark for CTI summarization task.","Considering the importance of attack process, a novel fine-grained subtask of attack process summarization is proposed to enable defenders to assess risk, identify security gaps, vulnerabilities, and so on.","Specifically, we first design a multi-stage annotation pipeline to gather and annotate the CTI data, and then benchmark the CTISum with a collection of extractive and abstractive summarization methods.","Experimental results show that current state-of-the-art models exhibit limitations when applied to CTISum, underscoring the fact that automatically producing concise summaries of CTI reports remains an open research challenge."],"url":"http://arxiv.org/abs/2408.06576v1"}
{"created":"2024-08-13 02:08:32","title":"Social Debiasing for Fair Multi-modal LLMs","abstract":"Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.","sentences":["Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities.","However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender.","This paper addresses the issue of social biases in MLLMs by i)","Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets.","ii) Proposing an Anti-Stereotype Debiasing strategy (ASD).","Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases.","Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance."],"url":"http://arxiv.org/abs/2408.06569v1"}
{"created":"2024-08-13 02:07:00","title":"AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out Strategies","abstract":"In recent years, with the rapid application of large language models across various fields, the scale of these models has gradually increased, and the resources required for their pre-training have grown exponentially. Training an LLM from scratch will cost a lot of computation resources while scaling up from a smaller model is a more efficient approach and has thus attracted significant attention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B Mixture of Experts (MoE) language model that has 8 experts with 16 billion parameters each and is developed using an innovative training methodology called EfficientScale. This approach optimizes performance while minimizing data requirements through a two-stage process. The first stage, termed Scale-Up, initializes the larger model with weights from a pre-trained smaller model, enabling substantial knowledge transfer and continuous pretraining with significantly less data. The second stage, Scale-Out, uses a pre-trained dense model to initialize the MoE experts, further enhancing knowledge transfer and performance. Extensive validation experiments on 1.8B and 7B models compared various initialization schemes, achieving models that maintain and reduce loss during continuous pretraining. Utilizing the optimal scheme, we successfully trained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating significant improvements in performance and training efficiency.","sentences":["In recent years, with the rapid application of large language models across various fields, the scale of these models has gradually increased, and the resources required for their pre-training have grown exponentially.","Training an LLM from scratch will cost a lot of computation resources while scaling up from a smaller model is a more efficient approach and has thus attracted significant attention.","In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B Mixture of Experts (MoE) language model that has 8 experts with 16 billion parameters each and is developed using an innovative training methodology called EfficientScale.","This approach optimizes performance while minimizing data requirements through a two-stage process.","The first stage, termed Scale-Up, initializes the larger model with weights from a pre-trained smaller model, enabling substantial knowledge transfer and continuous pretraining with significantly less data.","The second stage, Scale-Out, uses a pre-trained dense model to initialize the MoE experts, further enhancing knowledge transfer and performance.","Extensive validation experiments on 1.8B and 7B models compared various initialization schemes, achieving models that maintain and reduce loss during continuous pretraining.","Utilizing the optimal scheme, we successfully trained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating significant improvements in performance and training efficiency."],"url":"http://arxiv.org/abs/2408.06567v1"}
{"created":"2024-08-13 01:14:27","title":"Prioritizing Modalities: Flexible Importance Scheduling in Federated Multimodal Learning","abstract":"Federated Learning (FL) is a distributed machine learning approach that enables devices to collaboratively train models without sharing their local data, ensuring user privacy and scalability. However, applying FL to real-world data presents challenges, particularly as most existing FL research focuses on unimodal data. Multimodal Federated Learning (MFL) has emerged to address these challenges, leveraging modality-specific encoder models to process diverse datasets. Current MFL methods often uniformly allocate computational frequencies across all modalities, which is inefficient for IoT devices with limited resources. In this paper, we propose FlexMod, a novel approach to enhance computational efficiency in MFL by adaptively allocating training resources for each modality encoder based on their importance and training requirements. We employ prototype learning to assess the quality of modality encoders, use Shapley values to quantify the importance of each modality, and adopt the Deep Deterministic Policy Gradient (DDPG) method from deep reinforcement learning to optimize the allocation of training resources. Our method prioritizes critical modalities, optimizing model performance and resource utilization. Experimental results on three real-world datasets demonstrate that our proposed method significantly improves the performance of MFL models.","sentences":["Federated Learning (FL) is a distributed machine learning approach that enables devices to collaboratively train models without sharing their local data, ensuring user privacy and scalability.","However, applying FL to real-world data presents challenges, particularly as most existing FL research focuses on unimodal data.","Multimodal Federated Learning (MFL) has emerged to address these challenges, leveraging modality-specific encoder models to process diverse datasets.","Current MFL methods often uniformly allocate computational frequencies across all modalities, which is inefficient for IoT devices with limited resources.","In this paper, we propose FlexMod, a novel approach to enhance computational efficiency in MFL by adaptively allocating training resources for each modality encoder based on their importance and training requirements.","We employ prototype learning to assess the quality of modality encoders, use Shapley values to quantify the importance of each modality, and adopt the Deep Deterministic Policy Gradient (DDPG) method from deep reinforcement learning to optimize the allocation of training resources.","Our method prioritizes critical modalities, optimizing model performance and resource utilization.","Experimental results on three real-world datasets demonstrate that our proposed method significantly improves the performance of MFL models."],"url":"http://arxiv.org/abs/2408.06549v1"}
{"created":"2024-08-13 00:06:56","title":"Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality Parallel Data Outperforms Traditional Web-Crawled Data","abstract":"Recent research in neural machine translation (NMT) has shown that training on high-quality machine-generated data can outperform training on human-generated data. This work accompanies the first-ever release of a LLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and multi-sentence examples. We perform extensive experiments to demonstrate the quality of our dataset in terms of its downstream impact on NMT model performance. We find that training from scratch on our (machine-generated) dataset outperforms training on the (web-crawled) WMT'23 training dataset (which is 300 times larger), and also outperforms training on the top-quality subset of the WMT'23 training dataset. We also find that performing self-distillation by finetuning the LLM which generated this dataset outperforms the LLM's strong few-shot baseline. These findings corroborate the quality of our dataset, and demonstrate the value of high-quality machine-generated data in improving performance of NMT models.","sentences":["Recent research in neural machine translation (NMT) has shown that training on high-quality machine-generated data can outperform training on human-generated data.","This work accompanies the first-ever release of a LLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and multi-sentence examples.","We perform extensive experiments to demonstrate the quality of our dataset in terms of its downstream impact on NMT model performance.","We find that training from scratch on our (machine-generated) dataset outperforms training on the (web-crawled)","WMT'23 training dataset (which is 300 times larger), and also outperforms training on the top-quality subset of the WMT'23 training dataset.","We also find that performing self-distillation by finetuning the LLM which generated this dataset outperforms the LLM's strong few-shot baseline.","These findings corroborate the quality of our dataset, and demonstrate the value of high-quality machine-generated data in improving performance of NMT models."],"url":"http://arxiv.org/abs/2408.06537v1"}
{"created":"2024-08-13 00:04:17","title":"A Comparison of Imitation Learning Algorithms for Bimanual Manipulation","abstract":"Amidst the wide popularity of imitation learning algorithms in robotics, their properties regarding hyperparameter sensitivity, ease of training, data efficiency, and performance have not been well-studied in high-precision industry-inspired environments. In this work, we demonstrate the limitations and benefits of prominent imitation learning approaches and analyze their capabilities regarding these properties. We evaluate each algorithm on a complex bimanual manipulation task involving an over-constrained dynamics system in a setting involving multiple contacts between the manipulated object and the environment. While we find that imitation learning is well suited to solve such complex tasks, not all algorithms are equal in terms of handling environmental and hyperparameter perturbations, training requirements, performance, and ease of use. We investigate the empirical influence of these key characteristics by employing a carefully designed experimental procedure and learning environment. Paper website: https://bimanual-imitation.github.io/","sentences":["Amidst the wide popularity of imitation learning algorithms in robotics, their properties regarding hyperparameter sensitivity, ease of training, data efficiency, and performance have not been well-studied in high-precision industry-inspired environments.","In this work, we demonstrate the limitations and benefits of prominent imitation learning approaches and analyze their capabilities regarding these properties.","We evaluate each algorithm on a complex bimanual manipulation task involving an over-constrained dynamics system in a setting involving multiple contacts between the manipulated object and the environment.","While we find that imitation learning is well suited to solve such complex tasks, not all algorithms are equal in terms of handling environmental and hyperparameter perturbations, training requirements, performance, and ease of use.","We investigate the empirical influence of these key characteristics by employing a carefully designed experimental procedure and learning environment.","Paper website: https://bimanual-imitation.github.io/"],"url":"http://arxiv.org/abs/2408.06536v1"}
{"created":"2024-08-12 23:10:39","title":"Operator Learning Using Random Features: A Tool for Scientific Computing","abstract":"Supervised operator learning centers on the use of training data, in the form of input-output pairs, to estimate maps between infinite-dimensional spaces. It is emerging as a powerful tool to complement traditional scientific computing, which may often be framed in terms of operators mapping between spaces of functions. Building on the classical random features methodology for scalar regression, this paper introduces the function-valued random features method. This leads to a supervised operator learning architecture that is practical for nonlinear problems yet is structured enough to facilitate efficient training through the optimization of a convex, quadratic cost. Due to the quadratic structure, the trained model is equipped with convergence guarantees and error and complexity bounds, properties that are not readily available for most other operator learning architectures. At its core, the proposed approach builds a linear combination of random operators. This turns out to be a low-rank approximation of an operator-valued kernel ridge regression algorithm, and hence the method also has strong connections to Gaussian process regression. The paper designs function-valued random features that are tailored to the structure of two nonlinear operator learning benchmark problems arising from parametric partial differential equations. Numerical results demonstrate the scalability, discretization invariance, and transferability of the function-valued random features method.","sentences":["Supervised operator learning centers on the use of training data, in the form of input-output pairs, to estimate maps between infinite-dimensional spaces.","It is emerging as a powerful tool to complement traditional scientific computing, which may often be framed in terms of operators mapping between spaces of functions.","Building on the classical random features methodology for scalar regression, this paper introduces the function-valued random features method.","This leads to a supervised operator learning architecture that is practical for nonlinear problems yet is structured enough to facilitate efficient training through the optimization of a convex, quadratic cost.","Due to the quadratic structure, the trained model is equipped with convergence guarantees and error and complexity bounds, properties that are not readily available for most other operator learning architectures.","At its core, the proposed approach builds a linear combination of random operators.","This turns out to be a low-rank approximation of an operator-valued kernel ridge regression algorithm, and hence the method also has strong connections to Gaussian process regression.","The paper designs function-valued random features that are tailored to the structure of two nonlinear operator learning benchmark problems arising from parametric partial differential equations.","Numerical results demonstrate the scalability, discretization invariance, and transferability of the function-valued random features method."],"url":"http://arxiv.org/abs/2408.06526v1"}
{"created":"2024-08-12 22:04:55","title":"De-cluttering Scatterplots with Integral Images","abstract":"Scatterplots provide a visual representation of bivariate data (or 2D embeddings of multivariate data) that allows for effective analyses of data dependencies, clusters, trends, and outliers. Unfortunately, classical scatterplots suffer from scalability issues, since growing data sizes eventually lead to overplotting and visual clutter on a screen with a fixed resolution, which hinders the data analysis process. We propose an algorithm that compensates for irregular sample distributions by a smooth transformation of the scatterplot's visual domain. Our algorithm evaluates the scatterplot's density distribution to compute a regularization mapping based on integral images of the rasterized density function. The mapping preserves the samples' neighborhood relations. Few regularization iterations suffice to achieve a nearly uniform sample distribution that efficiently uses the available screen space. We further propose approaches to visually convey the transformation that was applied to the scatterplot and compare them in a user study. We present a novel parallel algorithm for fast GPU-based integral-image computation, which allows for integrating our de-cluttering approach into interactive visual data analysis systems.","sentences":["Scatterplots provide a visual representation of bivariate data (or 2D embeddings of multivariate data) that allows for effective analyses of data dependencies, clusters, trends, and outliers.","Unfortunately, classical scatterplots suffer from scalability issues, since growing data sizes eventually lead to overplotting and visual clutter on a screen with a fixed resolution, which hinders the data analysis process.","We propose an algorithm that compensates for irregular sample distributions by a smooth transformation of the scatterplot's visual domain.","Our algorithm evaluates the scatterplot's density distribution to compute a regularization mapping based on integral images of the rasterized density function.","The mapping preserves the samples' neighborhood relations.","Few regularization iterations suffice to achieve a nearly uniform sample distribution that efficiently uses the available screen space.","We further propose approaches to visually convey the transformation that was applied to the scatterplot and compare them in a user study.","We present a novel parallel algorithm for fast GPU-based integral-image computation, which allows for integrating our de-cluttering approach into interactive visual data analysis systems."],"url":"http://arxiv.org/abs/2408.06513v1"}
{"created":"2024-08-12 22:01:23","title":"Modeling and Simulation of Traffic on I-485 via Linear Systems and Iterative Methods","abstract":"Iterative methods such as Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR) are fundamental tools in solving large systems of linear equations across various scientific fields, particularly in the field of data science which has become increasingly relevant in the past decade. Iterative methods' use of matrix multiplication rather than matrix inverses makes them ideal for solving large systems quickly. Our research explores the factors of each method that define their respective strengths, limitations, and convergence behaviors to understand how these methods address drawbacks encountered when performing matrix operations by hand, as well as how they can be used in real world applications. After implementing each method by hand to understand how the algorithms work, we developed a Python program that assesses a user-given matrix based on each method's specific convergence criteria. The program compares the spectral radii of all three methods and chooses to execute whichever will yield the fastest convergence rate. Our research revealed the importance of mathematical modeling and understanding specific properties of the coefficient matrix. We observed that Gauss-Seidel is usually the most efficient method because it is faster than Jacobi and doesn't have as strict requirements as SOR, however SOR is ideal in terms of computation speed. We applied the knowledge we gained to create a traffic flow model of the I-485 highway in Charlotte. After creating a program that generates the matrix for this model, we were able to iteratively approximate the flow of cars through neighboring exits using data from the N.C. Department of Transportation. This information identifies which areas are the most congested and can be used to inform future infrastructure development.","sentences":["Iterative methods such as Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR) are fundamental tools in solving large systems of linear equations across various scientific fields, particularly in the field of data science which has become increasingly relevant in the past decade.","Iterative methods' use of matrix multiplication rather than matrix inverses makes them ideal for solving large systems quickly.","Our research explores the factors of each method that define their respective strengths, limitations, and convergence behaviors to understand how these methods address drawbacks encountered when performing matrix operations by hand, as well as how they can be used in real world applications.","After implementing each method by hand to understand how the algorithms work, we developed a Python program that assesses a user-given matrix based on each method's specific convergence criteria.","The program compares the spectral radii of all three methods and chooses to execute whichever will yield the fastest convergence rate.","Our research revealed the importance of mathematical modeling and understanding specific properties of the coefficient matrix.","We observed that Gauss-Seidel is usually the most efficient method because it is faster than Jacobi and doesn't have as strict requirements as SOR, however SOR is ideal in terms of computation speed.","We applied the knowledge we gained to create a traffic flow model of the I-485 highway in Charlotte.","After creating a program that generates the matrix for this model, we were able to iteratively approximate the flow of cars through neighboring exits using data from the N.C. Department of Transportation.","This information identifies which areas are the most congested and can be used to inform future infrastructure development."],"url":"http://arxiv.org/abs/2408.06511v1"}
{"created":"2024-08-12 21:57:18","title":"Fooling SHAP with Output Shuffling Attacks","abstract":"Explainable AI~(XAI) methods such as SHAP can help discover feature attributions in black-box models. If the method reveals a significant attribution from a ``protected feature'' (e.g., gender, race) on the model output, the model is considered unfair. However, adversarial attacks can subvert the detection of XAI methods. Previous approaches to constructing such an adversarial model require access to underlying data distribution, which may not be possible in many practical scenarios. We relax this constraint and propose a novel family of attacks, called shuffling attacks, that are data-agnostic. The proposed attack strategies can adapt any trained machine learning model to fool Shapley value-based explanations. We prove that Shapley values cannot detect shuffling attacks. However, algorithms that estimate Shapley values, such as linear SHAP and SHAP, can detect these attacks with varying degrees of effectiveness. We demonstrate the efficacy of the attack strategies by comparing the performance of linear SHAP and SHAP using real-world datasets.","sentences":["Explainable AI~(XAI) methods such as SHAP can help discover feature attributions in black-box models.","If the method reveals a significant attribution from a ``protected feature'' (e.g., gender, race) on the model output, the model is considered unfair.","However, adversarial attacks can subvert the detection of XAI methods.","Previous approaches to constructing such an adversarial model require access to underlying data distribution, which may not be possible in many practical scenarios.","We relax this constraint and propose a novel family of attacks, called shuffling attacks, that are data-agnostic.","The proposed attack strategies can adapt any trained machine learning model to fool Shapley value-based explanations.","We prove that Shapley values cannot detect shuffling attacks.","However, algorithms that estimate Shapley values, such as linear SHAP and SHAP, can detect these attacks with varying degrees of effectiveness.","We demonstrate the efficacy of the attack strategies by comparing the performance of linear SHAP and SHAP using real-world datasets."],"url":"http://arxiv.org/abs/2408.06509v1"}
{"created":"2024-08-12 21:47:15","title":"Benchmarking tree species classification from proximally-sensed laser scanning data: introducing the FOR-species20K dataset","abstract":"Proximally-sensed laser scanning offers significant potential for automated forest data capture, but challenges remain in automatically identifying tree species without additional ground data. Deep learning (DL) shows promise for automation, yet progress is slowed by the lack of large, diverse, openly available labeled datasets of single tree point clouds. This has impacted the robustness of DL models and the ability to establish best practices for species classification.   To overcome these challenges, the FOR-species20K benchmark dataset was created, comprising over 20,000 tree point clouds from 33 species, captured using terrestrial (TLS), mobile (MLS), and drone laser scanning (ULS) across various European forests, with some data from other regions. This dataset enables the benchmarking of DL models for tree species classification, including both point cloud-based (PointNet++, MinkNet, MLP-Mixer, DGCNNs) and multi-view image-based methods (SimpleView, DetailView, YOLOv5).   2D image-based models generally performed better (average OA = 0.77) than 3D point cloud-based models (average OA = 0.72), with consistent results across different scanning platforms and sensors. The top model, DetailView, was particularly robust, handling data imbalances well and generalizing effectively across tree sizes.   The FOR-species20K dataset, available at https://zenodo.org/records/13255198, is a key resource for developing and benchmarking DL models for tree species classification using laser scanning data, providing a foundation for future advancements in the field.","sentences":["Proximally-sensed laser scanning offers significant potential for automated forest data capture, but challenges remain in automatically identifying tree species without additional ground data.","Deep learning (DL) shows promise for automation, yet progress is slowed by the lack of large, diverse, openly available labeled datasets of single tree point clouds.","This has impacted the robustness of DL models and the ability to establish best practices for species classification.   ","To overcome these challenges, the FOR-species20K benchmark dataset was created, comprising over 20,000 tree point clouds from 33 species, captured using terrestrial (TLS), mobile (MLS), and drone laser scanning (ULS) across various European forests, with some data from other regions.","This dataset enables the benchmarking of DL models for tree species classification, including both point cloud-based (PointNet++, MinkNet, MLP-Mixer, DGCNNs) and multi-view image-based methods (SimpleView, DetailView, YOLOv5).   ","2D image-based models generally performed better (average OA = 0.77) than 3D point cloud-based models (average OA = 0.72), with consistent results across different scanning platforms and sensors.","The top model, DetailView, was particularly robust, handling data imbalances well and generalizing effectively across tree sizes.   ","The FOR-species20K dataset, available at https://zenodo.org/records/13255198, is a key resource for developing and benchmarking DL models for tree species classification using laser scanning data, providing a foundation for future advancements in the field."],"url":"http://arxiv.org/abs/2408.06507v1"}
{"created":"2024-08-12 21:44:43","title":"TacSL: A Library for Visuotactile Sensor Simulation and Learning","abstract":"For both humans and robots, the sense of touch, known as tactile sensing, is critical for performing contact-rich manipulation tasks. Three key challenges in robotic tactile sensing are 1) interpreting sensor signals, 2) generating sensor signals in novel scenarios, and 3) learning sensor-based policies. For visuotactile sensors, interpretation has been facilitated by their close relationship with vision sensors (e.g., RGB cameras). However, generation is still difficult, as visuotactile sensors typically involve contact, deformation, illumination, and imaging, all of which are expensive to simulate; in turn, policy learning has been challenging, as simulation cannot be leveraged for large-scale data collection. We present \\textbf{TacSL} (\\textit{taxel}), a library for GPU-based visuotactile sensor simulation and learning. \\textbf{TacSL} can be used to simulate visuotactile images and extract contact-force distributions over $200\\times$ faster than the prior state-of-the-art, all within the widely-used Isaac Gym simulator. Furthermore, \\textbf{TacSL} provides a learning toolkit containing multiple sensor models, contact-intensive training environments, and online/offline algorithms that can facilitate policy learning for sim-to-real applications. On the algorithmic side, we introduce a novel online reinforcement-learning algorithm called asymmetric actor-critic distillation (\\sysName), designed to effectively and efficiently learn tactile-based policies in simulation that can transfer to the real world. Finally, we demonstrate the utility of our library and algorithms by evaluating the benefits of distillation and multimodal sensing for contact-rich manip ulation tasks, and most critically, performing sim-to-real transfer. Supplementary videos and results are at \\url{https://iakinola23.github.io/tacsl/}.","sentences":["For both humans and robots, the sense of touch, known as tactile sensing, is critical for performing contact-rich manipulation tasks.","Three key challenges in robotic tactile sensing are 1) interpreting sensor signals, 2) generating sensor signals in novel scenarios, and 3) learning sensor-based policies.","For visuotactile sensors, interpretation has been facilitated by their close relationship with vision sensors (e.g., RGB cameras).","However, generation is still difficult, as visuotactile sensors typically involve contact, deformation, illumination, and imaging, all of which are expensive to simulate; in turn, policy learning has been challenging, as simulation cannot be leveraged for large-scale data collection.","We present \\textbf{TacSL} (\\textit{taxel}), a library for GPU-based visuotactile sensor simulation and learning.","\\textbf{TacSL} can be used to simulate visuotactile images and extract contact-force distributions over $200\\times$ faster than the prior state-of-the-art, all within the widely-used Isaac Gym simulator.","Furthermore, \\textbf{TacSL} provides a learning toolkit containing multiple sensor models, contact-intensive training environments, and online/offline algorithms that can facilitate policy learning for sim-to-real applications.","On the algorithmic side, we introduce a novel online reinforcement-learning algorithm called asymmetric actor-critic distillation (\\sysName), designed to effectively and efficiently learn tactile-based policies in simulation that can transfer to the real world.","Finally, we demonstrate the utility of our library and algorithms by evaluating the benefits of distillation and multimodal sensing for contact-rich manip ulation tasks, and most critically, performing sim-to-real transfer.","Supplementary videos and results are at \\url{https://iakinola23.github.io/tacsl/}."],"url":"http://arxiv.org/abs/2408.06506v1"}
{"created":"2024-08-12 21:40:39","title":"Multilingual Crowd-Based Requirements Engineering Using Large Language Models","abstract":"A central challenge for ensuring the success of software projects is to assure the convergence of developers' and users' views. While the availability of large amounts of user data from social media, app store reviews, and support channels bears many benefits, it still remains unclear how software development teams can effectively use this data. We present an LLM-powered approach called DeeperMatcher that helps agile teams use crowd-based requirements engineering (CrowdRE) in their issue and task management. We are currently implementing a command-line tool that enables developers to match issues with relevant user reviews. We validated our approach on an existing English dataset from a well-known open-source project. Additionally, to check how well DeeperMatcher works for other languages, we conducted a single-case mechanism experiment alongside developers of a local project that has issues and user feedback in Brazilian Portuguese. Our preliminary analysis indicates that the accuracy of our approach is highly dependent on the text embedding method used. We discuss further refinements needed for reliable crowd-based requirements engineering with multilingual support.","sentences":["A central challenge for ensuring the success of software projects is to assure the convergence of developers' and users' views.","While the availability of large amounts of user data from social media, app store reviews, and support channels bears many benefits, it still remains unclear how software development teams can effectively use this data.","We present an LLM-powered approach called DeeperMatcher that helps agile teams use crowd-based requirements engineering (CrowdRE) in their issue and task management.","We are currently implementing a command-line tool that enables developers to match issues with relevant user reviews.","We validated our approach on an existing English dataset from a well-known open-source project.","Additionally, to check how well DeeperMatcher works for other languages, we conducted a single-case mechanism experiment alongside developers of a local project that has issues and user feedback in Brazilian Portuguese.","Our preliminary analysis indicates that the accuracy of our approach is highly dependent on the text embedding method used.","We discuss further refinements needed for reliable crowd-based requirements engineering with multilingual support."],"url":"http://arxiv.org/abs/2408.06505v1"}
{"created":"2024-08-12 20:51:31","title":"Path Partitions of Phylogenetic Networks","abstract":"In phylogenetics, evolution is traditionally represented in a tree-like manner. However, phylogenetic networks can be more appropriate for representing evolutionary events such as hybridization, horizontal gene transfer, and others. In particular, the class of forest-based networks was recently introduced to represent introgression, in which genes are swapped between between species. A network is forest-based if it can be obtained by adding arcs to a collection of trees, so that the endpoints of the new arcs are in different trees. This contrasts with so-called tree-based networks, which are formed by adding arcs within a single tree.   We are interested in the computational complexity of recognizing forest-based networks, which was recently left as an open problem by Huber et al. Forest-based networks coincide with directed acyclic graphs that can be partitioned into induced paths, each ending at a leaf of the original graph. Several types of path partitions have been studied in the graph theory literature, but to our knowledge this type of leaf induced path partition has not been considered before. The study of forest-based networks in terms of these partitions allows us to establish closer relationships between phylogenetics and algorithmic graph theory, and to provide answers to problems in both fields.   We show that deciding whether a network is forest-based is NP-complete, even on input networks that are tree-based, binary, and have only three leaves. This shows that partitioning a directed acyclic graph into three induced paths is NP-complete, answering a recent question of Ferneau et al. We then show that the problem is polynomial-time solvable on binary networks with two leaves and on the class of orchards. Finally, for undirected graphs, we introduce unrooted forest-based networks and provide hardness results for this class as well.","sentences":["In phylogenetics, evolution is traditionally represented in a tree-like manner.","However, phylogenetic networks can be more appropriate for representing evolutionary events such as hybridization, horizontal gene transfer, and others.","In particular, the class of forest-based networks was recently introduced to represent introgression, in which genes are swapped between between species.","A network is forest-based if it can be obtained by adding arcs to a collection of trees, so that the endpoints of the new arcs are in different trees.","This contrasts with so-called tree-based networks, which are formed by adding arcs within a single tree.   ","We are interested in the computational complexity of recognizing forest-based networks, which was recently left as an open problem by Huber et al.","Forest-based networks coincide with directed acyclic graphs that can be partitioned into induced paths, each ending at a leaf of the original graph.","Several types of path partitions have been studied in the graph theory literature, but to our knowledge this type of leaf induced path partition has not been considered before.","The study of forest-based networks in terms of these partitions allows us to establish closer relationships between phylogenetics and algorithmic graph theory, and to provide answers to problems in both fields.   ","We show that deciding whether a network is forest-based is NP-complete, even on input networks that are tree-based, binary, and have only three leaves.","This shows that partitioning a directed acyclic graph into three induced paths is NP-complete, answering a recent question of Ferneau et al.","We then show that the problem is polynomial-time solvable on binary networks with two leaves and on the class of orchards.","Finally, for undirected graphs, we introduce unrooted forest-based networks and provide hardness results for this class as well."],"url":"http://arxiv.org/abs/2408.06489v1"}
{"created":"2024-08-12 20:41:07","title":"Implicit Neural Representation For Accurate CFD Flow Field Prediction","abstract":"Despite the plethora of deep learning frameworks for flow field prediction, most of them deal with flow fields on regular domains, and although the best ones can cope with irregular domains, they mostly rely on graph networks, so that real industrial applications remain currently elusive. We present a deep learning framework for 3D flow field prediction applied to blades of aircraft engine turbines and compressors. Crucially, we view any 3D field as a function from coordinates that is modeled by a neural network we call the backbone-net. It inherits the property of coordinate-based MLPs, namely the discretization-agnostic representation of flow fields in domains of arbitrary topology at infinite resolution. First, we demonstrate the performance of the backbone-net solo in regressing 3D steady simulations of single blade rows in various flow regimes: it can accurately render important flow characteristics such as boundary layers, wakes and shock waves. Second, we introduce a hyper-net that maps the surface mesh of a blade to the parameters of the backbone-net. By doing so, the flow solution can be directly predicted from the blade geometry, irrespective of its parameterization. Together, backbone-net and hyper-net form a highly-accurate memory-efficient data-driven proxy to CFD solvers with good generalization on unseen geometries.","sentences":["Despite the plethora of deep learning frameworks for flow field prediction, most of them deal with flow fields on regular domains, and although the best ones can cope with irregular domains, they mostly rely on graph networks, so that real industrial applications remain currently elusive.","We present a deep learning framework for 3D flow field prediction applied to blades of aircraft engine turbines and compressors.","Crucially, we view any 3D field as a function from coordinates that is modeled by a neural network we call the backbone-net.","It inherits the property of coordinate-based MLPs, namely the discretization-agnostic representation of flow fields in domains of arbitrary topology at infinite resolution.","First, we demonstrate the performance of the backbone-net solo in regressing 3D steady simulations of single blade rows in various flow regimes: it can accurately render important flow characteristics such as boundary layers, wakes and shock waves.","Second, we introduce a hyper-net that maps the surface mesh of a blade to the parameters of the backbone-net.","By doing so, the flow solution can be directly predicted from the blade geometry, irrespective of its parameterization.","Together, backbone-net and hyper-net form a highly-accurate memory-efficient data-driven proxy to CFD solvers with good generalization on unseen geometries."],"url":"http://arxiv.org/abs/2408.06486v1"}
{"created":"2024-08-12 20:32:54","title":"Clock Auctions Augmented with Unreliable Advice","abstract":"We provide the first analysis of clock auctions through the learning-augmented framework. Deferred-acceptance clock auctions are a compelling class of mechanisms satisfying a unique list of highly practical properties, including obvious strategy-proofness, transparency, and unconditional winner privacy, making them particularly well-suited for real-world applications. However, early work that evaluated their performance from a worst-case analysis standpoint concluded that no deterministic clock auction can achieve much better than an $O(\\log n)$ approximation of the optimal social welfare (where $n$ is the number of bidders participating in the auction), even in seemingly very simple settings.   To overcome this overly pessimistic impossibility result, which heavily depends on the assumption that the designer has no information regarding the preferences of the participating bidders, we leverage the learning-augmented framework. This framework assumes that the designer is provided with some advice regarding what the optimal solution may be. This advice may be the product of machine-learning algorithms applied to historical data, so it can provide very useful guidance, but it can also be highly unreliable.   Our main results are learning-augmented clock auctions that use this advice to achieve much stronger performance guarantees whenever the advice is accurate (known as consistency), while simultaneously maintaining worst-case guarantees even if this advice is arbitrarily inaccurate (known as robustness). Specifically, for the standard notion of consistency, we provide a clock auction that achieves the best of both worlds: $(1+\\epsilon)$-consistency for any constant $\\epsilon > 0$ and $O(\\log n)$ robustness. We then also consider a much stronger notion of consistency and provide an auction that achieves the optimal trade-off between this notion of consistency and robustness.","sentences":["We provide the first analysis of clock auctions through the learning-augmented framework.","Deferred-acceptance clock auctions are a compelling class of mechanisms satisfying a unique list of highly practical properties, including obvious strategy-proofness, transparency, and unconditional winner privacy, making them particularly well-suited for real-world applications.","However, early work that evaluated their performance from a worst-case analysis standpoint concluded that no deterministic clock auction can achieve much better than an $O(\\log n)$ approximation of the optimal social welfare (where $n$ is the number of bidders participating in the auction), even in seemingly very simple settings.   ","To overcome this overly pessimistic impossibility result, which heavily depends on the assumption that the designer has no information regarding the preferences of the participating bidders, we leverage the learning-augmented framework.","This framework assumes that the designer is provided with some advice regarding what the optimal solution may be.","This advice may be the product of machine-learning algorithms applied to historical data, so it can provide very useful guidance, but it can also be highly unreliable.   ","Our main results are learning-augmented clock auctions that use this advice to achieve much stronger performance guarantees whenever the advice is accurate (known as consistency), while simultaneously maintaining worst-case guarantees even if this advice is arbitrarily inaccurate (known as robustness).","Specifically, for the standard notion of consistency, we provide a clock auction that achieves the best of both worlds: $(1+\\epsilon)$-consistency for any constant $\\epsilon > 0$ and $O(\\log n)$ robustness.","We then also consider a much stronger notion of consistency and provide an auction that achieves the optimal trade-off between this notion of consistency and robustness."],"url":"http://arxiv.org/abs/2408.06483v1"}
{"created":"2024-08-12 20:21:03","title":"Quasi-Monte Carlo Beyond Hardy-Krause","abstract":"The classical approaches to numerically integrating a function $f$ are Monte Carlo (MC) and quasi-Monte Carlo (QMC) methods. MC methods use random samples to evaluate $f$ and have error $O(\\sigma(f)/\\sqrt{n})$, where $\\sigma(f)$ is the standard deviation of $f$. QMC methods are based on evaluating $f$ at explicit point sets with low discrepancy, and as given by the classical Koksma-Hlawka inequality, they have error $\\widetilde{O}(\\sigma_{\\mathsf{HK}}(f)/n)$, where $\\sigma_{\\mathsf{HK}}(f)$ is the variation of $f$ in the sense of Hardy and Krause. These two methods have distinctive advantages and shortcomings, and a fundamental question is to find a method that combines the advantages of both.   In this work, we give a simple randomized algorithm that produces QMC point sets with the following desirable features: (1) It achieves substantially better error than given by the classical Koksma-Hlawka inequality. In particular, it has error $\\widetilde{O}(\\sigma_{\\mathsf{SO}}(f)/n)$, where $\\sigma_{\\mathsf{SO}}(f)$ is a new measure of variation that we introduce, which is substantially smaller than the Hardy-Krause variation. (2) The algorithm only requires random samples from the underlying distribution, which makes it as flexible as MC. (3) It automatically achieves the best of both MC and QMC (and the above improvement over Hardy-Krause variation) in an optimal way. (4) The algorithm is extremely efficient, with an amortized $\\widetilde{O}(1)$ runtime per sample.   Our method is based on the classical transference principle in geometric discrepancy, combined with recent algorithmic innovations in combinatorial discrepancy that besides producing low-discrepancy colorings, also guarantee certain subgaussian properties. This allows us to bypass several limitations of previous works in bridging the gap between MC and QMC methods and go beyond the Hardy-Krause variation.","sentences":["The classical approaches to numerically integrating a function $f$ are Monte Carlo (MC) and quasi-Monte Carlo (QMC) methods.","MC methods use random samples to evaluate $f$ and have error $O(\\sigma(f)/\\sqrt{n})$, where $\\sigma(f)$ is the standard deviation of $f$. QMC methods are based on evaluating $f$ at explicit point sets with low discrepancy, and as given by the classical Koksma-Hlawka inequality, they have error $\\widetilde{O}(\\sigma_{\\mathsf{HK}}(f)/n)$, where $\\sigma_{\\mathsf{HK}}(f)$ is the variation of $f$ in the sense of Hardy and Krause.","These two methods have distinctive advantages and shortcomings, and a fundamental question is to find a method that combines the advantages of both.   ","In this work, we give a simple randomized algorithm that produces QMC point sets with the following desirable features: (1) It achieves substantially better error than given by the classical Koksma-Hlawka inequality.","In particular, it has error $\\widetilde{O}(\\sigma_{\\mathsf{SO}}(f)/n)$, where $\\sigma_{\\mathsf{SO}}(f)$ is a new measure of variation that we introduce, which is substantially smaller than the Hardy-Krause variation.","(2) The algorithm only requires random samples from the underlying distribution, which makes it as flexible as MC.","(3) It automatically achieves the best of both MC and QMC (and the above improvement over Hardy-Krause variation) in an optimal way.","(4) The algorithm is extremely efficient, with an amortized $\\widetilde{O}(1)$ runtime per sample.   ","Our method is based on the classical transference principle in geometric discrepancy, combined with recent algorithmic innovations in combinatorial discrepancy that besides producing low-discrepancy colorings, also guarantee certain subgaussian properties.","This allows us to bypass several limitations of previous works in bridging the gap between MC and QMC methods and go beyond the Hardy-Krause variation."],"url":"http://arxiv.org/abs/2408.06475v1"}
{"created":"2024-08-12 20:19:27","title":"TOGGL: Transcribing Overlapping Speech with Staggered Labeling","abstract":"Transcribing the speech of multiple overlapping speakers typically requires separating the audio into multiple streams and recognizing each one independently. More recent work jointly separates and transcribes, but requires a separate decoding component for each speaker. We propose the TOGGL model to simultaneously transcribe the speech of multiple speakers. The TOGGL model uses special output tokens to attribute the speech to each speaker with only a single decoder. Our approach generalizes beyond two speakers, even when trained only on two-speaker data. We demonstrate superior performance compared to competing approaches on a conversational speech dataset. Our approach also improves performance on single-speaker audio.","sentences":["Transcribing the speech of multiple overlapping speakers typically requires separating the audio into multiple streams and recognizing each one independently.","More recent work jointly separates and transcribes, but requires a separate decoding component for each speaker.","We propose the TOGGL model to simultaneously transcribe the speech of multiple speakers.","The TOGGL model uses special output tokens to attribute the speech to each speaker with only a single decoder.","Our approach generalizes beyond two speakers, even when trained only on two-speaker data.","We demonstrate superior performance compared to competing approaches on a conversational speech dataset.","Our approach also improves performance on single-speaker audio."],"url":"http://arxiv.org/abs/2408.06474v1"}
{"created":"2024-08-12 19:32:28","title":"Kernel Sum of Squares for Data Adapted Kernel Learning of Dynamical Systems from Data: A global optimization approach","abstract":"This paper examines the application of the Kernel Sum of Squares (KSOS) method for enhancing kernel learning from data, particularly in the context of dynamical systems. Traditional kernel-based methods, despite their theoretical soundness and numerical efficiency, frequently struggle with selecting optimal base kernels and parameter tuning, especially with gradient-based methods prone to local optima. KSOS mitigates these issues by leveraging a global optimization framework with kernel-based surrogate functions, thereby achieving more reliable and precise learning of dynamical systems. Through comprehensive numerical experiments on the Logistic Map, Henon Map, and Lorentz System, KSOS is shown to consistently outperform gradient descent in minimizing the relative-$\\rho$ metric and improving kernel accuracy. These results highlight KSOS's effectiveness in predicting the behavior of chaotic dynamical systems, demonstrating its capability to adapt kernels to underlying dynamics and enhance the robustness and predictive power of kernel-based approaches, making it a valuable asset for time series analysis in various scientific fields.","sentences":["This paper examines the application of the Kernel Sum of Squares (KSOS) method for enhancing kernel learning from data, particularly in the context of dynamical systems.","Traditional kernel-based methods, despite their theoretical soundness and numerical efficiency, frequently struggle with selecting optimal base kernels and parameter tuning, especially with gradient-based methods prone to local optima.","KSOS mitigates these issues by leveraging a global optimization framework with kernel-based surrogate functions, thereby achieving more reliable and precise learning of dynamical systems.","Through comprehensive numerical experiments on the Logistic Map, Henon Map, and Lorentz System, KSOS is shown to consistently outperform gradient descent in minimizing the relative-$\\rho$ metric and improving kernel accuracy.","These results highlight KSOS's effectiveness in predicting the behavior of chaotic dynamical systems, demonstrating its capability to adapt kernels to underlying dynamics and enhance the robustness and predictive power of kernel-based approaches, making it a valuable asset for time series analysis in various scientific fields."],"url":"http://arxiv.org/abs/2408.06465v1"}
{"created":"2024-08-12 19:28:57","title":"Statistical Quality Comparison of the Bitstrings Generated by a Physical Unclonable Function across Xilinx, Altera and Microsemi Devices","abstract":"Entropy or randomness represents a foundational security property in security-related operations, such as key generation. Key generation in turn is central to security protocols such as authentication and encryption. Physical unclonable functions (PUF) are hardware-based primitives that can serve as key generation engines in modern microelectronic devices and applications. PUFs derive entropy from manufacturing variations that exist naturally within and across otherwise identical copies of a device. However, the levels of random variations that represent entropy, which are strongly correlated to the quality of the PUF-generated bitstrings, vary from one manufacturer to another. In this paper, we evaluate entropy across a set of devices manufactured by three mainstream FPGA vendors, Xilinx, Altera and Microsemi. The devices selected for evaluation are considered low-end commercial devices to make the analysis relevant to IoT applications. The SiRF PUF is used in the evaluation, and is constructed nearly identically across the three vendor devices, setting aside minor differences that exist in certain logic element primitives used within the PUF architecture, and which have only a minor impact on our comparative analysis. The SiRF PUF uses a high-resolution time-to-digital converter (TDC) crafted from high-speed carry-chain logic embedded within each device to measure path delays in an engineered netlist of logic gates as a source of entropy. Therefore, our analysis includes an evaluation of actual path delay variation as it exists across the three device classes, as well as a statistical evaluation of the PUF-generated bitstrings. A reliablity analysis is also provided using data collected in industrial-standard temperature experiments to round out the evaluation of important statistical properties of the PUF.","sentences":["Entropy or randomness represents a foundational security property in security-related operations, such as key generation.","Key generation in turn is central to security protocols such as authentication and encryption.","Physical unclonable functions (PUF) are hardware-based primitives that can serve as key generation engines in modern microelectronic devices and applications.","PUFs derive entropy from manufacturing variations that exist naturally within and across otherwise identical copies of a device.","However, the levels of random variations that represent entropy, which are strongly correlated to the quality of the PUF-generated bitstrings, vary from one manufacturer to another.","In this paper, we evaluate entropy across a set of devices manufactured by three mainstream FPGA vendors, Xilinx, Altera and Microsemi.","The devices selected for evaluation are considered low-end commercial devices to make the analysis relevant to IoT applications.","The SiRF PUF is used in the evaluation, and is constructed nearly identically across the three vendor devices, setting aside minor differences that exist in certain logic element primitives used within the PUF architecture, and which have only a minor impact on our comparative analysis.","The SiRF PUF uses a high-resolution time-to-digital converter (TDC) crafted from high-speed carry-chain logic embedded within each device to measure path delays in an engineered netlist of logic gates as a source of entropy.","Therefore, our analysis includes an evaluation of actual path delay variation as it exists across the three device classes, as well as a statistical evaluation of the PUF-generated bitstrings.","A reliablity analysis is also provided using data collected in industrial-standard temperature experiments to round out the evaluation of important statistical properties of the PUF."],"url":"http://arxiv.org/abs/2408.06463v1"}
{"created":"2024-08-12 19:21:34","title":"Evaluating Privacy Measures for Load Hiding","abstract":"In smart grids, the use of smart meters to measure electricity consumption at a household level raises privacy concerns. To address them, researchers have designed various load hiding algorithms that manipulate the electricity consumption measured. To compare how well these algorithms preserve privacy, various privacy measures have been proposed. However, there currently is no consensus on which privacy measure is most appropriate to use. In this study, we aim to identify the most effective privacy measure(s) for load hiding algorithms. We have crafted a series of experiments to assess the effectiveness of these measures. found 20 of the 25 measures studied to be ineffective. Next, focused on the well-known \"appliance usage\" secret, we have designed synthetic data to find the measure that best deals with this secret. We observe that such a measure, a variant of mutual information, actually exists.","sentences":["In smart grids, the use of smart meters to measure electricity consumption at a household level raises privacy concerns.","To address them, researchers have designed various load hiding algorithms that manipulate the electricity consumption measured.","To compare how well these algorithms preserve privacy, various privacy measures have been proposed.","However, there currently is no consensus on which privacy measure is most appropriate to use.","In this study, we aim to identify the most effective privacy measure(s) for load hiding algorithms.","We have crafted a series of experiments to assess the effectiveness of these measures.","found 20 of the 25 measures studied to be ineffective.","Next, focused on the well-known \"appliance usage\" secret, we have designed synthetic data to find the measure that best deals with this secret.","We observe that such a measure, a variant of mutual information, actually exists."],"url":"http://arxiv.org/abs/2408.06460v1"}
{"created":"2024-08-12 19:17:57","title":"Advanced Vision Transformers and Open-Set Learning for Robust Mosquito Classification: A Novel Approach to Entomological Studies","abstract":"Mosquito-related diseases pose a significant threat to global public health, necessitating efficient and accurate mosquito classification for effective surveillance and control. This work presents an innovative approach to mosquito classification by leveraging state-of-the-art vision transformers and open-set learning techniques. A novel framework has been introduced that integrates Transformer-based deep learning models with comprehensive data augmentation and preprocessing methods, enabling robust and precise identification of ten mosquito species. The Swin Transformer model achieves the best performance for traditional closed-set learning with 99.80\\% accuracy and 0.998 F1 score. The lightweight MobileViT technique attains an almost similar accuracy of 98.90\\% with significantly reduced parameters and model complexities. Next, the applied deep learning models' adaptability and generalizability in a static environment have been enhanced by using new classes of data samples during the inference stage that have not been included in the training set. The proposed framework's ability to handle unseen classes like insects similar to mosquitoes, even humans, through open-set learning further enhances its practical applicability employing the OpenMax technique and Weibull distribution. The traditional CNN model, Xception, outperforms the latest transformer with higher accuracy and F1 score for open-set learning. The study's findings highlight the transformative potential of advanced deep-learning architectures in entomology, providing a strong groundwork for future research and development in mosquito surveillance and vector control. The implications of this work extend beyond mosquito classification, offering valuable insights for broader ecological and environmental monitoring applications.","sentences":["Mosquito-related diseases pose a significant threat to global public health, necessitating efficient and accurate mosquito classification for effective surveillance and control.","This work presents an innovative approach to mosquito classification by leveraging state-of-the-art vision transformers and open-set learning techniques.","A novel framework has been introduced that integrates Transformer-based deep learning models with comprehensive data augmentation and preprocessing methods, enabling robust and precise identification of ten mosquito species.","The Swin Transformer model achieves the best performance for traditional closed-set learning with 99.80\\% accuracy and 0.998 F1 score.","The lightweight MobileViT technique attains an almost similar accuracy of 98.90\\% with significantly reduced parameters and model complexities.","Next, the applied deep learning models' adaptability and generalizability in a static environment have been enhanced by using new classes of data samples during the inference stage that have not been included in the training set.","The proposed framework's ability to handle unseen classes like insects similar to mosquitoes, even humans, through open-set learning further enhances its practical applicability employing the OpenMax technique and Weibull distribution.","The traditional CNN model, Xception, outperforms the latest transformer with higher accuracy and F1 score for open-set learning.","The study's findings highlight the transformative potential of advanced deep-learning architectures in entomology, providing a strong groundwork for future research and development in mosquito surveillance and vector control.","The implications of this work extend beyond mosquito classification, offering valuable insights for broader ecological and environmental monitoring applications."],"url":"http://arxiv.org/abs/2408.06457v1"}
{"created":"2024-08-12 19:14:55","title":"Massively Parallel Minimum Spanning Tree in General Metric Spaces","abstract":"We study the minimum spanning tree (MST) problem in the massively parallel computation (MPC) model. Our focus is particularly on the *strictly sublinear* regime of MPC where the space per machine is $O(n^\\delta)$. Here $n$ is the number of vertices and constant $\\delta \\in (0, 1)$ can be made arbitrarily small. The MST problem admits a simple and folklore $O(\\log n)$-round algorithm in the MPC model. When the weights can be arbitrary, this matches a conditional lower bound of $\\Omega(\\log n)$ which follows from a well-known 1vs2-Cycle conjecture. As such, much of the literature focuses on breaking the logarithmic barrier in more structured variants of the problem, such as when the vertices correspond to points in low- [ANOY14, STOC'14] or high-dimensional Euclidean spaces [JMNZ, SODA'24].   In this work, we focus more generally on metric spaces. Namely, all pairwise weights are provided and guaranteed to satisfy the triangle inequality, but are otherwise unconstrained. We show that for any $\\varepsilon > 0$, a $(1+\\varepsilon)$-approximate MST can be found in $O(\\log \\frac{1}{\\varepsilon} + \\log \\log n)$ rounds, which is the first $o(\\log n)$-round algorithm for finding any constant approximation in this setting. Other than being applicable to more general weight functions, our algorithm also slightly improves the $O(\\log \\log n \\cdot \\log \\log \\log n)$ round-complexity of [JMNZ24, SODA'24] and significantly improves its approximation from a large constant to $1+\\varepsilon$.   On the lower bound side, we prove that under the 1vs2-Cycle conjecture, $\\Omega(\\log \\frac{1}{\\varepsilon})$ rounds are needed for finding a $(1+\\varepsilon)$-approximate MST in general metrics. It is worth noting that while many existing lower bounds in the MPC model under the 1vs2-Cycle conjecture only hold against \"component stable\" algorithms, our lower bound applies to *all* algorithms.","sentences":["We study the minimum spanning tree (MST) problem in the massively parallel computation (MPC) model.","Our focus is particularly on the *strictly sublinear* regime of MPC where the space per machine is $O(n^\\delta)$. Here $n$ is the number of vertices and constant $\\delta \\in (0, 1)$ can be made arbitrarily small.","The MST problem admits a simple and folklore $O(\\log n)$-round algorithm in the MPC model.","When the weights can be arbitrary, this matches a conditional lower bound of $\\Omega(\\log n)$ which follows from a well-known 1vs2-Cycle conjecture.","As such, much of the literature focuses on breaking the logarithmic barrier in more structured variants of the problem, such as when the vertices correspond to points in low-","[ANOY14, STOC'14] or high-dimensional Euclidean spaces","[JMNZ, SODA'24].   In this work, we focus more generally on metric spaces.","Namely, all pairwise weights are provided and guaranteed to satisfy the triangle inequality, but are otherwise unconstrained.","We show that for any $\\varepsilon > 0$, a $(1+\\varepsilon)$-approximate MST can be found in $O(\\log \\frac{1}{\\varepsilon} + \\log \\log n)$ rounds, which is the first $o(\\log n)$-round algorithm for finding any constant approximation in this setting.","Other than being applicable to more general weight functions, our algorithm also slightly improves the $O(\\log \\log n \\cdot \\log \\log \\log n)$ round-complexity of [JMNZ24, SODA'24] and significantly improves its approximation from a large constant to $1+\\varepsilon$.   On the lower bound side, we prove that under the 1vs2-Cycle conjecture, $\\Omega(\\log \\frac{1}{\\varepsilon})$ rounds are needed for finding a $(1+\\varepsilon)$-approximate MST in general metrics.","It is worth noting that while many existing lower bounds in the MPC model under the 1vs2-Cycle conjecture only hold against \"component stable\" algorithms, our lower bound applies to *all* algorithms."],"url":"http://arxiv.org/abs/2408.06455v1"}
{"created":"2024-08-12 18:55:12","title":"Tactile Melodies: A Desk-Mounted Haptics for Perceiving Musical Experiences","abstract":"This paper introduces a novel interface for experiencing music through haptic impulses to the palm of the hand. It presents a practical implementation of the system exploring the realm of musical haptics through the translation of MIDI data from a Digital Audio Workstation (DAW) into haptic sensations, from a set of haptic actuators, in real-time. It also includes a suitable music-to-haptic mapping strategy to translate notes from musical instruments to haptic feedback. The haptic actuators, placed strategically on the palmar surface of the hand allowed users to perceive music and were able to identify melody and rhythm of different musical compositions. A pilot user study conducted intended to assess the accuracy of the interface by testing the participants to select the correct audio presentation from the haptic presentation of the same musical composition. It presents a comparative study, differentiating between those with prior musical background and those without, in identifying the correct audio counterpart solely through haptic inputs. This pilot study delves into how users perceive and interpret haptic feedback within the context of musical compositions. The study showed promising results in enriching our understanding of user responses to haptic feedback in musical scenarios and exploring the intricacies of user experience with the system and its impact on musical interpretation.","sentences":["This paper introduces a novel interface for experiencing music through haptic impulses to the palm of the hand.","It presents a practical implementation of the system exploring the realm of musical haptics through the translation of MIDI data from a Digital Audio Workstation (DAW) into haptic sensations, from a set of haptic actuators, in real-time.","It also includes a suitable music-to-haptic mapping strategy to translate notes from musical instruments to haptic feedback.","The haptic actuators, placed strategically on the palmar surface of the hand allowed users to perceive music and were able to identify melody and rhythm of different musical compositions.","A pilot user study conducted intended to assess the accuracy of the interface by testing the participants to select the correct audio presentation from the haptic presentation of the same musical composition.","It presents a comparative study, differentiating between those with prior musical background and those without, in identifying the correct audio counterpart solely through haptic inputs.","This pilot study delves into how users perceive and interpret haptic feedback within the context of musical compositions.","The study showed promising results in enriching our understanding of user responses to haptic feedback in musical scenarios and exploring the intricacies of user experience with the system and its impact on musical interpretation."],"url":"http://arxiv.org/abs/2408.06449v1"}
{"created":"2024-08-12 18:49:02","title":"Multi-View Neural Differential Equations for Continuous-Time Stream Data in Long-Term Traffic Forecasting","abstract":"Long-term traffic flow forecasting plays a crucial role in intelligent transportation as it allows traffic managers to adjust their decisions in advance. However, the problem is challenging due to spatio-temporal correlations and complex dynamic patterns in continuous-time stream data. Neural Differential Equations (NDEs) are among the state-of-the-art methods for learning continuous-time traffic dynamics. However, the traditional NDE models face issues in long-term traffic forecasting due to failures in capturing delayed traffic patterns, dynamic edge (location-to-location correlation) patterns, and abrupt trend patterns. To fill this gap, we propose a new NDE architecture called Multi-View Neural Differential Equations. Our model captures current states, delayed states, and trends in different state variables (views) by learning latent multiple representations within Neural Differential Equations. Extensive experiments conducted on several real-world traffic datasets demonstrate that our proposed method outperforms the state-of-the-art and achieves superior prediction accuracy for long-term forecasting and robustness with noisy or missing inputs.","sentences":["Long-term traffic flow forecasting plays a crucial role in intelligent transportation as it allows traffic managers to adjust their decisions in advance.","However, the problem is challenging due to spatio-temporal correlations and complex dynamic patterns in continuous-time stream data.","Neural Differential Equations (NDEs) are among the state-of-the-art methods for learning continuous-time traffic dynamics.","However, the traditional NDE models face issues in long-term traffic forecasting due to failures in capturing delayed traffic patterns, dynamic edge (location-to-location correlation) patterns, and abrupt trend patterns.","To fill this gap, we propose a new NDE architecture called Multi-View Neural Differential Equations.","Our model captures current states, delayed states, and trends in different state variables (views) by learning latent multiple representations within Neural Differential Equations.","Extensive experiments conducted on several real-world traffic datasets demonstrate that our proposed method outperforms the state-of-the-art and achieves superior prediction accuracy for long-term forecasting and robustness with noisy or missing inputs."],"url":"http://arxiv.org/abs/2408.06445v1"}
{"created":"2024-08-12 18:17:30","title":"BFTBrain: Adaptive BFT Consensus with Reinforcement Learning","abstract":"This paper presents BFTBrain, a reinforcement learning (RL) based Byzantine fault-tolerant (BFT) system that provides significant operational benefits: a plug-and-play system suitable for a broad set of hardware and network configurations, and adjusts effectively in real-time to changing fault scenarios and workloads. BFTBrain adapts to system conditions and application needs by switching between a set of BFT protocols in real-time. Two main advances contribute to BFTBrain's agility and performance. First, BFTBrain is based on a systematic, thorough modeling of metrics that correlate the performance of the studied BFT protocols with varying fault scenarios and workloads. These metrics are fed as features to BFTBrain's RL engine in order to choose the best-performing BFT protocols in real-time. Second, BFTBrain coordinates RL in a decentralized manner which is resilient to adversarial data pollution, where nodes share local metering values and reach the same learning output by consensus. As a result, in addition to providing significant operational benefits, BFTBrain improves throughput over fixed protocols by $18\\%$ to $119\\%$ under dynamic conditions and outperforms state-of-the-art learning based approaches by $44\\%$ to $154\\%$.","sentences":["This paper presents BFTBrain, a reinforcement learning (RL) based Byzantine fault-tolerant (BFT) system that provides significant operational benefits: a plug-and-play system suitable for a broad set of hardware and network configurations, and adjusts effectively in real-time to changing fault scenarios and workloads.","BFTBrain adapts to system conditions and application needs by switching between a set of BFT protocols in real-time.","Two main advances contribute to BFTBrain's agility and performance.","First, BFTBrain is based on a systematic, thorough modeling of metrics that correlate the performance of the studied BFT protocols with varying fault scenarios and workloads.","These metrics are fed as features to BFTBrain's RL engine in order to choose the best-performing BFT protocols in real-time.","Second, BFTBrain coordinates RL in a decentralized manner which is resilient to adversarial data pollution, where nodes share local metering values and reach the same learning output by consensus.","As a result, in addition to providing significant operational benefits, BFTBrain improves throughput over fixed protocols by $18\\%$ to $119\\%$ under dynamic conditions and outperforms state-of-the-art learning based approaches by $44\\%$ to $154\\%$."],"url":"http://arxiv.org/abs/2408.06432v1"}
{"created":"2024-08-12 18:01:50","title":"Evaluating Language Models on Entity Disambiguation in Tables","abstract":"Tables are crucial containers of information, but understanding their meaning may be challenging. Indeed, recently, there has been a focus on Semantic Table Interpretation (STI), i.e., the task that involves the semantic annotation of tabular data to disambiguate their meaning. Over the years, there has been a surge in interest in data-driven approaches based on deep learning that have increasingly been combined with heuristic-based approaches. In the last period, the advent of Large Language Models (LLMs) has led to a new category of approaches for table annotation. The interest in this research field, characterised by multiple challenges, has led to a proliferation of approaches employing different techniques. However, these approaches have not been consistently evaluated on a common ground, making evaluation and comparison difficult. This work proposes an extensive evaluation of four state-of-the-art (SOTA) approaches - Alligator (formerly s-elBat), Dagobah, TURL, and TableLlama; the first two belong to the family of heuristic-based algorithms, while the others are respectively encoder-only and decoder-only LLMs. The primary objective is to measure the ability of these approaches to solve the entity disambiguation task, with the ultimate aim of charting new research paths in the field.","sentences":["Tables are crucial containers of information, but understanding their meaning may be challenging.","Indeed, recently, there has been a focus on Semantic Table Interpretation (STI), i.e., the task that involves the semantic annotation of tabular data to disambiguate their meaning.","Over the years, there has been a surge in interest in data-driven approaches based on deep learning that have increasingly been combined with heuristic-based approaches.","In the last period, the advent of Large Language Models (LLMs) has led to a new category of approaches for table annotation.","The interest in this research field, characterised by multiple challenges, has led to a proliferation of approaches employing different techniques.","However, these approaches have not been consistently evaluated on a common ground, making evaluation and comparison difficult.","This work proposes an extensive evaluation of four state-of-the-art (SOTA) approaches - Alligator (formerly s-elBat), Dagobah, TURL, and TableLlama; the first two belong to the family of heuristic-based algorithms, while the others are respectively encoder-only and decoder-only LLMs.","The primary objective is to measure the ability of these approaches to solve the entity disambiguation task, with the ultimate aim of charting new research paths in the field."],"url":"http://arxiv.org/abs/2408.06423v1"}
{"created":"2024-08-12 17:52:29","title":"Moo-ving Beyond Tradition: Revolutionizing Cattle Behavioural Phenotyping with Pose Estimation Techniques","abstract":"The cattle industry has been a major contributor to the economy of many countries, including the US and Canada. The integration of Artificial Intelligence (AI) has revolutionized this sector, mirroring its transformative impact across all industries by enabling scalable and automated monitoring and intervention practices. AI has also introduced tools and methods that automate many tasks previously performed by human labor with the help of computer vision, including health inspections. Among these methods, pose estimation has a special place; pose estimation is the process of finding the position of joints in an image of animals. Analyzing the pose of animal subjects enables precise identification and tracking of the animal's movement and the movements of its body parts. By summarizing the video and imagery data into movement and joint location using pose estimation and then analyzing this information, we can address the scalability challenge in cattle management, focusing on health monitoring, behavioural phenotyping and welfare concerns. Our study reviews recent advancements in pose estimation methodologies, their applicability in improving the cattle industry, existing challenges, and gaps in this field. Furthermore, we propose an initiative to enhance open science frameworks within this field of study by launching a platform designed to connect industry and academia.","sentences":["The cattle industry has been a major contributor to the economy of many countries, including the US and Canada.","The integration of Artificial Intelligence (AI) has revolutionized this sector, mirroring its transformative impact across all industries by enabling scalable and automated monitoring and intervention practices.","AI has also introduced tools and methods that automate many tasks previously performed by human labor with the help of computer vision, including health inspections.","Among these methods, pose estimation has a special place; pose estimation is the process of finding the position of joints in an image of animals.","Analyzing the pose of animal subjects enables precise identification and tracking of the animal's movement and the movements of its body parts.","By summarizing the video and imagery data into movement and joint location using pose estimation and then analyzing this information, we can address the scalability challenge in cattle management, focusing on health monitoring, behavioural phenotyping and welfare concerns.","Our study reviews recent advancements in pose estimation methodologies, their applicability in improving the cattle industry, existing challenges, and gaps in this field.","Furthermore, we propose an initiative to enhance open science frameworks within this field of study by launching a platform designed to connect industry and academia."],"url":"http://arxiv.org/abs/2408.06336v1"}
{"created":"2024-08-12 17:52:11","title":"LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for Humor Classification","abstract":"This paper explores humor detection through a linguistic lens, prioritizing syntactic, semantic, and contextual features over computational methods in Natural Language Processing. We categorize features into syntactic, semantic, and contextual dimensions, including lexicons, structural statistics, Word2Vec, WordNet, and phonetic style. Our proposed model, Colbert, utilizes BERT embeddings and parallel hidden layers to capture sentence congruity. By combining syntactic, semantic, and contextual features, we train Colbert for humor detection. Feature engineering examines essential syntactic and semantic features alongside BERT embeddings. SHAP interpretations and decision trees identify influential features, revealing that a holistic approach improves humor detection accuracy on unseen data. Integrating linguistic cues from different dimensions enhances the model's ability to understand humor complexity beyond traditional computational methods.","sentences":["This paper explores humor detection through a linguistic lens, prioritizing syntactic, semantic, and contextual features over computational methods in Natural Language Processing.","We categorize features into syntactic, semantic, and contextual dimensions, including lexicons, structural statistics, Word2Vec, WordNet, and phonetic style.","Our proposed model, Colbert, utilizes BERT embeddings and parallel hidden layers to capture sentence congruity.","By combining syntactic, semantic, and contextual features, we train Colbert for humor detection.","Feature engineering examines essential syntactic and semantic features alongside BERT embeddings.","SHAP interpretations and decision trees identify influential features, revealing that a holistic approach improves humor detection accuracy on unseen data.","Integrating linguistic cues from different dimensions enhances the model's ability to understand humor complexity beyond traditional computational methods."],"url":"http://arxiv.org/abs/2408.06335v1"}
{"created":"2024-08-12 17:48:55","title":"Animate, or Inanimate, That is the Question for Large Language Models","abstract":"The cognitive essence of humans is deeply intertwined with the concept of animacy, which plays an essential role in shaping their memory, vision, and multi-layered language understanding. Although animacy appears in language via nuanced constraints on verbs and adjectives, it is also learned and refined through extralinguistic information. Similarly, we assume that the LLMs' limited abilities to understand natural language when processing animacy are motivated by the fact that these models are trained exclusively on text.   Hence, the question this paper aims to answer arises: can LLMs, in their digital wisdom, process animacy in a similar way to what humans would do? We then propose a systematic analysis via prompting approaches. In particular, we probe different LLMs by prompting them using animate, inanimate, usual, and stranger contexts. Results reveal that, although LLMs have been trained predominantly on textual data, they exhibit human-like behavior when faced with typical animate and inanimate entities in alignment with earlier studies. Hence, LLMs can adapt to understand unconventional situations by recognizing oddities as animated without needing to interface with unspoken cognitive triggers humans rely on to break down animations.","sentences":["The cognitive essence of humans is deeply intertwined with the concept of animacy, which plays an essential role in shaping their memory, vision, and multi-layered language understanding.","Although animacy appears in language via nuanced constraints on verbs and adjectives, it is also learned and refined through extralinguistic information.","Similarly, we assume that the LLMs' limited abilities to understand natural language when processing animacy are motivated by the fact that these models are trained exclusively on text.   ","Hence, the question this paper aims to answer arises: can LLMs, in their digital wisdom, process animacy in a similar way to what humans would do?","We then propose a systematic analysis via prompting approaches.","In particular, we probe different LLMs by prompting them using animate, inanimate, usual, and stranger contexts.","Results reveal that, although LLMs have been trained predominantly on textual data, they exhibit human-like behavior when faced with typical animate and inanimate entities in alignment with earlier studies.","Hence, LLMs can adapt to understand unconventional situations by recognizing oddities as animated without needing to interface with unspoken cognitive triggers humans rely on to break down animations."],"url":"http://arxiv.org/abs/2408.06332v1"}
{"created":"2024-08-12 17:47:32","title":"Integration of blockchain in smart systems: problems and opportunities for real-time sensor data storage","abstract":"The internet of things (IoT) and other emerging ubiquitous technologies are supporting the rapid spread of smart systems, which has underlined the need for safe, open, and decentralized data storage solutions. With its inherent decentralization and immutability, blockchain offers itself as a potential solution for these requirements. However, the practicality of incorporating blockchain into real-time sensor data storage systems is a topic that demands in-depth examination. While blockchain promises unmatched data security and auditability, some intrinsic qualities, namely scalability restrictions, transactional delays, and escalating storage demands, impede its seamless deployment in high-frequency, voluminous data contexts typical of real-time sensors. This essay launches a methodical investigation into these difficulties, illuminating their underlying causes, potential effects, and potential countermeasures. In addition, we present a novel pragmatic experimental setup and analysis of blockchain for smart system applications, with an extended discussion of the benefits and disadvantages of deploying blockchain based solutions for smart system ecosystems.","sentences":["The internet of things (IoT) and other emerging ubiquitous technologies are supporting the rapid spread of smart systems, which has underlined the need for safe, open, and decentralized data storage solutions.","With its inherent decentralization and immutability, blockchain offers itself as a potential solution for these requirements.","However, the practicality of incorporating blockchain into real-time sensor data storage systems is a topic that demands in-depth examination.","While blockchain promises unmatched data security and auditability, some intrinsic qualities, namely scalability restrictions, transactional delays, and escalating storage demands, impede its seamless deployment in high-frequency, voluminous data contexts typical of real-time sensors.","This essay launches a methodical investigation into these difficulties, illuminating their underlying causes, potential effects, and potential countermeasures.","In addition, we present a novel pragmatic experimental setup and analysis of blockchain for smart system applications, with an extended discussion of the benefits and disadvantages of deploying blockchain based solutions for smart system ecosystems."],"url":"http://arxiv.org/abs/2408.06331v1"}
{"created":"2024-08-12 17:44:17","title":"VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents","abstract":"Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train \\& test data, and part of fine-tuned open LMMs are available at \\url{https://github.com/THUDM/VisualAgentBench}.","sentences":["Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents.","These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence.","However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments.","To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities.","Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models.","Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning.","Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents.","Code, train \\& test data, and part of fine-tuned open LMMs are available at \\url{https://github.com/THUDM/VisualAgentBench}."],"url":"http://arxiv.org/abs/2408.06327v1"}
{"created":"2024-08-12 17:43:48","title":"Online Vehicle Routing with Pickups and Deliveries under Time-Dependent Travel-Time Constraints","abstract":"The Vehicle Routing Problem with pickups, deliveries and spatiotemporal service constraints ($VRPPDSTC$) is a quite challenging algorithmic problem that can be dealt with in either an offline or an online fashion. In this work, we focus on a generalization, called $VRPPDSTCtd$, in which the travel-time metric is \\emph{time-dependent}: the traversal-time per road segment (represented as a directed arc) is determined by some function of the departure-time from its tail towards its head. Time-dependence makes things much more complicated, even for the simpler problem of computing earliest-arrival-time paths which is a crucial subroutine to be solved (numerous times) by $VRPPDSTCtd$ schedulers.   We propose two \\emph{online} schedulers of requests to workers, one which is a time-dependent variant of the classical Plain-Insertion heuristic, and an extension of it trying to digest some sort of forecasts for future demands for service. We enrich these two online schedulers with two additional heuristics, one targeting for distance-balanced assignments of work loads to the workers and another that makes local-search-improvements to the produced solutions.   We conduct a careful experimental evaluation of the proposed algorithms on a real-world instance, with or without these heuristics, and compare their quality with human-curated assignments provided by professional experts (human operators at actual pickup-and-delivery control centers), and also with feasible solutions constructed from a relaxed MILP formulation of $VRPPDSTCtd$, which is also introduced in this paper.   Our findings are quite encouraging, demonstrating that the proposed algorithms produce solutions which (i) are significant improvements over the human-curated assignments, and (ii) have overall quality pretty close to that of the (extremely time-consuming) solutions provided by an exact solver for the MILP formulation.","sentences":["The Vehicle Routing Problem with pickups, deliveries and spatiotemporal service constraints ($VRPPDSTC$) is a quite challenging algorithmic problem that can be dealt with in either an offline or an online fashion.","In this work, we focus on a generalization, called $VRPPDSTCtd$, in which the travel-time metric is \\emph{time-dependent}: the traversal-time per road segment (represented as a directed arc) is determined by some function of the departure-time from its tail towards its head.","Time-dependence makes things much more complicated, even for the simpler problem of computing earliest-arrival-time paths which is a crucial subroutine to be solved (numerous times) by $VRPPDSTCtd$ schedulers.   ","We propose two \\emph{online} schedulers of requests to workers, one which is a time-dependent variant of the classical Plain-Insertion heuristic, and an extension of it trying to digest some sort of forecasts for future demands for service.","We enrich these two online schedulers with two additional heuristics, one targeting for distance-balanced assignments of work loads to the workers and another that makes local-search-improvements to the produced solutions.   ","We conduct a careful experimental evaluation of the proposed algorithms on a real-world instance, with or without these heuristics, and compare their quality with human-curated assignments provided by professional experts (human operators at actual pickup-and-delivery control centers), and also with feasible solutions constructed from a relaxed MILP formulation of $VRPPDSTCtd$, which is also introduced in this paper.   ","Our findings are quite encouraging, demonstrating that the proposed algorithms produce solutions which (i) are significant improvements over the human-curated assignments, and (ii) have overall quality pretty close to that of the (extremely time-consuming) solutions provided by an exact solver for the MILP formulation."],"url":"http://arxiv.org/abs/2408.06324v2"}
{"created":"2024-08-12 17:42:46","title":"EqNIO: Subequivariant Neural Inertial Odometry","abstract":"Presently, neural networks are widely employed to accurately estimate 2D displacements and associated uncertainties from Inertial Measurement Unit (IMU) data that can be integrated into stochastic filter networks like the Extended Kalman Filter (EKF) as measurements and uncertainties for the update step in the filter. However, such neural approaches overlook symmetry which is a crucial inductive bias for model generalization. This oversight is notable because (i) physical laws adhere to symmetry principles when considering the gravity axis, meaning there exists the same transformation for both the physical entity and the resulting trajectory, and (ii) displacements should remain equivariant to frame transformations when the inertial frame changes. To address this, we propose a subequivariant framework by: (i) deriving fundamental layers such as linear and nonlinear layers for a subequivariant network, designed to handle sequences of vectors and scalars, (ii) employing the subequivariant network to predict an equivariant frame for the sequence of inertial measurements. This predicted frame can then be utilized for extracting invariant features through projection, which are integrated with arbitrary network architectures, (iii) transforming the invariant output by frame transformation to obtain equivariant displacements and covariances. We demonstrate the effectiveness and generalization of our Equivariant Framework on a filter-based approach with TLIO architecture for TLIO and Aria datasets, and an end-to-end deep learning approach with RONIN architecture for RONIN, RIDI and OxIOD datasets.","sentences":["Presently, neural networks are widely employed to accurately estimate 2D displacements and associated uncertainties from Inertial Measurement Unit (IMU) data that can be integrated into stochastic filter networks like the Extended Kalman Filter (EKF) as measurements and uncertainties for the update step in the filter.","However, such neural approaches overlook symmetry which is a crucial inductive bias for model generalization.","This oversight is notable because (i) physical laws adhere to symmetry principles when considering the gravity axis, meaning there exists the same transformation for both the physical entity and the resulting trajectory, and (ii) displacements should remain equivariant to frame transformations when the inertial frame changes.","To address this, we propose a subequivariant framework by: (i) deriving fundamental layers such as linear and nonlinear layers for a subequivariant network, designed to handle sequences of vectors and scalars, (ii) employing the subequivariant network to predict an equivariant frame for the sequence of inertial measurements.","This predicted frame can then be utilized for extracting invariant features through projection, which are integrated with arbitrary network architectures, (iii) transforming the invariant output by frame transformation to obtain equivariant displacements and covariances.","We demonstrate the effectiveness and generalization of our Equivariant Framework on a filter-based approach with TLIO architecture for TLIO and Aria datasets, and an end-to-end deep learning approach with RONIN architecture for RONIN, RIDI and OxIOD datasets."],"url":"http://arxiv.org/abs/2408.06321v1"}
{"created":"2024-08-12 17:22:15","title":"Dynamic Traffic Assignment for Public Transport with Vehicle Capacities","abstract":"Traffic assignment is a core component of many urban transport planning tools. It is used to determine how traffic is distributed over a transportation network. We study the task of computing traffic assignments for public transport: Given a public transit network, a timetable, vehicle capacities and a demand (i.e. a list of passengers, each with an associated origin, destination, and departure time), the goal is to predict the resulting passenger flow and the corresponding load of each vehicle. Microscopic stochastic simulation of individual passengers is a standard, but computationally expensive approach. Briem et al. (2017) have shown that a clever adaptation of the Connection Scan Algorithm (CSA) can lead to highly efficient traffic assignment algorithms, but ignores vehicle capacities, resulting in overcrowded vehicles. Taking their work as a starting point, we here propose a new and extended model that guarantees capacity-feasible assignments and incorporates dynamic network congestion effects such as crowded vehicles, denied boarding, and dwell time delays. Moreover, we also incorporate learning and adaptation of individual passengers based on their experience with the network. Applications include studying the evolution of perceived travel times as a result of adaptation, the impact of an increase in capacity, or network effects due to changes in the timetable such as the addition or the removal of a service or a whole line. The proposed framework has been experimentally evaluated with public transport networks of G\\\"ottingen and Stuttgart (Germany). The simulation proves to be highly efficient. On a standard PC the computation of a traffic assignment takes just a few seconds per simulation day.","sentences":["Traffic assignment is a core component of many urban transport planning tools.","It is used to determine how traffic is distributed over a transportation network.","We study the task of computing traffic assignments for public transport: Given a public transit network, a timetable, vehicle capacities and a demand (i.e. a list of passengers, each with an associated origin, destination, and departure time), the goal is to predict the resulting passenger flow and the corresponding load of each vehicle.","Microscopic stochastic simulation of individual passengers is a standard, but computationally expensive approach.","Briem et al.","(2017) have shown that a clever adaptation of the Connection Scan Algorithm (CSA) can lead to highly efficient traffic assignment algorithms, but ignores vehicle capacities, resulting in overcrowded vehicles.","Taking their work as a starting point, we here propose a new and extended model that guarantees capacity-feasible assignments and incorporates dynamic network congestion effects such as crowded vehicles, denied boarding, and dwell time delays.","Moreover, we also incorporate learning and adaptation of individual passengers based on their experience with the network.","Applications include studying the evolution of perceived travel times as a result of adaptation, the impact of an increase in capacity, or network effects due to changes in the timetable such as the addition or the removal of a service or a whole line.","The proposed framework has been experimentally evaluated with public transport networks of G\\\"ottingen and Stuttgart (Germany).","The simulation proves to be highly efficient.","On a standard PC the computation of a traffic assignment takes just a few seconds per simulation day."],"url":"http://arxiv.org/abs/2408.06308v1"}
