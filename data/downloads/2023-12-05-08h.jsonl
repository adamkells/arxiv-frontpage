{"created":"2023-12-04 18:59:59","title":"PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness","abstract":"We propose the task of Panoptic Scene Completion (PSC) which extends the recently popular Semantic Scene Completion (SSC) task with instance-level information to produce a richer understanding of the 3D scene. Our PSC proposal utilizes a hybrid mask-based technique on the non-empty voxels from sparse multi-scale completions. Whereas the SSC literature overlooks uncertainty which is critical for robotics applications, we instead propose an efficient ensembling to estimate both voxel-wise and instance-wise uncertainties along PSC. This is achieved by building on a multi-input multi-output (MIMO) strategy, while improving performance and yielding better uncertainty for little additional compute. Additionally, we introduce a technique to aggregate permutation-invariant mask predictions. Our experiments demonstrate that our method surpasses all baselines in both Panoptic Scene Completion and uncertainty estimation on three large-scale autonomous driving datasets. Our code and data are available at https://astra-vision.github.io/PaSCo .","sentences":["We propose the task of Panoptic Scene Completion (PSC) which extends the recently popular Semantic Scene Completion (SSC) task with instance-level information to produce a richer understanding of the 3D scene.","Our PSC proposal utilizes a hybrid mask-based technique on the non-empty voxels from sparse multi-scale completions.","Whereas the SSC literature overlooks uncertainty which is critical for robotics applications, we instead propose an efficient ensembling to estimate both voxel-wise and instance-wise uncertainties along PSC.","This is achieved by building on a multi-input multi-output (MIMO) strategy, while improving performance and yielding better uncertainty for little additional compute.","Additionally, we introduce a technique to aggregate permutation-invariant mask predictions.","Our experiments demonstrate that our method surpasses all baselines in both Panoptic Scene Completion and uncertainty estimation on three large-scale autonomous driving datasets.","Our code and data are available at https://astra-vision.github.io/PaSCo ."],"url":"http://arxiv.org/abs/2312.02158v1"}
{"created":"2023-12-04 18:59:55","title":"GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis","abstract":"We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.","sentences":["We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner.","The proposed method enables 2K-resolution rendering under a sparse-view camera setting.","Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization.","To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space.","The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed."],"url":"http://arxiv.org/abs/2312.02155v1"}
{"created":"2023-12-04 18:59:50","title":"Aligning and Prompting Everything All at Once for Universal Visual Perception","abstract":"Vision foundation models have been explored recently to build general-purpose vision systems. However, predominant paradigms, driven by casting instance-level tasks as an object-word alignment, bring heavy cross-modality interaction, which is not effective in prompting object detection and visual grounding. Another line of work that focuses on pixel-level tasks often encounters a large annotation gap of things and stuff, and suffers from mutual interference between foreground-object and background-class segmentation. In stark contrast to the prevailing methods, we present APE, a universal visual perception model for aligning and prompting everything all at once in an image to perform diverse tasks, i.e., detection, segmentation, and grounding, as an instance-level sentence-object matching paradigm. Specifically, APE advances the convergence of detection and grounding by reformulating language-guided grounding as open-vocabulary detection, which efficiently scales up model prompting to thousands of category vocabularies and region descriptions while maintaining the effectiveness of cross-modality fusion. To bridge the granularity gap of different pixel-level tasks, APE equalizes semantic and panoptic segmentation to proxy instance learning by considering any isolated regions as individual instances. APE aligns vision and language representation on broad data with natural and challenging characteristics all at once without task-specific fine-tuning. The extensive experiments on over 160 datasets demonstrate that, with only one-suit of weights, APE outperforms (or is on par with) the state-of-the-art models, proving that an effective yet universal perception for anything aligning and prompting is indeed feasible. Codes and trained models are released at https://github.com/shenyunhang/APE.","sentences":["Vision foundation models have been explored recently to build general-purpose vision systems.","However, predominant paradigms, driven by casting instance-level tasks as an object-word alignment, bring heavy cross-modality interaction, which is not effective in prompting object detection and visual grounding.","Another line of work that focuses on pixel-level tasks often encounters a large annotation gap of things and stuff, and suffers from mutual interference between foreground-object and background-class segmentation.","In stark contrast to the prevailing methods, we present APE, a universal visual perception model for aligning and prompting everything all at once in an image to perform diverse tasks, i.e., detection, segmentation, and grounding, as an instance-level sentence-object matching paradigm.","Specifically, APE advances the convergence of detection and grounding by reformulating language-guided grounding as open-vocabulary detection, which efficiently scales up model prompting to thousands of category vocabularies and region descriptions while maintaining the effectiveness of cross-modality fusion.","To bridge the granularity gap of different pixel-level tasks, APE equalizes semantic and panoptic segmentation to proxy instance learning by considering any isolated regions as individual instances.","APE aligns vision and language representation on broad data with natural and challenging characteristics all at once without task-specific fine-tuning.","The extensive experiments on over 160 datasets demonstrate that, with only one-suit of weights, APE outperforms (or is on par with) the state-of-the-art models, proving that an effective yet universal perception for anything aligning and prompting is indeed feasible.","Codes and trained models are released at https://github.com/shenyunhang/APE."],"url":"http://arxiv.org/abs/2312.02153v1"}
{"created":"2023-12-04 18:59:44","title":"Steerers: A framework for rotation equivariant keypoint descriptors","abstract":"Image keypoint descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction. However, descriptions output by learned descriptors are typically not robust to camera rotation. While they can be made more robust by, e.g., data augmentation, this degrades performance on upright images. Another approach is test-time augmentation, which incurs a significant increase in runtime. We instead learn a linear transform in description space that encodes rotations of the input image. We call this linear transform a steerer since it allows us to transform the descriptions as if the image was rotated. From representation theory we know all possible steerers for the rotation group. Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize a descriptor given a fixed steerer. We perform experiments in all of these three settings and obtain state-of-the-art results on the rotation invariant image matching benchmarks AIMS and Roto-360. We publish code and model weights at github.com/georg-bn/rotation-steerers.","sentences":["Image keypoint descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction.","However, descriptions output by learned descriptors are typically not robust to camera rotation.","While they can be made more robust by, e.g., data augmentation, this degrades performance on upright images.","Another approach is test-time augmentation, which incurs a significant increase in runtime.","We instead learn a linear transform in description space that encodes rotations of the input image.","We call this linear transform a steerer since it allows us to transform the descriptions as if the image was rotated.","From representation theory we know all possible steerers for the rotation group.","Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize a descriptor given a fixed steerer.","We perform experiments in all of these three settings and obtain state-of-the-art results on the rotation invariant image matching benchmarks AIMS and Roto-360.","We publish code and model weights at github.com/georg-bn/rotation-steerers."],"url":"http://arxiv.org/abs/2312.02152v1"}
{"created":"2023-12-04 18:59:36","title":"Guarding Barlow Twins Against Overfitting with Mixed Samples","abstract":"Self-supervised Learning (SSL) aims to learn transferable feature representations for downstream applications without relying on labeled data. The Barlow Twins algorithm, renowned for its widespread adoption and straightforward implementation compared to its counterparts like contrastive learning methods, minimizes feature redundancy while maximizing invariance to common corruptions. Optimizing for the above objective forces the network to learn useful representations, while avoiding noisy or constant features, resulting in improved downstream task performance with limited adaptation. Despite Barlow Twins' proven effectiveness in pre-training, the underlying SSL objective can inadvertently cause feature overfitting due to the lack of strong interaction between the samples unlike the contrastive learning approaches. From our experiments, we observe that optimizing for the Barlow Twins objective doesn't necessarily guarantee sustained improvements in representation quality beyond a certain pre-training phase, and can potentially degrade downstream performance on some datasets. To address this challenge, we introduce Mixed Barlow Twins, which aims to improve sample interaction during Barlow Twins training via linearly interpolated samples. This results in an additional regularization term to the original Barlow Twins objective, assuming linear interpolation in the input space translates to linearly interpolated features in the feature space. Pre-training with this regularization effectively mitigates feature overfitting and further enhances the downstream performance on CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets. The code and checkpoints are available at: https://github.com/wgcban/mix-bt.git","sentences":["Self-supervised Learning (SSL) aims to learn transferable feature representations for downstream applications without relying on labeled data.","The Barlow Twins algorithm, renowned for its widespread adoption and straightforward implementation compared to its counterparts like contrastive learning methods, minimizes feature redundancy while maximizing invariance to common corruptions.","Optimizing for the above objective forces the network to learn useful representations, while avoiding noisy or constant features, resulting in improved downstream task performance with limited adaptation.","Despite Barlow Twins' proven effectiveness in pre-training, the underlying SSL objective can inadvertently cause feature overfitting due to the lack of strong interaction between the samples unlike the contrastive learning approaches.","From our experiments, we observe that optimizing for the Barlow Twins objective doesn't necessarily guarantee sustained improvements in representation quality beyond a certain pre-training phase, and can potentially degrade downstream performance on some datasets.","To address this challenge, we introduce Mixed Barlow Twins, which aims to improve sample interaction during Barlow Twins training via linearly interpolated samples.","This results in an additional regularization term to the original Barlow Twins objective, assuming linear interpolation in the input space translates to linearly interpolated features in the feature space.","Pre-training with this regularization effectively mitigates feature overfitting and further enhances the downstream performance on CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets.","The code and checkpoints are available at: https://github.com/wgcban/mix-bt.git"],"url":"http://arxiv.org/abs/2312.02151v1"}
{"created":"2023-12-04 18:59:19","title":"Learning Polynomial Problems with $SL(2,\\mathbb{R})$ Equivariance","abstract":"Optimizing and certifying the positivity of polynomials are fundamental primitives across mathematics and engineering applications, from dynamical systems to operations research. However, solving these problems in practice requires large semidefinite programs, with poor scaling in dimension and degree. In this work, we demonstrate for the first time that neural networks can effectively solve such problems in a data-driven fashion, achieving tenfold speedups while retaining high accuracy. Moreover, we observe that these polynomial learning problems are equivariant to the non-compact group $SL(2,\\mathbb{R})$, which consists of area-preserving linear transformations. We therefore adapt our learning pipelines to accommodate this structure, including data augmentation, a new $SL(2,\\mathbb{R})$-equivariant architecture, and an architecture equivariant with respect to its maximal compact subgroup, $SO(2, \\mathbb{R})$. Surprisingly, the most successful approaches in practice do not enforce equivariance to the entire group, which we prove arises from an unusual lack of architecture universality for $SL(2,\\mathbb{R})$ in particular. A consequence of this result, which is of independent interest, is that there exists an equivariant function for which there is no sequence of equivariant polynomials multiplied by arbitrary invariants that approximates the original function. This is a rare example of a symmetric problem where data augmentation outperforms a fully equivariant architecture, and provides interesting lessons in both theory and practice for other problems with non-compact symmetries.","sentences":["Optimizing and certifying the positivity of polynomials are fundamental primitives across mathematics and engineering applications, from dynamical systems to operations research.","However, solving these problems in practice requires large semidefinite programs, with poor scaling in dimension and degree.","In this work, we demonstrate for the first time that neural networks can effectively solve such problems in a data-driven fashion, achieving tenfold speedups while retaining high accuracy.","Moreover, we observe that these polynomial learning problems are equivariant to the non-compact group $SL(2,\\mathbb{R})$, which consists of area-preserving linear transformations.","We therefore adapt our learning pipelines to accommodate this structure, including data augmentation, a new $SL(2,\\mathbb{R})$-equivariant architecture, and an architecture equivariant with respect to its maximal compact subgroup, $SO(2, \\mathbb{R})$. Surprisingly, the most successful approaches in practice do not enforce equivariance to the entire group, which we prove arises from an unusual lack of architecture universality for $SL(2,\\mathbb{R})$ in particular.","A consequence of this result, which is of independent interest, is that there exists an equivariant function for which there is no sequence of equivariant polynomials multiplied by arbitrary invariants that approximates the original function.","This is a rare example of a symmetric problem where data augmentation outperforms a fully equivariant architecture, and provides interesting lessons in both theory and practice for other problems with non-compact symmetries."],"url":"http://arxiv.org/abs/2312.02146v1"}
{"created":"2023-12-04 18:59:13","title":"Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation","abstract":"Monocular depth estimation is a fundamental computer vision task. Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough. The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures. Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains. This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation. We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge. The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data. It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases. Project page: https://marigoldmonodepth.github.io.","sentences":["Monocular depth estimation is a fundamental computer vision task.","Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough.","The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures.","Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains.","This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation.","We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge.","The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data.","It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases.","Project page: https://marigoldmonodepth.github.io."],"url":"http://arxiv.org/abs/2312.02145v1"}
{"created":"2023-12-04 18:58:57","title":"Competition-Level Problems Are Effective Evaluators of LLMs","abstract":"Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future.","sentences":["Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently.","This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills.","We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered.","Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems.","We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges.","Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future."],"url":"http://arxiv.org/abs/2312.02143v1"}
{"created":"2023-12-04 18:58:20","title":"iMatching: Imperative Correspondence Learning","abstract":"Learning feature correspondence is a foundational task in computer vision, holding immense importance for downstream applications such as visual odometry and 3D reconstruction. Despite recent progress in data-driven models, feature correspondence learning is still limited by the lack of accurate per-pixel correspondence labels. To overcome this difficulty, we introduce a new self-supervised scheme, imperative learning (IL), for training feature correspondence. It enables correspondence learning on arbitrary uninterrupted videos without any camera pose or depth labels, heralding a new era for self-supervised correspondence learning. Specifically, we formulated the problem of correspondence learning as a bilevel optimization, which takes the reprojection error from bundle adjustment as a supervisory signal for the model. To avoid large memory and computation overhead, we leverage the stationary point to effectively back-propagate the implicit gradients through bundle adjustment. Through extensive experiments, we demonstrate superior performance on tasks including feature matching and pose estimation, in which we obtained an average of 30% accuracy gain over the state-of-the-art matching models.","sentences":["Learning feature correspondence is a foundational task in computer vision, holding immense importance for downstream applications such as visual odometry and 3D reconstruction.","Despite recent progress in data-driven models, feature correspondence learning is still limited by the lack of accurate per-pixel correspondence labels.","To overcome this difficulty, we introduce a new self-supervised scheme, imperative learning (IL), for training feature correspondence.","It enables correspondence learning on arbitrary uninterrupted videos without any camera pose or depth labels, heralding a new era for self-supervised correspondence learning.","Specifically, we formulated the problem of correspondence learning as a bilevel optimization, which takes the reprojection error from bundle adjustment as a supervisory signal for the model.","To avoid large memory and computation overhead, we leverage the stationary point to effectively back-propagate the implicit gradients through bundle adjustment.","Through extensive experiments, we demonstrate superior performance on tasks including feature matching and pose estimation, in which we obtained an average of 30% accuracy gain over the state-of-the-art matching models."],"url":"http://arxiv.org/abs/2312.02141v1"}
{"created":"2023-12-04 18:54:34","title":"Hot PATE: Private Aggregation of Distributions for Diverse Task","abstract":"The Private Aggregation of Teacher Ensembles (PATE) framework~\\cite{PapernotAEGT:ICLR2017} is a versatile approach to privacy-preserving machine learning. In PATE, teacher models are trained on distinct portions of sensitive data, and their predictions are privately aggregated to label new training examples for a student model.   Until now, PATE has primarily been explored with classification-like tasks, where each example possesses a ground-truth label, and knowledge is transferred to the student by labeling public examples. Generative AI models, however, excel in open ended \\emph{diverse} tasks with multiple valid responses and scenarios that may not align with traditional labeled examples. Furthermore, the knowledge of models is often encapsulated in the response distribution itself and may be transferred from teachers to student in a more fluid way. We propose \\emph{hot PATE}, tailored for the diverse setting. In hot PATE, each teacher model produces a response distribution and the aggregation method must preserve both privacy and diversity of responses. We demonstrate, analytically and empirically, that hot PATE achieves privacy-utility tradeoffs that are comparable to, and in diverse settings, significantly surpass, the baseline ``cold'' PATE.","sentences":["The Private Aggregation of Teacher Ensembles (PATE) framework~\\cite{PapernotAEGT:","ICLR2017} is a versatile approach to privacy-preserving machine learning.","In PATE, teacher models are trained on distinct portions of sensitive data, and their predictions are privately aggregated to label new training examples for a student model.   ","Until now, PATE has primarily been explored with classification-like tasks, where each example possesses a ground-truth label, and knowledge is transferred to the student by labeling public examples.","Generative AI models, however, excel in open ended \\emph{diverse} tasks with multiple valid responses and scenarios that may not align with traditional labeled examples.","Furthermore, the knowledge of models is often encapsulated in the response distribution itself and may be transferred from teachers to student in a more fluid way.","We propose \\emph{hot PATE}, tailored for the diverse setting.","In hot PATE, each teacher model produces a response distribution and the aggregation method must preserve both privacy and diversity of responses.","We demonstrate, analytically and empirically, that hot PATE achieves privacy-utility tradeoffs that are comparable to, and in diverse settings, significantly surpass, the baseline ``cold'' PATE."],"url":"http://arxiv.org/abs/2312.02132v1"}
{"created":"2023-12-04 18:52:26","title":"TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques","abstract":"Recent advances in language models (LMs), have demonstrated significant efficacy in tasks related to the arts and humanities. While LMs have exhibited exceptional performance across a wide range of natural language processing tasks, there are notable challenges associated with their utilization on small datasets and their ability to replicate more creative human capacities. In this study, we aim to address these challenges by training a Persian classical poetry generation model using a transformer architecture on a specialized dataset with no pretraining. Additionally, we propose a novel decoding method to enhance coherence and meaningfulness in the generated poetry, effectively managing the tradeoff between diversity and quality. Furthermore, the results of our training approach and the proposed decoding method are evaluated through comprehensive set of automatic and human evaluations and showed its superior capability to generate coherent and meaningful poetry in compare to other decoding methods and an existing Persian large language model (LLM).","sentences":["Recent advances in language models (LMs), have demonstrated significant efficacy in tasks related to the arts and humanities.","While LMs have exhibited exceptional performance across a wide range of natural language processing tasks, there are notable challenges associated with their utilization on small datasets and their ability to replicate more creative human capacities.","In this study, we aim to address these challenges by training a Persian classical poetry generation model using a transformer architecture on a specialized dataset with no pretraining.","Additionally, we propose a novel decoding method to enhance coherence and meaningfulness in the generated poetry, effectively managing the tradeoff between diversity and quality.","Furthermore, the results of our training approach and the proposed decoding method are evaluated through comprehensive set of automatic and human evaluations and showed its superior capability to generate coherent and meaningful poetry in compare to other decoding methods and an existing Persian large language model (LLM)."],"url":"http://arxiv.org/abs/2312.02125v1"}
{"created":"2023-12-04 18:51:44","title":"VerA: Versatile Anonymization Fit for Clinical Facial Images","abstract":"The escalating legislative demand for data privacy in facial image dissemination has underscored the significance of image anonymization. Recent advancements in the field surpass traditional pixelation or blur methods, yet they predominantly address regular single images. This leaves clinical image anonymization -- a necessity for illustrating medical interventions -- largely unaddressed. We present VerA, a versatile facial image anonymization that is fit for clinical facial images where: (1) certain semantic areas must be preserved to show medical intervention results, and (2) anonymizing image pairs is crucial for showing before-and-after results. VerA outperforms or is on par with state-of-the-art methods in de-identification and photorealism for regular images. In addition, we validate our results on paired anonymization, and on the anonymization of both single and paired clinical images with extensive quantitative and qualitative evaluation.","sentences":["The escalating legislative demand for data privacy in facial image dissemination has underscored the significance of image anonymization.","Recent advancements in the field surpass traditional pixelation or blur methods, yet they predominantly address regular single images.","This leaves clinical image anonymization -- a necessity for illustrating medical interventions -- largely unaddressed.","We present VerA, a versatile facial image anonymization that is fit for clinical facial images where: (1) certain semantic areas must be preserved to show medical intervention results, and (2) anonymizing image pairs is crucial for showing before-and-after results.","VerA outperforms or is on par with state-of-the-art methods in de-identification and photorealism for regular images.","In addition, we validate our results on paired anonymization, and on the anonymization of both single and paired clinical images with extensive quantitative and qualitative evaluation."],"url":"http://arxiv.org/abs/2312.02124v1"}
{"created":"2023-12-04 18:50:35","title":"Magicoder: Source Code Is All You Need","abstract":"We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references.","sentences":["We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters.","Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code.","Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data.","The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion.","Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).","Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references."],"url":"http://arxiv.org/abs/2312.02120v1"}
{"created":"2023-12-04 18:46:42","title":"A Framework for Self-Intersecting Surfaces (SOS): Symmetric Optimisation for Stability","abstract":"In this paper, we give a stable and efficient method for fixing self-intersections and non-manifold parts in a given embedded simplicial complex. In addition, we show how symmetric properties can be used for further optimisation. We prove an initialisation criterion for computation of the outer hull of an embedded simplicial complex. To regularise the outer hull of the retriangulated surface, we present a method to remedy non-manifold edges and points. We also give a modification of the outer hull algorithm to determine chambers of complexes which gives rise to many new insights. All of these methods have applications in many areas, for example in 3D-printing, artistic realisations of 3D models or fixing errors introduced by scanning equipment applied for tomography. Implementations of the proposed algorithms are given in the computer algebra system GAP4. For verification of our methods, we use a data-set of highly self-intersecting symmetric icosahedra.","sentences":["In this paper, we give a stable and efficient method for fixing self-intersections and non-manifold parts in a given embedded simplicial complex.","In addition, we show how symmetric properties can be used for further optimisation.","We prove an initialisation criterion for computation of the outer hull of an embedded simplicial complex.","To regularise the outer hull of the retriangulated surface, we present a method to remedy non-manifold edges and points.","We also give a modification of the outer hull algorithm to determine chambers of complexes which gives rise to many new insights.","All of these methods have applications in many areas, for example in 3D-printing, artistic realisations of 3D models or fixing errors introduced by scanning equipment applied for tomography.","Implementations of the proposed algorithms are given in the computer algebra system GAP4.","For verification of our methods, we use a data-set of highly self-intersecting symmetric icosahedra."],"url":"http://arxiv.org/abs/2312.02113v1"}
{"created":"2023-12-04 18:43:45","title":"TriDeNT: Triple Deep Network Training for Privileged Knowledge Distillation in Histopathology","abstract":"Computational pathology models rarely utilise data that will not be available for inference. This means most models cannot learn from highly informative data such as additional immunohistochemical (IHC) stains and spatial transcriptomics. We present TriDeNT, a novel self-supervised method for utilising privileged data that is not available during inference to improve performance. We demonstrate the efficacy of this method for a range of different paired data including immunohistochemistry, spatial transcriptomics and expert nuclei annotations. In all settings, TriDeNT outperforms other state-of-the-art methods in downstream tasks, with observed improvements of up to 101%. Furthermore, we provide qualitative and quantitative measurements of the features learned by these models and how they differ from baselines. TriDeNT offers a novel method to distil knowledge from scarce or costly data during training, to create significantly better models for routine inputs.","sentences":["Computational pathology models rarely utilise data that will not be available for inference.","This means most models cannot learn from highly informative data such as additional immunohistochemical (IHC) stains and spatial transcriptomics.","We present TriDeNT, a novel self-supervised method for utilising privileged data that is not available during inference to improve performance.","We demonstrate the efficacy of this method for a range of different paired data including immunohistochemistry, spatial transcriptomics and expert nuclei annotations.","In all settings, TriDeNT outperforms other state-of-the-art methods in downstream tasks, with observed improvements of up to 101%.","Furthermore, we provide qualitative and quantitative measurements of the features learned by these models and how they differ from baselines.","TriDeNT offers a novel method to distil knowledge from scarce or costly data during training, to create significantly better models for routine inputs."],"url":"http://arxiv.org/abs/2312.02111v1"}
{"created":"2023-12-04 18:26:31","title":"Mitigating Data Injection Attacks on Federated Learning","abstract":"Federated learning is a technique that allows multiple entities to collaboratively train models using their data without compromising data privacy. However, despite its advantages, federated learning can be susceptible to false data injection attacks. In these scenarios, a malicious entity with control over specific agents in the network can manipulate the learning process, leading to a suboptimal model. Consequently, addressing these data injection attacks presents a significant research challenge in federated learning systems. In this paper, we propose a novel technique to detect and mitigate data injection attacks on federated learning systems. Our mitigation method is a local scheme, performed during a single instance of training by the coordinating node, allowing the mitigation during the convergence of the algorithm. Whenever an agent is suspected to be an attacker, its data will be ignored for a certain period, this decision will often be re-evaluated. We prove that with probability 1, after a finite time, all attackers will be ignored while the probability of ignoring a trustful agent becomes 0, provided that there is a majority of truthful agents. Simulations show that when the coordinating node detects and isolates all the attackers, the model recovers and converges to the truthful model.","sentences":["Federated learning is a technique that allows multiple entities to collaboratively train models using their data without compromising data privacy.","However, despite its advantages, federated learning can be susceptible to false data injection attacks.","In these scenarios, a malicious entity with control over specific agents in the network can manipulate the learning process, leading to a suboptimal model.","Consequently, addressing these data injection attacks presents a significant research challenge in federated learning systems.","In this paper, we propose a novel technique to detect and mitigate data injection attacks on federated learning systems.","Our mitigation method is a local scheme, performed during a single instance of training by the coordinating node, allowing the mitigation during the convergence of the algorithm.","Whenever an agent is suspected to be an attacker, its data will be ignored for a certain period, this decision will often be re-evaluated.","We prove that with probability 1, after a finite time, all attackers will be ignored while the probability of ignoring a trustful agent becomes 0, provided that there is a majority of truthful agents.","Simulations show that when the coordinating node detects and isolates all the attackers, the model recovers and converges to the truthful model."],"url":"http://arxiv.org/abs/2312.02102v1"}
{"created":"2023-12-04 18:16:27","title":"Inapproximability of Maximum Diameter Clustering for Few Clusters","abstract":"We consider the k-diameter clustering problem, where the goal is to partition a set of points in a metric space into $k$ clusters, minimizing the maximum distance between any two points in the same cluster. In general metrics, k-diameter is known to be NP-hard, while it has a $2$-approximation algorithm (Gonzalez'85). Complementing this algorithm, it is known that k-diameter is NP-hard to approximate within a factor better than $2$ in the $\\ell_1$ and $\\ell_\\infty$ metrics, and within a factor of $1.969$ in the $\\ell_2$ metric (Feder-Greene'88).   When $k\\geq 3$ is fixed, k-diameter remains NP-hard to approximate within a factor better than $2$ in the $\\ell_\\infty$ metric (Megiddo'90). However, its approximability in this setting has not previously been studied in the $\\ell_1$ and $\\ell_2$ metrics, though a $1.415$-approximation algorithm in the $\\ell_2$ metric follows from a known result (Badoiu et al.'02). In this paper, we address the remaining gap by showing new hardness of approximation results that hold even when $k=3$. Specifically, we prove that 3-diameter is NP-hard to approximate within a factor better than $1.5$ in the $\\ell_1$ metric, and within a factor of $1.304$ in the $\\ell_2$ metric.","sentences":["We consider the k-diameter clustering problem, where the goal is to partition a set of points in a metric space into $k$ clusters, minimizing the maximum distance between any two points in the same cluster.","In general metrics, k-diameter is known to be NP-hard, while it has a $2$-approximation algorithm (Gonzalez'85).","Complementing this algorithm, it is known that k-diameter is NP-hard to approximate within a factor better than $2$ in the $\\ell_1$ and $\\ell_\\infty$ metrics, and within a factor of $1.969$ in the $\\ell_2$ metric (Feder-Greene'88).   ","When $k\\geq 3$ is fixed, k-diameter remains NP-hard to approximate within a factor better than $2$ in the $\\ell_\\infty$ metric (Megiddo'90).","However, its approximability in this setting has not previously been studied in the $\\ell_1$ and $\\ell_2$ metrics, though a $1.415$-approximation algorithm in the $\\ell_2$ metric follows from a known result (Badoiu et al.'02).","In this paper, we address the remaining gap by showing new hardness of approximation results that hold even when $k=3$. Specifically, we prove that 3-diameter is NP-hard to approximate within a factor better than $1.5$ in the $\\ell_1$ metric, and within a factor of $1.304$ in the $\\ell_2$ metric."],"url":"http://arxiv.org/abs/2312.02097v1"}
{"created":"2023-12-04 18:13:58","title":"Single-sample versus case-control sampling scheme for Positive Unlabeled data: the story of two scenarios","abstract":"In the paper we argue that performance of the classifiers based on Empirical Risk Minimization (ERM) for positive unlabeled data, which are designed for case-control sampling scheme may significantly deteriorate when applied to a single-sample scenario. We reveal why their behavior depends, in all but very specific cases, on the scenario. Also, we introduce a single-sample case analogue of the popular non-negative risk classifier designed for case-control data and compare its performance with the original proposal. We show that the significant differences occur between them, especiall when half or more positive of observations are labeled. The opposite case when ERM minimizer designed for the case-control case is applied for single-sample data is also considered and similar conclusions are drawn. Taking into account difference of scenarios requires a sole, but crucial, change in the definition of the Empirical Risk.","sentences":["In the paper we argue that performance of the classifiers based on Empirical Risk Minimization (ERM) for positive unlabeled data, which are designed for case-control sampling scheme may significantly deteriorate when applied to a single-sample scenario.","We reveal why their behavior depends, in all but very specific cases, on the scenario.","Also, we introduce a single-sample case analogue of the popular non-negative risk classifier designed for case-control data and compare its performance with the original proposal.","We show that the significant differences occur between them, especiall when half or more positive of observations are labeled.","The opposite case when ERM minimizer designed for the case-control case is applied for single-sample data is also considered and similar conclusions are drawn.","Taking into account difference of scenarios requires a sole, but crucial, change in the definition of the Empirical Risk."],"url":"http://arxiv.org/abs/2312.02095v1"}
{"created":"2023-12-04 18:10:20","title":"Cultural Differences in Students' Privacy Concerns in Learning Analytics across Germany, South Korea, Spain, Sweden, and the United States","abstract":"Applications of learning analytics (LA) can raise concerns from students about their privacy in higher education contexts. Developing effective privacy-enhancing practices requires a systematic understanding of students' privacy concerns and how they vary across national and cultural dimensions. We conducted a survey study with established instruments to measure privacy concerns and cultural values for university students in five countries (Germany, South Korea, Spain, Sweden, and the United States; N = 762). The results show that students generally trusted institutions with their data and disclosed information as they perceived the risks to be manageable even though they felt somewhat limited in their ability to control their privacy. Across the five countries, German and Swedish students stood out as the most trusting and least concerned, especially compared to US students who reported greater perceived risk and less control. Students in South Korea and Spain responded similarly on all five privacy dimensions (perceived privacy risk, perceived privacy control, privacy concerns, trusting beliefs, and non-self-disclosure behavior), despite their significant cultural differences. Culture measured at the individual level affected the antecedents and outcomes of privacy concerns more than country-level culture. Perceived privacy risk and privacy control increase with power distance. Trusting beliefs increase with a desire for uncertainty avoidance and lower masculinity. Non-self-disclosure behaviors rise with power distance and masculinity, and decrease with more uncertainty avoidance. Thus, cultural values related to trust in institutions, social equality and risk-taking should be considered when developing privacy-enhancing practices and policies in higher education.","sentences":["Applications of learning analytics (LA) can raise concerns from students about their privacy in higher education contexts.","Developing effective privacy-enhancing practices requires a systematic understanding of students' privacy concerns and how they vary across national and cultural dimensions.","We conducted a survey study with established instruments to measure privacy concerns and cultural values for university students in five countries (Germany, South Korea, Spain, Sweden, and the United States; N = 762).","The results show that students generally trusted institutions with their data and disclosed information as they perceived the risks to be manageable even though they felt somewhat limited in their ability to control their privacy.","Across the five countries, German and Swedish students stood out as the most trusting and least concerned, especially compared to US students who reported greater perceived risk and less control.","Students in South Korea and Spain responded similarly on all five privacy dimensions (perceived privacy risk, perceived privacy control, privacy concerns, trusting beliefs, and non-self-disclosure behavior), despite their significant cultural differences.","Culture measured at the individual level affected the antecedents and outcomes of privacy concerns more than country-level culture.","Perceived privacy risk and privacy control increase with power distance.","Trusting beliefs increase with a desire for uncertainty avoidance and lower masculinity.","Non-self-disclosure behaviors rise with power distance and masculinity, and decrease with more uncertainty avoidance.","Thus, cultural values related to trust in institutions, social equality and risk-taking should be considered when developing privacy-enhancing practices and policies in higher education."],"url":"http://arxiv.org/abs/2312.02093v1"}
{"created":"2023-12-04 18:06:41","title":"Physics simulation capabilities of LLMs","abstract":"[Abridged abstract] Large Language Models (LLMs) can solve some undergraduate-level to graduate-level physics textbook problems and are proficient at coding. Combining these two capabilities could one day enable AI systems to simulate and predict the physical world.   We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to research-level computational physics problems. We condition LLM generation on the use of well-documented and widely-used packages to elicit coding capabilities in the physics and astrophysics domains. We contribute $\\sim 50$ original and challenging problems in celestial mechanics (with REBOUND), stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear dynamics (with SciPy). Since our problems do not admit unique solutions, we evaluate LLM performance on several soft metrics: counts of lines that contain different types of errors (coding, physics, necessity and sufficiency) as well as a more \"educational\" Pass-Fail metric focused on capturing the salient physical ingredients of the problem at hand.   As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems, although about 40\\% of the solutions could plausibly get a passing grade. About $70-90 \\%$ of the code lines produced are necessary, sufficient and correct (coding \\& physics). Physics and coding errors are the most common, with some unnecessary or insufficient lines. We observe significant variations across problem class and difficulty. We identify several failure modes of GPT4 in the computational physics domain.   Our reconnaissance work provides a snapshot of current computational capabilities in classical physics and points to obvious improvement targets if AI systems are ever to reach a basic level of autonomy in physics simulation capabilities.","sentences":["[Abridged abstract] Large Language Models (LLMs) can solve some undergraduate-level to graduate-level physics textbook problems and are proficient at coding.","Combining these two capabilities could one day enable AI systems to simulate and predict the physical world.   ","We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to research-level computational physics problems.","We condition LLM generation on the use of well-documented and widely-used packages to elicit coding capabilities in the physics and astrophysics domains.","We contribute $\\sim 50$ original and challenging problems in celestial mechanics (with REBOUND), stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear dynamics (with SciPy).","Since our problems do not admit unique solutions, we evaluate LLM performance on several soft metrics: counts of lines that contain different types of errors (coding, physics, necessity and sufficiency) as well as a more \"educational\" Pass-Fail metric focused on capturing the salient physical ingredients of the problem at hand.   ","As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems, although about 40\\% of the solutions could plausibly get a passing grade.","About $70-90 \\%$ of the code lines produced are necessary, sufficient and correct (coding \\& physics).","Physics and coding errors are the most common, with some unnecessary or insufficient lines.","We observe significant variations across problem class and difficulty.","We identify several failure modes of GPT4 in the computational physics domain.   ","Our reconnaissance work provides a snapshot of current computational capabilities in classical physics and points to obvious improvement targets if AI systems are ever to reach a basic level of autonomy in physics simulation capabilities."],"url":"http://arxiv.org/abs/2312.02091v1"}
{"created":"2023-12-04 17:46:57","title":"Deep Set Neural Networks for forecasting asynchronous bioprocess timeseries","abstract":"Cultivation experiments often produce sparse and irregular time series. Classical approaches based on mechanistic models, like Maximum Likelihood fitting or Monte-Carlo Markov chain sampling, can easily account for sparsity and time-grid irregularities, but most statistical and Machine Learning tools are not designed for handling sparse data out-of-the-box. Among popular approaches there are various schemes for filling missing values (imputation) and interpolation into a regular grid (alignment). However, such methods transfer the biases of the interpolation or imputation models to the target model. We show that Deep Set Neural Networks equipped with triplet encoding of the input data can successfully handle bio-process data without any need for imputation or alignment procedures. The method is agnostic to the particular nature of the time series and can be adapted for any task, for example, online monitoring, predictive control, design of experiments, etc. In this work, we focus on forecasting. We argue that such an approach is especially suitable for typical cultivation processes, demonstrate the performance of the method on several forecasting tasks using data generated from macrokinetic growth models under realistic conditions, and compare the method to a conventional fitting procedure and methods based on imputation and alignment.","sentences":["Cultivation experiments often produce sparse and irregular time series.","Classical approaches based on mechanistic models, like Maximum Likelihood fitting or Monte-Carlo Markov chain sampling, can easily account for sparsity and time-grid irregularities, but most statistical and Machine Learning tools are not designed for handling sparse data out-of-the-box.","Among popular approaches there are various schemes for filling missing values (imputation) and interpolation into a regular grid (alignment).","However, such methods transfer the biases of the interpolation or imputation models to the target model.","We show that Deep Set Neural Networks equipped with triplet encoding of the input data can successfully handle bio-process data without any need for imputation or alignment procedures.","The method is agnostic to the particular nature of the time series and can be adapted for any task, for example, online monitoring, predictive control, design of experiments, etc.","In this work, we focus on forecasting.","We argue that such an approach is especially suitable for typical cultivation processes, demonstrate the performance of the method on several forecasting tasks using data generated from macrokinetic growth models under realistic conditions, and compare the method to a conventional fitting procedure and methods based on imputation and alignment."],"url":"http://arxiv.org/abs/2312.02079v1"}
{"created":"2023-12-04 17:41:52","title":"Integrating AI into CCTV Systems: A Comprehensive Evaluation of Smart Video Surveillance in Community Space","abstract":"This article presents an AI-enabled Smart Video Surveillance (SVS) designed to enhance safety in community spaces such as educational and recreational areas, and small businesses. The proposed system innovatively integrates with existing CCTV and wired camera networks, simplifying its adoption across various community cases to leverage recent AI advancements. Our SVS system, focusing on privacy, uses metadata instead of pixel data for activity recognition, aligning with ethical standards. It features cloud-based infrastructure and a mobile app for real-time, privacy-conscious alerts in communities.   This article notably pioneers a comprehensive real-world evaluation of the SVS system, covering AI-driven visual processing, statistical analysis, database management, cloud communication, and user notifications. It's also the first to assess an end-to-end anomaly detection system's performance, vital for identifying potential public safety incidents.   For our evaluation, we implemented the system in a community college, serving as an ideal model to exemplify the proposed system's capabilities. Our findings in this setting demonstrate the system's robustness, with throughput, latency, and scalability effectively managing 16 CCTV cameras. The system maintained a consistent 16.5 frames per second (FPS) over a 21-hour operation. The average end-to-end latency for detecting behavioral anomalies and alerting users was 26.76 seconds.","sentences":["This article presents an AI-enabled Smart Video Surveillance (SVS) designed to enhance safety in community spaces such as educational and recreational areas, and small businesses.","The proposed system innovatively integrates with existing CCTV and wired camera networks, simplifying its adoption across various community cases to leverage recent AI advancements.","Our SVS system, focusing on privacy, uses metadata instead of pixel data for activity recognition, aligning with ethical standards.","It features cloud-based infrastructure and a mobile app for real-time, privacy-conscious alerts in communities.   ","This article notably pioneers a comprehensive real-world evaluation of the SVS system, covering AI-driven visual processing, statistical analysis, database management, cloud communication, and user notifications.","It's also the first to assess an end-to-end anomaly detection system's performance, vital for identifying potential public safety incidents.   ","For our evaluation, we implemented the system in a community college, serving as an ideal model to exemplify the proposed system's capabilities.","Our findings in this setting demonstrate the system's robustness, with throughput, latency, and scalability effectively managing 16 CCTV cameras.","The system maintained a consistent 16.5 frames per second (FPS) over a 21-hour operation.","The average end-to-end latency for detecting behavioral anomalies and alerting users was 26.76 seconds."],"url":"http://arxiv.org/abs/2312.02078v1"}
{"created":"2023-12-04 17:37:41","title":"Federated Learning is Better with Non-Homomorphic Encryption","abstract":"Traditional AI methodologies necessitate centralized data collection, which becomes impractical when facing problems with network communication, data privacy, or storage capacity. Federated Learning (FL) offers a paradigm that empowers distributed AI model training without collecting raw data. There are different choices for providing privacy during FL training. One of the popular methodologies is employing Homomorphic Encryption (HE) - a breakthrough in privacy-preserving computation from Cryptography. However, these methods have a price in the form of extra computation and memory footprint. To resolve these issues, we propose an innovative framework that synergizes permutation-based compressors with Classical Cryptography, even though employing Classical Cryptography was assumed to be impossible in the past in the context of FL. Our framework offers a way to replace HE with cheaper Classical Cryptography primitives which provides security for the training process. It fosters asynchronous communication and provides flexible deployment options in various communication topologies.","sentences":["Traditional AI methodologies necessitate centralized data collection, which becomes impractical when facing problems with network communication, data privacy, or storage capacity.","Federated Learning (FL) offers a paradigm that empowers distributed AI model training without collecting raw data.","There are different choices for providing privacy during FL training.","One of the popular methodologies is employing Homomorphic Encryption (HE) - a breakthrough in privacy-preserving computation from Cryptography.","However, these methods have a price in the form of extra computation and memory footprint.","To resolve these issues, we propose an innovative framework that synergizes permutation-based compressors with Classical Cryptography, even though employing Classical Cryptography was assumed to be impossible in the past in the context of FL.","Our framework offers a way to replace HE with cheaper Classical Cryptography primitives which provides security for the training process.","It fosters asynchronous communication and provides flexible deployment options in various communication topologies."],"url":"http://arxiv.org/abs/2312.02074v1"}
{"created":"2023-12-04 17:18:24","title":"Right-sizing compute resource allocations for bioinformatics tools with Total Perspective Vortex","abstract":"In biomedical research, computational methods have become indispensable and their use is increasing, making the efficient allocation of computing resources paramount. Practitioners routinely allocate resources far in excess of what is required for batch processing jobs, leading to not just inflated wait times and costs, but also unnecessary carbon emissions. This is not without reason however, as accurately determining resource needs is complex, affected by the nature of tools, data size, and analysis parameters, especially on popular servers that handle numerous jobs. The Galaxy platform, a web-based hub for biomedical analysis used globally by scientists, exemplifies this challenge. Serving nearly half a million registered users and managing around 2 million monthly jobs, Galaxy's growth outpaces the resources at its disposal. This is necessitating smarter resource utilization. To address this, we have developed a tool named Total Perspective Vortex (TPV) - a software package that right-sizes resource allocations for each job. TPV is able to dynamically set resource requirements for individual jobs and perform meta-scheduling across heterogeneous resources. It also includes a first-ever community-curated database of default resource requirements for nearly 1,000 popular bioinformatics tools. Deployments in Galaxy Australia and Europe demonstrate its effectiveness with meta-scheduling user jobs and an improved experience for systems administrators managing Galaxy servers.","sentences":["In biomedical research, computational methods have become indispensable and their use is increasing, making the efficient allocation of computing resources paramount.","Practitioners routinely allocate resources far in excess of what is required for batch processing jobs, leading to not just inflated wait times and costs, but also unnecessary carbon emissions.","This is not without reason however, as accurately determining resource needs is complex, affected by the nature of tools, data size, and analysis parameters, especially on popular servers that handle numerous jobs.","The Galaxy platform, a web-based hub for biomedical analysis used globally by scientists, exemplifies this challenge.","Serving nearly half a million registered users and managing around 2 million monthly jobs, Galaxy's growth outpaces the resources at its disposal.","This is necessitating smarter resource utilization.","To address this, we have developed a tool named Total Perspective Vortex (TPV) - a software package that right-sizes resource allocations for each job.","TPV is able to dynamically set resource requirements for individual jobs and perform meta-scheduling across heterogeneous resources.","It also includes a first-ever community-curated database of default resource requirements for nearly 1,000 popular bioinformatics tools.","Deployments in Galaxy Australia and Europe demonstrate its effectiveness with meta-scheduling user jobs and an improved experience for systems administrators managing Galaxy servers."],"url":"http://arxiv.org/abs/2312.02060v1"}
{"created":"2023-12-04 17:10:25","title":"DUCK: Distance-based Unlearning via Centroid Kinematics","abstract":"Machine Unlearning is rising as a new field, driven by the pressing necessity of ensuring privacy in modern artificial intelligence models. This technique primarily aims to eradicate any residual influence of a specific subset of data from the knowledge acquired by a neural model during its training. This work introduces a novel unlearning algorithm, denoted as Distance-based Unlearning via Centroid Kinematics (DUCK), which employs metric learning to guide the removal of samples matching the nearest incorrect centroid in the embedding space. Evaluation of the algorithm's performance is conducted across various benchmark datasets in two distinct scenarios, class removal, and homogeneous sampling removal, obtaining state-of-the-art performance. We introduce a novel metric, called Adaptive Unlearning Score (AUS), encompassing not only the efficacy of the unlearning process in forgetting target data but also quantifying the performance loss relative to the original model. Moreover, we propose a novel membership inference attack to assess the algorithm's capacity to erase previously acquired knowledge, designed to be adaptable to future methodologies.","sentences":["Machine Unlearning is rising as a new field, driven by the pressing necessity of ensuring privacy in modern artificial intelligence models.","This technique primarily aims to eradicate any residual influence of a specific subset of data from the knowledge acquired by a neural model during its training.","This work introduces a novel unlearning algorithm, denoted as Distance-based Unlearning via Centroid Kinematics (DUCK), which employs metric learning to guide the removal of samples matching the nearest incorrect centroid in the embedding space.","Evaluation of the algorithm's performance is conducted across various benchmark datasets in two distinct scenarios, class removal, and homogeneous sampling removal, obtaining state-of-the-art performance.","We introduce a novel metric, called Adaptive Unlearning Score (AUS), encompassing not only the efficacy of the unlearning process in forgetting target data but also quantifying the performance loss relative to the original model.","Moreover, we propose a novel membership inference attack to assess the algorithm's capacity to erase previously acquired knowledge, designed to be adaptable to future methodologies."],"url":"http://arxiv.org/abs/2312.02052v1"}
{"created":"2023-12-04 17:02:59","title":"Isomorphism for Tournaments of Small Twin Width","abstract":"We prove that isomorphism of tournaments of twin width at most $k$ can be decided in time $k^{O(\\log k)}n^{O(1)}$. This implies that the isomorphism problem for classes of tournaments of bounded or moderately growing twin width is in polynomial time. By comparison, there are classes of undirected graphs of bounded twin width that are isomorphism complete, that is, the isomorphism problem for the classes is as hard as the general graph isomorphism problem. Twin width is a graph parameter that has been introduced only recently (Bonnet et al., FOCS 2020), but has received a lot of attention in structural graph theory since then. On directed graphs, it is functionally smaller than clique width. We prove that on tournaments (but not on general directed graphs) it is also functionally smaller than directed tree width (and thus, the same also holds for cut width and directed path width). Hence, our result implies that tournament isomorphism testing is also fixed-parameter tractable when parameterized by any of these parameters.   Our isomorphism algorithm heavily employs group-theoretic techniques. This seems to be necessary: as a second main result, we show that the combinatorial Weisfeiler-Leman algorithm does not decide isomorphism of tournaments of twin width at most 35 if its dimension is $o(\\sqrt n)$. (Throughout this abstract, $n$ is the order of the input graphs.)","sentences":["We prove that isomorphism of tournaments of twin width at most $k$ can be decided in time $k^{O(\\log k)}n^{O(1)}$.","This implies that the isomorphism problem for classes of tournaments of bounded or moderately growing twin width is in polynomial time.","By comparison, there are classes of undirected graphs of bounded twin width that are isomorphism complete, that is, the isomorphism problem for the classes is as hard as the general graph isomorphism problem.","Twin width is a graph parameter that has been introduced only recently (Bonnet et al., FOCS 2020), but has received a lot of attention in structural graph theory since then.","On directed graphs, it is functionally smaller than clique width.","We prove that on tournaments (but not on general directed graphs)","it is also functionally smaller than directed tree width (and thus, the same also holds for cut width and directed path width).","Hence, our result implies that tournament isomorphism testing is also fixed-parameter tractable when parameterized by any of these parameters.   ","Our isomorphism algorithm heavily employs group-theoretic techniques.","This seems to be necessary: as a second main result, we show that the combinatorial Weisfeiler-Leman algorithm does not decide isomorphism of tournaments of twin width at most 35 if its dimension is $o(\\sqrt n)$. (Throughout this abstract, $n$ is the order of the input graphs.)"],"url":"http://arxiv.org/abs/2312.02048v1"}
{"created":"2023-12-04 16:54:40","title":"GFS: Graph-based Feature Synthesis for Prediction over Relational Databases","abstract":"Relational databases are extensively utilized in a variety of modern information system applications, and they always carry valuable data patterns. There are a huge number of data mining or machine learning tasks conducted on relational databases. However, it is worth noting that there are limited machine learning models specifically designed for relational databases, as most models are primarily tailored for single table settings. Consequently, the prevalent approach for training machine learning models on data stored in relational databases involves performing feature engineering to merge the data from multiple tables into a single table and subsequently applying single table models. This approach not only requires significant effort in feature engineering but also destroys the inherent relational structure present in the data. To address these challenges, we propose a novel framework called Graph-based Feature Synthesis (GFS). GFS formulates the relational database as a heterogeneous graph, thereby preserving the relational structure within the data. By leveraging the inductive bias from single table models, GFS effectively captures the intricate relationships inherent in each table. Additionally, the whole framework eliminates the need for manual feature engineering. In the extensive experiment over four real-world multi-table relational databases, GFS outperforms previous methods designed for relational databases, demonstrating its superior performance.","sentences":["Relational databases are extensively utilized in a variety of modern information system applications, and they always carry valuable data patterns.","There are a huge number of data mining or machine learning tasks conducted on relational databases.","However, it is worth noting that there are limited machine learning models specifically designed for relational databases, as most models are primarily tailored for single table settings.","Consequently, the prevalent approach for training machine learning models on data stored in relational databases involves performing feature engineering to merge the data from multiple tables into a single table and subsequently applying single table models.","This approach not only requires significant effort in feature engineering but also destroys the inherent relational structure present in the data.","To address these challenges, we propose a novel framework called Graph-based Feature Synthesis (GFS).","GFS formulates the relational database as a heterogeneous graph, thereby preserving the relational structure within the data.","By leveraging the inductive bias from single table models, GFS effectively captures the intricate relationships inherent in each table.","Additionally, the whole framework eliminates the need for manual feature engineering.","In the extensive experiment over four real-world multi-table relational databases, GFS outperforms previous methods designed for relational databases, demonstrating its superior performance."],"url":"http://arxiv.org/abs/2312.02037v1"}
{"created":"2023-12-04 16:51:23","title":"Implicit Learning of Scene Geometry from Poses for Global Localization","abstract":"Global visual localization estimates the absolute pose of a camera using a single image, in a previously mapped area. Obtaining the pose from a single image enables many robotics and augmented/virtual reality applications. Inspired by latest advances in deep learning, many existing approaches directly learn and regress 6 DoF pose from an input image. However, these methods do not fully utilize the underlying scene geometry for pose regression. The challenge in monocular relocalization is the minimal availability of supervised training data, which is just the corresponding 6 DoF poses of the images. In this paper, we propose to utilize these minimal available labels (.i.e, poses) to learn the underlying 3D geometry of the scene and use the geometry to estimate the 6 DoF camera pose. We present a learning method that uses these pose labels and rigid alignment to learn two 3D geometric representations (\\textit{X, Y, Z coordinates}) of the scene, one in camera coordinate frame and the other in global coordinate frame. Given a single image, it estimates these two 3D scene representations, which are then aligned to estimate a pose that matches the pose label. This formulation allows for the active inclusion of additional learning constraints to minimize 3D alignment errors between the two 3D scene representations, and 2D re-projection errors between the 3D global scene representation and 2D image pixels, resulting in improved localization accuracy. During inference, our model estimates the 3D scene geometry in camera and global frames and aligns them rigidly to obtain pose in real-time. We evaluate our work on three common visual localization datasets, conduct ablation studies, and show that our method exceeds state-of-the-art regression methods' pose accuracy on all datasets.","sentences":["Global visual localization estimates the absolute pose of a camera using a single image, in a previously mapped area.","Obtaining the pose from a single image enables many robotics and augmented/virtual reality applications.","Inspired by latest advances in deep learning, many existing approaches directly learn and regress 6 DoF pose from an input image.","However, these methods do not fully utilize the underlying scene geometry for pose regression.","The challenge in monocular relocalization is the minimal availability of supervised training data, which is just the corresponding 6 DoF poses of the images.","In this paper, we propose to utilize these minimal available labels (.i.e, poses) to learn the underlying 3D geometry of the scene and use the geometry to estimate the 6 DoF camera pose.","We present a learning method that uses these pose labels and rigid alignment to learn two 3D geometric representations (\\textit{X, Y, Z coordinates}) of the scene, one in camera coordinate frame and the other in global coordinate frame.","Given a single image, it estimates these two 3D scene representations, which are then aligned to estimate a pose that matches the pose label.","This formulation allows for the active inclusion of additional learning constraints to minimize 3D alignment errors between the two 3D scene representations, and 2D re-projection errors between the 3D global scene representation and 2D image pixels, resulting in improved localization accuracy.","During inference, our model estimates the 3D scene geometry in camera and global frames and aligns them rigidly to obtain pose in real-time.","We evaluate our work on three common visual localization datasets, conduct ablation studies, and show that our method exceeds state-of-the-art regression methods' pose accuracy on all datasets."],"url":"http://arxiv.org/abs/2312.02029v1"}
{"created":"2023-12-04 16:36:29","title":"Optimal Data Generation in Multi-Dimensional Parameter Spaces, using Bayesian Optimization","abstract":"Acquiring a substantial number of data points for training accurate machine learning (ML) models is a big challenge in scientific fields where data collection is resource-intensive. Here, we propose a novel approach for constructing a minimal yet highly informative database for training ML models in complex multi-dimensional parameter spaces. To achieve this, we mimic the underlying relation between the output and input parameters using Gaussian process regression (GPR). Using a set of known data, GPR provides predictive means and standard deviation for the unknown data. Given the predicted standard deviation by GPR, we select data points using Bayesian optimization to obtain an efficient database for training ML models. We compare the performance of ML models trained on databases obtained through this method, with databases obtained using traditional approaches. Our results demonstrate that the ML models trained on the database obtained using Bayesian optimization approach consistently outperform the other two databases, achieving high accuracy with a significantly smaller number of data points. Our work contributes to the resource-efficient collection of data in high-dimensional complex parameter spaces, to achieve high precision machine learning predictions.","sentences":["Acquiring a substantial number of data points for training accurate machine learning (ML) models is a big challenge in scientific fields where data collection is resource-intensive.","Here, we propose a novel approach for constructing a minimal yet highly informative database for training ML models in complex multi-dimensional parameter spaces.","To achieve this, we mimic the underlying relation between the output and input parameters using Gaussian process regression (GPR).","Using a set of known data, GPR provides predictive means and standard deviation for the unknown data.","Given the predicted standard deviation by GPR, we select data points using Bayesian optimization to obtain an efficient database for training ML models.","We compare the performance of ML models trained on databases obtained through this method, with databases obtained using traditional approaches.","Our results demonstrate that the ML models trained on the database obtained using Bayesian optimization approach consistently outperform the other two databases, achieving high accuracy with a significantly smaller number of data points.","Our work contributes to the resource-efficient collection of data in high-dimensional complex parameter spaces, to achieve high precision machine learning predictions."],"url":"http://arxiv.org/abs/2312.02012v1"}
{"created":"2023-12-04 16:32:51","title":"Towards Learning a Generalist Model for Embodied Navigation","abstract":"Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries. Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios. Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation. Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based instruction. The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks. This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide range of capabilities required by embodied navigation. We conduct extensive experiments to evaluate the performance and generalizability of our model. The experimental results demonstrate that our unified model achieves state-of-the-art performance on CVDN, SOON, and ScanQA. Specifically, it surpasses the previous stats-of-the-art method by a significant margin of 29% in goal progress on CVDN. Moreover, our model also demonstrates strong generalizability and presents impressive results on unseen tasks, e.g., embodied question answering and 3D captioning.","sentences":["Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries.","Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios.","Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation.","Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM.","It adapts LLMs to embodied navigation by introducing schema-based instruction.","The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks.","This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide range of capabilities required by embodied navigation.","We conduct extensive experiments to evaluate the performance and generalizability of our model.","The experimental results demonstrate that our unified model achieves state-of-the-art performance on CVDN, SOON, and ScanQA.","Specifically, it surpasses the previous stats-of-the-art method by a significant margin of 29% in goal progress on CVDN.","Moreover, our model also demonstrates strong generalizability and presents impressive results on unseen tasks, e.g., embodied question answering and 3D captioning."],"url":"http://arxiv.org/abs/2312.02010v1"}
{"created":"2023-12-04 16:30:19","title":"Multi-Agent Behavior Retrieval","abstract":"This paper aims to enable multi-agent systems to effectively utilize past memories to adapt to novel collaborative tasks in a data-efficient fashion. We propose the Multi-Agent Coordination Skill Database, a repository for storing a collection of coordinated behaviors associated with the key vector distinctive to them. Our Transformer-based skill encoder effectively captures spatio-temporal interactions that contribute to coordination and provide a skill representation unique to each coordinated behavior. By leveraging a small number of demonstrations of the target task, the database allows us to train the policy using a dataset augmented with the retrieved demonstrations. Experimental evaluations clearly demonstrate that our method achieves a significantly higher success rate in push manipulation tasks compared to baseline methods like few-shot imitation learning. Furthermore, we validate the effectiveness of our retrieve-and-learn framework in a real environment using a team of wheeled robots.","sentences":["This paper aims to enable multi-agent systems to effectively utilize past memories to adapt to novel collaborative tasks in a data-efficient fashion.","We propose the Multi-Agent Coordination Skill Database, a repository for storing a collection of coordinated behaviors associated with the key vector distinctive to them.","Our Transformer-based skill encoder effectively captures spatio-temporal interactions that contribute to coordination and provide a skill representation unique to each coordinated behavior.","By leveraging a small number of demonstrations of the target task, the database allows us to train the policy using a dataset augmented with the retrieved demonstrations.","Experimental evaluations clearly demonstrate that our method achieves a significantly higher success rate in push manipulation tasks compared to baseline methods like few-shot imitation learning.","Furthermore, we validate the effectiveness of our retrieve-and-learn framework in a real environment using a team of wheeled robots."],"url":"http://arxiv.org/abs/2312.02008v1"}
{"created":"2023-12-04 16:25:18","title":"A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly","abstract":"Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes findings into \"The Good\" (beneficial LLM applications), \"The Bad\" (offensive applications), and \"The Ugly\" (vulnerabilities and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code and data security, outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.","sentences":["Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation.","They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation).","In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks.","This paper explores the intersection of LLMs with security and privacy.","Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs.","Through a comprehensive literature review, the paper categorizes findings into \"The Good\" (beneficial LLM applications), \"The Bad\" (offensive applications), and \"The Ugly\" (vulnerabilities and their defenses).","We have some interesting findings.","For example, LLMs have proven to enhance code and data security, outperforming traditional methods.","However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities.","We have identified areas that require further research efforts.","For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality.","Safe instruction tuning, a recent development, requires more exploration.","We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity."],"url":"http://arxiv.org/abs/2312.02003v1"}
{"created":"2023-12-04 16:14:43","title":"A Generative Self-Supervised Framework using Functional Connectivity in fMRI Data","abstract":"Deep neural networks trained on Functional Connectivity (FC) networks extracted from functional Magnetic Resonance Imaging (fMRI) data have gained popularity due to the increasing availability of data and advances in model architectures, including Graph Neural Network (GNN). Recent research on the application of GNN to FC suggests that exploiting the time-varying properties of the FC could significantly improve the accuracy and interpretability of the model prediction. However, the high cost of acquiring high-quality fMRI data and corresponding phenotypic labels poses a hurdle to their application in real-world settings, such that a model na\\\"ively trained in a supervised fashion can suffer from insufficient performance or a lack of generalization on a small number of data. In addition, most Self-Supervised Learning (SSL) approaches for GNNs to date adopt a contrastive strategy, which tends to lose appropriate semantic information when the graph structure is perturbed or does not leverage both spatial and temporal information simultaneously. In light of these challenges, we propose a generative SSL approach that is tailored to effectively harness spatio-temporal information within dynamic FC. Our empirical results, experimented with large-scale (>50,000) fMRI datasets, demonstrate that our approach learns valuable representations and enables the construction of accurate and robust models when fine-tuned for downstream tasks.","sentences":["Deep neural networks trained on Functional Connectivity (FC) networks extracted from functional Magnetic Resonance Imaging (fMRI) data have gained popularity due to the increasing availability of data and advances in model architectures, including Graph Neural Network (GNN).","Recent research on the application of GNN to FC suggests that exploiting the time-varying properties of the FC could significantly improve the accuracy and interpretability of the model prediction.","However, the high cost of acquiring high-quality fMRI data and corresponding phenotypic labels poses a hurdle to their application in real-world settings, such that a model na\\\"ively trained in a supervised fashion can suffer from insufficient performance or a lack of generalization on a small number of data.","In addition, most Self-Supervised Learning (SSL) approaches for GNNs to date adopt a contrastive strategy, which tends to lose appropriate semantic information when the graph structure is perturbed or does not leverage both spatial and temporal information simultaneously.","In light of these challenges, we propose a generative SSL approach that is tailored to effectively harness spatio-temporal information within dynamic FC.","Our empirical results, experimented with large-scale (>50,000) fMRI datasets, demonstrate that our approach learns valuable representations and enables the construction of accurate and robust models when fine-tuned for downstream tasks."],"url":"http://arxiv.org/abs/2312.01994v1"}
{"created":"2023-12-04 16:08:47","title":"SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention","abstract":"We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new method of fine-tuning proposed by us, called up-training. It converts pre-trained or already fine-tuned Transformer-based robotic policies of quadratic time complexity (including massive billion-parameter vision-language-action models or VLAs), into their efficient linear-attention counterparts maintaining high quality. We demonstrate the effectiveness of SARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the first VLA robotic policies pre-trained on internet-scale data, as well as (b) Point Cloud Transformer (PCT) robotic policies operating on large point clouds. We complement our results with the rigorous mathematical analysis providing deeper insight into the phenomenon of SARA.","sentences":["We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment.","SARA-RT relies on the new method of fine-tuning proposed by us, called up-training.","It converts pre-trained or already fine-tuned Transformer-based robotic policies of quadratic time complexity (including massive billion-parameter vision-language-action models or VLAs), into their efficient linear-attention counterparts maintaining high quality.","We demonstrate the effectiveness of SARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the first VLA robotic policies pre-trained on internet-scale data, as well as (b) Point Cloud Transformer (PCT) robotic policies operating on large point clouds.","We complement our results with the rigorous mathematical analysis providing deeper insight into the phenomenon of SARA."],"url":"http://arxiv.org/abs/2312.01990v1"}
{"created":"2023-12-04 15:59:27","title":"UniGS: Unified Representation for Image Generation and Segmentation","abstract":"This paper introduces a novel unified representation of diffusion models for image generation and segmentation. Specifically, we use a colormap to represent entity-level masks, addressing the challenge of varying entity numbers while aligning the representation closely with the image RGB domain. Two novel modules, including the location-aware color palette and progressive dichotomy module, are proposed to support our mask representation. On the one hand, a location-aware palette guarantees the colors' consistency to entities' locations. On the other hand, the progressive dichotomy module can efficiently decode the synthesized colormap to high-quality entity-level masks in a depth-first binary search without knowing the cluster numbers. To tackle the issue of lacking large-scale segmentation training data, we employ an inpainting pipeline and then improve the flexibility of diffusion models across various tasks, including inpainting, image synthesis, referring segmentation, and entity segmentation. Comprehensive experiments validate the efficiency of our approach, demonstrating comparable segmentation mask quality to state-of-the-art and adaptability to multiple tasks. The code will be released at \\href{https://github.com/qqlu/Entity}{https://github.com/qqlu/Entity}.","sentences":["This paper introduces a novel unified representation of diffusion models for image generation and segmentation.","Specifically, we use a colormap to represent entity-level masks, addressing the challenge of varying entity numbers while aligning the representation closely with the image RGB domain.","Two novel modules, including the location-aware color palette and progressive dichotomy module, are proposed to support our mask representation.","On the one hand, a location-aware palette guarantees the colors' consistency to entities' locations.","On the other hand, the progressive dichotomy module can efficiently decode the synthesized colormap to high-quality entity-level masks in a depth-first binary search without knowing the cluster numbers.","To tackle the issue of lacking large-scale segmentation training data, we employ an inpainting pipeline and then improve the flexibility of diffusion models across various tasks, including inpainting, image synthesis, referring segmentation, and entity segmentation.","Comprehensive experiments validate the efficiency of our approach, demonstrating comparable segmentation mask quality to state-of-the-art and adaptability to multiple tasks.","The code will be released at \\href{https://github.com/qqlu/Entity}{https://github.com/qqlu/Entity}."],"url":"http://arxiv.org/abs/2312.01985v1"}
{"created":"2023-12-04 15:50:16","title":"Unveiling Competition Dynamics in Mobile App Markets through User Reviews","abstract":"User reviews published in mobile app repositories are essential for understanding user satisfaction and engagement within a specific market segment. Manual analysis of these reviews is impractical due to the large volume of available data, while automatic analysis poses several challenges, including data synthesis and effective reporting. These challenges complicate the task for app providers in identifying hidden patterns and significant events related to app acceptance, especially in assessing the influence of competitor apps. Furthermore, review-based analysis is mostly limited to a single app or a single app provider, excluding potential market and competition analysis. Following a case-study research method in the microblogging app market, we introduce an automatic, novel approach to support mobile app market analysis processes through quantitative metrics and event detection techniques based on newly published user reviews. Significant events are proactively identified and summarized by comparing metric deviations with historical baseline indicators within the lifecycle of a mobile app. Results from our case study show empirical evidence of the detection of relevant events within the selected market segment, including software- or release-based events, contextual events and the emergence of new competitors.","sentences":["User reviews published in mobile app repositories are essential for understanding user satisfaction and engagement within a specific market segment.","Manual analysis of these reviews is impractical due to the large volume of available data, while automatic analysis poses several challenges, including data synthesis and effective reporting.","These challenges complicate the task for app providers in identifying hidden patterns and significant events related to app acceptance, especially in assessing the influence of competitor apps.","Furthermore, review-based analysis is mostly limited to a single app or a single app provider, excluding potential market and competition analysis.","Following a case-study research method in the microblogging app market, we introduce an automatic, novel approach to support mobile app market analysis processes through quantitative metrics and event detection techniques based on newly published user reviews.","Significant events are proactively identified and summarized by comparing metric deviations with historical baseline indicators within the lifecycle of a mobile app.","Results from our case study show empirical evidence of the detection of relevant events within the selected market segment, including software- or release-based events, contextual events and the emergence of new competitors."],"url":"http://arxiv.org/abs/2312.01981v1"}
{"created":"2023-12-04 15:33:00","title":"CaRL: Cascade Reinforcement Learning with State Space Splitting for O-RAN based Traffic Steering","abstract":"The Open Radio Access Network (O-RAN) architecture empowers intelligent and automated optimization of the RAN through applications deployed on the RAN Intelligent Controller (RIC) platform, enabling capabilities beyond what is achievable with traditional RAN solutions. Within this paradigm, Traffic Steering (TS) emerges as a pivotal RIC application that focuses on optimizing cell-level mobility settings in near-real-time, aiming to significantly improve network spectral efficiency. In this paper, we design a novel TS algorithm based on a Cascade Reinforcement Learning (CaRL) framework. We propose state space factorization and policy decomposition to reduce the need for large models and well-labeled datasets. For each sub-state space, an RL sub-policy will be trained to learn an optimized mapping onto the action space. To apply CaRL on new network regions, we propose a knowledge transfer approach to initialize a new sub-policy based on knowledge learned by the trained policies. To evaluate CaRL, we build a data-driven and scalable RIC digital twin (DT) that is modeled using important real-world data, including network configuration, user geo-distribution, and traffic demand, among others, from a tier-1 mobile operator in the US. We evaluate CaRL on two DT scenarios representing two network clusters in two different cities and compare its performance with the business-as-usual (BAU) policy and other competing optimization approaches using heuristic and Q-table algorithms. Benchmarking results show that CaRL performs the best and improves the average cluster-aggregated downlink throughput over the BAU policy by 24% and 18% in these two scenarios, respectively.","sentences":["The Open Radio Access Network (O-RAN) architecture empowers intelligent and automated optimization of the RAN through applications deployed on the RAN Intelligent Controller (RIC) platform, enabling capabilities beyond what is achievable with traditional RAN solutions.","Within this paradigm, Traffic Steering (TS) emerges as a pivotal RIC application that focuses on optimizing cell-level mobility settings in near-real-time, aiming to significantly improve network spectral efficiency.","In this paper, we design a novel TS algorithm based on a Cascade Reinforcement Learning (CaRL) framework.","We propose state space factorization and policy decomposition to reduce the need for large models and well-labeled datasets.","For each sub-state space, an RL sub-policy will be trained to learn an optimized mapping onto the action space.","To apply CaRL on new network regions, we propose a knowledge transfer approach to initialize a new sub-policy based on knowledge learned by the trained policies.","To evaluate CaRL, we build a data-driven and scalable RIC digital twin (DT) that is modeled using important real-world data, including network configuration, user geo-distribution, and traffic demand, among others, from a tier-1 mobile operator in the US.","We evaluate CaRL on two DT scenarios representing two network clusters in two different cities and compare its performance with the business-as-usual (BAU) policy and other competing optimization approaches using heuristic and Q-table algorithms.","Benchmarking results show that CaRL performs the best and improves the average cluster-aggregated downlink throughput over the BAU policy by 24% and 18% in these two scenarios, respectively."],"url":"http://arxiv.org/abs/2312.01970v1"}
{"created":"2023-12-04 15:26:46","title":"Augmenting Channel Charting with Classical Wireless Source Localization Techniques","abstract":"Channel Charting aims to construct a map of the radio environment by leveraging similarity relationships found in high-dimensional channel state information. Although resulting channel charts usually accurately represent local neighborhood relationships, even under conditions with strong multipath propagation, they often fall short in capturing global geometric features. On the other hand, classical model-based localization methods, such as triangulation and multilateration, can easily localize signal sources in the global coordinate frame. However, these methods rely heavily on the assumption of line-of-sight channels and distributed antenna deployments. Based on measured data, we compare classical source localization techniques to channel charts with respect to localization performance. We suggest and evaluate methods to enhance Channel Charting with model-based localization approaches: One approach involves using information derived from classical localization methods to map channel chart locations to physical positions after conventional training of the forward charting function. Foremost, though, we suggest to incorporate information from model-based approaches during the training of the forward charting function in what we call \"augmented Channel Charting\". We demonstrate that Channel Charting can outperform classical localization methods on the considered dataset.","sentences":["Channel Charting aims to construct a map of the radio environment by leveraging similarity relationships found in high-dimensional channel state information.","Although resulting channel charts usually accurately represent local neighborhood relationships, even under conditions with strong multipath propagation, they often fall short in capturing global geometric features.","On the other hand, classical model-based localization methods, such as triangulation and multilateration, can easily localize signal sources in the global coordinate frame.","However, these methods rely heavily on the assumption of line-of-sight channels and distributed antenna deployments.","Based on measured data, we compare classical source localization techniques to channel charts with respect to localization performance.","We suggest and evaluate methods to enhance Channel Charting with model-based localization approaches: One approach involves using information derived from classical localization methods to map channel chart locations to physical positions after conventional training of the forward charting function.","Foremost, though, we suggest to incorporate information from model-based approaches during the training of the forward charting function in what we call \"augmented Channel Charting\".","We demonstrate that Channel Charting can outperform classical localization methods on the considered dataset."],"url":"http://arxiv.org/abs/2312.01968v1"}
{"created":"2023-12-04 15:16:34","title":"Mechanical Comparison of Arrangement Strategies for Topological Interlocking Assemblies","abstract":"Topological Interlocking assemblies are arrangements of blocks kinematically constrained by a fixed frame, such that all rigid body motions of each block are constrained only by its permanent contact with other blocks and the frame. In the literature several blocks are introduced that can be arranged into different interlocking assemblies. In this study we investigate the influence of arrangement on the overall structural behaviour of the resulting interlocking assemblies. This is performed using the Versatile Block, as it can be arranged in three different doubly periodic ways given by wallpaper symmetries. Our focus lies on the load transfer mechanisms from the assembly onto the frame. For fast a priori evaluation of the assemblies we introduce a combinatorial model called Interlocking Flows. To investigate our assemblies from a mechanical point of view we conduct several finite element studies. These reveal a strong influence of arrangement on the structural behaviour, for instance, an impact on both the point and amount of maximum deflection. The results of the finite element analysis are in very good agreement with the predictions of the Interlocking Flow model. Our source code, data and examples are available under https://doi.org/10.5281/zenodo.10246034.","sentences":["Topological Interlocking assemblies are arrangements of blocks kinematically constrained by a fixed frame, such that all rigid body motions of each block are constrained only by its permanent contact with other blocks and the frame.","In the literature several blocks are introduced that can be arranged into different interlocking assemblies.","In this study we investigate the influence of arrangement on the overall structural behaviour of the resulting interlocking assemblies.","This is performed using the Versatile Block, as it can be arranged in three different doubly periodic ways given by wallpaper symmetries.","Our focus lies on the load transfer mechanisms from the assembly onto the frame.","For fast a priori evaluation of the assemblies we introduce a combinatorial model called Interlocking Flows.","To investigate our assemblies from a mechanical point of view we conduct several finite element studies.","These reveal a strong influence of arrangement on the structural behaviour, for instance, an impact on both the point and amount of maximum deflection.","The results of the finite element analysis are in very good agreement with the predictions of the Interlocking Flow model.","Our source code, data and examples are available under https://doi.org/10.5281/zenodo.10246034."],"url":"http://arxiv.org/abs/2312.01958v1"}
{"created":"2023-12-04 15:16:12","title":"Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective","abstract":"This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \\url{https://github.com/vicgalle/distilled-self-critique}.","sentences":["This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model.","Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs.","Code released at \\url{https://github.com/vicgalle/distilled-self-critique}."],"url":"http://arxiv.org/abs/2312.01957v1"}
{"created":"2023-12-04 15:10:44","title":"DFTWS for blockchain: Deterministic, Fair and Transparent Winner Selection","abstract":"This publication describes the block winner selection process that will be used in a novel Proof-of-Useful-Work blockchain for High Energy Physics that the authors are currently working on. Instead of spamming hashing operations to mine blocks, miners will be running Monte Carlo simulations to support a real-world HEP experiment with useful data. The block problems will be defined by a Root Authority which is represented by a HEP experiment like CBM. The focus in this publication is a mechanism that allows the Root Authority to select a winner from a list of nodes that solved a block problem. The mechanism is designed so that winner selection is deterministic, fair and transparent. This mechanism allows every node to verify the fairness of the winner selection process without giving the nodes a tool to be able to improve their own winning chances.","sentences":["This publication describes the block winner selection process that will be used in a novel Proof-of-Useful-Work blockchain for High Energy Physics that the authors are currently working on.","Instead of spamming hashing operations to mine blocks, miners will be running Monte Carlo simulations to support a real-world HEP experiment with useful data.","The block problems will be defined by a Root Authority which is represented by a HEP experiment like CBM.","The focus in this publication is a mechanism that allows the Root Authority to select a winner from a list of nodes that solved a block problem.","The mechanism is designed so that winner selection is deterministic, fair and transparent.","This mechanism allows every node to verify the fairness of the winner selection process without giving the nodes a tool to be able to improve their own winning chances."],"url":"http://arxiv.org/abs/2312.01951v1"}
{"created":"2023-12-04 14:55:58","title":"Foundations for Transfer in Reinforcement Learning: A Taxonomy of Knowledge Modalities","abstract":"Contemporary artificial intelligence systems exhibit rapidly growing abilities accompanied by the growth of required resources, expansive datasets and corresponding investments into computing infrastructure. Although earlier successes predominantly focus on constrained settings, recent strides in fundamental research and applications aspire to create increasingly general systems. This evolving landscape presents a dual panorama of opportunities and challenges in refining the generalisation and transfer of knowledge - the extraction from existing sources and adaptation as a comprehensive foundation for tackling new problems. Within the domain of reinforcement learning (RL), the representation of knowledge manifests through various modalities, including dynamics and reward models, value functions, policies, and the original data. This taxonomy systematically targets these modalities and frames its discussion based on their inherent properties and alignment with different objectives and mechanisms for transfer. Where possible, we aim to provide coarse guidance delineating approaches which address requirements such as limiting environment interactions, maximising computational efficiency, and enhancing generalisation across varying axes of change. Finally, we analyse reasons contributing to the prevalence or scarcity of specific forms of transfer, the inherent potential behind pushing these frontiers, and underscore the significance of transitioning from designed to learned transfer.","sentences":["Contemporary artificial intelligence systems exhibit rapidly growing abilities accompanied by the growth of required resources, expansive datasets and corresponding investments into computing infrastructure.","Although earlier successes predominantly focus on constrained settings, recent strides in fundamental research and applications aspire to create increasingly general systems.","This evolving landscape presents a dual panorama of opportunities and challenges in refining the generalisation and transfer of knowledge - the extraction from existing sources and adaptation as a comprehensive foundation for tackling new problems.","Within the domain of reinforcement learning (RL), the representation of knowledge manifests through various modalities, including dynamics and reward models, value functions, policies, and the original data.","This taxonomy systematically targets these modalities and frames its discussion based on their inherent properties and alignment with different objectives and mechanisms for transfer.","Where possible, we aim to provide coarse guidance delineating approaches which address requirements such as limiting environment interactions, maximising computational efficiency, and enhancing generalisation across varying axes of change.","Finally, we analyse reasons contributing to the prevalence or scarcity of specific forms of transfer, the inherent potential behind pushing these frontiers, and underscore the significance of transitioning from designed to learned transfer."],"url":"http://arxiv.org/abs/2312.01939v1"}
{"created":"2023-12-04 14:44:31","title":"Efficiency of Unsupervised Anomaly Detection Methods on Software Logs","abstract":"Software log analysis can be laborious and time consuming. Time and labeled data are usually lacking in industrial settings. This paper studies unsupervised and time efficient methods for anomaly detection. We study two custom and two established models. The custom models are: an OOV (Out-Of-Vocabulary) detector, which counts the terms in the test data that are not present in the training data, and the Rarity Model (RM), which calculates a rarity score for terms based on their infrequency. The established models are KMeans and Isolation Forest. The models are evaluated on four public datasets (BGL, Thunderbird, Hadoop, HDFS) with three different representation techniques for the log messages (Words, character Trigrams, Parsed events). We used the AUC-ROC metric for the evaluation. The results reveal discrepancies based on the dataset and representation technique. Different configurations are advised based on specific requirements. For speed, the OOV detector with word representation is optimal. For accuracy, the OOV detector combined with trigram representation yields the highest AUC-ROC (0.846). When dealing with unfiltered data where training includes both normal and anomalous instances, the most effective combination is the Isolation Forest with event representation, achieving an AUC-ROC of 0.829.","sentences":["Software log analysis can be laborious and time consuming.","Time and labeled data are usually lacking in industrial settings.","This paper studies unsupervised and time efficient methods for anomaly detection.","We study two custom and two established models.","The custom models are: an OOV (Out-Of-Vocabulary) detector, which counts the terms in the test data that are not present in the training data, and the Rarity Model (RM), which calculates a rarity score for terms based on their infrequency.","The established models are KMeans and Isolation Forest.","The models are evaluated on four public datasets (BGL, Thunderbird, Hadoop, HDFS) with three different representation techniques for the log messages (Words, character Trigrams, Parsed events).","We used the AUC-ROC metric for the evaluation.","The results reveal discrepancies based on the dataset and representation technique.","Different configurations are advised based on specific requirements.","For speed, the OOV detector with word representation is optimal.","For accuracy, the OOV detector combined with trigram representation yields the highest AUC-ROC (0.846).","When dealing with unfiltered data where training includes both normal and anomalous instances, the most effective combination is the Isolation Forest with event representation, achieving an AUC-ROC of 0.829."],"url":"http://arxiv.org/abs/2312.01934v1"}
{"created":"2023-12-04 14:29:28","title":"A Machine Learning Approach Towards SKILL Code Autocompletion","abstract":"As Moore's Law continues to increase the complexity of electronic systems, Electronic Design Automation (EDA) must advance to meet global demand. An important example of an EDA technology is SKILL, a scripting language used to customize and extend EDA software. Recently, code generation models using the transformer architecture have achieved impressive results in academic settings and have even been used in commercial developer tools to improve developer productivity. To the best of our knowledge, this study is the first to apply transformers to SKILL code autocompletion towards improving the productivity of hardware design engineers. In this study, a novel, data-efficient methodology for generating SKILL code is proposed and experimentally validated. More specifically, we propose a novel methodology for (i) creating a high-quality SKILL dataset with both unlabeled and labeled data, (ii) a training strategy where T5 models pre-trained on general programming language code are fine-tuned on our custom SKILL dataset using unsupervised and supervised learning, and (iii) evaluating synthesized SKILL code. We show that models trained using the proposed methodology outperform baselines in terms of human-judgment score and BLEU score. A major challenge faced was the extremely small amount of available SKILL code data that can be used to train a transformer model to generate SKILL code. Despite our validated improvements, the extremely small dataset available to us was still not enough to train a model that can reliably autocomplete SKILL code. We discuss this and other limitations as well as future work that could address these limitations.","sentences":["As Moore's Law continues to increase the complexity of electronic systems, Electronic Design Automation (EDA) must advance to meet global demand.","An important example of an EDA technology is SKILL, a scripting language used to customize and extend EDA software.","Recently, code generation models using the transformer architecture have achieved impressive results in academic settings and have even been used in commercial developer tools to improve developer productivity.","To the best of our knowledge, this study is the first to apply transformers to SKILL code autocompletion towards improving the productivity of hardware design engineers.","In this study, a novel, data-efficient methodology for generating SKILL code is proposed and experimentally validated.","More specifically, we propose a novel methodology for (i) creating a high-quality SKILL dataset with both unlabeled and labeled data, (ii) a training strategy where T5 models pre-trained on general programming language code are fine-tuned on our custom SKILL dataset using unsupervised and supervised learning, and (iii) evaluating synthesized SKILL code.","We show that models trained using the proposed methodology outperform baselines in terms of human-judgment score and BLEU score.","A major challenge faced was the extremely small amount of available SKILL code data that can be used to train a transformer model to generate SKILL code.","Despite our validated improvements, the extremely small dataset available to us was still not enough to train a model that can reliably autocomplete SKILL code.","We discuss this and other limitations as well as future work that could address these limitations."],"url":"http://arxiv.org/abs/2312.01921v1"}
{"created":"2023-12-04 14:17:40","title":"Resource Leak Checker (RLC#) for C# Code using CodeQL","abstract":"Resource leaks occur when a program fails to release a finite resource after it is no longer needed. These leaks are a significant cause of real-world crashes and performance issues. Given their critical impact on software performance and security, detecting and preventing resource leaks is a crucial problem.   Recent research has proposed a specify-and-check approach to prevent resource leaks. In this approach, programmers write resource management specifications that guide how resources are stored, passed around, and released within an application. We have developed a tool called RLC#, for detecting resource leaks in C# code. Inspired by the Resource Leak Checker (RLC) from the Checker Framework, RLC# employs CodeQL for intraprocedural data flow analysis. The tool operates in a modular fashion and relies on resource management specifications integrated at method boundaries for interprocedural analysis.   In practice, RLC# has successfully identified 24 resource leaks in open-source projects and internal proprietary Azure microservices. Its implementation is declarative, and it scales well. While it incurs a reasonable false positive rate, the burden on developers is minimal, involving the addition of specifications to the source code.","sentences":["Resource leaks occur when a program fails to release a finite resource after it is no longer needed.","These leaks are a significant cause of real-world crashes and performance issues.","Given their critical impact on software performance and security, detecting and preventing resource leaks is a crucial problem.   ","Recent research has proposed a specify-and-check approach to prevent resource leaks.","In this approach, programmers write resource management specifications that guide how resources are stored, passed around, and released within an application.","We have developed a tool called RLC#, for detecting resource leaks in C# code.","Inspired by the Resource Leak Checker (RLC) from the Checker Framework, RLC# employs CodeQL for intraprocedural data flow analysis.","The tool operates in a modular fashion and relies on resource management specifications integrated at method boundaries for interprocedural analysis.   ","In practice, RLC# has successfully identified 24 resource leaks in open-source projects and internal proprietary Azure microservices.","Its implementation is declarative, and it scales well.","While it incurs a reasonable false positive rate, the burden on developers is minimal, involving the addition of specifications to the source code."],"url":"http://arxiv.org/abs/2312.01912v1"}
{"created":"2023-12-04 14:02:56","title":"Unsupervised Anomaly Detection using Aggregated Normative Diffusion","abstract":"Early detection of anomalies in medical images such as brain MRI is highly relevant for diagnosis and treatment of many conditions. Supervised machine learning methods are limited to a small number of pathologies where there is good availability of labeled data. In contrast, unsupervised anomaly detection (UAD) has the potential to identify a broader spectrum of anomalies by spotting deviations from normal patterns. Our research demonstrates that existing state-of-the-art UAD approaches do not generalise well to diverse types of anomalies in realistic multi-modal MR data. To overcome this, we introduce a new UAD method named Aggregated Normative Diffusion (ANDi). ANDi operates by aggregating differences between predicted denoising steps and ground truth backwards transitions in Denoising Diffusion Probabilistic Models (DDPMs) that have been trained on pyramidal Gaussian noise. We validate ANDi against three recent UAD baselines, and across three diverse brain MRI datasets. We show that ANDi, in some cases, substantially surpasses these baselines and shows increased robustness to varying types of anomalies. Particularly in detecting multiple sclerosis (MS) lesions, ANDi achieves improvements of up to 178% in terms of AUPRC.","sentences":["Early detection of anomalies in medical images such as brain MRI is highly relevant for diagnosis and treatment of many conditions.","Supervised machine learning methods are limited to a small number of pathologies where there is good availability of labeled data.","In contrast, unsupervised anomaly detection (UAD) has the potential to identify a broader spectrum of anomalies by spotting deviations from normal patterns.","Our research demonstrates that existing state-of-the-art UAD approaches do not generalise well to diverse types of anomalies in realistic multi-modal MR data.","To overcome this, we introduce a new UAD method named Aggregated Normative Diffusion (ANDi).","ANDi operates by aggregating differences between predicted denoising steps and ground truth backwards transitions in Denoising Diffusion Probabilistic Models (DDPMs) that have been trained on pyramidal Gaussian noise.","We validate ANDi against three recent UAD baselines, and across three diverse brain MRI datasets.","We show that ANDi, in some cases, substantially surpasses these baselines and shows increased robustness to varying types of anomalies.","Particularly in detecting multiple sclerosis (MS) lesions, ANDi achieves improvements of up to 178% in terms of AUPRC."],"url":"http://arxiv.org/abs/2312.01904v1"}
{"created":"2023-12-04 13:40:22","title":"Non-Intrusive Load Monitoring for Feeder-Level EV Charging Detection: Sliding Window-based Approaches to Offline and Online Detection","abstract":"Understanding electric vehicle (EV) charging on the distribution network is key to effective EV charging management and aiding decarbonization across the energy and transport sectors. Advanced metering infrastructure has allowed distribution system operators and utility companies to collect high-resolution load data from their networks. These advancements enable the non-intrusive load monitoring (NILM) technique to detect EV charging using load measurement data. While existing studies primarily focused on NILM for EV charging detection in individual households, there is a research gap on EV charging detection at the feeder level, presenting unique challenges due to the combined load measurement from multiple households. In this paper, we develop a novel and effective approach for EV detection at the feeder level, involving sliding-window feature extraction and classical machine learning techniques, specifically models like XGBoost and Random Forest. Our developed method offers a lightweight and efficient solution, capable of quick training. Moreover, our developed method is versatile, supporting both offline and online EV charging detection. Our experimental results demonstrate high-accuracy EV charging detection at the feeder level, achieving an F-Score of 98.88% in offline detection and 93.01% in online detection.","sentences":["Understanding electric vehicle (EV) charging on the distribution network is key to effective EV charging management and aiding decarbonization across the energy and transport sectors.","Advanced metering infrastructure has allowed distribution system operators and utility companies to collect high-resolution load data from their networks.","These advancements enable the non-intrusive load monitoring (NILM) technique to detect EV charging using load measurement data.","While existing studies primarily focused on NILM for EV charging detection in individual households, there is a research gap on EV charging detection at the feeder level, presenting unique challenges due to the combined load measurement from multiple households.","In this paper, we develop a novel and effective approach for EV detection at the feeder level, involving sliding-window feature extraction and classical machine learning techniques, specifically models like XGBoost and Random Forest.","Our developed method offers a lightweight and efficient solution, capable of quick training.","Moreover, our developed method is versatile, supporting both offline and online EV charging detection.","Our experimental results demonstrate high-accuracy EV charging detection at the feeder level, achieving an F-Score of 98.88% in offline detection and 93.01% in online detection."],"url":"http://arxiv.org/abs/2312.01887v1"}
{"created":"2023-12-04 13:33:51","title":"Correlation and Unintended Biases on Univariate and Multivariate Decision Trees","abstract":"Decision Trees are accessible, interpretable, and well-performing classification models. A plethora of variants with increasing expressiveness has been proposed in the last forty years. We contrast the two families of univariate DTs, whose split functions partition data through axis-parallel hyperplanes, and multivariate DTs, whose splits instead partition data through oblique hyperplanes. The latter include the former, hence multivariate DTs are in principle more powerful. Surprisingly enough, however, univariate DTs consistently show comparable performances in the literature. We analyze the reasons behind this, both with synthetic and real-world benchmark datasets. Our research questions test whether the pre-processing phase of removing correlation among features in datasets has an impact on the relative performances of univariate vs multivariate DTs. We find that existing benchmark datasets are likely biased towards favoring univariate DTs.","sentences":["Decision Trees are accessible, interpretable, and well-performing classification models.","A plethora of variants with increasing expressiveness has been proposed in the last forty years.","We contrast the two families of univariate DTs, whose split functions partition data through axis-parallel hyperplanes, and multivariate DTs, whose splits instead partition data through oblique hyperplanes.","The latter include the former, hence multivariate DTs are in principle more powerful.","Surprisingly enough, however, univariate DTs consistently show comparable performances in the literature.","We analyze the reasons behind this, both with synthetic and real-world benchmark datasets.","Our research questions test whether the pre-processing phase of removing correlation among features in datasets has an impact on the relative performances of univariate vs multivariate DTs.","We find that existing benchmark datasets are likely biased towards favoring univariate DTs."],"url":"http://arxiv.org/abs/2312.01884v1"}
{"created":"2023-12-04 13:09:37","title":"The CURE To Vulnerabilities in RPKI Validation","abstract":"Over recent years, the Resource Public Key Infrastructure (RPKI) has seen increasing adoption, with now 37.8% of the major networks filtering bogus BGP routes. Systems interact with the RPKI over Relying Party (RP) implementations that fetch RPKI objects and feed BGP routers with the validated prefix-ownership data. Consequently, any vulnerabilities or flaws within the RP software can substantially threaten the stability and security of Internet routing. We uncover severe flaws in all popular RP implementations, making them susceptible to path traversal attacks, remotely triggered crashes, and inherent inconsistencies, violating RPKI standards. We report a total of 18 vulnerabilities that canbe exploited to downgrade RPKI validation in border routers or, worse, enable poisoning of the validation process, resulting in malicious prefixes being wrongfully validated and legitimate RPKI-covered prefixes failing validation. Furthermore, our research discloses inconsistencies in the validation process, with two popular implementations leaving 8149 prefixes unprotected from hijacks, 6405 of which belong to Amazon. While these findings are significant in their own right, our principal contribution lies in developing CURE, the first-of-its-kind system to systematically detect bugs, vulnerabilities, and RFC compliance issues in RP implementations via automated test generation. CURE is a powerful RPKI publication point emulator that enables easy and efficient fuzzing of complex RP validation pipelines. It is designed with a set of novel techniques, utilizing differential and stateful fuzzing. We generated over 600 million test cases and tested all popular RPs on them. Following our disclosure, the vendors already assigned CVEs to the vulnerabilities we found.","sentences":["Over recent years, the Resource Public Key Infrastructure (RPKI) has seen increasing adoption, with now 37.8% of the major networks filtering bogus BGP routes.","Systems interact with the RPKI over Relying Party (RP) implementations that fetch RPKI objects and feed BGP routers with the validated prefix-ownership data.","Consequently, any vulnerabilities or flaws within the RP software can substantially threaten the stability and security of Internet routing.","We uncover severe flaws in all popular RP implementations, making them susceptible to path traversal attacks, remotely triggered crashes, and inherent inconsistencies, violating RPKI standards.","We report a total of 18 vulnerabilities that canbe exploited to downgrade RPKI validation in border routers or, worse, enable poisoning of the validation process, resulting in malicious prefixes being wrongfully validated and legitimate RPKI-covered prefixes failing validation.","Furthermore, our research discloses inconsistencies in the validation process, with two popular implementations leaving 8149 prefixes unprotected from hijacks, 6405 of which belong to Amazon.","While these findings are significant in their own right, our principal contribution lies in developing CURE, the first-of-its-kind system to systematically detect bugs, vulnerabilities, and RFC compliance issues in RP implementations via automated test generation.","CURE is a powerful RPKI publication point emulator that enables easy and efficient fuzzing of complex RP validation pipelines.","It is designed with a set of novel techniques, utilizing differential and stateful fuzzing.","We generated over 600 million test cases and tested all popular RPs on them.","Following our disclosure, the vendors already assigned CVEs to the vulnerabilities we found."],"url":"http://arxiv.org/abs/2312.01872v1"}
{"created":"2023-12-04 13:06:09","title":"TCP Slice: A semi-distributed TCP algorithm for Delay-constrained Applications","abstract":"The TCP congestion control protocol serves as the cornerstone of reliable internet communication. However, as new applications require more specific guarantees regarding data rate and delay, network management must adapt. Thus, service providers are shifting from decentralized to centralized control of the network using a software-defined network controller (SDN). The SDN classifies applications and allocates logically separate resources called slices, over the physical network. We propose TCP Slice, a congestion control algorithm that meets specific delay and bandwidth guarantees. Obtaining closed-form delay bounds for a client is challenging due to dependencies on other clients and their traffic stochasticity. We use network calculus to derive the client's delay bound and incorporate it as a constraint in the Network Utility Maximization problem. We solve the resulting optimization using dual decomposition and obtain a semi-distributed TCP protocol that can be implemented with the help of SDN controller and the use of an Explicit Congestion Notification (ECN) bit. Additionally, we also propose a proactive approach for congestion control using digital twin. TCP Slice represents a significant step towards accommodating evolving internet traffic patterns and the need for better network management in the face of increasing application diversity.","sentences":["The TCP congestion control protocol serves as the cornerstone of reliable internet communication.","However, as new applications require more specific guarantees regarding data rate and delay, network management must adapt.","Thus, service providers are shifting from decentralized to centralized control of the network using a software-defined network controller (SDN).","The SDN classifies applications and allocates logically separate resources called slices, over the physical network.","We propose TCP Slice, a congestion control algorithm that meets specific delay and bandwidth guarantees.","Obtaining closed-form delay bounds for a client is challenging due to dependencies on other clients and their traffic stochasticity.","We use network calculus to derive the client's delay bound and incorporate it as a constraint in the Network Utility Maximization problem.","We solve the resulting optimization using dual decomposition and obtain a semi-distributed TCP protocol that can be implemented with the help of SDN controller and the use of an Explicit Congestion Notification (ECN) bit.","Additionally, we also propose a proactive approach for congestion control using digital twin.","TCP Slice represents a significant step towards accommodating evolving internet traffic patterns and the need for better network management in the face of increasing application diversity."],"url":"http://arxiv.org/abs/2312.01869v1"}
{"created":"2023-12-04 12:48:44","title":"Unveiling Objects with SOLA: An Annotation-Free Image Search on the Object Level for Automotive Data Sets","abstract":"Huge image data sets are the fundament for the development of the perception of automated driving systems. A large number of images is necessary to train robust neural networks that can cope with diverse situations. A sufficiently large data set contains challenging situations and objects. For testing the resulting functions, it is necessary that these situations and objects can be found and extracted from the data set. While it is relatively easy to record a large amount of unlabeled data, it is far more difficult to find demanding situations and objects. However, during the development of perception systems, it must be possible to access challenging data without having to perform lengthy and time-consuming annotations. A developer must therefore be able to search dynamically for specific situations and objects in a data set. Thus, we designed a method which is based on state-of-the-art neural networks to search for objects with certain properties within an image. For the ease of use, the query of this search is described using natural language. To determine the time savings and performance gains, we evaluated our method qualitatively and quantitatively on automotive data sets.","sentences":["Huge image data sets are the fundament for the development of the perception of automated driving systems.","A large number of images is necessary to train robust neural networks that can cope with diverse situations.","A sufficiently large data set contains challenging situations and objects.","For testing the resulting functions, it is necessary that these situations and objects can be found and extracted from the data set.","While it is relatively easy to record a large amount of unlabeled data, it is far more difficult to find demanding situations and objects.","However, during the development of perception systems, it must be possible to access challenging data without having to perform lengthy and time-consuming annotations.","A developer must therefore be able to search dynamically for specific situations and objects in a data set.","Thus, we designed a method which is based on state-of-the-art neural networks to search for objects with certain properties within an image.","For the ease of use, the query of this search is described using natural language.","To determine the time savings and performance gains, we evaluated our method qualitatively and quantitatively on automotive data sets."],"url":"http://arxiv.org/abs/2312.01860v1"}
{"created":"2023-12-04 12:44:22","title":"The Intractability of the Picker Routing Problem","abstract":"The Picker Routing Problem (PRP), which consists in finding a minimum-length tour between a set of storage locations in a warehouse, is one of the most important problems in the warehousing logistics literature. Despite its popularity, the tractability of the PRP in conventional multi-block warehouses remains an open question. This technical note aims to fill this research gap by establishing that the PRP is strongly NP-hard.","sentences":["The Picker Routing Problem (PRP), which consists in finding a minimum-length tour between a set of storage locations in a warehouse, is one of the most important problems in the warehousing logistics literature.","Despite its popularity, the tractability of the PRP in conventional multi-block warehouses remains an open question.","This technical note aims to fill this research gap by establishing that the PRP is strongly NP-hard."],"url":"http://arxiv.org/abs/2312.01857v1"}
{"created":"2023-12-04 12:31:45","title":"Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation","abstract":"When models, e.g., for semantic segmentation, are applied to images that are vastly different from training data, the performance will drop significantly. Domain adaptation methods try to overcome this issue, but need samples from the target domain. However, this might not always be feasible for various reasons and therefore domain generalization methods are useful as they do not require any target data. We present a new diffusion-based domain extension (DIDEX) method and employ a diffusion model to generate a pseudo-target domain with diverse text prompts. In contrast to existing methods, this allows to control the style and content of the generated images and to introduce a high diversity. In a second step, we train a generalizing model by adapting towards this pseudo-target domain. We outperform previous approaches by a large margin across various datasets and architectures without using any real data. For the generalization from GTA5, we improve state-of-the-art mIoU performance by 3.8% absolute on average and for SYNTHIA by 11.8% absolute, marking a big step for the generalization performance on these benchmarks. Code is available at https://github.com/JNiemeijer/DIDEX","sentences":["When models, e.g., for semantic segmentation, are applied to images that are vastly different from training data, the performance will drop significantly.","Domain adaptation methods try to overcome this issue, but need samples from the target domain.","However, this might not always be feasible for various reasons and therefore domain generalization methods are useful as they do not require any target data.","We present a new diffusion-based domain extension (DIDEX) method and employ a diffusion model to generate a pseudo-target domain with diverse text prompts.","In contrast to existing methods, this allows to control the style and content of the generated images and to introduce a high diversity.","In a second step, we train a generalizing model by adapting towards this pseudo-target domain.","We outperform previous approaches by a large margin across various datasets and architectures without using any real data.","For the generalization from GTA5, we improve state-of-the-art mIoU performance by 3.8% absolute on average and for SYNTHIA by 11.8% absolute, marking a big step for the generalization performance on these benchmarks.","Code is available at https://github.com/JNiemeijer/DIDEX"],"url":"http://arxiv.org/abs/2312.01850v1"}
{"created":"2023-12-04 12:25:46","title":"Exploring the Viability of Synthetic Audio Data for Audio-Based Dialogue State Tracking","abstract":"Dialogue state tracking plays a crucial role in extracting information in task-oriented dialogue systems. However, preceding research are limited to textual modalities, primarily due to the shortage of authentic human audio datasets. We address this by investigating synthetic audio data for audio-based DST. To this end, we develop cascading and end-to-end models, train them with our synthetic audio dataset, and test them on actual human speech data. To facilitate evaluation tailored to audio modalities, we introduce a novel PhonemeF1 to capture pronunciation similarity. Experimental results showed that models trained solely on synthetic datasets can generalize their performance to human voice data. By eliminating the dependency on human speech data collection, these insights pave the way for significant practical advancements in audio-based DST. Data and code are available at https://github.com/JihyunLee1/E2E-DST.","sentences":["Dialogue state tracking plays a crucial role in extracting information in task-oriented dialogue systems.","However, preceding research are limited to textual modalities, primarily due to the shortage of authentic human audio datasets.","We address this by investigating synthetic audio data for audio-based DST.","To this end, we develop cascading and end-to-end models, train them with our synthetic audio dataset, and test them on actual human speech data.","To facilitate evaluation tailored to audio modalities, we introduce a novel PhonemeF1 to capture pronunciation similarity.","Experimental results showed that models trained solely on synthetic datasets can generalize their performance to human voice data.","By eliminating the dependency on human speech data collection, these insights pave the way for significant practical advancements in audio-based DST.","Data and code are available at https://github.com/JihyunLee1/E2E-DST."],"url":"http://arxiv.org/abs/2312.01842v1"}
{"created":"2023-12-04 12:25:05","title":"An AI-based solution for the cold start and data sparsity problems in the recommendation systems","abstract":"In recent years, the amount of data available on the internet and the number of users who utilize the Internet have increased at an unparalleled pace. The exponential development in the quantity of digital information accessible and the number of Internet users has created the possibility for information overload, impeding fast access to items of interest on the Internet. Information retrieval systems like as Google, DevilFinder, and Altavista have partly overcome this challenge, but prioritizing and customization of information (where a system maps accessible material to a user's interests and preferences) were lacking. This has resulted in a higher-than-ever need for recommender systems. Recommender systems are information filtering systems that address the issue of information overload by filtering important information fragments from a huge volume of dynamically produced data based on the user's interests, favorite things, preferences and ratings on the desired item. Recommender systems can figure out if a person would like an item or not based on their profile.","sentences":["In recent years, the amount of data available on the internet and the number of users who utilize the Internet have increased at an unparalleled pace.","The exponential development in the quantity of digital information accessible and the number of Internet users has created the possibility for information overload, impeding fast access to items of interest on the Internet.","Information retrieval systems like as Google, DevilFinder, and Altavista have partly overcome this challenge, but prioritizing and customization of information (where a system maps accessible material to a user's interests and preferences) were lacking.","This has resulted in a higher-than-ever need for recommender systems.","Recommender systems are information filtering systems that address the issue of information overload by filtering important information fragments from a huge volume of dynamically produced data based on the user's interests, favorite things, preferences and ratings on the desired item.","Recommender systems can figure out if a person would like an item or not based on their profile."],"url":"http://arxiv.org/abs/2312.01840v1"}
{"created":"2023-12-04 12:20:25","title":"Prompting Disentangled Embeddings for Knowledge Graph Completion with Pre-trained Language Model","abstract":"Both graph structures and textual information play a critical role in Knowledge Graph Completion (KGC). With the success of Pre-trained Language Models (PLMs) such as BERT, they have been applied for text encoding for KGC. However, the current methods mostly prefer to fine-tune PLMs, leading to huge training costs and limited scalability to larger PLMs. In contrast, we propose to utilize prompts and perform KGC on a frozen PLM with only the prompts trained. Accordingly, we propose a new KGC method named PDKGC with two prompts -- a hard task prompt which is to adapt the KGC task to the PLM pre-training task of token prediction, and a disentangled structure prompt which learns disentangled graph representation so as to enable the PLM to combine more relevant structure knowledge with the text information. With the two prompts, PDKGC builds a textual predictor and a structural predictor, respectively, and their combination leads to more comprehensive entity prediction. Solid evaluation on two widely used KGC datasets has shown that PDKGC often outperforms the baselines including the state-of-the-art, and its components are all effective. Our codes and data are available at https://github.com/genggengcss/PDKGC.","sentences":["Both graph structures and textual information play a critical role in Knowledge Graph Completion (KGC).","With the success of Pre-trained Language Models (PLMs) such as BERT, they have been applied for text encoding for KGC.","However, the current methods mostly prefer to fine-tune PLMs, leading to huge training costs and limited scalability to larger PLMs.","In contrast, we propose to utilize prompts and perform KGC on a frozen PLM with only the prompts trained.","Accordingly, we propose a new KGC method named PDKGC with two prompts -- a hard task prompt which is to adapt the KGC task to the PLM pre-training task of token prediction, and a disentangled structure prompt which learns disentangled graph representation so as to enable the PLM to combine more relevant structure knowledge with the text information.","With the two prompts, PDKGC builds a textual predictor and a structural predictor, respectively, and their combination leads to more comprehensive entity prediction.","Solid evaluation on two widely used KGC datasets has shown that PDKGC often outperforms the baselines including the state-of-the-art, and its components are all effective.","Our codes and data are available at https://github.com/genggengcss/PDKGC."],"url":"http://arxiv.org/abs/2312.01837v1"}
{"created":"2023-12-04 12:16:02","title":"Few Clicks Suffice: Active Test-Time Adaptation for Semantic Segmentation","abstract":"Test-time adaptation (TTA) adapts the pre-trained models during inference using unlabeled test data and has received a lot of research attention due to its potential practical value. Unfortunately, without any label supervision, existing TTA methods rely heavily on heuristic or empirical studies. Where to update the model always falls into suboptimal or brings more computational resource consumption. Meanwhile, there is still a significant performance gap between the TTA approaches and their supervised counterparts. Motivated by active learning, in this work, we propose the active test-time adaptation for semantic segmentation setup. Specifically, we introduce the human-in-the-loop pattern during the testing phase, which queries very few labels to facilitate predictions and model updates in an online manner. To do so, we propose a simple but effective ATASeg framework, which consists of two parts, i.e., model adapter and label annotator. Extensive experiments demonstrate that ATASeg bridges the performance gap between TTA methods and their supervised counterparts with only extremely few annotations, even one click for labeling surpasses known SOTA TTA methods by 2.6% average mIoU on ACDC benchmark. Empirical results imply that progress in either the model adapter or the label annotator will bring improvements to the ATASeg framework, giving it large research and reality potential.","sentences":["Test-time adaptation (TTA) adapts the pre-trained models during inference using unlabeled test data and has received a lot of research attention due to its potential practical value.","Unfortunately, without any label supervision, existing TTA methods rely heavily on heuristic or empirical studies.","Where to update the model always falls into suboptimal or brings more computational resource consumption.","Meanwhile, there is still a significant performance gap between the TTA approaches and their supervised counterparts.","Motivated by active learning, in this work, we propose the active test-time adaptation for semantic segmentation setup.","Specifically, we introduce the human-in-the-loop pattern during the testing phase, which queries very few labels to facilitate predictions and model updates in an online manner.","To do so, we propose a simple but effective ATASeg framework, which consists of two parts, i.e., model adapter and label annotator.","Extensive experiments demonstrate that ATASeg bridges the performance gap between TTA methods and their supervised counterparts with only extremely few annotations, even one click for labeling surpasses known SOTA TTA methods by 2.6% average mIoU on ACDC benchmark.","Empirical results imply that progress in either the model adapter or the label annotator will bring improvements to the ATASeg framework, giving it large research and reality potential."],"url":"http://arxiv.org/abs/2312.01835v1"}
{"created":"2023-12-04 12:16:02","title":"Integrated Drill Boom Hole-Seeking Control via Reinforcement Learning","abstract":"Intelligent drill boom hole-seeking is a promising technology for enhancing drilling efficiency, mitigating potential safety hazards, and relieving human operators. Most existing intelligent drill boom control methods rely on a hierarchical control framework based on inverse kinematics. However, these methods are generally time-consuming due to the computational complexity of inverse kinematics and the inefficiency of the sequential execution of multiple joints. To tackle these challenges, this study proposes an integrated drill boom control method based on Reinforcement Learning (RL). We develop an integrated drill boom control framework that utilizes a parameterized policy to directly generate control inputs for all joints at each time step, taking advantage of joint posture and target hole information. By formulating the hole-seeking task as a Markov decision process, contemporary mainstream RL algorithms can be directly employed to learn a hole-seeking policy, thus eliminating the need for inverse kinematics solutions and promoting cooperative multi-joint control. To enhance the drilling accuracy throughout the entire drilling process, we devise a state representation that combines Denavit-Hartenberg joint information and preview hole-seeking discrepancy data. Simulation results show that the proposed method significantly outperforms traditional methods in terms of hole-seeking accuracy and time efficiency.","sentences":["Intelligent drill boom hole-seeking is a promising technology for enhancing drilling efficiency, mitigating potential safety hazards, and relieving human operators.","Most existing intelligent drill boom control methods rely on a hierarchical control framework based on inverse kinematics.","However, these methods are generally time-consuming due to the computational complexity of inverse kinematics and the inefficiency of the sequential execution of multiple joints.","To tackle these challenges, this study proposes an integrated drill boom control method based on Reinforcement Learning (RL).","We develop an integrated drill boom control framework that utilizes a parameterized policy to directly generate control inputs for all joints at each time step, taking advantage of joint posture and target hole information.","By formulating the hole-seeking task as a Markov decision process, contemporary mainstream RL algorithms can be directly employed to learn a hole-seeking policy, thus eliminating the need for inverse kinematics solutions and promoting cooperative multi-joint control.","To enhance the drilling accuracy throughout the entire drilling process, we devise a state representation that combines Denavit-Hartenberg joint information and preview hole-seeking discrepancy data.","Simulation results show that the proposed method significantly outperforms traditional methods in terms of hole-seeking accuracy and time efficiency."],"url":"http://arxiv.org/abs/2312.01836v1"}
{"created":"2023-12-04 11:45:44","title":"Class Symbolic Regression: Gotta Fit 'Em All","abstract":"We introduce \"Class Symbolic Regression\" a first framework for automatically finding a single analytical functional form that accurately fits multiple datasets - each governed by its own (possibly) unique set of fitting parameters. This hierarchical framework leverages the common constraint that all the members of a single class of physical phenomena follow a common governing law. Our approach extends the capabilities of our earlier Physical Symbolic Optimization ($\\Phi$-SO) framework for Symbolic Regression, which integrates dimensional analysis constraints and deep reinforcement learning for symbolic analytical function discovery from data. We demonstrate the efficacy of this novel approach by applying it to a panel of synthetic toy case datasets and showcase its practical utility for astrophysics by successfully extracting an analytic galaxy potential from a set of simulated orbits approximating stellar streams.","sentences":["We introduce \"Class Symbolic Regression\" a first framework for automatically finding a single analytical functional form that accurately fits multiple datasets - each governed by its own (possibly) unique set of fitting parameters.","This hierarchical framework leverages the common constraint that all the members of a single class of physical phenomena follow a common governing law.","Our approach extends the capabilities of our earlier Physical Symbolic Optimization ($\\Phi$-SO) framework for Symbolic Regression, which integrates dimensional analysis constraints and deep reinforcement learning for symbolic analytical function discovery from data.","We demonstrate the efficacy of this novel approach by applying it to a panel of synthetic toy case datasets and showcase its practical utility for astrophysics by successfully extracting an analytic galaxy potential from a set of simulated orbits approximating stellar streams."],"url":"http://arxiv.org/abs/2312.01816v1"}
{"created":"2023-12-04 10:37:58","title":"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics","abstract":"This research focuses on how Large Language Models (LLMs) can help with path planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A*, aims to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to facilitate few-shot near-optimal path planning. Prompts are used to 1) provide LLMs with essential information like environment, cost, heuristics, etc.; 2) communicate human feedback to LLMs on intermediate planning results. This makes the whole path planning process a `white box' and human feedback guides LLM A* to converge quickly compared to other data-driven methods such as reinforcement learning-based (RL) path planning. In addition, it makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques. Comparative analysis against A* and RL shows that LLM A* is more efficient in terms of search space and achieves an on-a-par path with A* and a better path than RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks.","sentences":["This research focuses on how Large Language Models (LLMs) can help with path planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner.","A novel framework named LLM A*, aims to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to facilitate few-shot near-optimal path planning.","Prompts are used to 1) provide LLMs with essential information like environment, cost, heuristics, etc.; 2) communicate human feedback to LLMs on intermediate planning results.","This makes the whole path planning process a `white box' and human feedback guides LLM A* to converge quickly compared to other data-driven methods such as reinforcement learning-based (RL) path planning.","In addition, it makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques.","Comparative analysis against A* and RL shows that LLM A* is more efficient in terms of search space and achieves an on-a-par path with A* and a better path than RL.","The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks."],"url":"http://arxiv.org/abs/2312.01797v1"}
{"created":"2023-12-04 10:35:46","title":"Distributed Continual Learning with CoCoA in High-dimensional Linear Regression","abstract":"We consider estimation under scenarios where the signals of interest exhibit change of characteristics over time. In particular, we consider the continual learning problem where different tasks, e.g., data with different distributions, arrive sequentially and the aim is to perform well on the newly arrived task without performance degradation on the previously seen tasks. In contrast to the continual learning literature focusing on the centralized setting, we investigate the problem from a distributed estimation perspective. We consider the well-established distributed learning algorithm COCOA, which distributes the model parameters and the corresponding features over the network. We provide exact analytical characterization for the generalization error of COCOA under continual learning for linear regression in a range of scenarios, where overparameterization is of particular interest. These analytical results characterize how the generalization error depends on the network structure, the task similarity and the number of tasks, and show how these dependencies are intertwined. In particular, our results show that the generalization error can be significantly reduced by adjusting the network size, where the most favorable network size depends on task similarity and the number of tasks. We present numerical results verifying the theoretical analysis and illustrate the continual learning performance of COCOA with a digit classification task.","sentences":["We consider estimation under scenarios where the signals of interest exhibit change of characteristics over time.","In particular, we consider the continual learning problem where different tasks, e.g., data with different distributions, arrive sequentially and the aim is to perform well on the newly arrived task without performance degradation on the previously seen tasks.","In contrast to the continual learning literature focusing on the centralized setting, we investigate the problem from a distributed estimation perspective.","We consider the well-established distributed learning algorithm COCOA, which distributes the model parameters and the corresponding features over the network.","We provide exact analytical characterization for the generalization error of COCOA under continual learning for linear regression in a range of scenarios, where overparameterization is of particular interest.","These analytical results characterize how the generalization error depends on the network structure, the task similarity and the number of tasks, and show how these dependencies are intertwined.","In particular, our results show that the generalization error can be significantly reduced by adjusting the network size, where the most favorable network size depends on task similarity and the number of tasks.","We present numerical results verifying the theoretical analysis and illustrate the continual learning performance of COCOA with a digit classification task."],"url":"http://arxiv.org/abs/2312.01795v1"}
{"created":"2023-12-04 10:27:38","title":"Wild-Tab: A Benchmark For Out-Of-Distribution Generalization In Tabular Regression","abstract":"Out-of-Distribution (OOD) generalization, a cornerstone for building robust machine learning models capable of handling data diverging from the training set's distribution, is an ongoing challenge in deep learning. While significant progress has been observed in computer vision and natural language processing, its exploration in tabular data, ubiquitous in many industrial applications, remains nascent. To bridge this gap, we present Wild-Tab, a large-scale benchmark tailored for OOD generalization in tabular regression tasks. The benchmark incorporates 3 industrial datasets sourced from fields like weather prediction and power consumption estimation, providing a challenging testbed for evaluating OOD performance under real-world conditions. Our extensive experiments, evaluating 10 distinct OOD generalization methods on Wild-Tab, reveal nuanced insights. We observe that many of these methods often struggle to maintain high-performance levels on unseen data, with OOD performance showing a marked drop compared to in-distribution performance. At the same time, Empirical Risk Minimization (ERM), despite its simplicity, delivers robust performance across all evaluations, rivaling the results of state-of-the-art methods. Looking forward, we hope that the release of Wild-Tab will facilitate further research on OOD generalization and aid in the deployment of machine learning models in various real-world contexts where handling distribution shifts is a crucial requirement.","sentences":["Out-of-Distribution (OOD) generalization, a cornerstone for building robust machine learning models capable of handling data diverging from the training set's distribution, is an ongoing challenge in deep learning.","While significant progress has been observed in computer vision and natural language processing, its exploration in tabular data, ubiquitous in many industrial applications, remains nascent.","To bridge this gap, we present Wild-Tab, a large-scale benchmark tailored for OOD generalization in tabular regression tasks.","The benchmark incorporates 3 industrial datasets sourced from fields like weather prediction and power consumption estimation, providing a challenging testbed for evaluating OOD performance under real-world conditions.","Our extensive experiments, evaluating 10 distinct OOD generalization methods on Wild-Tab, reveal nuanced insights.","We observe that many of these methods often struggle to maintain high-performance levels on unseen data, with OOD performance showing a marked drop compared to in-distribution performance.","At the same time, Empirical Risk Minimization (ERM), despite its simplicity, delivers robust performance across all evaluations, rivaling the results of state-of-the-art methods.","Looking forward, we hope that the release of Wild-Tab will facilitate further research on OOD generalization and aid in the deployment of machine learning models in various real-world contexts where handling distribution shifts is a crucial requirement."],"url":"http://arxiv.org/abs/2312.01792v1"}
{"created":"2023-12-04 10:20:36","title":"Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection","abstract":"With the proliferation of social media, there has been a sharp increase in offensive content, particularly targeting vulnerable groups, exacerbating social problems such as hatred, racism, and sexism. Detecting offensive language use is crucial to prevent offensive language from being widely shared on social media. However, the accurate detection of irony, implication, and various forms of hate speech on social media remains a challenge. Natural language-based deep learning models require extensive training with large, comprehensive, and labeled datasets. Unfortunately, manually creating such datasets is both costly and error-prone. Additionally, the presence of human-bias in offensive language datasets is a major concern for deep learning models. In this paper, we propose a linguistic data augmentation approach to reduce bias in labeling processes, which aims to mitigate the influence of human bias by leveraging the power of machines to improve the accuracy and fairness of labeling processes. This approach has the potential to improve offensive language classification tasks across multiple languages and reduce the prevalence of offensive content on social media.","sentences":["With the proliferation of social media, there has been a sharp increase in offensive content, particularly targeting vulnerable groups, exacerbating social problems such as hatred, racism, and sexism.","Detecting offensive language use is crucial to prevent offensive language from being widely shared on social media.","However, the accurate detection of irony, implication, and various forms of hate speech on social media remains a challenge.","Natural language-based deep learning models require extensive training with large, comprehensive, and labeled datasets.","Unfortunately, manually creating such datasets is both costly and error-prone.","Additionally, the presence of human-bias in offensive language datasets is a major concern for deep learning models.","In this paper, we propose a linguistic data augmentation approach to reduce bias in labeling processes, which aims to mitigate the influence of human bias by leveraging the power of machines to improve the accuracy and fairness of labeling processes.","This approach has the potential to improve offensive language classification tasks across multiple languages and reduce the prevalence of offensive content on social media."],"url":"http://arxiv.org/abs/2312.01787v1"}
{"created":"2023-12-04 10:19:59","title":"Output-sensitive Complexity of Multi-Objective Integer Network Flow Problems","abstract":"This paper addresses the output-sensitive complexity for linear multi-objective integer minimum cost flow (MOIMCF) problems and provides insights about the time complexity for enumerating all supported nondominated vectors. The paper shows that there can not exist an output-polynomial time algorithm for the enumeration of all supported nondominated vectors that determine the vectors in an ordered way in the outcome space unless NP = P. Moreover, novel methods for identifying supported nondominated vectors in bi-objective minimum cost flow (BOIMCF) problems are proposed, accompanied by a numerical comparison between decision- and objective-space methods. A novel, equivalent and more compact formulation of the minimum cost flow ILP formulation used in the e-constrained-scalarization approach is introduced, demonstrating enhanced efficiency in the numerical tests","sentences":["This paper addresses the output-sensitive complexity for linear multi-objective integer minimum cost flow (MOIMCF) problems and provides insights about the time complexity for enumerating all supported nondominated vectors.","The paper shows that there can not exist an output-polynomial time algorithm for the enumeration of all supported nondominated vectors that determine the vectors in an ordered way in the outcome space unless NP = P. Moreover, novel methods for identifying supported nondominated vectors in bi-objective minimum cost flow (BOIMCF) problems are proposed, accompanied by a numerical comparison between decision- and objective-space methods.","A novel, equivalent and more compact formulation of the minimum cost flow ILP formulation used in the e-constrained-scalarization approach is introduced, demonstrating enhanced efficiency in the numerical tests"],"url":"http://arxiv.org/abs/2312.01786v1"}
{"created":"2023-12-04 09:40:11","title":"Dynamic Erasing Network Based on Multi-Scale Temporal Features for Weakly Supervised Video Anomaly Detection","abstract":"The goal of weakly supervised video anomaly detection is to learn a detection model using only video-level labeled data. However, prior studies typically divide videos into fixed-length segments without considering the complexity or duration of anomalies. Moreover, these studies usually just detect the most abnormal segments, potentially overlooking the completeness of anomalies. To address these limitations, we propose a Dynamic Erasing Network (DE-Net) for weakly supervised video anomaly detection, which learns multi-scale temporal features. Specifically, to handle duration variations of abnormal events, we first propose a multi-scale temporal modeling module, capable of extracting features from segments of varying lengths and capturing both local and global visual information across different temporal scales. Then, we design a dynamic erasing strategy, which dynamically assesses the completeness of the detected anomalies and erases prominent abnormal segments in order to encourage the model to discover gentle abnormal segments in a video. The proposed method obtains favorable performance compared to several state-of-the-art approaches on three datasets: XD-Violence, TAD, and UCF-Crime. Code will be made available at https://github.com/ArielZc/DE-Net.","sentences":["The goal of weakly supervised video anomaly detection is to learn a detection model using only video-level labeled data.","However, prior studies typically divide videos into fixed-length segments without considering the complexity or duration of anomalies.","Moreover, these studies usually just detect the most abnormal segments, potentially overlooking the completeness of anomalies.","To address these limitations, we propose a Dynamic Erasing Network (DE-Net) for weakly supervised video anomaly detection, which learns multi-scale temporal features.","Specifically, to handle duration variations of abnormal events, we first propose a multi-scale temporal modeling module, capable of extracting features from segments of varying lengths and capturing both local and global visual information across different temporal scales.","Then, we design a dynamic erasing strategy, which dynamically assesses the completeness of the detected anomalies and erases prominent abnormal segments in order to encourage the model to discover gentle abnormal segments in a video.","The proposed method obtains favorable performance compared to several state-of-the-art approaches on three datasets: XD-Violence, TAD, and UCF-Crime.","Code will be made available at https://github.com/ArielZc/DE-Net."],"url":"http://arxiv.org/abs/2312.01764v1"}
{"created":"2023-12-04 09:36:21","title":"On Gradient Boosted Decision Trees and Neural Rankers: A Case-Study on Short-Video Recommendations at ShareChat","abstract":"Practitioners who wish to build real-world applications that rely on ranking models, need to decide which modelling paradigm to follow. This is not an easy choice to make, as the research literature on this topic has been shifting in recent years. In particular, whilst Gradient Boosted Decision Trees (GBDTs) have reigned supreme for more than a decade, the flexibility of neural networks has allowed them to catch up, and recent works report accuracy metrics that are on par. Nevertheless, practical systems require considerations beyond mere accuracy metrics to decide on a modelling approach.   This work describes our experiences in balancing some of the trade-offs that arise, presenting a case study on a short-video recommendation application. We highlight (1) neural networks' ability to handle large training data size, user- and item-embeddings allows for more accurate models than GBDTs in this setting, and (2) because GBDTs are less reliant on specialised hardware, they can provide an equally accurate model at a lower cost. We believe these findings are of relevance to researchers in both academia and industry, and hope they can inspire practitioners who need to make similar modelling choices in the future.","sentences":["Practitioners who wish to build real-world applications that rely on ranking models, need to decide which modelling paradigm to follow.","This is not an easy choice to make, as the research literature on this topic has been shifting in recent years.","In particular, whilst Gradient Boosted Decision Trees (GBDTs) have reigned supreme for more than a decade, the flexibility of neural networks has allowed them to catch up, and recent works report accuracy metrics that are on par.","Nevertheless, practical systems require considerations beyond mere accuracy metrics to decide on a modelling approach.   ","This work describes our experiences in balancing some of the trade-offs that arise, presenting a case study on a short-video recommendation application.","We highlight (1) neural networks' ability to handle large training data size, user- and item-embeddings allows for more accurate models than GBDTs in this setting, and (2) because GBDTs are less reliant on specialised hardware, they can provide an equally accurate model at a lower cost.","We believe these findings are of relevance to researchers in both academia and industry, and hope they can inspire practitioners who need to make similar modelling choices in the future."],"url":"http://arxiv.org/abs/2312.01760v1"}
{"created":"2023-12-04 09:36:11","title":"Faster Sublinear-Time Edit Distance","abstract":"We study the fundamental problem of approximating the edit distance of two strings. After an extensive line of research led to the development of a constant-factor approximation algorithm in almost-linear time, recent years have witnessed a notable shift in focus towards sublinear-time algorithms. Here, the task is typically formalized as the $(k, K)$-gap edit distance problem: Distinguish whether the edit distance of two strings is at most $k$ or more than $K$.   Surprisingly, it is still possible to compute meaningful approximations in this challenging regime. Nevertheless, in almost all previous work, truly sublinear running time of $O(n^{1-\\varepsilon})$ (for a constant $\\varepsilon > 0$) comes at the price of at least polynomial gap $K \\ge k \\cdot n^{\\Omega(\\varepsilon)}$. Only recently, [Bringmann, Cassis, Fischer, and Nakos; STOC'22] broke through this barrier and solved the sub-polynomial $(k, k^{1+o(1)})$-gap edit distance problem in time $O(n/k + k^{4+o(1)})$, which is truly sublinear if $n^{\\Omega(1)} \\le k \\le n^{\\frac14-\\Omega(1)}$.The $n/k$ term is inevitable (already for Hamming distance), but it remains an important task to optimize the $\\mathrm{poly}(k)$ term and, in general, solve the $(k, k^{1+o(1)})$-gap edit distance problem in sublinear-time for larger values of $k$.   In this work, we design an improved algorithm for the $(k, k^{1+o(1)})$-gap edit distance problem in sublinear time $O(n/k + k^{2+o(1)})$, yielding a significant quadratic speed-up over the previous $O(n/k + k^{4+o(1)})$-time algorithm. Notably, our algorithm is unconditionally almost-optimal (up to subpolynomial factors) in the regime where $k \\leq n^{\\frac13}$ and improves upon the state of the art for $k \\leq n^{\\frac12-o(1)}$.","sentences":["We study the fundamental problem of approximating the edit distance of two strings.","After an extensive line of research led to the development of a constant-factor approximation algorithm in almost-linear time, recent years have witnessed a notable shift in focus towards sublinear-time algorithms.","Here, the task is typically formalized as the $(k, K)$-gap edit distance problem: Distinguish whether the edit distance of two strings is at most $k$ or more than $K$.   Surprisingly, it is still possible to compute meaningful approximations in this challenging regime.","Nevertheless, in almost all previous work, truly sublinear running time of $O(n^{1-\\varepsilon})$ (for a constant $\\varepsilon > 0$) comes at the price of at least polynomial gap $K \\ge k \\cdot n^{\\Omega(\\varepsilon)}$. Only recently, [Bringmann, Cassis, Fischer, and Nakos; STOC'22] broke through this barrier and solved the sub-polynomial $(k, k^{1+o(1)})$-gap edit distance problem in time $O(n/k + k^{4+o(1)})$, which is truly sublinear if $n^{\\Omega(1)} \\le k \\le n^{\\frac14-\\Omega(1)}$.The $n/k$ term is inevitable (already for Hamming distance), but it remains an important task to optimize the $\\mathrm{poly}(k)$ term and, in general, solve the $(k, k^{1+o(1)})$-gap edit distance problem in sublinear-time for larger values of $k$.   ","In this work, we design an improved algorithm for the $(k, k^{1+o(1)})$-gap edit distance problem in sublinear time $O(n/k + k^{2+o(1)})$, yielding a significant quadratic speed-up over the previous $O(n/k + k^{4+o(1)})$-time algorithm.","Notably, our algorithm is unconditionally almost-optimal (up to subpolynomial factors) in the regime where $k \\leq n^{\\frac13}$ and improves upon the state of the art for $k \\leq n^{\\frac12-o(1)}$."],"url":"http://arxiv.org/abs/2312.01759v1"}
{"created":"2023-12-04 09:35:36","title":"CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation","abstract":"Zero-shot age estimation aims to learn feature information about age from input images and make inferences about a given person's image or video frame without specific sample data. The development of zero-shot age estimation can improve the efficiency and accuracy of various applications (e.g., age verification and secure access control, etc.), while also promoting research on multi-modal and zero-shot learning in the social media field. For example, zero-sample age estimation can be used to create social networks focused on specific age groups. However, existing methods mainly focus on supervised, labeled age estimation learning, and the prediction effect of zero-shot learning is very poor. To tackle the above issues, we propose a novel CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation (CZL-CIAE). Specifically, we first introduce the CLIP model to extract image features and text semantic information respectively, and map them into a highly semantically aligned high-dimensional feature space. Next, we designed a new Transformer architecture (i.e., FourierFormer) to achieve channel evolution and spatial interaction of images, and to fuse image and text semantic information. Finally, we introduce reversible age estimation, which uses end-to-end error feedback to reduce the error rate of age predictions. Through extensive experiments on multiple data sets, CZL-CIAE has achieved better age prediction results.","sentences":["Zero-shot age estimation aims to learn feature information about age from input images and make inferences about a given person's image or video frame without specific sample data.","The development of zero-shot age estimation can improve the efficiency and accuracy of various applications (e.g., age verification and secure access control, etc.), while also promoting research on multi-modal and zero-shot learning in the social media field.","For example, zero-sample age estimation can be used to create social networks focused on specific age groups.","However, existing methods mainly focus on supervised, labeled age estimation learning, and the prediction effect of zero-shot learning is very poor.","To tackle the above issues, we propose a novel CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation (CZL-CIAE).","Specifically, we first introduce the CLIP model to extract image features and text semantic information respectively, and map them into a highly semantically aligned high-dimensional feature space.","Next, we designed a new Transformer architecture (i.e., FourierFormer) to achieve channel evolution and spatial interaction of images, and to fuse image and text semantic information.","Finally, we introduce reversible age estimation, which uses end-to-end error feedback to reduce the error rate of age predictions.","Through extensive experiments on multiple data sets, CZL-CIAE has achieved better age prediction results."],"url":"http://arxiv.org/abs/2312.01758v1"}
{"created":"2023-12-04 09:25:54","title":"Cybersecurity threats in FinTech: A systematic review","abstract":"The rapid evolution of the Smart-everything movement and Artificial Intelligence (AI) advancements have given rise to sophisticated cyber threats that traditional methods cannot counteract. Cyber threats are extremely critical in financial technology (FinTech) as a data-centric sector expected to provide 24/7 services. This paper introduces a novel and refined taxonomy of security threats in FinTech and conducts a comprehensive systematic review of defensive strategies. Through PRISMA methodology applied to 74 selected studies and topic modeling, we identified 11 central cyber threats, with 43 papers detailing them, and pinpointed 9 corresponding defense strategies, as covered in 31 papers. This in-depth analysis offers invaluable insights for stakeholders ranging from banks and enterprises to global governmental bodies, highlighting both the current challenges in FinTech and effective countermeasures, as well as directions for future research.","sentences":["The rapid evolution of the Smart-everything movement and Artificial Intelligence (AI) advancements have given rise to sophisticated cyber threats that traditional methods cannot counteract.","Cyber threats are extremely critical in financial technology (FinTech) as a data-centric sector expected to provide 24/7 services.","This paper introduces a novel and refined taxonomy of security threats in FinTech and conducts a comprehensive systematic review of defensive strategies.","Through PRISMA methodology applied to 74 selected studies and topic modeling, we identified 11 central cyber threats, with 43 papers detailing them, and pinpointed 9 corresponding defense strategies, as covered in 31 papers.","This in-depth analysis offers invaluable insights for stakeholders ranging from banks and enterprises to global governmental bodies, highlighting both the current challenges in FinTech and effective countermeasures, as well as directions for future research."],"url":"http://arxiv.org/abs/2312.01752v1"}
{"created":"2023-12-04 09:11:06","title":"Deep CNN for Coherent Seismic Noise Removal: A Perspective","abstract":"Seismic denoising is an important processing step before subsequent imaging and interpretation, which consumes a significant amount of time, whether it is for Quality control or for the associated computations. We present results of our work in training convolutional neural networks for denoising seismic data, specifically attenuation of surface related multiples and removal of overlap of shot energies during simultaneous-shooting survey. The proposed methodology is being explored not only for its ability to minimize human involvement but also because of the trained filter's ability to accelerate the process, hence, reduce processing time.","sentences":["Seismic denoising is an important processing step before subsequent imaging and interpretation, which consumes a significant amount of time, whether it is for Quality control or for the associated computations.","We present results of our work in training convolutional neural networks for denoising seismic data, specifically attenuation of surface related multiples and removal of overlap of shot energies during simultaneous-shooting survey.","The proposed methodology is being explored not only for its ability to minimize human involvement but also because of the trained filter's ability to accelerate the process, hence, reduce processing time."],"url":"http://arxiv.org/abs/2312.01748v1"}
{"created":"2023-12-04 09:10:25","title":"Open-DDVM: A Reproduction and Extension of Diffusion Model for Optical Flow Estimation","abstract":"Recently, Google proposes DDVM which for the first time demonstrates that a general diffusion model for image-to-image translation task works impressively well on optical flow estimation task without any specific designs like RAFT. However, DDVM is still a closed-source model with the expensive and private Palette-style pretraining. In this technical report, we present the first open-source DDVM by reproducing it. We study several design choices and find those important ones. By training on 40k public data with 4 GPUs, our reproduction achieves comparable performance to the closed-source DDVM. The code and model have been released in https://github.com/DQiaole/FlowDiffusion_pytorch.","sentences":["Recently, Google proposes DDVM which for the first time demonstrates that a general diffusion model for image-to-image translation task works impressively well on optical flow estimation task without any specific designs like RAFT.","However, DDVM is still a closed-source model with the expensive and private Palette-style pretraining.","In this technical report, we present the first open-source DDVM by reproducing it.","We study several design choices and find those important ones.","By training on 40k public data with 4 GPUs, our reproduction achieves comparable performance to the closed-source DDVM.","The code and model have been released in https://github.com/DQiaole/FlowDiffusion_pytorch."],"url":"http://arxiv.org/abs/2312.01746v1"}
{"created":"2023-12-04 09:06:41","title":"SRSNetwork: Siamese Reconstruction-Segmentation Networks based on Dynamic-Parameter Convolution","abstract":"In this paper, we present a high-performance deep neural network for weak target image segmentation, including medical image segmentation and infrared image segmentation. To this end, this work analyzes the existing dynamic convolutions and proposes dynamic parameter convolution (DPConv). Furthermore, it reevaluates the relationship between reconstruction tasks and segmentation tasks from the perspective of DPConv, leading to the proposal of a dual-network model called the Siamese Reconstruction-Segmentation Network (SRSNet). The proposed model is not only a universal network but also enhances the segmentation performance without altering its structure, leveraging the reconstruction task. Additionally, as the amount of training data for the reconstruction network increases, the performance of the segmentation network also improves synchronously. On seven datasets including five medical datasets and two infrared image datasets, our SRSNet consistently achieves the best segmentation results. The code is released at https://github.com/fidshu/SRSNet.","sentences":["In this paper, we present a high-performance deep neural network for weak target image segmentation, including medical image segmentation and infrared image segmentation.","To this end, this work analyzes the existing dynamic convolutions and proposes dynamic parameter convolution (DPConv).","Furthermore, it reevaluates the relationship between reconstruction tasks and segmentation tasks from the perspective of DPConv, leading to the proposal of a dual-network model called the Siamese Reconstruction-Segmentation Network (SRSNet).","The proposed model is not only a universal network but also enhances the segmentation performance without altering its structure, leveraging the reconstruction task.","Additionally, as the amount of training data for the reconstruction network increases, the performance of the segmentation network also improves synchronously.","On seven datasets including five medical datasets and two infrared image datasets, our SRSNet consistently achieves the best segmentation results.","The code is released at https://github.com/fidshu/SRSNet."],"url":"http://arxiv.org/abs/2312.01741v1"}
{"created":"2023-12-04 09:03:06","title":"Divide-and-Conquer Strategy for Large-Scale Dynamic Bayesian Network Structure Learning","abstract":"Dynamic Bayesian Networks (DBNs), renowned for their interpretability, have become increasingly vital in representing complex stochastic processes in various domains such as gene expression analysis, healthcare, and traffic prediction. Structure learning of DBNs from data is challenging, particularly for datasets with thousands of variables. Most current algorithms for DBN structure learning are adaptations from those used in static Bayesian Networks (BNs), and are typically focused on small-scale problems. In order to solve large-scale problems while taking full advantage of existing algorithms, this paper introduces a novel divide-and-conquer strategy, originally developed for static BNs, and adapts it for large-scale DBN structure learning. In this work, we specifically concentrate on 2 Time-sliced Bayesian Networks (2-TBNs), a special class of DBNs. Furthermore, we leverage the prior knowledge of 2-TBNs to enhance the performance of the strategy we introduce. Our approach significantly improves the scalability and accuracy of 2-TBN structure learning. Experimental results demonstrate the effectiveness of our method, showing substantial improvements over existing algorithms in both computational efficiency and structure learning accuracy. On problem instances with more than 1,000 variables, our approach improves two accuracy metrics by 74.45% and 110.94% on average , respectively, while reducing runtime by 93.65% on average.","sentences":["Dynamic Bayesian Networks (DBNs), renowned for their interpretability, have become increasingly vital in representing complex stochastic processes in various domains such as gene expression analysis, healthcare, and traffic prediction.","Structure learning of DBNs from data is challenging, particularly for datasets with thousands of variables.","Most current algorithms for DBN structure learning are adaptations from those used in static Bayesian Networks (BNs), and are typically focused on small-scale problems.","In order to solve large-scale problems while taking full advantage of existing algorithms, this paper introduces a novel divide-and-conquer strategy, originally developed for static BNs, and adapts it for large-scale DBN structure learning.","In this work, we specifically concentrate on 2 Time-sliced Bayesian Networks (2-TBNs), a special class of DBNs.","Furthermore, we leverage the prior knowledge of 2-TBNs to enhance the performance of the strategy we introduce.","Our approach significantly improves the scalability and accuracy of 2-TBN structure learning.","Experimental results demonstrate the effectiveness of our method, showing substantial improvements over existing algorithms in both computational efficiency and structure learning accuracy.","On problem instances with more than 1,000 variables, our approach improves two accuracy metrics by 74.45% and 110.94% on average , respectively, while reducing runtime by 93.65% on average."],"url":"http://arxiv.org/abs/2312.01739v1"}
{"created":"2023-12-04 09:02:17","title":"Generalizing Political Leaning Inference to Multi-Party Systems: Insights from the UK Political Landscape","abstract":"An ability to infer the political leaning of social media users can help in gathering opinion polls thereby leading to a better understanding of public opinion. While there has been a body of research attempting to infer the political leaning of social media users, this has been typically simplified as a binary classification problem (e.g. left vs right) and has been limited to a single location, leading to a dearth of investigation into more complex, multiclass classification and its generalizability to different locations, particularly those with multi-party systems. Our work performs the first such effort by studying political leaning inference in three of the UK's nations (Scotland, Wales and Northern Ireland), each of which has a different political landscape composed of multiple parties. To do so, we collect and release a dataset comprising users labelled by their political leaning as well as interactions with one another. We investigate the ability to predict the political leaning of users by leveraging these interactions in challenging scenarios such as few-shot learning, where training data is scarce, as well as assessing the applicability to users with different levels of political engagement. We show that interactions in the form of retweets between users can be a very powerful feature to enable political leaning inference, leading to consistent and robust results across different regions with multi-party systems. However, we also see that there is room for improvement in predicting the political leaning of users who are less engaged in politics.","sentences":["An ability to infer the political leaning of social media users can help in gathering opinion polls thereby leading to a better understanding of public opinion.","While there has been a body of research attempting to infer the political leaning of social media users, this has been typically simplified as a binary classification problem (e.g. left vs right) and has been limited to a single location, leading to a dearth of investigation into more complex, multiclass classification and its generalizability to different locations, particularly those with multi-party systems.","Our work performs the first such effort by studying political leaning inference in three of the UK's nations (Scotland, Wales and Northern Ireland), each of which has a different political landscape composed of multiple parties.","To do so, we collect and release a dataset comprising users labelled by their political leaning as well as interactions with one another.","We investigate the ability to predict the political leaning of users by leveraging these interactions in challenging scenarios such as few-shot learning, where training data is scarce, as well as assessing the applicability to users with different levels of political engagement.","We show that interactions in the form of retweets between users can be a very powerful feature to enable political leaning inference, leading to consistent and robust results across different regions with multi-party systems.","However, we also see that there is room for improvement in predicting the political leaning of users who are less engaged in politics."],"url":"http://arxiv.org/abs/2312.01738v1"}
{"created":"2023-12-04 08:38:54","title":"EdgeConvFormer: Dynamic Graph CNN and Transformer based Anomaly Detection in Multivariate Time Series","abstract":"Transformer-based models for anomaly detection in multivariate time series can benefit from the self-attention mechanism due to its advantage in modeling long-term dependencies. However, Transformer-based anomaly detection models have problems such as a large amount of data being required for training, standard positional encoding is not suitable for multivariate time series data, and the interdependence between time series is not considered. To address these limitations, we propose a novel anomaly detection method, named EdgeConvFormer, which integrates Time2vec embedding, stacked dynamic graph CNN, and Transformer to extract global and local spatial-time information. This design of EdgeConvFormer empowers it with decomposition capacities for complex time series, progressive spatiotemporal correlation discovery between time series, and representation aggregation of multi-scale features. Experiments demonstrate that EdgeConvFormer can learn the spatial-temporal correlations from multivariate time series data and achieve better anomaly detection performance than the state-of-the-art approaches on many real-world datasets of different scales.","sentences":["Transformer-based models for anomaly detection in multivariate time series can benefit from the self-attention mechanism due to its advantage in modeling long-term dependencies.","However, Transformer-based anomaly detection models have problems such as a large amount of data being required for training, standard positional encoding is not suitable for multivariate time series data, and the interdependence between time series is not considered.","To address these limitations, we propose a novel anomaly detection method, named EdgeConvFormer, which integrates Time2vec embedding, stacked dynamic graph CNN, and Transformer to extract global and local spatial-time information.","This design of EdgeConvFormer empowers it with decomposition capacities for complex time series, progressive spatiotemporal correlation discovery between time series, and representation aggregation of multi-scale features.","Experiments demonstrate that EdgeConvFormer can learn the spatial-temporal correlations from multivariate time series data and achieve better anomaly detection performance than the state-of-the-art approaches on many real-world datasets of different scales."],"url":"http://arxiv.org/abs/2312.01729v1"}
{"created":"2023-12-04 08:35:31","title":"ImputeFormer: Graph Transformers for Generalizable Spatiotemporal Imputation","abstract":"This paper focuses on the multivariate time series imputation problem using deep neural architectures. The ubiquitous issue of missing data in both scientific and engineering tasks necessitates the development of an effective and general imputation model. Leveraging the wisdom and expertise garnered from low-rank imputation methods, we power the canonical Transformers with three key knowledge-driven enhancements, including projected temporal attention, global adaptive graph convolution, and Fourier imputation loss. These task-agnostic inductive biases exploit the inherent structures of incomplete time series, and thus make our model versatile for a variety of imputation problems. We demonstrate its superiority in terms of accuracy, efficiency, and flexibility on heterogeneous datasets, including traffic speed, traffic volume, solar energy, smart metering, and air quality. Comprehensive case studies are performed to further strengthen the interpretability. Promising empirical results provide strong conviction that incorporating time series primitives, such as low-rank properties, can substantially facilitate the development of a generalizable model to approach a wide range of spatiotemporal imputation problems.","sentences":["This paper focuses on the multivariate time series imputation problem using deep neural architectures.","The ubiquitous issue of missing data in both scientific and engineering tasks necessitates the development of an effective and general imputation model.","Leveraging the wisdom and expertise garnered from low-rank imputation methods, we power the canonical Transformers with three key knowledge-driven enhancements, including projected temporal attention, global adaptive graph convolution, and Fourier imputation loss.","These task-agnostic inductive biases exploit the inherent structures of incomplete time series, and thus make our model versatile for a variety of imputation problems.","We demonstrate its superiority in terms of accuracy, efficiency, and flexibility on heterogeneous datasets, including traffic speed, traffic volume, solar energy, smart metering, and air quality.","Comprehensive case studies are performed to further strengthen the interpretability.","Promising empirical results provide strong conviction that incorporating time series primitives, such as low-rank properties, can substantially facilitate the development of a generalizable model to approach a wide range of spatiotemporal imputation problems."],"url":"http://arxiv.org/abs/2312.01728v1"}
{"created":"2023-12-04 07:43:02","title":"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites","abstract":"Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.","sentences":["Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks.","To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced.","However, LVLMs may suffer from different types of object hallucinations.","Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image).","The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods.","In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs.","We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions.","We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}).","Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality.","The code can be found at https://github.com/Anonymousanoy/FOHE."],"url":"http://arxiv.org/abs/2312.01701v1"}
{"created":"2023-12-04 07:42:16","title":"Data Management For Large Language Models: A Survey","abstract":"Data plays a fundamental role in the training of Large Language Models (LLMs). Effective data management, particularly in the formulation of a well-suited training dataset, holds significance for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning phases. Despite the considerable importance of data management, the current research community still falls short in providing a systematic analysis of the rationale behind management strategy selection, its consequential effects, methodologies for evaluating curated datasets, and the ongoing pursuit of improved strategies. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey provides a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various noteworthy aspects of data management strategy design: data quantity, data quality, domain/task composition, etc. Looking toward the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through effective data management practices. The collection of the latest papers is available at https://github.com/ZigeW/data_management_LLM.","sentences":["Data plays a fundamental role in the training of Large Language Models (LLMs).","Effective data management, particularly in the formulation of a well-suited training dataset, holds significance for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning phases.","Despite the considerable importance of data management, the current research community still falls short in providing a systematic analysis of the rationale behind management strategy selection, its consequential effects, methodologies for evaluating curated datasets, and the ongoing pursuit of improved strategies.","Consequently, the exploration of data management has attracted more and more attention among the research community.","This survey provides a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various noteworthy aspects of data management strategy design: data quantity, data quality, domain/task composition, etc.","Looking toward the future, we extrapolate existing challenges and outline promising directions for development in this field.","Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through effective data management practices.","The collection of the latest papers is available at https://github.com/ZigeW/data_management_LLM."],"url":"http://arxiv.org/abs/2312.01700v1"}
{"created":"2023-12-04 07:39:05","title":"Rethinking Urban Mobility Prediction: A Super-Multivariate Time Series Forecasting Approach","abstract":"Long-term urban mobility predictions play a crucial role in the effective management of urban facilities and services. Conventionally, urban mobility data has been structured as spatiotemporal videos, treating longitude and latitude grids as fundamental pixels. Consequently, video prediction methods, relying on Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), have been instrumental in this domain. In our research, we introduce a fresh perspective on urban mobility prediction. Instead of oversimplifying urban mobility data as traditional video data, we regard it as a complex multivariate time series. This perspective involves treating the time-varying values of each grid in each channel as individual time series, necessitating a thorough examination of temporal dynamics, cross-variable correlations, and frequency-domain insights for precise and reliable predictions. To address this challenge, we present the Super-Multivariate Urban Mobility Transformer (SUMformer), which utilizes a specially designed attention mechanism to calculate temporal and cross-variable correlations and reduce computational costs stemming from a large number of time series. SUMformer also employs low-frequency filters to extract essential information for long-term predictions. Furthermore, SUMformer is structured with a temporal patch merge mechanism, forming a hierarchical framework that enables the capture of multi-scale correlations. Consequently, it excels in urban mobility pattern modeling and long-term prediction, outperforming current state-of-the-art methods across three real-world datasets.","sentences":["Long-term urban mobility predictions play a crucial role in the effective management of urban facilities and services.","Conventionally, urban mobility data has been structured as spatiotemporal videos, treating longitude and latitude grids as fundamental pixels.","Consequently, video prediction methods, relying on Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), have been instrumental in this domain.","In our research, we introduce a fresh perspective on urban mobility prediction.","Instead of oversimplifying urban mobility data as traditional video data, we regard it as a complex multivariate time series.","This perspective involves treating the time-varying values of each grid in each channel as individual time series, necessitating a thorough examination of temporal dynamics, cross-variable correlations, and frequency-domain insights for precise and reliable predictions.","To address this challenge, we present the Super-Multivariate Urban Mobility Transformer (SUMformer), which utilizes a specially designed attention mechanism to calculate temporal and cross-variable correlations and reduce computational costs stemming from a large number of time series.","SUMformer also employs low-frequency filters to extract essential information for long-term predictions.","Furthermore, SUMformer is structured with a temporal patch merge mechanism, forming a hierarchical framework that enables the capture of multi-scale correlations.","Consequently, it excels in urban mobility pattern modeling and long-term prediction, outperforming current state-of-the-art methods across three real-world datasets."],"url":"http://arxiv.org/abs/2312.01699v1"}
{"created":"2023-12-04 07:23:21","title":"Tab-Attention: Self-Attention-based Stacked Generalization for Imbalanced Credit Default Prediction","abstract":"Accurately credit default prediction faces challenges due to imbalanced data and low correlation between features and labels. Existing default prediction studies on the basis of gradient boosting decision trees (GBDT), deep learning techniques, and feature selection strategies can have varying degrees of success depending on the specific task. Motivated by this, we propose Tab-Attention, a novel self-attention-based stacked generalization method for credit default prediction. This approach ensembles the potential proprietary knowledge contributions from multi-view feature spaces, to cope with low feature correlation and imbalance. We organize multi-view feature spaces according to the latent linear or nonlinear strengths between features and labels. Meanwhile, the f1 score assists the model in imbalance training to find the optimal state for identifying minority default samples. Our Tab-Attention achieves superior Recall_1 and f1_1 of default intention recognition than existing GBDT-based models and advanced deep learning by about 32.92% and 16.05% on average, respectively, while maintaining outstanding overall performance and prediction performance for non-default samples. The proposed method could ensemble essential knowledge through the self-attention mechanism, which is of great significance for a more robust future prediction system.","sentences":["Accurately credit default prediction faces challenges due to imbalanced data and low correlation between features and labels.","Existing default prediction studies on the basis of gradient boosting decision trees (GBDT), deep learning techniques, and feature selection strategies can have varying degrees of success depending on the specific task.","Motivated by this, we propose Tab-Attention, a novel self-attention-based stacked generalization method for credit default prediction.","This approach ensembles the potential proprietary knowledge contributions from multi-view feature spaces, to cope with low feature correlation and imbalance.","We organize multi-view feature spaces according to the latent linear or nonlinear strengths between features and labels.","Meanwhile, the f1 score assists the model in imbalance training to find the optimal state for identifying minority default samples.","Our Tab-Attention achieves superior Recall_1 and f1_1 of default intention recognition than existing GBDT-based models and advanced deep learning by about 32.92% and 16.05% on average, respectively, while maintaining outstanding overall performance and prediction performance for non-default samples.","The proposed method could ensemble essential knowledge through the self-attention mechanism, which is of great significance for a more robust future prediction system."],"url":"http://arxiv.org/abs/2312.01688v1"}
{"created":"2023-12-04 07:21:27","title":"Optimizing Bus Travel: A Novel Approach to Feature Mining with P-KMEANS and P-LDA Algorithms","abstract":"Customizing services for bus travel can bolster its attractiveness, optimize usage, alleviate traffic congestion, and diminish carbon emissions. This potential is realized by harnessing recent advancements in positioning communication facilities, the Internet of Things, and artificial intelligence for feature mining in public transportation. However, the inherent complexities of disorganized and unstructured public transportation data introduce substantial challenges to travel feature extraction. This study presents a bus travel feature extraction method rooted in Point of Interest (POI) data, employing enhanced P-KMENAS and P-LDA algorithms to overcome these limitations. While the KMEANS algorithm adeptly segments passenger travel paths into distinct clusters, its outcomes can be influenced by the initial K value. On the other hand, Latent Dirichlet Allocation (LDA) excels at feature identification and probabilistic interpretations yet encounters difficulties with feature intermingling and nuanced sub-feature interactions. Incorporating the POI dimension enhances our understanding of travel behavior, aligning it more closely with passenger attributes and facilitating easier data analysis. By incorporating POI data, our refined P-KMENAS and P-LDA algorithms grant a holistic insight into travel behaviors and attributes, effectively mitigating the limitations above. Consequently, this POI-centric algorithm effectively amalgamates diverse POI attributes, delineates varied travel contexts, and imparts probabilistic metrics to feature properties. Our method successfully mines the diverse aspects of bus travel, such as age, occupation, gender, sports, cost, safety, and personality traits. It effectively calculates relationships between individual travel behaviors and assigns explanatory and evaluative probabilities to POI labels, thereby enhancing bus travel optimization.","sentences":["Customizing services for bus travel can bolster its attractiveness, optimize usage, alleviate traffic congestion, and diminish carbon emissions.","This potential is realized by harnessing recent advancements in positioning communication facilities, the Internet of Things, and artificial intelligence for feature mining in public transportation.","However, the inherent complexities of disorganized and unstructured public transportation data introduce substantial challenges to travel feature extraction.","This study presents a bus travel feature extraction method rooted in Point of Interest (POI) data, employing enhanced P-KMENAS and P-LDA algorithms to overcome these limitations.","While the KMEANS algorithm adeptly segments passenger travel paths into distinct clusters, its outcomes can be influenced by the initial K value.","On the other hand, Latent Dirichlet Allocation (LDA) excels at feature identification and probabilistic interpretations yet encounters difficulties with feature intermingling and nuanced sub-feature interactions.","Incorporating the POI dimension enhances our understanding of travel behavior, aligning it more closely with passenger attributes and facilitating easier data analysis.","By incorporating POI data, our refined P-KMENAS and P-LDA algorithms grant a holistic insight into travel behaviors and attributes, effectively mitigating the limitations above.","Consequently, this POI-centric algorithm effectively amalgamates diverse POI attributes, delineates varied travel contexts, and imparts probabilistic metrics to feature properties.","Our method successfully mines the diverse aspects of bus travel, such as age, occupation, gender, sports, cost, safety, and personality traits.","It effectively calculates relationships between individual travel behaviors and assigns explanatory and evaluative probabilities to POI labels, thereby enhancing bus travel optimization."],"url":"http://arxiv.org/abs/2312.01687v1"}
{"created":"2023-12-04 07:06:02","title":"With Great Humor Comes Great Developer Engagement","abstract":"The worldwide collaborative effort for the creation of software is technically and socially demanding. The more engaged developers are, the more value they impart to the software they create. Engaged developers, such as Margaret Hamilton programming Apollo 11, can succeed in tackling the most difficult engineering tasks. In this paper, we dive deep into an original vector of engagement - humor - and study how it fuels developer engagement. First, we collect qualitative and quantitative data about the humorous elements present within three significant, real-world software projects: faker, which helps developers introduce humor within their tests; lolcommits, which captures a photograph after each contribution made by a developer; and volkswagen, an exercise in satire, which accidentally led to the invention of an impactful software tool. Second, through a developer survey, we receive unique insights from 125 developers, who share their real-life experiences with humor in software. Our analysis of the three case studies highlights the prevalence of humor in software, and unveils the worldwide community of developers who are enthusiastic about both software and humor. We also learn about the caveats of humor in software through the valuable insights shared by our survey respondents. We report clear evidence that, when practiced responsibly, humor increases developer engagement and supports them in addressing hard engineering and cognitive tasks. The most actionable highlight of our work is that software tests and documentation are the best locations in code to practice humor.","sentences":["The worldwide collaborative effort for the creation of software is technically and socially demanding.","The more engaged developers are, the more value they impart to the software they create.","Engaged developers, such as Margaret Hamilton programming Apollo 11, can succeed in tackling the most difficult engineering tasks.","In this paper, we dive deep into an original vector of engagement - humor - and study how it fuels developer engagement.","First, we collect qualitative and quantitative data about the humorous elements present within three significant, real-world software projects: faker, which helps developers introduce humor within their tests; lolcommits, which captures a photograph after each contribution made by a developer; and volkswagen, an exercise in satire, which accidentally led to the invention of an impactful software tool.","Second, through a developer survey, we receive unique insights from 125 developers, who share their real-life experiences with humor in software.","Our analysis of the three case studies highlights the prevalence of humor in software, and unveils the worldwide community of developers who are enthusiastic about both software and humor.","We also learn about the caveats of humor in software through the valuable insights shared by our survey respondents.","We report clear evidence that, when practiced responsibly, humor increases developer engagement and supports them in addressing hard engineering and cognitive tasks.","The most actionable highlight of our work is that software tests and documentation are the best locations in code to practice humor."],"url":"http://arxiv.org/abs/2312.01680v1"}
{"created":"2023-12-04 07:01:54","title":"Jellyfish: A Large Language Model for Data Preprocessing","abstract":"In this paper, we present Jellyfish, an open-source LLM as a universal task solver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned with the datasets of several typical DP tasks including error detection, data imputation, schema matching, and entity matching, and delivers generalizability to other tasks. Remarkably, Jellyfish can operate on a local, single, and low-priced GPU with its 13 billion parameters, ensuring data security and enabling further tuning. Its proficiency in understanding natural language allows users to manually craft instructions for DP tasks. Unlike many existing methods that heavily rely on prior knowledge, Jellyfish acquires domain knowledge during its tuning process and integrates optional knowledge injection during inference. A distinctive feature of Jellyfish is its interpreter, which elucidates its output decisions. To construct Jellyfish, we develop a series of pre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance serializer, which automatically translates raw data into model prompts, and a knowledge injector, which optionally introduces task- and dataset-specific knowledge to enhance DP performance. Our evaluation of Jellyfish, using a range of real datasets, shows its competitiveness compared to state-of-the-art methods and its strong generalizability to unseen tasks. Jellyfish's performance rivals that of GPT series models, and its interpreter offers enhanced reasoning capabilities compared to GPT-3.5. Furthermore, our evaluation highlights the effectiveness of the techniques employed in constructing Jellyfish. Our model is available at Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish .","sentences":["In this paper, we present Jellyfish, an open-source LLM as a universal task solver for DP.","Built on the Llama 2 13B model, Jellyfish is instruction-tuned with the datasets of several typical DP tasks including error detection, data imputation, schema matching, and entity matching, and delivers generalizability to other tasks.","Remarkably, Jellyfish can operate on a local, single, and low-priced GPU with its 13 billion parameters, ensuring data security and enabling further tuning.","Its proficiency in understanding natural language allows users to manually craft instructions for DP tasks.","Unlike many existing methods that heavily rely on prior knowledge, Jellyfish acquires domain knowledge during its tuning process and integrates optional knowledge injection during inference.","A distinctive feature of Jellyfish is its interpreter, which elucidates its output decisions.","To construct Jellyfish, we develop a series of pre-tuning and DP-tuning techniques.","Jellyfish is equipped with an instance serializer, which automatically translates raw data into model prompts, and a knowledge injector, which optionally introduces task- and dataset-specific knowledge to enhance DP performance.","Our evaluation of Jellyfish, using a range of real datasets, shows its competitiveness compared to state-of-the-art methods and its strong generalizability to unseen tasks.","Jellyfish's performance rivals that of GPT series models, and its interpreter offers enhanced reasoning capabilities compared to GPT-3.5.","Furthermore, our evaluation highlights the effectiveness of the techniques employed in constructing Jellyfish.","Our model is available at Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish ."],"url":"http://arxiv.org/abs/2312.01678v1"}
{"created":"2023-12-04 06:51:46","title":"EDALearn: A Comprehensive RTL-to-Signoff EDA Benchmark for Democratized and Reproducible ML for EDA Research","abstract":"The application of Machine Learning (ML) in Electronic Design Automation (EDA) for Very Large-Scale Integration (VLSI) design has garnered significant research attention. Despite the requirement for extensive datasets to build effective ML models, most studies are limited to smaller, internally generated datasets due to the lack of comprehensive public resources. In response, we introduce EDALearn, the first holistic, open-source benchmark suite specifically for ML tasks in EDA. This benchmark suite presents an end-to-end flow from synthesis to physical implementation, enriching data collection across various stages. It fosters reproducibility and promotes research into ML transferability across different technology nodes. Accommodating a wide range of VLSI design instances and sizes, our benchmark aptly represents the complexity of contemporary VLSI designs. Additionally, we provide an in-depth data analysis, enabling users to fully comprehend the attributes and distribution of our data, which is essential for creating efficient ML models. Our contributions aim to encourage further advances in the ML-EDA domain.","sentences":["The application of Machine Learning (ML) in Electronic Design Automation (EDA) for Very Large-Scale Integration (VLSI) design has garnered significant research attention.","Despite the requirement for extensive datasets to build effective ML models, most studies are limited to smaller, internally generated datasets due to the lack of comprehensive public resources.","In response, we introduce EDALearn, the first holistic, open-source benchmark suite specifically for ML tasks in EDA.","This benchmark suite presents an end-to-end flow from synthesis to physical implementation, enriching data collection across various stages.","It fosters reproducibility and promotes research into ML transferability across different technology nodes.","Accommodating a wide range of VLSI design instances and sizes, our benchmark aptly represents the complexity of contemporary VLSI designs.","Additionally, we provide an in-depth data analysis, enabling users to fully comprehend the attributes and distribution of our data, which is essential for creating efficient ML models.","Our contributions aim to encourage further advances in the ML-EDA domain."],"url":"http://arxiv.org/abs/2312.01674v1"}
{"created":"2023-12-04 05:52:59","title":"A text-dependent speaker verification application framework based on Chinese numerical string corpus","abstract":"Researches indicate that text-dependent speaker verification (TD-SV) often outperforms text-independent verification (TI-SV) in short speech scenarios. However, collecting large-scale fixed text speech data is challenging, and as speech length increases, factors like sentence rhythm and pauses affect TDSV's sensitivity to text sequence. Based on these factors, We propose the hypothesis that strategies such as more fine-grained pooling methods on time scales and decoupled representations of speech speaker embedding and text embedding are more suitable for TD-SV. We have introduced an end-to-end TD-SV system based on a dataset comprising longer Chinese numerical string texts. It contains a text embedding network, a speaker embedding network, and back-end fusion. First, we recorded a dataset consisting of long Chinese numerical text named SHAL, which is publicly available on the Open-SLR website. We addressed the issue of dataset scarcity by augmenting it using Tacotron2 and HiFi-GAN. Next, we introduced a dual representation of speech with text embedding and speaker embedding. In the text embedding network, we employed an enhanced Transformer and introduced a triple loss that includes text classification loss, CTC loss, and decoder loss. For the speaker embedding network, we enhanced a sliding window attentive statistics pooling (SWASP), combined with attentive statistics pooling (ASP) to create a multi-scale pooling method. Finally, we fused text embedding and speaker embedding. Our pooling methods achieved an equal error rate (EER) performance improvement of 49.2% on Hi-Mia and 75.0% on SHAL, respectively.","sentences":["Researches indicate that text-dependent speaker verification (TD-SV) often outperforms text-independent verification (TI-SV) in short speech scenarios.","However, collecting large-scale fixed text speech data is challenging, and as speech length increases, factors like sentence rhythm and pauses affect TDSV's sensitivity to text sequence.","Based on these factors, We propose the hypothesis that strategies such as more fine-grained pooling methods on time scales and decoupled representations of speech speaker embedding and text embedding are more suitable for TD-SV.","We have introduced an end-to-end TD-SV system based on a dataset comprising longer Chinese numerical string texts.","It contains a text embedding network, a speaker embedding network, and back-end fusion.","First, we recorded a dataset consisting of long Chinese numerical text named SHAL, which is publicly available on the Open-SLR website.","We addressed the issue of dataset scarcity by augmenting it using Tacotron2 and HiFi-GAN.","Next, we introduced a dual representation of speech with text embedding and speaker embedding.","In the text embedding network, we employed an enhanced Transformer and introduced a triple loss that includes text classification loss, CTC loss, and decoder loss.","For the speaker embedding network, we enhanced a sliding window attentive statistics pooling (SWASP), combined with attentive statistics pooling (ASP) to create a multi-scale pooling method.","Finally, we fused text embedding and speaker embedding.","Our pooling methods achieved an equal error rate (EER) performance improvement of 49.2% on Hi-Mia and 75.0% on SHAL, respectively."],"url":"http://arxiv.org/abs/2312.01645v1"}
{"created":"2023-12-04 05:42:56","title":"SequencePAR: Understanding Pedestrian Attributes via A Sequence Generation Paradigm","abstract":"Current pedestrian attribute recognition (PAR) algorithms are developed based on multi-label or multi-task learning frameworks, which aim to discriminate the attributes using specific classification heads. However, these discriminative models are easily influenced by imbalanced data or noisy samples. Inspired by the success of generative models, we rethink the pedestrian attribute recognition scheme and believe the generative models may perform better on modeling dependencies and complexity between human attributes. In this paper, we propose a novel sequence generation paradigm for pedestrian attribute recognition, termed SequencePAR. It extracts the pedestrian features using a pre-trained CLIP model and embeds the attribute set into query tokens under the guidance of text prompts. Then, a Transformer decoder is proposed to generate the human attributes by incorporating the visual features and attribute query tokens. The masked multi-head attention layer is introduced into the decoder module to prevent the model from remembering the next attribute while making attribute predictions during training. Extensive experiments on multiple widely used pedestrian attribute recognition datasets fully validated the effectiveness of our proposed SequencePAR. The source code and pre-trained models will be released at https://github.com/Event-AHU/OpenPAR.","sentences":["Current pedestrian attribute recognition (PAR) algorithms are developed based on multi-label or multi-task learning frameworks, which aim to discriminate the attributes using specific classification heads.","However, these discriminative models are easily influenced by imbalanced data or noisy samples.","Inspired by the success of generative models, we rethink the pedestrian attribute recognition scheme and believe the generative models may perform better on modeling dependencies and complexity between human attributes.","In this paper, we propose a novel sequence generation paradigm for pedestrian attribute recognition, termed SequencePAR.","It extracts the pedestrian features using a pre-trained CLIP model and embeds the attribute set into query tokens under the guidance of text prompts.","Then, a Transformer decoder is proposed to generate the human attributes by incorporating the visual features and attribute query tokens.","The masked multi-head attention layer is introduced into the decoder module to prevent the model from remembering the next attribute while making attribute predictions during training.","Extensive experiments on multiple widely used pedestrian attribute recognition datasets fully validated the effectiveness of our proposed SequencePAR.","The source code and pre-trained models will be released at https://github.com/Event-AHU/OpenPAR."],"url":"http://arxiv.org/abs/2312.01640v1"}
{"created":"2023-12-04 05:13:59","title":"CLAMP: Contrastive LAnguage Model Prompt-tuning","abstract":"Large language models (LLMs) have emerged as powerful general-purpose interfaces for many machine learning problems. Recent work has adapted LLMs to generative visual tasks like image captioning, visual question answering, and visual chat, using a relatively small amount of instruction-tuning data. In this paper, we explore whether modern LLMs can also be adapted to classifying an image into a set of categories. First, we evaluate multimodal LLMs that are tuned for generative tasks on zero-shot image classification and find that their performance is far below that of specialized models like CLIP. We then propose an approach for light fine-tuning of LLMs using the same contrastive image-caption matching objective as CLIP. Our results show that LLMs can, indeed, achieve good image classification performance when adapted this way. Our approach beats state-of-the-art mLLMs by 13% and slightly outperforms contrastive learning with a custom text model, while also retaining the LLM's generative abilities. LLM initialization appears to particularly help classification in domains under-represented in the visual pre-training data.","sentences":["Large language models (LLMs) have emerged as powerful general-purpose interfaces for many machine learning problems.","Recent work has adapted LLMs to generative visual tasks like image captioning, visual question answering, and visual chat, using a relatively small amount of instruction-tuning data.","In this paper, we explore whether modern LLMs can also be adapted to classifying an image into a set of categories.","First, we evaluate multimodal LLMs that are tuned for generative tasks on zero-shot image classification and find that their performance is far below that of specialized models like CLIP.","We then propose an approach for light fine-tuning of LLMs using the same contrastive image-caption matching objective as CLIP.","Our results show that LLMs can, indeed, achieve good image classification performance when adapted this way.","Our approach beats state-of-the-art mLLMs by 13% and slightly outperforms contrastive learning with a custom text model, while also retaining the LLM's generative abilities.","LLM initialization appears to particularly help classification in domains under-represented in the visual pre-training data."],"url":"http://arxiv.org/abs/2312.01629v1"}
{"created":"2023-12-04 04:49:10","title":"GVFs in the Real World: Making Predictions Online for Water Treatment","abstract":"In this paper we investigate the use of reinforcement-learning based prediction approaches for a real drinking-water treatment plant. Developing such a prediction system is a critical step on the path to optimizing and automating water treatment. Before that, there are many questions to answer about the predictability of the data, suitable neural network architectures, how to overcome partial observability and more. We first describe this dataset, and highlight challenges with seasonality, nonstationarity, partial observability, and heterogeneity across sensors and operation modes of the plant. We then describe General Value Function (GVF) predictions -- discounted cumulative sums of observations -- and highlight why they might be preferable to classical n-step predictions common in time series prediction. We discuss how to use offline data to appropriately pre-train our temporal difference learning (TD) agents that learn these GVF predictions, including how to select hyperparameters for online fine-tuning in deployment. We find that the TD-prediction agent obtains an overall lower normalized mean-squared error than the n-step prediction agent. Finally, we show the importance of learning in deployment, by comparing a TD agent trained purely offline with no online updating to a TD agent that learns online. This final result is one of the first to motivate the importance of adapting predictions in real-time, for non-stationary high-volume systems in the real world.","sentences":["In this paper we investigate the use of reinforcement-learning based prediction approaches for a real drinking-water treatment plant.","Developing such a prediction system is a critical step on the path to optimizing and automating water treatment.","Before that, there are many questions to answer about the predictability of the data, suitable neural network architectures, how to overcome partial observability and more.","We first describe this dataset, and highlight challenges with seasonality, nonstationarity, partial observability, and heterogeneity across sensors and operation modes of the plant.","We then describe General Value Function (GVF) predictions -- discounted cumulative sums of observations -- and highlight why they might be preferable to classical n-step predictions common in time series prediction.","We discuss how to use offline data to appropriately pre-train our temporal difference learning (TD) agents that learn these GVF predictions, including how to select hyperparameters for online fine-tuning in deployment.","We find that the TD-prediction agent obtains an overall lower normalized mean-squared error than the n-step prediction agent.","Finally, we show the importance of learning in deployment, by comparing a TD agent trained purely offline with no online updating to a TD agent that learns online.","This final result is one of the first to motivate the importance of adapting predictions in real-time, for non-stationary high-volume systems in the real world."],"url":"http://arxiv.org/abs/2312.01624v1"}
{"created":"2023-12-04 04:47:48","title":"Universal Segmentation at Arbitrary Granularity with Language Instruction","abstract":"This paper aims to achieve universal segmentation of arbitrary semantic level. Despite significant progress in recent years, specialist segmentation approaches are limited to specific tasks and data distribution. Retraining a new model for adaptation to new scenarios or settings takes expensive computation and time cost, which raises the demand for versatile and universal segmentation model that can cater to various granularity. Although some attempts have been made for unifying different segmentation tasks or generalization to various scenarios, limitations in the definition of paradigms and input-output spaces make it difficult for them to achieve accurate understanding of content at arbitrary granularity. To this end, we present UniLSeg, a universal segmentation model that can perform segmentation at any semantic level with the guidance of language instructions. For training UniLSeg, we reorganize a group of tasks from original diverse distributions into a unified data format, where images with texts describing segmentation targets as input and corresponding masks are output. Combined with a automatic annotation engine for utilizing numerous unlabeled data, UniLSeg achieves excellent performance on various tasks and settings, surpassing both specialist and unified segmentation models.","sentences":["This paper aims to achieve universal segmentation of arbitrary semantic level.","Despite significant progress in recent years, specialist segmentation approaches are limited to specific tasks and data distribution.","Retraining a new model for adaptation to new scenarios or settings takes expensive computation and time cost, which raises the demand for versatile and universal segmentation model that can cater to various granularity.","Although some attempts have been made for unifying different segmentation tasks or generalization to various scenarios, limitations in the definition of paradigms and input-output spaces make it difficult for them to achieve accurate understanding of content at arbitrary granularity.","To this end, we present UniLSeg, a universal segmentation model that can perform segmentation at any semantic level with the guidance of language instructions.","For training UniLSeg, we reorganize a group of tasks from original diverse distributions into a unified data format, where images with texts describing segmentation targets as input and corresponding masks are output.","Combined with a automatic annotation engine for utilizing numerous unlabeled data, UniLSeg achieves excellent performance on various tasks and settings, surpassing both specialist and unified segmentation models."],"url":"http://arxiv.org/abs/2312.01623v1"}
{"created":"2023-12-04 04:14:50","title":"Heroes: Lightweight Federated Learning with Neural Composition and Adaptive Local Update in Heterogeneous Edge Networks","abstract":"Federated Learning (FL) enables distributed clients to collaboratively train models without exposing their private data. However, it is difficult to implement efficient FL due to limited resources. Most existing works compress the transmitted gradients or prune the global model to reduce the resource cost, but leave the compressed or pruned parameters under-optimized, which degrades the training performance. To address this issue, the neural composition technique constructs size-adjustable models by composing low-rank tensors, allowing every parameter in the global model to learn the knowledge from all clients. Nevertheless, some tensors can only be optimized by a small fraction of clients, thus the global model may get insufficient training, leading to a long completion time, especially in heterogeneous edge scenarios. To this end, we enhance the neural composition technique, enabling all parameters to be fully trained. Further, we propose a lightweight FL framework, called Heroes, with enhanced neural composition and adaptive local update. A greedy-based algorithm is designed to adaptively assign the proper tensors and local update frequencies for participating clients according to their heterogeneous capabilities and resource budgets. Extensive experiments demonstrate that Heroes can reduce traffic consumption by about 72.05\\% and provide up to 2.97$\\times$ speedup compared to the baselines.","sentences":["Federated Learning (FL) enables distributed clients to collaboratively train models without exposing their private data.","However, it is difficult to implement efficient FL due to limited resources.","Most existing works compress the transmitted gradients or prune the global model to reduce the resource cost, but leave the compressed or pruned parameters under-optimized, which degrades the training performance.","To address this issue, the neural composition technique constructs size-adjustable models by composing low-rank tensors, allowing every parameter in the global model to learn the knowledge from all clients.","Nevertheless, some tensors can only be optimized by a small fraction of clients, thus the global model may get insufficient training, leading to a long completion time, especially in heterogeneous edge scenarios.","To this end, we enhance the neural composition technique, enabling all parameters to be fully trained.","Further, we propose a lightweight FL framework, called Heroes, with enhanced neural composition and adaptive local update.","A greedy-based algorithm is designed to adaptively assign the proper tensors and local update frequencies for participating clients according to their heterogeneous capabilities and resource budgets.","Extensive experiments demonstrate that Heroes can reduce traffic consumption by about 72.05\\% and provide up to 2.97$\\times$ speedup compared to the baselines."],"url":"http://arxiv.org/abs/2312.01617v1"}
{"created":"2023-12-04 03:38:04","title":"TextAug: Test time Text Augmentation for Multimodal Person Re-identification","abstract":"Multimodal Person Reidentification is gaining popularity in the research community due to its effectiveness compared to counter-part unimodal frameworks. However, the bottleneck for multimodal deep learning is the need for a large volume of multimodal training examples. Data augmentation techniques such as cropping, flipping, rotation, etc. are often employed in the image domain to improve the generalization of deep learning models. Augmenting in other modalities than images, such as text, is challenging and requires significant computational resources and external data sources. In this study, we investigate the effectiveness of two computer vision data augmentation techniques: cutout and cutmix, for text augmentation in multi-modal person re-identification. Our approach merges these two augmentation strategies into one strategy called CutMixOut which involves randomly removing words or sub-phrases from a sentence (Cutout) and blending parts of two or more sentences to create diverse examples (CutMix) with a certain probability assigned to each operation. This augmentation was implemented at inference time without any prior training. Our results demonstrate that the proposed technique is simple and effective in improving the performance on multiple multimodal person re-identification benchmarks.","sentences":["Multimodal Person Reidentification is gaining popularity in the research community due to its effectiveness compared to counter-part unimodal frameworks.","However, the bottleneck for multimodal deep learning is the need for a large volume of multimodal training examples.","Data augmentation techniques such as cropping, flipping, rotation, etc. are often employed in the image domain to improve the generalization of deep learning models.","Augmenting in other modalities than images, such as text, is challenging and requires significant computational resources and external data sources.","In this study, we investigate the effectiveness of two computer vision data augmentation techniques: cutout and cutmix, for text augmentation in multi-modal person re-identification.","Our approach merges these two augmentation strategies into one strategy called CutMixOut which involves randomly removing words or sub-phrases from a sentence (Cutout) and blending parts of two or more sentences to create diverse examples (CutMix) with a certain probability assigned to each operation.","This augmentation was implemented at inference time without any prior training.","Our results demonstrate that the proposed technique is simple and effective in improving the performance on multiple multimodal person re-identification benchmarks."],"url":"http://arxiv.org/abs/2312.01605v1"}
{"created":"2023-12-04 03:18:51","title":"Good Questions Help Zero-Shot Image Reasoning","abstract":"Aligning the recent large language models (LLMs) with computer vision models leads to large vision-language models (LVLMs), which have paved the way for zero-shot image reasoning tasks. However, LVLMs are usually trained on short high-level captions only referring to sparse focus regions in images. Such a ``tunnel vision'' limits LVLMs to exploring other relevant contexts in complex scenes. To address this challenge, we introduce Question-Driven Visual Exploration (QVix), a novel prompting strategy that enhances the exploratory capabilities of LVLMs in zero-shot reasoning tasks. QVix leverages LLMs' strong language prior to generate input-exploratory questions with more details than the original query, guiding LVLMs to explore visual content more comprehensively and uncover subtle or peripheral details. QVix enables a wider exploration of visual scenes, improving the LVLMs' reasoning accuracy and depth in tasks such as visual question answering and visual entailment. Our evaluations on various challenging zero-shot vision-language benchmarks, including ScienceQA and fine-grained visual classification, demonstrate that QVix significantly outperforms existing methods, highlighting its effectiveness in bridging the gap between complex visual data and LVLMs' exploratory abilities.","sentences":["Aligning the recent large language models (LLMs) with computer vision models leads to large vision-language models (LVLMs), which have paved the way for zero-shot image reasoning tasks.","However, LVLMs are usually trained on short high-level captions only referring to sparse focus regions in images.","Such a ``tunnel vision'' limits LVLMs to exploring other relevant contexts in complex scenes.","To address this challenge, we introduce Question-Driven Visual Exploration (QVix), a novel prompting strategy that enhances the exploratory capabilities of LVLMs in zero-shot reasoning tasks.","QVix leverages LLMs' strong language prior to generate input-exploratory questions with more details than the original query, guiding LVLMs to explore visual content more comprehensively and uncover subtle or peripheral details.","QVix enables a wider exploration of visual scenes, improving the LVLMs' reasoning accuracy and depth in tasks such as visual question answering and visual entailment.","Our evaluations on various challenging zero-shot vision-language benchmarks, including ScienceQA and fine-grained visual classification, demonstrate that QVix significantly outperforms existing methods, highlighting its effectiveness in bridging the gap between complex visual data and LVLMs' exploratory abilities."],"url":"http://arxiv.org/abs/2312.01598v1"}
{"created":"2023-12-04 03:16:48","title":"Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment","abstract":"Language models have been supervised with both language-only objective and visual grounding in existing studies of visual-grounded language learning. However, due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not. As a result, during representation learning, there is a mismatch between the visual information and the contextual meaning of the sentence. To overcome this limitation, we propose GroundedBERT - a grounded language learning method that enhances the BERT representation with visually grounded information. GroundedBERT comprises two components: (i) the original BERT which captures the contextual representation of words learned from the language corpora, and (ii) a visual grounding module which captures visual information learned from visual-grounded datasets. Moreover, we employ Optimal Transport (OT), specifically its partial variant, to solve the fractional alignment problem between the two modalities. Our proposed method significantly outperforms the baseline language models on various language tasks of the GLUE and SQuAD datasets.","sentences":["Language models have been supervised with both language-only objective and visual grounding in existing studies of visual-grounded language learning.","However, due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not.","As a result, during representation learning, there is a mismatch between the visual information and the contextual meaning of the sentence.","To overcome this limitation, we propose GroundedBERT - a grounded language learning method that enhances the BERT representation with visually grounded information.","GroundedBERT comprises two components: (i) the original BERT which captures the contextual representation of words learned from the language corpora, and (ii) a visual grounding module which captures visual information learned from visual-grounded datasets.","Moreover, we employ Optimal Transport (OT), specifically its partial variant, to solve the fractional alignment problem between the two modalities.","Our proposed method significantly outperforms the baseline language models on various language tasks of the GLUE and SQuAD datasets."],"url":"http://arxiv.org/abs/2312.01592v1"}
{"created":"2023-12-04 03:11:36","title":"Euclidean Bottleneck Steiner Tree is Fixed-Parameter Tractable","abstract":"In the Euclidean Bottleneck Steiner Tree problem, the input consists of a set of $n$ points in $\\mathbb{R}^2$ called terminals and a parameter $k$, and the goal is to compute a Steiner tree that spans all the terminals and contains at most $k$ points of $\\mathbb{R}^2$ as Steiner points such that the maximum edge-length of the Steiner tree is minimized, where the length of a tree edge is the Euclidean distance between its two endpoints. The problem is well-studied and is known to be NP-hard. In this paper, we give a $k^{O(k)} n^{O(1)}$-time algorithm for Euclidean Bottleneck Steiner Tree, which implies that the problem is fixed-parameter tractable (FPT). This settles an open question explicitly asked by Bae et al. [Algorithmica, 2011], who showed that the $\\ell_1$ and $\\ell_{\\infty}$ variants of the problem are FPT. Our approach can be generalized to the problem with $\\ell_p$ metric for any rational $1 \\le p \\le \\infty$, or even other metrics on $\\mathbb{R}^2$.","sentences":["In the Euclidean Bottleneck Steiner Tree problem, the input consists of a set of $n$ points in $\\mathbb{R}^2$ called terminals and a parameter $k$, and the goal is to compute a Steiner tree that spans all the terminals and contains at most $k$ points of $\\mathbb{R}^2$ as Steiner points such that the maximum edge-length of the Steiner tree is minimized, where the length of a tree edge is the Euclidean distance between its two endpoints.","The problem is well-studied and is known to be NP-hard.","In this paper, we give a $k^{O(k)} n^{O(1)}$-time algorithm for Euclidean Bottleneck Steiner Tree, which implies that the problem is fixed-parameter tractable (FPT).","This settles an open question explicitly asked by Bae et al.","[Algorithmica, 2011], who showed that the $\\ell_1$ and $\\ell_{\\infty}$ variants of the problem are FPT.","Our approach can be generalized to the problem with $\\ell_p$ metric for any rational $1 \\le p \\le \\infty$, or even other metrics on $\\mathbb{R}^2$."],"url":"http://arxiv.org/abs/2312.01589v1"}
{"created":"2023-12-04 03:09:31","title":"ActiveClean: Generating Line-Level Vulnerability Data via Active Learning","abstract":"Deep learning vulnerability detection tools are increasing in popularity and have been shown to be effective. These tools rely on large volume of high quality training data, which are very hard to get. Most of the currently available datasets provide function-level labels, reporting whether a function is vulnerable or not vulnerable. However, for a vulnerability detection to be useful, we need to also know the lines that are relevant to the vulnerability. This paper makes efforts towards developing systematic tools and proposes. ActiveClean to generate the large volume of line-level vulnerability data from commits. That is, in addition to function-level labels, it also reports which lines in the function are likely responsible for vulnerability detection. In the past, static analysis has been applied to clean commits to generate line-level data. Our approach based on active learning, which is easy to use and scalable, provide a complementary approach to static analysis. We designed semantic and syntactic properties from commit lines and use them to train the model. We evaluated our approach on both Java and C datasets processing more than 4.3K commits and 119K commit lines. AcitveClean achieved an F1 score between 70-74. Further, we also show that active learning is effective by using just 400 training data to reach F1 score of 70.23. Using ActiveClean, we generate the line-level labels for the entire FFMpeg project in the Devign dataset, including 5K functions, and also detected incorrect function-level labels. We demonstrated that using our cleaned data, LineVul, a SOTA line-level vulnerability detection tool, detected 70 more vulnerable lines and 18 more vulnerable functions, and improved Top 10 accuracy from 66% to 73%.","sentences":["Deep learning vulnerability detection tools are increasing in popularity and have been shown to be effective.","These tools rely on large volume of high quality training data, which are very hard to get.","Most of the currently available datasets provide function-level labels, reporting whether a function is vulnerable or not vulnerable.","However, for a vulnerability detection to be useful, we need to also know the lines that are relevant to the vulnerability.","This paper makes efforts towards developing systematic tools and proposes.","ActiveClean to generate the large volume of line-level vulnerability data from commits.","That is, in addition to function-level labels, it also reports which lines in the function are likely responsible for vulnerability detection.","In the past, static analysis has been applied to clean commits to generate line-level data.","Our approach based on active learning, which is easy to use and scalable, provide a complementary approach to static analysis.","We designed semantic and syntactic properties from commit lines and use them to train the model.","We evaluated our approach on both Java and C datasets processing more than 4.3K commits and 119K commit lines.","AcitveClean achieved an F1 score between 70-74.","Further, we also show that active learning is effective by using just 400 training data to reach F1 score of 70.23.","Using ActiveClean, we generate the line-level labels for the entire FFMpeg project in the Devign dataset, including 5K functions, and also detected incorrect function-level labels.","We demonstrated that using our cleaned data, LineVul, a SOTA line-level vulnerability detection tool, detected 70 more vulnerable lines and 18 more vulnerable functions, and improved Top 10 accuracy from 66% to 73%."],"url":"http://arxiv.org/abs/2312.01588v1"}
{"created":"2023-12-04 02:48:40","title":"OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection","abstract":"Deep neural networks (DNNs) have been found vulnerable to backdoor attacks, raising security concerns about their deployment in mission-critical applications. There are various approaches to detect backdoor attacks, however they all make certain assumptions about the target attack to be detected and require equal and huge numbers of clean and backdoor samples for training, which renders these detection methods quite limiting in real-world circumstances.   This study proposes a novel one-class classification framework called One-class Graph Embedding Classification (OCGEC) that uses GNNs for model-level backdoor detection with only a little amount of clean data. First, we train thousands of tiny models as raw datasets from a small number of clean datasets. Following that, we design a ingenious model-to-graph method for converting the model's structural details and weight features into graph data. We then pre-train a generative self-supervised graph autoencoder (GAE) to better learn the features of benign models in order to detect backdoor models without knowing the attack strategy. After that, we dynamically combine the GAE and one-class classifier optimization goals to form classification boundaries that distinguish backdoor models from benign models.   Our OCGEC combines the powerful representation capabilities of graph neural networks with the utility of one-class classification techniques in the field of anomaly detection. In comparison to other baselines, it achieves AUC scores of more than 98% on a number of tasks, which far exceeds existing methods for detection even when they rely on a huge number of positive and negative samples. Our pioneering application of graphic scenarios for generic backdoor detection can provide new insights that can be used to improve other backdoor defense tasks. Code is available at https://github.com/jhy549/OCGEC.","sentences":["Deep neural networks (DNNs) have been found vulnerable to backdoor attacks, raising security concerns about their deployment in mission-critical applications.","There are various approaches to detect backdoor attacks, however they all make certain assumptions about the target attack to be detected and require equal and huge numbers of clean and backdoor samples for training, which renders these detection methods quite limiting in real-world circumstances.   ","This study proposes a novel one-class classification framework called One-class Graph Embedding Classification (OCGEC) that uses GNNs for model-level backdoor detection with only a little amount of clean data.","First, we train thousands of tiny models as raw datasets from a small number of clean datasets.","Following that, we design a ingenious model-to-graph method for converting the model's structural details and weight features into graph data.","We then pre-train a generative self-supervised graph autoencoder (GAE) to better learn the features of benign models in order to detect backdoor models without knowing the attack strategy.","After that, we dynamically combine the GAE and one-class classifier optimization goals to form classification boundaries that distinguish backdoor models from benign models.   ","Our OCGEC combines the powerful representation capabilities of graph neural networks with the utility of one-class classification techniques in the field of anomaly detection.","In comparison to other baselines, it achieves AUC scores of more than 98% on a number of tasks, which far exceeds existing methods for detection even when they rely on a huge number of positive and negative samples.","Our pioneering application of graphic scenarios for generic backdoor detection can provide new insights that can be used to improve other backdoor defense tasks.","Code is available at https://github.com/jhy549/OCGEC."],"url":"http://arxiv.org/abs/2312.01585v1"}
{"created":"2023-12-04 02:23:32","title":"RJHMC-Tree for Exploration of the Bayesian Decision Tree Posterior","abstract":"Decision trees have found widespread application within the machine learning community due to their flexibility and interpretability. This paper is directed towards learning decision trees from data using a Bayesian approach, which is challenging due to the potentially enormous parameter space required to span all tree models. Several approaches have been proposed to combat this challenge, with one of the more successful being Markov chain Monte Carlo (MCMC) methods. The efficacy and efficiency of MCMC methods fundamentally rely on the quality of the so-called proposals, which is the focus of this paper. In particular, this paper investigates using a Hamiltonian Monte Carlo (HMC) approach to explore the posterior of Bayesian decision trees more efficiently by exploiting the geometry of the likelihood within a global update scheme. Two implementations of the novel algorithm are developed and compared to existing methods by testing against standard datasets in the machine learning and Bayesian decision tree literature. HMC-based methods are shown to perform favourably with respect to predictive test accuracy, acceptance rate, and tree complexity.","sentences":["Decision trees have found widespread application within the machine learning community due to their flexibility and interpretability.","This paper is directed towards learning decision trees from data using a Bayesian approach, which is challenging due to the potentially enormous parameter space required to span all tree models.","Several approaches have been proposed to combat this challenge, with one of the more successful being Markov chain Monte Carlo (MCMC) methods.","The efficacy and efficiency of MCMC methods fundamentally rely on the quality of the so-called proposals, which is the focus of this paper.","In particular, this paper investigates using a Hamiltonian Monte Carlo (HMC) approach to explore the posterior of Bayesian decision trees more efficiently by exploiting the geometry of the likelihood within a global update scheme.","Two implementations of the novel algorithm are developed and compared to existing methods by testing against standard datasets in the machine learning and Bayesian decision tree literature.","HMC-based methods are shown to perform favourably with respect to predictive test accuracy, acceptance rate, and tree complexity."],"url":"http://arxiv.org/abs/2312.01577v1"}
{"created":"2023-12-04 01:49:24","title":"Multimodal Speech Emotion Recognition Using Modality-specific Self-Supervised Frameworks","abstract":"Emotion recognition is a topic of significant interest in assistive robotics due to the need to equip robots with the ability to comprehend human behavior, facilitating their effective interaction in our society. Consequently, efficient and dependable emotion recognition systems supporting optimal human-machine communication are required. Multi-modality (including speech, audio, text, images, and videos) is typically exploited in emotion recognition tasks. Much relevant research is based on merging multiple data modalities and training deep learning models utilizing low-level data representations. However, most existing emotion databases are not large (or complex) enough to allow machine learning approaches to learn detailed representations. This paper explores modalityspecific pre-trained transformer frameworks for self-supervised learning of speech and text representations for data-efficient emotion recognition while achieving state-of-the-art performance in recognizing emotions. This model applies feature-level fusion using nonverbal cue data points from motion capture to provide multimodal speech emotion recognition. The model was trained using the publicly available IEMOCAP dataset, achieving an overall accuracy of 77.58% for four emotions, outperforming state-of-the-art approaches","sentences":["Emotion recognition is a topic of significant interest in assistive robotics due to the need to equip robots with the ability to comprehend human behavior, facilitating their effective interaction in our society.","Consequently, efficient and dependable emotion recognition systems supporting optimal human-machine communication are required.","Multi-modality (including speech, audio, text, images, and videos) is typically exploited in emotion recognition tasks.","Much relevant research is based on merging multiple data modalities and training deep learning models utilizing low-level data representations.","However, most existing emotion databases are not large (or complex) enough to allow machine learning approaches to learn detailed representations.","This paper explores modalityspecific pre-trained transformer frameworks for self-supervised learning of speech and text representations for data-efficient emotion recognition while achieving state-of-the-art performance in recognizing emotions.","This model applies feature-level fusion using nonverbal cue data points from motion capture to provide multimodal speech emotion recognition.","The model was trained using the publicly available IEMOCAP dataset, achieving an overall accuracy of 77.58% for four emotions, outperforming state-of-the-art approaches"],"url":"http://arxiv.org/abs/2312.01568v1"}
{"created":"2023-12-04 01:42:52","title":"Finding mixed memberships in categorical data","abstract":"Latent class analysis is a fundamental problem in categorical data analysis, which often encounters overlapping latent classes that further challenge the problem. This paper addresses the problem of finding latent mixed memberships of subjects in categorical data with polytomous responses under the Grade of Membership (GoM) model, which allows each subject to be associated with a membership score in each latent class. We propose two efficient spectral algorithms for estimating latent mixed memberships and other GoM parameters. Our algorithms are developed by using the singular value decomposition of a regularized Laplacian matrix. We establish their convergence rates under a mild condition on data sparsity. We also provide a measure to evaluate the quality of estimated mixed memberships for real-world categorical data and determine the number of latent classes based on this measure. Finally, we demonstrate the performances of our methods in both computer-generated and real-world categorical data.","sentences":["Latent class analysis is a fundamental problem in categorical data analysis, which often encounters overlapping latent classes that further challenge the problem.","This paper addresses the problem of finding latent mixed memberships of subjects in categorical data with polytomous responses under the Grade of Membership (GoM) model, which allows each subject to be associated with a membership score in each latent class.","We propose two efficient spectral algorithms for estimating latent mixed memberships and other GoM parameters.","Our algorithms are developed by using the singular value decomposition of a regularized Laplacian matrix.","We establish their convergence rates under a mild condition on data sparsity.","We also provide a measure to evaluate the quality of estimated mixed memberships for real-world categorical data and determine the number of latent classes based on this measure.","Finally, we demonstrate the performances of our methods in both computer-generated and real-world categorical data."],"url":"http://arxiv.org/abs/2312.01565v1"}
{"created":"2023-12-04 01:28:38","title":"Multi-View Person Matching and 3D Pose Estimation with Arbitrary Uncalibrated Camera Networks","abstract":"Cross-view person matching and 3D human pose estimation in multi-camera networks are particularly difficult when the cameras are extrinsically uncalibrated. Existing efforts generally require large amounts of 3D data for training neural networks or known camera poses for geometric constraints to solve the problem. However, camera poses and 3D data annotation are usually expensive and not always available. We present a method, PME, that solves the two tasks without requiring either information. Our idea is to address cross-view person matching as a clustering problem using each person as a cluster center, then obtain correspondences from person matches, and estimate 3D human poses through multi-view triangulation and bundle adjustment. We solve the clustering problem by introducing a \"size constraint\" using the number of cameras and a \"source constraint\" using the fact that two people from the same camera view should not match, to narrow the solution space to a small feasible region. The 2D human poses used in clustering are obtained through a pre-trained 2D pose detector, so our method does not require expensive 3D training data for each new scene. We extensively evaluate our method on three open datasets and two indoor and outdoor datasets collected using arbitrarily set cameras. Our method outperforms other methods by a large margin on cross-view person matching, reaches SOTA performance on 3D human pose estimation without using either camera poses or 3D training data, and shows good generalization ability across five datasets of various environment settings.","sentences":["Cross-view person matching and 3D human pose estimation in multi-camera networks are particularly difficult when the cameras are extrinsically uncalibrated.","Existing efforts generally require large amounts of 3D data for training neural networks or known camera poses for geometric constraints to solve the problem.","However, camera poses and 3D data annotation are usually expensive and not always available.","We present a method, PME, that solves the two tasks without requiring either information.","Our idea is to address cross-view person matching as a clustering problem using each person as a cluster center, then obtain correspondences from person matches, and estimate 3D human poses through multi-view triangulation and bundle adjustment.","We solve the clustering problem by introducing a \"size constraint\" using the number of cameras and a \"source constraint\" using the fact that two people from the same camera view should not match, to narrow the solution space to a small feasible region.","The 2D human poses used in clustering are obtained through a pre-trained 2D pose detector, so our method does not require expensive 3D training data for each new scene.","We extensively evaluate our method on three open datasets and two indoor and outdoor datasets collected using arbitrarily set cameras.","Our method outperforms other methods by a large margin on cross-view person matching, reaches SOTA performance on 3D human pose estimation without using either camera poses or 3D training data, and shows good generalization ability across five datasets of various environment settings."],"url":"http://arxiv.org/abs/2312.01561v1"}
{"created":"2023-12-04 00:42:25","title":"Using human and robot synthetic data for training smart hand tools","abstract":"The future of work does not require a choice between human and robot. Aside from explicit human-robot collaboration, robotics can play an increasingly important role in helping train workers as well as the tools they may use, especially in complex tasks that may be difficult to automate or effectively roboticize. This paper introduces a form of smart tool for use by human workers and shows how training the tool for task recognition, one of the key requirements, can be accomplished. Machine learning (ML) with purely human-based data can be extremely laborious and time-consuming. First, we show how data synthetically-generated by a robot can be leveraged in the ML training process. Later, we demonstrate how fine-tuning ML models for individual physical tasks and workers can significantly scale up the benefits of using ML to provide this feedback. Experimental results show the effectiveness and scalability of our approach, as we test data size versus accuracy. Smart hand tools of the type introduced here can provide insights and real-time analytics on efficient and safe tool usage and operation, thereby enhancing human participation and skill in a wide range of work environments. Using robotic platforms to help train smart tools will be essential, particularly given the diverse types of applications for which smart hand tools are envisioned for human use.","sentences":["The future of work does not require a choice between human and robot.","Aside from explicit human-robot collaboration, robotics can play an increasingly important role in helping train workers as well as the tools they may use, especially in complex tasks that may be difficult to automate or effectively roboticize.","This paper introduces a form of smart tool for use by human workers and shows how training the tool for task recognition, one of the key requirements, can be accomplished.","Machine learning (ML) with purely human-based data can be extremely laborious and time-consuming.","First, we show how data synthetically-generated by a robot can be leveraged in the ML training process.","Later, we demonstrate how fine-tuning ML models for individual physical tasks and workers can significantly scale up the benefits of using ML to provide this feedback.","Experimental results show the effectiveness and scalability of our approach, as we test data size versus accuracy.","Smart hand tools of the type introduced here can provide insights and real-time analytics on efficient and safe tool usage and operation, thereby enhancing human participation and skill in a wide range of work environments.","Using robotic platforms to help train smart tools will be essential, particularly given the diverse types of applications for which smart hand tools are envisioned for human use."],"url":"http://arxiv.org/abs/2312.01550v1"}
{"created":"2023-12-04 00:31:16","title":"Near-Optimal Algorithms for Gaussians with Huber Contamination: Mean Estimation and Linear Regression","abstract":"We study the fundamental problems of Gaussian mean estimation and linear regression with Gaussian covariates in the presence of Huber contamination. Our main contribution is the design of the first sample near-optimal and almost linear-time algorithms with optimal error guarantees for both of these problems. Specifically, for Gaussian robust mean estimation on $\\mathbb{R}^d$ with contamination parameter $\\epsilon \\in (0, \\epsilon_0)$ for a small absolute constant $\\epsilon_0$, we give an algorithm with sample complexity $n = \\tilde{O}(d/\\epsilon^2)$ and almost linear runtime that approximates the target mean within $\\ell_2$-error $O(\\epsilon)$. This improves on prior work that achieved this error guarantee with polynomially suboptimal sample and time complexity. For robust linear regression, we give the first algorithm with sample complexity $n = \\tilde{O}(d/\\epsilon^2)$ and almost linear runtime that approximates the target regressor within $\\ell_2$-error $O(\\epsilon)$. This is the first polynomial sample and time algorithm achieving the optimal error guarantee, answering an open question in the literature. At the technical level, we develop a methodology that yields almost-linear time algorithms for multi-directional filtering that may be of broader interest.","sentences":["We study the fundamental problems of Gaussian mean estimation and linear regression with Gaussian covariates in the presence of Huber contamination.","Our main contribution is the design of the first sample near-optimal and almost linear-time algorithms with optimal error guarantees for both of these problems.","Specifically, for Gaussian robust mean estimation on $\\mathbb{R}^d$ with contamination parameter $\\epsilon \\in (0, \\epsilon_0)$ for a small absolute constant $\\epsilon_0$, we give an algorithm with sample complexity $n = \\tilde{O}(d/\\epsilon^2)$ and almost linear runtime that approximates the target mean within $\\ell_2$-error $O(\\epsilon)$. This improves on prior work that achieved this error guarantee with polynomially suboptimal sample and time complexity.","For robust linear regression, we give the first algorithm with sample complexity $n = \\tilde{O}(d/\\epsilon^2)$ and almost linear runtime that approximates the target regressor within $\\ell_2$-error $O(\\epsilon)$. This is the first polynomial sample and time algorithm achieving the optimal error guarantee, answering an open question in the literature.","At the technical level, we develop a methodology that yields almost-linear time algorithms for multi-directional filtering that may be of broader interest."],"url":"http://arxiv.org/abs/2312.01547v1"}
{"created":"2023-12-03 23:59:03","title":"Revisiting Non-separable Binary Classification and its Applications in Anomaly Detection","abstract":"The inability to linearly classify XOR has motivated much of deep learning. We revisit this age-old problem and show that linear classification of XOR is indeed possible. Instead of separating data between halfspaces, we propose a slightly different paradigm, equality separation, that adapts the SVM objective to distinguish data within or outside the margin. Our classifier can then be integrated into neural network pipelines with a smooth approximation. From its properties, we intuit that equality separation is suitable for anomaly detection. To formalize this notion, we introduce closing numbers, a quantitative measure on the capacity for classifiers to form closed decision regions for anomaly detection. Springboarding from this theoretical connection between binary classification and anomaly detection, we test our hypothesis on supervised anomaly detection experiments, showing that equality separation can detect both seen and unseen anomalies.","sentences":["The inability to linearly classify XOR has motivated much of deep learning.","We revisit this age-old problem and show that linear classification of XOR is indeed possible.","Instead of separating data between halfspaces, we propose a slightly different paradigm, equality separation, that adapts the SVM objective to distinguish data within or outside the margin.","Our classifier can then be integrated into neural network pipelines with a smooth approximation.","From its properties, we intuit that equality separation is suitable for anomaly detection.","To formalize this notion, we introduce closing numbers, a quantitative measure on the capacity for classifiers to form closed decision regions for anomaly detection.","Springboarding from this theoretical connection between binary classification and anomaly detection, we test our hypothesis on supervised anomaly detection experiments, showing that equality separation can detect both seen and unseen anomalies."],"url":"http://arxiv.org/abs/2312.01541v1"}
{"created":"2023-12-03 23:40:12","title":"Robust Computer Vision in an Ever-Changing World: A Survey of Techniques for Tackling Distribution Shifts","abstract":"AI applications are becoming increasingly visible to the general public. There is a notable gap between the theoretical assumptions researchers make about computer vision models and the reality those models face when deployed in the real world. One of the critical reasons for this gap is a challenging problem known as distribution shift. Distribution shifts tend to vary with complexity of the data, dataset size, and application type. In our paper, we discuss the identification of such a prominent gap, exploring the concept of distribution shift and its critical significance. We provide an in-depth overview of various types of distribution shifts, elucidate their distinctions, and explore techniques within the realm of the data-centric domain employed to address them. Distribution shifts can occur during every phase of the machine learning pipeline, from the data collection stage to the stage of training a machine learning model to the stage of final model deployment. As a result, it raises concerns about the overall robustness of the machine learning techniques for computer vision applications that are deployed publicly for consumers. Different deep learning models each tailored for specific type of data and tasks, architectural pipelines; highlighting how variations in data preprocessing and feature extraction can impact robustness., data augmentation strategies (e.g. geometric, synthetic and learning-based); demonstrating their role in enhancing model generalization, and training mechanisms (e.g. transfer learning, zero-shot) fall under the umbrella of data-centric methods. Each of these components form an integral part of the neural-network we analyze contributing uniquely to strengthening model robustness against distribution shifts. We compare and contrast numerous AI models that are built for mitigating shifts in hidden stratification and spurious correlations, ...","sentences":["AI applications are becoming increasingly visible to the general public.","There is a notable gap between the theoretical assumptions researchers make about computer vision models and the reality those models face when deployed in the real world.","One of the critical reasons for this gap is a challenging problem known as distribution shift.","Distribution shifts tend to vary with complexity of the data, dataset size, and application type.","In our paper, we discuss the identification of such a prominent gap, exploring the concept of distribution shift and its critical significance.","We provide an in-depth overview of various types of distribution shifts, elucidate their distinctions, and explore techniques within the realm of the data-centric domain employed to address them.","Distribution shifts can occur during every phase of the machine learning pipeline, from the data collection stage to the stage of training a machine learning model to the stage of final model deployment.","As a result, it raises concerns about the overall robustness of the machine learning techniques for computer vision applications that are deployed publicly for consumers.","Different deep learning models each tailored for specific type of data and tasks, architectural pipelines; highlighting how variations in data preprocessing and feature extraction can impact robustness., data augmentation strategies (e.g. geometric, synthetic and learning-based); demonstrating their role in enhancing model generalization, and training mechanisms (e.g. transfer learning, zero-shot) fall under the umbrella of data-centric methods.","Each of these components form an integral part of the neural-network we analyze contributing uniquely to strengthening model robustness against distribution shifts.","We compare and contrast numerous AI models that are built for mitigating shifts in hidden stratification and spurious correlations, ..."],"url":"http://arxiv.org/abs/2312.01540v1"}
{"created":"2023-12-03 23:36:16","title":"Recurrent Distance-Encoding Neural Networks for Graph Representation Learning","abstract":"Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but suffer from high computational complexity and have to rely on ad-hoc positional encoding to bake in the graph inductive bias. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a parallelizable linear recurrent network over the chain of distances to provide a natural encoding of its neighborhood structure. With no need for positional encoding, we empirically show that the performance of our model is highly competitive compared with that of state-of-the-art graph transformers on various benchmarks, at a drastically reduced computational complexity. In addition, we show that our model is theoretically more expressive than one-hop message passing neural networks.","sentences":["Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing information from distant nodes effectively.","Conversely, graph transformers allow each node to attend to all other nodes directly, but suffer from high computational complexity and have to rely on ad-hoc positional encoding to bake in the graph inductive bias.","In this paper, we propose a new architecture to reconcile these challenges.","Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a parallelizable linear recurrent network over the chain of distances to provide a natural encoding of its neighborhood structure.","With no need for positional encoding, we empirically show that the performance of our model is highly competitive compared with that of state-of-the-art graph transformers on various benchmarks, at a drastically reduced computational complexity.","In addition, we show that our model is theoretically more expressive than one-hop message passing neural networks."],"url":"http://arxiv.org/abs/2312.01538v1"}
{"created":"2023-12-03 23:30:48","title":"Unlocking the Potential of Federated Learning: The Symphony of Dataset Distillation via Deep Generative Latents","abstract":"Data heterogeneity presents significant challenges for federated learning (FL). Recently, dataset distillation techniques have been introduced, and performed at the client level, to attempt to mitigate some of these challenges. In this paper, we propose a highly efficient FL dataset distillation framework on the server side, significantly reducing both the computational and communication demands on local devices while enhancing the clients' privacy. Unlike previous strategies that perform dataset distillation on local devices and upload synthetic data to the server, our technique enables the server to leverage prior knowledge from pre-trained deep generative models to synthesize essential data representations from a heterogeneous model architecture. This process allows local devices to train smaller surrogate models while enabling the training of a larger global model on the server, effectively minimizing resource utilization. We substantiate our claim with a theoretical analysis, demonstrating the asymptotic resemblance of the process to the hypothetical ideal of completely centralized training on a heterogeneous dataset. Empirical evidence from our comprehensive experiments indicates our method's superiority, delivering an accuracy enhancement of up to 40% over non-dataset-distillation techniques in highly heterogeneous FL contexts, and surpassing existing dataset-distillation methods by 18%. In addition to the high accuracy, our framework converges faster than the baselines because rather than the server trains on several sets of heterogeneous data distributions, it trains on a multi-modal distribution. Our code is available at https://github.com/FedDG23/FedDG-main.git","sentences":["Data heterogeneity presents significant challenges for federated learning (FL).","Recently, dataset distillation techniques have been introduced, and performed at the client level, to attempt to mitigate some of these challenges.","In this paper, we propose a highly efficient FL dataset distillation framework on the server side, significantly reducing both the computational and communication demands on local devices while enhancing the clients' privacy.","Unlike previous strategies that perform dataset distillation on local devices and upload synthetic data to the server, our technique enables the server to leverage prior knowledge from pre-trained deep generative models to synthesize essential data representations from a heterogeneous model architecture.","This process allows local devices to train smaller surrogate models while enabling the training of a larger global model on the server, effectively minimizing resource utilization.","We substantiate our claim with a theoretical analysis, demonstrating the asymptotic resemblance of the process to the hypothetical ideal of completely centralized training on a heterogeneous dataset.","Empirical evidence from our comprehensive experiments indicates our method's superiority, delivering an accuracy enhancement of up to 40% over non-dataset-distillation techniques in highly heterogeneous FL contexts, and surpassing existing dataset-distillation methods by 18%.","In addition to the high accuracy, our framework converges faster than the baselines because rather than the server trains on several sets of heterogeneous data distributions, it trains on a multi-modal distribution.","Our code is available at https://github.com/FedDG23/FedDG-main.git"],"url":"http://arxiv.org/abs/2312.01537v1"}
{"created":"2023-12-03 23:03:22","title":"T3D: Towards 3D Medical Image Understanding through Vision-Language Pre-training","abstract":"Expert annotation of 3D medical image for downstream analysis is resource-intensive, posing challenges in clinical applications. Visual self-supervised learning (vSSL), though effective for learning visual invariance, neglects the incorporation of domain knowledge from medicine. To incorporate medical knowledge into visual representation learning, vision-language pre-training (VLP) has shown promising results in 2D image. However, existing VLP approaches become generally impractical when applied to high-resolution 3D medical images due to GPU hardware constraints and the potential loss of critical details caused by downsampling, which is the intuitive solution to hardware constraints. To address the above limitations, we introduce T3D, the first VLP framework designed for high-resolution 3D medical images. T3D incorporates two text-informed pretext tasks: (\\lowerromannumeral{1}) text-informed contrastive learning; (\\lowerromannumeral{2}) text-informed image restoration. These tasks focus on learning 3D visual representations from high-resolution 3D medical images and integrating clinical knowledge from radiology reports, without distorting information through forced alignment of downsampled volumes with detailed anatomical text. Trained on a newly curated large-scale dataset of 3D medical images and radiology reports, T3D significantly outperforms current vSSL methods in tasks like organ and tumor segmentation, as well as disease classification. This underlines T3D's potential in representation learning for 3D medical image analysis. All data and code will be available upon acceptance.","sentences":["Expert annotation of 3D medical image for downstream analysis is resource-intensive, posing challenges in clinical applications.","Visual self-supervised learning (vSSL), though effective for learning visual invariance, neglects the incorporation of domain knowledge from medicine.","To incorporate medical knowledge into visual representation learning, vision-language pre-training (VLP) has shown promising results in 2D image.","However, existing VLP approaches become generally impractical when applied to high-resolution 3D medical images due to GPU hardware constraints and the potential loss of critical details caused by downsampling, which is the intuitive solution to hardware constraints.","To address the above limitations, we introduce T3D, the first VLP framework designed for high-resolution 3D medical images.","T3D incorporates two text-informed pretext tasks: (\\lowerromannumeral{1}) text-informed contrastive learning; (\\lowerromannumeral{2}) text-informed image restoration.","These tasks focus on learning 3D visual representations from high-resolution 3D medical images and integrating clinical knowledge from radiology reports, without distorting information through forced alignment of downsampled volumes with detailed anatomical text.","Trained on a newly curated large-scale dataset of 3D medical images and radiology reports, T3D significantly outperforms current vSSL methods in tasks like organ and tumor segmentation, as well as disease classification.","This underlines T3D's potential in representation learning for 3D medical image analysis.","All data and code will be available upon acceptance."],"url":"http://arxiv.org/abs/2312.01529v1"}
{"created":"2023-12-03 22:44:04","title":"G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training","abstract":"Recently, medical vision-language pre-training (VLP) has reached substantial progress to learn global visual representation from medical images and their paired radiology reports. However, medical imaging tasks in real world usually require finer granularity in visual features. These tasks include visual localization tasks (e.g., semantic segmentation, object detection) and visual grounding task. Yet, current medical VLP methods face challenges in learning these fine-grained features, as they primarily focus on brute-force alignment between image patches and individual text tokens for local visual feature learning, which is suboptimal for downstream dense prediction tasks. In this work, we propose a new VLP framework, named \\textbf{G}lobal to \\textbf{D}ense level representation learning (G2D) that achieves significantly improved granularity and more accurate grounding for the learned features, compared to existing medical VLP approaches. In particular, G2D learns dense and semantically-grounded image representations via a pseudo segmentation task parallel with the global vision-language alignment. Notably, generating pseudo segmentation targets does not incur extra trainable parameters: they are obtained on the fly during VLP with a parameter-free processor. G2D achieves superior performance across 6 medical imaging tasks and 25 diseases, particularly in semantic segmentation, which necessitates fine-grained, semantically-grounded image features. In this task, G2D surpasses peer models even when fine-tuned with just 1\\% of the training data, compared to the 100\\% used by these models. The code will be released upon acceptance.","sentences":["Recently, medical vision-language pre-training (VLP) has reached substantial progress to learn global visual representation from medical images and their paired radiology reports.","However, medical imaging tasks in real world usually require finer granularity in visual features.","These tasks include visual localization tasks (e.g., semantic segmentation, object detection) and visual grounding task.","Yet, current medical VLP methods face challenges in learning these fine-grained features, as they primarily focus on brute-force alignment between image patches and individual text tokens for local visual feature learning, which is suboptimal for downstream dense prediction tasks.","In this work, we propose a new VLP framework, named \\textbf{G}lobal to \\textbf{D}ense level representation learning (G2D) that achieves significantly improved granularity and more accurate grounding for the learned features, compared to existing medical VLP approaches.","In particular, G2D learns dense and semantically-grounded image representations via a pseudo segmentation task parallel with the global vision-language alignment.","Notably, generating pseudo segmentation targets does not incur extra trainable parameters: they are obtained on the fly during VLP with a parameter-free processor.","G2D achieves superior performance across 6 medical imaging tasks and 25 diseases, particularly in semantic segmentation, which necessitates fine-grained, semantically-grounded image features.","In this task, G2D surpasses peer models even when fine-tuned with just 1\\% of the training data, compared to the 100\\% used by these models.","The code will be released upon acceptance."],"url":"http://arxiv.org/abs/2312.01522v1"}
{"created":"2023-12-03 21:52:51","title":"SoK: The Gap Between Data Rights Ideals and Reality","abstract":"As information economies burgeon, they unlock innovation and economic wealth while posing novel threats to civil liberties and altering power dynamics between individuals, companies, and governments. Legislatures have reacted with privacy laws designed to empower individuals over their data. These laws typically create rights for \"data subjects\" (individuals) to make requests of data collectors (companies and governments). The European Union General Data Protection Regulation (GDPR) exemplifies this, granting extensive data rights to data subjects, a model embraced globally. However, the question remains: do these rights-based privacy laws effectively empower individuals over their data? This paper scrutinizes these approaches by reviewing 201 interdisciplinary empirical studies, news articles, and blog posts. We pinpoint 15 key questions concerning the efficacy of rights allocations. The literature often presents conflicting results regarding the effectiveness of rights-based frameworks, but it generally emphasizes their limitations. We offer recommendations to policymakers and Computer Science (CS) groups committed to these frameworks, and suggest alternative privacy regulation approaches.","sentences":["As information economies burgeon, they unlock innovation and economic wealth while posing novel threats to civil liberties and altering power dynamics between individuals, companies, and governments.","Legislatures have reacted with privacy laws designed to empower individuals over their data.","These laws typically create rights for \"data subjects\" (individuals) to make requests of data collectors (companies and governments).","The European Union General Data Protection Regulation (GDPR) exemplifies this, granting extensive data rights to data subjects, a model embraced globally.","However, the question remains: do these rights-based privacy laws effectively empower individuals over their data?","This paper scrutinizes these approaches by reviewing 201 interdisciplinary empirical studies, news articles, and blog posts.","We pinpoint 15 key questions concerning the efficacy of rights allocations.","The literature often presents conflicting results regarding the effectiveness of rights-based frameworks, but it generally emphasizes their limitations.","We offer recommendations to policymakers and Computer Science (CS) groups committed to these frameworks, and suggest alternative privacy regulation approaches."],"url":"http://arxiv.org/abs/2312.01511v1"}
{"created":"2023-12-03 20:21:08","title":"Normed Spaces for Graph Embedding","abstract":"Theoretical results from discrete geometry suggest that normed spaces can abstractly embed finite metric spaces with surprisingly low theoretical bounds on distortion in low dimensions. In this paper, inspired by this theoretical insight, we highlight normed spaces as a more flexible and computationally efficient alternative to several popular Riemannian manifolds for learning graph embeddings. Normed space embeddings significantly outperform several popular manifolds on a large range of synthetic and real-world graph reconstruction benchmark datasets while requiring significantly fewer computational resources. We also empirically verify the superiority of normed space embeddings on growing families of graphs associated with negative, zero, and positive curvature, further reinforcing the flexibility of normed spaces in capturing diverse graph structures as graph sizes increase. Lastly, we demonstrate the utility of normed space embeddings on two applied graph embedding tasks, namely, link prediction and recommender systems. Our work highlights the potential of normed spaces for geometric graph representation learning, raises new research questions, and offers a valuable tool for experimental mathematics in the field of finite metric space embeddings. We make our code and data publically available.","sentences":["Theoretical results from discrete geometry suggest that normed spaces can abstractly embed finite metric spaces with surprisingly low theoretical bounds on distortion in low dimensions.","In this paper, inspired by this theoretical insight, we highlight normed spaces as a more flexible and computationally efficient alternative to several popular Riemannian manifolds for learning graph embeddings.","Normed space embeddings significantly outperform several popular manifolds on a large range of synthetic and real-world graph reconstruction benchmark datasets while requiring significantly fewer computational resources.","We also empirically verify the superiority of normed space embeddings on growing families of graphs associated with negative, zero, and positive curvature, further reinforcing the flexibility of normed spaces in capturing diverse graph structures as graph sizes increase.","Lastly, we demonstrate the utility of normed space embeddings on two applied graph embedding tasks, namely, link prediction and recommender systems.","Our work highlights the potential of normed spaces for geometric graph representation learning, raises new research questions, and offers a valuable tool for experimental mathematics in the field of finite metric space embeddings.","We make our code and data publically available."],"url":"http://arxiv.org/abs/2312.01502v1"}
{"created":"2023-12-03 19:07:30","title":"ADT: Agent-based Dynamic Thresholding for Anomaly Detection","abstract":"The complexity and scale of IT systems are increasing dramatically, posing many challenges to real-world anomaly detection. Deep learning anomaly detection has emerged, aiming at feature learning and anomaly scoring, which has gained tremendous success. However, little work has been done on the thresholding problem despite it being a critical factor for the effectiveness of anomaly detection. In this paper, we model thresholding in anomaly detection as a Markov Decision Process and propose an agent-based dynamic thresholding (ADT) framework based on a deep Q-network. The proposed method can be integrated into many systems that require dynamic thresholding. An auto-encoder is utilized in this study to obtain feature representations and produce anomaly scores for complex input data. ADT can adjust thresholds adaptively by utilizing the anomaly scores from the auto-encoder and significantly improve anomaly detection performance. The properties of ADT are studied through experiments on three real-world datasets and compared with benchmarks, hence demonstrating its thresholding capability, data-efficient learning, stability, and robustness. Our study validates the effectiveness of reinforcement learning in optimal thresholding control in anomaly detection.","sentences":["The complexity and scale of IT systems are increasing dramatically, posing many challenges to real-world anomaly detection.","Deep learning anomaly detection has emerged, aiming at feature learning and anomaly scoring, which has gained tremendous success.","However, little work has been done on the thresholding problem despite it being a critical factor for the effectiveness of anomaly detection.","In this paper, we model thresholding in anomaly detection as a Markov Decision Process and propose an agent-based dynamic thresholding (ADT) framework based on a deep Q-network.","The proposed method can be integrated into many systems that require dynamic thresholding.","An auto-encoder is utilized in this study to obtain feature representations and produce anomaly scores for complex input data.","ADT can adjust thresholds adaptively by utilizing the anomaly scores from the auto-encoder and significantly improve anomaly detection performance.","The properties of ADT are studied through experiments on three real-world datasets and compared with benchmarks, hence demonstrating its thresholding capability, data-efficient learning, stability, and robustness.","Our study validates the effectiveness of reinforcement learning in optimal thresholding control in anomaly detection."],"url":"http://arxiv.org/abs/2312.01488v1"}
{"created":"2023-12-03 19:02:52","title":"BetterMinton Service: Analyzing the Badminton Service using Open Kinetic Chain","abstract":"We present a badminton training system that focuses on the backhand short service. Unlike the prior motor skill training systems which focus on the trainee's posture, our system analyzes the process of moving joints with the open kinetic chain (OKC), which helps align movement and minimize muscle use for better joint control. We process the users' mocap data to visually show their last service process comparing to 4 ideal OKC characteristics that we collected from a 6-sub-elite formative study as well as recommended contact posture. We validate our system through a 12-user study that measures serving accuracy, qualitative feedback, and skeletal data with users at various skill levels and open source our skeletal analysis model for future use. While the participants' overall service accuracy was not significantly improved, our results show that our system helps participants in the short term to fine-tune their service motion closer to our ideal 4 OKC characteristics.","sentences":["We present a badminton training system that focuses on the backhand short service.","Unlike the prior motor skill training systems which focus on the trainee's posture, our system analyzes the process of moving joints with the open kinetic chain (OKC), which helps align movement and minimize muscle use for better joint control.","We process the users' mocap data to visually show their last service process comparing to 4 ideal OKC characteristics that we collected from a 6-sub-elite formative study as well as recommended contact posture.","We validate our system through a 12-user study that measures serving accuracy, qualitative feedback, and skeletal data with users at various skill levels and open source our skeletal analysis model for future use.","While the participants' overall service accuracy was not significantly improved, our results show that our system helps participants in the short term to fine-tune their service motion closer to our ideal 4 OKC characteristics."],"url":"http://arxiv.org/abs/2312.01487v1"}
{"created":"2023-12-03 18:41:54","title":"OpenVoice: Versatile Instant Voice Cloning","abstract":"We introduce OpenVoice, a versatile voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. OpenVoice represents a significant advancement in addressing the following open challenges in the field: 1) Flexible Voice Style Control. OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. The voice styles are not directly copied from and constrained by the style of the reference speaker. Previous approaches lacked the ability to flexibly manipulate voice styles after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set. Unlike previous approaches, which typically require extensive massive-speaker multi-lingual (MSML) dataset for all languages, OpenVoice can clone voices into a new language without any massive-speaker training data for that language. OpenVoice is also computationally efficient, costing tens of times less than commercially available APIs that offer even inferior performance. To foster further research in the field, we have made the source code and trained model publicly accessible. We also provide qualitative results in our demo website. Prior to its public release, our internal version of OpenVoice was used tens of millions of times by users worldwide between May and October 2023, serving as the backend of MyShell.ai.","sentences":["We introduce OpenVoice, a versatile voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages.","OpenVoice represents a significant advancement in addressing the following open challenges in the field: 1) Flexible Voice Style Control.","OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker.","The voice styles are not directly copied from and constrained by the style of the reference speaker.","Previous approaches lacked the ability to flexibly manipulate voice styles after cloning.","2) Zero-Shot Cross-Lingual Voice Cloning.","OpenVoice achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set.","Unlike previous approaches, which typically require extensive massive-speaker multi-lingual (MSML) dataset for all languages, OpenVoice can clone voices into a new language without any massive-speaker training data for that language.","OpenVoice is also computationally efficient, costing tens of times less than commercially available APIs that offer even inferior performance.","To foster further research in the field, we have made the source code and trained model publicly accessible.","We also provide qualitative results in our demo website.","Prior to its public release, our internal version of OpenVoice was used tens of millions of times by users worldwide between May and October 2023, serving as the backend of MyShell.ai."],"url":"http://arxiv.org/abs/2312.01479v1"}
{"created":"2023-12-03 18:23:48","title":"Context-Enhanced Relational Operators with Vector Embeddings","abstract":"Collecting data, extracting value, and combining insights from relational and context-rich multi-modal sources in data processing pipelines presents a challenge for traditional relational DBMS. While relational operators allow declarative and optimizable query specification, they are limited to data transformations unsuitable for capturing or analyzing context. On the other hand, representation learning models can map context-rich data into embeddings, allowing machine-automated context processing but requiring imperative data transformation integration with the analytical query.   To bridge this dichotomy, we present a context-enhanced relational join and introduce an embedding operator composable with relational operators. This enables hybrid relational and context-rich vector data processing, with algebraic equivalences compatible with relational algebra and corresponding logical and physical optimizations. We investigate model-operator interaction with vector data processing and study the characteristics of the E-join operator. Using an example of string embeddings, we demonstrate enabling hybrid context-enhanced processing on relational join operators with vector embeddings. The importance of holistic optimization, from logical to physical, is demonstrated in an order of magnitude execution time improvement.","sentences":["Collecting data, extracting value, and combining insights from relational and context-rich multi-modal sources in data processing pipelines presents a challenge for traditional relational DBMS.","While relational operators allow declarative and optimizable query specification, they are limited to data transformations unsuitable for capturing or analyzing context.","On the other hand, representation learning models can map context-rich data into embeddings, allowing machine-automated context processing but requiring imperative data transformation integration with the analytical query.   ","To bridge this dichotomy, we present a context-enhanced relational join and introduce an embedding operator composable with relational operators.","This enables hybrid relational and context-rich vector data processing, with algebraic equivalences compatible with relational algebra and corresponding logical and physical optimizations.","We investigate model-operator interaction with vector data processing and study the characteristics of the E-join operator.","Using an example of string embeddings, we demonstrate enabling hybrid context-enhanced processing on relational join operators with vector embeddings.","The importance of holistic optimization, from logical to physical, is demonstrated in an order of magnitude execution time improvement."],"url":"http://arxiv.org/abs/2312.01476v1"}
{"created":"2023-12-03 17:48:40","title":"Exploring Adversarial Robustness of LiDAR-Camera Fusion Model in Autonomous Driving","abstract":"Our study assesses the adversarial robustness of LiDAR-camera fusion models in 3D object detection. We introduce an attack technique that, by simply adding a limited number of physically constrained adversarial points above a car, can make the car undetectable by the fusion model. Experimental results reveal that even without changes to the image data channel, the fusion model can be deceived solely by manipulating the LiDAR data channel. This finding raises safety concerns in the field of autonomous driving. Further, we explore how the quantity of adversarial points, the distance between the front-near car and the LiDAR-equipped car, and various angular factors affect the attack success rate. We believe our research can contribute to the understanding of multi-sensor robustness, offering insights and guidance to enhance the safety of autonomous driving.","sentences":["Our study assesses the adversarial robustness of LiDAR-camera fusion models in 3D object detection.","We introduce an attack technique that, by simply adding a limited number of physically constrained adversarial points above a car, can make the car undetectable by the fusion model.","Experimental results reveal that even without changes to the image data channel, the fusion model can be deceived solely by manipulating the LiDAR data channel.","This finding raises safety concerns in the field of autonomous driving.","Further, we explore how the quantity of adversarial points, the distance between the front-near car and the LiDAR-equipped car, and various angular factors affect the attack success rate.","We believe our research can contribute to the understanding of multi-sensor robustness, offering insights and guidance to enhance the safety of autonomous driving."],"url":"http://arxiv.org/abs/2312.01468v1"}
{"created":"2023-12-03 16:24:50","title":"Looking Inside Out: Anticipating Driver Intent From Videos","abstract":"Anticipating driver intention is an important task when vehicles of mixed and varying levels of human/machine autonomy share roadways. Driver intention can be leveraged to improve road safety, such as warning surrounding vehicles in the event the driver is attempting a dangerous maneuver. In this work, we propose a novel method of utilizing in-cabin and external camera data to improve state-of-the-art (SOTA) performance in predicting future driver actions. Compared to existing methods, our approach explicitly extracts object and road-level features from external camera data, which we demonstrate are important features for predicting driver intention. Using our handcrafted features as inputs for both a transformer and an LSTM-based architecture, we empirically show that jointly utilizing in-cabin and external features improves performance compared to using in-cabin features alone. Furthermore, our models predict driver maneuvers more accurately and earlier than existing approaches, with an accuracy of 87.5% and an average prediction time of 4.35 seconds before the maneuver takes place. We release our model configurations and training scripts on https://github.com/ykung83/Driver-Intent-Prediction","sentences":["Anticipating driver intention is an important task when vehicles of mixed and varying levels of human/machine autonomy share roadways.","Driver intention can be leveraged to improve road safety, such as warning surrounding vehicles in the event the driver is attempting a dangerous maneuver.","In this work, we propose a novel method of utilizing in-cabin and external camera data to improve state-of-the-art (SOTA) performance in predicting future driver actions.","Compared to existing methods, our approach explicitly extracts object and road-level features from external camera data, which we demonstrate are important features for predicting driver intention.","Using our handcrafted features as inputs for both a transformer and an LSTM-based architecture, we empirically show that jointly utilizing in-cabin and external features improves performance compared to using in-cabin features alone.","Furthermore, our models predict driver maneuvers more accurately and earlier than existing approaches, with an accuracy of 87.5% and an average prediction time of 4.35 seconds before the maneuver takes place.","We release our model configurations and training scripts on https://github.com/ykung83/Driver-Intent-Prediction"],"url":"http://arxiv.org/abs/2312.01444v1"}
{"created":"2023-12-03 15:40:10","title":"D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for Few-shot Action Recognition","abstract":"Adapting large pre-trained image models to few-shot action recognition has proven to be an effective and efficient strategy for learning robust feature extractors, which is essential for few-shot learning. Typical fine-tuning based adaptation paradigm is prone to overfitting in the few-shot learning scenarios and offers little modeling flexibility for learning temporal features in video data. In this work we present the Disentangled-and-Deformable Spatio-Temporal Adapter (D$^2$ST-Adapter), a novel adapter tuning framework for few-shot action recognition, which is designed in a dual-pathway architecture to encode spatial and temporal features in a disentangled manner. Furthermore, we devise the Deformable Spatio-Temporal Attention module as the core component of D$^2$ST-Adapter, which can be tailored to model both spatial and temporal features in corresponding pathways, allowing our D$^2$ST-Adapter to encode features in a global view in 3D spatio-temporal space while maintaining a lightweight design. Extensive experiments with instantiations of our method on both pre-trained ResNet and ViT demonstrate the superiority of our method over state-of-the-art methods for few-shot action recognition. Our method is particularly well-suited to challenging scenarios where temporal dynamics are critical for action recognition.","sentences":["Adapting large pre-trained image models to few-shot action recognition has proven to be an effective and efficient strategy for learning robust feature extractors, which is essential for few-shot learning.","Typical fine-tuning based adaptation paradigm is prone to overfitting in the few-shot learning scenarios and offers little modeling flexibility for learning temporal features in video data.","In this work we present the Disentangled-and-Deformable Spatio-Temporal Adapter (D$^2$ST-Adapter), a novel adapter tuning framework for few-shot action recognition, which is designed in a dual-pathway architecture to encode spatial and temporal features in a disentangled manner.","Furthermore, we devise the Deformable Spatio-Temporal Attention module as the core component of D$^2$ST-Adapter, which can be tailored to model both spatial and temporal features in corresponding pathways, allowing our D$^2$ST-Adapter to encode features in a global view in 3D spatio-temporal space while maintaining a lightweight design.","Extensive experiments with instantiations of our method on both pre-trained ResNet and ViT demonstrate the superiority of our method over state-of-the-art methods for few-shot action recognition.","Our method is particularly well-suited to challenging scenarios where temporal dynamics are critical for action recognition."],"url":"http://arxiv.org/abs/2312.01431v1"}
{"created":"2023-12-03 15:34:46","title":"Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars","abstract":"Interpretability methods aim to understand the algorithm implemented by a trained model (e.g., a Transofmer) by examining various aspects of the model, such as the weight matrices or the attention patterns. In this work, through a combination of theoretical results and carefully controlled experiments on synthetic data, we take a critical view of methods that exclusively focus on individual parts of the model, rather than consider the network as a whole. We consider a simple synthetic setup of learning a (bounded) Dyck language. Theoretically, we show that the set of models that (exactly or approximately) solve this task satisfy a structural characterization derived from ideas in formal languages (the pumping lemma). We use this characterization to show that the set of optima is qualitatively rich; in particular, the attention pattern of a single layer can be ``nearly randomized'', while preserving the functionality of the network. We also show via extensive experiments that these constructions are not merely a theoretical artifact: even after severely constraining the architecture of the model, vastly different solutions can be reached via standard training. Thus, interpretability claims based on inspecting individual heads or weight matrices in the Transformer can be misleading.","sentences":["Interpretability methods aim to understand the algorithm implemented by a trained model (e.g., a Transofmer) by examining various aspects of the model, such as the weight matrices or the attention patterns.","In this work, through a combination of theoretical results and carefully controlled experiments on synthetic data, we take a critical view of methods that exclusively focus on individual parts of the model, rather than consider the network as a whole.","We consider a simple synthetic setup of learning a (bounded) Dyck language.","Theoretically, we show that the set of models that (exactly or approximately) solve this task satisfy a structural characterization derived from ideas in formal languages (the pumping lemma).","We use this characterization to show that the set of optima is qualitatively rich; in particular, the attention pattern of a single layer can be ``nearly randomized'', while preserving the functionality of the network.","We also show via extensive experiments that these constructions are not merely a theoretical artifact: even after severely constraining the architecture of the model, vastly different solutions can be reached via standard training.","Thus, interpretability claims based on inspecting individual heads or weight matrices in the Transformer can be misleading."],"url":"http://arxiv.org/abs/2312.01429v1"}
{"created":"2023-12-03 14:49:36","title":"Finding and counting small tournaments in large tournaments","abstract":"We present new algorithms for counting and detecting small tournaments in a given tournament. In particular, it is proved that every tournament on four vertices (there are four) can be detected in $O(n^2)$ time and counted in $O(n^\\omega)$ time where $\\omega < 2.373$ is the matrix multiplication exponent. It is also proved that any tournament on five vertices (there are $12$) can be counted in $O(n^{\\omega+1})$ time. As for lower-bounds, we prove that for almost all $k$-vertex tournaments, the complexity of the detection problem is not easier than the complexity of the corresponding well-studied counting problem for {\\em undirected cliques} of order $k-O(\\log k)$.","sentences":["We present new algorithms for counting and detecting small tournaments in a given tournament.","In particular, it is proved that every tournament on four vertices (there are four) can be detected in $O(n^2)$ time and counted in $O(n^\\omega)$ time where $\\omega < 2.373$ is the matrix multiplication exponent.","It is also proved that any tournament on five vertices (there are $12$) can be counted in $O(n^{\\omega+1})$ time.","As for lower-bounds, we prove that for almost all $k$-vertex tournaments, the complexity of the detection problem is not easier than the complexity of the corresponding well-studied counting problem for {\\em undirected cliques} of order $k-O(\\log k)$."],"url":"http://arxiv.org/abs/2312.01419v1"}
