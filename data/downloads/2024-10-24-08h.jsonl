{"created":"2024-10-23 17:59:52","title":"Prioritized Generative Replay","abstract":"Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of \"relevance functions\" that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents.","sentences":["Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function.","However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning.","While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare.","In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience.","This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of \"relevance functions\" that push these generations towards more useful parts of an agent's acquired history.","We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics.","Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains.","We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting.","We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents."],"url":"http://arxiv.org/abs/2410.18082v1"}
{"created":"2024-10-23 17:58:49","title":"ALTA: Compiler-Based Analysis of Transformers","abstract":"We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework -- language specification, symbolic interpreter, and weight compiler -- available to the community to enable further applications and insights.","sentences":["We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights.","ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights.","ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages.","ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps.","We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm.","To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal.","This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings.","We make the ALTA framework -- language specification, symbolic interpreter, and weight compiler -- available to the community to enable further applications and insights."],"url":"http://arxiv.org/abs/2410.18077v1"}
{"created":"2024-10-23 17:58:45","title":"Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration","abstract":"Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.","sentences":["Unsupervised pretraining has been transformative in many supervised domains.","However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement.","In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies.","While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration.","Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits.","Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples.","Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently.","We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks.","Code: https://github.com/rail-berkeley/supe."],"url":"http://arxiv.org/abs/2410.18076v1"}
{"created":"2024-10-23 17:57:14","title":"ProFL: Performative Robust Optimal Federated Learning","abstract":"Performative prediction (PP) is a framework that captures distribution shifts that occur during the training of machine learning models due to their deployment. As the trained model is used, its generated data could cause the model to evolve, leading to deviations from the original data distribution. The impact of such model-induced distribution shifts in the federated learning (FL) setup remains unexplored despite being increasingly likely to transpire in real-life use cases. Although Jin et al. (2024) recently extended PP to FL in a straightforward manner, the resulting model only converges to a performative stable point, which may be far from optimal. The methods in Izzo et al. (2021); Miller et al. (2021) can find a performative optimal point in centralized settings, but they require the performative risk to be convex and the training data to be noiseless, assumptions often violated in realistic FL systems. This paper overcomes all of these shortcomings and proposes Performative robust optimal Federated Learning (ProFL), an algorithm that finds performative optimal points in FL from noisy and contaminated data. We present the convergence analysis under the Polyak-Lojasiewicz condition, which applies to non-convex objectives. Extensive experiments on multiple datasets validate our proposed algorithms' efficiency.","sentences":["Performative prediction (PP) is a framework that captures distribution shifts that occur during the training of machine learning models due to their deployment.","As the trained model is used, its generated data could cause the model to evolve, leading to deviations from the original data distribution.","The impact of such model-induced distribution shifts in the federated learning (FL) setup remains unexplored despite being increasingly likely to transpire in real-life use cases.","Although Jin et al. (2024) recently extended PP to FL in a straightforward manner, the resulting model only converges to a performative stable point, which may be far from optimal.","The methods in Izzo et al. (2021); Miller et al. (2021) can find a performative optimal point in centralized settings, but they require the performative risk to be convex and the training data to be noiseless, assumptions often violated in realistic FL systems.","This paper overcomes all of these shortcomings and proposes Performative robust optimal Federated Learning (ProFL), an algorithm that finds performative optimal points in FL from noisy and contaminated data.","We present the convergence analysis under the Polyak-Lojasiewicz condition, which applies to non-convex objectives.","Extensive experiments on multiple datasets validate our proposed algorithms' efficiency."],"url":"http://arxiv.org/abs/2410.18075v1"}
{"created":"2024-10-23 17:56:33","title":"UnCLe: Unsupervised Continual Learning of Depth Completion","abstract":"We propose UnCLe, a standardized benchmark for Unsupervised Continual Learning of a multimodal depth estimation task: Depth completion aims to infer a dense depth map from a pair of synchronized RGB image and sparse depth map. We benchmark depth completion models under the practical scenario of unsupervised learning over continuous streams of data. Existing methods are typically trained on a static, or stationary, dataset. However, when adapting to novel non-stationary distributions, they \"catastrophically forget\" previously learned information. UnCLe simulates these non-stationary distributions by adapting depth completion models to sequences of datasets containing diverse scenes captured from distinct domains using different visual and range sensors. We adopt representative methods from continual learning paradigms and translate them to enable unsupervised continual learning of depth completion. We benchmark these models for indoor and outdoor and investigate the degree of catastrophic forgetting through standard quantitative metrics. Furthermore, we introduce model inversion quality as an additional measure of forgetting. We find that unsupervised continual learning of depth completion is an open problem, and we invite researchers to leverage UnCLe as a development platform.","sentences":["We propose UnCLe, a standardized benchmark for Unsupervised Continual Learning of a multimodal depth estimation task: Depth completion aims to infer a dense depth map from a pair of synchronized RGB image and sparse depth map.","We benchmark depth completion models under the practical scenario of unsupervised learning over continuous streams of data.","Existing methods are typically trained on a static, or stationary, dataset.","However, when adapting to novel non-stationary distributions, they \"catastrophically forget\" previously learned information.","UnCLe simulates these non-stationary distributions by adapting depth completion models to sequences of datasets containing diverse scenes captured from distinct domains using different visual and range sensors.","We adopt representative methods from continual learning paradigms and translate them to enable unsupervised continual learning of depth completion.","We benchmark these models for indoor and outdoor and investigate the degree of catastrophic forgetting through standard quantitative metrics.","Furthermore, we introduce model inversion quality as an additional measure of forgetting.","We find that unsupervised continual learning of depth completion is an open problem, and we invite researchers to leverage UnCLe as a development platform."],"url":"http://arxiv.org/abs/2410.18074v1"}
{"created":"2024-10-23 17:53:11","title":"Training Free Guided Flow Matching with Optimal Control","abstract":"Controlled generation with pre-trained Diffusion and Flow Matching models has vast applications. One strategy for guiding ODE-based generative models is through optimizing a target loss $R(x_1)$ while staying close to the prior distribution. Along this line, some recent work showed the effectiveness of guiding flow model by differentiating through its ODE sampling process. Despite the superior performance, the theoretical understanding of this line of methods is still preliminary, leaving space for algorithm improvement. Moreover, existing methods predominately focus on Euclidean data manifold, and there is a compelling need for guided flow methods on complex geometries such as SO(3), which prevails in high-stake scientific applications like protein design. We present OC-Flow, a general and theoretically grounded training-free framework for guided flow matching using optimal control. Building upon advances in optimal control theory, we develop effective and practical algorithms for solving optimal control in guided ODE-based generation and provide a systematic theoretical analysis of the convergence guarantee in both Euclidean and SO(3). We show that existing backprop-through-ODE methods can be interpreted as special cases of Euclidean OC-Flow. OC-Flow achieved superior performance in extensive experiments on text-guided image manipulation, conditional molecule generation, and all-atom peptide design.","sentences":["Controlled generation with pre-trained Diffusion and Flow Matching models has vast applications.","One strategy for guiding ODE-based generative models is through optimizing a target loss $R(x_1)$ while staying close to the prior distribution.","Along this line, some recent work showed the effectiveness of guiding flow model by differentiating through its ODE sampling process.","Despite the superior performance, the theoretical understanding of this line of methods is still preliminary, leaving space for algorithm improvement.","Moreover, existing methods predominately focus on Euclidean data manifold, and there is a compelling need for guided flow methods on complex geometries such as SO(3), which prevails in high-stake scientific applications like protein design.","We present OC-Flow, a general and theoretically grounded training-free framework for guided flow matching using optimal control.","Building upon advances in optimal control theory, we develop effective and practical algorithms for solving optimal control in guided ODE-based generation and provide a systematic theoretical analysis of the convergence guarantee in both Euclidean and SO(3).","We show that existing backprop-through-ODE methods can be interpreted as special cases of Euclidean OC-Flow.","OC-Flow achieved superior performance in extensive experiments on text-guided image manipulation, conditional molecule generation, and all-atom peptide design."],"url":"http://arxiv.org/abs/2410.18070v1"}
{"created":"2024-10-23 17:42:07","title":"SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation","abstract":"Robot learning has proven to be a general and effective technique for programming manipulators. Imitation learning is able to teach robots solely from human demonstrations but is bottlenecked by the capabilities of the demonstrations. Reinforcement learning uses exploration to discover better behaviors; however, the space of possible improvements can be too large to start from scratch. And for both techniques, the learning difficulty increases proportional to the length of the manipulation task. Accounting for this, we propose SPIRE, a system that first uses Task and Motion Planning (TAMP) to decompose tasks into smaller learning subproblems and second combines imitation and reinforcement learning to maximize their strengths. We develop novel strategies to train learning agents when deployed in the context of a planning system. We evaluate SPIRE on a suite of long-horizon and contact-rich robot manipulation problems. We find that SPIRE outperforms prior approaches that integrate imitation learning, reinforcement learning, and planning by 35% to 50% in average task performance, is 6 times more data efficient in the number of human demonstrations needed to train proficient agents, and learns to complete tasks nearly twice as efficiently. View https://sites.google.com/view/spire-corl-2024 for more details.","sentences":["Robot learning has proven to be a general and effective technique for programming manipulators.","Imitation learning is able to teach robots solely from human demonstrations but is bottlenecked by the capabilities of the demonstrations.","Reinforcement learning uses exploration to discover better behaviors; however, the space of possible improvements can be too large to start from scratch.","And for both techniques, the learning difficulty increases proportional to the length of the manipulation task.","Accounting for this, we propose SPIRE, a system that first uses Task and Motion Planning (TAMP) to decompose tasks into smaller learning subproblems and second combines imitation and reinforcement learning to maximize their strengths.","We develop novel strategies to train learning agents when deployed in the context of a planning system.","We evaluate SPIRE on a suite of long-horizon and contact-rich robot manipulation problems.","We find that SPIRE outperforms prior approaches that integrate imitation learning, reinforcement learning, and planning by 35% to 50% in average task performance, is 6 times more data efficient in the number of human demonstrations needed to train proficient agents, and learns to complete tasks nearly twice as efficiently.","View https://sites.google.com/view/spire-corl-2024 for more details."],"url":"http://arxiv.org/abs/2410.18065v1"}
{"created":"2024-10-23 17:30:50","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","abstract":"Machine Unlearning (MU) is critical for enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs), by removing specific private or hazardous information. While MU has made significant progress in textual and visual modalities, multimodal unlearning (MMU) remains significantly underexplored, partially due to the absence of a suitable open-source benchmark. To address this, we introduce CLEAR, a new benchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling a thorough evaluation across modalities. We assess 10 MU methods, adapting them for MMU, and highlight new challenges specific to multimodal forgetting. We also demonstrate that simple $\\ell_1$ regularization on LoRA weights significantly mitigates catastrophic forgetting, preserving model performance on retained data. The dataset is available at https://huggingface.co/datasets/therem/CLEAR","sentences":["Machine Unlearning (MU) is critical for enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs), by removing specific private or hazardous information.","While MU has made significant progress in textual and visual modalities, multimodal unlearning (MMU) remains significantly underexplored, partially due to the absence of a suitable open-source benchmark.","To address this, we introduce CLEAR, a new benchmark designed to evaluate MMU methods.","CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling a thorough evaluation across modalities.","We assess 10 MU methods, adapting them for MMU, and highlight new challenges specific to multimodal forgetting.","We also demonstrate that simple $\\ell_1$ regularization on LoRA weights significantly mitigates catastrophic forgetting, preserving model performance on retained data.","The dataset is available at https://huggingface.co/datasets/therem/CLEAR"],"url":"http://arxiv.org/abs/2410.18057v1"}
{"created":"2024-10-23 17:24:58","title":"LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering","abstract":"Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the \"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG's understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system's components and fine-tuning strategies. Data and code are available at https://github.com/QingFei1/LongRAG.","sentences":["Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions.","Existing long-context Large Language Models (LLMs) for LCQA often struggle with the \"lost in the middle\" issue.","Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence.","However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise.","To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG's understanding of complex long-context knowledge (i.e., global information and factual details).","We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs.","Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%).","Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system's components and fine-tuning strategies.","Data and code are available at https://github.com/QingFei1/LongRAG."],"url":"http://arxiv.org/abs/2410.18050v1"}
{"created":"2024-10-23 17:02:59","title":"GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration","abstract":"Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs' internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.","sentences":["Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing.","Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs' internal reasoning ability, resulting in suboptimal performance.","To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving.","By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis.","GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems.","Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question.","(3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming.","Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy.","The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam."],"url":"http://arxiv.org/abs/2410.18032v1"}
{"created":"2024-10-23 17:00:13","title":"Cross-lingual Transfer of Reward Models in Multilingual Alignment","abstract":"Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments. In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English. Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4% average increase in Multilingual RewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through the representation shifts. Finally, we perform multilingual alignment to exemplify how cross-lingual transfer in RM propagates to enhanced multilingual instruction-following capability, along with extensive analyses on off-the-shelf RMs. We release the code, model, and data.","sentences":["Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs).","However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments.","In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English.","Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4% average increase in Multilingual RewardBench.","Furthermore, we analyze the cross-lingual transfer of RMs through the representation shifts.","Finally, we perform multilingual alignment to exemplify how cross-lingual transfer in RM propagates to enhanced multilingual instruction-following capability, along with extensive analyses on off-the-shelf RMs.","We release the code, model, and data."],"url":"http://arxiv.org/abs/2410.18027v1"}
{"created":"2024-10-23 16:25:36","title":"Inferring stability properties of chaotic systems on autoencoders' latent spaces","abstract":"The data-driven learning of solutions of partial differential equations can be based on a divide-and-conquer strategy. First, the high dimensional data is compressed to a latent space with an autoencoder; and, second, the temporal dynamics are inferred on the latent space with a form of recurrent neural network. In chaotic systems and turbulence, convolutional autoencoders and echo state networks (CAE-ESN) successfully forecast the dynamics, but little is known about whether the stability properties can also be inferred. We show that the CAE-ESN model infers the invariant stability properties and the geometry of the tangent space in the low-dimensional manifold (i.e. the latent space) through Lyapunov exponents and covariant Lyapunov vectors. This work opens up new opportunities for inferring the stability of high-dimensional chaotic systems in latent spaces.","sentences":["The data-driven learning of solutions of partial differential equations can be based on a divide-and-conquer strategy.","First, the high dimensional data is compressed to a latent space with an autoencoder; and, second, the temporal dynamics are inferred on the latent space with a form of recurrent neural network.","In chaotic systems and turbulence, convolutional autoencoders and echo state networks (CAE-ESN) successfully forecast the dynamics, but little is known about whether the stability properties can also be inferred.","We show that the CAE-ESN model infers the invariant stability properties and the geometry of the tangent space in the low-dimensional manifold (i.e. the latent space) through Lyapunov exponents and covariant Lyapunov vectors.","This work opens up new opportunities for inferring the stability of high-dimensional chaotic systems in latent spaces."],"url":"http://arxiv.org/abs/2410.18003v1"}
{"created":"2024-10-23 16:12:59","title":"Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices","abstract":"Analyzing the structure of sampled features from an input data distribution is challenging when constrained by limited measurements in both the number of inputs and features. Traditional approaches often rely on the eigenvalue spectrum of the sample covariance matrix derived from finite measurement matrices; however, these spectra are sensitive to the size of the measurement matrix, leading to biased insights. In this paper, we introduce a novel algorithm that provides unbiased estimates of the spectral moments of the kernel integral operator in the limit of infinite inputs and features from finitely sampled measurement matrices. Our method, based upon dynamic programming, is efficient and capable of estimating the moments of the operator spectrum. We demonstrate the accuracy of our estimator on radial basis function (RBF) kernels, highlighting its consistency with the theoretical spectra. Furthermore, we showcase the practical utility and robustness of our method in understanding the geometry of learned representations in neural networks.","sentences":["Analyzing the structure of sampled features from an input data distribution is challenging when constrained by limited measurements in both the number of inputs and features.","Traditional approaches often rely on the eigenvalue spectrum of the sample covariance matrix derived from finite measurement matrices; however, these spectra are sensitive to the size of the measurement matrix, leading to biased insights.","In this paper, we introduce a novel algorithm that provides unbiased estimates of the spectral moments of the kernel integral operator in the limit of infinite inputs and features from finitely sampled measurement matrices.","Our method, based upon dynamic programming, is efficient and capable of estimating the moments of the operator spectrum.","We demonstrate the accuracy of our estimator on radial basis function (RBF) kernels, highlighting its consistency with the theoretical spectra.","Furthermore, we showcase the practical utility and robustness of our method in understanding the geometry of learned representations in neural networks."],"url":"http://arxiv.org/abs/2410.17998v1"}
{"created":"2024-10-23 16:12:03","title":"Characterization of the multiplicity of solutions for camera pose given two vertically-aligned landmarks and accelerometer","abstract":"We consider the problem of recovering the position and orientation of a camera equipped with an accelerometer from sensor images of two labeled landmarks whose positions in a coordinate system aligned in a known way with gravity are known. This a variant on the much studied P$n$P problem of recovering camera position and orientation from $n$ points without any gravitational data. It is proved that in three types of singular cases there are infinitely many solutions, in another type of case there is one, and in a final type of case there are two. A precise characterization of each type of case. In particular, there is always a unique solution in the practically interesting case where the two landmarks are at the same altitude and the camera is at a different altitude. This case is studied by numerical simulation and an implementation on a consumer cellphone. It is also proved that if the two landmarks are unlabeled, then apart from the same singular cases, there are still always one or two solutions.","sentences":["We consider the problem of recovering the position and orientation of a camera equipped with an accelerometer from sensor images of two labeled landmarks whose positions in a coordinate system aligned in a known way with gravity are known.","This a variant on the much studied P$n$P problem of recovering camera position and orientation from $n$ points without any gravitational data.","It is proved that in three types of singular cases there are infinitely many solutions, in another type of case there is one, and in a final type of case there are two.","A precise characterization of each type of case.","In particular, there is always a unique solution in the practically interesting case where the two landmarks are at the same altitude and the camera is at a different altitude.","This case is studied by numerical simulation and an implementation on a consumer cellphone.","It is also proved that if the two landmarks are unlabeled, then apart from the same singular cases, there are still always one or two solutions."],"url":"http://arxiv.org/abs/2410.17997v1"}
{"created":"2024-10-23 16:08:00","title":"AI driven health recommender","abstract":"As AI emerged as highest valued technology, We used that to create a web application that makes a patient work easier .It detects the disease name based on the symptoms given by the patient and recommends medication for respective disease, precautions to take, diet to follow and workouts to do, so the disease can be minimized. The web application is made with clean and Realtime data by using Machine learning as root. We used flask to create a user-friendly platform.","sentences":["As AI emerged as highest valued technology, We used that to create a web application that makes a patient work easier .It","detects the disease name based on the symptoms given by the patient and recommends medication for respective disease, precautions to take, diet to follow and workouts to do, so the disease can be minimized.","The web application is made with clean and Realtime data by using Machine learning as root.","We used flask to create a user-friendly platform."],"url":"http://arxiv.org/abs/2410.17991v1"}
{"created":"2024-10-23 16:04:05","title":"Striking a New Chord: Neural Networks in Music Information Dynamics","abstract":"Initiating a quest to unravel the complexities of musical aesthetics through the lens of information dynamics, our study delves into the realm of musical sequence modeling, drawing a parallel between the sequential structured nature of music and natural language.   Despite the prevalence of neural network models in MIR, the modeling of symbolic music events as applied to music cognition and music neuroscience has largely relied on statistical models. In this \"proof of concept\" paper we posit the superiority of neural network models over statistical models for predicting musical events. Specifically, we compare LSTM, Transformer, and GPT models against a widely-used markov model to predict a chord event following a sequence of chords.   Utilizing chord sequences from the McGill Billboard dataset, we trained each model to predict the next chord from a given sequence of chords. We found that neural models significantly outperformed statistical ones in our study. Specifically, the LSTM with attention model led with an accuracy of 0.85, followed by Transformer models at 0.58, Transformer with GPT head at 0.56, and standard LSTM at 0.43. Variable Order Markov and Markov trailed behind with accuracies of 0.31 and 0.23, respectively. Encouraged by these results, we extended our investigation to multidimensional modeling, employing a many-to-one LSTM, LSTM with attention, Transformer, and GPT predictors. These models were trained on both chord and melody lines as two-dimensional data using the CoCoPops Billboard dataset, achieving an accuracy of 0.21, 0.56, 0.39, and 0.24 respectively in predicting the next chord.","sentences":["Initiating a quest to unravel the complexities of musical aesthetics through the lens of information dynamics, our study delves into the realm of musical sequence modeling, drawing a parallel between the sequential structured nature of music and natural language.   ","Despite the prevalence of neural network models in MIR, the modeling of symbolic music events as applied to music cognition and music neuroscience has largely relied on statistical models.","In this \"proof of concept\" paper we posit the superiority of neural network models over statistical models for predicting musical events.","Specifically, we compare LSTM, Transformer, and GPT models against a widely-used markov model to predict a chord event following a sequence of chords.   ","Utilizing chord sequences from the McGill Billboard dataset, we trained each model to predict the next chord from a given sequence of chords.","We found that neural models significantly outperformed statistical ones in our study.","Specifically, the LSTM with attention model led with an accuracy of 0.85, followed by Transformer models at 0.58, Transformer with GPT head at 0.56, and standard LSTM at 0.43.","Variable Order Markov and Markov trailed behind with accuracies of 0.31 and 0.23, respectively.","Encouraged by these results, we extended our investigation to multidimensional modeling, employing a many-to-one LSTM, LSTM with attention, Transformer, and GPT predictors.","These models were trained on both chord and melody lines as two-dimensional data using the CoCoPops Billboard dataset, achieving an accuracy of 0.21, 0.56, 0.39, and 0.24 respectively in predicting the next chord."],"url":"http://arxiv.org/abs/2410.17989v1"}
{"created":"2024-10-23 16:01:31","title":"A Pipeline for Segmenting and Structuring RGB-D Data for Robotics Applications","abstract":"We introduce a novel pipeline for segmenting and structuring color and depth (RGB-D) data. Existing processing pipelines for RGB-D data have focused on extracting geometric information alone. This approach precludes the development of more advanced robotic navigation and manipulation algorithms, which benefit from a semantic understanding of their environment. Our pipeline can segment RGB-D data into accurate semantic masks. These masks are then used to fuse raw captured point clouds into semantically separated point clouds. We store this information using the Universal Scene Description (USD) file format, a format suitable for easy querying by downstream robotics algorithms, human-friendly visualization, and robotics simulation.","sentences":["We introduce a novel pipeline for segmenting and structuring color and depth (RGB-D) data.","Existing processing pipelines for RGB-D data have focused on extracting geometric information alone.","This approach precludes the development of more advanced robotic navigation and manipulation algorithms, which benefit from a semantic understanding of their environment.","Our pipeline can segment RGB-D data into accurate semantic masks.","These masks are then used to fuse raw captured point clouds into semantically separated point clouds.","We store this information using the Universal Scene Description (USD) file format, a format suitable for easy querying by downstream robotics algorithms, human-friendly visualization, and robotics simulation."],"url":"http://arxiv.org/abs/2410.17988v1"}
{"created":"2024-10-23 16:00:14","title":"Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data","abstract":"Federated Learning (FL) is an evolving paradigm that enables multiple parties to collaboratively train models without sharing raw data. Among its variants, Vertical Federated Learning (VFL) is particularly relevant in real-world, cross-organizational collaborations, where distinct features of a shared instance group are contributed by different parties. In these scenarios, parties are often linked using fuzzy identifiers, leading to a common practice termed as multi-party fuzzy VFL. Existing models generally address either multi-party VFL or fuzzy VFL between two parties. Extending these models to practical multi-party fuzzy VFL typically results in significant performance degradation and increased costs for maintaining privacy. To overcome these limitations, we introduce the Federated Transformer (FeT), a novel framework that supports multi-party VFL with fuzzy identifiers. FeT innovatively encodes these identifiers into data representations and employs a transformer architecture distributed across different parties, incorporating three new techniques to enhance performance. Furthermore, we have developed a multi-party privacy framework for VFL that integrates differential privacy with secure multi-party computation, effectively protecting local representations while minimizing associated utility costs. Our experiments demonstrate that the FeT surpasses the baseline models by up to 46\\% in terms of accuracy when scaled to 50 parties. Additionally, in two-party fuzzy VFL settings, FeT also shows improved performance and privacy over cutting-edge VFL models.","sentences":["Federated Learning (FL) is an evolving paradigm that enables multiple parties to collaboratively train models without sharing raw data.","Among its variants, Vertical Federated Learning (VFL) is particularly relevant in real-world, cross-organizational collaborations, where distinct features of a shared instance group are contributed by different parties.","In these scenarios, parties are often linked using fuzzy identifiers, leading to a common practice termed as multi-party fuzzy VFL.","Existing models generally address either multi-party VFL or fuzzy VFL between two parties.","Extending these models to practical multi-party fuzzy VFL typically results in significant performance degradation and increased costs for maintaining privacy.","To overcome these limitations, we introduce the Federated Transformer (FeT), a novel framework that supports multi-party VFL with fuzzy identifiers.","FeT innovatively encodes these identifiers into data representations and employs a transformer architecture distributed across different parties, incorporating three new techniques to enhance performance.","Furthermore, we have developed a multi-party privacy framework for VFL that integrates differential privacy with secure multi-party computation, effectively protecting local representations while minimizing associated utility costs.","Our experiments demonstrate that the FeT surpasses the baseline models by up to 46\\% in terms of accuracy when scaled to 50 parties.","Additionally, in two-party fuzzy VFL settings, FeT also shows improved performance and privacy over cutting-edge VFL models."],"url":"http://arxiv.org/abs/2410.17986v1"}
{"created":"2024-10-23 15:37:08","title":"Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages","abstract":"This exploratory study investigates the potential of multilingual Automatic Post-Editing (APE) systems to enhance the quality of machine translations for low-resource Indo-Aryan languages. Focusing on two closely related language pairs, English-Marathi and English-Hindi, we exploit the linguistic similarities to develop a robust multilingual APE model. To facilitate cross-linguistic transfer, we generate synthetic Hindi-Marathi and Marathi-Hindi APE triplets. Additionally, we incorporate a Quality Estimation (QE)-APE multi-task learning framework. While the experimental results underline the complementary nature of APE and QE, we also observe that QE-APE multitask learning facilitates effective domain adaptation. Our experiments demonstrate that the multilingual APE models outperform their corresponding English-Hindi and English-Marathi single-pair models by $2.5$ and $2.39$ TER points, respectively, with further notable improvements over the multilingual APE model observed through multi-task learning ($+1.29$ and $+1.44$ TER points), data augmentation ($+0.53$ and $+0.45$ TER points) and domain adaptation ($+0.35$ and $+0.45$ TER points). We release the synthetic data, code, and models accrued during this study publicly at https://github.com/cfiltnlp/Multilingual-APE.","sentences":["This exploratory study investigates the potential of multilingual Automatic Post-Editing (APE) systems to enhance the quality of machine translations for low-resource Indo-Aryan languages.","Focusing on two closely related language pairs, English-Marathi and English-Hindi, we exploit the linguistic similarities to develop a robust multilingual APE model.","To facilitate cross-linguistic transfer, we generate synthetic Hindi-Marathi and Marathi-Hindi APE triplets.","Additionally, we incorporate a Quality Estimation (QE)-APE multi-task learning framework.","While the experimental results underline the complementary nature of APE and QE, we also observe that QE-APE multitask learning facilitates effective domain adaptation.","Our experiments demonstrate that the multilingual APE models outperform their corresponding English-Hindi and English-Marathi single-pair models by $2.5$ and $2.39$ TER points, respectively, with further notable improvements over the multilingual APE model observed through multi-task learning ($+1.29$ and $+1.44$ TER points), data augmentation ($+0.53$ and $+0.45$ TER points) and domain adaptation ($+0.35$ and $+0.45$ TER points).","We release the synthetic data, code, and models accrued during this study publicly at https://github.com/cfiltnlp/Multilingual-APE."],"url":"http://arxiv.org/abs/2410.17973v1"}
{"created":"2024-10-23 15:36:43","title":"Dynamic Spectrum Access for Ambient Backscatter Communication-assisted D2D Systems with Quantum Reinforcement Learning","abstract":"Spectrum access is an essential problem in device-to-device (D2D) communications. However, with the recent growth in the number of mobile devices, the wireless spectrum is becoming scarce, resulting in low spectral efficiency for D2D communications. To address this problem, this paper aims to integrate the ambient backscatter communication technology into D2D devices to allow them to backscatter ambient RF signals to transmit their data when the shared spectrum is occupied by mobile users. To obtain the optimal spectrum access policy, i.e., stay idle or access the shared spectrum and perform active transmissions or backscattering ambient RF signals for transmissions, to maximize the average throughput for D2D users, deep reinforcement learning (DRL) can be adopted. However, DRL-based solutions may require long training time due to the curse of dimensionality issue as well as complex deep neural network architectures. For that, we develop a novel quantum reinforcement learning (RL) algorithm that can achieve a faster convergence rate with fewer training parameters compared to DRL thanks to the quantum superposition and quantum entanglement principles. Specifically, instead of using conventional deep neural networks, the proposed quantum RL algorithm uses a parametrized quantum circuit to approximate an optimal policy. Extensive simulations then demonstrate that the proposed solution not only can significantly improve the average throughput of D2D devices when the shared spectrum is busy but also can achieve much better performance in terms of convergence rate and learning complexity compared to existing DRL-based methods.","sentences":["Spectrum access is an essential problem in device-to-device (D2D) communications.","However, with the recent growth in the number of mobile devices, the wireless spectrum is becoming scarce, resulting in low spectral efficiency for D2D communications.","To address this problem, this paper aims to integrate the ambient backscatter communication technology into D2D devices to allow them to backscatter ambient RF signals to transmit their data when the shared spectrum is occupied by mobile users.","To obtain the optimal spectrum access policy, i.e., stay idle or access the shared spectrum and perform active transmissions or backscattering ambient RF signals for transmissions, to maximize the average throughput for D2D users, deep reinforcement learning (DRL) can be adopted.","However, DRL-based solutions may require long training time due to the curse of dimensionality issue as well as complex deep neural network architectures.","For that, we develop a novel quantum reinforcement learning (RL) algorithm that can achieve a faster convergence rate with fewer training parameters compared to DRL thanks to the quantum superposition and quantum entanglement principles.","Specifically, instead of using conventional deep neural networks, the proposed quantum RL algorithm uses a parametrized quantum circuit to approximate an optimal policy.","Extensive simulations then demonstrate that the proposed solution not only can significantly improve the average throughput of D2D devices when the shared spectrum is busy but also can achieve much better performance in terms of convergence rate and learning complexity compared to existing DRL-based methods."],"url":"http://arxiv.org/abs/2410.17971v1"}
{"created":"2024-10-23 15:36:08","title":"Optical Generative Models","abstract":"Generative models cover various application areas, including image, video and music synthesis, natural language processing, and molecular design, among many others. As digital generative models become larger, scalable inference in a fast and energy-efficient manner becomes a challenge. Here, we present optical generative models inspired by diffusion models, where a shallow and fast digital encoder first maps random noise into phase patterns that serve as optical generative seeds for a desired data distribution; a jointly-trained free-space-based reconfigurable decoder all-optically processes these generative seeds to create novel images (never seen before) following the target data distribution. Except for the illumination power and the random seed generation through a shallow encoder, these optical generative models do not consume computing power during the synthesis of novel images. We report the optical generation of monochrome and multi-color novel images of handwritten digits, fashion products, butterflies, and human faces, following the data distributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets, respectively, achieving an overall performance comparable to digital neural network-based generative models. To experimentally demonstrate optical generative models, we used visible light to generate, in a snapshot, novel images of handwritten digits and fashion products. These optical generative models might pave the way for energy-efficient, scalable and rapid inference tasks, further exploiting the potentials of optics and photonics for artificial intelligence-generated content.","sentences":["Generative models cover various application areas, including image, video and music synthesis, natural language processing, and molecular design, among many others.","As digital generative models become larger, scalable inference in a fast and energy-efficient manner becomes a challenge.","Here, we present optical generative models inspired by diffusion models, where a shallow and fast digital encoder first maps random noise into phase patterns that serve as optical generative seeds for a desired data distribution; a jointly-trained free-space-based reconfigurable decoder all-optically processes these generative seeds to create novel images (never seen before) following the target data distribution.","Except for the illumination power and the random seed generation through a shallow encoder, these optical generative models do not consume computing power during the synthesis of novel images.","We report the optical generation of monochrome and multi-color novel images of handwritten digits, fashion products, butterflies, and human faces, following the data distributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets, respectively, achieving an overall performance comparable to digital neural network-based generative models.","To experimentally demonstrate optical generative models, we used visible light to generate, in a snapshot, novel images of handwritten digits and fashion products.","These optical generative models might pave the way for energy-efficient, scalable and rapid inference tasks, further exploiting the potentials of optics and photonics for artificial intelligence-generated content."],"url":"http://arxiv.org/abs/2410.17970v1"}
{"created":"2024-10-23 15:27:59","title":"Lower Bounds for Convexity Testing","abstract":"We consider the problem of testing whether an unknown and arbitrary set $S \\subseteq \\mathbb{R}^n$ (given as a black-box membership oracle) is convex, versus $\\varepsilon$-far from every convex set, under the standard Gaussian distribution. The current state-of-the-art testing algorithms for this problem make $2^{\\tilde{O}(\\sqrt{n})\\cdot \\mathrm{poly}(1/\\varepsilon)}$ non-adaptive queries, both for the standard testing problem and for tolerant testing.   We give the first lower bounds for convexity testing in the black-box query model:   - We show that any one-sided tester (which may be adaptive) must use at least $n^{\\Omega(1)}$ queries in order to test to some constant accuracy $\\varepsilon>0$.   - We show that any non-adaptive tolerant tester (which may make two-sided errors) must use at least $2^{\\Omega(n^{1/4})}$ queries to distinguish sets that are $\\varepsilon_1$-close to convex versus $\\varepsilon_2$-far from convex, for some absolute constants $0<\\varepsilon_1<\\varepsilon_2$.   Finally, we also show that for any constant $c>0$, any non-adaptive tester (which may make two-sided errors) must use at least $n^{1/4 - c}$ queries in order to test to some constant accuracy $\\varepsilon>0$.","sentences":["We consider the problem of testing whether an unknown and arbitrary set $S \\subseteq \\mathbb{R}^n$ (given as a black-box membership oracle) is convex, versus $\\varepsilon$-far from every convex set, under the standard Gaussian distribution.","The current state-of-the-art testing algorithms for this problem make $2^{\\tilde{O}(\\sqrt{n})\\cdot \\mathrm{poly}(1/\\varepsilon)}$ non-adaptive queries, both for the standard testing problem and for tolerant testing.   ","We give the first lower bounds for convexity testing in the black-box query model:   - We show that any one-sided tester (which may be adaptive) must use at least $n^{\\Omega(1)}$ queries in order to test to some constant accuracy $\\varepsilon>0$.   - We show that any non-adaptive tolerant tester (which may make two-sided errors) must use at least $2^{\\Omega(n^{1/4})}$ queries to distinguish sets that are $\\varepsilon_1$-close to convex versus $\\varepsilon_2$-far from convex, for some absolute constants $0<\\varepsilon_1<\\varepsilon_2$.   Finally, we also show that for any constant $c>0$, any non-adaptive tester (which may make two-sided errors) must use at least $n^{1/4 - c}$ queries in order to test to some constant accuracy $\\varepsilon>0$."],"url":"http://arxiv.org/abs/2410.17958v1"}
{"created":"2024-10-23 15:24:16","title":"SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains","abstract":"Retrieval-augmented generation (RAG) enhances the question-answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips the LLM with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes the LLM on instruction-following, question-answering, and search-related data. Then, it prompts the same LLM to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples. By leveraging these synthetic examples, the LLM can improve their performance on domain-specific RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three domains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.","sentences":["Retrieval-augmented generation (RAG) enhances the question-answering (QA) abilities of large language models (LLMs) by integrating external knowledge.","However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data.","To tackle this, we propose SimRAG, a self-training approach that equips the LLM with joint capabilities of question answering and question generation for domain adaptation.","Our method first fine-tunes the LLM on instruction-following, question-answering, and search-related data.","Then, it prompts the same LLM to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples.","By leveraging these synthetic examples, the LLM can improve their performance on domain-specific RAG tasks.","Experiments on 11 datasets, spanning two backbone sizes and three domains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%."],"url":"http://arxiv.org/abs/2410.17952v1"}
{"created":"2024-10-23 15:18:07","title":"Theoretically Grounded Pruning of Large Ground Sets for Constrained, Discrete Optimization","abstract":"Modern instances of combinatorial optimization problems often exhibit billion-scale ground sets, which have many uninformative or redundant elements. In this work, we develop light-weight pruning algorithms to quickly discard elements that are unlikely to be part of an optimal solution. Under mild assumptions on the instance, we prove theoretical guarantees on the fraction of the optimal value retained and the size of the resulting pruned ground set. Through extensive experiments on real-world datasets for various applications, we demonstrate that our algorithm, QuickPrune, efficiently prunes over 90% of the ground set and outperforms state-of-the-art classical and machine learning heuristics for pruning.","sentences":["Modern instances of combinatorial optimization problems often exhibit billion-scale ground sets, which have many uninformative or redundant elements.","In this work, we develop light-weight pruning algorithms to quickly discard elements that are unlikely to be part of an optimal solution.","Under mild assumptions on the instance, we prove theoretical guarantees on the fraction of the optimal value retained and the size of the resulting pruned ground set.","Through extensive experiments on real-world datasets for various applications, we demonstrate that our algorithm, QuickPrune, efficiently prunes over 90% of the ground set and outperforms state-of-the-art classical and machine learning heuristics for pruning."],"url":"http://arxiv.org/abs/2410.17945v1"}
{"created":"2024-10-23 14:55:53","title":"Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning","abstract":"One of the biggest challenges of building artificial intelligence (AI) model in healthcare area is the data sharing. Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausted, costly, and sometimes impossible. In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America and Asia) while without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness. Technically, blockchain-enabled federated learning is implemented with adaption to make it meet with the privacy and safety requirements of healthcare data, meanwhile rewards honest participation and penalize malicious activities using its on-chain incentive mechanism. Experimental results show that the proposed framework is effective, efficient, and privacy preserved. Its prediction accuracy is much better than the models trained from limited personal data and is similar to, and even slightly better than, the results from a centralized dataset. This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity.","sentences":["One of the biggest challenges of building artificial intelligence (AI) model in healthcare area is the data sharing.","Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausted, costly, and sometimes impossible.","In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America and Asia) while without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness.","Technically, blockchain-enabled federated learning is implemented with adaption to make it meet with the privacy and safety requirements of healthcare data, meanwhile rewards honest participation and penalize malicious activities using its on-chain incentive mechanism.","Experimental results show that the proposed framework is effective, efficient, and privacy preserved.","Its prediction accuracy is much better than the models trained from limited personal data and is similar to, and even slightly better than, the results from a centralized dataset.","This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity."],"url":"http://arxiv.org/abs/2410.17933v1"}
{"created":"2024-10-23 14:47:12","title":"SJMalloc: the security-conscious, fast, thread-safe and memory-efficient heap allocator","abstract":"Heap-based exploits that leverage memory management errors continue to pose a significant threat to application security. The root cause of these vulnerabilities are the memory management errors within the applications, however various hardened allocator designs have been proposed as mitigation. A common feature of these designs is the strategic decision to store heap metadata separately from the application data in use, thereby reducing the risk of metadata corruption leading to security breaches. Despite their potential benefits, hardened allocators have not been widely adopted in real-world applications. The primary barrier to their adoption is the performance overheads they introduce. These overheads can negatively impact the efficiency and speed of applications, which is a critical consideration for developers and system administrators. Having learned from previous implementations, we developed SJMalloc, a general-purpose, high-performance allocator that addresses these concerns. SJMalloc stores its metadata out-of-band, away from the application's data on the heap. This design choice not only enhances security but also improves performance. Across a variety of real-world workloads, SJMalloc demonstrates a ~6% performance improvement compared to GLibcs allocator, while using only ~5% more memory. Furthermore, SJMalloc successfully passes the generic elements of the GLibc malloc testsuite and can thus be used as a drop-in replacement for the standard allocator, offering an easy upgrade path for enhanced security and performance without requiring changes to existing applications.","sentences":["Heap-based exploits that leverage memory management errors continue to pose a significant threat to application security.","The root cause of these vulnerabilities are the memory management errors within the applications, however various hardened allocator designs have been proposed as mitigation.","A common feature of these designs is the strategic decision to store heap metadata separately from the application data in use, thereby reducing the risk of metadata corruption leading to security breaches.","Despite their potential benefits, hardened allocators have not been widely adopted in real-world applications.","The primary barrier to their adoption is the performance overheads they introduce.","These overheads can negatively impact the efficiency and speed of applications, which is a critical consideration for developers and system administrators.","Having learned from previous implementations, we developed SJMalloc, a general-purpose, high-performance allocator that addresses these concerns.","SJMalloc stores its metadata out-of-band, away from the application's data on the heap.","This design choice not only enhances security but also improves performance.","Across a variety of real-world workloads, SJMalloc demonstrates a ~6% performance improvement compared to GLibcs allocator, while using only ~5% more memory.","Furthermore, SJMalloc successfully passes the generic elements of the GLibc malloc testsuite and can thus be used as a drop-in replacement for the standard allocator, offering an easy upgrade path for enhanced security and performance without requiring changes to existing applications."],"url":"http://arxiv.org/abs/2410.17928v1"}
{"created":"2024-10-23 14:38:57","title":"Gaze-Assisted Medical Image Segmentation","abstract":"The annotation of patient organs is a crucial part of various diagnostic and treatment procedures, such as radiotherapy planning. Manual annotation is extremely time-consuming, while its automation using modern image analysis techniques has not yet reached levels sufficient for clinical adoption. This paper investigates the idea of semi-supervised medical image segmentation using human gaze as interactive input for segmentation correction. In particular, we fine-tuned the Segment Anything Model in Medical Images (MedSAM), a public solution that uses various prompt types as additional input for semi-automated segmentation correction. We used human gaze data from reading abdominal images as a prompt for fine-tuning MedSAM. The model was validated on a public WORD database, which consists of 120 CT scans of 16 abdominal organs. The results of the gaze-assisted MedSAM were shown to be superior to the results of the state-of-the-art segmentation models. In particular, the average Dice coefficient for 16 abdominal organs was 85.8%, 86.7%, 81.7%, and 90.5% for nnUNetV2, ResUNet, original MedSAM, and our gaze-assisted MedSAM model, respectively.","sentences":["The annotation of patient organs is a crucial part of various diagnostic and treatment procedures, such as radiotherapy planning.","Manual annotation is extremely time-consuming, while its automation using modern image analysis techniques has not yet reached levels sufficient for clinical adoption.","This paper investigates the idea of semi-supervised medical image segmentation using human gaze as interactive input for segmentation correction.","In particular, we fine-tuned the Segment Anything Model in Medical Images (MedSAM), a public solution that uses various prompt types as additional input for semi-automated segmentation correction.","We used human gaze data from reading abdominal images as a prompt for fine-tuning MedSAM.","The model was validated on a public WORD database, which consists of 120 CT scans of 16 abdominal organs.","The results of the gaze-assisted MedSAM were shown to be superior to the results of the state-of-the-art segmentation models.","In particular, the average Dice coefficient for 16 abdominal organs was 85.8%, 86.7%, 81.7%, and 90.5% for nnUNetV2, ResUNet, original MedSAM, and our gaze-assisted MedSAM model, respectively."],"url":"http://arxiv.org/abs/2410.17920v1"}
{"created":"2024-10-23 14:34:39","title":"Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation","abstract":"Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks. However, in a temporal setting, multi-modal data are often inherently asynchronous. EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose. When clinical prediction is needed, the last available CXR image might have been outdated, leading to suboptimal predictions. To address this challenge, we propose DDL-CXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images. Our approach leverages latent diffusion models for patient-specific generation strategically conditioned on a previous CXR image and EHR time series, providing information regarding anatomical structures and disease progressions, respectively. In this way, the interaction across modalities could be better captured by the latent CXR generation process, ultimately improving the prediction performance. Experiments using MIMIC datasets show that the proposed model could effectively address asynchronicity in multimodal fusion and consistently outperform existing methods.","sentences":["Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks.","However, in a temporal setting, multi-modal data are often inherently asynchronous.","EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose.","When clinical prediction is needed, the last available CXR image might have been outdated, leading to suboptimal predictions.","To address this challenge, we propose DDL-CXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images.","Our approach leverages latent diffusion models for patient-specific generation strategically conditioned on a previous CXR image and EHR time series, providing information regarding anatomical structures and disease progressions, respectively.","In this way, the interaction across modalities could be better captured by the latent CXR generation process, ultimately improving the prediction performance.","Experiments using MIMIC datasets show that the proposed model could effectively address asynchronicity in multimodal fusion and consistently outperform existing methods."],"url":"http://arxiv.org/abs/2410.17918v1"}
{"created":"2024-10-23 14:33:11","title":"Deep learning for model correction of dynamical systems with data scarcity","abstract":"We present a deep learning framework for correcting existing dynamical system models utilizing only a scarce high-fidelity data set. In many practical situations, one has a low-fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the inherent limitation of the model and the complexity of the underlying physics. When high resolution data become available, it is natural to seek model correction to improve the resolution of the model predictions. We focus on the case when the amount of high-fidelity data is so small that most of the existing data driven modeling methods cannot be applied. In this paper, we address these challenges with a model-correction method which only requires a scarce high-fidelity data set. Our method first seeks a deep neural network (DNN) model to approximate the existing low-fidelity model. By using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning (TL). After TL, an improved DNN model with high prediction accuracy to the underlying dynamics is obtained. One distinct feature of the propose method is that it does not assume a specific form of the model correction terms. Instead, it offers an inherent correction to the low-fidelity model via TL. A set of numerical examples are presented to demonstrate the effectiveness of the proposed method.","sentences":["We present a deep learning framework for correcting existing dynamical system models utilizing only a scarce high-fidelity data set.","In many practical situations, one has a low-fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the inherent limitation of the model and the complexity of the underlying physics.","When high resolution data become available, it is natural to seek model correction to improve the resolution of the model predictions.","We focus on the case when the amount of high-fidelity data is so small that most of the existing data driven modeling methods cannot be applied.","In this paper, we address these challenges with a model-correction method which only requires a scarce high-fidelity data set.","Our method first seeks a deep neural network (DNN) model to approximate the existing low-fidelity model.","By using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning (TL).","After TL, an improved DNN model with high prediction accuracy to the underlying dynamics is obtained.","One distinct feature of the propose method is that it does not assume a specific form of the model correction terms.","Instead, it offers an inherent correction to the low-fidelity model via TL.","A set of numerical examples are presented to demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2410.17913v1"}
{"created":"2024-10-23 14:28:32","title":"Slot: Provenance-Driven APT Detection through Graph Reinforcement Learning","abstract":"Advanced Persistent Threats (APTs) represent sophisticated cyberattacks characterized by their ability to remain undetected within the victim system for extended periods, aiming to exfiltrate sensitive data or disrupt operations. Existing detection approaches often struggle to effectively identify these complex threats, construct the attack chain for defense facilitation, or resist adversarial attacks. To overcome these challenges, we propose Slot, an advanced APT detection approach based on provenance graphs and graph reinforcement learning. Slot excels in uncovering multi-level hidden relationships, such as causal, contextual, and indirect connections, among system behaviors through provenance graph mining. By pioneering the integration of graph reinforcement learning, Slot dynamically adapts to new user activities and evolving attack strategies, enhancing its resilience against adversarial attacks. Additionally, Slot automatically constructs the attack chain according to detected attacks with clustering algorithms, providing precise identification of attack paths and facilitating the development of defense strategies. Evaluations with real-world datasets demonstrate Slot's outstanding accuracy, efficiency, adaptability, and robustness in APT detection, with most metrics surpassing state-of-the-art methods. Additionally, case studies conducted to assess Slot's effectiveness in supporting APT defense further establish it as a practical and reliable tool for cybersecurity protection.","sentences":["Advanced Persistent Threats (APTs) represent sophisticated cyberattacks characterized by their ability to remain undetected within the victim system for extended periods, aiming to exfiltrate sensitive data or disrupt operations.","Existing detection approaches often struggle to effectively identify these complex threats, construct the attack chain for defense facilitation, or resist adversarial attacks.","To overcome these challenges, we propose Slot, an advanced APT detection approach based on provenance graphs and graph reinforcement learning.","Slot excels in uncovering multi-level hidden relationships, such as causal, contextual, and indirect connections, among system behaviors through provenance graph mining.","By pioneering the integration of graph reinforcement learning, Slot dynamically adapts to new user activities and evolving attack strategies, enhancing its resilience against adversarial attacks.","Additionally, Slot automatically constructs the attack chain according to detected attacks with clustering algorithms, providing precise identification of attack paths and facilitating the development of defense strategies.","Evaluations with real-world datasets demonstrate Slot's outstanding accuracy, efficiency, adaptability, and robustness in APT detection, with most metrics surpassing state-of-the-art methods.","Additionally, case studies conducted to assess Slot's effectiveness in supporting APT defense further establish it as a practical and reliable tool for cybersecurity protection."],"url":"http://arxiv.org/abs/2410.17910v1"}
{"created":"2024-10-23 14:26:51","title":"Adaptive Test Generation with Qgrams","abstract":"Adaptive Random Testing (ART) has faced criticism, particularly for its computational inefficiency, as highlighted by Arcuri and Briand. Their analysis clarified how ART requires a quadratic number of distance computations as the number of test executions increases, which limits its scalability in scenarios requiring extensive testing to uncover faults. Simulation results support this, showing that the computational overhead of these distance calculations often outweighs ART's benefits. While various ART variants have attempted to reduce these costs, they frequently do so at the expense of fault detection, lack complexity guarantees, or are restricted to specific input types, such as numerical or discrete data.   In this paper, we introduce a novel framework for adaptive random testing that replaces pairwise distance computations with a compact aggregation of past executions, such as counting the Qgrams observed in previous runs. Test case selection then leverages this aggregated data to measure diversity (e.g., entropy of Qgrams), allowing us to reduce the computational complexity from quadratic to linear.   Experiments with a benchmark of six web applications, show that ART with Qgrams covers, on average, 4x more unique targets than random testing, and 3.5x more than ART using traditional distance-based methods.","sentences":["Adaptive Random Testing (ART) has faced criticism, particularly for its computational inefficiency, as highlighted by Arcuri and Briand.","Their analysis clarified how ART requires a quadratic number of distance computations as the number of test executions increases, which limits its scalability in scenarios requiring extensive testing to uncover faults.","Simulation results support this, showing that the computational overhead of these distance calculations often outweighs ART's benefits.","While various ART variants have attempted to reduce these costs, they frequently do so at the expense of fault detection, lack complexity guarantees, or are restricted to specific input types, such as numerical or discrete data.   ","In this paper, we introduce a novel framework for adaptive random testing that replaces pairwise distance computations with a compact aggregation of past executions, such as counting the Qgrams observed in previous runs.","Test case selection then leverages this aggregated data to measure diversity (e.g., entropy of Qgrams), allowing us to reduce the computational complexity from quadratic to linear.   ","Experiments with a benchmark of six web applications, show that ART with Qgrams covers, on average, 4x more unique targets than random testing, and 3.5x more than ART using traditional distance-based methods."],"url":"http://arxiv.org/abs/2410.17907v1"}
{"created":"2024-10-23 14:26:35","title":"Leveraging Deep Learning for Time Series Extrinsic Regression in predicting photometric metallicity of Fundamental-mode RR Lyrae Stars","abstract":"Astronomy is entering an unprecedented era of Big Data science, driven by missions like the ESA's Gaia telescope, which aims to map the Milky Way in three dimensions. Gaia's vast dataset presents a monumental challenge for traditional analysis methods. The sheer scale of this data exceeds the capabilities of manual exploration, necessitating the utilization of advanced computational techniques. In response to this challenge, we developed a novel approach leveraging deep learning to estimate the metallicity of fundamental mode (ab-type) RR Lyrae stars from their light curves in the Gaia optical G-band. Our study explores applying deep learning techniques, particularly advanced neural network architectures, in predicting photometric metallicity from time-series data. Our deep learning models demonstrated notable predictive performance, with a low mean absolute error (MAE) of 0.0565, the root mean square error (RMSE) achieved is 0.0765 and a high $R^2$ regression performance of 0.9401 measured by cross-validation. The weighted mean absolute error (wMAE) is 0.0563, while the weighted root mean square error (wRMSE) is 0.0763. These results showcase the effectiveness of our approach in accurately estimating metallicity values. Our work underscores the importance of deep learning in astronomical research, particularly with large datasets from missions like Gaia. By harnessing the power of deep learning methods, we can provide precision in analyzing vast datasets, contributing to more precise and comprehensive insights into complex astronomical phenomena.","sentences":["Astronomy is entering an unprecedented era of Big Data science, driven by missions like the ESA's Gaia telescope, which aims to map the Milky Way in three dimensions.","Gaia's vast dataset presents a monumental challenge for traditional analysis methods.","The sheer scale of this data exceeds the capabilities of manual exploration, necessitating the utilization of advanced computational techniques.","In response to this challenge, we developed a novel approach leveraging deep learning to estimate the metallicity of fundamental mode (ab-type) RR Lyrae stars from their light curves in the Gaia optical G-band.","Our study explores applying deep learning techniques, particularly advanced neural network architectures, in predicting photometric metallicity from time-series data.","Our deep learning models demonstrated notable predictive performance, with a low mean absolute error (MAE) of 0.0565, the root mean square error (RMSE) achieved is 0.0765 and a high $R^2$ regression performance of 0.9401 measured by cross-validation.","The weighted mean absolute error (wMAE) is 0.0563, while the weighted root mean square error (wRMSE) is 0.0763.","These results showcase the effectiveness of our approach in accurately estimating metallicity values.","Our work underscores the importance of deep learning in astronomical research, particularly with large datasets from missions like Gaia.","By harnessing the power of deep learning methods, we can provide precision in analyzing vast datasets, contributing to more precise and comprehensive insights into complex astronomical phenomena."],"url":"http://arxiv.org/abs/2410.17906v1"}
{"created":"2024-10-23 14:18:25","title":"ELAICHI: Enhancing Low-resource TTS by Addressing Infrequent and Low-frequency Character Bigrams","abstract":"Recent advancements in Text-to-Speech (TTS) technology have led to natural-sounding speech for English, primarily due to the availability of large-scale, high-quality web data. However, many other languages lack access to such resources, relying instead on limited studio-quality data. This scarcity results in synthesized speech that often suffers from intelligibility issues, particularly with low-frequency character bigrams. In this paper, we propose three solutions to address this challenge. First, we leverage high-quality data from linguistically or geographically related languages to improve TTS for the target language. Second, we utilize low-quality Automatic Speech Recognition (ASR) data recorded in non-studio environments, which is refined using denoising and speech enhancement models. Third, we apply knowledge distillation from large-scale models using synthetic data to generate more robust outputs. Our experiments with Hindi demonstrate significant reductions in intelligibility issues, as validated by human evaluators. We propose this methodology as a viable alternative for languages with limited access to high-quality data, enabling them to collectively benefit from shared resources.","sentences":["Recent advancements in Text-to-Speech (TTS) technology have led to natural-sounding speech for English, primarily due to the availability of large-scale, high-quality web data.","However, many other languages lack access to such resources, relying instead on limited studio-quality data.","This scarcity results in synthesized speech that often suffers from intelligibility issues, particularly with low-frequency character bigrams.","In this paper, we propose three solutions to address this challenge.","First, we leverage high-quality data from linguistically or geographically related languages to improve TTS for the target language.","Second, we utilize low-quality Automatic Speech Recognition (ASR) data recorded in non-studio environments, which is refined using denoising and speech enhancement models.","Third, we apply knowledge distillation from large-scale models using synthetic data to generate more robust outputs.","Our experiments with Hindi demonstrate significant reductions in intelligibility issues, as validated by human evaluators.","We propose this methodology as a viable alternative for languages with limited access to high-quality data, enabling them to collectively benefit from shared resources."],"url":"http://arxiv.org/abs/2410.17901v1"}
{"created":"2024-10-23 14:16:34","title":"Scalable Offline Reinforcement Learning for Mean Field Games","abstract":"Reinforcement learning algorithms for mean-field games offer a scalable framework for optimizing policies in large populations of interacting agents. Existing methods often depend on online interactions or access to system dynamics, limiting their practicality in real-world scenarios where such interactions are infeasible or difficult to model. In this paper, we present Offline Munchausen Mirror Descent (Off-MMD), a novel mean-field RL algorithm that approximates equilibrium policies in mean-field games using purely offline data. By leveraging iterative mirror descent and importance sampling techniques, Off-MMD estimates the mean-field distribution from static datasets without relying on simulation or environment dynamics. Additionally, we incorporate techniques from offline reinforcement learning to address common issues like Q-value overestimation, ensuring robust policy learning even with limited data coverage. Our algorithm scales to complex environments and demonstrates strong performance on benchmark tasks like crowd exploration or navigation, highlighting its applicability to real-world multi-agent systems where online experimentation is infeasible. We empirically demonstrate the robustness of Off-MMD to low-quality datasets and conduct experiments to investigate its sensitivity to hyperparameter choices.","sentences":["Reinforcement learning algorithms for mean-field games offer a scalable framework for optimizing policies in large populations of interacting agents.","Existing methods often depend on online interactions or access to system dynamics, limiting their practicality in real-world scenarios where such interactions are infeasible or difficult to model.","In this paper, we present Offline Munchausen Mirror Descent (Off-MMD), a novel mean-field RL algorithm that approximates equilibrium policies in mean-field games using purely offline data.","By leveraging iterative mirror descent and importance sampling techniques, Off-MMD estimates the mean-field distribution from static datasets without relying on simulation or environment dynamics.","Additionally, we incorporate techniques from offline reinforcement learning to address common issues like Q-value overestimation, ensuring robust policy learning even with limited data coverage.","Our algorithm scales to complex environments and demonstrates strong performance on benchmark tasks like crowd exploration or navigation, highlighting its applicability to real-world multi-agent systems where online experimentation is infeasible.","We empirically demonstrate the robustness of Off-MMD to low-quality datasets and conduct experiments to investigate its sensitivity to hyperparameter choices."],"url":"http://arxiv.org/abs/2410.17898v1"}
{"created":"2024-10-23 14:00:48","title":"SpeakGer: A meta-data enriched speech corpus of German state and federal parliaments","abstract":"The application of natural language processing on political texts as well as speeches has become increasingly relevant in political sciences due to the ability to analyze large text corpora which cannot be read by a single person. But such text corpora often lack critical meta information, detailing for instance the party, age or constituency of the speaker, that can be used to provide an analysis tailored to more fine-grained research questions. To enable researchers to answer such questions with quantitative approaches such as natural language processing, we provide the SpeakGer data set, consisting of German parliament debates from all 16 federal states of Germany as well as the German Bundestag from 1947-2023, split into a total of 10,806,105 speeches. This data set includes rich meta data in form of information on both reactions from the audience towards the speech as well as information about the speaker's party, their age, their constituency and their party's political alignment, which enables a deeper analysis. We further provide three exploratory analyses, detailing topic shares of different parties throughout time, a descriptive analysis of the development of the age of an average speaker as well as a sentiment analysis of speeches of different parties with regards to the COVID-19 pandemic.","sentences":["The application of natural language processing on political texts as well as speeches has become increasingly relevant in political sciences due to the ability to analyze large text corpora which cannot be read by a single person.","But such text corpora often lack critical meta information, detailing for instance the party, age or constituency of the speaker, that can be used to provide an analysis tailored to more fine-grained research questions.","To enable researchers to answer such questions with quantitative approaches such as natural language processing, we provide the SpeakGer data set, consisting of German parliament debates from all 16 federal states of Germany as well as the German Bundestag from 1947-2023, split into a total of 10,806,105 speeches.","This data set includes rich meta data in form of information on both reactions from the audience towards the speech as well as information about the speaker's party, their age, their constituency and their party's political alignment, which enables a deeper analysis.","We further provide three exploratory analyses, detailing topic shares of different parties throughout time, a descriptive analysis of the development of the age of an average speaker as well as a sentiment analysis of speeches of different parties with regards to the COVID-19 pandemic."],"url":"http://arxiv.org/abs/2410.17886v1"}
{"created":"2024-10-23 13:58:39","title":"R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models","abstract":"Existing Large Multimodal Models (LMMs) struggle with mathematical geometric reasoning due to a lack of high-quality image-text paired data. Current geometric data generation approaches, which apply preset templates to generate geometric data or use Large Language Models (LLMs) to rephrase questions and answers (Q&A), unavoidably limit data accuracy and diversity. To synthesize higher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT) geometry problem generation pipeline. First, we introduce GeoChain to produce high-fidelity geometric images and corresponding descriptions highlighting relations among geometric elements. We then design a Reverse A&Q method that reasons step-by-step based on the descriptions and generates questions in reverse from the reasoning results. Experiments demonstrate that the proposed method brings significant and consistent improvements on multiple LMM baselines, achieving new performance records in the 2B, 7B, and 8B settings. Notably, R-CoT-8B significantly outperforms previous state-of-the-art open-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while also surpassing the closed-source model GPT-4o by an average of 13% across both datasets. The code is available at https://github.com/dle666/R-CoT.","sentences":["Existing Large Multimodal Models (LMMs) struggle with mathematical geometric reasoning due to a lack of high-quality image-text paired data.","Current geometric data generation approaches, which apply preset templates to generate geometric data or use Large Language Models (LLMs) to rephrase questions and answers (Q&A), unavoidably limit data accuracy and diversity.","To synthesize higher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT) geometry problem generation pipeline.","First, we introduce GeoChain to produce high-fidelity geometric images and corresponding descriptions highlighting relations among geometric elements.","We then design a Reverse A&Q method that reasons step-by-step based on the descriptions and generates questions in reverse from the reasoning results.","Experiments demonstrate that the proposed method brings significant and consistent improvements on multiple LMM baselines, achieving new performance records in the 2B, 7B, and 8B settings.","Notably, R-CoT-8B significantly outperforms previous state-of-the-art open-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while also surpassing the closed-source model GPT-4o by an average of 13% across both datasets.","The code is available at https://github.com/dle666/R-CoT."],"url":"http://arxiv.org/abs/2410.17885v1"}
{"created":"2024-10-23 13:55:42","title":"Identifiable Representation and Model Learning for Latent Dynamic Systems","abstract":"Learning identifiable representations and models from low-level observations is useful for an intelligent spacecraft to reliability finish downstream tasks. For temporal observations, to ensure that the data generating process is provably inverted, most existing works either assume the noise variables in the dynamic mechanisms are (conditionally) independent, or require interventions which can directly affect each latent variable. However, in practice, the relationship between the exogenous inputs/interventions and the latent variables may follow some complex deterministic mechanisms. In this work, we study the problem of identifiable representation and model learning for latent dynamic systems. The key idea is that we use an inductive bias inspired by controllable canonical forms, which is invariant, sparse, and input dependent by definition. We prove that, for linear or affine nonlinear latent dynamic systems, it is possible to identify the representations up to scaling and determine the models up to some simple transformations. The results have potential to provide some theoretical guarantees for developing more trustworthy decision-making and control methods for intelligent spacecrafts.","sentences":["Learning identifiable representations and models from low-level observations is useful for an intelligent spacecraft to reliability finish downstream tasks.","For temporal observations, to ensure that the data generating process is provably inverted, most existing works either assume the noise variables in the dynamic mechanisms are (conditionally) independent, or require interventions which can directly affect each latent variable.","However, in practice, the relationship between the exogenous inputs/interventions and the latent variables may follow some complex deterministic mechanisms.","In this work, we study the problem of identifiable representation and model learning for latent dynamic systems.","The key idea is that we use an inductive bias inspired by controllable canonical forms, which is invariant, sparse, and input dependent by definition.","We prove that, for linear or affine nonlinear latent dynamic systems, it is possible to identify the representations up to scaling and determine the models up to some simple transformations.","The results have potential to provide some theoretical guarantees for developing more trustworthy decision-making and control methods for intelligent spacecrafts."],"url":"http://arxiv.org/abs/2410.17882v1"}
{"created":"2024-10-23 13:52:22","title":"A utility-based spatial analysis of residential street-level conditions; A case study of Rotterdam","abstract":"Residential location choices are traditionally modelled using factors related to accessibility and socioeconomic environments, neglecting the importance of local street-level conditions. Arguably, this neglect is due to data practices. Today, however, street-level images -- which are highly effective at encoding street-level conditions -- are widely available. Additionally, recent advances in discrete choice models incorporating computer vision capabilities offer opportunities to integrate street-level conditions into residential location choice analysis. This study leverages these developments to investigate the spatial distribution of utility derived from street-level conditions in residential location choices on a city-wide scale. In our case study of Rotterdam, the Netherlands, we find that the utility derived from street-level conditions varies significantly on a highly localised scale, with conditions rapidly changing even within neighbourhoods. Our results also reveal that the high real-estate prices in the city centre cannot be attributed to attractive street-level conditions. Furthermore, whereas the city centre is characterised by relatively unattractive residential street-level conditions, neighbourhoods in the southern part of the city -- often perceived as problematic -- exhibit surprisingly appealing street-level environments. The methodological contribution of this paper is that it advances the discrete choice models incorporating computer vision capabilities by introducing a semantic regularisation layer to the model. Thereby, it adds explainability and eliminates the need for a separate pipeline to extract information from images, streamlining the analysis. As such, this paper's findings and methodological advancements pave the way for further studies to explore integrating street-level conditions in urban planning.","sentences":["Residential location choices are traditionally modelled using factors related to accessibility and socioeconomic environments, neglecting the importance of local street-level conditions.","Arguably, this neglect is due to data practices.","Today, however, street-level images -- which are highly effective at encoding street-level conditions -- are widely available.","Additionally, recent advances in discrete choice models incorporating computer vision capabilities offer opportunities to integrate street-level conditions into residential location choice analysis.","This study leverages these developments to investigate the spatial distribution of utility derived from street-level conditions in residential location choices on a city-wide scale.","In our case study of Rotterdam, the Netherlands, we find that the utility derived from street-level conditions varies significantly on a highly localised scale, with conditions rapidly changing even within neighbourhoods.","Our results also reveal that the high real-estate prices in the city centre cannot be attributed to attractive street-level conditions.","Furthermore, whereas the city centre is characterised by relatively unattractive residential street-level conditions, neighbourhoods in the southern part of the city -- often perceived as problematic -- exhibit surprisingly appealing street-level environments.","The methodological contribution of this paper is that it advances the discrete choice models incorporating computer vision capabilities by introducing a semantic regularisation layer to the model.","Thereby, it adds explainability and eliminates the need for a separate pipeline to extract information from images, streamlining the analysis.","As such, this paper's findings and methodological advancements pave the way for further studies to explore integrating street-level conditions in urban planning."],"url":"http://arxiv.org/abs/2410.17880v1"}
{"created":"2024-10-23 13:50:27","title":"Relaxed Equivariance via Multitask Learning","abstract":"Incorporating equivariance as an inductive bias into deep learning architectures to take advantage of the data symmetry has been successful in multiple applications, such as chemistry and dynamical systems. In particular, roto-translations are crucial for effectively modeling geometric graphs and molecules, where understanding the 3D structures enhances generalization. However, equivariant models often pose challenges due to their high computational complexity. In this paper, we introduce REMUL, a training procedure for approximating equivariance with multitask learning. We show that unconstrained models (which do not build equivariance into the architecture) can learn approximate symmetries by minimizing an additional simple equivariance loss. By formulating equivariance as a new learning objective, we can control the level of approximate equivariance in the model. Our method achieves competitive performance compared to equivariant baselines while being $10 \\times$ faster at inference and $2.5 \\times$ at training.","sentences":["Incorporating equivariance as an inductive bias into deep learning architectures to take advantage of the data symmetry has been successful in multiple applications, such as chemistry and dynamical systems.","In particular, roto-translations are crucial for effectively modeling geometric graphs and molecules, where understanding the 3D structures enhances generalization.","However, equivariant models often pose challenges due to their high computational complexity.","In this paper, we introduce REMUL, a training procedure for approximating equivariance with multitask learning.","We show that unconstrained models (which do not build equivariance into the architecture) can learn approximate symmetries by minimizing an additional simple equivariance loss.","By formulating equivariance as a new learning objective, we can control the level of approximate equivariance in the model.","Our method achieves competitive performance compared to equivariant baselines while being $10 \\times$ faster at inference and $2.5 \\times$ at training."],"url":"http://arxiv.org/abs/2410.17878v1"}
{"created":"2024-10-23 13:30:02","title":"DataTales: A Benchmark for Real-World Intelligent Data Narration","abstract":"We introduce DataTales, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives. Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications. DataTales addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field. Our findings highlights the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies.","sentences":["We introduce DataTales, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives.","Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications.","DataTales addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field.","Our findings highlights the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies."],"url":"http://arxiv.org/abs/2410.17859v1"}
{"created":"2024-10-23 13:06:42","title":"Truly Sub-Nyquist Method Based Matrix Pencil and CRT with Super Resolution","abstract":"The emergence of ultra-wideband (UWB) and high-throughput signals has necessitated advancements in data sampling technologies1. Sub-Nyquist sampling methods, such as the modulated wideband converter (MWC) and compressed auto-correlation spectrum sensing (CCS), address the limitations of traditional analog-to-digital converters (ADCs) by capturing signals below the Nyquist rate. However, these methods face challenges like spectral leakage and complex hardware requirements. This paper proposes a novel super-resolution generalized eigenvalue method that integrates the matrix pencil method with the Chinese Remainder Theorem (CRT) to enhance signal processing capabilities within a true sub-Nyquist framework3. This approach aims to improve frequency resolution and accuracy in high-frequency signal extraction, with potential applications in telecommunications, radar, and medical imaging.","sentences":["The emergence of ultra-wideband (UWB) and high-throughput signals has necessitated advancements in data sampling technologies1.","Sub-Nyquist sampling methods, such as the modulated wideband converter (MWC) and compressed auto-correlation spectrum sensing (CCS), address the limitations of traditional analog-to-digital converters (ADCs) by capturing signals below the Nyquist rate.","However, these methods face challenges like spectral leakage and complex hardware requirements.","This paper proposes a novel super-resolution generalized eigenvalue method that integrates the matrix pencil method with the Chinese Remainder Theorem (CRT) to enhance signal processing capabilities within a true sub-Nyquist framework3.","This approach aims to improve frequency resolution and accuracy in high-frequency signal extraction, with potential applications in telecommunications, radar, and medical imaging."],"url":"http://arxiv.org/abs/2410.17841v1"}
{"created":"2024-10-23 12:40:33","title":"RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models for Multi-label Chest X-ray Classification","abstract":"In this paper we introduce RE-tune, a novel approach for fine-tuning pre-trained Multimodal Biomedical Vision-Language models (VLMs) in Incremental Learning scenarios for multi-label chest disease diagnosis. RE-tune freezes the backbones and only trains simple adaptors on top of the Image and Text encoders of the VLM. By engineering positive and negative text prompts for diseases, we leverage the ability of Large Language Models to steer the training trajectory. We evaluate RE-tune in three realistic incremental learning scenarios: class-incremental, label-incremental, and data-incremental. Our results demonstrate that Biomedical VLMs are natural continual learners and prevent catastrophic forgetting. RE-tune not only achieves accurate multi-label classification results, but also prioritizes patient privacy and it distinguishes itself through exceptional computational efficiency, rendering it highly suitable for broad adoption in real-world healthcare settings.","sentences":["In this paper we introduce RE-tune, a novel approach for fine-tuning pre-trained Multimodal Biomedical Vision-Language models (VLMs) in Incremental Learning scenarios for multi-label chest disease diagnosis.","RE-tune freezes the backbones and only trains simple adaptors on top of the Image and Text encoders of the VLM.","By engineering positive and negative text prompts for diseases, we leverage the ability of Large Language Models to steer the training trajectory.","We evaluate RE-tune in three realistic incremental learning scenarios: class-incremental, label-incremental, and data-incremental.","Our results demonstrate that Biomedical VLMs are natural continual learners and prevent catastrophic forgetting.","RE-tune not only achieves accurate multi-label classification results, but also prioritizes patient privacy and it distinguishes itself through exceptional computational efficiency, rendering it highly suitable for broad adoption in real-world healthcare settings."],"url":"http://arxiv.org/abs/2410.17827v1"}
{"created":"2024-10-23 12:32:21","title":"Att2CPC: Attention-Guided Lossy Attribute Compression of Point Clouds","abstract":"With the great progress of 3D sensing and acquisition technology, the volume of point cloud data has grown dramatically, which urges the development of efficient point cloud compression methods. In this paper, we focus on the task of learned lossy point cloud attribute compression (PCAC). We propose an efficient attention-based method for lossy compression of point cloud attributes leveraging on an autoencoder architecture. Specifically, at the encoding side, we conduct multiple downsampling to best exploit the local attribute patterns, in which effective External Cross Attention (ECA) is devised to hierarchically aggregate features by intergrating attributes and geometry contexts. At the decoding side, the attributes of the point cloud are progressively reconstructed based on the multi-scale representation and the zero-padding upsampling tactic. To the best of our knowledge, this is the first approach to introduce attention mechanism to point-based lossy PCAC task. We verify the compression efficiency of our model on various sequences, including human body frames, sparse objects, and large-scale point cloud scenes. Experiments show that our method achieves an average improvement of 1.15 dB and 2.13 dB in BD-PSNR of Y channel and YUV channel, respectively, when comparing with the state-of-the-art point-based method Deep-PCAC. Codes of this paper are available at https://github.com/I2-Multimedia-Lab/Att2CPC.","sentences":["With the great progress of 3D sensing and acquisition technology, the volume of point cloud data has grown dramatically, which urges the development of efficient point cloud compression methods.","In this paper, we focus on the task of learned lossy point cloud attribute compression (PCAC).","We propose an efficient attention-based method for lossy compression of point cloud attributes leveraging on an autoencoder architecture.","Specifically, at the encoding side, we conduct multiple downsampling to best exploit the local attribute patterns, in which effective External Cross Attention (ECA) is devised to hierarchically aggregate features by intergrating attributes and geometry contexts.","At the decoding side, the attributes of the point cloud are progressively reconstructed based on the multi-scale representation and the zero-padding upsampling tactic.","To the best of our knowledge, this is the first approach to introduce attention mechanism to point-based lossy PCAC task.","We verify the compression efficiency of our model on various sequences, including human body frames, sparse objects, and large-scale point cloud scenes.","Experiments show that our method achieves an average improvement of 1.15 dB and 2.13 dB in BD-PSNR of Y channel and YUV channel, respectively, when comparing with the state-of-the-art point-based method Deep-PCAC.","Codes of this paper are available at https://github.com/I2-Multimedia-Lab/Att2CPC."],"url":"http://arxiv.org/abs/2410.17823v1"}
{"created":"2024-10-23 12:27:34","title":"On the formalization of the notion of a concurrent algorithm","abstract":"Previous papers give accounts of quests for satisfactory formalizations of the classical informal notion of an algorithm and the contemporary informal notion of an interactive algoritm. In this paper, an attempt is made to generalize the results of the former quest to the contemporary informal notion of a concurrent algorithm. The notion of a concurrent proto-algorithm is introduced. The thought is that concurrent algorithms are equivalence classes of concurrent proto-algorithms under an appropriate equivalence relation. Three equivalence relations are defined. Two of them are deemed to be bounds for an appropriate equivalence relation and the third is likely an appropriate one. The connection between concurrency and non-determinism in the presented setting is also addressed.","sentences":["Previous papers give accounts of quests for satisfactory formalizations of the classical informal notion of an algorithm and the contemporary informal notion of an interactive algoritm.","In this paper, an attempt is made to generalize the results of the former quest to the contemporary informal notion of a concurrent algorithm.","The notion of a concurrent proto-algorithm is introduced.","The thought is that concurrent algorithms are equivalence classes of concurrent proto-algorithms under an appropriate equivalence relation.","Three equivalence relations are defined.","Two of them are deemed to be bounds for an appropriate equivalence relation and the third is likely an appropriate one.","The connection between concurrency and non-determinism in the presented setting is also addressed."],"url":"http://arxiv.org/abs/2410.17821v1"}
{"created":"2024-10-23 11:59:49","title":"GenUDC: High Quality 3D Mesh Generation with Unsigned Dual Contouring Representation","abstract":"Generating high-quality meshes with complex structures and realistic surfaces is the primary goal of 3D generative models. Existing methods typically employ sequence data or deformable tetrahedral grids for mesh generation. However, sequence-based methods have difficulty producing complex structures with many faces due to memory limits. The deformable tetrahedral grid-based method MeshDiffusion fails to recover realistic surfaces due to the inherent ambiguity in deformable grids. We propose the GenUDC framework to address these challenges by leveraging the Unsigned Dual Contouring (UDC) as the mesh representation. UDC discretizes a mesh in a regular grid and divides it into the face and vertex parts, recovering both complex structures and fine details. As a result, the one-to-one mapping between UDC and mesh resolves the ambiguity problem. In addition, GenUDC adopts a two-stage, coarse-to-fine generative process for 3D mesh generation. It first generates the face part as a rough shape and then the vertex part to craft a detailed shape. Extensive evaluations demonstrate the superiority of UDC as a mesh representation and the favorable performance of GenUDC in mesh generation. The code and trained models are available at https://github.com/TrepangCat/GenUDC.","sentences":["Generating high-quality meshes with complex structures and realistic surfaces is the primary goal of 3D generative models.","Existing methods typically employ sequence data or deformable tetrahedral grids for mesh generation.","However, sequence-based methods have difficulty producing complex structures with many faces due to memory limits.","The deformable tetrahedral grid-based method MeshDiffusion fails to recover realistic surfaces due to the inherent ambiguity in deformable grids.","We propose the GenUDC framework to address these challenges by leveraging the Unsigned Dual Contouring (UDC) as the mesh representation.","UDC discretizes a mesh in a regular grid and divides it into the face and vertex parts, recovering both complex structures and fine details.","As a result, the one-to-one mapping between UDC and mesh resolves the ambiguity problem.","In addition, GenUDC adopts a two-stage, coarse-to-fine generative process for 3D mesh generation.","It first generates the face part as a rough shape and then the vertex part to craft a detailed shape.","Extensive evaluations demonstrate the superiority of UDC as a mesh representation and the favorable performance of GenUDC in mesh generation.","The code and trained models are available at https://github.com/TrepangCat/GenUDC."],"url":"http://arxiv.org/abs/2410.17802v1"}
{"created":"2024-10-23 11:58:58","title":"OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation","abstract":"Full-duplex spoken dialogue systems significantly advance over traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions. However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech. In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency. To achieve full-duplex communication capabilities, we propose a multi-stage post-training scheme that progressively adapts a text-based large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM. The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning. Throughout all training stages, we standardize the data using a flattening operation, which allows us to unify the training methods and the model architecture across different modalities and tasks. Our approach offers a straightforward modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems. Audio samples of dialogues generated by OmniFlatten can be found at this web site (https://omniflatten.github.io/).","sentences":["Full-duplex spoken dialogue systems significantly advance over traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions.","However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech.","In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency.","To achieve full-duplex communication capabilities, we propose a multi-stage post-training scheme that progressively adapts a text-based large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM.","The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning.","Throughout all training stages, we standardize the data using a flattening operation, which allows us to unify the training methods and the model architecture across different modalities and tasks.","Our approach offers a straightforward modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems.","Audio samples of dialogues generated by OmniFlatten can be found at this web site (https://omniflatten.github.io/)."],"url":"http://arxiv.org/abs/2410.17799v1"}
{"created":"2024-10-23 11:47:04","title":"Enhancing Federated Learning Convergence with Dynamic Data Queue and Data Entropy-driven Participant Selection","abstract":"Federated Learning (FL) is a decentralized approach for collaborative model training on edge devices. This distributed method of model training offers advantages in privacy, security, regulatory compliance, and cost-efficiency. Our emphasis in this research lies in addressing statistical complexity in FL, especially when the data stored locally across devices is not identically and independently distributed (non-IID). We have observed an accuracy reduction of up to approximately 10\\% to 30\\%, particularly in skewed scenarios where each edge device trains with only 1 class of data. This reduction is attributed to weight divergence, quantified using the Euclidean distance between device-level class distributions and the population distribution, resulting in a bias term (\\(\\delta_k\\)). As a solution, we present a method to improve convergence in FL by creating a global subset of data on the server and dynamically distributing it across devices using a Dynamic Data queue-driven Federated Learning (DDFL). Next, we leverage Data Entropy metrics to observe the process during each training round and enable reasonable device selection for aggregation. Furthermore, we provide a convergence analysis of our proposed DDFL to justify their viability in practical FL scenarios, aiming for better device selection, a non-sub-optimal global model, and faster convergence. We observe that our approach results in a substantial accuracy boost of approximately 5\\% for the MNIST dataset, around 18\\% for CIFAR-10, and 20\\% for CIFAR-100 with a 10\\% global subset of data, outperforming the state-of-the-art (SOTA) aggregation algorithms.","sentences":["Federated Learning (FL) is a decentralized approach for collaborative model training on edge devices.","This distributed method of model training offers advantages in privacy, security, regulatory compliance, and cost-efficiency.","Our emphasis in this research lies in addressing statistical complexity in FL, especially when the data stored locally across devices is not identically and independently distributed (non-IID).","We have observed an accuracy reduction of up to approximately 10\\% to 30\\%, particularly in skewed scenarios where each edge device trains with only 1 class of data.","This reduction is attributed to weight divergence, quantified using the Euclidean distance between device-level class distributions and the population distribution, resulting in a bias term (\\(\\delta_k\\)).","As a solution, we present a method to improve convergence in FL by creating a global subset of data on the server and dynamically distributing it across devices using a Dynamic Data queue-driven Federated Learning (DDFL).","Next, we leverage Data Entropy metrics to observe the process during each training round and enable reasonable device selection for aggregation.","Furthermore, we provide a convergence analysis of our proposed DDFL to justify their viability in practical FL scenarios, aiming for better device selection, a non-sub-optimal global model, and faster convergence.","We observe that our approach results in a substantial accuracy boost of approximately 5\\% for the MNIST dataset, around 18\\% for CIFAR-10, and 20\\% for CIFAR-100 with a 10\\% global subset of data, outperforming the state-of-the-art (SOTA) aggregation algorithms."],"url":"http://arxiv.org/abs/2410.17792v1"}
{"created":"2024-10-23 11:44:38","title":"FirePower: Towards a Foundation with Generalizable Knowledge for Architecture-Level Power Modeling","abstract":"Power efficiency is a critical design objective in modern processor design. A high-fidelity architecture-level power modeling method is greatly needed by CPU architects for guiding early optimizations. However, traditional architecture-level power models can not meet the accuracy requirement, largely due to the discrepancy between the power model and actual design implementation. While some machine learning (ML)-based architecture-level power modeling methods have been proposed in recent years, the data-hungry ML model training process requires sufficient similar known designs, which are unrealistic in many development scenarios.   This work proposes a new power modeling solution FirePower that targets few-shot learning scenario for new target architectures. FirePower proposes multiple new policies to utilize cross-architecture knowledge. First, it develops power models at component level, and components are defined in a power-friendly manner. Second, it supports different generalization strategies for models of different components. Third, it formulates generalizable and architecture-specific design knowledge into two separate models. FirePower also supports the evaluation of the generalization quality. In our experiments, FirePower can achieve a low error percentage of 5.8% and a high correlation R of 0.98 on average only using two configurations of target architecture. This is 8.8% lower in error percentage and 0.03 higher in R compared with directly training McPAT-Calib baseline on configurations of target architecture.","sentences":["Power efficiency is a critical design objective in modern processor design.","A high-fidelity architecture-level power modeling method is greatly needed by CPU architects for guiding early optimizations.","However, traditional architecture-level power models can not meet the accuracy requirement, largely due to the discrepancy between the power model and actual design implementation.","While some machine learning (ML)-based architecture-level power modeling methods have been proposed in recent years, the data-hungry ML model training process requires sufficient similar known designs, which are unrealistic in many development scenarios.   ","This work proposes a new power modeling solution FirePower that targets few-shot learning scenario for new target architectures.","FirePower proposes multiple new policies to utilize cross-architecture knowledge.","First, it develops power models at component level, and components are defined in a power-friendly manner.","Second, it supports different generalization strategies for models of different components.","Third, it formulates generalizable and architecture-specific design knowledge into two separate models.","FirePower also supports the evaluation of the generalization quality.","In our experiments, FirePower can achieve a low error percentage of 5.8% and a high correlation R of 0.98 on average only using two configurations of target architecture.","This is 8.8% lower in error percentage and 0.03 higher in R compared with directly training McPAT-Calib baseline on configurations of target architecture."],"url":"http://arxiv.org/abs/2410.17789v1"}
{"created":"2024-10-23 11:37:20","title":"Large Language Models Engineer Too Many Simple Features For Tabular Data","abstract":"Tabular machine learning problems often require time-consuming and labor-intensive feature engineering. Recent efforts have focused on using large language models (LLMs) to capitalize on their potential domain knowledge. At the same time, researchers have observed ethically concerning negative biases in other LLM-related use cases, such as text generation. These developments motivated us to investigate whether LLMs exhibit a bias that negatively impacts the performance of feature engineering. While not ethically concerning, such a bias could hinder practitioners from fully utilizing LLMs for automated data science. Therefore, we propose a method to detect potential biases by detecting anomalies in the frequency of operators (e.g., adding two features) suggested by LLMs when engineering new features. Our experiments evaluate the bias of four LLMs, two big frontier and two small open-source models, across 27 tabular datasets. Our results indicate that LLMs are biased toward simple operators, such as addition, and can fail to utilize more complex operators, such as grouping followed by aggregations. Furthermore, the bias can negatively impact the predictive performance when using LLM-generated features. Our results call for mitigating bias when using LLMs for feature engineering.","sentences":["Tabular machine learning problems often require time-consuming and labor-intensive feature engineering.","Recent efforts have focused on using large language models (LLMs) to capitalize on their potential domain knowledge.","At the same time, researchers have observed ethically concerning negative biases in other LLM-related use cases, such as text generation.","These developments motivated us to investigate whether LLMs exhibit a bias that negatively impacts the performance of feature engineering.","While not ethically concerning, such a bias could hinder practitioners from fully utilizing LLMs for automated data science.","Therefore, we propose a method to detect potential biases by detecting anomalies in the frequency of operators (e.g., adding two features) suggested by LLMs when engineering new features.","Our experiments evaluate the bias of four LLMs, two big frontier and two small open-source models, across 27 tabular datasets.","Our results indicate that LLMs are biased toward simple operators, such as addition, and can fail to utilize more complex operators, such as grouping followed by aggregations.","Furthermore, the bias can negatively impact the predictive performance when using LLM-generated features.","Our results call for mitigating bias when using LLMs for feature engineering."],"url":"http://arxiv.org/abs/2410.17787v1"}
{"created":"2024-10-23 11:35:44","title":"TranSPORTmer: A Holistic Approach to Trajectory Understanding in Multi-Agent Sports","abstract":"Understanding trajectories in multi-agent scenarios requires addressing various tasks, including predicting future movements, imputing missing observations, inferring the status of unseen agents, and classifying different global states. Traditional data-driven approaches often handle these tasks separately with specialized models. We introduce TranSPORTmer, a unified transformer-based framework capable of addressing all these tasks, showcasing its application to the intricate dynamics of multi-agent sports scenarios like soccer and basketball. Using Set Attention Blocks, TranSPORTmer effectively captures temporal dynamics and social interactions in an equivariant manner. The model's tasks are guided by an input mask that conceals missing or yet-to-be-predicted observations. Additionally, we introduce a CLS extra agent to classify states along soccer trajectories, including passes, possessions, uncontrolled states, and out-of-play intervals, contributing to an enhancement in modeling trajectories. Evaluations on soccer and basketball datasets show that TranSPORTmer outperforms state-of-the-art task-specific models in player forecasting, player forecasting-imputation, ball inference, and ball imputation. https://youtu.be/8VtSRm8oGoE","sentences":["Understanding trajectories in multi-agent scenarios requires addressing various tasks, including predicting future movements, imputing missing observations, inferring the status of unseen agents, and classifying different global states.","Traditional data-driven approaches often handle these tasks separately with specialized models.","We introduce TranSPORTmer, a unified transformer-based framework capable of addressing all these tasks, showcasing its application to the intricate dynamics of multi-agent sports scenarios like soccer and basketball.","Using Set Attention Blocks, TranSPORTmer effectively captures temporal dynamics and social interactions in an equivariant manner.","The model's tasks are guided by an input mask that conceals missing or yet-to-be-predicted observations.","Additionally, we introduce a CLS extra agent to classify states along soccer trajectories, including passes, possessions, uncontrolled states, and out-of-play intervals, contributing to an enhancement in modeling trajectories.","Evaluations on soccer and basketball datasets show that TranSPORTmer outperforms state-of-the-art task-specific models in player forecasting, player forecasting-imputation, ball inference, and ball imputation.","https://youtu.be/8VtSRm8oGoE"],"url":"http://arxiv.org/abs/2410.17785v1"}
{"created":"2024-10-23 11:32:38","title":"Pointer: An Energy-Efficient ReRAM-based Point Cloud Recognition Accelerator with Inter-layer and Intra-layer Optimizations","abstract":"Point cloud is an important data structure for a wide range of applications, including robotics, AR/VR, and autonomous driving. To process the point cloud, many deep-learning-based point cloud recognition algorithms have been proposed. However, to meet the requirement of applications like autonomous driving, the algorithm must be fast enough, rendering accelerators necessary at the inference stage. But existing point cloud accelerators are still inefficient due to two challenges. First, the multi-layer perceptron (MLP) during feature computation is the performance bottleneck. Second, the feature vector fetching operation incurs heavy DRAM access.   In this paper, we propose Pointer, an efficient Resistive Random Access Memory (ReRAM)-based point cloud recognition accelerator with inter- and intra-layer optimizations. It proposes three techniques for point cloud acceleration. First, Pointer adopts ReRAM-based architecture to significantly accelerate the MLP in feature computation. Second, to reduce DRAM access, Pointer proposes inter-layer coordination. It schedules the next layer to fetch the results of the previous layer as soon as they are available, which allows on-chip fetching thus reduces DRAM access. Third, Pointer proposes topology-aware intra-layer reordering, which improves the execution order for better data locality. Pointer proves to achieve 40x to 393x speedup and 22x to 163x energy efficiency over prior accelerators without any accuracy loss.","sentences":["Point cloud is an important data structure for a wide range of applications, including robotics, AR/VR, and autonomous driving.","To process the point cloud, many deep-learning-based point cloud recognition algorithms have been proposed.","However, to meet the requirement of applications like autonomous driving, the algorithm must be fast enough, rendering accelerators necessary at the inference stage.","But existing point cloud accelerators are still inefficient due to two challenges.","First, the multi-layer perceptron (MLP) during feature computation is the performance bottleneck.","Second, the feature vector fetching operation incurs heavy DRAM access.   ","In this paper, we propose Pointer, an efficient Resistive Random Access Memory (ReRAM)-based point cloud recognition accelerator with inter- and intra-layer optimizations.","It proposes three techniques for point cloud acceleration.","First, Pointer adopts ReRAM-based architecture to significantly accelerate the MLP in feature computation.","Second, to reduce DRAM access, Pointer proposes inter-layer coordination.","It schedules the next layer to fetch the results of the previous layer as soon as they are available, which allows on-chip fetching thus reduces DRAM access.","Third, Pointer proposes topology-aware intra-layer reordering, which improves the execution order for better data locality.","Pointer proves to achieve 40x to 393x speedup and 22x to 163x energy efficiency over prior accelerators without any accuracy loss."],"url":"http://arxiv.org/abs/2410.17782v1"}
{"created":"2024-10-23 11:29:21","title":"Tight Bounds for Online Balanced Partitioning in the Generalized Learning Model","abstract":"Resource allocation in distributed and networked systems such as the Cloud is becoming increasingly flexible, allowing these systems to dynamically adjust toward the workloads they serve, in a demand-aware manner.   Online balanced partitioning is a fundamental optimization problem underlying such self-adjusting systems. We are given a set of $\\ell$ servers. On each server we can schedule up to $k$ processes simultaneously. The demand is described as a sequence of requests $\\sigma_t=\\{p_i, p_{j}\\}$, which means that the two processes $p_i,p_{j}$ communicate. A process can be migrated from one server to another which costs 1 unit per process move. If the communicating processes are on different servers, it further incurs a communication cost of 1 unit for this request. The objective is to minimize the competitive ratio: the cost of serving such a request sequence compared to the cost incurred by an optimal offline algorithm.   Henzinger et al. (at SIGMETRICS 2019) introduced a learning variant of this problem where the cost of an online algorithm is compared to the cost of a static offline algorithm that does not perform any communication, but which simply learns the communication graph and keeps the discovered connected components together. This problem variant was recently also studied at SODA 2021.   In this paper, we consider a more general learning model (i.e., stronger adversary), where the offline algorithm is not restricted to keep connected components together. Our main contribution are tight bounds for this problem. In particular, we present two deterministic online algorithms: (1) an online algorithm with competitive ratio $O(\\max(\\sqrt{k\\ell \\log k}, \\ell \\log k))$ and augmentation $1+\\epsilon$; (2) an online algorithm with competitive ratio $O(\\sqrt{k})$ and augmentation $2+\\epsilon$. We further present lower bounds showing optimality of these bounds.","sentences":["Resource allocation in distributed and networked systems such as the Cloud is becoming increasingly flexible, allowing these systems to dynamically adjust toward the workloads they serve, in a demand-aware manner.   ","Online balanced partitioning is a fundamental optimization problem underlying such self-adjusting systems.","We are given a set of $\\ell$ servers.","On each server we can schedule up to $k$ processes simultaneously.","The demand is described as a sequence of requests $\\sigma_t=\\{p_i, p_{j}\\}$, which means that the two processes $p_i,p_{j}$ communicate.","A process can be migrated from one server to another which costs 1 unit per process move.","If the communicating processes are on different servers, it further incurs a communication cost of 1 unit for this request.","The objective is to minimize the competitive ratio: the cost of serving such a request sequence compared to the cost incurred by an optimal offline algorithm.   ","Henzinger et al. (at SIGMETRICS 2019) introduced a learning variant of this problem where the cost of an online algorithm is compared to the cost of a static offline algorithm that does not perform any communication, but which simply learns the communication graph and keeps the discovered connected components together.","This problem variant was recently also studied at SODA 2021.   ","In this paper, we consider a more general learning model (i.e., stronger adversary), where the offline algorithm is not restricted to keep connected components together.","Our main contribution are tight bounds for this problem.","In particular, we present two deterministic online algorithms: (1) an online algorithm with competitive ratio $O(\\max(\\sqrt{k\\ell \\log k}, \\ell \\log k))$ and augmentation $1+\\epsilon$; (2) an online algorithm with competitive ratio $O(\\sqrt{k})$ and augmentation $2+\\epsilon$. We further present lower bounds showing optimality of these bounds."],"url":"http://arxiv.org/abs/2410.17777v1"}
{"created":"2024-10-23 11:19:48","title":"Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models","abstract":"A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pretrained vision-language foundation models in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a kitchen play dataset show that NILS can autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets while alleviating several shortcomings of crowdsourced human annotations, such as low data quality and diversity. We use NILS to label over 115k trajectories obtained from over 430 hours of robot data. We open-source our auto-labeling code and generated annotations on our website: http://robottasklabeling.github.io.","sentences":["A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets.","Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability.","To this end, we introduce NILS: Natural language Instruction Labeling for Scalability.","NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention.","NILS combines pretrained vision-language foundation models in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets.","Evaluations on BridgeV2, Fractal, and a kitchen play dataset show that NILS can autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets while alleviating several shortcomings of crowdsourced human annotations, such as low data quality and diversity.","We use NILS to label over 115k trajectories obtained from over 430 hours of robot data.","We open-source our auto-labeling code and generated annotations on our website: http://robottasklabeling.github.io."],"url":"http://arxiv.org/abs/2410.17772v1"}
{"created":"2024-10-23 11:01:39","title":"Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted Transformer Network","abstract":"Quality-of-Service (QoS) prediction is a critical task in the service lifecycle, enabling precise and adaptive service recommendations by anticipating performance variations over time in response to evolving network uncertainties and user preferences. However, contemporary QoS prediction methods frequently encounter data sparsity and cold-start issues, which hinder accurate QoS predictions and limit the ability to capture diverse user preferences. Additionally, these methods often assume QoS data reliability, neglecting potential credibility issues such as outliers and the presence of greysheep users and services with atypical invocation patterns. Furthermore, traditional approaches fail to leverage diverse features, including domain-specific knowledge and complex higher-order patterns, essential for accurate QoS predictions. In this paper, we introduce a real-time, trust-aware framework for temporal QoS prediction to address the aforementioned challenges, featuring an end-to-end deep architecture called the Hypergraph Convoluted Transformer Network (HCTN). HCTN combines a hypergraph structure with graph convolution over hyper-edges to effectively address high-sparsity issues by capturing complex, high-order correlations. Complementing this, the transformer network utilizes multi-head attention along with parallel 1D convolutional layers and fully connected dense blocks to capture both fine-grained and coarse-grained dynamic patterns. Additionally, our approach includes a sparsity-resilient solution for detecting greysheep users and services, incorporating their unique characteristics to improve prediction accuracy. Trained with a robust loss function resistant to outliers, HCTN demonstrated state-of-the-art performance on the large-scale WSDREAM-2 datasets for response time and throughput.","sentences":["Quality-of-Service (QoS) prediction is a critical task in the service lifecycle, enabling precise and adaptive service recommendations by anticipating performance variations over time in response to evolving network uncertainties and user preferences.","However, contemporary QoS prediction methods frequently encounter data sparsity and cold-start issues, which hinder accurate QoS predictions and limit the ability to capture diverse user preferences.","Additionally, these methods often assume QoS data reliability, neglecting potential credibility issues such as outliers and the presence of greysheep users and services with atypical invocation patterns.","Furthermore, traditional approaches fail to leverage diverse features, including domain-specific knowledge and complex higher-order patterns, essential for accurate QoS predictions.","In this paper, we introduce a real-time, trust-aware framework for temporal QoS prediction to address the aforementioned challenges, featuring an end-to-end deep architecture called the Hypergraph Convoluted Transformer Network (HCTN).","HCTN combines a hypergraph structure with graph convolution over hyper-edges to effectively address high-sparsity issues by capturing complex, high-order correlations.","Complementing this, the transformer network utilizes multi-head attention along with parallel 1D convolutional layers and fully connected dense blocks to capture both fine-grained and coarse-grained dynamic patterns.","Additionally, our approach includes a sparsity-resilient solution for detecting greysheep users and services, incorporating their unique characteristics to improve prediction accuracy.","Trained with a robust loss function resistant to outliers, HCTN demonstrated state-of-the-art performance on the large-scale WSDREAM-2 datasets for response time and throughput."],"url":"http://arxiv.org/abs/2410.17762v1"}
{"created":"2024-10-23 10:56:05","title":"Topology meets Machine Learning: An Introduction using the Euler Characteristic Transform","abstract":"This overview article makes the case for how topological concepts can enrich research in machine learning. Using the Euler Characteristic Transform (ECT), a geometrical-topological invariant, as a running example, I present different use cases that result in more efficient models for analyzing point clouds, graphs, and meshes. Moreover, I outline a vision for how topological concepts could be used in the future, comprising (1) the learning of functions on topological spaces, (2) the building of hybrid models that imbue neural networks with knowledge about the topological information in data, and (3) the analysis of qualitative properties of neural networks. With current research already addressing some of these aspects, this article thus serves as an introduction and invitation to this nascent area of research.","sentences":["This overview article makes the case for how topological concepts can enrich research in machine learning.","Using the Euler Characteristic Transform (ECT), a geometrical-topological invariant, as a running example, I present different use cases that result in more efficient models for analyzing point clouds, graphs, and meshes.","Moreover, I outline a vision for how topological concepts could be used in the future, comprising (1) the learning of functions on topological spaces, (2) the building of hybrid models that imbue neural networks with knowledge about the topological information in data, and (3) the analysis of qualitative properties of neural networks.","With current research already addressing some of these aspects, this article thus serves as an introduction and invitation to this nascent area of research."],"url":"http://arxiv.org/abs/2410.17760v1"}
{"created":"2024-10-23 10:50:07","title":"Escaping the Forest: Sparse Interpretable Neural Networks for Tabular Data","abstract":"Tabular datasets are widely used in scientific disciplines such as biology. While these disciplines have already adopted AI methods to enhance their findings and analysis, they mainly use tree-based methods due to their interpretability. At the same time, artificial neural networks have been shown to offer superior flexibility and depth for rich and complex non-tabular problems, but they are falling behind tree-based models for tabular data in terms of performance and interpretability. Although sparsity has been shown to improve the interpretability and performance of ANN models for complex non-tabular datasets, enforcing sparsity structurally and formatively for tabular data before training the model, remains an open question. To address this question, we establish a method that infuses sparsity in neural networks by utilising attention mechanisms to capture the features' importance in tabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with attention mechanisms, are more effective than tree-based models, reaching the state-of-the-art on biological datasets. They further permit the extraction of insights from these datasets and achieve better performance than post-hoc methods like SHAP.","sentences":["Tabular datasets are widely used in scientific disciplines such as biology.","While these disciplines have already adopted AI methods to enhance their findings and analysis, they mainly use tree-based methods due to their interpretability.","At the same time, artificial neural networks have been shown to offer superior flexibility and depth for rich and complex non-tabular problems, but they are falling behind tree-based models for tabular data in terms of performance and interpretability.","Although sparsity has been shown to improve the interpretability and performance of ANN models for complex non-tabular datasets, enforcing sparsity structurally and formatively for tabular data before training the model, remains an open question.","To address this question, we establish a method that infuses sparsity in neural networks by utilising attention mechanisms to capture the features' importance in tabular datasets.","We show that our models, Sparse TABular NET or sTAB-Net with attention mechanisms, are more effective than tree-based models, reaching the state-of-the-art on biological datasets.","They further permit the extraction of insights from these datasets and achieve better performance than post-hoc methods like SHAP."],"url":"http://arxiv.org/abs/2410.17758v1"}
{"created":"2024-10-23 10:28:17","title":"VISAGE: Video Synthesis using Action Graphs for Surgery","abstract":"Surgical data science (SDS) is a field that analyzes patient data before, during, and after surgery to improve surgical outcomes and skills. However, surgical data is scarce, heterogeneous, and complex, which limits the applicability of existing machine learning methods. In this work, we introduce the novel task of future video generation in laparoscopic surgery. This task can augment and enrich the existing surgical data and enable various applications, such as simulation, analysis, and robot-aided surgery. Ultimately, it involves not only understanding the current state of the operation but also accurately predicting the dynamic and often unpredictable nature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis using Action Graphs for Surgery), leverages the power of action scene graphs to capture the sequential nature of laparoscopic procedures and utilizes diffusion models to synthesize temporally coherent video sequences. VISAGE predicts the future frames given only a single initial frame, and the action graph triplets. By incorporating domain-specific knowledge through the action graph, VISAGE ensures the generated videos adhere to the expected visual and motion patterns observed in real laparoscopic procedures. The results of our experiments demonstrate high-fidelity video generation for laparoscopy procedures, which enables various applications in SDS.","sentences":["Surgical data science (SDS) is a field that analyzes patient data before, during, and after surgery to improve surgical outcomes and skills.","However, surgical data is scarce, heterogeneous, and complex, which limits the applicability of existing machine learning methods.","In this work, we introduce the novel task of future video generation in laparoscopic surgery.","This task can augment and enrich the existing surgical data and enable various applications, such as simulation, analysis, and robot-aided surgery.","Ultimately, it involves not only understanding the current state of the operation but also accurately predicting the dynamic and often unpredictable nature of surgical procedures.","Our proposed method, VISAGE (VIdeo Synthesis using Action Graphs for Surgery), leverages the power of action scene graphs to capture the sequential nature of laparoscopic procedures and utilizes diffusion models to synthesize temporally coherent video sequences.","VISAGE predicts the future frames given only a single initial frame, and the action graph triplets.","By incorporating domain-specific knowledge through the action graph, VISAGE ensures the generated videos adhere to the expected visual and motion patterns observed in real laparoscopic procedures.","The results of our experiments demonstrate high-fidelity video generation for laparoscopy procedures, which enables various applications in SDS."],"url":"http://arxiv.org/abs/2410.17751v1"}
{"created":"2024-10-23 10:16:08","title":"Multi-Layered Safety of Redundant Robot Manipulators via Task-Oriented Planning and Control","abstract":"Ensuring safety is crucial to promote the application of robot manipulators in open workspace. Factors such as sensor errors or unpredictable collisions make the environment full of uncertainties. In this work, we investigate these potential safety challenges on redundant robot manipulators, and propose a task-oriented planning and control framework to achieve multi-layered safety while maintaining efficient task execution. Our approach consists of two main parts: a task-oriented trajectory planner based on multiple-shooting model predictive control method, and a torque controller that allows safe and efficient collision reaction using only proprioceptive data. Through extensive simulations and real-hardware experiments, we demonstrate that the proposed framework can effectively handle uncertain static or dynamic obstacles, and perform disturbance resistance in manipulation tasks when unforeseen contacts occur. All code will be open-sourced to benefit the community.","sentences":["Ensuring safety is crucial to promote the application of robot manipulators in open workspace.","Factors such as sensor errors or unpredictable collisions make the environment full of uncertainties.","In this work, we investigate these potential safety challenges on redundant robot manipulators, and propose a task-oriented planning and control framework to achieve multi-layered safety while maintaining efficient task execution.","Our approach consists of two main parts: a task-oriented trajectory planner based on multiple-shooting model predictive control method, and a torque controller that allows safe and efficient collision reaction using only proprioceptive data.","Through extensive simulations and real-hardware experiments, we demonstrate that the proposed framework can effectively handle uncertain static or dynamic obstacles, and perform disturbance resistance in manipulation tasks when unforeseen contacts occur.","All code will be open-sourced to benefit the community."],"url":"http://arxiv.org/abs/2410.17742v1"}
{"created":"2024-10-23 09:42:17","title":"Continual Learning on a Data Diet","abstract":"Continual Learning (CL) methods usually learn from all available data. However, this is not the case in human cognition which efficiently focuses on key experiences while disregarding the redundant information. Similarly, not all data points in a dataset have equal potential; some can be more informative than others. This disparity may significantly impact the performance, as both the quality and quantity of samples directly influence the model's generalizability and efficiency. Drawing inspiration from this, we explore the potential of learning from important samples and present an empirical study for evaluating coreset selection techniques in the context of CL to stimulate research in this unexplored area. We train different continual learners on increasing amounts of selected samples and investigate the learning-forgetting dynamics by shedding light on the underlying mechanisms driving their improved stability-plasticity balance. We present several significant observations: learning from selectively chosen samples (i) enhances incremental accuracy, (ii) improves knowledge retention of previous tasks, and (iii) refines learned representations. This analysis contributes to a deeper understanding of selective learning strategies in CL scenarios.","sentences":["Continual Learning (CL) methods usually learn from all available data.","However, this is not the case in human cognition which efficiently focuses on key experiences while disregarding the redundant information.","Similarly, not all data points in a dataset have equal potential; some can be more informative than others.","This disparity may significantly impact the performance, as both the quality and quantity of samples directly influence the model's generalizability and efficiency.","Drawing inspiration from this, we explore the potential of learning from important samples and present an empirical study for evaluating coreset selection techniques in the context of CL to stimulate research in this unexplored area.","We train different continual learners on increasing amounts of selected samples and investigate the learning-forgetting dynamics by shedding light on the underlying mechanisms driving their improved stability-plasticity balance.","We present several significant observations: learning from selectively chosen samples (i) enhances incremental accuracy, (ii) improves knowledge retention of previous tasks, and (iii) refines learned representations.","This analysis contributes to a deeper understanding of selective learning strategies in CL scenarios."],"url":"http://arxiv.org/abs/2410.17715v1"}
{"created":"2024-10-23 09:39:26","title":"A Data-Driven Odyssey in Solar Vehicles","abstract":"Solar vehicles, which simultaneously produce and consume energy, require meticulous energy management. However, potential users often feel uncertain about their operation compared to conventional vehicles. This study presents a simulator designed to help users understand long-distance travel in solar vehicles and recognize the importance of proper energy management. By utilizing Google Maps data and weather information, the simulator replicates real-world driving conditions and provides a dashboard displaying vehicle status, updated hourly based on user-inputted speed. Users can explore various speed policy scenarios and receive recommendations for optimal driving strategies. The simulator's effectiveness was validated using the route of the World Solar Challenge (WSC). This research enables users to monitor energy dynamics before a journey, enhancing their understanding of energy management and informing appropriate speed decisions.","sentences":["Solar vehicles, which simultaneously produce and consume energy, require meticulous energy management.","However, potential users often feel uncertain about their operation compared to conventional vehicles.","This study presents a simulator designed to help users understand long-distance travel in solar vehicles and recognize the importance of proper energy management.","By utilizing Google Maps data and weather information, the simulator replicates real-world driving conditions and provides a dashboard displaying vehicle status, updated hourly based on user-inputted speed.","Users can explore various speed policy scenarios and receive recommendations for optimal driving strategies.","The simulator's effectiveness was validated using the route of the World Solar Challenge (WSC).","This research enables users to monitor energy dynamics before a journey, enhancing their understanding of energy management and informing appropriate speed decisions."],"url":"http://arxiv.org/abs/2410.17712v1"}
{"created":"2024-10-23 09:36:21","title":"Beware of Calibration Data for Pruning Large Language Models","abstract":"As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency. Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a small amount of calibration data to assess the importance of parameters. Previous research has primarily focused on designing advanced pruning methods, while different calibration data's impact on pruning performance still lacks systematical exploration. We fill this blank and surprisingly observe that the effects of calibration data even value more than designing advanced pruning strategies, especially for high sparsity. Our preliminary exploration also discloses that using calibration data similar to the training data can yield better performance. As pre-training data is usually inaccessible for advanced LLMs, we further provide a self-generating calibration data synthesis strategy to construct feasible calibration data. We conduct experiments on the recent strong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that the proposed method outperforms commonly used calibration data and can effectively enhance strong pruning methods (e.g., Wanda, OWL).","sentences":["As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency.","Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a small amount of calibration data to assess the importance of parameters.","Previous research has primarily focused on designing advanced pruning methods, while different calibration data's impact on pruning performance still lacks systematical exploration.","We fill this blank and surprisingly observe that the effects of calibration data even value more than designing advanced pruning strategies, especially for high sparsity.","Our preliminary exploration also discloses that using calibration data similar to the training data can yield better performance.","As pre-training data is usually inaccessible for advanced LLMs, we further provide a self-generating calibration data synthesis strategy to construct feasible calibration data.","We conduct experiments on the recent strong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that the proposed method outperforms commonly used calibration data and can effectively enhance strong pruning methods (e.g., Wanda, OWL)."],"url":"http://arxiv.org/abs/2410.17711v1"}
{"created":"2024-10-23 09:22:43","title":"Scalable Random Feature Latent Variable Models","abstract":"Random feature latent variable models (RFLVMs) represent the state-of-the-art in latent variable models, capable of handling non-Gaussian likelihoods and effectively uncovering patterns in high-dimensional data. However, their heavy reliance on Monte Carlo sampling results in scalability issues which makes it difficult to use these models for datasets with a massive number of observations. To scale up RFLVMs, we turn to the optimization-based variational Bayesian inference (VBI) algorithm which is known for its scalability compared to sampling-based methods. However, implementing VBI for RFLVMs poses challenges, such as the lack of explicit probability distribution functions (PDFs) for the Dirichlet process (DP) in the kernel learning component, and the incompatibility of existing VBI algorithms with RFLVMs. To address these issues, we introduce a stick-breaking construction for DP to obtain an explicit PDF and a novel VBI algorithm called ``block coordinate descent variational inference\" (BCD-VI). This enables the development of a scalable version of RFLVMs, or in short, SRFLVM. Our proposed method shows scalability, computational efficiency, superior performance in generating informative latent representations and the ability of imputing missing data across various real-world datasets, outperforming state-of-the-art competitors.","sentences":["Random feature latent variable models (RFLVMs) represent the state-of-the-art in latent variable models, capable of handling non-Gaussian likelihoods and effectively uncovering patterns in high-dimensional data.","However, their heavy reliance on Monte Carlo sampling results in scalability issues which makes it difficult to use these models for datasets with a massive number of observations.","To scale up RFLVMs, we turn to the optimization-based variational Bayesian inference (VBI) algorithm which is known for its scalability compared to sampling-based methods.","However, implementing VBI for RFLVMs poses challenges, such as the lack of explicit probability distribution functions (PDFs) for the Dirichlet process (DP) in the kernel learning component, and the incompatibility of existing VBI algorithms with RFLVMs.","To address these issues, we introduce a stick-breaking construction for DP to obtain an explicit PDF and a novel VBI algorithm called ``block coordinate descent variational inference\" (BCD-VI).","This enables the development of a scalable version of RFLVMs, or in short, SRFLVM.","Our proposed method shows scalability, computational efficiency, superior performance in generating informative latent representations and the ability of imputing missing data across various real-world datasets, outperforming state-of-the-art competitors."],"url":"http://arxiv.org/abs/2410.17700v1"}
{"created":"2024-10-23 09:10:42","title":"Flexible Process Variant Binding in Information Systems with Software Product Line Engineering","abstract":"Different organisations often run similar digitised business processes to achieve their business goals. However, organisations often need to slightly adapt the business processes implemented in an information system in order to adopt them. Various approaches have been proposed to manage variants in process models. While these approaches mainly deal with control flow variability, in previous work we introduced an approach to manage implementation variants of digitised business processes. In this context Software Product Line (SPL) Engineering was applied to manage a set of common core artefacts including a process model from which Process-Aware Information Systems (PAIS) can be derived, which differ in the implementation of their process activities. % substitute the implementation of activities of a business process. When deriving a PAIS, implementations are selected for each process activity and then included in the PAIS at compilation time. One challenge that has not yet been solved is giving users of digitised business processes the option of selecting several features at runtime, i.e. selecting multiple activity implementations at runtime. This paper extends our previous work by not only allowing for the selection of activity implementations at compile time, but also at start time and runtime. Consequently, it becomes possible to defer the decision as to which features should be selected to start time and runtime. Furthermore, multiple implementations of a particular activity may be selected and executed concurrently. As another challenge different organisation may want to collect and base their decision on different information in a digitised business process. Consequently, the presented approach also allows customising the input and output data of activities when deriving a PAIS for a specific organisation.","sentences":["Different organisations often run similar digitised business processes to achieve their business goals.","However, organisations often need to slightly adapt the business processes implemented in an information system in order to adopt them.","Various approaches have been proposed to manage variants in process models.","While these approaches mainly deal with control flow variability, in previous work we introduced an approach to manage implementation variants of digitised business processes.","In this context Software Product Line (SPL) Engineering was applied to manage a set of common core artefacts including a process model from which Process-Aware Information Systems (PAIS) can be derived, which differ in the implementation of their process activities.","% substitute the implementation of activities of a business process.","When deriving a PAIS, implementations are selected for each process activity and then included in the PAIS at compilation time.","One challenge that has not yet been solved is giving users of digitised business processes the option of selecting several features at runtime, i.e. selecting multiple activity implementations at runtime.","This paper extends our previous work by not only allowing for the selection of activity implementations at compile time, but also at start time and runtime.","Consequently, it becomes possible to defer the decision as to which features should be selected to start time and runtime.","Furthermore, multiple implementations of a particular activity may be selected and executed concurrently.","As another challenge different organisation may want to collect and base their decision on different information in a digitised business process.","Consequently, the presented approach also allows customising the input and output data of activities when deriving a PAIS for a specific organisation."],"url":"http://arxiv.org/abs/2410.17689v1"}
{"created":"2024-10-23 08:49:51","title":"Towards a Similarity-adjusted Surprisal Theory","abstract":"Surprisal theory posits that the cognitive effort required to comprehend a word is determined by its contextual predictability, quantified as surprisal. Traditionally, surprisal theory treats words as distinct entities, overlooking any potential similarity between them. Giulianelli et al. (2023) address this limitation by introducing information value, a measure of predictability designed to account for similarities between communicative units. Our work leverages Ricotta and Szeidl's (2006) diversity index to extend surprisal into a metric that we term similarity-adjusted surprisal, exposing a mathematical relationship between surprisal and information value. Similarity-adjusted surprisal aligns with information value when considering graded similarities and reduces to standard surprisal when words are treated as distinct. Experimental results with reading time data indicate that similarity-adjusted surprisal adds predictive power beyond standard surprisal for certain datasets, suggesting it serves as a complementary measure of comprehension effort.","sentences":["Surprisal theory posits that the cognitive effort required to comprehend a word is determined by its contextual predictability, quantified as surprisal.","Traditionally, surprisal theory treats words as distinct entities, overlooking any potential similarity between them.","Giulianelli et al. (2023) address this limitation by introducing information value, a measure of predictability designed to account for similarities between communicative units.","Our work leverages Ricotta and Szeidl's (2006) diversity index to extend surprisal into a metric that we term similarity-adjusted surprisal, exposing a mathematical relationship between surprisal and information value.","Similarity-adjusted surprisal aligns with information value when considering graded similarities and reduces to standard surprisal when words are treated as distinct.","Experimental results with reading time data indicate that similarity-adjusted surprisal adds predictive power beyond standard surprisal for certain datasets, suggesting it serves as a complementary measure of comprehension effort."],"url":"http://arxiv.org/abs/2410.17676v1"}
{"created":"2024-10-23 08:09:48","title":"Testing Deep Learning Recommender Systems Models on Synthetic GAN-Generated Datasets","abstract":"The published method Generative Adversarial Networks for Recommender Systems (GANRS) allows generating data sets for collaborative filtering recommendation systems. The GANRS source code is available along with a representative set of generated datasets. We have tested the GANRS method by creating multiple synthetic datasets from three different real datasets taken as a source. Experiments include variations in the number of users in the synthetic datasets, as well as a different number of samples. We have also selected six state-of-the-art collaborative filtering deep learning models to test both their comparative performance and the GANRS method. The results show a consistent behavior of the generated datasets compared to the source ones; particularly, in the obtained values and trends of the precision and recall quality measures. The tested deep learning models have also performed as expected on all synthetic datasets, making it possible to compare the results with those obtained from the real source data. Future work is proposed, including different cold start scenarios, unbalanced data, and demographic fairness.","sentences":["The published method Generative Adversarial Networks for Recommender Systems (GANRS) allows generating data sets for collaborative filtering recommendation systems.","The GANRS source code is available along with a representative set of generated datasets.","We have tested the GANRS method by creating multiple synthetic datasets from three different real datasets taken as a source.","Experiments include variations in the number of users in the synthetic datasets, as well as a different number of samples.","We have also selected six state-of-the-art collaborative filtering deep learning models to test both their comparative performance and the GANRS method.","The results show a consistent behavior of the generated datasets compared to the source ones; particularly, in the obtained values and trends of the precision and recall quality measures.","The tested deep learning models have also performed as expected on all synthetic datasets, making it possible to compare the results with those obtained from the real source data.","Future work is proposed, including different cold start scenarios, unbalanced data, and demographic fairness."],"url":"http://arxiv.org/abs/2410.17651v1"}
{"created":"2024-10-23 08:07:00","title":"Towards Active Participant-Centric Vertical Federated Learning: Some Representations May Be All You Need","abstract":"Vertical Federated Learning (VFL) enables collaborative model training across different participants with distinct features and common samples, while preserving data privacy. Existing VFL methodologies often struggle with realistic data partitions, typically incurring high communication costs and significant operational complexity. In this work, we introduce a novel simplified approach to VFL, Active Participant-Centric VFL (APC-VFL), that, to the best of our knowledge, is the first to require only a single communication round between participants, and allows the active participant to do inference in a non collaborative fashion. This method integrates unsupervised representation learning with knowledge distillation to achieve comparable accuracy to traditional VFL methods based on vertical split learning in classical settings, reducing required communication rounds by up to $4200\\times$, while being more flexible. Our approach also shows improvements compared to non-federated local models, as well as a comparable VFL proposal, VFedTrans, offering an efficient and flexible solution for collaborative learning.","sentences":["Vertical Federated Learning (VFL) enables collaborative model training across different participants with distinct features and common samples, while preserving data privacy.","Existing VFL methodologies often struggle with realistic data partitions, typically incurring high communication costs and significant operational complexity.","In this work, we introduce a novel simplified approach to VFL, Active Participant-Centric VFL (APC-VFL), that, to the best of our knowledge, is the first to require only a single communication round between participants, and allows the active participant to do inference in a non collaborative fashion.","This method integrates unsupervised representation learning with knowledge distillation to achieve comparable accuracy to traditional VFL methods based on vertical split learning in classical settings, reducing required communication rounds by up to $4200\\times$, while being more flexible.","Our approach also shows improvements compared to non-federated local models, as well as a comparable VFL proposal, VFedTrans, offering an efficient and flexible solution for collaborative learning."],"url":"http://arxiv.org/abs/2410.17648v1"}
{"created":"2024-10-23 07:58:33","title":"Fixed-Parameter Tractability of Hedge Cut","abstract":"In the Hedge Cut problem, the edges of a graph are partitioned into groups called hedges, and the question is what is the minimum number of hedges to delete to disconnect the graph. Ghaffari, Karger, and Panigrahi [SODA 2017] showed that Hedge Cut can be solved in quasipolynomial-time, raising the hope for a polynomial time algorithm. Jaffke, Lima, Masar\\'ik, Pilipczuk, and Souza [SODA 2023] complemented this result by showing that assuming the Exponential Time Hypothesis (ETH), no polynomial-time algorithm exists. In this paper, we show that Hedge Cut is fixed-parameter tractable parameterized by the solution size $\\ell$ by providing an algorithm with running time $\\binom{O(\\log n) + \\ell}{\\ell} \\cdot m^{O(1)}$, which can be upper bounded by $c^{\\ell} \\cdot (n+m)^{O(1)}$ for any constant $c>1$. This running time captures at the same time the fact that the problem is quasipolynomial-time solvable, and that it is fixed-parameter tractable parameterized by $\\ell$. We further generalize this algorithm to an algorithm with running time $\\binom{O(k \\log n) + \\ell}{\\ell} \\cdot n^{O(k)} \\cdot m^{O(1)}$ for Hedge $k$-Cut.","sentences":["In the Hedge Cut problem, the edges of a graph are partitioned into groups called hedges, and the question is what is the minimum number of hedges to delete to disconnect the graph.","Ghaffari, Karger, and Panigrahi","[SODA 2017] showed that Hedge Cut can be solved in quasipolynomial-time, raising the hope for a polynomial time algorithm.","Jaffke, Lima, Masar\\'ik, Pilipczuk, and Souza [SODA 2023] complemented this result by showing that assuming the Exponential Time Hypothesis (ETH), no polynomial-time algorithm exists.","In this paper, we show that Hedge Cut is fixed-parameter tractable parameterized by the solution size $\\ell$ by providing an algorithm with running time $\\binom{O(\\log n) + \\ell}{\\ell} \\cdot m^{O(1)}$, which can be upper bounded by $c^{\\ell} \\cdot (n+m)^{O(1)}$ for any constant $c>1$. This running time captures at the same time the fact that the problem is quasipolynomial-time solvable, and that it is fixed-parameter tractable parameterized by $\\ell$. We further generalize this algorithm to an algorithm with running time $\\binom{O(k \\log n) + \\ell}{\\ell} \\cdot n^{O(k)} \\cdot m^{O(1)}$ for Hedge $k$-Cut."],"url":"http://arxiv.org/abs/2410.17641v1"}
{"created":"2024-10-23 07:56:48","title":"MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models","abstract":"Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO). Existing visual alignment methods, primarily designed for single-image scenarios, struggle to effectively handle the complexity of multi-image tasks due to the scarcity of diverse training data and the high cost of annotating chosen/rejected pairs. We present Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a visual preference alignment approach that effectively handles multi-image inputs. MIA-DPO mitigates the scarcity of diverse multi-image training data by extending single-image data with unrelated images arranged in grid collages or pic-in-pic formats, significantly reducing the costs associated with multi-image data annotations. Our observation reveals that attention values of LVLMs vary considerably across different images. We use attention values to identify and filter out rejected responses the model may have mistakenly focused on. Our attention-aware selection for constructing the chosen/rejected pairs without relying on (i) human annotation, (ii) extra data, and (iii) external models or APIs. MIA-DPO is compatible with various architectures and outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the recent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's ability to understand single images.","sentences":["Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs.","This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO).","Existing visual alignment methods, primarily designed for single-image scenarios, struggle to effectively handle the complexity of multi-image tasks due to the scarcity of diverse training data and the high cost of annotating chosen/rejected pairs.","We present Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a visual preference alignment approach that effectively handles multi-image inputs.","MIA-DPO mitigates the scarcity of diverse multi-image training data by extending single-image data with unrelated images arranged in grid collages or pic-in-pic formats, significantly reducing the costs associated with multi-image data annotations.","Our observation reveals that attention values of LVLMs vary considerably across different images.","We use attention values to identify and filter out rejected responses the model may have mistakenly focused on.","Our attention-aware selection for constructing the chosen/rejected pairs without relying on (i) human annotation, (ii) extra data, and (iii) external models or APIs.","MIA-DPO is compatible with various architectures and outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the recent InternLM-XC2.5.","Moreover, MIA-DPO has a minimal effect on the model's ability to understand single images."],"url":"http://arxiv.org/abs/2410.17637v1"}
{"created":"2024-10-23 07:44:14","title":"Feature Learning in Attention Mechanisms Is More Compact and Stable Than in Convolution","abstract":"Attention and convolution are fundamental techniques in machine learning. While they use different approaches to learn features - attention mechanisms capture both global and local data relathionships, while convolutional layers focus on local patterns - both methods are effective for various tasks. Although the feature learning of both models is well-studied individually, there has not been a direct comparison of their feature learning dynamics. In this paper, we compare their Lipschitz continuity with respect to the Wasserstein distance and covering numbers under similar settings. We demonstrate that attention processes data in a more compact and stable manner. Compactness refers to the lower variance and intrinsic dimensionality of the activation outputs, while stability refers to the changes between inputs and outputs. We validate our findings through experiments using topological data analysis, measuring the 1-, 2-, and infinity-Wasserstein distances between the outputs of each layer from both models. Furthermore, we extend our comparison to Vision Transformers (ViTs) and ResNets, showing that while ViTs have higher output variance, their feature learning is more stable than that of ResNets.","sentences":["Attention and convolution are fundamental techniques in machine learning.","While they use different approaches to learn features - attention mechanisms capture both global and local data relathionships, while convolutional layers focus on local patterns - both methods are effective for various tasks.","Although the feature learning of both models is well-studied individually, there has not been a direct comparison of their feature learning dynamics.","In this paper, we compare their Lipschitz continuity with respect to the Wasserstein distance and covering numbers under similar settings.","We demonstrate that attention processes data in a more compact and stable manner.","Compactness refers to the lower variance and intrinsic dimensionality of the activation outputs, while stability refers to the changes between inputs and outputs.","We validate our findings through experiments using topological data analysis, measuring the 1-, 2-, and infinity-Wasserstein distances between the outputs of each layer from both models.","Furthermore, we extend our comparison to Vision Transformers (ViTs) and ResNets, showing that while ViTs have higher output variance, their feature learning is more stable than that of ResNets."],"url":"http://arxiv.org/abs/2410.17628v1"}
{"created":"2024-10-23 07:26:19","title":"Bridging the Gaps: Utilizing Unlabeled Face Recognition Datasets to Boost Semi-Supervised Facial Expression Recognition","abstract":"In recent years, Facial Expression Recognition (FER) has gained increasing attention. Most current work focuses on supervised learning, which requires a large amount of labeled and diverse images, while FER suffers from the scarcity of large, diverse datasets and annotation difficulty. To address these problems, we focus on utilizing large unlabeled Face Recognition (FR) datasets to boost semi-supervised FER. Specifically, we first perform face reconstruction pre-training on large-scale facial images without annotations to learn features of facial geometry and expression regions, followed by two-stage fine-tuning on FER datasets with limited labels. In addition, to further alleviate the scarcity of labeled and diverse images, we propose a Mixup-based data augmentation strategy tailored for facial images, and the loss weights of real and virtual images are determined according to the intersection-over-union (IoU) of the faces in the two images. Experiments on RAF-DB, AffectNet, and FERPlus show that our method outperforms existing semi-supervised FER methods and achieves new state-of-the-art performance. Remarkably, with only 5%, 25% training sets,our method achieves 64.02% on AffectNet,and 88.23% on RAF-DB, which is comparable to fully supervised state-of-the-art methods. Codes will be made publicly available at https://github.com/zhelishisongjie/SSFER.","sentences":["In recent years, Facial Expression Recognition (FER) has gained increasing attention.","Most current work focuses on supervised learning, which requires a large amount of labeled and diverse images, while FER suffers from the scarcity of large, diverse datasets and annotation difficulty.","To address these problems, we focus on utilizing large unlabeled Face Recognition (FR) datasets to boost semi-supervised FER.","Specifically, we first perform face reconstruction pre-training on large-scale facial images without annotations to learn features of facial geometry and expression regions, followed by two-stage fine-tuning on FER datasets with limited labels.","In addition, to further alleviate the scarcity of labeled and diverse images, we propose a Mixup-based data augmentation strategy tailored for facial images, and the loss weights of real and virtual images are determined according to the intersection-over-union (IoU) of the faces in the two images.","Experiments on RAF-DB, AffectNet, and FERPlus show that our method outperforms existing semi-supervised FER methods and achieves new state-of-the-art performance.","Remarkably, with only 5%, 25% training sets,our method achieves 64.02% on AffectNet,and 88.23% on RAF-DB, which is comparable to fully supervised state-of-the-art methods.","Codes will be made publicly available at https://github.com/zhelishisongjie/SSFER."],"url":"http://arxiv.org/abs/2410.17622v1"}
{"created":"2024-10-23 07:17:31","title":"From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database Management","abstract":"This study investigates the effectiveness of Large Language Models (LLMs) in processing semi-structured data from PDF documents into structured formats, specifically examining their application in updating the Finnish Sports Clubs Database. Through action research methodology, we developed and evaluated an AI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus models to process data from 72 sports federation membership reports. The system achieved a 90% success rate in automated processing, successfully handling 65 of 72 files without errors and converting over 7,900 rows of data. While the initial development time was comparable to traditional manual processing (three months), the implemented system shows potential for reducing future processing time by approximately 90%. Key challenges included handling multilingual content, processing multi-page datasets, and managing extraneous information. The findings suggest that while LLMs demonstrate significant potential for automating semi-structured data processing tasks, optimal results are achieved through a hybrid approach combining AI automation with selective human oversight. This research contributes to the growing body of literature on practical LLM applications in organizational data management and provides insights into the transformation of traditional data processing workflows.","sentences":["This study investigates the effectiveness of Large Language Models (LLMs) in processing semi-structured data from PDF documents into structured formats, specifically examining their application in updating the Finnish Sports Clubs Database.","Through action research methodology, we developed and evaluated an AI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus models to process data from 72 sports federation membership reports.","The system achieved a 90% success rate in automated processing, successfully handling 65 of 72 files without errors and converting over 7,900 rows of data.","While the initial development time was comparable to traditional manual processing (three months), the implemented system shows potential for reducing future processing time by approximately 90%.","Key challenges included handling multilingual content, processing multi-page datasets, and managing extraneous information.","The findings suggest that while LLMs demonstrate significant potential for automating semi-structured data processing tasks, optimal results are achieved through a hybrid approach combining AI automation with selective human oversight.","This research contributes to the growing body of literature on practical LLM applications in organizational data management and provides insights into the transformation of traditional data processing workflows."],"url":"http://arxiv.org/abs/2410.17619v1"}
{"created":"2024-10-23 07:14:37","title":"Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in Heterogeneous Information Networks","abstract":"This paper explores the applications and challenges of graph neural networks (GNNs) in processing complex graph data brought about by the rapid development of the Internet. Given the heterogeneity and redundancy problems that graph data often have, traditional GNN methods may be overly dependent on the initial structure and attribute information of the graph, which limits their ability to accurately simulate more complex relationships and patterns in the graph. Therefore, this study proposes a graph neural network model under a self-supervised learning framework, which can flexibly combine different types of additional information of the attribute graph and its nodes, so as to better mine the deep features in the graph data. By introducing a self-supervisory mechanism, it is expected to improve the adaptability of existing models to the diversity and complexity of graph data and improve the overall performance of the model.","sentences":["This paper explores the applications and challenges of graph neural networks (GNNs) in processing complex graph data brought about by the rapid development of the Internet.","Given the heterogeneity and redundancy problems that graph data often have, traditional GNN methods may be overly dependent on the initial structure and attribute information of the graph, which limits their ability to accurately simulate more complex relationships and patterns in the graph.","Therefore, this study proposes a graph neural network model under a self-supervised learning framework, which can flexibly combine different types of additional information of the attribute graph and its nodes, so as to better mine the deep features in the graph data.","By introducing a self-supervisory mechanism, it is expected to improve the adaptability of existing models to the diversity and complexity of graph data and improve the overall performance of the model."],"url":"http://arxiv.org/abs/2410.17617v1"}
{"created":"2024-10-23 07:06:08","title":"ImDy: Human Inverse Dynamics from Imitated Observations","abstract":"Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis. However, it is hindered from wider application to general motion due to its limited scalability. Conventional optimization-based ID requires expensive laboratory setups, restricting its availability. To alleviate this problem, we propose to exploit the recently progressive human motion imitation algorithms to learn human inverse dynamics in a data-driven manner. The key insight is that the human ID knowledge is implicitly possessed by motion imitators, though not directly applicable. In light of this, we devise an efficient data collection pipeline with state-of-the-art motion imitation algorithms and physics simulators, resulting in a large-scale human inverse dynamics benchmark as Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint torque and full-body ground reaction force data. With ImDy, we train a data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised manner, which conducts ID and ground reaction force estimation simultaneously. Experiments on ImDy and real-world data demonstrate the impressive competency of ImDyS in human inverse dynamics and ground reaction force estimation. Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is exhibited with downstream applications. The project page is https://foruck.github.io/ImDy/.","sentences":["Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis.","However, it is hindered from wider application to general motion due to its limited scalability.","Conventional optimization-based ID requires expensive laboratory setups, restricting its availability.","To alleviate this problem, we propose to exploit the recently progressive human motion imitation algorithms to learn human inverse dynamics in a data-driven manner.","The key insight is that the human ID knowledge is implicitly possessed by motion imitators, though not directly applicable.","In light of this, we devise an efficient data collection pipeline with state-of-the-art motion imitation algorithms and physics simulators, resulting in a large-scale human inverse dynamics benchmark as Imitated Dynamics (ImDy).","ImDy contains over 150 hours of motion with joint torque and full-body ground reaction force data.","With ImDy, we train a data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised manner, which conducts ID and ground reaction force estimation simultaneously.","Experiments on ImDy and real-world data demonstrate the impressive competency of ImDyS in human inverse dynamics and ground reaction force estimation.","Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is exhibited with downstream applications.","The project page is https://foruck.github.io/ImDy/."],"url":"http://arxiv.org/abs/2410.17610v1"}
{"created":"2024-10-23 07:01:16","title":"Towards Effective Data-Free Knowledge Distillation via Diverse Diffusion Augmentation","abstract":"Data-free knowledge distillation (DFKD) has emerged as a pivotal technique in the domain of model compression, substantially reducing the dependency on the original training data. Nonetheless, conventional DFKD methods that employ synthesized training data are prone to the limitations of inadequate diversity and discrepancies in distribution between the synthesized and original datasets. To address these challenges, this paper introduces an innovative approach to DFKD through diverse diffusion augmentation (DDA). Specifically, we revise the paradigm of common data synthesis in DFKD to a composite process through leveraging diffusion models subsequent to data synthesis for self-supervised augmentation, which generates a spectrum of data samples with similar distributions while retaining controlled variations. Furthermore, to mitigate excessive deviation in the embedding space, we introduce an image filtering technique grounded in cosine similarity to maintain fidelity during the knowledge distillation process. Comprehensive experiments conducted on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets showcase the superior performance of our method across various teacher-student network configurations, outperforming the contemporary state-of-the-art DFKD methods. Code will be available at:https://github.com/SLGSP/DDA.","sentences":["Data-free knowledge distillation (DFKD) has emerged as a pivotal technique in the domain of model compression, substantially reducing the dependency on the original training data.","Nonetheless, conventional DFKD methods that employ synthesized training data are prone to the limitations of inadequate diversity and discrepancies in distribution between the synthesized and original datasets.","To address these challenges, this paper introduces an innovative approach to DFKD through diverse diffusion augmentation (DDA).","Specifically, we revise the paradigm of common data synthesis in DFKD to a composite process through leveraging diffusion models subsequent to data synthesis for self-supervised augmentation, which generates a spectrum of data samples with similar distributions while retaining controlled variations.","Furthermore, to mitigate excessive deviation in the embedding space, we introduce an image filtering technique grounded in cosine similarity to maintain fidelity during the knowledge distillation process.","Comprehensive experiments conducted on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets showcase the superior performance of our method across various teacher-student network configurations, outperforming the contemporary state-of-the-art DFKD methods.","Code will be available at:https://github.com/SLGSP/DDA."],"url":"http://arxiv.org/abs/2410.17606v1"}
{"created":"2024-10-23 06:40:13","title":"A Kernel Perspective on Distillation-based Collaborative Learning","abstract":"Over the past decade, there is a growing interest in collaborative learning that can enhance AI models of multiple parties. However, it is still challenging to enhance performance them without sharing private data and models from individual parties. One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice. To tackle this problem, we rigorously analyze a representative distillation-based algorithm in the view of kernel regression. This work provides the first theoretical results to prove the (nearly) minimax optimality of the nonparametric collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. Inspired by our theoretical results, we also propose a practical distillation-based collaborative learning algorithm based on neural network architecture. Our algorithm successfully bridges the gap between our theoretical assumptions and practical settings with neural networks through feature kernel matching. We simulate various regression tasks to verify our theory and demonstrate the practical feasibility of our proposed algorithm.","sentences":["Over the past decade, there is a growing interest in collaborative learning that can enhance AI models of multiple parties.","However, it is still challenging to enhance performance them without sharing private data and models from individual parties.","One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice.","To tackle this problem, we rigorously analyze a representative distillation-based algorithm in the view of kernel regression.","This work provides the first theoretical results to prove the (nearly) minimax optimality of the nonparametric collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments.","Inspired by our theoretical results, we also propose a practical distillation-based collaborative learning algorithm based on neural network architecture.","Our algorithm successfully bridges the gap between our theoretical assumptions and practical settings with neural networks through feature kernel matching.","We simulate various regression tasks to verify our theory and demonstrate the practical feasibility of our proposed algorithm."],"url":"http://arxiv.org/abs/2410.17592v1"}
{"created":"2024-10-23 05:55:21","title":"Adversarial Domain Adaptation for Metal Cutting Sound Detection: Leveraging Abundant Lab Data for Scarce Industry Data","abstract":"Cutting state monitoring in the milling process is crucial for improving manufacturing efficiency and tool life. Cutting sound detection using machine learning (ML) models, inspired by experienced machinists, can be employed as a cost-effective and non-intrusive monitoring method in a complex manufacturing environment. However, labeling industry data for training is costly and time-consuming. Moreover, industry data is often scarce. In this study, we propose a novel adversarial domain adaptation (DA) approach to leverage abundant lab data to learn from scarce industry data, both labeled, for training a cutting-sound detection model. Rather than adapting the features from separate domains directly, we project them first into two separate latent spaces that jointly work as the feature space for learning domain-independent representations. We also analyze two different mechanisms for adversarial learning where the discriminator works as an adversary and a critic in separate settings, enabling our model to learn expressive domain-invariant and domain-ingrained features, respectively. We collected cutting sound data from multiple sensors in different locations, prepared datasets from lab and industry domain, and evaluated our learning models on them. Experiments showed that our models outperformed the multi-layer perceptron based vanilla domain adaptation models in labeling tasks on the curated datasets, achieving near 92%, 82% and 85% accuracy respectively for three different sensors installed in industry settings.","sentences":["Cutting state monitoring in the milling process is crucial for improving manufacturing efficiency and tool life.","Cutting sound detection using machine learning (ML) models, inspired by experienced machinists, can be employed as a cost-effective and non-intrusive monitoring method in a complex manufacturing environment.","However, labeling industry data for training is costly and time-consuming.","Moreover, industry data is often scarce.","In this study, we propose a novel adversarial domain adaptation (DA) approach to leverage abundant lab data to learn from scarce industry data, both labeled, for training a cutting-sound detection model.","Rather than adapting the features from separate domains directly, we project them first into two separate latent spaces that jointly work as the feature space for learning domain-independent representations.","We also analyze two different mechanisms for adversarial learning where the discriminator works as an adversary and a critic in separate settings, enabling our model to learn expressive domain-invariant and domain-ingrained features, respectively.","We collected cutting sound data from multiple sensors in different locations, prepared datasets from lab and industry domain, and evaluated our learning models on them.","Experiments showed that our models outperformed the multi-layer perceptron based vanilla domain adaptation models in labeling tasks on the curated datasets, achieving near 92%, 82% and 85% accuracy respectively for three different sensors installed in industry settings."],"url":"http://arxiv.org/abs/2410.17574v1"}
{"created":"2024-10-23 05:54:41","title":"Securing Federated Learning Against Novel and Classic Backdoor Threats During Foundation Model Integration","abstract":"Federated learning (FL) enables decentralized model training while preserving privacy. Recently, integrating Foundation Models (FMs) into FL has boosted performance but also introduced a novel backdoor attack mechanism. Attackers can exploit the FM's capabilities to embed backdoors into synthetic data generated by FMs used for model fusion, subsequently infecting all client models through knowledge sharing without involvement in the long-lasting FL process. These novel attacks render existing FL backdoor defenses ineffective, as they primarily detect anomalies among client updates, which may appear uniformly malicious under this attack. Our work proposes a novel data-free defense strategy by constraining abnormal activations in the hidden feature space during model aggregation on the server. The activation constraints, optimized using synthetic data alongside FL training, mitigate the attack while barely affecting model performance, as the parameters remain untouched. Extensive experiments demonstrate its effectiveness against both novel and classic backdoor attacks, outperforming existing defenses while maintaining model performance.","sentences":["Federated learning (FL) enables decentralized model training while preserving privacy.","Recently, integrating Foundation Models (FMs) into FL has boosted performance but also introduced a novel backdoor attack mechanism.","Attackers can exploit the FM's capabilities to embed backdoors into synthetic data generated by FMs used for model fusion, subsequently infecting all client models through knowledge sharing without involvement in the long-lasting FL process.","These novel attacks render existing FL backdoor defenses ineffective, as they primarily detect anomalies among client updates, which may appear uniformly malicious under this attack.","Our work proposes a novel data-free defense strategy by constraining abnormal activations in the hidden feature space during model aggregation on the server.","The activation constraints, optimized using synthetic data alongside FL training, mitigate the attack while barely affecting model performance, as the parameters remain untouched.","Extensive experiments demonstrate its effectiveness against both novel and classic backdoor attacks, outperforming existing defenses while maintaining model performance."],"url":"http://arxiv.org/abs/2410.17573v1"}
{"created":"2024-10-23 05:19:51","title":"Differentially Private Learning Needs Better Model Initialization and Self-Distillation","abstract":"Differentially private SGD (DPSGD) enables privacy-preserving training of language models, but often reduces utility, diversity, and linguistic quality. We introduce DPRefine, a three-phase method that initializes a model using data synthesis from a small pre-trained LM with rigorous filtering, applies DP finetuning on private data, and performs self-distillation to refine outputs. This approach significantly outperforms vanilla DPSGD, with AlpacaEval preferring DPRefine's generations in 78.4% of cases across all datasets. Our analysis reveals that DPRefine reduces linguistic errors in generated text by 84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD. It also reduces inconsistencies of non-private models, such as hallucinated details and misattributed quotes. We find that small models like GPT-2 can be effective for initialization and distillation, highlighting their potential in enabling scalable and efficient deployment of privacy-preserving language.","sentences":["Differentially private SGD (DPSGD) enables privacy-preserving training of language models, but often reduces utility, diversity, and linguistic quality.","We introduce DPRefine, a three-phase method that initializes a model using data synthesis from a small pre-trained LM with rigorous filtering, applies DP finetuning on private data, and performs self-distillation to refine outputs.","This approach significantly outperforms vanilla DPSGD, with AlpacaEval preferring DPRefine's generations in 78.4% of cases across all datasets.","Our analysis reveals that DPRefine reduces linguistic errors in generated text by 84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.","It also reduces inconsistencies of non-private models, such as hallucinated details and misattributed quotes.","We find that small models like GPT-2 can be effective for initialization and distillation, highlighting their potential in enabling scalable and efficient deployment of privacy-preserving language."],"url":"http://arxiv.org/abs/2410.17566v1"}
{"created":"2024-10-23 05:19:20","title":"Double Banking on Knowledge: Customized Modulation and Prototypes for Multi-Modality Semi-supervised Medical Image Segmentation","abstract":"Multi-modality (MM) semi-supervised learning (SSL) based medical image segmentation has recently gained increasing attention for its ability to utilize MM data and reduce reliance on labeled images. However, current methods face several challenges: (1) Complex network designs hinder scalability to scenarios with more than two modalities. (2) Focusing solely on modality-invariant representation while neglecting modality-specific features, leads to incomplete MM learning. (3) Leveraging unlabeled data with generative methods can be unreliable for SSL. To address these problems, we propose Double Bank Dual Consistency (DBDC), a novel MM-SSL approach for medical image segmentation. To address challenge (1), we propose a modality all-in-one segmentation network that accommodates data from any number of modalities, removing the limitation on modality count. To address challenge (2), we design two learnable plug-in banks, Modality-Level Modulation bank (MLMB) and Modality-Level Prototype (MLPB) bank, to capture both modality-invariant and modality-specific knowledge. These banks are updated using our proposed Modality Prototype Contrastive Learning (MPCL). Additionally, we design Modality Adaptive Weighting (MAW) to dynamically adjust learning weights for each modality, ensuring balanced MM learning as different modalities learn at different rates. Finally, to address challenge (3), we introduce a Dual Consistency (DC) strategy that enforces consistency at both the image and feature levels without relying on generative methods. We evaluate our method on a 2-to-4 modality segmentation task using three open-source datasets, and extensive experiments show that our method outperforms state-of-the-art approaches.","sentences":["Multi-modality (MM) semi-supervised learning (SSL) based medical image segmentation has recently gained increasing attention for its ability to utilize MM data and reduce reliance on labeled images.","However, current methods face several challenges: (1) Complex network designs hinder scalability to scenarios with more than two modalities.","(2) Focusing solely on modality-invariant representation while neglecting modality-specific features, leads to incomplete MM learning.","(3) Leveraging unlabeled data with generative methods can be unreliable for SSL.","To address these problems, we propose Double Bank Dual Consistency (DBDC), a novel MM-SSL approach for medical image segmentation.","To address challenge (1), we propose a modality all-in-one segmentation network that accommodates data from any number of modalities, removing the limitation on modality count.","To address challenge (2), we design two learnable plug-in banks, Modality-Level Modulation bank (MLMB) and Modality-Level Prototype (MLPB) bank, to capture both modality-invariant and modality-specific knowledge.","These banks are updated using our proposed Modality Prototype Contrastive Learning (MPCL).","Additionally, we design Modality Adaptive Weighting (MAW) to dynamically adjust learning weights for each modality, ensuring balanced MM learning as different modalities learn at different rates.","Finally, to address challenge (3), we introduce a Dual Consistency (DC) strategy that enforces consistency at both the image and feature levels without relying on generative methods.","We evaluate our method on a 2-to-4 modality segmentation task using three open-source datasets, and extensive experiments show that our method outperforms state-of-the-art approaches."],"url":"http://arxiv.org/abs/2410.17565v1"}
{"created":"2024-10-23 04:43:03","title":"FairDgcl: Fairness-aware Recommendation with Dynamic Graph Contrastive Learning","abstract":"As trustworthy AI continues to advance, the fairness issue in recommendations has received increasing attention. A recommender system is considered unfair when it produces unequal outcomes for different user groups based on user-sensitive attributes (e.g., age, gender). Some researchers have proposed data augmentation-based methods aiming at alleviating user-level unfairness by altering the skewed distribution of training data among various user groups. Despite yielding promising results, they often rely on fairness-related assumptions that may not align with reality, potentially reducing the data quality and negatively affecting model effectiveness. To tackle this issue, in this paper, we study how to implement high-quality data augmentation to improve recommendation fairness. Specifically, we propose FairDgcl, a dynamic graph adversarial contrastive learning framework aiming at improving fairness in recommender system. First, FairDgcl develops an adversarial contrastive network with a view generator and a view discriminator to learn generating fair augmentation strategies in an adversarial style. Then, we propose two dynamic, learnable models to generate contrastive views within contrastive learning framework, which automatically fine-tune the augmentation strategies. Meanwhile, we theoretically show that FairDgcl can simultaneously generate enhanced representations that possess both fairness and accuracy. Lastly, comprehensive experiments conducted on four real-world datasets demonstrate the effectiveness of the proposed FairDgcl.","sentences":["As trustworthy AI continues to advance, the fairness issue in recommendations has received increasing attention.","A recommender system is considered unfair when it produces unequal outcomes for different user groups based on user-sensitive attributes (e.g., age, gender).","Some researchers have proposed data augmentation-based methods aiming at alleviating user-level unfairness by altering the skewed distribution of training data among various user groups.","Despite yielding promising results, they often rely on fairness-related assumptions that may not align with reality, potentially reducing the data quality and negatively affecting model effectiveness.","To tackle this issue, in this paper, we study how to implement high-quality data augmentation to improve recommendation fairness.","Specifically, we propose FairDgcl, a dynamic graph adversarial contrastive learning framework aiming at improving fairness in recommender system.","First, FairDgcl develops an adversarial contrastive network with a view generator and a view discriminator to learn generating fair augmentation strategies in an adversarial style.","Then, we propose two dynamic, learnable models to generate contrastive views within contrastive learning framework, which automatically fine-tune the augmentation strategies.","Meanwhile, we theoretically show that FairDgcl can simultaneously generate enhanced representations that possess both fairness and accuracy.","Lastly, comprehensive experiments conducted on four real-world datasets demonstrate the effectiveness of the proposed FairDgcl."],"url":"http://arxiv.org/abs/2410.17555v1"}
{"created":"2024-10-23 03:53:46","title":"ProtoLens: Advancing Prototype Learning for Fine-Grained Interpretability in Text Classification","abstract":"Deep neural networks have achieved remarkable performance in various text-based tasks but often lack interpretability, making them less suitable for applications where transparency is critical. To address this, we propose ProtoLens, a novel prototype-based model that provides fine-grained, sub-sentence level interpretability for text classification. ProtoLens uses a Prototype-aware Span Extraction module to identify relevant text spans associated with learned prototypes and a Prototype Alignment mechanism to ensure prototypes are semantically meaningful throughout training. By aligning the prototype embeddings with human-understandable examples, ProtoLens provides interpretable predictions while maintaining competitive accuracy. Extensive experiments demonstrate that ProtoLens outperforms both prototype-based and non-interpretable baselines on multiple text classification benchmarks. Code and data are available at \\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}.","sentences":["Deep neural networks have achieved remarkable performance in various text-based tasks but often lack interpretability, making them less suitable for applications where transparency is critical.","To address this, we propose ProtoLens, a novel prototype-based model that provides fine-grained, sub-sentence level interpretability for text classification.","ProtoLens uses a Prototype-aware Span Extraction module to identify relevant text spans associated with learned prototypes and a Prototype Alignment mechanism to ensure prototypes are semantically meaningful throughout training.","By aligning the prototype embeddings with human-understandable examples, ProtoLens provides interpretable predictions while maintaining competitive accuracy.","Extensive experiments demonstrate that ProtoLens outperforms both prototype-based and non-interpretable baselines on multiple text classification benchmarks.","Code and data are available at \\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}."],"url":"http://arxiv.org/abs/2410.17546v1"}
{"created":"2024-10-23 03:50:32","title":"Predicting 30-Day Hospital Readmission in Medicare Patients: Insights from an LSTM Deep Learning Model","abstract":"Readmissions among Medicare beneficiaries are a major problem for the US healthcare system from a perspective of both healthcare operations and patient caregiving outcomes. Our study analyzes Medicare hospital readmissions using LSTM networks with feature engineering to assess feature contributions. We selected variables from admission-level data, inpatient medical history and patient demography. The LSTM model is designed to capture temporal dynamics from admission-level and patient-level data. On a case study on the MIMIC dataset, the LSTM model outperformed the logistic regression baseline, accurately leveraging temporal features to predict readmission. The major features were the Charlson Comorbidity Index, hospital length of stay, the hospital admissions over the past 6 months, while demographic variables were less impactful. This work suggests that LSTM networks offers a more promising approach to improve Medicare patient readmission prediction. It captures temporal interactions in patient databases, enhancing current prediction models for healthcare providers. Adoption of predictive models into clinical practice may be more effective in identifying Medicare patients to provide early and targeted interventions to improve patient outcomes.","sentences":["Readmissions among Medicare beneficiaries are a major problem for the US healthcare system from a perspective of both healthcare operations and patient caregiving outcomes.","Our study analyzes Medicare hospital readmissions using LSTM networks with feature engineering to assess feature contributions.","We selected variables from admission-level data, inpatient medical history and patient demography.","The LSTM model is designed to capture temporal dynamics from admission-level and patient-level data.","On a case study on the MIMIC dataset, the LSTM model outperformed the logistic regression baseline, accurately leveraging temporal features to predict readmission.","The major features were the Charlson Comorbidity Index, hospital length of stay, the hospital admissions over the past 6 months, while demographic variables were less impactful.","This work suggests that LSTM networks offers a more promising approach to improve Medicare patient readmission prediction.","It captures temporal interactions in patient databases, enhancing current prediction models for healthcare providers.","Adoption of predictive models into clinical practice may be more effective in identifying Medicare patients to provide early and targeted interventions to improve patient outcomes."],"url":"http://arxiv.org/abs/2410.17545v1"}
{"created":"2024-10-23 03:43:45","title":"Collision-free Exploration by Mobile Agents Using Pebbles","abstract":"In this paper, we study collision-free graph exploration in an anonymous pot labeled network. Two identical mobile agents, starting from different nodes in $G$ have to explore the nodes of $G$ in such a way that for every node $v$ in $G$, at least one mobile agent visits $v$ and no two agents are in the same node in any round and stop. The agents know the size of the graph but do not know its topology. If an agent arrives in the one-hop neighborhood of the other agent, both agents can detect the presence of the other agent but have no idea at which neighboring node the other agent resides. The agents may wake up in different rounds An agent, after waking up, has no knowledge about the wake-up time of the other agent.   We study the problem of collision-free exploration where some pebbles are placed by an Oracle at the nodes of the graph to assist the agents in achieving collision-free exploration. The Oracle knows the graph, the starting positions of the agents, and their wake-up schedule, and it places some pebbles that may be of different colors, at most one at each node. The number of different colors of the pebbles placed by the Oracle is called the {\\it color index} of the corresponding pebble placement algorithm. The central question we study is as follows: \"What is the minimum number $z$ such that there exists a collision-free exploration of a given graph with pebble placement of color index $z$?\" For general graphs, we show that it is impossible to design an algorithm that achieves collision-free exploration with color index 1. We propose an exploration algorithm with color index 3. We also proposed a polynomial exploration algorithm for bipartite graphs with color index 2.","sentences":["In this paper, we study collision-free graph exploration in an anonymous pot labeled network.","Two identical mobile agents, starting from different nodes in $G$ have to explore the nodes of $G$ in such a way that for every node $v$ in $G$, at least one mobile agent visits $v$ and no two agents are in the same node in any round and stop.","The agents know the size of the graph but do not know its topology.","If an agent arrives in the one-hop neighborhood of the other agent, both agents can detect the presence of the other agent but have no idea at which neighboring node the other agent resides.","The agents may wake up in different rounds An agent, after waking up, has no knowledge about the wake-up time of the other agent.   ","We study the problem of collision-free exploration where some pebbles are placed by an Oracle at the nodes of the graph to assist the agents in achieving collision-free exploration.","The Oracle knows the graph, the starting positions of the agents, and their wake-up schedule, and it places some pebbles that may be of different colors, at most one at each node.","The number of different colors of the pebbles placed by the Oracle is called the {\\it color index} of the corresponding pebble placement algorithm.","The central question we study is as follows: \"What is the minimum number $z$ such that there exists a collision-free exploration of a given graph with pebble placement of color index $z$?\"","For general graphs, we show that it is impossible to design an algorithm that achieves collision-free exploration with color index 1.","We propose an exploration algorithm with color index 3.","We also proposed a polynomial exploration algorithm for bipartite graphs with color index 2."],"url":"http://arxiv.org/abs/2410.17542v1"}
{"created":"2024-10-23 03:38:31","title":"Primal-Dual Spectral Representation for Off-policy Evaluation","abstract":"Off-policy evaluation (OPE) is one of the most fundamental problems in reinforcement learning (RL) to estimate the expected long-term payoff of a given target policy with only experiences from another behavior policy that is potentially unknown. The distribution correction estimation (DICE) family of estimators have advanced the state of the art in OPE by breaking the curse of horizon. However, the major bottleneck of applying DICE estimators lies in the difficulty of solving the saddle-point optimization involved, especially with neural network implementations. In this paper, we tackle this challenge by establishing a linear representation of value function and stationary distribution correction ratio, i.e., primal and dual variables in the DICE framework, using the spectral decomposition of the transition operator. Such primal-dual representation not only bypasses the non-convex non-concave optimization in vanilla DICE, therefore enabling an computational efficient algorithm, but also paves the way for more efficient utilization of historical data. We highlight that our algorithm, SpectralDICE, is the first to leverage the linear representation of primal-dual variables that is both computation and sample efficient, the performance of which is supported by a rigorous theoretical sample complexity guarantee and a thorough empirical evaluation on various benchmarks.","sentences":["Off-policy evaluation (OPE) is one of the most fundamental problems in reinforcement learning (RL) to estimate the expected long-term payoff of a given target policy with only experiences from another behavior policy that is potentially unknown.","The distribution correction estimation (DICE) family of estimators have advanced the state of the art in OPE by breaking the curse of horizon.","However, the major bottleneck of applying DICE estimators lies in the difficulty of solving the saddle-point optimization involved, especially with neural network implementations.","In this paper, we tackle this challenge by establishing a linear representation of value function and stationary distribution correction ratio, i.e., primal and dual variables in the DICE framework, using the spectral decomposition of the transition operator.","Such primal-dual representation not only bypasses the non-convex non-concave optimization in vanilla DICE, therefore enabling an computational efficient algorithm, but also paves the way for more efficient utilization of historical data.","We highlight that our algorithm, SpectralDICE, is the first to leverage the linear representation of primal-dual variables that is both computation and sample efficient, the performance of which is supported by a rigorous theoretical sample complexity guarantee and a thorough empirical evaluation on various benchmarks."],"url":"http://arxiv.org/abs/2410.17538v1"}
{"created":"2024-10-23 03:25:55","title":"FedGMark: Certifiably Robust Watermarking for Federated Graph Learning","abstract":"Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients. However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft. Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner. We take the first step to protect the ownership of FedGL models via backdoor-based watermarking. Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees. To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL. FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks. It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks. Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark. Source code is available at: https://github.com/Yuxin104/FedGMark.","sentences":["Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients.","However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft.","Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner.","We take the first step to protect the ownership of FedGL models via backdoor-based watermarking.","Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees.","To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL.","FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks.","It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks.","Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark.","Source code is available at: https://github.com/Yuxin104/FedGMark."],"url":"http://arxiv.org/abs/2410.17533v1"}
{"created":"2024-10-23 03:19:15","title":"Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact","abstract":"Multilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.","sentences":["Multilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries.","While theoretical foundations are well-established, practical implementation guidelines remain scattered.","This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments.","We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications.","Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods.","Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development.","Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers.","We examine practical solutions through real-world applications in customer service, search engines, and machine translation.","By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems."],"url":"http://arxiv.org/abs/2410.17532v1"}
{"created":"2024-10-23 03:05:33","title":"GDDA: Semantic OOD Detection on Graphs under Covariate Shift via Score-Based Diffusion Models","abstract":"Out-of-distribution (OOD) detection poses a significant challenge for Graph Neural Networks (GNNs), particularly in open-world scenarios with varying distribution shifts. Most existing OOD detection methods on graphs primarily focus on identifying instances in test data domains caused by either semantic shifts (changes in data classes) or covariate shifts (changes in data features), while leaving the simultaneous occurrence of both distribution shifts under-explored. In this work, we address both types of shifts simultaneously and introduce a novel challenge for OOD detection on graphs: graph-level semantic OOD detection under covariate shift. In this scenario, variations between the training and test domains result from the concurrent presence of both covariate and semantic shifts, where only graphs associated with unknown classes are identified as OOD samples (OODs). To tackle this challenge, we propose a novel two-phase framework called Graph Disentangled Diffusion Augmentation (GDDA). The first phase focuses on disentangling graph representations into domain-invariant semantic factors and domain-specific style factors. In the second phase, we introduce a novel distribution-shift-controlled score-based generative diffusion model that generates latent factors outside the training semantic and style spaces. Additionally, auxiliary pseudo-in-distribution (InD) and pseudo-OOD graph representations are employed to enhance the effectiveness of the energy-based semantic OOD detector. Extensive empirical studies on three benchmark datasets demonstrate that our approach outperforms state-of-the-art baselines.","sentences":["Out-of-distribution (OOD) detection poses a significant challenge for Graph Neural Networks (GNNs), particularly in open-world scenarios with varying distribution shifts.","Most existing OOD detection methods on graphs primarily focus on identifying instances in test data domains caused by either semantic shifts (changes in data classes) or covariate shifts (changes in data features), while leaving the simultaneous occurrence of both distribution shifts under-explored.","In this work, we address both types of shifts simultaneously and introduce a novel challenge for OOD detection on graphs: graph-level semantic OOD detection under covariate shift.","In this scenario, variations between the training and test domains result from the concurrent presence of both covariate and semantic shifts, where only graphs associated with unknown classes are identified as OOD samples (OODs).","To tackle this challenge, we propose a novel two-phase framework called Graph Disentangled Diffusion Augmentation (GDDA).","The first phase focuses on disentangling graph representations into domain-invariant semantic factors and domain-specific style factors.","In the second phase, we introduce a novel distribution-shift-controlled score-based generative diffusion model that generates latent factors outside the training semantic and style spaces.","Additionally, auxiliary pseudo-in-distribution (InD) and pseudo-OOD graph representations are employed to enhance the effectiveness of the energy-based semantic OOD detector.","Extensive empirical studies on three benchmark datasets demonstrate that our approach outperforms state-of-the-art baselines."],"url":"http://arxiv.org/abs/2410.17526v1"}
{"created":"2024-10-23 03:04:24","title":"Physics-driven AI for Channel Estimation in Cellular Network","abstract":"In cellular mobile networks, wireless channel quality (CQ) is a crucial factor in determining communication performance and user's network experience. Accurately predicting CQ based on real environmental characteristics, specific base station configurations and user trajectories can help network operators optimize base station deployment, improving coverage and capacity. The Received Signal Reference Power (RSRP) and Signal-to-Interference-plus-Noise Ratio (SINR) of user equipment (UE) are key indicators of CQ in wireless communication. However, existing researches have limitations in terms of generation accuracy. Regression methods such as statistical inference and random forests fail to effectively capture the unique characteristics of wireless environments; theoretical derivations relying on specific communication protocols lack generalization capability; data-driven machine learning (ML) methods like Long Short-Term Memory (LSTM) Network often suffer from a lack of interpretability. To overcome these limitations, we propose physics-informed diffusion models, which accurately generate RSRP and SINR at UE based on the wireless environment, base station configurations, and user trajectories. The model adopts a modular and end-to-end design, employing a teacher-student framework to achieve knowledge distillation. This method integrates expert knowledge into the training of diffusion models, enhancing both the interpretability and accuracy, while also facilitating faster convergence of the model parameters. Furthermore, it allows for self-adaptation in various scenarios through few-shot learning. This approach provides valuable guidance for optimizing base station deployment, predicting user network experience, and building real-world simulators.","sentences":["In cellular mobile networks, wireless channel quality (CQ) is a crucial factor in determining communication performance and user's network experience.","Accurately predicting CQ based on real environmental characteristics, specific base station configurations and user trajectories can help network operators optimize base station deployment, improving coverage and capacity.","The Received Signal Reference Power (RSRP) and Signal-to-Interference-plus-Noise Ratio (SINR) of user equipment (UE) are key indicators of CQ in wireless communication.","However, existing researches have limitations in terms of generation accuracy.","Regression methods such as statistical inference and random forests fail to effectively capture the unique characteristics of wireless environments; theoretical derivations relying on specific communication protocols lack generalization capability; data-driven machine learning (ML) methods like Long Short-Term Memory (LSTM) Network often suffer from a lack of interpretability.","To overcome these limitations, we propose physics-informed diffusion models, which accurately generate RSRP and SINR at UE based on the wireless environment, base station configurations, and user trajectories.","The model adopts a modular and end-to-end design, employing a teacher-student framework to achieve knowledge distillation.","This method integrates expert knowledge into the training of diffusion models, enhancing both the interpretability and accuracy, while also facilitating faster convergence of the model parameters.","Furthermore, it allows for self-adaptation in various scenarios through few-shot learning.","This approach provides valuable guidance for optimizing base station deployment, predicting user network experience, and building real-world simulators."],"url":"http://arxiv.org/abs/2410.17525v1"}
{"created":"2024-10-23 02:25:53","title":"Congestion Forecast for Trains with Railroad-Graph-based Semi-Supervised Learning using Sparse Passenger Reports","abstract":"Forecasting rail congestion is crucial for efficient mobility in transport systems. We present rail congestion forecasting using reports from passengers collected through a transit application. Although reports from passengers have received attention from researchers, ensuring a sufficient volume of reports is challenging due to passenger's reluctance. The limited number of reports results in the sparsity of the congestion label, which can be an issue in building a stable prediction model. To address this issue, we propose a semi-supervised method for congestion forecasting for trains, or SURCONFORT. Our key idea is twofold: firstly, we adopt semi-supervised learning to leverage sparsely labeled data and many unlabeled data. Secondly, in order to complement the unlabeled data from nearby stations, we design a railway network-oriented graph and apply the graph to semi-supervised graph regularization. Empirical experiments with actual reporting data show that SURCONFORT improved the forecasting performance by 14.9% over state-of-the-art methods under the label sparsity.","sentences":["Forecasting rail congestion is crucial for efficient mobility in transport systems.","We present rail congestion forecasting using reports from passengers collected through a transit application.","Although reports from passengers have received attention from researchers, ensuring a sufficient volume of reports is challenging due to passenger's reluctance.","The limited number of reports results in the sparsity of the congestion label, which can be an issue in building a stable prediction model.","To address this issue, we propose a semi-supervised method for congestion forecasting for trains, or SURCONFORT.","Our key idea is twofold: firstly, we adopt semi-supervised learning to leverage sparsely labeled data and many unlabeled data.","Secondly, in order to complement the unlabeled data from nearby stations, we design a railway network-oriented graph and apply the graph to semi-supervised graph regularization.","Empirical experiments with actual reporting data show that SURCONFORT improved the forecasting performance by 14.9% over state-of-the-art methods under the label sparsity."],"url":"http://arxiv.org/abs/2410.17510v1"}
{"created":"2024-10-23 02:22:07","title":"WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models","abstract":"The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques.","sentences":["The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices.","Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency.","However, the inherent relationship between model weights and LLM unlearning has not been extensively examined.","In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation.","By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks.","We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation.","Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b.","To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning.","It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques."],"url":"http://arxiv.org/abs/2410.17509v1"}
{"created":"2024-10-23 02:18:07","title":"Validating a PTAS for Triangle-Free 2-Matching via a Simple Decomposition Theorem","abstract":"A triangle-free (simple) 2-matching is an edge set that has at most $2$ edges incident to each vertex and contains no cycle of length $3$. For the problem of finding a maximum cardinality triangle-free 2-matching in a given graph, a complicated exact algorithm was proposed by Hartvigsen. Recently, a simple PTAS using local search was presented by Bosch-Calvo, Grandoni, and Ameli, but its validity proof is not easy. In this paper, we show a natural and simple decomposition theorem for triangle-free 2-matchings, which leads to a simpler validity proof of the PTAS for the problem.","sentences":["A triangle-free (simple) 2-matching is an edge set that has at most $2$ edges incident to each vertex and contains no cycle of length $3$. For the problem of finding a maximum cardinality triangle-free 2-matching in a given graph, a complicated exact algorithm was proposed by Hartvigsen.","Recently, a simple PTAS using local search was presented by Bosch-Calvo, Grandoni, and Ameli, but its validity proof is not easy.","In this paper, we show a natural and simple decomposition theorem for triangle-free 2-matchings, which leads to a simpler validity proof of the PTAS for the problem."],"url":"http://arxiv.org/abs/2410.17508v1"}
{"created":"2024-10-23 02:09:40","title":"Detecting fake review buyers using network structure: Direct evidence from Amazon","abstract":"Online reviews significantly impact consumers' decision-making process and firms' economic outcomes and are widely seen as crucial to the success of online markets. Firms, therefore, have a strong incentive to manipulate ratings using fake reviews. This presents a problem that academic researchers have tried to solve over two decades and on which platforms expend a large amount of resources. Nevertheless, the prevalence of fake reviews is arguably higher than ever. To combat this, we collect a dataset of reviews for thousands of Amazon products and develop a general and highly accurate method for detecting fake reviews. A unique difference between previous datasets and ours is that we directly observe which sellers buy fake reviews. Thus, while prior research has trained models using lab-generated reviews or proxies for fake reviews, we are able to train a model using actual fake reviews. We show that products that buy fake reviews are highly clustered in the product-reviewer network. Therefore, features constructed from this network are highly predictive of which products buy fake reviews. We show that our network-based approach is also successful at detecting fake reviews even without ground truth data, as unsupervised clustering methods can accurately identify fake review buyers by identifying clusters of products that are closely connected in the network. While text or metadata can be manipulated to evade detection, network-based features are more costly to manipulate because these features result directly from the inherent limitations of buying reviews from online review marketplaces, making our detection approach more robust to manipulation.","sentences":["Online reviews significantly impact consumers' decision-making process and firms' economic outcomes and are widely seen as crucial to the success of online markets.","Firms, therefore, have a strong incentive to manipulate ratings using fake reviews.","This presents a problem that academic researchers have tried to solve over two decades and on which platforms expend a large amount of resources.","Nevertheless, the prevalence of fake reviews is arguably higher than ever.","To combat this, we collect a dataset of reviews for thousands of Amazon products and develop a general and highly accurate method for detecting fake reviews.","A unique difference between previous datasets and ours is that we directly observe which sellers buy fake reviews.","Thus, while prior research has trained models using lab-generated reviews or proxies for fake reviews, we are able to train a model using actual fake reviews.","We show that products that buy fake reviews are highly clustered in the product-reviewer network.","Therefore, features constructed from this network are highly predictive of which products buy fake reviews.","We show that our network-based approach is also successful at detecting fake reviews even without ground truth data, as unsupervised clustering methods can accurately identify fake review buyers by identifying clusters of products that are closely connected in the network.","While text or metadata can be manipulated to evade detection, network-based features are more costly to manipulate because these features result directly from the inherent limitations of buying reviews from online review marketplaces, making our detection approach more robust to manipulation."],"url":"http://arxiv.org/abs/2410.17507v1"}
{"created":"2024-10-23 02:03:49","title":"An Ontology-Enabled Approach For User-Centered and Knowledge-Enabled Explanations of AI Systems","abstract":"Explainable Artificial Intelligence (AI) focuses on helping humans understand the working of AI systems or their decisions and has been a cornerstone of AI for decades. Recent research in explainability has focused on explaining the workings of AI models or model explainability. There have also been several position statements and review papers detailing the needs of end-users for user-centered explainability but fewer implementations. Hence, this thesis seeks to bridge some gaps between model and user-centered explainability. We create an explanation ontology (EO) to represent literature-derived explanation types via their supporting components. We implement a knowledge-augmented question-answering (QA) pipeline to support contextual explanations in a clinical setting. Finally, we are implementing a system to combine explanations from different AI methods and data modalities. Within the EO, we can represent fifteen different explanation types, and we have tested these representations in six exemplar use cases. We find that knowledge augmentations improve the performance of base large language models in the contextualized QA, and the performance is variable across disease groups. In the same setting, clinicians also indicated that they prefer to see actionability as one of the main foci in explanations. In our explanations combination method, we plan to use similarity metrics to determine the similarity of explanations in a chronic disease detection setting. Overall, through this thesis, we design methods that can support knowledge-enabled explanations across different use cases, accounting for the methods in today's AI era that can generate the supporting components of these explanations and domain knowledge sources that can enhance them.","sentences":["Explainable Artificial Intelligence (AI) focuses on helping humans understand the working of AI systems or their decisions and has been a cornerstone of AI for decades.","Recent research in explainability has focused on explaining the workings of AI models or model explainability.","There have also been several position statements and review papers detailing the needs of end-users for user-centered explainability but fewer implementations.","Hence, this thesis seeks to bridge some gaps between model and user-centered explainability.","We create an explanation ontology (EO) to represent literature-derived explanation types via their supporting components.","We implement a knowledge-augmented question-answering (QA) pipeline to support contextual explanations in a clinical setting.","Finally, we are implementing a system to combine explanations from different AI methods and data modalities.","Within the EO, we can represent fifteen different explanation types, and we have tested these representations in six exemplar use cases.","We find that knowledge augmentations improve the performance of base large language models in the contextualized QA, and the performance is variable across disease groups.","In the same setting, clinicians also indicated that they prefer to see actionability as one of the main foci in explanations.","In our explanations combination method, we plan to use similarity metrics to determine the similarity of explanations in a chronic disease detection setting.","Overall, through this thesis, we design methods that can support knowledge-enabled explanations across different use cases, accounting for the methods in today's AI era that can generate the supporting components of these explanations and domain knowledge sources that can enhance them."],"url":"http://arxiv.org/abs/2410.17504v1"}
{"created":"2024-10-23 01:11:29","title":"X-MOBILITY: End-To-End Generalizable Navigation via World Modeling","abstract":"General-purpose navigation in challenging environments remains a significant problem in robotics, with current state-of-the-art approaches facing myriad limitations. Classical approaches struggle with cluttered settings and require extensive tuning, while learning-based methods face difficulties generalizing to out-of-distribution environments. This paper introduces X-Mobility, an end-to-end generalizable navigation model that overcomes existing challenges by leveraging three key ideas. First, X-Mobility employs an auto-regressive world modeling architecture with a latent state space to capture world dynamics. Second, a diverse set of multi-head decoders enables the model to learn a rich state representation that correlates strongly with effective navigation skills. Third, by decoupling world modeling from action policy, our architecture can train effectively on a variety of data sources, both with and without expert policies: off-policy data allows the model to learn world dynamics, while on-policy data with supervisory control enables optimal action policy learning. Through extensive experiments, we demonstrate that X-Mobility not only generalizes effectively but also surpasses current state-of-the-art navigation approaches. Additionally, X-Mobility also achieves zero-shot Sim2Real transferability and shows strong potential for cross-embodiment generalization.","sentences":["General-purpose navigation in challenging environments remains a significant problem in robotics, with current state-of-the-art approaches facing myriad limitations.","Classical approaches struggle with cluttered settings and require extensive tuning, while learning-based methods face difficulties generalizing to out-of-distribution environments.","This paper introduces X-Mobility, an end-to-end generalizable navigation model that overcomes existing challenges by leveraging three key ideas.","First, X-Mobility employs an auto-regressive world modeling architecture with a latent state space to capture world dynamics.","Second, a diverse set of multi-head decoders enables the model to learn a rich state representation that correlates strongly with effective navigation skills.","Third, by decoupling world modeling from action policy, our architecture can train effectively on a variety of data sources, both with and without expert policies: off-policy data allows the model to learn world dynamics, while on-policy data with supervisory control enables optimal action policy learning.","Through extensive experiments, we demonstrate that X-Mobility not only generalizes effectively but also surpasses current state-of-the-art navigation approaches.","Additionally, X-Mobility also achieves zero-shot Sim2Real transferability and shows strong potential for cross-embodiment generalization."],"url":"http://arxiv.org/abs/2410.17491v1"}
{"created":"2024-10-23 00:59:27","title":"Unsupervised Domain Adaptation for Action Recognition via Self-Ensembling and Conditional Embedding Alignment","abstract":"Recent advancements in deep learning-based wearable human action recognition (wHAR) have improved the capture and classification of complex motions, but adoption remains limited due to the lack of expert annotations and domain discrepancies from user variations. Limited annotations hinder the model's ability to generalize to out-of-distribution samples. While data augmentation can improve generalizability, unsupervised augmentation techniques must be applied carefully to avoid introducing noise. Unsupervised domain adaptation (UDA) addresses domain discrepancies by aligning conditional distributions with labeled target samples, but vanilla pseudo-labeling can lead to error propagation. To address these challenges, we propose $\\mu$DAR, a novel joint optimization architecture comprised of three functions: (i) consistency regularizer between augmented samples to improve model classification generalizability, (ii) temporal ensemble for robust pseudo-label generation and (iii) conditional distribution alignment to improve domain generalizability. The temporal ensemble works by aggregating predictions from past epochs to smooth out noisy pseudo-label predictions, which are then used in the conditional distribution alignment module to minimize kernel-based class-wise conditional maximum mean discrepancy ($k$CMMD) between the source and target feature space to learn a domain invariant embedding. The consistency-regularized augmentations ensure that multiple augmentations of the same sample share the same labels; this results in (a) strong generalization with limited source domain samples and (b) consistent pseudo-label generation in target samples. The novel integration of these three modules in $\\mu$DAR results in a range of $\\approx$ 4-12% average macro-F1 score improvement over six state-of-the-art UDA methods in four benchmark wHAR datasets","sentences":["Recent advancements in deep learning-based wearable human action recognition (wHAR) have improved the capture and classification of complex motions, but adoption remains limited due to the lack of expert annotations and domain discrepancies from user variations.","Limited annotations hinder the model's ability to generalize to out-of-distribution samples.","While data augmentation can improve generalizability, unsupervised augmentation techniques must be applied carefully to avoid introducing noise.","Unsupervised domain adaptation (UDA) addresses domain discrepancies by aligning conditional distributions with labeled target samples, but vanilla pseudo-labeling can lead to error propagation.","To address these challenges, we propose $\\mu$DAR, a novel joint optimization architecture comprised of three functions: (i) consistency regularizer between augmented samples to improve model classification generalizability, (ii) temporal ensemble for robust pseudo-label generation and (iii) conditional distribution alignment to improve domain generalizability.","The temporal ensemble works by aggregating predictions from past epochs to smooth out noisy pseudo-label predictions, which are then used in the conditional distribution alignment module to minimize kernel-based class-wise conditional maximum mean discrepancy ($k$CMMD) between the source and target feature space to learn a domain invariant embedding.","The consistency-regularized augmentations ensure that multiple augmentations of the same sample share the same labels; this results in (a) strong generalization with limited source domain samples and (b) consistent pseudo-label generation in target samples.","The novel integration of these three modules in $\\mu$DAR results in a range of $\\approx$ 4-12% average macro-F1 score improvement over six state-of-the-art UDA methods in four benchmark wHAR datasets"],"url":"http://arxiv.org/abs/2410.17489v1"}
{"created":"2024-10-23 00:36:06","title":"VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning","abstract":"Recent studies have augmented large language models (LLMs) with speech capabilities, leading to the development of speech language models (SpeechLMs). Earlier SpeechLMs focused on single-turn speech-based question answering (QA), where user input comprised a speech context and a text question. More recent studies have extended this to multi-turn conversations, though they often require complex, multi-stage supervised fine-tuning (SFT) with diverse data. Another critical challenge with SpeechLMs is catastrophic forgetting-where models optimized for speech tasks suffer significant degradation in text-only performance. To mitigate these issues, we propose a novel single-stage joint speech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone. Our joint SFT combines text-only SFT data with three types of speech-related data: speech recognition and translation, speech-based QA, and mixed-modal SFT. Compared to previous SpeechLMs with 7B or 13B parameters, our 3B model demonstrates superior performance across various speech benchmarks while preserving the original capabilities on text-only tasks. Furthermore, our model shows emergent abilities of effectively handling previously unseen prompts and tasks, including multi-turn, mixed-modal inputs.","sentences":["Recent studies have augmented large language models (LLMs) with speech capabilities, leading to the development of speech language models (SpeechLMs).","Earlier SpeechLMs focused on single-turn speech-based question answering (QA), where user input comprised a speech context and a text question.","More recent studies have extended this to multi-turn conversations, though they often require complex, multi-stage supervised fine-tuning (SFT) with diverse data.","Another critical challenge with SpeechLMs is catastrophic forgetting-where models optimized for speech tasks suffer significant degradation in text-only performance.","To mitigate these issues, we propose a novel single-stage joint speech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone.","Our joint SFT combines text-only SFT data with three types of speech-related data: speech recognition and translation, speech-based QA, and mixed-modal SFT.","Compared to previous SpeechLMs with 7B or 13B parameters, our 3B model demonstrates superior performance across various speech benchmarks while preserving the original capabilities on text-only tasks.","Furthermore, our model shows emergent abilities of effectively handling previously unseen prompts and tasks, including multi-turn, mixed-modal inputs."],"url":"http://arxiv.org/abs/2410.17485v1"}
{"created":"2024-10-23 00:31:17","title":"Which Client is Reliable?: A Reliable and Personalized Prompt-based Federated Learning for Medical Image Question Answering","abstract":"Conventional medical artificial intelligence (AI) models face barriers in clinical application and ethical issues owing to their inability to handle the privacy-sensitive characteristics of medical data. We present a novel personalized federated learning (pFL) method for medical visual question answering (VQA) models, addressing privacy reliability challenges in the medical domain. Our method introduces learnable prompts into a Transformer architecture to efficiently train it on diverse medical datasets without massive computational costs. Then we introduce a reliable client VQA model that incorporates Dempster-Shafer evidence theory to quantify uncertainty in predictions, enhancing the model's reliability. Furthermore, we propose a novel inter-client communication mechanism that uses maximum likelihood estimation to balance accuracy and uncertainty, fostering efficient integration of insights across clients.","sentences":["Conventional medical artificial intelligence (AI) models face barriers in clinical application and ethical issues owing to their inability to handle the privacy-sensitive characteristics of medical data.","We present a novel personalized federated learning (pFL) method for medical visual question answering (VQA) models, addressing privacy reliability challenges in the medical domain.","Our method introduces learnable prompts into a Transformer architecture to efficiently train it on diverse medical datasets without massive computational costs.","Then we introduce a reliable client VQA model that incorporates Dempster-Shafer evidence theory to quantify uncertainty in predictions, enhancing the model's reliability.","Furthermore, we propose a novel inter-client communication mechanism that uses maximum likelihood estimation to balance accuracy and uncertainty, fostering efficient integration of insights across clients."],"url":"http://arxiv.org/abs/2410.17484v1"}
{"created":"2024-10-22 23:57:37","title":"Composing Diffusion Policies for Few-shot Learning of Movement Trajectories","abstract":"Humans can perform various combinations of physical skills without having to relearn skills from scratch every single time. For example, we can swing a bat when walking without having to re-learn such a policy from scratch by composing the individual skills of walking and bat swinging. Enabling robots to combine or compose skills is essential so they can learn novel skills and tasks faster with fewer real world samples. To this end, we propose a novel compositional approach called DSE- Diffusion Score Equilibrium that enables few-shot learning for novel skills by utilizing a combination of base policy priors. Our method is based on probabilistically composing diffusion policies to better model the few-shot demonstration data-distribution than any individual policy. Our goal here is to learn robot motions few-shot and not necessarily goal oriented trajectories. Unfortunately we lack a general purpose metric to evaluate the error between a skill or motion and the provided demonstrations. Hence, we propose a probabilistic measure - Maximum Mean Discrepancy on the Forward Kinematics Kernel (MMD-FK), that is task and action space agnostic. By using our few-shot learning approach DSE, we show that we are able to achieve a reduction of over 30% in MMD-FK across skills and number of demonstrations. Moreover, we show the utility of our approach through real world experiments by teaching novel trajectories to a robot in 5 demonstrations.","sentences":["Humans can perform various combinations of physical skills without having to relearn skills from scratch every single time.","For example, we can swing a bat when walking without having to re-learn such a policy from scratch by composing the individual skills of walking and bat swinging.","Enabling robots to combine or compose skills is essential so they can learn novel skills and tasks faster with fewer real world samples.","To this end, we propose a novel compositional approach called DSE- Diffusion Score Equilibrium that enables few-shot learning for novel skills by utilizing a combination of base policy priors.","Our method is based on probabilistically composing diffusion policies to better model the few-shot demonstration data-distribution than any individual policy.","Our goal here is to learn robot motions few-shot and not necessarily goal oriented trajectories.","Unfortunately we lack a general purpose metric to evaluate the error between a skill or motion and the provided demonstrations.","Hence, we propose a probabilistic measure - Maximum Mean Discrepancy on the Forward Kinematics Kernel (MMD-FK), that is task and action space agnostic.","By using our few-shot learning approach DSE, we show that we are able to achieve a reduction of over 30% in MMD-FK across skills and number of demonstrations.","Moreover, we show the utility of our approach through real world experiments by teaching novel trajectories to a robot in 5 demonstrations."],"url":"http://arxiv.org/abs/2410.17479v1"}
{"created":"2024-10-22 23:14:09","title":"DROP: Distributional and Regular Optimism and Pessimism for Reinforcement Learning","abstract":"In reinforcement learning (RL), temporal difference (TD) error is known to be related to the firing rate of dopamine neurons. It has been observed that each dopamine neuron does not behave uniformly, but each responds to the TD error in an optimistic or pessimistic manner, interpreted as a kind of distributional RL. To explain such a biological data, a heuristic model has also been designed with learning rates asymmetric for the positive and negative TD errors. However, this heuristic model is not theoretically-grounded and unknown whether it can work as a RL algorithm. This paper therefore introduces a novel theoretically-grounded model with optimism and pessimism, which is derived from control as inference. In combination with ensemble learning, a distributional value function as a critic is estimated from regularly introduced optimism and pessimism. Based on its central value, a policy in an actor is improved. This proposed algorithm, so-called DROP (distributional and regular optimism and pessimism), is compared on dynamic tasks. Although the heuristic model showed poor learning performance, DROP showed excellent one in all tasks with high generality. In other words, it was suggested that DROP is a new model that can elicit the potential contributions of optimism and pessimism.","sentences":["In reinforcement learning (RL), temporal difference (TD) error is known to be related to the firing rate of dopamine neurons.","It has been observed that each dopamine neuron does not behave uniformly, but each responds to the TD error in an optimistic or pessimistic manner, interpreted as a kind of distributional RL.","To explain such a biological data, a heuristic model has also been designed with learning rates asymmetric for the positive and negative TD errors.","However, this heuristic model is not theoretically-grounded and unknown whether it can work as a RL algorithm.","This paper therefore introduces a novel theoretically-grounded model with optimism and pessimism, which is derived from control as inference.","In combination with ensemble learning, a distributional value function as a critic is estimated from regularly introduced optimism and pessimism.","Based on its central value, a policy in an actor is improved.","This proposed algorithm, so-called DROP (distributional and regular optimism and pessimism), is compared on dynamic tasks.","Although the heuristic model showed poor learning performance, DROP showed excellent one in all tasks with high generality.","In other words, it was suggested that DROP is a new model that can elicit the potential contributions of optimism and pessimism."],"url":"http://arxiv.org/abs/2410.17473v1"}
{"created":"2024-10-22 22:52:14","title":"AdaptoML-UX: An Adaptive User-centered GUI-based AutoML Toolkit for Non-AI Experts and HCI Researchers","abstract":"The increasing integration of machine learning across various domains has underscored the necessity for accessible systems that non-experts can utilize effectively. To address this need, the field of automated machine learning (AutoML) has developed tools to simplify the construction and optimization of ML pipelines. However, existing AutoML solutions often lack efficiency in creating online pipelines and ease of use for Human-Computer Interaction (HCI) applications. Therefore, in this paper, we introduce AdaptoML-UX, an adaptive framework that incorporates automated feature engineering, machine learning, and incremental learning to assist non-AI experts in developing robust, user-centered ML models. Our toolkit demonstrates the capability to adapt efficiently to diverse problem domains and datasets, particularly in HCI, thereby reducing the necessity for manual experimentation and conserving time and resources. Furthermore, it supports model personalization through incremental learning, customizing models to individual user behaviors. HCI researchers can employ AdaptoML-UX (\\url{https://github.com/MichaelSargious/AdaptoML_UX}) without requiring specialized expertise, as it automates the selection of algorithms, feature engineering, and hyperparameter tuning based on the unique characteristics of the data.","sentences":["The increasing integration of machine learning across various domains has underscored the necessity for accessible systems that non-experts can utilize effectively.","To address this need, the field of automated machine learning (AutoML) has developed tools to simplify the construction and optimization of ML pipelines.","However, existing AutoML solutions often lack efficiency in creating online pipelines and ease of use for Human-Computer Interaction (HCI) applications.","Therefore, in this paper, we introduce AdaptoML-UX, an adaptive framework that incorporates automated feature engineering, machine learning, and incremental learning to assist non-AI experts in developing robust, user-centered ML models.","Our toolkit demonstrates the capability to adapt efficiently to diverse problem domains and datasets, particularly in HCI, thereby reducing the necessity for manual experimentation and conserving time and resources.","Furthermore, it supports model personalization through incremental learning, customizing models to individual user behaviors.","HCI researchers can employ AdaptoML-UX (\\url{https://github.com/MichaelSargious/AdaptoML_UX}) without requiring specialized expertise, as it automates the selection of algorithms, feature engineering, and hyperparameter tuning based on the unique characteristics of the data."],"url":"http://arxiv.org/abs/2410.17469v1"}
{"created":"2024-10-22 22:49:01","title":"Bauplan: zero-copy, scale-up FaaS for data pipelines","abstract":"Chaining functions for longer workloads is a key use case for FaaS platforms in data applications. However, modern data pipelines differ significantly from typical serverless use cases (e.g., webhooks and microservices); this makes it difficult to retrofit existing pipeline frameworks due to structural constraints. In this paper, we describe these limitations in detail and introduce bauplan, a novel FaaS programming model and serverless runtime designed for data practitioners. bauplan enables users to declaratively define functional Directed Acyclic Graphs (DAGs) along with their runtime environments, which are then efficiently executed on cloud-based workers. We show that bauplan achieves both better performance and a superior developer experience for data workloads by making the trade-off of reducing generality in favor of data-awareness","sentences":["Chaining functions for longer workloads is a key use case for FaaS platforms in data applications.","However, modern data pipelines differ significantly from typical serverless use cases (e.g., webhooks and microservices); this makes it difficult to retrofit existing pipeline frameworks due to structural constraints.","In this paper, we describe these limitations in detail and introduce bauplan, a novel FaaS programming model and serverless runtime designed for data practitioners.","bauplan enables users to declaratively define functional Directed Acyclic Graphs (DAGs) along with their runtime environments, which are then efficiently executed on cloud-based workers.","We show that bauplan achieves both better performance and a superior developer experience for data workloads by making the trade-off of reducing generality in favor of data-awareness"],"url":"http://arxiv.org/abs/2410.17465v1"}
{"created":"2024-10-22 22:43:14","title":"Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation","abstract":"Time series data is ubiquitous across various domains, including manufacturing, finance, and healthcare. High-quality annotations are essential for effectively understanding time series and facilitating downstream tasks; however, obtaining such annotations is challenging, particularly in mission-critical domains. In this paper, we propose TESSA, a multi-agent system designed to automatically generate both general and domain-specific annotations for time series data. TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent. The general agent captures common patterns and knowledge across multiple source domains, leveraging both time-series-wise and text-wise features to generate general annotations. Meanwhile, the domain-specific agent utilizes limited annotations from the target domain to learn domain-specific terminology and generate targeted annotations. Extensive experiments on multiple synthetic and real-world datasets demonstrate that TESSA effectively generates high-quality annotations, outperforming existing methods.","sentences":["Time series data is ubiquitous across various domains, including manufacturing, finance, and healthcare.","High-quality annotations are essential for effectively understanding time series and facilitating downstream tasks; however, obtaining such annotations is challenging, particularly in mission-critical domains.","In this paper, we propose TESSA, a multi-agent system designed to automatically generate both general and domain-specific annotations for time series data.","TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent.","The general agent captures common patterns and knowledge across multiple source domains, leveraging both time-series-wise and text-wise features to generate general annotations.","Meanwhile, the domain-specific agent utilizes limited annotations from the target domain to learn domain-specific terminology and generate targeted annotations.","Extensive experiments on multiple synthetic and real-world datasets demonstrate that TESSA effectively generates high-quality annotations, outperforming existing methods."],"url":"http://arxiv.org/abs/2410.17462v1"}
{"created":"2024-10-22 22:31:03","title":"Data Obfuscation through Latent Space Projection (LSP) for Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and Finance Fraud Detection","abstract":"As AI systems increasingly integrate into critical societal sectors, the demand for robust privacy-preserving methods has escalated. This paper introduces Data Obfuscation through Latent Space Projection (LSP), a novel technique aimed at enhancing AI governance and ensuring Responsible AI compliance. LSP uses machine learning to project sensitive data into a latent space, effectively obfuscating it while preserving essential features for model training and inference. Unlike traditional privacy methods like differential privacy or homomorphic encryption, LSP transforms data into an abstract, lower-dimensional form, achieving a delicate balance between data utility and privacy. Leveraging autoencoders and adversarial training, LSP separates sensitive from non-sensitive information, allowing for precise control over privacy-utility trade-offs. We validate LSP's effectiveness through experiments on benchmark datasets and two real-world case studies: healthcare cancer diagnosis and financial fraud analysis. Our results show LSP achieves high performance (98.7% accuracy in image classification) while providing strong privacy (97.3% protection against sensitive attribute inference), outperforming traditional anonymization and privacy-preserving methods. The paper also examines LSP's alignment with global AI governance frameworks, such as GDPR, CCPA, and HIPAA, highlighting its contribution to fairness, transparency, and accountability. By embedding privacy within the machine learning pipeline, LSP offers a promising approach to developing AI systems that respect privacy while delivering valuable insights. We conclude by discussing future research directions, including theoretical privacy guarantees, integration with federated learning, and enhancing latent space interpretability, positioning LSP as a critical tool for ethical AI advancement.","sentences":["As AI systems increasingly integrate into critical societal sectors, the demand for robust privacy-preserving methods has escalated.","This paper introduces Data Obfuscation through Latent Space Projection (LSP), a novel technique aimed at enhancing AI governance and ensuring Responsible AI compliance.","LSP uses machine learning to project sensitive data into a latent space, effectively obfuscating it while preserving essential features for model training and inference.","Unlike traditional privacy methods like differential privacy or homomorphic encryption, LSP transforms data into an abstract, lower-dimensional form, achieving a delicate balance between data utility and privacy.","Leveraging autoencoders and adversarial training, LSP separates sensitive from non-sensitive information, allowing for precise control over privacy-utility trade-offs.","We validate LSP's effectiveness through experiments on benchmark datasets and two real-world case studies: healthcare cancer diagnosis and financial fraud analysis.","Our results show LSP achieves high performance (98.7% accuracy in image classification) while providing strong privacy (97.3% protection against sensitive attribute inference), outperforming traditional anonymization and privacy-preserving methods.","The paper also examines LSP's alignment with global AI governance frameworks, such as GDPR, CCPA, and HIPAA, highlighting its contribution to fairness, transparency, and accountability.","By embedding privacy within the machine learning pipeline, LSP offers a promising approach to developing AI systems that respect privacy while delivering valuable insights.","We conclude by discussing future research directions, including theoretical privacy guarantees, integration with federated learning, and enhancing latent space interpretability, positioning LSP as a critical tool for ethical AI advancement."],"url":"http://arxiv.org/abs/2410.17459v1"}
{"created":"2024-10-22 22:21:39","title":"mmWave-Whisper: Phone Call Eavesdropping and Transcription Using Millimeter-Wave Radar","abstract":"This paper introduces mmWave-Whisper, a system that demonstrates the feasibility of full-corpus automated speech recognition (ASR) on phone calls eavesdropped remotely using off-the-shelf frequency modulated continuous wave (FMCW) millimeter-wave radars. Operating in the 77-81 GHz range, mmWave-Whisper captures earpiece vibrations from smartphones, converts them into audio, and processes the audio to produce speech transcriptions automatically. Unlike previous work that focused on loudspeakers or limited vocabulary, this is the first work to perform such a speech recognition by handling large vocabulary and full sentences on earpiece vibrations from smartphones. This approach expands the potential of radar-audio eavesdropping. mmWave-Whisper addresses challenges such as the lack of large scale training datasets, low SNR, and limited frequency information in radar data through a systematic pipeline designed to leverage synthetic training data, domain adaptation, and inference by incorporating OpenAI's Whisper automatic speech recognition model. The system achieves a word accuracy rate of 44.74% and a character accuracy rate of 62.52% over a range of 25 cm to 125 cm. The paper highlights emerging misuse modalities of AI as the technology evolves rapidly.","sentences":["This paper introduces mmWave-Whisper, a system that demonstrates the feasibility of full-corpus automated speech recognition (ASR) on phone calls eavesdropped remotely using off-the-shelf frequency modulated continuous wave (FMCW) millimeter-wave radars.","Operating in the 77-81 GHz range, mmWave-Whisper captures earpiece vibrations from smartphones, converts them into audio, and processes the audio to produce speech transcriptions automatically.","Unlike previous work that focused on loudspeakers or limited vocabulary, this is the first work to perform such a speech recognition by handling large vocabulary and full sentences on earpiece vibrations from smartphones.","This approach expands the potential of radar-audio eavesdropping.","mmWave-Whisper addresses challenges such as the lack of large scale training datasets, low SNR, and limited frequency information in radar data through a systematic pipeline designed to leverage synthetic training data, domain adaptation, and inference by incorporating OpenAI's Whisper automatic speech recognition model.","The system achieves a word accuracy rate of 44.74% and a character accuracy rate of 62.52% over a range of 25 cm to 125 cm.","The paper highlights emerging misuse modalities of AI as the technology evolves rapidly."],"url":"http://arxiv.org/abs/2410.17457v1"}
{"created":"2024-10-22 21:50:52","title":"In Context Learning and Reasoning for Symbolic Regression with Large Language Models","abstract":"Large Language Models (LLMs) are transformer-based machine learning models that have shown remarkable performance in tasks for which they were not explicitly trained. Here, we explore the potential of LLMs to perform symbolic regression -- a machine-learning method for finding simple and accurate equations from datasets. We prompt GPT-4 to suggest expressions from data, which are then optimized and evaluated using external Python tools. These results are fed back to GPT-4, which proposes improved expressions while optimizing for complexity and loss. Using chain-of-thought prompting, we instruct GPT-4 to analyze the data, prior expressions, and the scientific context (expressed in natural language) for each problem before generating new expressions. We evaluated the workflow in rediscovery of five well-known scientific equations from experimental data, and on an additional dataset without a known equation. GPT-4 successfully rediscovered all five equations, and in general, performed better when prompted to use a scratchpad and consider scientific context. We also demonstrate how strategic prompting improves the model's performance and how the natural language interface simplifies integrating theory with data. Although this approach does not outperform established SR programs where target equations are more complex, LLMs can nonetheless iterate toward improved solutions while following instructions and incorporating scientific context in natural language.","sentences":["Large Language Models (LLMs) are transformer-based machine learning models that have shown remarkable performance in tasks for which they were not explicitly trained.","Here, we explore the potential of LLMs to perform symbolic regression -- a machine-learning method for finding simple and accurate equations from datasets.","We prompt GPT-4 to suggest expressions from data, which are then optimized and evaluated using external Python tools.","These results are fed back to GPT-4, which proposes improved expressions while optimizing for complexity and loss.","Using chain-of-thought prompting, we instruct GPT-4 to analyze the data, prior expressions, and the scientific context (expressed in natural language) for each problem before generating new expressions.","We evaluated the workflow in rediscovery of five well-known scientific equations from experimental data, and on an additional dataset without a known equation.","GPT-4 successfully rediscovered all five equations, and in general, performed better when prompted to use a scratchpad and consider scientific context.","We also demonstrate how strategic prompting improves the model's performance and how the natural language interface simplifies integrating theory with data.","Although this approach does not outperform established SR programs where target equations are more complex, LLMs can nonetheless iterate toward improved solutions while following instructions and incorporating scientific context in natural language."],"url":"http://arxiv.org/abs/2410.17448v1"}
{"created":"2024-10-22 21:46:55","title":"Guaranteeing Conservation Laws with Projection in Physics-Informed Neural Networks","abstract":"Physics-informed neural networks (PINNs) incorporate physical laws into their training to efficiently solve partial differential equations (PDEs) with minimal data. However, PINNs fail to guarantee adherence to conservation laws, which are also important to consider in modeling physical systems. To address this, we proposed PINN-Proj, a PINN-based model that uses a novel projection method to enforce conservation laws. We found that PINN-Proj substantially outperformed PINN in conserving momentum and lowered prediction error by three to four orders of magnitude from the best benchmark tested. PINN-Proj also performed marginally better in the separate task of state prediction on three PDE datasets.","sentences":["Physics-informed neural networks (PINNs) incorporate physical laws into their training to efficiently solve partial differential equations (PDEs) with minimal data.","However, PINNs fail to guarantee adherence to conservation laws, which are also important to consider in modeling physical systems.","To address this, we proposed PINN-Proj, a PINN-based model that uses a novel projection method to enforce conservation laws.","We found that PINN-Proj substantially outperformed PINN in conserving momentum and lowered prediction error by three to four orders of magnitude from the best benchmark tested.","PINN-Proj also performed marginally better in the separate task of state prediction on three PDE datasets."],"url":"http://arxiv.org/abs/2410.17445v1"}
{"created":"2024-10-22 20:57:10","title":"Uncovering RL Integration in SSL Loss: Objective-Specific Implications for Data-Efficient RL","abstract":"In this study, we investigate the effect of SSL objective modifications within the SPR framework, focusing on specific adjustments such as terminal state masking and prioritized replay weighting, which were not explicitly addressed in the original design. While these modifications are specific to RL, they are not universally applicable across all RL algorithms. Therefore, we aim to assess their impact on performance and explore other SSL objectives that do not accommodate these adjustments like Barlow Twins and VICReg. We evaluate six SPR variants on the Atari 100k benchmark, including versions both with and without these modifications. Additionally, we test the performance of these objectives on the DeepMind Control Suite, where such modifications are absent. Our findings reveal that incorporating specific SSL modifications within SPR significantly enhances performance, and this influence extends to subsequent frameworks like SR-SPR and BBF, highlighting the critical importance of SSL objective selection and related adaptations in achieving data efficiency in self-predictive reinforcement learning.","sentences":["In this study, we investigate the effect of SSL objective modifications within the SPR framework, focusing on specific adjustments such as terminal state masking and prioritized replay weighting, which were not explicitly addressed in the original design.","While these modifications are specific to RL, they are not universally applicable across all RL algorithms.","Therefore, we aim to assess their impact on performance and explore other SSL objectives that do not accommodate these adjustments like Barlow Twins and VICReg.","We evaluate six SPR variants on the Atari 100k benchmark, including versions both with and without these modifications.","Additionally, we test the performance of these objectives on the DeepMind Control Suite, where such modifications are absent.","Our findings reveal that incorporating specific SSL modifications within SPR significantly enhances performance, and this influence extends to subsequent frameworks like SR-SPR and BBF, highlighting the critical importance of SSL objective selection and related adaptations in achieving data efficiency in self-predictive reinforcement learning."],"url":"http://arxiv.org/abs/2410.17428v1"}
{"created":"2024-10-22 20:55:20","title":"Sketching, Moment Estimation, and the L\u00e9vy-Khintchine Representation Theorem","abstract":"In the $d$-dimensional turnstile streaming model, a frequency vector $\\mathbf{x}=(\\mathbf{x}(1),\\ldots,\\mathbf{x}(n))\\in (\\mathbb{R}^d)^n$ is updated entry-wisely over a stream. We consider the problem of \\emph{$f$-moment estimation} for which one wants to estimate $$f(\\mathbf{x})=\\sum_{v\\in[n]}f(\\mathbf{x}(v))$$ with a small-space sketch.   In this work we present a simple and generic scheme to construct sketches with the novel idea of hashing indices to \\emph{L\\'evy processes}, from which one can estimate the $f$-moment $f(\\mathbf{x})$ where $f$ is the \\emph{characteristic exponent} of the L\\'evy process. The fundamental \\emph{L\\'evy-Khintchine{} representation theorem} completely characterizes the space of all possible characteristic exponents, which in turn characterizes the set of $f$-moments that can be estimated by this generic scheme.   The new scheme has strong explanatory power. It unifies the construction of many existing sketches ($F_0$, $L_0$, $L_2$, $L_\\alpha$, $L_{p,q}$, etc.) and it implies the tractability of many nearly periodic functions that were previously unclassified. Furthermore, the scheme can be conveniently generalized to multidimensional cases ($d\\geq 2$) by considering multidimensional L\\'evy processes and can be further generalized to estimate \\emph{heterogeneous moments} by projecting different indices with different L\\'evy processes. We conjecture that the set of tractable functions can be characterized using the L\\'evy-Khintchine representation theorem via what we called the \\emph{Fourier-Hahn-L\\'evy} method.","sentences":["In the $d$-dimensional turnstile streaming model, a frequency vector $\\mathbf{x}=(\\mathbf{x}(1),\\ldots,\\mathbf{x}(n))\\in (\\mathbb{R}^d)^n$ is updated entry-wisely over a stream.","We consider the problem of \\emph{$f$-moment estimation} for which one wants to estimate $$f(\\mathbf{x})=\\sum_{v\\in[n]}f(\\mathbf{x}(v))$$ with a small-space sketch.   ","In this work we present a simple and generic scheme to construct sketches with the novel idea of hashing indices to \\emph{L\\'evy processes}, from which one can estimate the $f$-moment $f(\\mathbf{x})$ where $f$ is the \\emph{characteristic exponent} of the L\\'evy process.","The fundamental \\emph{L\\'evy-Khintchine{} representation theorem} completely characterizes the space of all possible characteristic exponents, which in turn characterizes the set of $f$-moments that can be estimated by this generic scheme.   ","The new scheme has strong explanatory power.","It unifies the construction of many existing sketches ($F_0$, $L_0$, $L_2$, $L_\\alpha$, $L_{p,q}$, etc.)","and it implies the tractability of many nearly periodic functions that were previously unclassified.","Furthermore, the scheme can be conveniently generalized to multidimensional cases ($d\\geq 2$) by considering multidimensional L\\'evy processes and can be further generalized to estimate \\emph{heterogeneous moments} by projecting different indices with different L\\'evy processes.","We conjecture that the set of tractable functions can be characterized using the L\\'evy-Khintchine representation theorem via what we called the \\emph{Fourier-Hahn-L\\'evy} method."],"url":"http://arxiv.org/abs/2410.17426v1"}
{"created":"2024-10-22 20:40:53","title":"End-to-End Optimization and Learning of Fair Court Schedules","abstract":"Criminal courts across the United States handle millions of cases every year, and the scheduling of those cases must accommodate a diverse set of constraints, including the preferences and availability of courts, prosecutors, and defense teams. When criminal court schedules are formed, defendants' scheduling preferences often take the least priority, although defendants may face significant consequences (including arrest or detention) for missed court dates. Additionally, studies indicate that defendants' nonappearances impose costs on the courts and other system stakeholders. To address these issues, courts and commentators have begun to recognize that pretrial outcomes for defendants and for the system would be improved with greater attention to court processes, including \\emph{court scheduling practices}. There is thus a need for fair criminal court pretrial scheduling systems that account for defendants' preferences and availability, but the collection of such data poses logistical challenges. Furthermore, optimizing schedules fairly across various parties' preferences is a complex optimization problem, even when such data is available. In an effort to construct such a fair scheduling system under data uncertainty, this paper proposes a joint optimization and learning framework that combines machine learning models trained end-to-end with efficient matching algorithms. This framework aims to produce court scheduling schedules that optimize a principled measure of fairness, balancing the availability and preferences of all parties.","sentences":["Criminal courts across the United States handle millions of cases every year, and the scheduling of those cases must accommodate a diverse set of constraints, including the preferences and availability of courts, prosecutors, and defense teams.","When criminal court schedules are formed, defendants' scheduling preferences often take the least priority, although defendants may face significant consequences (including arrest or detention) for missed court dates.","Additionally, studies indicate that defendants' nonappearances impose costs on the courts and other system stakeholders.","To address these issues, courts and commentators have begun to recognize that pretrial outcomes for defendants and for the system would be improved with greater attention to court processes, including \\emph{court scheduling practices}.","There is thus a need for fair criminal court pretrial scheduling systems that account for defendants' preferences and availability, but the collection of such data poses logistical challenges.","Furthermore, optimizing schedules fairly across various parties' preferences is a complex optimization problem, even when such data is available.","In an effort to construct such a fair scheduling system under data uncertainty, this paper proposes a joint optimization and learning framework that combines machine learning models trained end-to-end with efficient matching algorithms.","This framework aims to produce court scheduling schedules that optimize a principled measure of fairness, balancing the availability and preferences of all parties."],"url":"http://arxiv.org/abs/2410.17415v1"}
{"created":"2024-10-22 20:39:21","title":"Scalable Influence and Fact Tracing for Large Language Model Pretraining","abstract":"Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation. However, it has been challenging to date to apply these methods to the full scale of LLM pretraining. In this paper, we refine existing gradient-based methods to work effectively at scale, allowing us to retrieve influential examples for an 8B-parameter language model from a pretraining corpus of over 160B tokens with no need for subsampling or pre-filtering. Our method combines several techniques, including optimizer state correction, a task-specific Hessian approximation, and normalized encodings, which we find to be critical for performance at scale. In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts. These results demonstrate a misalignment between factual attribution and causal influence. With increasing model size and training tokens, we find that influence more closely aligns with attribution. Finally, we examine different types of examples identified as influential by our method, finding that while many directly entail a particular fact, others support the same output by reinforcing priors on relation types, common entities, and names.","sentences":["Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation.","However, it has been challenging to date to apply these methods to the full scale of LLM pretraining.","In this paper, we refine existing gradient-based methods to work effectively at scale, allowing us to retrieve influential examples for an 8B-parameter language model from a pretraining corpus of over 160B tokens with no need for subsampling or pre-filtering.","Our method combines several techniques, including optimizer state correction, a task-specific Hessian approximation, and normalized encodings, which we find to be critical for performance at scale.","In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts.","These results demonstrate a misalignment between factual attribution and causal influence.","With increasing model size and training tokens, we find that influence more closely aligns with attribution.","Finally, we examine different types of examples identified as influential by our method, finding that while many directly entail a particular fact, others support the same output by reinforcing priors on relation types, common entities, and names."],"url":"http://arxiv.org/abs/2410.17413v1"}
{"created":"2024-10-22 20:36:27","title":"Harnessing Visualization for Climate Action and Sustainable Future","abstract":"The urgency of climate change is now recognized globally. As humanity confronts the critical need to mitigate climate change and foster sustainability, data visualization emerges as a powerful tool with a unique capacity to communicate insights crucial for understanding environmental complexities. This paper explores the critical need for designing and investigating responsible data visualization that can act as a catalyst for engaging communities within global climate action and sustainability efforts. Grounded in prior work and reflecting on a decade of community engagement research, I propose five critical considerations: (1) inclusive and accessible visualizations for enhancing climate education and communication, (2) interactive visualizations for fostering agency and deepening engagement,   (3) in-situ visualizations for reducing spatial indirection,   (4) shared immersive experiences for catalyzing collective action, and (5) accurate, transparent, and credible visualizations for ensuring trust and integrity. These considerations offer strategies and new directions for visualization research, aiming to enhance community engagement, deepen involvement, and foster collective action on critical socio-technical including and beyond climate change.","sentences":["The urgency of climate change is now recognized globally.","As humanity confronts the critical need to mitigate climate change and foster sustainability, data visualization emerges as a powerful tool with a unique capacity to communicate insights crucial for understanding environmental complexities.","This paper explores the critical need for designing and investigating responsible data visualization that can act as a catalyst for engaging communities within global climate action and sustainability efforts.","Grounded in prior work and reflecting on a decade of community engagement research, I propose five critical considerations: (1) inclusive and accessible visualizations for enhancing climate education and communication, (2) interactive visualizations for fostering agency and deepening engagement,   (3) in-situ visualizations for reducing spatial indirection,   (4) shared immersive experiences for catalyzing collective action, and (5) accurate, transparent, and credible visualizations for ensuring trust and integrity.","These considerations offer strategies and new directions for visualization research, aiming to enhance community engagement, deepen involvement, and foster collective action on critical socio-technical including and beyond climate change."],"url":"http://arxiv.org/abs/2410.17411v1"}
{"created":"2024-10-22 20:33:16","title":"Learning Graph Filters for Structure-Function Coupling based Hub Node Identification","abstract":"Over the past two decades, tools from network science have been leveraged to characterize the organization of both structural and functional networks of the brain. One such measure of network organization is hub node identification. Hubs are specialized nodes within a network that link distinct brain units corresponding to specialized functional processes. Conventional methods for identifying hub nodes utilize different types of centrality measures and participation coefficient to profile various aspects of nodal importance. These methods solely rely on the functional connectivity networks constructed from functional magnetic resonance imaging (fMRI), ignoring the structure-function coupling in the brain. In this paper, we introduce a graph signal processing (GSP) based hub detection framework that utilizes both the structural connectivity and the functional activation to identify hub nodes. The proposed framework models functional activity as graph signals on the structural connectivity. Hub nodes are then detected based on the premise that hub nodes are sparse, have higher level of activity compared to their neighbors, and the non-hub nodes' activity can be modeled as the output of a graph-based filter. Based on these assumptions, an optimization framework, GraFHub, is formulated to learn the coefficients of the optimal polynomial graph filter and detect the hub nodes. The proposed framework is evaluated on both simulated data and resting state fMRI (rs-fMRI) data from Human Connectome Project (HCP).","sentences":["Over the past two decades, tools from network science have been leveraged to characterize the organization of both structural and functional networks of the brain.","One such measure of network organization is hub node identification.","Hubs are specialized nodes within a network that link distinct brain units corresponding to specialized functional processes.","Conventional methods for identifying hub nodes utilize different types of centrality measures and participation coefficient to profile various aspects of nodal importance.","These methods solely rely on the functional connectivity networks constructed from functional magnetic resonance imaging (fMRI), ignoring the structure-function coupling in the brain.","In this paper, we introduce a graph signal processing (GSP) based hub detection framework that utilizes both the structural connectivity and the functional activation to identify hub nodes.","The proposed framework models functional activity as graph signals on the structural connectivity.","Hub nodes are then detected based on the premise that hub nodes are sparse, have higher level of activity compared to their neighbors, and the non-hub nodes' activity can be modeled as the output of a graph-based filter.","Based on these assumptions, an optimization framework, GraFHub, is formulated to learn the coefficients of the optimal polynomial graph filter and detect the hub nodes.","The proposed framework is evaluated on both simulated data and resting state fMRI (rs-fMRI) data from Human Connectome Project (HCP)."],"url":"http://arxiv.org/abs/2410.17410v1"}
{"created":"2024-10-22 20:33:10","title":"Geometric Graph Neural Network Modeling of Human Interactions in Crowded Environments","abstract":"Modeling human trajectories in crowded environments is challenging due to the complex nature of pedestrian behavior and interactions. This paper proposes a geometric graph neural network (GNN) architecture that integrates domain knowledge from psychological studies to model pedestrian interactions and predict future trajectories. Unlike prior studies using complete graphs, we define interaction neighborhoods using pedestrians' field of view, motion direction, and distance-based kernel functions to construct graph representations of crowds. Evaluations across multiple datasets demonstrate improved prediction accuracy through reduced average and final displacement error metrics. Our findings underscore the importance of integrating domain knowledge with data-driven approaches for effective modeling of human interactions in crowds.","sentences":["Modeling human trajectories in crowded environments is challenging due to the complex nature of pedestrian behavior and interactions.","This paper proposes a geometric graph neural network (GNN) architecture that integrates domain knowledge from psychological studies to model pedestrian interactions and predict future trajectories.","Unlike prior studies using complete graphs, we define interaction neighborhoods using pedestrians' field of view, motion direction, and distance-based kernel functions to construct graph representations of crowds.","Evaluations across multiple datasets demonstrate improved prediction accuracy through reduced average and final displacement error metrics.","Our findings underscore the importance of integrating domain knowledge with data-driven approaches for effective modeling of human interactions in crowds."],"url":"http://arxiv.org/abs/2410.17409v1"}
{"created":"2024-10-22 20:28:57","title":"ProveRAG: Provenance-Driven Vulnerability Analysis with Automated Retrieval-Augmented LLMs","abstract":"In cybersecurity, security analysts face the challenge of mitigating newly discovered vulnerabilities in real-time, with over 300,000 Common Vulnerabilities and Exposures (CVEs) identified since 1999. The sheer volume of known vulnerabilities complicates the detection of patterns for unknown threats. While LLMs can assist, they often hallucinate and lack alignment with recent threats. Over 25,000 vulnerabilities have been identified so far in 2024, which are introduced after popular LLMs' (e.g., GPT-4) training data cutoff. This raises a major challenge of leveraging LLMs in cybersecurity, where accuracy and up-to-date information are paramount. In this work, we aim to improve the adaptation of LLMs in vulnerability analysis by mimicking how analysts perform such tasks. We propose ProveRAG, an LLM-powered system designed to assist in rapidly analyzing CVEs with automated retrieval augmentation of web data while self-evaluating its responses with verifiable evidence. ProveRAG incorporates a self-critique mechanism to help alleviate omission and hallucination common in the output of LLMs applied in cybersecurity applications. The system cross-references data from verifiable sources (NVD and CWE), giving analysts confidence in the actionable insights provided. Our results indicate that ProveRAG excels in delivering verifiable evidence to the user with over 99% and 97% accuracy in exploitation and mitigation strategies, respectively. This system outperforms direct prompting and chunking retrieval in vulnerability analysis by overcoming temporal and context-window limitations. ProveRAG guides analysts to secure their systems more effectively while documenting the process for future audits.","sentences":["In cybersecurity, security analysts face the challenge of mitigating newly discovered vulnerabilities in real-time, with over 300,000 Common Vulnerabilities and Exposures (CVEs) identified since 1999.","The sheer volume of known vulnerabilities complicates the detection of patterns for unknown threats.","While LLMs can assist, they often hallucinate and lack alignment with recent threats.","Over 25,000 vulnerabilities have been identified so far in 2024, which are introduced after popular LLMs' (e.g., GPT-4) training data cutoff.","This raises a major challenge of leveraging LLMs in cybersecurity, where accuracy and up-to-date information are paramount.","In this work, we aim to improve the adaptation of LLMs in vulnerability analysis by mimicking how analysts perform such tasks.","We propose ProveRAG, an LLM-powered system designed to assist in rapidly analyzing CVEs with automated retrieval augmentation of web data while self-evaluating its responses with verifiable evidence.","ProveRAG incorporates a self-critique mechanism to help alleviate omission and hallucination common in the output of LLMs applied in cybersecurity applications.","The system cross-references data from verifiable sources (NVD and CWE), giving analysts confidence in the actionable insights provided.","Our results indicate that ProveRAG excels in delivering verifiable evidence to the user with over 99% and 97% accuracy in exploitation and mitigation strategies, respectively.","This system outperforms direct prompting and chunking retrieval in vulnerability analysis by overcoming temporal and context-window limitations.","ProveRAG guides analysts to secure their systems more effectively while documenting the process for future audits."],"url":"http://arxiv.org/abs/2410.17406v1"}
{"created":"2024-10-22 20:24:33","title":"A Polylogarithmic Approximation for Directed Steiner Forest in Planar Digraphs","abstract":"We consider Directed Steiner Forest (DSF), a fundamental problem in network design. The input to DSF is a directed edge-weighted graph $G = (V, E)$ and a collection of vertex pairs $\\{(s_i, t_i)\\}_{i \\in [k]}$. The goal is to find a minimum cost subgraph $H$ of $G$ such that $H$ contains an $s_i$-$t_i$ path for each $i \\in [k]$. DSF is NP-Hard and is known to be hard to approximate to a factor of $\\Omega(2^{\\log^{1 - \\epsilon}(n)})$ for any fixed $\\epsilon > 0$ [DK'99]. DSF admits approximation ratios of $O(k^{1/2 + \\epsilon})$ [CEGS'11] and $O(n^{2/3 + \\epsilon})$ [BBMRY'13].   In this work we show that in planar digraphs, an important and useful class of graphs in both theory and practice, DSF is much more tractable. We obtain an $O(\\log^6 k)$-approximation algorithm via the junction tree technique. Our main technical contribution is to prove the existence of a low density junction tree in planar digraphs. To find an approximate junction tree we rely on recent results on rooted directed network design problems [FM'23, CJKZZ'24], in particular, on an LP-based algorithm for the Directed Steiner Tree problem [CJKZZ'24]. Our work and several other recent ones on algorithms for planar digraphs [FM'23, KS'21, CJKZZ'24] are built upon structural insights on planar graph reachability and shortest path separators [Thorup'04].","sentences":["We consider Directed Steiner Forest (DSF), a fundamental problem in network design.","The input to DSF is a directed edge-weighted graph $G = (V, E)$ and a collection of vertex pairs $\\{(s_i, t_i)\\}_{i \\in","[k]}$. The goal is to find a minimum cost subgraph $H$ of $G$ such that $H$ contains an $s_i$-$t_i$ path for each $i \\in","[k]$. DSF is NP-Hard and is known to be hard to approximate to a factor of $\\Omega(2^{\\log^{1 - \\epsilon}(n)})$ for any fixed $\\epsilon > 0$","[DK'99].","DSF admits approximation ratios of $O(k^{1/2 + \\epsilon})$","[CEGS'11] and $O(n^{2/3","+ \\epsilon})$","[BBMRY'13].   In this work we show that in planar digraphs, an important and useful class of graphs in both theory and practice, DSF is much more tractable.","We obtain an $O(\\log^6 k)$-approximation algorithm via the junction tree technique.","Our main technical contribution is to prove the existence of a low density junction tree in planar digraphs.","To find an approximate junction tree we rely on recent results on rooted directed network design problems [FM'23, CJKZZ'24], in particular, on an LP-based algorithm for the Directed Steiner Tree problem [CJKZZ'24].","Our work and several other recent ones on algorithms for planar digraphs [FM'23, KS'21, CJKZZ'24] are built upon structural insights on planar graph reachability and shortest path separators","[Thorup'04]."],"url":"http://arxiv.org/abs/2410.17403v1"}
{"created":"2024-10-22 20:18:26","title":"AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents","abstract":"Vision Language Models (VLMs) have revolutionized the creation of generalist web agents, empowering them to autonomously complete diverse tasks on real-world websites, thereby boosting human efficiency and productivity. However, despite their remarkable capabilities, the safety and security of these agents against malicious attacks remain critically underexplored, raising significant concerns about their safe deployment. To uncover and exploit such vulnerabilities in web agents, we provide AdvWeb, a novel black-box attack framework designed against web agents. AdvWeb trains an adversarial prompter model that generates and injects adversarial prompts into web pages, misleading web agents into executing targeted adversarial actions such as inappropriate stock purchases or incorrect bank transactions, actions that could lead to severe real-world consequences. With only black-box access to the web agent, we train and optimize the adversarial prompter model using DPO, leveraging both successful and failed attack strings against the target agent. Unlike prior approaches, our adversarial string injection maintains stealth and control: (1) the appearance of the website remains unchanged before and after the attack, making it nearly impossible for users to detect tampering, and (2) attackers can modify specific substrings within the generated adversarial string to seamlessly change the attack objective (e.g., purchasing stocks from a different company), enhancing attack flexibility and efficiency. We conduct extensive evaluations, demonstrating that AdvWeb achieves high success rates in attacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings expose critical vulnerabilities in current LLM/VLM-based agents, emphasizing the urgent need for developing more reliable web agents and effective defenses. Our code and data are available at https://ai-secure.github.io/AdvWeb/ .","sentences":["Vision Language Models (VLMs) have revolutionized the creation of generalist web agents, empowering them to autonomously complete diverse tasks on real-world websites, thereby boosting human efficiency and productivity.","However, despite their remarkable capabilities, the safety and security of these agents against malicious attacks remain critically underexplored, raising significant concerns about their safe deployment.","To uncover and exploit such vulnerabilities in web agents, we provide AdvWeb, a novel black-box attack framework designed against web agents.","AdvWeb trains an adversarial prompter model that generates and injects adversarial prompts into web pages, misleading web agents into executing targeted adversarial actions such as inappropriate stock purchases or incorrect bank transactions, actions that could lead to severe real-world consequences.","With only black-box access to the web agent, we train and optimize the adversarial prompter model using DPO, leveraging both successful and failed attack strings against the target agent.","Unlike prior approaches, our adversarial string injection maintains stealth and control: (1) the appearance of the website remains unchanged before and after the attack, making it nearly impossible for users to detect tampering, and (2) attackers can modify specific substrings within the generated adversarial string to seamlessly change the attack objective (e.g., purchasing stocks from a different company), enhancing attack flexibility and efficiency.","We conduct extensive evaluations, demonstrating that AdvWeb achieves high success rates in attacking SOTA GPT-4V-based VLM agent across various web tasks.","Our findings expose critical vulnerabilities in current LLM/VLM-based agents, emphasizing the urgent need for developing more reliable web agents and effective defenses.","Our code and data are available at https://ai-secure.github.io/AdvWeb/ ."],"url":"http://arxiv.org/abs/2410.17401v1"}
{"created":"2024-10-22 20:18:02","title":"Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata","abstract":"Current version identification (VI) datasets often lack sufficient size and musical diversity to train robust neural networks (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped potential of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions containing about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model complexities or data augmentations, which achieves competitive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the extracted audio features, and a trained model, are all publicly available online.","sentences":["Current version identification (VI) datasets often lack sufficient size and musical diversity to train robust neural networks (NNs).","Additionally, their non-representative clique size distributions prevent realistic system evaluations.","To address these challenges, we explore the untapped potential of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions containing about 1,900,000 versions across 348,000 cliques.","Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques.","This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets.","We demonstrate the utility of our dataset by training a baseline NN without extensive model complexities or data augmentations, which achieves competitive results on the SHS100K and Da-TACOS datasets.","Our dataset, along with the tools used for its creation, the extracted audio features, and a trained model, are all publicly available online."],"url":"http://arxiv.org/abs/2410.17400v1"}
{"created":"2024-10-22 20:01:39","title":"packetLSTM: Dynamic LSTM Framework for Streaming Data with Varying Feature Space","abstract":"We study the online learning problem characterized by the varying input feature space of streaming data. Although LSTMs have been employed to effectively capture the temporal nature of streaming data, they cannot handle the dimension-varying streams in an online learning setting. Therefore, we propose a dynamic LSTM-based novel method, called packetLSTM, to model the dimension-varying streams. The packetLSTM's dynamic framework consists of an evolving packet of LSTMs, each dedicated to processing one input feature. Each LSTM retains the local information of its corresponding feature, while a shared common memory consolidates global information. This configuration facilitates continuous learning and mitigates the issue of forgetting, even when certain features are absent for extended time periods. The idea of utilizing one LSTM per feature coupled with a dimension-invariant operator for information aggregation enhances the dynamic nature of packetLSTM. This dynamic nature is evidenced by the model's ability to activate, deactivate, and add new LSTMs as required, thus seamlessly accommodating varying input dimensions. The packetLSTM achieves state-of-the-art results on five datasets, and its underlying principle is extended to other RNN types, like GRU and vanilla RNN.","sentences":["We study the online learning problem characterized by the varying input feature space of streaming data.","Although LSTMs have been employed to effectively capture the temporal nature of streaming data, they cannot handle the dimension-varying streams in an online learning setting.","Therefore, we propose a dynamic LSTM-based novel method, called packetLSTM, to model the dimension-varying streams.","The packetLSTM's dynamic framework consists of an evolving packet of LSTMs, each dedicated to processing one input feature.","Each LSTM retains the local information of its corresponding feature, while a shared common memory consolidates global information.","This configuration facilitates continuous learning and mitigates the issue of forgetting, even when certain features are absent for extended time periods.","The idea of utilizing one LSTM per feature coupled with a dimension-invariant operator for information aggregation enhances the dynamic nature of packetLSTM.","This dynamic nature is evidenced by the model's ability to activate, deactivate, and add new LSTMs as required, thus seamlessly accommodating varying input dimensions.","The packetLSTM achieves state-of-the-art results on five datasets, and its underlying principle is extended to other RNN types, like GRU and vanilla RNN."],"url":"http://arxiv.org/abs/2410.17394v1"}
{"created":"2024-10-22 19:58:56","title":"Revealing The Secret Power: How Algorithms Can Influence Content Visibility on Social Media","abstract":"Online social media platforms significantly influence public debates by shaping the information users encounter. Content visibility on these platforms is regulated by recommendation algorithms designed to maximize user engagement using individual-level data, including personal preferences and interactions. These algorithms play a crucial role in information dissemination, yet their inner workings are often undisclosed, raising concerns about potential manipulation of visibility. While algorithms may be intended to limit the spread of harmful content, they can also be exploited to suppress dissenting voices without users' awareness. The suspicion that platforms deliberately reduce the visibility of certain users or content - commonly known as shadow banning - has garnered significant public attention, with numerous figures advocating for greater transparency around this practice. In this study, we perform a quantitative study geared to identify suspicious changes in content visibility on Twitter (now known as X). We build and study a dataset of over 13 million tweets from more than 5 million users discussing the Ukraine conflict, including each tweet's number of views and metadata, aiming to detect reduced or inflated visibility patterns. We investigate how visibility correlates with factors such as authors' stance, role, interaction networks, and content narratives. Our findings reveal significant variations in visibility, likely driven by algorithmic interventions. These results highlight the need for greater transparency in regulating online information ecosystems to prevent algorithmic manipulation that could undermine public discourse and the fairness of debates.","sentences":["Online social media platforms significantly influence public debates by shaping the information users encounter.","Content visibility on these platforms is regulated by recommendation algorithms designed to maximize user engagement using individual-level data, including personal preferences and interactions.","These algorithms play a crucial role in information dissemination, yet their inner workings are often undisclosed, raising concerns about potential manipulation of visibility.","While algorithms may be intended to limit the spread of harmful content, they can also be exploited to suppress dissenting voices without users' awareness.","The suspicion that platforms deliberately reduce the visibility of certain users or content - commonly known as shadow banning - has garnered significant public attention, with numerous figures advocating for greater transparency around this practice.","In this study, we perform a quantitative study geared to identify suspicious changes in content visibility on Twitter (now known as X).","We build and study a dataset of over 13 million tweets from more than 5 million users discussing the Ukraine conflict, including each tweet's number of views and metadata, aiming to detect reduced or inflated visibility patterns.","We investigate how visibility correlates with factors such as authors' stance, role, interaction networks, and content narratives.","Our findings reveal significant variations in visibility, likely driven by algorithmic interventions.","These results highlight the need for greater transparency in regulating online information ecosystems to prevent algorithmic manipulation that could undermine public discourse and the fairness of debates."],"url":"http://arxiv.org/abs/2410.17390v1"}
{"created":"2024-10-22 19:51:37","title":"ICT Sector Greenhouse Gas Emissions -- Issues and Trends","abstract":"As Information and Communication Technology (ICT) use has become more prevalent, there has been a growing concern in how its associated greenhouse gas emissions will impact the climate. Estimating such ICT emissions is a difficult undertaking due to its complexity, its rapidly changing nature, and the lack of accurate and up-to-date data on individual stakeholder emissions. In this paper we provide a framework for estimating ICT's carbon footprint and identify some of the issues that impede the task. We attempt to gain greater insight into the factors affecting the ICT sector by drawing on a number of interviews with industry experts. We conclude that more accurate emissions estimates will only be possible with a more more detailed, industry informed, understanding of the whole ICT landscape and much more transparent reporting of energy usage and emissions data by ICT stakeholders.","sentences":["As Information and Communication Technology (ICT) use has become more prevalent, there has been a growing concern in how its associated greenhouse gas emissions will impact the climate.","Estimating such ICT emissions is a difficult undertaking due to its complexity, its rapidly changing nature, and the lack of accurate and up-to-date data on individual stakeholder emissions.","In this paper we provide a framework for estimating ICT's carbon footprint and identify some of the issues that impede the task.","We attempt to gain greater insight into the factors affecting the ICT sector by drawing on a number of interviews with industry experts.","We conclude that more accurate emissions estimates will only be possible with a more more detailed, industry informed, understanding of the whole ICT landscape and much more transparent reporting of energy usage and emissions data by ICT stakeholders."],"url":"http://arxiv.org/abs/2410.17388v1"}
