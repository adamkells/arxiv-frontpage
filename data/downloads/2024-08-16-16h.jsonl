{"created":"2024-08-14 17:50:27","title":"End-to-end Semantic-centric Video-based Multimodal Affective Computing","abstract":"In the pathway toward Artificial General Intelligence (AGI), understanding human's affection is essential to enhance machine's cognition abilities. For achieving more sensual human-AI interaction, Multimodal Affective Computing (MAC) in human-spoken videos has attracted increasing attention. However, previous methods are mainly devoted to designing multimodal fusion algorithms, suffering from two issues: semantic imbalance caused by diverse pre-processing operations and semantic mismatch raised by inconsistent affection content contained in different modalities comparing with the multimodal ground truth. Besides, the usage of manual features extractors make they fail in building end-to-end pipeline for multiple MAC downstream tasks. To address above challenges, we propose a novel end-to-end framework named SemanticMAC to compute multimodal semantic-centric affection for human-spoken videos. We firstly employ pre-trained Transformer model in multimodal data pre-processing and design Affective Perceiver module to capture unimodal affective information. Moreover, we present a semantic-centric approach to unify multimodal representation learning in three ways, including gated feature interaction, multi-task pseudo label generation, and intra-/inter-sample contrastive learning. Finally, SemanticMAC effectively learn specific- and shared-semantic representations in the guidance of semantic-centric labels. Extensive experimental results demonstrate that our approach surpass the state-of-the-art methods on 7 public datasets in four MAC downstream tasks.","sentences":["In the pathway toward Artificial General Intelligence (AGI), understanding human's affection is essential to enhance machine's cognition abilities.","For achieving more sensual human-AI interaction, Multimodal Affective Computing (MAC) in human-spoken videos has attracted increasing attention.","However, previous methods are mainly devoted to designing multimodal fusion algorithms, suffering from two issues: semantic imbalance caused by diverse pre-processing operations and semantic mismatch raised by inconsistent affection content contained in different modalities comparing with the multimodal ground truth.","Besides, the usage of manual features extractors make they fail in building end-to-end pipeline for multiple MAC downstream tasks.","To address above challenges, we propose a novel end-to-end framework named SemanticMAC to compute multimodal semantic-centric affection for human-spoken videos.","We firstly employ pre-trained Transformer model in multimodal data pre-processing and design Affective Perceiver module to capture unimodal affective information.","Moreover, we present a semantic-centric approach to unify multimodal representation learning in three ways, including gated feature interaction, multi-task pseudo label generation, and intra-/inter-sample contrastive learning.","Finally, SemanticMAC effectively learn specific- and shared-semantic representations in the guidance of semantic-centric labels.","Extensive experimental results demonstrate that our approach surpass the state-of-the-art methods on 7 public datasets in four MAC downstream tasks."],"url":"http://arxiv.org/abs/2408.07694v1"}
{"created":"2024-08-14 17:45:13","title":"Detecting Near-Duplicate Face Images","abstract":"Near-duplicate images are often generated when applying repeated photometric and geometric transformations that produce imperceptible variants of the original image. Consequently, a deluge of near-duplicates can be circulated online posing copyright infringement concerns. The concerns are more severe when biometric data is altered through such nuanced transformations. In this work, we address the challenge of near-duplicate detection in face images by, firstly, identifying the original image from a set of near-duplicates and, secondly, deducing the relationship between the original image and the near-duplicates. We construct a tree-like structure, called an Image Phylogeny Tree (IPT) using a graph-theoretic approach to estimate the relationship, i.e., determine the sequence in which they have been generated. We further extend our method to create an ensemble of IPTs known as Image Phylogeny Forests (IPFs). We rigorously evaluate our method to demonstrate robustness across other modalities, unseen transformations by latest generative models and IPT configurations, thereby significantly advancing the state-of-the-art performance by 42% on IPF reconstruction accuracy.","sentences":["Near-duplicate images are often generated when applying repeated photometric and geometric transformations that produce imperceptible variants of the original image.","Consequently, a deluge of near-duplicates can be circulated online posing copyright infringement concerns.","The concerns are more severe when biometric data is altered through such nuanced transformations.","In this work, we address the challenge of near-duplicate detection in face images by, firstly, identifying the original image from a set of near-duplicates and, secondly, deducing the relationship between the original image and the near-duplicates.","We construct a tree-like structure, called an Image Phylogeny Tree (IPT) using a graph-theoretic approach to estimate the relationship, i.e., determine the sequence in which they have been generated.","We further extend our method to create an ensemble of IPTs known as Image Phylogeny Forests (IPFs).","We rigorously evaluate our method to demonstrate robustness across other modalities, unseen transformations by latest generative models and IPT configurations, thereby significantly advancing the state-of-the-art performance by 42% on IPF reconstruction accuracy."],"url":"http://arxiv.org/abs/2408.07689v1"}
{"created":"2024-08-14 17:16:50","title":"Deep Learning: a Heuristic Three-stage Mechanism for Grid Searches to Optimize the Future Risk Prediction of Breast Cancer Metastasis Using EHR-based Clinical Data","abstract":"A grid search, at the cost of training and testing a large number of models, is an effective way to optimize the prediction performance of deep learning models. A challenging task concerning grid search is the time management. Without a good time management scheme, a grid search can easily be set off as a mission that will not finish in our lifetime. In this study, we introduce a heuristic three-stage mechanism for managing the running time of low-budget grid searches, and the sweet-spot grid search (SSGS) and randomized grid search (RGS) strategies for improving model prediction performance, in predicting the 5-year, 10-year, and 15-year risk of breast cancer metastasis. We develop deep feedforward neural network (DFNN) models and optimize them through grid searches. We conduct eight cycles of grid searches by applying our three-stage mechanism and SSGS and RGS strategies. We conduct various SHAP analyses including unique ones that interpret the importance of the DFNN-model hyperparameters. Our results show that grid search can greatly improve model prediction. The grid searches we conducted improved the risk prediction of 5-year, 10-year, and 15-year breast cancer metastasis by 18.6%, 16.3%, and 17.3% respectively, over the average performance of all corresponding models we trained using the RGS strategy. We not only demonstrate best model performance but also characterize grid searches from various aspects such as their capabilities of discovering decent models and the unit grid search time. The three-stage mechanism worked effectively. It made our low-budget grid searches feasible and manageable, and in the meantime helped improve model prediction performance. Our SHAP analyses identified both clinical risk factors important for the prediction of future risk of breast cancer metastasis, and DFNN-model hyperparameters important to the prediction of performance scores.","sentences":["A grid search, at the cost of training and testing a large number of models, is an effective way to optimize the prediction performance of deep learning models.","A challenging task concerning grid search is the time management.","Without a good time management scheme, a grid search can easily be set off as a mission that will not finish in our lifetime.","In this study, we introduce a heuristic three-stage mechanism for managing the running time of low-budget grid searches, and the sweet-spot grid search (SSGS) and randomized grid search (RGS) strategies for improving model prediction performance, in predicting the 5-year, 10-year, and 15-year risk of breast cancer metastasis.","We develop deep feedforward neural network (DFNN) models and optimize them through grid searches.","We conduct eight cycles of grid searches by applying our three-stage mechanism and SSGS and RGS strategies.","We conduct various SHAP analyses including unique ones that interpret the importance of the DFNN-model hyperparameters.","Our results show that grid search can greatly improve model prediction.","The grid searches we conducted improved the risk prediction of 5-year, 10-year, and 15-year breast cancer metastasis by 18.6%, 16.3%, and 17.3% respectively, over the average performance of all corresponding models we trained using the RGS strategy.","We not only demonstrate best model performance but also characterize grid searches from various aspects such as their capabilities of discovering decent models and the unit grid search time.","The three-stage mechanism worked effectively.","It made our low-budget grid searches feasible and manageable, and in the meantime helped improve model prediction performance.","Our SHAP analyses identified both clinical risk factors important for the prediction of future risk of breast cancer metastasis, and DFNN-model hyperparameters important to the prediction of performance scores."],"url":"http://arxiv.org/abs/2408.07673v2"}
{"created":"2024-08-14 16:58:48","title":"Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities","abstract":"Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.","sentences":["Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation.","As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively.","However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques.","This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions.","Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods.","Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc.","Finally, we highlight the remaining challenges of model merging and discuss future research directions.","A comprehensive list of papers about model merging is available at \\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."],"url":"http://arxiv.org/abs/2408.07666v2"}
{"created":"2024-08-14 16:55:06","title":"Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models","abstract":"Warning: This paper may contain texts with uncomfortable content.   Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.","sentences":["Warning: This paper may contain texts with uncomfortable content.   ","Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech.","However, these models often exhibit biases due to the nature of their training data.","Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases.","This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs.","By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases.","Our experiments reveal significant insights into their performance and bias levels.","The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies."],"url":"http://arxiv.org/abs/2408.07665v1"}
{"created":"2024-08-14 16:49:25","title":"Interpretable Graph Neural Networks for Heterogeneous Tabular Data","abstract":"Many machine learning algorithms for tabular data produce black-box models, which prevent users from understanding the rationale behind the model predictions. In their unconstrained form, graph neural networks fall into this category, and they have further limited abilities to handle heterogeneous data. To overcome these limitations, an approach is proposed, called IGNH (Interpretable Graph Neural Network for Heterogeneous tabular data), which handles both categorical and numerical features, while constraining the learning process to generate exact feature attributions together with the predictions. A large-scale empirical investigation is presented, showing that the feature attributions provided by IGNH align with Shapley values that are computed post hoc. Furthermore, the results show that IGNH outperforms two powerful machine learning algorithms for tabular data, Random Forests and TabNet, while reaching a similar level of performance as XGBoost.","sentences":["Many machine learning algorithms for tabular data produce black-box models, which prevent users from understanding the rationale behind the model predictions.","In their unconstrained form, graph neural networks fall into this category, and they have further limited abilities to handle heterogeneous data.","To overcome these limitations, an approach is proposed, called IGNH (Interpretable Graph Neural Network for Heterogeneous tabular data), which handles both categorical and numerical features, while constraining the learning process to generate exact feature attributions together with the predictions.","A large-scale empirical investigation is presented, showing that the feature attributions provided by IGNH align with Shapley values that are computed post hoc.","Furthermore, the results show that IGNH outperforms two powerful machine learning algorithms for tabular data, Random Forests and TabNet, while reaching a similar level of performance as XGBoost."],"url":"http://arxiv.org/abs/2408.07661v1"}
{"created":"2024-08-14 16:29:57","title":"Development of simulation model for Single Carrier Transceiver for Nanosatellite","abstract":"CubeSat is a nanosatellite concept emerged from a paper published by Stanford University and with their low cost nature and extreme feasibility , more started researching on nano satellites. New technology emerged , paving path to many academics and small vendors to create their own CubeSat models .   This nanosatellite requires a transceiver to maintain its communication between it's systems and the ground station, which helps it navigate and collects data gained from its programmed functions. This transceiver system consists mainly of a transmitter and a receiver. The transmitter manages sending data from the satellite to ground station while the receiver captures the data and instruction sent from the ground station to the satellite. These systems were built using separate digital communication devices in the beginning, with many critical limitations with respect to the space and scalability of the modules to be attached and the programmability of hardware materials and the concept of system-on -board emerged. Meanwhile, As the size of electronic devices minimized with the research conducted, FPGA (Filed Programmable Logic Array) was introduced as an architecture to be used in various applications and research needs. The reason FPGA was mention was for the fact that it provides flexibility in the designing of transceiver ed with design and prototypes implementation at a low cost competitional electronic ware and the system -on chip Concept was introduced . This research describes the development a system-on-chip transceiver model for nanosatellites which contains a single carrier . Keywords-Single Carrier, transceiver, system C, FPGA","sentences":["CubeSat is a nanosatellite concept emerged from a paper published by Stanford University and with their low cost nature and extreme feasibility , more started researching on nano satellites.","New technology emerged , paving path to many academics and small vendors to create their own CubeSat models .   ","This nanosatellite requires a transceiver to maintain its communication between it's systems and the ground station, which helps it navigate and collects data gained from its programmed functions.","This transceiver system consists mainly of a transmitter and a receiver.","The transmitter manages sending data from the satellite to ground station while the receiver captures the data and instruction sent from the ground station to the satellite.","These systems were built using separate digital communication devices in the beginning, with many critical limitations with respect to the space and scalability of the modules to be attached and the programmability of hardware materials and the concept of system-on -board emerged.","Meanwhile, As the size of electronic devices minimized with the research conducted, FPGA (Filed Programmable Logic Array) was introduced as an architecture to be used in various applications and research needs.","The reason FPGA was mention was for the fact that it provides flexibility in the designing of transceiver ed with design and prototypes implementation at a low cost competitional electronic ware and the system -on chip Concept was introduced .","This research describes the development a system-on-chip transceiver model for nanosatellites which contains a single carrier .","Keywords-Single Carrier, transceiver, system C, FPGA"],"url":"http://arxiv.org/abs/2408.07655v1"}
{"created":"2024-08-14 16:27:16","title":"The Semantics of Metapropramming in Prolog","abstract":"This paper describes a semantics for pure Prolog programs with negation that provides meaning to metaprograms. Metaprograms are programs that construct and use data structures as programs. In Prolog a primary mataprogramming construct is the use of a variable as a literal in the body of a clause. The traditional Prolog 3-line metainterpreter is another example of a metaprogram. The account given here also supplies a meaning for clauses that have a variable as head, even though most Prolog systems do not support such clauses. This semantics naturally includes such programs, giving them their intuitive meaning.   Ideas from M. Denecker and his colleagues form the basis of this approach. The key idea is to notice that if we give meanings to all propositional programs and treat Prolog rules with variables as the set of their ground instances, then we can give meanings to all programs. We must treat Prolog rules (which may be metarules) as templates for generating ground propositional rules, and not as first-order formulas, which they may not be. We use parameterized inductive definitions to give propositional models to Prolog programs, in which the propositions are expressions. Then the set of expressions of a propositional model determine a first-order Herbrand Model, providing a first-order logical semantics for all (pure) Prolog programs, including metaprograms.   We give examples to show the applicability of this theory. We also demonstrate how this theory makes proofs of some important properties of metaprograms very straightforward.","sentences":["This paper describes a semantics for pure Prolog programs with negation that provides meaning to metaprograms.","Metaprograms are programs that construct and use data structures as programs.","In Prolog a primary mataprogramming construct is the use of a variable as a literal in the body of a clause.","The traditional Prolog 3-line metainterpreter is another example of a metaprogram.","The account given here also supplies a meaning for clauses that have a variable as head, even though most Prolog systems do not support such clauses.","This semantics naturally includes such programs, giving them their intuitive meaning.   Ideas from M. Denecker and his colleagues form the basis of this approach.","The key idea is to notice that if we give meanings to all propositional programs and treat Prolog rules with variables as the set of their ground instances, then we can give meanings to all programs.","We must treat Prolog rules (which may be metarules) as templates for generating ground propositional rules, and not as first-order formulas, which they may not be.","We use parameterized inductive definitions to give propositional models to Prolog programs, in which the propositions are expressions.","Then the set of expressions of a propositional model determine a first-order Herbrand Model, providing a first-order logical semantics for all (pure) Prolog programs, including metaprograms.   ","We give examples to show the applicability of this theory.","We also demonstrate how this theory makes proofs of some important properties of metaprograms very straightforward."],"url":"http://arxiv.org/abs/2408.07652v1"}
{"created":"2024-08-14 16:21:28","title":"Exact Trajectory Similarity Search With N-tree: An Efficient Metric Index for kNN and Range Queries","abstract":"Similarity search is the problem of finding in a collection of objects those that are similar to a given query object. It is a fundamental problem in modern applications and the objects considered may be as diverse as locations in space, text documents, images, twitter messages, or trajectories of moving objects.   In this paper we are motivated by the latter application. Trajectories are recorded movements of mobile objects such as vehicles, animals, public transportation, or parts of the human body. We propose a novel distance function called DistanceAvg to capture the similarity of such movements. To be practical, it is necessary to provide indexing for this distance measure.   Fortunately we do not need to start from scratch. A generic and unifying approach is metric space, which organizes the set of objects solely by a distance (similarity) function with certain natural properties. Our function DistanceAvg is a metric.   Although metric indexes have been studied for decades and many such structures are available, they do not offer the best performance with trajectories. In this paper we propose a new design, which outperforms the best existing indexes for kNN queries and is equally good for range queries. It is especially suitable for expensive distance functions as they occur in trajectory similarity search. In many applications, kNN queries are more practical than range queries as it may be difficult to determine an appropriate search radius. Our index provides exact result sets for the given distance function.","sentences":["Similarity search is the problem of finding in a collection of objects those that are similar to a given query object.","It is a fundamental problem in modern applications and the objects considered may be as diverse as locations in space, text documents, images, twitter messages, or trajectories of moving objects.   ","In this paper we are motivated by the latter application.","Trajectories are recorded movements of mobile objects such as vehicles, animals, public transportation, or parts of the human body.","We propose a novel distance function called DistanceAvg to capture the similarity of such movements.","To be practical, it is necessary to provide indexing for this distance measure.   ","Fortunately we do not need to start from scratch.","A generic and unifying approach is metric space, which organizes the set of objects solely by a distance (similarity) function with certain natural properties.","Our function DistanceAvg is a metric.   ","Although metric indexes have been studied for decades and many such structures are available, they do not offer the best performance with trajectories.","In this paper we propose a new design, which outperforms the best existing indexes for kNN queries and is equally good for range queries.","It is especially suitable for expensive distance functions as they occur in trajectory similarity search.","In many applications, kNN queries are more practical than range queries as it may be difficult to determine an appropriate search radius.","Our index provides exact result sets for the given distance function."],"url":"http://arxiv.org/abs/2408.07650v1"}
{"created":"2024-08-14 16:18:51","title":"Adaptive Behavioral AI: Reinforcement Learning to Enhance Pharmacy Services","abstract":"Pharmacies are critical in healthcare systems, particularly in low- and middle-income countries. Procuring pharmacists with the right behavioral interventions or nudges can enhance their skills, public health awareness, and pharmacy inventory management, ensuring access to essential medicines that ultimately benefit their patients. We introduce a reinforcement learning operational system to deliver personalized behavioral interventions through mobile health applications. We illustrate its potential by discussing a series of initial experiments run with SwipeRx, an all-in-one app for pharmacists, including B2B e-commerce, in Indonesia. The proposed method has broader applications extending beyond pharmacy operations to optimize healthcare delivery.","sentences":["Pharmacies are critical in healthcare systems, particularly in low- and middle-income countries.","Procuring pharmacists with the right behavioral interventions or nudges can enhance their skills, public health awareness, and pharmacy inventory management, ensuring access to essential medicines that ultimately benefit their patients.","We introduce a reinforcement learning operational system to deliver personalized behavioral interventions through mobile health applications.","We illustrate its potential by discussing a series of initial experiments run with SwipeRx, an all-in-one app for pharmacists, including B2B e-commerce, in Indonesia.","The proposed method has broader applications extending beyond pharmacy operations to optimize healthcare delivery."],"url":"http://arxiv.org/abs/2408.07647v1"}
{"created":"2024-08-14 16:13:03","title":"Boosting Unconstrained Face Recognition with Targeted Style Adversary","abstract":"While deep face recognition models have demonstrated remarkable performance, they often struggle on the inputs from domains beyond their training data. Recent attempts aim to expand the training set by relying on computationally expensive and inherently challenging image-space augmentation of image generation modules. In an orthogonal direction, we present a simple yet effective method to expand the training data by interpolating between instance-level feature statistics across labeled and unlabeled sets. Our method, dubbed Targeted Style Adversary (TSA), is motivated by two observations: (i) the input domain is reflected in feature statistics, and (ii) face recognition model performance is influenced by style information. Shifting towards an unlabeled style implicitly synthesizes challenging training instances. We devise a recognizability metric to constraint our framework to preserve the inherent identity-related information of labeled instances. The efficacy of our method is demonstrated through evaluations on unconstrained benchmarks, outperforming or being on par with its competitors while offering nearly a 70\\% improvement in training speed and 40\\% less memory consumption.","sentences":["While deep face recognition models have demonstrated remarkable performance, they often struggle on the inputs from domains beyond their training data.","Recent attempts aim to expand the training set by relying on computationally expensive and inherently challenging image-space augmentation of image generation modules.","In an orthogonal direction, we present a simple yet effective method to expand the training data by interpolating between instance-level feature statistics across labeled and unlabeled sets.","Our method, dubbed Targeted Style Adversary (TSA), is motivated by two observations: (i) the input domain is reflected in feature statistics, and (ii) face recognition model performance is influenced by style information.","Shifting towards an unlabeled style implicitly synthesizes challenging training instances.","We devise a recognizability metric to constraint our framework to preserve the inherent identity-related information of labeled instances.","The efficacy of our method is demonstrated through evaluations on unconstrained benchmarks, outperforming or being on par with its competitors while offering nearly a 70\\% improvement in training speed and 40\\% less memory consumption."],"url":"http://arxiv.org/abs/2408.07642v1"}
{"created":"2024-08-14 15:56:27","title":"Towards Fair and Rigorous Evaluations: Hyperparameter Optimization for Top-N Recommendation Task with Implicit Feedback","abstract":"The widespread use of the internet has led to an overwhelming amount of data, which has resulted in the problem of information overload. Recommender systems have emerged as a solution to this problem by providing personalized recommendations to users based on their preferences and historical data. However, as recommendation models become increasingly complex, finding the best hyperparameter combination for different models has become a challenge. The high-dimensional hyperparameter search space poses numerous challenges for researchers, and failure to disclose hyperparameter settings may impede the reproducibility of research results. In this paper, we investigate the Top-N implicit recommendation problem and focus on optimizing the benchmark recommendation algorithm commonly used in comparative experiments using hyperparameter optimization algorithms. We propose a research methodology that follows the principles of a fair comparison, employing seven types of hyperparameter search algorithms to fine-tune six common recommendation algorithms on three datasets. We have identified the most suitable hyperparameter search algorithms for various recommendation algorithms on different types of datasets as a reference for later study. This study contributes to algorithmic research in recommender systems based on hyperparameter optimization, providing a fair basis for comparison.","sentences":["The widespread use of the internet has led to an overwhelming amount of data, which has resulted in the problem of information overload.","Recommender systems have emerged as a solution to this problem by providing personalized recommendations to users based on their preferences and historical data.","However, as recommendation models become increasingly complex, finding the best hyperparameter combination for different models has become a challenge.","The high-dimensional hyperparameter search space poses numerous challenges for researchers, and failure to disclose hyperparameter settings may impede the reproducibility of research results.","In this paper, we investigate the Top-N implicit recommendation problem and focus on optimizing the benchmark recommendation algorithm commonly used in comparative experiments using hyperparameter optimization algorithms.","We propose a research methodology that follows the principles of a fair comparison, employing seven types of hyperparameter search algorithms to fine-tune six common recommendation algorithms on three datasets.","We have identified the most suitable hyperparameter search algorithms for various recommendation algorithms on different types of datasets as a reference for later study.","This study contributes to algorithmic research in recommender systems based on hyperparameter optimization, providing a fair basis for comparison."],"url":"http://arxiv.org/abs/2408.07630v1"}
{"created":"2024-08-14 15:55:31","title":"Optimizing HIV Patient Engagement with Reinforcement Learning in Resource-Limited Settings","abstract":"By providing evidence-based clinical decision support, digital tools and electronic health records can revolutionize patient management, especially in resource-poor settings where fewer health workers are available and often need more training. When these tools are integrated with AI, they can offer personalized support and adaptive interventions, effectively connecting community health workers (CHWs) and healthcare facilities. The CHARM (Community Health Access & Resource Management) app is an AI-native mobile app for CHWs. Developed through a joint partnership of Causal Foundry (CF) and mothers2mothers (m2m), CHARM empowers CHWs, mainly local women, by streamlining case management, enhancing learning, and improving communication. This paper details CHARM's development, integration, and upcoming reinforcement learning-based adaptive interventions, all aimed at enhancing health worker engagement, efficiency, and patient outcomes, thereby enhancing CHWs' capabilities and community health.","sentences":["By providing evidence-based clinical decision support, digital tools and electronic health records can revolutionize patient management, especially in resource-poor settings where fewer health workers are available and often need more training.","When these tools are integrated with AI, they can offer personalized support and adaptive interventions, effectively connecting community health workers (CHWs) and healthcare facilities.","The CHARM (Community Health Access & Resource Management) app is an AI-native mobile app for CHWs.","Developed through a joint partnership of Causal Foundry (CF) and mothers2mothers (m2m), CHARM empowers CHWs, mainly local women, by streamlining case management, enhancing learning, and improving communication.","This paper details CHARM's development, integration, and upcoming reinforcement learning-based adaptive interventions, all aimed at enhancing health worker engagement, efficiency, and patient outcomes, thereby enhancing CHWs' capabilities and community health."],"url":"http://arxiv.org/abs/2408.07629v1"}
{"created":"2024-08-14 15:54:00","title":"Embodied Biocomputing Sequential Circuits with Data Processing and Storage for Neurons-on-a-chip","abstract":"With conventional silicon-based computing approaching its physical and efficiency limits, biocomputing emerges as a promising alternative. This approach utilises biomaterials such as DNA and neurons as an interesting alternative to data processing and storage. This study explores the potential of neuronal biocomputing to rival silicon-based systems. We explore neuronal logic gates and sequential circuits that mimic conventional computer architectures. Through mathematical modelling, optimisation, and computer simulation, we demonstrate the operational capabilities of neuronal sequential circuits. These circuits include a neuronal NAND gate, SR Latch flip-flop, and D flip-flop memory units. Our approach involves manipulating neuron communication, synaptic conductance, spike buffers, neuron types, and specific neuronal network topology designs. The experiments demonstrate the practicality of encoding binary information using patterns of neuronal activity and overcoming synchronization difficulties with neuronal buffers and inhibition strategies. Our results confirm the effectiveness and scalability of neuronal logic circuits, showing that they maintain a stable metabolic burden even in complex data storage configurations. Our study not only demonstrates the concept of embodied biocomputing by manipulating neuronal properties for digital signal processing but also establishes the foundation for cutting-edge biocomputing technologies. Our designs open up possibilities for using neurons as energy-efficient computing solutions. These solutions have the potential to become an alternate to silicon-based systems by providing a carbon-neutral, biologically feasible alternative.","sentences":["With conventional silicon-based computing approaching its physical and efficiency limits, biocomputing emerges as a promising alternative.","This approach utilises biomaterials such as DNA and neurons as an interesting alternative to data processing and storage.","This study explores the potential of neuronal biocomputing to rival silicon-based systems.","We explore neuronal logic gates and sequential circuits that mimic conventional computer architectures.","Through mathematical modelling, optimisation, and computer simulation, we demonstrate the operational capabilities of neuronal sequential circuits.","These circuits include a neuronal NAND gate, SR Latch flip-flop, and D flip-flop memory units.","Our approach involves manipulating neuron communication, synaptic conductance, spike buffers, neuron types, and specific neuronal network topology designs.","The experiments demonstrate the practicality of encoding binary information using patterns of neuronal activity and overcoming synchronization difficulties with neuronal buffers and inhibition strategies.","Our results confirm the effectiveness and scalability of neuronal logic circuits, showing that they maintain a stable metabolic burden even in complex data storage configurations.","Our study not only demonstrates the concept of embodied biocomputing by manipulating neuronal properties for digital signal processing but also establishes the foundation for cutting-edge biocomputing technologies.","Our designs open up possibilities for using neurons as energy-efficient computing solutions.","These solutions have the potential to become an alternate to silicon-based systems by providing a carbon-neutral, biologically feasible alternative."],"url":"http://arxiv.org/abs/2408.07628v1"}
{"created":"2024-08-14 15:44:51","title":"Latent Anomaly Detection Through Density Matrices","abstract":"This paper introduces a novel anomaly detection framework that combines the robust statistical principles of density-estimation-based anomaly detection methods with the representation-learning capabilities of deep learning models. The method originated from this framework is presented in two different versions: a shallow approach employing a density-estimation model based on adaptive Fourier features and density matrices, and a deep approach that integrates an autoencoder to learn a low-dimensional representation of the data. By estimating the density of new samples, both methods are able to find normality scores. The methods can be seamlessly integrated into an end-to-end architecture and optimized using gradient-based optimization techniques. To evaluate their performance, extensive experiments were conducted on various benchmark datasets. The results demonstrate that both versions of the method can achieve comparable or superior performance when compared to other state-of-the-art methods. Notably, the shallow approach performs better on datasets with fewer dimensions, while the autoencoder-based approach shows improved performance on datasets with higher dimensions.","sentences":["This paper introduces a novel anomaly detection framework that combines the robust statistical principles of density-estimation-based anomaly detection methods with the representation-learning capabilities of deep learning models.","The method originated from this framework is presented in two different versions: a shallow approach employing a density-estimation model based on adaptive Fourier features and density matrices, and a deep approach that integrates an autoencoder to learn a low-dimensional representation of the data.","By estimating the density of new samples, both methods are able to find normality scores.","The methods can be seamlessly integrated into an end-to-end architecture and optimized using gradient-based optimization techniques.","To evaluate their performance, extensive experiments were conducted on various benchmark datasets.","The results demonstrate that both versions of the method can achieve comparable or superior performance when compared to other state-of-the-art methods.","Notably, the shallow approach performs better on datasets with fewer dimensions, while the autoencoder-based approach shows improved performance on datasets with higher dimensions."],"url":"http://arxiv.org/abs/2408.07623v1"}
{"created":"2024-08-14 15:31:15","title":"Prophet Inequalities: Competing with the Top $\\ell$ Items is Easy","abstract":"We explore a novel variant of the classical prophet inequality problem, where the values of a sequence of items are drawn i.i.d. from some distribution, and an online decision maker must select one item irrevocably. We establish that the competitive ratio between the expected optimal performance of the online decision maker compared to that of a prophet, who uses the average of the top $\\ell$ items, must be greater than $\\ell/c_{\\ell}$, with $c_{\\ell}$ the solution to an integral equation. We prove that this lower bound is larger than $1-1/(\\exp(\\ell)-1)$. This implies that the bound converges exponentially fast to $1$ as $\\ell$ grows. In particular, the bound for $\\ell=2$ is $2/c_{2} \\approx 0.966$ which is much closer to $1$ than the classical bound of $0.745$ for $\\ell=1$. Additionally, the proposed algorithm can be extended to a more general scenario, where the decision maker is permitted to select $k$ items. This subsumes the $k$ multi-unit i.i.d. prophet problem and provides the current best asymptotic guarantees, as well as enables broader understanding in the more general framework. Finally, we prove a nearly tight competitive ratio when only static threshold policies are allowed.","sentences":["We explore a novel variant of the classical prophet inequality problem, where the values of a sequence of items are drawn i.i.d.","from some distribution, and an online decision maker must select one item irrevocably.","We establish that the competitive ratio between the expected optimal performance of the online decision maker compared to that of a prophet, who uses the average of the top $\\ell$ items, must be greater than $\\ell/c_{\\ell}$, with $c_{\\ell}$ the solution to an integral equation.","We prove that this lower bound is larger than $1-1/(\\exp(\\ell)-1)$.","This implies that the bound converges exponentially fast to $1$ as $\\ell$ grows.","In particular, the bound for $\\ell=2$ is $2/c_{2} \\approx 0.966$ which is much closer to $1$ than the classical bound of $0.745$ for $\\ell=1$. Additionally, the proposed algorithm can be extended to a more general scenario, where the decision maker is permitted to select $k$ items.","This subsumes the $k$ multi-unit i.i.d. prophet problem and provides the current best asymptotic guarantees, as well as enables broader understanding in the more general framework.","Finally, we prove a nearly tight competitive ratio when only static threshold policies are allowed."],"url":"http://arxiv.org/abs/2408.07616v1"}
{"created":"2024-08-14 15:28:28","title":"Practical Considerations for Differential Privacy","abstract":"Differential privacy is the gold standard for statistical data release. Used by governments, companies, and academics, its mathematically rigorous guarantees and worst-case assumptions on the strength and knowledge of attackers make it a robust and compelling framework for reasoning about privacy. However, even with landmark successes, differential privacy has not achieved widespread adoption in everyday data use and data protection. In this work we examine some of the practical obstacles that stand in the way.","sentences":["Differential privacy is the gold standard for statistical data release.","Used by governments, companies, and academics, its mathematically rigorous guarantees and worst-case assumptions on the strength and knowledge of attackers make it a robust and compelling framework for reasoning about privacy.","However, even with landmark successes, differential privacy has not achieved widespread adoption in everyday data use and data protection.","In this work we examine some of the practical obstacles that stand in the way."],"url":"http://arxiv.org/abs/2408.07614v1"}
{"created":"2024-08-14 15:26:10","title":"Rethinking the Key Factors for the Generalization of Remote Sensing Stereo Matching Networks","abstract":"Stereo matching, a critical step of 3D reconstruction, has fully shifted towards deep learning due to its strong feature representation of remote sensing images. However, ground truth for stereo matching task relies on expensive airborne LiDAR data, thus making it difficult to obtain enough samples for supervised learning. To improve the generalization ability of stereo matching networks on cross-domain data from different sensors and scenarios, in this paper, we dedicate to study key training factors from three perspectives. (1) For the selection of training dataset, it is important to select data with similar regional target distribution as the test set instead of utilizing data from the same sensor. (2) For model structure, cascaded structure that flexibly adapts to different sizes of features is preferred. (3) For training manner, unsupervised methods generalize better than supervised methods, and we design an unsupervised early-stop strategy to help retain the best model with pre-trained weights as the basis. Extensive experiments are conducted to support the previous findings, on the basis of which we present an unsupervised stereo matching network with good generalization performance. We release the source code and the datasets at https://github.com/Elenairene/RKF_RSSM to reproduce the results and encourage future work.","sentences":["Stereo matching, a critical step of 3D reconstruction, has fully shifted towards deep learning due to its strong feature representation of remote sensing images.","However, ground truth for stereo matching task relies on expensive airborne LiDAR data, thus making it difficult to obtain enough samples for supervised learning.","To improve the generalization ability of stereo matching networks on cross-domain data from different sensors and scenarios, in this paper, we dedicate to study key training factors from three perspectives.","(1) For the selection of training dataset, it is important to select data with similar regional target distribution as the test set instead of utilizing data from the same sensor.","(2) For model structure, cascaded structure that flexibly adapts to different sizes of features is preferred.","(3) For training manner, unsupervised methods generalize better than supervised methods, and we design an unsupervised early-stop strategy to help retain the best model with pre-trained weights as the basis.","Extensive experiments are conducted to support the previous findings, on the basis of which we present an unsupervised stereo matching network with good generalization performance.","We release the source code and the datasets at https://github.com/Elenairene/RKF_RSSM to reproduce the results and encourage future work."],"url":"http://arxiv.org/abs/2408.07613v1"}
{"created":"2024-08-14 15:10:13","title":"Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving","abstract":"The field of autonomous driving increasingly demands high-quality annotated video training data. In this paper, we propose Panacea+, a powerful and universally applicable framework for generating video data in driving scenes. Built upon the foundation of our previous work, Panacea, Panacea+ adopts a multi-view appearance noise prior mechanism and a super-resolution module for enhanced consistency and increased resolution. Extensive experiments show that the generated video samples from Panacea+ greatly benefit a wide range of tasks on different datasets, including 3D object tracking, 3D object detection, and lane detection tasks on the nuScenes and Argoverse 2 dataset. These results strongly prove Panacea+ to be a valuable data generation framework for autonomous driving.","sentences":["The field of autonomous driving increasingly demands high-quality annotated video training data.","In this paper, we propose Panacea+, a powerful and universally applicable framework for generating video data in driving scenes.","Built upon the foundation of our previous work, Panacea, Panacea+ adopts a multi-view appearance noise prior mechanism and a super-resolution module for enhanced consistency and increased resolution.","Extensive experiments show that the generated video samples from Panacea+ greatly benefit a wide range of tasks on different datasets, including 3D object tracking, 3D object detection, and lane detection tasks on the nuScenes and Argoverse 2 dataset.","These results strongly prove Panacea+ to be a valuable data generation framework for autonomous driving."],"url":"http://arxiv.org/abs/2408.07605v1"}
{"created":"2024-08-14 14:49:25","title":"Crossover Designs in Software Engineering Experiments: Review of the State of Analysis","abstract":"Experimentation is an essential method for causal inference in any empirical discipline. Crossover-design experiments are common in Software Engineering (SE) research. In these, subjects apply more than one treatment in different orders. This design increases the amount of obtained data and deals with subject variability but introduces threats to internal validity like the learning and carryover effect. Vegas et al. reviewed the state of practice for crossover designs in SE research and provided guidelines on how to address its threats during data analysis while still harnessing its benefits. In this paper, we reflect on the impact of these guidelines and review the state of analysis of crossover design experiments in SE publications between 2015 and March 2024. To this end, by conducting a forward snowballing of the guidelines, we survey 136 publications reporting 67 crossover-design experiments and evaluate their data analysis against the provided guidelines. The results show that the validity of data analyses has improved compared to the original state of analysis. Still, despite the explicit guidelines, only 29.5% of all threats to validity were addressed properly. While the maturation and the optimal sequence threats are properly addressed in 35.8% and 38.8% of all studies in our sample respectively, the carryover threat is only modeled in about 3% of the observed cases. The lack of adherence to the analysis guidelines threatens the validity of the conclusions drawn from crossover design experiments","sentences":["Experimentation is an essential method for causal inference in any empirical discipline.","Crossover-design experiments are common in Software Engineering (SE) research.","In these, subjects apply more than one treatment in different orders.","This design increases the amount of obtained data and deals with subject variability but introduces threats to internal validity like the learning and carryover effect.","Vegas et al. reviewed the state of practice for crossover designs in SE research and provided guidelines on how to address its threats during data analysis while still harnessing its benefits.","In this paper, we reflect on the impact of these guidelines and review the state of analysis of crossover design experiments in SE publications between 2015 and March 2024.","To this end, by conducting a forward snowballing of the guidelines, we survey 136 publications reporting 67 crossover-design experiments and evaluate their data analysis against the provided guidelines.","The results show that the validity of data analyses has improved compared to the original state of analysis.","Still, despite the explicit guidelines, only 29.5% of all threats to validity were addressed properly.","While the maturation and the optimal sequence threats are properly addressed in 35.8% and 38.8% of all studies in our sample respectively, the carryover threat is only modeled in about 3% of the observed cases.","The lack of adherence to the analysis guidelines threatens the validity of the conclusions drawn from crossover design experiments"],"url":"http://arxiv.org/abs/2408.07594v1"}
