{"created":"2024-11-25 18:59:53","title":"Factorized Visual Tokenization and Generation","abstract":"Visual tokenizers are fundamental to image generation. They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation. Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes. Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability a critical challenge. In this work, we introduce Factorized Quantization (FQ), a novel approach that revitalizes VQ-based tokenizers by decomposing a large codebook into multiple independent sub-codebooks. This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization. To ensure each sub-codebook captures distinct and complementary information, we propose a disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks. Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations. This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations. Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance. We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation. https://showlab.github.io/FQGAN","sentences":["Visual tokenizers are fundamental to image generation.","They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation.","Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes.","Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability a critical challenge.","In this work, we introduce Factorized Quantization (FQ), a novel approach that revitalizes VQ-based tokenizers by decomposing a large codebook into multiple independent sub-codebooks.","This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization.","To ensure each sub-codebook captures distinct and complementary information, we propose a disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks.","Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations.","This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations.","Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance.","We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation.","https://showlab.github.io/FQGAN"],"url":"http://arxiv.org/abs/2411.16681v1"}
{"created":"2024-11-25 18:53:56","title":"Diffusion Features for Zero-Shot 6DoF Object Pose Estimation","abstract":"Zero-shot object pose estimation enables the retrieval of object poses from images without necessitating object-specific training. In recent approaches this is facilitated by vision foundation models (VFM), which are pre-trained models that are effectively general-purpose feature extractors. The characteristics exhibited by these VFMs vary depending on the training data, network architecture, and training paradigm. The prevailing choice in this field are self-supervised Vision Transformers (ViT). This study assesses the influence of Latent Diffusion Model (LDM) backbones on zero-shot pose estimation. In order to facilitate a comparison between the two families of models on a common ground we adopt and modify a recent approach. Therefore, a template-based multi-staged method for estimating poses in a zero-shot fashion using LDMs is presented. The efficacy of the proposed approach is empirically evaluated on three standard datasets for object-specific 6DoF pose estimation. The experiments demonstrate an Average Recall improvement of up to 27% over the ViT baseline. The source code is available at: https://github.com/BvG1993/DZOP.","sentences":["Zero-shot object pose estimation enables the retrieval of object poses from images without necessitating object-specific training.","In recent approaches this is facilitated by vision foundation models (VFM), which are pre-trained models that are effectively general-purpose feature extractors.","The characteristics exhibited by these VFMs vary depending on the training data, network architecture, and training paradigm.","The prevailing choice in this field are self-supervised Vision Transformers (ViT).","This study assesses the influence of Latent Diffusion Model (LDM) backbones on zero-shot pose estimation.","In order to facilitate a comparison between the two families of models on a common ground we adopt and modify a recent approach.","Therefore, a template-based multi-staged method for estimating poses in a zero-shot fashion using LDMs is presented.","The efficacy of the proposed approach is empirically evaluated on three standard datasets for object-specific 6DoF pose estimation.","The experiments demonstrate an Average Recall improvement of up to 27% over the ViT baseline.","The source code is available at: https://github.com/BvG1993/DZOP."],"url":"http://arxiv.org/abs/2411.16668v1"}
{"created":"2024-11-25 18:53:49","title":"OPMOS: Ordered Parallel Multi-Objective Shortest-Path","abstract":"The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimal solutions from a start node to a destination node in a multi-attribute graph. To solve the NP-hard MOS problem, the literature explores heuristic multi-objective A*-style algorithmic approaches. A generalized MOS algorithm maintains a \"frontier\" of partial paths at each node and performs ordered processing to ensure that Pareto-optimal paths are generated to reach the goal node. The algorithm becomes computationally intractable as the number of objectives increases due to a rapid increase in the non-dominated paths, and the concomitantly large increase in Pareto-optimal solutions. While prior works have focused on algorithmic methods to reduce the complexity, we tackle this challenge by exploiting parallelism using an algorithm-architecture approach. The key insight is that MOS algorithms rely on the ordered execution of partial paths to maintain high work efficiency. The OPMOS framework, proposed herein, unlocks ordered parallelism and efficiently exploits the concurrent execution of multiple paths in MOS. Experimental evaluation using the NVIDIA GH200 Superchip shows the performance scaling potential of OPMOS on work efficiency and parallelism using a real-world application to ship routing.","sentences":["The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimal solutions from a start node to a destination node in a multi-attribute graph.","To solve the NP-hard MOS problem, the literature explores heuristic multi-objective A*-style algorithmic approaches.","A generalized MOS algorithm maintains a \"frontier\" of partial paths at each node and performs ordered processing to ensure that Pareto-optimal paths are generated to reach the goal node.","The algorithm becomes computationally intractable as the number of objectives increases due to a rapid increase in the non-dominated paths, and the concomitantly large increase in Pareto-optimal solutions.","While prior works have focused on algorithmic methods to reduce the complexity, we tackle this challenge by exploiting parallelism using an algorithm-architecture approach.","The key insight is that MOS algorithms rely on the ordered execution of partial paths to maintain high work efficiency.","The OPMOS framework, proposed herein, unlocks ordered parallelism and efficiently exploits the concurrent execution of multiple paths in MOS.","Experimental evaluation using the NVIDIA GH200 Superchip shows the performance scaling potential of OPMOS on work efficiency and parallelism using a real-world application to ship routing."],"url":"http://arxiv.org/abs/2411.16667v1"}
{"created":"2024-11-25 18:28:26","title":"Self-Generated Critiques Boost Reward Modeling for Language Models","abstract":"Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.","sentences":["Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF).","However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format.","We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability.","Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision.","Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation.","Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency.","Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy."],"url":"http://arxiv.org/abs/2411.16646v1"}
{"created":"2024-11-25 18:27:39","title":"Exploring Discrete Flow Matching for 3D De Novo Molecule Generation","abstract":"Deep generative models that produce novel molecular structures have the potential to facilitate chemical discovery. Flow matching is a recently proposed generative modeling framework that has achieved impressive performance on a variety of tasks including those on biomolecular structures. The seminal flow matching framework was developed only for continuous data. However, de novo molecular design tasks require generating discrete data such as atomic elements or sequences of amino acid residues. Several discrete flow matching methods have been proposed recently to address this gap. In this work we benchmark the performance of existing discrete flow matching methods for 3D de novo small molecule generation and provide explanations of their differing behavior. As a result we present FlowMol-CTMC, an open-source model that achieves state of the art performance for 3D de novo design with fewer learnable parameters than existing methods. Additionally, we propose the use of metrics that capture molecule quality beyond local chemical valency constraints and towards higher-order structural motifs. These metrics show that even though basic constraints are satisfied, the models tend to produce unusual and potentially problematic functional groups outside of the training data distribution. Code and trained models for reproducing this work are available at \\url{https://github.com/dunni3/FlowMol}.","sentences":["Deep generative models that produce novel molecular structures have the potential to facilitate chemical discovery.","Flow matching is a recently proposed generative modeling framework that has achieved impressive performance on a variety of tasks including those on biomolecular structures.","The seminal flow matching framework was developed only for continuous data.","However, de novo molecular design tasks require generating discrete data such as atomic elements or sequences of amino acid residues.","Several discrete flow matching methods have been proposed recently to address this gap.","In this work we benchmark the performance of existing discrete flow matching methods for 3D de novo small molecule generation and provide explanations of their differing behavior.","As a result we present FlowMol-CTMC, an open-source model that achieves state of the art performance for 3D de novo design with fewer learnable parameters than existing methods.","Additionally, we propose the use of metrics that capture molecule quality beyond local chemical valency constraints and towards higher-order structural motifs.","These metrics show that even though basic constraints are satisfied, the models tend to produce unusual and potentially problematic functional groups outside of the training data distribution.","Code and trained models for reproducing this work are available at \\url{https://github.com/dunni3/FlowMol}."],"url":"http://arxiv.org/abs/2411.16644v1"}
{"created":"2024-11-25 18:15:37","title":"K8s Pro Sentinel: Extend Secret Security in Kubernetes Cluster","abstract":"Microservice architecture is widely adopted among distributed systems. It follows the modular approach that decomposes large software applications into independent services. Kubernetes has become the standard tool for managing these microservices. It stores sensitive information like database passwords, API keys, and access tokens as Secret Objects. There are security mechanisms employed to safeguard these confidential data, such as encryption, Role Based Access Control (RBAC), and the least privilege principle. However, manually configuring these measures is time-consuming, requires specialized knowledge, and is prone to human error, thereby increasing the risks of misconfiguration. This research introduces K8s Pro Sentinel, an operator that automates the configuration of encryption and access control for Secret Objects by extending the Kubernetes API server. This automation reduces human error and enhances security within clusters. The performance and reliability of the Sentinel operator were evaluated using Red Hat Operator Scorecard and chaos engineering practices.","sentences":["Microservice architecture is widely adopted among distributed systems.","It follows the modular approach that decomposes large software applications into independent services.","Kubernetes has become the standard tool for managing these microservices.","It stores sensitive information like database passwords, API keys, and access tokens as Secret Objects.","There are security mechanisms employed to safeguard these confidential data, such as encryption, Role Based Access Control (RBAC), and the least privilege principle.","However, manually configuring these measures is time-consuming, requires specialized knowledge, and is prone to human error, thereby increasing the risks of misconfiguration.","This research introduces K8s Pro Sentinel, an operator that automates the configuration of encryption and access control for Secret Objects by extending the Kubernetes API server.","This automation reduces human error and enhances security within clusters.","The performance and reliability of the Sentinel operator were evaluated using Red Hat Operator Scorecard and chaos engineering practices."],"url":"http://arxiv.org/abs/2411.16639v1"}
{"created":"2024-11-25 18:03:50","title":"Inference-Time Policy Steering through Human Interactions","abstract":"Generative policies trained with human demonstrations can autonomously accomplish multimodal, long-horizon tasks. However, during inference, humans are often removed from the policy execution loop, limiting the ability to guide a pre-trained policy towards a specific sub-goal or trajectory shape among multiple predictions. Naive human intervention may inadvertently exacerbate distribution shift, leading to constraint violations or execution failures. To better align policy output with human intent without inducing out-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS) framework that leverages human interactions to bias the generative sampling process, rather than fine-tuning the policy on interaction data. We evaluate ITPS across three simulated and real-world benchmarks, testing three forms of human interaction and associated alignment distance metrics. Among six sampling strategies, our proposed stochastic sampling with diffusion policy achieves the best trade-off between alignment and distribution shift. Videos are available at https://yanweiw.github.io/itps/.","sentences":["Generative policies trained with human demonstrations can autonomously accomplish multimodal, long-horizon tasks.","However, during inference, humans are often removed from the policy execution loop, limiting the ability to guide a pre-trained policy towards a specific sub-goal or trajectory shape among multiple predictions.","Naive human intervention may inadvertently exacerbate distribution shift, leading to constraint violations or execution failures.","To better align policy output with human intent without inducing out-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS) framework that leverages human interactions to bias the generative sampling process, rather than fine-tuning the policy on interaction data.","We evaluate ITPS across three simulated and real-world benchmarks, testing three forms of human interaction and associated alignment distance metrics.","Among six sampling strategies, our proposed stochastic sampling with diffusion policy achieves the best trade-off between alignment and distribution shift.","Videos are available at https://yanweiw.github.io/itps/."],"url":"http://arxiv.org/abs/2411.16627v1"}
{"created":"2024-11-25 18:03:20","title":"Leakage-Robust Bayesian Persuasion","abstract":"We introduce the concept of leakage-robust Bayesian persuasion. Situated between public persuasion [KG11, CCG23, Xu20] and private persuasion [AB19], leakage-robust persuasion considers a setting where one or more signals privately sent by a sender to the receivers may be leaked. We study the design of leakage-robust persuasion schemes and quantify the price of robustness using two formalisms:   - The first notion, $k$-worst-case persuasiveness, requires a scheme to remain persuasive as long as each receiver observes at most $k$ leaked signals. We quantify the Price of Worst-case Robustness (PoWR$_k$) -- i.e., the gap in sender's utility as compared to the optimal private scheme -- as $\\Theta(\\min\\{2^k,n\\})$ for supermodular sender utilities and $\\Theta(k)$ for submodular or XOS utilities, where $n$ is the number of receivers. This result also establishes that in some instances, $\\Theta(\\log k)$ leakages are sufficient for the utility of the optimal leakage-robust persuasion to degenerate to that of public persuasion.   - The second notion, expected downstream utility robustness, relaxes the persuasiveness and considers the impact on sender's utility when receivers best respond to their observations. By quantifying the Price of Downstream Robustness (PoDR) as the gap between the sender's expected utility over random leakage patterns as compared to private persuasion, we show that over several natural and structured distributions of leakage patterns, PoDR improves PoWR to $\\Theta(k)$ or even $\\Theta(1)$, where $k$ is the maximum number of leaked signals observable to each receiver across leakage patterns in the distribution.   En route to these results, we show that subsampling and masking are general-purpose algorithmic paradigms for transforming private persuasion signaling schemes to leakage-robust ones, with minmax optimal loss in the sender's utility.","sentences":["We introduce the concept of leakage-robust Bayesian persuasion.","Situated between public persuasion [KG11, CCG23, Xu20] and private persuasion [AB19], leakage-robust persuasion considers a setting where one or more signals privately sent by a sender to the receivers may be leaked.","We study the design of leakage-robust persuasion schemes and quantify the price of robustness using two formalisms:   -","The first notion, $k$-worst-case persuasiveness, requires a scheme to remain persuasive as long as each receiver observes at most $k$ leaked signals.","We quantify the Price of Worst-case Robustness (PoWR$_k$) -- i.e., the gap in sender's utility as compared to the optimal private scheme -- as $\\Theta(\\min\\{2^k,n\\})$ for supermodular sender utilities and $\\Theta(k)$ for submodular or XOS utilities, where $n$ is the number of receivers.","This result also establishes that in some instances, $\\Theta(\\log k)$ leakages are sufficient for the utility of the optimal leakage-robust persuasion to degenerate to that of public persuasion.   -","The second notion, expected downstream utility robustness, relaxes the persuasiveness and considers the impact on sender's utility when receivers best respond to their observations.","By quantifying the Price of Downstream Robustness (PoDR) as the gap between the sender's expected utility over random leakage patterns as compared to private persuasion, we show that over several natural and structured distributions of leakage patterns, PoDR improves PoWR to $\\Theta(k)$ or even $\\Theta(1)$, where $k$ is the maximum number of leaked signals observable to each receiver across leakage patterns in the distribution.   ","En route to these results, we show that subsampling and masking are general-purpose algorithmic paradigms for transforming private persuasion signaling schemes to leakage-robust ones, with minmax optimal loss in the sender's utility."],"url":"http://arxiv.org/abs/2411.16624v1"}
{"created":"2024-11-25 17:57:52","title":"StructFormer: Document Structure-based Masked Attention and its Impact on Language Model Pre-Training","abstract":"Most state-of-the-art techniques for Language Models (LMs) today rely on transformer-based architectures and their ubiquitous attention mechanism. However, the exponential growth in computational requirements with longer input sequences confines Transformers to handling short passages. Recent efforts have aimed to address this limitation by introducing selective attention mechanisms, notably local and global attention. While sparse attention mechanisms, akin to full attention in being Turing-complete, have been theoretically established, their practical impact on pre-training remains unexplored. This study focuses on empirically assessing the influence of global attention on BERT pre-training. The primary steps involve creating an extensive corpus of structure-aware text through arXiv data, alongside a text-only counterpart. We carry out pre-training on these two datasets, investigate shifts in attention patterns, and assess their implications for downstream tasks. Our analysis underscores the significance of incorporating document structure into LM models, demonstrating their capacity to excel in more abstract tasks, such as document understanding.","sentences":["Most state-of-the-art techniques for Language Models (LMs) today rely on transformer-based architectures and their ubiquitous attention mechanism.","However, the exponential growth in computational requirements with longer input sequences confines Transformers to handling short passages.","Recent efforts have aimed to address this limitation by introducing selective attention mechanisms, notably local and global attention.","While sparse attention mechanisms, akin to full attention in being Turing-complete, have been theoretically established, their practical impact on pre-training remains unexplored.","This study focuses on empirically assessing the influence of global attention on BERT pre-training.","The primary steps involve creating an extensive corpus of structure-aware text through arXiv data, alongside a text-only counterpart.","We carry out pre-training on these two datasets, investigate shifts in attention patterns, and assess their implications for downstream tasks.","Our analysis underscores the significance of incorporating document structure into LM models, demonstrating their capacity to excel in more abstract tasks, such as document understanding."],"url":"http://arxiv.org/abs/2411.16618v1"}
{"created":"2024-11-25 17:31:34","title":"Approximation Algorithms for Combinatorial Optimization with Predictions","abstract":"We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time. We propose a systematic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or maximal) total weight. This gives simple (near-)linear time algorithms for, e.g., Vertex Cover, Steiner Tree, Min-Weight Perfect Matching, Knapsack, and Clique. Our algorithms produce optimal solutions when provided with perfect predictions and their approximation ratios smoothly degrade with increasing prediction error. With small enough prediction error we achieve approximation guarantees that are beyond reach without predictions in the given time bounds, as exemplified by the NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; we demonstrate this on the Steiner Tree problem. We conclude with an empirical evaluation of our approach.","sentences":["We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time.","We propose a systematic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or maximal) total weight.","This gives simple (near-)linear time algorithms for, e.g., Vertex Cover, Steiner Tree, Min-Weight Perfect Matching, Knapsack, and Clique.","Our algorithms produce optimal solutions when provided with perfect predictions and their approximation ratios smoothly degrade with increasing prediction error.","With small enough prediction error we achieve approximation guarantees that are beyond reach without predictions in the given time bounds, as exemplified by the NP-hardness and APX-hardness of many of the above problems.","Although we show our approach to be optimal for this class of problems as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; we demonstrate this on the Steiner Tree problem.","We conclude with an empirical evaluation of our approach."],"url":"http://arxiv.org/abs/2411.16600v1"}
{"created":"2024-11-25 17:28:44","title":"Location-Based Service (LBS) Data Quality Metrics and Effects on Mobility Inference","abstract":"Today, GPS-equipped mobile devices are ubiquitous, and they generate Location-Based Service (LBS) data, which has become a critical resource for understanding human mobility. However, inherent limitations in LBS datasets, primarily characterized by discontinuity and sparsity, may introduce significant biases in representing individual movement patterns. This study develops data quality metrics for LBS data, examines their disparities among different populations, and quantifies their effects on inferred individual movement, stays in particular, in the Boston Metropolitan Area. We find that data from higher-income, more educated, and predominantly white census block groups (CBGs) show higher sampling rates but paradoxically lower data quality. This contradiction may stem from greater privacy awareness in these communities. Additionally, we propose a new framework to resample LBS data and quantitatively evaluate the inferential biases associated with data of varying quality. This versatile framework can analyze the impacts originating from different data processing workflows with LBS data. Using linear regression models with clustered standard error, we assess the impact of data quality metrics on inferring the number of stay points. The results show that better data quality, characterized by the number of observations and temporal occupancy, can significantly reduce the bias when calculating the stay points of an individual. The introduction of additional data quality metrics into the regression model can further explain the bias. Overall, this study provides insights into how data quality can influence our understanding of human mobility patterns, highlighting the importance of carefully handling LBS data in research.","sentences":["Today, GPS-equipped mobile devices are ubiquitous, and they generate Location-Based Service (LBS) data, which has become a critical resource for understanding human mobility.","However, inherent limitations in LBS datasets, primarily characterized by discontinuity and sparsity, may introduce significant biases in representing individual movement patterns.","This study develops data quality metrics for LBS data, examines their disparities among different populations, and quantifies their effects on inferred individual movement, stays in particular, in the Boston Metropolitan Area.","We find that data from higher-income, more educated, and predominantly white census block groups (CBGs) show higher sampling rates but paradoxically lower data quality.","This contradiction may stem from greater privacy awareness in these communities.","Additionally, we propose a new framework to resample LBS data and quantitatively evaluate the inferential biases associated with data of varying quality.","This versatile framework can analyze the impacts originating from different data processing workflows with LBS data.","Using linear regression models with clustered standard error, we assess the impact of data quality metrics on inferring the number of stay points.","The results show that better data quality, characterized by the number of observations and temporal occupancy, can significantly reduce the bias when calculating the stay points of an individual.","The introduction of additional data quality metrics into the regression model can further explain the bias.","Overall, this study provides insights into how data quality can influence our understanding of human mobility patterns, highlighting the importance of carefully handling LBS data in research."],"url":"http://arxiv.org/abs/2411.16595v1"}
{"created":"2024-11-25 17:25:00","title":"Adversarial Attacks for Drift Detection","abstract":"Concept drift refers to the change of data distributions over time. While drift poses a challenge for learning models, requiring their continual adaption, it is also relevant in system monitoring to detect malfunctions, system failures, and unexpected behavior. In the latter case, the robust and reliable detection of drifts is imperative. This work studies the shortcomings of commonly used drift detection schemes. We show how to construct data streams that are drifting without being detected. We refer to those as drift adversarials. In particular, we compute all possible adversairals for common detection schemes and underpin our theoretical findings with empirical evaluations.","sentences":["Concept drift refers to the change of data distributions over time.","While drift poses a challenge for learning models, requiring their continual adaption, it is also relevant in system monitoring to detect malfunctions, system failures, and unexpected behavior.","In the latter case, the robust and reliable detection of drifts is imperative.","This work studies the shortcomings of commonly used drift detection schemes.","We show how to construct data streams that are drifting without being detected.","We refer to those as drift adversarials.","In particular, we compute all possible adversairals for common detection schemes and underpin our theoretical findings with empirical evaluations."],"url":"http://arxiv.org/abs/2411.16591v1"}
{"created":"2024-11-25 17:11:54","title":"Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision","abstract":"Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback. In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and train-time. We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of $76,321$ responses paired with step-level feedback. Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning. We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation. Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method. Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model. Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase its potential. Our code and datasets are at \\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.","sentences":["Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics.","However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback.","In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and train-time.","We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of $76,321$ responses paired with step-level feedback.","Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning.","We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation.","Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method.","Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model.","Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase its potential.","Our code and datasets are at \\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}."],"url":"http://arxiv.org/abs/2411.16579v1"}
{"created":"2024-11-25 17:07:19","title":"Forest Covers and Bounded Forest Covers","abstract":"We study approximation algorithms for the forest cover and bounded forest cover problems. A probabilistic $2+\\epsilon$ approximation algorithm for the forest cover problem is given using the method of dual fitting. A deterministic algorithm with a 2-approximation ratio that rounds the optimal solution to a linear program is given next. The 2-approximation for the forest cover is then used to give a 6-approximation for the bounded forest cover problem. The use of the probabilistic method to develop the $2+\\epsilon$ approximation algorithm may be of independent interest.","sentences":["We study approximation algorithms for the forest cover and bounded forest cover problems.","A probabilistic $2+\\epsilon$ approximation algorithm for the forest cover problem is given using the method of dual fitting.","A deterministic algorithm with a 2-approximation ratio that rounds the optimal solution to a linear program is given next.","The 2-approximation for the forest cover is then used to give a 6-approximation for the bounded forest cover problem.","The use of the probabilistic method to develop the $2+\\epsilon$ approximation algorithm may be of independent interest."],"url":"http://arxiv.org/abs/2411.16578v1"}
{"created":"2024-11-25 16:59:42","title":"Rethinking Diffusion for Text-Driven Human Motion Generation","abstract":"Since 2023, Vector Quantization (VQ)-based discrete generation methods have rapidly dominated human motion generation, primarily surpassing diffusion-based continuous generation methods in standard performance metrics. However, VQ-based methods have inherent limitations. Representing continuous motion data as limited discrete tokens leads to inevitable information loss, reduces the diversity of generated motions, and restricts their ability to function effectively as motion priors or generation guidance. In contrast, the continuous space generation nature of diffusion-based methods makes them well-suited to address these limitations and with even potential for model scalability. In this work, we systematically investigate why current VQ-based methods perform well and explore the limitations of existing diffusion-based methods from the perspective of motion data representation and distribution. Drawing on these insights, we preserve the inherent strengths of a diffusion-based human motion generation model and gradually optimize it with inspiration from VQ-based approaches. Our approach introduces a human motion diffusion model enabled to perform bidirectional masked autoregression, optimized with a reformed data representation and distribution. Additionally, we also propose more robust evaluation methods to fairly assess different-based methods. Extensive experiments on benchmark human motion generation datasets demonstrate that our method excels previous methods and achieves state-of-the-art performances.","sentences":["Since 2023, Vector Quantization (VQ)-based discrete generation methods have rapidly dominated human motion generation, primarily surpassing diffusion-based continuous generation methods in standard performance metrics.","However, VQ-based methods have inherent limitations.","Representing continuous motion data as limited discrete tokens leads to inevitable information loss, reduces the diversity of generated motions, and restricts their ability to function effectively as motion priors or generation guidance.","In contrast, the continuous space generation nature of diffusion-based methods makes them well-suited to address these limitations and with even potential for model scalability.","In this work, we systematically investigate why current VQ-based methods perform well and explore the limitations of existing diffusion-based methods from the perspective of motion data representation and distribution.","Drawing on these insights, we preserve the inherent strengths of a diffusion-based human motion generation model and gradually optimize it with inspiration from VQ-based approaches.","Our approach introduces a human motion diffusion model enabled to perform bidirectional masked autoregression, optimized with a reformed data representation and distribution.","Additionally, we also propose more robust evaluation methods to fairly assess different-based methods.","Extensive experiments on benchmark human motion generation datasets demonstrate that our method excels previous methods and achieves state-of-the-art performances."],"url":"http://arxiv.org/abs/2411.16575v1"}
{"created":"2024-11-25 16:52:21","title":"J-CaPA : Joint Channel and Pyramid Attention Improves Medical Image Segmentation","abstract":"Medical image segmentation is crucial for diagnosis and treatment planning. Traditional CNN-based models, like U-Net, have shown promising results but struggle to capture long-range dependencies and global context. To address these limitations, we propose a transformer-based architecture that jointly applies Channel Attention and Pyramid Attention mechanisms to improve multi-scale feature extraction and enhance segmentation performance for medical images. Increasing model complexity requires more training data, and we further improve model generalization with CutMix data augmentation. Our approach is evaluated on the Synapse multi-organ segmentation dataset, achieving a 6.9% improvement in Mean Dice score and a 39.9% improvement in Hausdorff Distance (HD95) over an implementation without our enhancements. Our proposed model demonstrates improved segmentation accuracy for complex anatomical structures, outperforming existing state-of-the-art methods.","sentences":["Medical image segmentation is crucial for diagnosis and treatment planning.","Traditional CNN-based models, like U-Net, have shown promising results but struggle to capture long-range dependencies and global context.","To address these limitations, we propose a transformer-based architecture that jointly applies Channel Attention and Pyramid Attention mechanisms to improve multi-scale feature extraction and enhance segmentation performance for medical images.","Increasing model complexity requires more training data, and we further improve model generalization with CutMix data augmentation.","Our approach is evaluated on the Synapse multi-organ segmentation dataset, achieving a 6.9% improvement in Mean Dice score and a 39.9% improvement in Hausdorff Distance (HD95) over an implementation without our enhancements.","Our proposed model demonstrates improved segmentation accuracy for complex anatomical structures, outperforming existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2411.16568v1"}
{"created":"2024-11-25 16:51:11","title":"Enhancing Few-Shot Learning with Integrated Data and GAN Model Approaches","abstract":"This paper presents an innovative approach to enhancing few-shot learning by integrating data augmentation with model fine-tuning in a framework designed to tackle the challenges posed by small-sample data. Recognizing the critical limitations of traditional machine learning models that require large datasets-especially in fields such as drug discovery, target recognition, and malicious traffic detection-this study proposes a novel strategy that leverages Generative Adversarial Networks (GANs) and advanced optimization techniques to improve model performance with limited data. Specifically, the paper addresses the noise and bias issues introduced by data augmentation methods, contrasting them with model-based approaches, such as fine-tuning and metric learning, which rely heavily on related datasets. By combining Markov Chain Monte Carlo (MCMC) sampling and discriminative model ensemble strategies within a GAN framework, the proposed model adjusts generative and discriminative distributions to simulate a broader range of relevant data. Furthermore, it employs MHLoss and a reparameterized GAN ensemble to enhance stability and accelerate convergence, ultimately leading to improved classification performance on small-sample images and structured datasets. Results confirm that the MhERGAN algorithm developed in this research is highly effective for few-shot learning, offering a practical solution that bridges data scarcity with high-performing model adaptability and generalization.","sentences":["This paper presents an innovative approach to enhancing few-shot learning by integrating data augmentation with model fine-tuning in a framework designed to tackle the challenges posed by small-sample data.","Recognizing the critical limitations of traditional machine learning models that require large datasets-especially in fields such as drug discovery, target recognition, and malicious traffic detection-this study proposes a novel strategy that leverages Generative Adversarial Networks (GANs) and advanced optimization techniques to improve model performance with limited data.","Specifically, the paper addresses the noise and bias issues introduced by data augmentation methods, contrasting them with model-based approaches, such as fine-tuning and metric learning, which rely heavily on related datasets.","By combining Markov Chain Monte Carlo (MCMC) sampling and discriminative model ensemble strategies within a GAN framework, the proposed model adjusts generative and discriminative distributions to simulate a broader range of relevant data.","Furthermore, it employs MHLoss and a reparameterized GAN ensemble to enhance stability and accelerate convergence, ultimately leading to improved classification performance on small-sample images and structured datasets.","Results confirm that the MhERGAN algorithm developed in this research is highly effective for few-shot learning, offering a practical solution that bridges data scarcity with high-performing model adaptability and generalization."],"url":"http://arxiv.org/abs/2411.16567v1"}
{"created":"2024-11-25 16:47:10","title":"EnStack: An Ensemble Stacking Framework of Large Language Models for Enhanced Vulnerability Detection in Source Code","abstract":"Automated detection of software vulnerabilities is critical for enhancing security, yet existing methods often struggle with the complexity and diversity of modern codebases. In this paper, we introduce EnStack, a novel ensemble stacking framework that enhances vulnerability detection using natural language processing (NLP) techniques. Our approach synergizes multiple pre-trained large language models (LLMs) specialized in code understanding CodeBERT for semantic analysis, GraphCodeBERT for structural representation, and UniXcoder for cross-modal capabilities. By fine-tuning these models on the Draper VDISC dataset and integrating their outputs through meta-classifiers such as Logistic Regression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack effectively captures intricate code patterns and vulnerabilities that individual models may overlook. The meta-classifiers consolidate the strengths of each LLM, resulting in a comprehensive model that excels in detecting subtle and complex vulnerabilities across diverse programming contexts. Experimental results demonstrate that EnStack significantly outperforms existing methods, achieving notable improvements in accuracy, precision, recall, and F1-score. This work highlights the potential of ensemble LLM approaches in code analysis tasks and offers valuable insights into applying NLP techniques for advancing automated vulnerability detection.","sentences":["Automated detection of software vulnerabilities is critical for enhancing security, yet existing methods often struggle with the complexity and diversity of modern codebases.","In this paper, we introduce EnStack, a novel ensemble stacking framework that enhances vulnerability detection using natural language processing (NLP) techniques.","Our approach synergizes multiple pre-trained large language models (LLMs) specialized in code understanding CodeBERT for semantic analysis, GraphCodeBERT for structural representation, and UniXcoder for cross-modal capabilities.","By fine-tuning these models on the Draper VDISC dataset and integrating their outputs through meta-classifiers such as Logistic Regression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack effectively captures intricate code patterns and vulnerabilities that individual models may overlook.","The meta-classifiers consolidate the strengths of each LLM, resulting in a comprehensive model that excels in detecting subtle and complex vulnerabilities across diverse programming contexts.","Experimental results demonstrate that EnStack significantly outperforms existing methods, achieving notable improvements in accuracy, precision, recall, and F1-score.","This work highlights the potential of ensemble LLM approaches in code analysis tasks and offers valuable insights into applying NLP techniques for advancing automated vulnerability detection."],"url":"http://arxiv.org/abs/2411.16561v1"}
{"created":"2024-11-25 16:32:29","title":"Representation Collapsing Problems in Vector Quantization","abstract":"Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.","sentences":["Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors.","It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models.","Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored.","In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values.","This collapse fundamentally compromises the model's ability to capture diverse data patterns.","By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions.","Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse.","Building on these findings, we propose potential solutions aimed at mitigating each collapse.","To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization."],"url":"http://arxiv.org/abs/2411.16550v1"}
{"created":"2024-11-25 16:29:43","title":"Float Self-Tagging","abstract":"Dynamic and polymorphic languages must attach information, such as types, to run time objects, and therefore adapt the memory layout of values to include space for this information. This is especially problematic in the case of IEEE754 double-precision floating-point numbers, which require exactly 64 bits, leaving no space for type information. The two main encodings in-use to this day, tagged pointers and NaN-tagging, either allocate floats on the heap or unbox them at the cost of an overhead when handling all other objects.   This paper presents self-tagging, a new approach to object tagging that can attach type information to 64-bit objects while retaining the ability to use all of their 64 bits for data. At its core, self-tagging exploits the fact that some bit sequences appear with very high probability. Superimposing tags with these frequent sequences allows encoding both 64-bit data and type within a single machine word. Implementations of self-tagging demonstrate that it unboxes all floats in practice, accelerating the execution time of float-intensive benchmarks in Scheme by 2.3$\\times$, and in JavaScript by 2.7$\\times$ without impacting the performance of other benchmarks, which makes it a good alternative to both tagged pointers and NaN-tagging.","sentences":["Dynamic and polymorphic languages must attach information, such as types, to run time objects, and therefore adapt the memory layout of values to include space for this information.","This is especially problematic in the case of IEEE754 double-precision floating-point numbers, which require exactly 64 bits, leaving no space for type information.","The two main encodings in-use to this day, tagged pointers and NaN-tagging, either allocate floats on the heap or unbox them at the cost of an overhead when handling all other objects.   ","This paper presents self-tagging, a new approach to object tagging that can attach type information to 64-bit objects while retaining the ability to use all of their 64 bits for data.","At its core, self-tagging exploits the fact that some bit sequences appear with very high probability.","Superimposing tags with these frequent sequences allows encoding both 64-bit data and type within a single machine word.","Implementations of self-tagging demonstrate that it unboxes all floats in practice, accelerating the execution time of float-intensive benchmarks in Scheme by 2.3$\\times$, and in JavaScript by 2.7$\\times$ without impacting the performance of other benchmarks, which makes it a good alternative to both tagged pointers and NaN-tagging."],"url":"http://arxiv.org/abs/2411.16544v1"}
{"created":"2024-11-25 16:21:34","title":"RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics","abstract":"Spatial understanding is a crucial capability for robots to make grounded decisions based on their environment. This foundational skill enables robots not only to perceive their surroundings but also to reason about and interact meaningfully within the world. In modern robotics, these capabilities are taken on by visual language models, and they face significant challenges when applied to spatial reasoning context due to their training data sources. These sources utilize general-purpose image datasets, and they often lack sophisticated spatial scene understanding capabilities. For example, the datasets do not address reference frame comprehension - spatial relationships require clear contextual understanding, whether from an ego-centric, object-centric, or world-centric perspective, which allow for effective real-world interaction. To address this issue, we introduce RoboSpatial, a large-scale spatial understanding dataset consisting of real indoor and tabletop scenes captured as 3D scans and egocentric images, annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M annotated spatial relationships, with paired 2D egocentric images and 3D scans to make it both 2D and 3D ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robotics manipulation.","sentences":["Spatial understanding is a crucial capability for robots to make grounded decisions based on their environment.","This foundational skill enables robots not only to perceive their surroundings but also to reason about and interact meaningfully within the world.","In modern robotics, these capabilities are taken on by visual language models, and they face significant challenges when applied to spatial reasoning context due to their training data sources.","These sources utilize general-purpose image datasets, and they often lack sophisticated spatial scene understanding capabilities.","For example, the datasets do not address reference frame comprehension - spatial relationships require clear contextual understanding, whether from an ego-centric, object-centric, or world-centric perspective, which allow for effective real-world interaction.","To address this issue, we introduce RoboSpatial, a large-scale spatial understanding dataset consisting of real indoor and tabletop scenes captured as 3D scans and egocentric images, annotated with rich spatial information relevant to robotics.","The dataset includes 1M images, 5K 3D scans, and 3M annotated spatial relationships, with paired 2D egocentric images and 3D scans to make it both 2D and 3D ready.","Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robotics manipulation."],"url":"http://arxiv.org/abs/2411.16537v1"}
{"created":"2024-11-25 16:18:39","title":"Continual Deep Reinforcement Learning with Task-Agnostic Policy Distillation","abstract":"Central to the development of universal learning systems is the ability to solve multiple tasks without retraining from scratch when new data arrives. This is crucial because each task requires significant training time. Addressing the problem of continual learning necessitates various methods due to the complexity of the problem space. This problem space includes: (1) addressing catastrophic forgetting to retain previously learned tasks, (2) demonstrating positive forward transfer for faster learning, (3) ensuring scalability across numerous tasks, and (4) facilitating learning without requiring task labels, even in the absence of clear task boundaries. In this paper, the Task-Agnostic Policy Distillation (TAPD) framework is introduced. This framework alleviates problems (1)-(4) by incorporating a task-agnostic phase, where an agent explores its environment without any external goal and maximizes only its intrinsic motivation. The knowledge gained during this phase is later distilled for further exploration. Therefore, the agent acts in a self-supervised manner by systematically seeking novel states. By utilizing task-agnostic distilled knowledge, the agent can solve downstream tasks more efficiently, leading to improved sample efficiency. Our code is available at the repository: https://github.com/wabbajack1/TAPD.","sentences":["Central to the development of universal learning systems is the ability to solve multiple tasks without retraining from scratch when new data arrives.","This is crucial because each task requires significant training time.","Addressing the problem of continual learning necessitates various methods due to the complexity of the problem space.","This problem space includes: (1) addressing catastrophic forgetting to retain previously learned tasks, (2) demonstrating positive forward transfer for faster learning, (3) ensuring scalability across numerous tasks, and (4) facilitating learning without requiring task labels, even in the absence of clear task boundaries.","In this paper, the Task-Agnostic Policy Distillation (TAPD) framework is introduced.","This framework alleviates problems (1)-(4) by incorporating a task-agnostic phase, where an agent explores its environment without any external goal and maximizes only its intrinsic motivation.","The knowledge gained during this phase is later distilled for further exploration.","Therefore, the agent acts in a self-supervised manner by systematically seeking novel states.","By utilizing task-agnostic distilled knowledge, the agent can solve downstream tasks more efficiently, leading to improved sample efficiency.","Our code is available at the repository: https://github.com/wabbajack1/TAPD."],"url":"http://arxiv.org/abs/2411.16532v1"}
{"created":"2024-11-25 16:10:05","title":"LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology Report Generation","abstract":"In the current paradigm of image captioning, deep learning models are trained to generate text from image embeddings of latent features. We challenge the assumption that these latent features ought to be high-dimensional vectors which require model fine tuning to handle. Here we propose Label Boosted Retrieval Augmented Generation (LaB-RAG), a text-based approach to image captioning that leverages image descriptors in the form of categorical labels to boost standard retrieval augmented generation (RAG) with pretrained large language models (LLMs). We study our method in the context of radiology report generation (RRG), where the task is to generate a clinician's report detailing their observations from a set of radiological images, such as X-rays. We argue that simple linear classifiers over extracted image embeddings can effectively transform X-rays into text-space as radiology-specific labels. In combination with standard RAG, we show that these derived text labels can be used with general-domain LLMs to generate radiology reports. Without ever training our generative language model or image feature encoder models, and without ever directly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves better results across natural language and radiology language metrics compared with other retrieval-based RRG methods, while attaining competitive results compared to other fine-tuned vision-language RRG models. We further present results of our experiments with various components of LaB-RAG to better understand our method. Finally, we critique the use of a popular RRG metric, arguing it is possible to artificially inflate its results without true data-leakage.","sentences":["In the current paradigm of image captioning, deep learning models are trained to generate text from image embeddings of latent features.","We challenge the assumption that these latent features ought to be high-dimensional vectors which require model fine tuning to handle.","Here we propose Label Boosted Retrieval Augmented Generation (LaB-RAG), a text-based approach to image captioning that leverages image descriptors in the form of categorical labels to boost standard retrieval augmented generation (RAG) with pretrained large language models (LLMs).","We study our method in the context of radiology report generation (RRG), where the task is to generate a clinician's report detailing their observations from a set of radiological images, such as X-rays.","We argue that simple linear classifiers over extracted image embeddings can effectively transform X-rays into text-space as radiology-specific labels.","In combination with standard RAG, we show that these derived text labels can be used with general-domain LLMs to generate radiology reports.","Without ever training our generative language model or image feature encoder models, and without ever directly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves better results across natural language and radiology language metrics compared with other retrieval-based RRG methods, while attaining competitive results compared to other fine-tuned vision-language RRG models.","We further present results of our experiments with various components of LaB-RAG to better understand our method.","Finally, we critique the use of a popular RRG metric, arguing it is possible to artificially inflate its results without true data-leakage."],"url":"http://arxiv.org/abs/2411.16523v1"}
{"created":"2024-11-25 16:00:04","title":"Curator Attack: When Blackbox Differential Privacy Auditing Loses Its Power","abstract":"A surge in data-driven applications enhances everyday life but also raises serious concerns about private information leakage. Hence many privacy auditing tools are emerging for checking if the data sanitization performed meets the privacy standard of the data owner. Blackbox auditing for differential privacy is particularly gaining popularity for its effectiveness and applicability to a wide range of scenarios. Yet, we identified that blackbox auditing is essentially flawed with its setting: small probabilities or densities are ignored due to inaccurate observation. Our argument is based on a solid false positive analysis from a hypothesis testing perspective, which is missed out by prior blackbox auditing tools. This oversight greatly reduces the reliability of these tools, as it allows malicious or incapable data curators to pass the auditing with an overstated privacy guarantee, posing significant risks to data owners. We demonstrate the practical existence of such threats in classical differential privacy mechanisms against four representative blackbox auditors with experimental validations. Our findings aim to reveal the limitations of blackbox auditing tools, empower the data owner with the awareness of risks in using these tools, and encourage the development of more reliable differential privacy auditing methods.","sentences":["A surge in data-driven applications enhances everyday life but also raises serious concerns about private information leakage.","Hence many privacy auditing tools are emerging for checking if the data sanitization performed meets the privacy standard of the data owner.","Blackbox auditing for differential privacy is particularly gaining popularity for its effectiveness and applicability to a wide range of scenarios.","Yet, we identified that blackbox auditing is essentially flawed with its setting: small probabilities or densities are ignored due to inaccurate observation.","Our argument is based on a solid false positive analysis from a hypothesis testing perspective, which is missed out by prior blackbox auditing tools.","This oversight greatly reduces the reliability of these tools, as it allows malicious or incapable data curators to pass the auditing with an overstated privacy guarantee, posing significant risks to data owners.","We demonstrate the practical existence of such threats in classical differential privacy mechanisms against four representative blackbox auditors with experimental validations.","Our findings aim to reveal the limitations of blackbox auditing tools, empower the data owner with the awareness of risks in using these tools, and encourage the development of more reliable differential privacy auditing methods."],"url":"http://arxiv.org/abs/2411.16516v1"}
{"created":"2024-11-25 15:36:29","title":"Multi-Resolution Generative Modeling of Human Motion from Limited Data","abstract":"We present a generative model that learns to synthesize human motion from limited training sequences. Our framework provides conditional generation and blending across multiple temporal resolutions. The model adeptly captures human motion patterns by integrating skeletal convolution layers and a multi-scale architecture. Our model contains a set of generative and adversarial networks, along with embedding modules, each tailored for generating motions at specific frame rates while exerting control over their content and details. Notably, our approach also extends to the synthesis of co-speech gestures, demonstrating its ability to generate synchronized gestures from speech inputs, even with limited paired data. Through direct synthesis of SMPL pose parameters, our approach avoids test-time adjustments to fit human body meshes. Experimental results showcase our model's ability to achieve extensive coverage of training examples, while generating diverse motions, as indicated by local and global diversity metrics.","sentences":["We present a generative model that learns to synthesize human motion from limited training sequences.","Our framework provides conditional generation and blending across multiple temporal resolutions.","The model adeptly captures human motion patterns by integrating skeletal convolution layers and a multi-scale architecture.","Our model contains a set of generative and adversarial networks, along with embedding modules, each tailored for generating motions at specific frame rates while exerting control over their content and details.","Notably, our approach also extends to the synthesis of co-speech gestures, demonstrating its ability to generate synchronized gestures from speech inputs, even with limited paired data.","Through direct synthesis of SMPL pose parameters, our approach avoids test-time adjustments to fit human body meshes.","Experimental results showcase our model's ability to achieve extensive coverage of training examples, while generating diverse motions, as indicated by local and global diversity metrics."],"url":"http://arxiv.org/abs/2411.16498v1"}
{"created":"2024-11-25 15:31:27","title":"O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?","abstract":"This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount.","sentences":["This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques.","While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks.","Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity.","Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA.","Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning.","We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field.","Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount."],"url":"http://arxiv.org/abs/2411.16489v1"}
{"created":"2024-11-25 15:25:31","title":"When Babies Teach Babies: Can student knowledge sharing outperform Teacher-Guided Distillation on small datasets?","abstract":"We present our submission to the BabyLM challenge, aiming to push the boundaries of data-efficient language model pretraining. Our method builds upon deep mutual learning, introducing a student model search for diverse initialization. We address the limitation of treating students equally by formulating weighted mutual learning as a bi-level optimization problem. The inner loop learns compact students through online distillation, while the outer loop optimizes weights for better knowledge distillation from diverse students. This dynamic weighting strategy eliminates the need for a teacher model, reducing computational requirements. Our evaluations show that teacher-less methods can match or surpass teacher-supervised approaches.","sentences":["We present our submission to the BabyLM challenge, aiming to push the boundaries of data-efficient language model pretraining.","Our method builds upon deep mutual learning, introducing a student model search for diverse initialization.","We address the limitation of treating students equally by formulating weighted mutual learning as a bi-level optimization problem.","The inner loop learns compact students through online distillation, while the outer loop optimizes weights for better knowledge distillation from diverse students.","This dynamic weighting strategy eliminates the need for a teacher model, reducing computational requirements.","Our evaluations show that teacher-less methods can match or surpass teacher-supervised approaches."],"url":"http://arxiv.org/abs/2411.16487v1"}
{"created":"2024-11-25 15:20:40","title":"Distributed, communication-efficient, and differentially private estimation of KL divergence","abstract":"A key task in managing distributed, sensitive data is to measure the extent to which a distribution changes. Understanding this drift can effectively support a variety of federated learning and analytics tasks. However, in many practical settings sharing such information can be undesirable (e.g., for privacy concerns) or infeasible (e.g., for high communication costs). In this work, we describe novel algorithmic approaches for estimating the KL divergence of data across federated models of computation, under differential privacy. We analyze their theoretical properties and present an empirical study of their performance. We explore parameter settings that optimize the accuracy of the algorithm catering to each of the settings; these provide sub-variations that are applicable to real-world tasks, addressing different context- and application-specific trust level requirements. Our experimental results confirm that our private estimators achieve accuracy comparable to a baseline algorithm without differential privacy guarantees.","sentences":["A key task in managing distributed, sensitive data is to measure the extent to which a distribution changes.","Understanding this drift can effectively support a variety of federated learning and analytics tasks.","However, in many practical settings sharing such information can be undesirable (e.g., for privacy concerns) or infeasible (e.g., for high communication costs).","In this work, we describe novel algorithmic approaches for estimating the KL divergence of data across federated models of computation, under differential privacy.","We analyze their theoretical properties and present an empirical study of their performance.","We explore parameter settings that optimize the accuracy of the algorithm catering to each of the settings; these provide sub-variations that are applicable to real-world tasks, addressing different context- and application-specific trust level requirements.","Our experimental results confirm that our private estimators achieve accuracy comparable to a baseline algorithm without differential privacy guarantees."],"url":"http://arxiv.org/abs/2411.16478v1"}
{"created":"2024-11-25 15:05:00","title":"On the Reconstruction of Training Data from Group Invariant Networks","abstract":"Reconstructing training data from trained neural networks is an active area of research with significant implications for privacy and explainability. Recent advances have demonstrated the feasibility of this process for several data types. However, reconstructing data from group-invariant neural networks poses distinct challenges that remain largely unexplored. This paper addresses this gap by first formulating the problem and discussing some of its basic properties. We then provide an experimental evaluation demonstrating that conventional reconstruction techniques are inadequate in this scenario. Specifically, we observe that the resulting data reconstructions gravitate toward symmetric inputs on which the group acts trivially, leading to poor-quality results. Finally, we propose two novel methods aiming to improve reconstruction in this setup and present promising preliminary experimental results. Our work sheds light on the complexities of reconstructing data from group invariant neural networks and offers potential avenues for future research in this domain.","sentences":["Reconstructing training data from trained neural networks is an active area of research with significant implications for privacy and explainability.","Recent advances have demonstrated the feasibility of this process for several data types.","However, reconstructing data from group-invariant neural networks poses distinct challenges that remain largely unexplored.","This paper addresses this gap by first formulating the problem and discussing some of its basic properties.","We then provide an experimental evaluation demonstrating that conventional reconstruction techniques are inadequate in this scenario.","Specifically, we observe that the resulting data reconstructions gravitate toward symmetric inputs on which the group acts trivially, leading to poor-quality results.","Finally, we propose two novel methods aiming to improve reconstruction in this setup and present promising preliminary experimental results.","Our work sheds light on the complexities of reconstructing data from group invariant neural networks and offers potential avenues for future research in this domain."],"url":"http://arxiv.org/abs/2411.16458v1"}
{"created":"2024-11-25 14:59:21","title":"Truffle: Efficient Data Passing for Data-Intensive Serverless Workflows in the Edge-Cloud Continuum","abstract":"Serverless computing promises a scalable, reliable, and cost-effective solution for running data-intensive applications and workflows in the heterogeneous and limited-resource environment of the Edge-Cloud Continuum. However, building and running data-intensive serverless workflows also brings new challenges that can significantly degrade the application performance. Cold start remains one of the main challenges that impact the total function execution time. Further, since the serverless functions are not directly addressable, Serverless workflows need to rely on external (storage) services to pass the input data to the downstream functions. Empirical evidence from our experiments shows that the cold start and the function data passing take up the most time in the function execution lifecycle.   In this paper, we introduce Truffle - a novel model and architecture that enables efficient inter-function data passing in the Edge-Cloud Continuum by introducing mechanisms that separate computation and I/O, allowing serverless functions to leverage the cold starts to their advantage. Truffle introduces Smart Data Prefetch (SDP) mechanism that abstracts the retrieval of input data for the serverless functions by triggering the data retrieval from the external storage during the function's startup. Truffle's Cold Start Pass (CSP) mechanism optimizes inter-function data passing and data exchange within serverless workflows in the Edge-Cloud Continuum by hooking into the functions' scheduling lifecycle to trigger early data passing during the function's cold start. Experimental results show that by leveraging the data prefetching and cold-start data passing, Truffle reduces the IO latency impact on the total function execution time by up to 77%, improving the function execution time by up to 46% compared to the state-of-the-art data passing approaches.","sentences":["Serverless computing promises a scalable, reliable, and cost-effective solution for running data-intensive applications and workflows in the heterogeneous and limited-resource environment of the Edge-Cloud Continuum.","However, building and running data-intensive serverless workflows also brings new challenges that can significantly degrade the application performance.","Cold start remains one of the main challenges that impact the total function execution time.","Further, since the serverless functions are not directly addressable, Serverless workflows need to rely on external (storage) services to pass the input data to the downstream functions.","Empirical evidence from our experiments shows that the cold start and the function data passing take up the most time in the function execution lifecycle.   ","In this paper, we introduce Truffle - a novel model and architecture that enables efficient inter-function data passing in the Edge-Cloud Continuum by introducing mechanisms that separate computation and I/O, allowing serverless functions to leverage the cold starts to their advantage.","Truffle introduces Smart Data Prefetch (SDP) mechanism that abstracts the retrieval of input data for the serverless functions by triggering the data retrieval from the external storage during the function's startup.","Truffle's Cold Start Pass (CSP) mechanism optimizes inter-function data passing and data exchange within serverless workflows in the Edge-Cloud Continuum by hooking into the functions' scheduling lifecycle to trigger early data passing during the function's cold start.","Experimental results show that by leveraging the data prefetching and cold-start data passing, Truffle reduces the IO latency impact on the total function execution time by up to 77%, improving the function execution time by up to 46% compared to the state-of-the-art data passing approaches."],"url":"http://arxiv.org/abs/2411.16451v1"}
{"created":"2024-11-25 14:51:34","title":"Model-based reinforcement corrosion prediction: Continuous calibration with Bayesian optimization and corrosion wire sensor data","abstract":"Chloride-induced corrosion significantly contributes to the degradation of reinforced concrete structures, making accurate predictions of chloride migration and its effects on material durability critical. This paper explores two modeling approaches to estimate the effective diffusion coefficient for chloride transport. The first approach follows Gehlen's interpretable diffusion model, which is based on established physical principles and incorporates time and temperature dependencies in predicting chloride migration. The second approach is a neural network-based method, where the neural network approximates the effective diffusion coefficient. In a subsequent step, the calibrated models are used to predict the penetration depth of the critical chloride content, taking into account the uncertainty in the critical chloride content. The models are calibrated using experimental data measured by a wire sensor installed in a concrete test bridge. The calibration results are compared to effective diffusion coefficients derived from drilling dust samples. A comparison of both approaches reveals the advantages of the physics-based model in terms of transparency and interpretability, while the neural network model demonstrates flexibility and adaptability in data-driven predictions. This study emphasizes the importance of combining traditional and machine learning-based methods to improve the accuracy of chloride migration predictions in reinforced concrete.","sentences":["Chloride-induced corrosion significantly contributes to the degradation of reinforced concrete structures, making accurate predictions of chloride migration and its effects on material durability critical.","This paper explores two modeling approaches to estimate the effective diffusion coefficient for chloride transport.","The first approach follows Gehlen's interpretable diffusion model, which is based on established physical principles and incorporates time and temperature dependencies in predicting chloride migration.","The second approach is a neural network-based method, where the neural network approximates the effective diffusion coefficient.","In a subsequent step, the calibrated models are used to predict the penetration depth of the critical chloride content, taking into account the uncertainty in the critical chloride content.","The models are calibrated using experimental data measured by a wire sensor installed in a concrete test bridge.","The calibration results are compared to effective diffusion coefficients derived from drilling dust samples.","A comparison of both approaches reveals the advantages of the physics-based model in terms of transparency and interpretability, while the neural network model demonstrates flexibility and adaptability in data-driven predictions.","This study emphasizes the importance of combining traditional and machine learning-based methods to improve the accuracy of chloride migration predictions in reinforced concrete."],"url":"http://arxiv.org/abs/2411.16447v1"}
{"created":"2024-11-25 14:44:26","title":"TIFeD: a Tiny Integer-based Federated learning algorithm with Direct feedback alignment","abstract":"Training machine and deep learning models directly on extremely resource-constrained devices is the next challenge in the field of tiny machine learning. The related literature in this field is very limited, since most of the solutions focus only on on-device inference or model adaptation through online learning, leaving the training to be carried out on external Cloud services. An interesting technological perspective is to exploit Federated Learning (FL), which allows multiple devices to collaboratively train a shared model in a distributed way. However, the main drawback of state-of-the-art FL algorithms is that they are not suitable for running on tiny devices. For the first time in the literature, in this paper we introduce TIFeD, a Tiny Integer-based Federated learning algorithm with Direct Feedback Alignment (DFA) entirely implemented by using an integer-only arithmetic and being specifically designed to operate on devices with limited resources in terms of memory, computation and energy. Besides the traditional full-network operating modality, in which each device of the FL setting trains the entire neural network on its own local data, we propose an innovative single-layer TIFeD implementation, which enables each device to train only a portion of the neural network model and opens the door to a new way of distributing the learning procedure across multiple devices. The experimental results show the feasibility and effectiveness of the proposed solution. The proposed TIFeD algorithm, with its full-network and single-layer implementations, is made available to the scientific community as a public repository.","sentences":["Training machine and deep learning models directly on extremely resource-constrained devices is the next challenge in the field of tiny machine learning.","The related literature in this field is very limited, since most of the solutions focus only on on-device inference or model adaptation through online learning, leaving the training to be carried out on external Cloud services.","An interesting technological perspective is to exploit Federated Learning (FL), which allows multiple devices to collaboratively train a shared model in a distributed way.","However, the main drawback of state-of-the-art FL algorithms is that they are not suitable for running on tiny devices.","For the first time in the literature, in this paper we introduce TIFeD, a Tiny Integer-based Federated learning algorithm with Direct Feedback Alignment (DFA) entirely implemented by using an integer-only arithmetic and being specifically designed to operate on devices with limited resources in terms of memory, computation and energy.","Besides the traditional full-network operating modality, in which each device of the FL setting trains the entire neural network on its own local data, we propose an innovative single-layer","TIFeD implementation, which enables each device to train only a portion of the neural network model and opens the door to a new way of distributing the learning procedure across multiple devices.","The experimental results show the feasibility and effectiveness of the proposed solution.","The proposed TIFeD algorithm, with its full-network and single-layer implementations, is made available to the scientific community as a public repository."],"url":"http://arxiv.org/abs/2411.16442v1"}
{"created":"2024-11-25 14:43:03","title":"AnonyNoise: Anonymizing Event Data with Smart Noise to Outsmart Re-Identification and Preserve Privacy","abstract":"The increasing capabilities of deep neural networks for re-identification, combined with the rise in public surveillance in recent years, pose a substantial threat to individual privacy. Event cameras were initially considered as a promising solution since their output is sparse and therefore difficult for humans to interpret. However, recent advances in deep learning proof that neural networks are able to reconstruct high-quality grayscale images and re-identify individuals using data from event cameras. In our paper, we contribute a crucial ethical discussion on data privacy and present the first event anonymization pipeline to prevent re-identification not only by humans but also by neural networks. Our method effectively introduces learnable data-dependent noise to cover personally identifiable information in raw event data, reducing attackers' re-identification capabilities by up to 60%, while maintaining substantial information for the performing of downstream tasks. Moreover, our anonymization generalizes well on unseen data and is robust against image reconstruction and inversion attacks. Code: https://github.com/dfki-av/AnonyNoise","sentences":["The increasing capabilities of deep neural networks for re-identification, combined with the rise in public surveillance in recent years, pose a substantial threat to individual privacy.","Event cameras were initially considered as a promising solution since their output is sparse and therefore difficult for humans to interpret.","However, recent advances in deep learning proof that neural networks are able to reconstruct high-quality grayscale images and re-identify individuals using data from event cameras.","In our paper, we contribute a crucial ethical discussion on data privacy and present the first event anonymization pipeline to prevent re-identification not only by humans but also by neural networks.","Our method effectively introduces learnable data-dependent noise to cover personally identifiable information in raw event data, reducing attackers' re-identification capabilities by up to 60%, while maintaining substantial information for the performing of downstream tasks.","Moreover, our anonymization generalizes well on unseen data and is robust against image reconstruction and inversion attacks.","Code: https://github.com/dfki-av/AnonyNoise"],"url":"http://arxiv.org/abs/2411.16440v1"}
{"created":"2024-11-25 14:37:24","title":"Finding Structure in Language Models","abstract":"When we speak, write or listen, we continuously make predictions based on our knowledge of a language's grammar. Remarkably, children acquire this grammatical knowledge within just a few years, enabling them to understand and generalise to novel constructions that have never been uttered before. Language models are powerful tools that create representations of language by incrementally predicting the next word in a sentence, and they have had a tremendous societal impact in recent years. The central research question of this thesis is whether these models possess a deep understanding of grammatical structure similar to that of humans. This question lies at the intersection of natural language processing, linguistics, and interpretability. To address it, we will develop novel interpretability techniques that enhance our understanding of the complex nature of large-scale language models. We approach our research question from three directions. First, we explore the presence of abstract linguistic information through structural priming, a key paradigm in psycholinguistics for uncovering grammatical structure in human language processing. Next, we examine various linguistic phenomena, such as adjective order and negative polarity items, and connect a model's comprehension of these phenomena to the data distribution on which it was trained. Finally, we introduce a controlled testbed for studying hierarchical structure in language models using various synthetic languages of increasing complexity and examine the role of feature interactions in modelling this structure. Our findings offer a detailed account of the grammatical knowledge embedded in language model representations and provide several directions for investigating fundamental linguistic questions using computational methods.","sentences":["When we speak, write or listen, we continuously make predictions based on our knowledge of a language's grammar.","Remarkably, children acquire this grammatical knowledge within just a few years, enabling them to understand and generalise to novel constructions that have never been uttered before.","Language models are powerful tools that create representations of language by incrementally predicting the next word in a sentence, and they have had a tremendous societal impact in recent years.","The central research question of this thesis is whether these models possess a deep understanding of grammatical structure similar to that of humans.","This question lies at the intersection of natural language processing, linguistics, and interpretability.","To address it, we will develop novel interpretability techniques that enhance our understanding of the complex nature of large-scale language models.","We approach our research question from three directions.","First, we explore the presence of abstract linguistic information through structural priming, a key paradigm in psycholinguistics for uncovering grammatical structure in human language processing.","Next, we examine various linguistic phenomena, such as adjective order and negative polarity items, and connect a model's comprehension of these phenomena to the data distribution on which it was trained.","Finally, we introduce a controlled testbed for studying hierarchical structure in language models using various synthetic languages of increasing complexity and examine the role of feature interactions in modelling this structure.","Our findings offer a detailed account of the grammatical knowledge embedded in language model representations and provide several directions for investigating fundamental linguistic questions using computational methods."],"url":"http://arxiv.org/abs/2411.16433v1"}
{"created":"2024-11-25 14:29:39","title":"Unsupervised Event Outlier Detection in Continuous Time","abstract":"Event sequence data record the occurrences of events in continuous time. Event sequence forecasting based on temporal point processes (TPPs) has been extensively studied, but outlier or anomaly detection, especially without any supervision from humans, is still underexplored. In this work, we develop, to the best our knowledge, the first unsupervised outlier detection approach to detecting abnormal events. Our novel unsupervised outlier detection framework is based on ideas from generative adversarial networks (GANs) and reinforcement learning (RL). We train a 'generator' that corrects outliers in the data with a 'discriminator' that learns to discriminate the corrected data from the real data, which may contain outliers. A key insight is that if the generator made a mistake in the correction, it would generate anomalies that are different from the anomalies in the real data, so it serves as data augmentation for the discriminator learning. Different from typical GAN-based outlier detection approaches, our method employs the generator to detect outliers in an online manner. The experimental results show that our method can detect event outliers more accurately than the state-of-the-art approaches.","sentences":["Event sequence data record the occurrences of events in continuous time.","Event sequence forecasting based on temporal point processes (TPPs) has been extensively studied, but outlier or anomaly detection, especially without any supervision from humans, is still underexplored.","In this work, we develop, to the best our knowledge, the first unsupervised outlier detection approach to detecting abnormal events.","Our novel unsupervised outlier detection framework is based on ideas from generative adversarial networks (GANs) and reinforcement learning (RL).","We train a 'generator' that corrects outliers in the data with a 'discriminator' that learns to discriminate the corrected data from the real data, which may contain outliers.","A key insight is that if the generator made a mistake in the correction, it would generate anomalies that are different from the anomalies in the real data, so it serves as data augmentation for the discriminator learning.","Different from typical GAN-based outlier detection approaches, our method employs the generator to detect outliers in an online manner.","The experimental results show that our method can detect event outliers more accurately than the state-of-the-art approaches."],"url":"http://arxiv.org/abs/2411.16427v1"}
{"created":"2024-11-25 14:27:07","title":"Turbofan Engine Remaining Useful Life (RUL) Prediction Based on Bi-Directional Long Short-Term Memory (BLSTM)","abstract":"The aviation industry is rapidly evolving, driven by advancements in technology. Turbofan engines used in commercial aerospace are very complex systems. The majority of turbofan engine components are susceptible to degradation over the life of their operation. Turbofan engine degradation has an impact to engine performance, operability, and reliability. Predicting accurate remaining useful life (RUL) of a commercial turbofan engine based on a variety of complex sensor data is of paramount importance for the safety of the passengers, safety of flight, and for cost effective operations. That is why it is essential for turbofan engines to be monitored, controlled, and maintained. RUL predictions can either come from model-based or data-based approaches. The model-based approach can be very expensive due to the complexity of the mathematical models and the deep expertise that is required in the domain of physical systems. The data-based approach is more frequently used nowadays thanks to the high computational complexity of computers, the advancements in Machine Learning (ML) models, and advancements in sensors. This paper is going to be focused on Bi-Directional Long Short-Term Memory (BLSTM) models but will also provide a benchmark of several RUL prediction databased models. The proposed RUL prediction models are going to be evaluated based on engine failure prediction benchmark dataset Commercial Modular Aero-Propulsion System Simulation (CMAPSS). The CMAPSS dataset is from NASA which contains turbofan engine run to failure events.","sentences":["The aviation industry is rapidly evolving, driven by advancements in technology.","Turbofan engines used in commercial aerospace are very complex systems.","The majority of turbofan engine components are susceptible to degradation over the life of their operation.","Turbofan engine degradation has an impact to engine performance, operability, and reliability.","Predicting accurate remaining useful life (RUL) of a commercial turbofan engine based on a variety of complex sensor data is of paramount importance for the safety of the passengers, safety of flight, and for cost effective operations.","That is why it is essential for turbofan engines to be monitored, controlled, and maintained.","RUL predictions can either come from model-based or data-based approaches.","The model-based approach can be very expensive due to the complexity of the mathematical models and the deep expertise that is required in the domain of physical systems.","The data-based approach is more frequently used nowadays thanks to the high computational complexity of computers, the advancements in Machine Learning (ML) models, and advancements in sensors.","This paper is going to be focused on Bi-Directional Long Short-Term Memory (BLSTM) models but will also provide a benchmark of several RUL prediction databased models.","The proposed RUL prediction models are going to be evaluated based on engine failure prediction benchmark dataset Commercial Modular Aero-Propulsion System Simulation (CMAPSS).","The CMAPSS dataset is from NASA which contains turbofan engine run to failure events."],"url":"http://arxiv.org/abs/2411.16422v1"}
{"created":"2024-11-25 14:25:39","title":"Machine Learning for the Digital Typhoon Dataset: Extensions to Multiple Basins and New Developments in Representations and Tasks","abstract":"This paper presents the Digital Typhoon Dataset V2, a new version of the longest typhoon satellite image dataset for 40+ years aimed at benchmarking machine learning models for long-term spatio-temporal data. The new addition in Dataset V2 is tropical cyclone data from the southern hemisphere, in addition to the northern hemisphere data in Dataset V1. Having data from two hemispheres allows us to ask new research questions about regional differences across basins and hemispheres. We also discuss new developments in representations and tasks of the dataset. We first introduce a self-supervised learning framework for representation learning. Combined with the LSTM model, we discuss performance on intensity forecasting and extra-tropical transition forecasting tasks. We then propose new tasks, such as the typhoon center estimation task. We show that an object detection-based model performs better for stronger typhoons. Finally, we study how machine learning models can generalize across basins and hemispheres, by training the model on the northern hemisphere data and testing it on the southern hemisphere data. The dataset is publicly available at \\url{http://agora.ex.nii.ac.jp/digital-typhoon/dataset/} and \\url{https://github.com/kitamoto-lab/digital-typhoon/}.","sentences":["This paper presents the Digital Typhoon Dataset V2, a new version of the longest typhoon satellite image dataset for 40+ years aimed at benchmarking machine learning models for long-term spatio-temporal data.","The new addition in Dataset V2 is tropical cyclone data from the southern hemisphere, in addition to the northern hemisphere data in Dataset V1.","Having data from two hemispheres allows us to ask new research questions about regional differences across basins and hemispheres.","We also discuss new developments in representations and tasks of the dataset.","We first introduce a self-supervised learning framework for representation learning.","Combined with the LSTM model, we discuss performance on intensity forecasting and extra-tropical transition forecasting tasks.","We then propose new tasks, such as the typhoon center estimation task.","We show that an object detection-based model performs better for stronger typhoons.","Finally, we study how machine learning models can generalize across basins and hemispheres, by training the model on the northern hemisphere data and testing it on the southern hemisphere data.","The dataset is publicly available at \\url{http://agora.ex.nii.ac.jp/digital-typhoon/dataset/} and \\url{https://github.com/kitamoto-lab/digital-typhoon/}."],"url":"http://arxiv.org/abs/2411.16421v1"}
{"created":"2024-11-25 14:14:25","title":"Low-Data Classification of Historical Music Manuscripts: A Few-Shot Learning Approach","abstract":"In this paper, we explore the intersection of technology and cultural preservation by developing a self-supervised learning framework for the classification of musical symbols in historical manuscripts. Optical Music Recognition (OMR) plays a vital role in digitising and preserving musical heritage, but historical documents often lack the labelled data required by traditional methods. We overcome this challenge by training a neural-based feature extractor on unlabelled data, enabling effective classification with minimal samples. Key contributions include optimising crop preprocessing for a self-supervised Convolutional Neural Network and evaluating classification methods, including SVM, multilayer perceptrons, and prototypical networks. Our experiments yield an accuracy of 87.66\\%, showcasing the potential of AI-driven methods to ensure the survival of historical music for future generations through advanced digital archiving techniques.","sentences":["In this paper, we explore the intersection of technology and cultural preservation by developing a self-supervised learning framework for the classification of musical symbols in historical manuscripts.","Optical Music Recognition (OMR) plays a vital role in digitising and preserving musical heritage, but historical documents often lack the labelled data required by traditional methods.","We overcome this challenge by training a neural-based feature extractor on unlabelled data, enabling effective classification with minimal samples.","Key contributions include optimising crop preprocessing for a self-supervised Convolutional Neural Network and evaluating classification methods, including SVM, multilayer perceptrons, and prototypical networks.","Our experiments yield an accuracy of 87.66\\%, showcasing the potential of AI-driven methods to ensure the survival of historical music for future generations through advanced digital archiving techniques."],"url":"http://arxiv.org/abs/2411.16408v1"}
{"created":"2024-11-25 14:12:24","title":"A Study on Unsupervised Domain Adaptation for Semantic Segmentation in the Era of Vision-Language Models","abstract":"Despite the recent progress in deep learning based computer vision, domain shifts are still one of the major challenges. Semantic segmentation for autonomous driving faces a wide range of domain shifts, e.g. caused by changing weather conditions, new geolocations and the frequent use of synthetic data in model training. Unsupervised domain adaptation (UDA) methods have emerged which adapt a model to a new target domain by only using unlabeled data of that domain. The variety of UDA methods is large but all of them use ImageNet pre-trained models. Recently, vision-language models have demonstrated strong generalization capabilities which may facilitate domain adaptation. We show that simply replacing the encoder of existing UDA methods like DACS by a vision-language pre-trained encoder can result in significant performance improvements of up to 10.0% mIoU on the GTA5-to-Cityscapes domain shift. For the generalization performance to unseen domains, the newly employed vision-language pre-trained encoder provides a gain of up to 13.7% mIoU across three unseen datasets. However, we find that not all UDA methods can be easily paired with the new encoder and that the UDA performance does not always likewise transfer into generalization performance. Finally, we perform our experiments on an adverse weather condition domain shift to further verify our findings on a pure real-to-real domain shift.","sentences":["Despite the recent progress in deep learning based computer vision, domain shifts are still one of the major challenges.","Semantic segmentation for autonomous driving faces a wide range of domain shifts, e.g. caused by changing weather conditions, new geolocations and the frequent use of synthetic data in model training.","Unsupervised domain adaptation (UDA) methods have emerged which adapt a model to a new target domain by only using unlabeled data of that domain.","The variety of UDA methods is large but all of them use ImageNet pre-trained models.","Recently, vision-language models have demonstrated strong generalization capabilities which may facilitate domain adaptation.","We show that simply replacing the encoder of existing UDA methods like DACS by a vision-language pre-trained encoder can result in significant performance improvements of up to 10.0% mIoU on the GTA5-to-Cityscapes domain shift.","For the generalization performance to unseen domains, the newly employed vision-language pre-trained encoder provides a gain of up to 13.7% mIoU across three unseen datasets.","However, we find that not all UDA methods can be easily paired with the new encoder and that the UDA performance does not always likewise transfer into generalization performance.","Finally, we perform our experiments on an adverse weather condition domain shift to further verify our findings on a pure real-to-real domain shift."],"url":"http://arxiv.org/abs/2411.16407v1"}
{"created":"2024-11-25 14:10:43","title":"Synthesising Handwritten Music with GANs: A Comprehensive Evaluation of CycleWGAN, ProGAN, and DCGAN","abstract":"The generation of handwritten music sheets is a crucial step toward enhancing Optical Music Recognition (OMR) systems, which rely on large and diverse datasets for optimal performance. However, handwritten music sheets, often found in archives, present challenges for digitisation due to their fragility, varied handwriting styles, and image quality. This paper addresses the data scarcity problem by applying Generative Adversarial Networks (GANs) to synthesise realistic handwritten music sheets. We provide a comprehensive evaluation of three GAN models - DCGAN, ProGAN, and CycleWGAN - comparing their ability to generate diverse and high-quality handwritten music images. The proposed CycleWGAN model, which enhances style transfer and training stability, significantly outperforms DCGAN and ProGAN in both qualitative and quantitative evaluations. CycleWGAN achieves superior performance, with an FID score of 41.87, an IS of 2.29, and a KID of 0.05, making it a promising solution for improving OMR systems.","sentences":["The generation of handwritten music sheets is a crucial step toward enhancing Optical Music Recognition (OMR) systems, which rely on large and diverse datasets for optimal performance.","However, handwritten music sheets, often found in archives, present challenges for digitisation due to their fragility, varied handwriting styles, and image quality.","This paper addresses the data scarcity problem by applying Generative Adversarial Networks (GANs) to synthesise realistic handwritten music sheets.","We provide a comprehensive evaluation of three GAN models - DCGAN, ProGAN, and CycleWGAN - comparing their ability to generate diverse and high-quality handwritten music images.","The proposed CycleWGAN model, which enhances style transfer and training stability, significantly outperforms DCGAN and ProGAN in both qualitative and quantitative evaluations.","CycleWGAN achieves superior performance, with an FID score of 41.87, an IS of 2.29, and a KID of 0.05, making it a promising solution for improving OMR systems."],"url":"http://arxiv.org/abs/2411.16405v1"}
{"created":"2024-11-25 14:10:30","title":"A Survey of Blockchain-Based Privacy Applications: An Analysis of Consent Management and Self-Sovereign Identity Approaches","abstract":"Modern distributed applications in healthcare, supply chain, and the Internet of Things handle a large amount of data in a diverse application setting with multiple stakeholders. Such applications leverage advanced artificial intelligence (AI) and machine learning algorithms to automate business processes. The proliferation of modern AI technologies increases the data demand. However, real-world networks often include private and sensitive information of businesses, users, and other organizations. Emerging data-protection regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) introduce policies around collecting, storing, and managing digital data. While Blockchain technology offers transparency, auditability, and immutability for multi-stakeholder applications, it lacks inherent support for privacy. Typically, privacy support is added to a blockchain-based application by incorporating cryptographic schemes, consent mechanisms, and self-sovereign identity. This article surveys the literature on blockchain-based privacy-preserving systems and identifies the tools for protecting privacy. Besides, consent mechanisms and identity management in the context of blockchain-based systems are also analyzed. The article concludes by highlighting the list of open challenges and further research opportunities.","sentences":["Modern distributed applications in healthcare, supply chain, and the Internet of Things handle a large amount of data in a diverse application setting with multiple stakeholders.","Such applications leverage advanced artificial intelligence (AI) and machine learning algorithms to automate business processes.","The proliferation of modern AI technologies increases the data demand.","However, real-world networks often include private and sensitive information of businesses, users, and other organizations.","Emerging data-protection regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) introduce policies around collecting, storing, and managing digital data.","While Blockchain technology offers transparency, auditability, and immutability for multi-stakeholder applications, it lacks inherent support for privacy.","Typically, privacy support is added to a blockchain-based application by incorporating cryptographic schemes, consent mechanisms, and self-sovereign identity.","This article surveys the literature on blockchain-based privacy-preserving systems and identifies the tools for protecting privacy.","Besides, consent mechanisms and identity management in the context of blockchain-based systems are also analyzed.","The article concludes by highlighting the list of open challenges and further research opportunities."],"url":"http://arxiv.org/abs/2411.16404v1"}
{"created":"2024-11-25 13:49:45","title":"FineWeb-zhtw: Scalable Curation of Traditional Chinese Text Data from the Web","abstract":"The quality and size of a pretraining dataset significantly influence the performance of large language models (LLMs). While there have been numerous efforts in the curation of such a dataset for English users, there is a relative lack of similar initiatives for Traditional Chinese. Building upon this foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored specifically for Traditional Chinese users. We came up with multiple stages of meticulously designed filters to cater to the linguistic difference between English and Traditional Chinese, to ensure comprehensiveness and quality. We determined effectiveness from querying dataset samples with three main objectives. Our code and datasets are publicly available.","sentences":["The quality and size of a pretraining dataset significantly influence the performance of large language models (LLMs).","While there have been numerous efforts in the curation of such a dataset for English users, there is a relative lack of similar initiatives for Traditional Chinese.","Building upon this foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored specifically for Traditional Chinese users.","We came up with multiple stages of meticulously designed filters to cater to the linguistic difference between English and Traditional Chinese, to ensure comprehensiveness and quality.","We determined effectiveness from querying dataset samples with three main objectives.","Our code and datasets are publicly available."],"url":"http://arxiv.org/abs/2411.16387v1"}
{"created":"2024-11-25 13:26:09","title":"A Review of Bayesian Uncertainty Quantification in Deep Probabilistic Image Segmentation","abstract":"Advancements in image segmentation play an integral role within the greater scope of Deep Learning-based computer vision. Furthermore, their widespread applicability in critical real-world tasks has given rise to challenges related to the reliability of such algorithms. Hence, uncertainty quantification has been extensively studied within this context, enabling expression of model ignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to prevent uninformed decision making. Due to the rapid adoption of Convolutional Neural Network (CNN)-based segmentation models in high-stake applications, a substantial body of research has been published on this very topic, causing its swift expansion into a distinct field. This work provides a comprehensive overview of probabilistic segmentation by discussing fundamental concepts in uncertainty that govern advancements in the field as well as the application to various tasks. We identify that quantifying aleatoric and epistemic uncertainty approximates Bayesian inference w.r.t. to either latent variables or model parameters, respectively. Moreover, literature on both uncertainties trace back to four key applications; (1) to quantify statistical inconsistencies in the annotation process due ambiguous images, (2) correlating prediction error with uncertainty, (3) expanding the model hypothesis space for better generalization, and (4) active learning. Then, a discussion follows that includes an overview of utilized datasets for each of the applications and comparison of the available methods. We also highlight challenges related to architectures, uncertainty-based active learning, standardization and benchmarking, and recommendations for future work such as methods based on single forward passes and models that appropriately leverage volumetric data.","sentences":["Advancements in image segmentation play an integral role within the greater scope of Deep Learning-based computer vision.","Furthermore, their widespread applicability in critical real-world tasks has given rise to challenges related to the reliability of such algorithms.","Hence, uncertainty quantification has been extensively studied within this context, enabling expression of model ignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to prevent uninformed decision making.","Due to the rapid adoption of Convolutional Neural Network (CNN)-based segmentation models in high-stake applications, a substantial body of research has been published on this very topic, causing its swift expansion into a distinct field.","This work provides a comprehensive overview of probabilistic segmentation by discussing fundamental concepts in uncertainty that govern advancements in the field as well as the application to various tasks.","We identify that quantifying aleatoric and epistemic uncertainty approximates Bayesian inference w.r.t.","to either latent variables or model parameters, respectively.","Moreover, literature on both uncertainties trace back to four key applications; (1) to quantify statistical inconsistencies in the annotation process due ambiguous images, (2) correlating prediction error with uncertainty, (3) expanding the model hypothesis space for better generalization, and (4) active learning.","Then, a discussion follows that includes an overview of utilized datasets for each of the applications and comparison of the available methods.","We also highlight challenges related to architectures, uncertainty-based active learning, standardization and benchmarking, and recommendations for future work such as methods based on single forward passes and models that appropriately leverage volumetric data."],"url":"http://arxiv.org/abs/2411.16370v1"}
{"created":"2024-11-25 13:10:24","title":"A Framework for Consistency Models in Distributed Systems","abstract":"We define am axiomatic timeless framework for asynchronous distributed systems, together with well-formedness and consistency axioms, which unifies and generalizes the expressive power of current approaches. 1) It combines classic serialization per-process with a global visibility. 2) It defines a physical realizability well-formedness axiom to prevent physically impossible causality cycles, while allowing possible and useful visibility cycles, to allow synchronization-oriented abstractions. 3) Allows adding time-based constraints, from a logical or physical clock, either partially or totally ordered, in an optional and orthogonal way, while keeping models themselves timeless. 4) It simultaneously generalizes from memory to general abstractions, from sequential to concurrent specifications, either total or partial, and beyond serial executions. 5) Defines basic consistency axioms: monotonic visibility, local visibility, and closed past. These are satisfied by what we call serial consistency, but can be used as building blocks for novel consistency models with histories not explainable by any serial execution. 6) Revisits classic pipelined and causal consistency, revealing weaknesses in previous axiomatic models for PRAM and causal memory. 7) Introduces convergence and arbitration as safety properties for consistency models, departing from the use of eventual consistency, which conflates safety and liveness. 8) Formulates and proves the CLAM theorem for asynchronous distributed systems: any wait-free implementation of practically all data abstractions cannot simultaneously satisfy Closed past, Local visibility, Arbitration, and Monotonic visibility. While technically incomparable, the CLAM theorem is practically stronger than the CAP theorem, as it allows reasoning about the design space and possible tradeoffs in highly available partition tolerant systems.","sentences":["We define am axiomatic timeless framework for asynchronous distributed systems, together with well-formedness and consistency axioms, which unifies and generalizes the expressive power of current approaches.","1) It combines classic serialization per-process with a global visibility.","2) It defines a physical realizability well-formedness axiom to prevent physically impossible causality cycles, while allowing possible and useful visibility cycles, to allow synchronization-oriented abstractions.","3) Allows adding time-based constraints, from a logical or physical clock, either partially or totally ordered, in an optional and orthogonal way, while keeping models themselves timeless.","4) It simultaneously generalizes from memory to general abstractions, from sequential to concurrent specifications, either total or partial, and beyond serial executions.","5) Defines basic consistency axioms: monotonic visibility, local visibility, and closed past.","These are satisfied by what we call serial consistency, but can be used as building blocks for novel consistency models with histories not explainable by any serial execution.","6) Revisits classic pipelined and causal consistency, revealing weaknesses in previous axiomatic models for PRAM and causal memory.","7) Introduces convergence and arbitration as safety properties for consistency models, departing from the use of eventual consistency, which conflates safety and liveness.","8) Formulates and proves the CLAM theorem for asynchronous distributed systems: any wait-free implementation of practically all data abstractions cannot simultaneously satisfy Closed past, Local visibility, Arbitration, and Monotonic visibility.","While technically incomparable, the CLAM theorem is practically stronger than the CAP theorem, as it allows reasoning about the design space and possible tradeoffs in highly available partition tolerant systems."],"url":"http://arxiv.org/abs/2411.16355v1"}
{"created":"2024-11-25 12:58:00","title":"Machine learning for cerebral blood vessels' malformations","abstract":"Cerebral aneurysms and arteriovenous malformations are life-threatening hemodynamic pathologies of the brain. While surgical intervention is often essential to prevent fatal outcomes, it carries significant risks both during the procedure and in the postoperative period, making the management of these conditions highly challenging. Parameters of cerebral blood flow, routinely monitored during medical interventions, could potentially be utilized in machine learning-assisted protocols for risk assessment and therapeutic prognosis. To this end, we developed a linear oscillatory model of blood velocity and pressure for clinical data acquired from neurosurgical operations. Using the method of Sparse Identification of Nonlinear Dynamics (SINDy), the parameters of our model can be reconstructed online within milliseconds from a short time series of the hemodynamic variables. The identified parameter values enable automated classification of the blood-flow pathologies by means of logistic regression, achieving an accuracy of 73 %. Our results demonstrate the potential of this model for both diagnostic and prognostic applications, providing a robust and interpretable framework for assessing cerebral blood vessel conditions.","sentences":["Cerebral aneurysms and arteriovenous malformations are life-threatening hemodynamic pathologies of the brain.","While surgical intervention is often essential to prevent fatal outcomes, it carries significant risks both during the procedure and in the postoperative period, making the management of these conditions highly challenging.","Parameters of cerebral blood flow, routinely monitored during medical interventions, could potentially be utilized in machine learning-assisted protocols for risk assessment and therapeutic prognosis.","To this end, we developed a linear oscillatory model of blood velocity and pressure for clinical data acquired from neurosurgical operations.","Using the method of Sparse Identification of Nonlinear Dynamics (SINDy), the parameters of our model can be reconstructed online within milliseconds from a short time series of the hemodynamic variables.","The identified parameter values enable automated classification of the blood-flow pathologies by means of logistic regression, achieving an accuracy of 73 %.","Our results demonstrate the potential of this model for both diagnostic and prognostic applications, providing a robust and interpretable framework for assessing cerebral blood vessel conditions."],"url":"http://arxiv.org/abs/2411.16349v1"}
{"created":"2024-11-25 12:49:55","title":"Towards Foundation Models for Critical Care Time Series","abstract":"Notable progress has been made in generalist medical large language models across various healthcare areas. However, large-scale modeling of in-hospital time series data - such as vital signs, lab results, and treatments in critical care - remains underexplored. Existing datasets are relatively small, but combining them can enhance patient diversity and improve model robustness. To effectively utilize these combined datasets for large-scale modeling, it is essential to address the distribution shifts caused by varying treatment policies, necessitating the harmonization of treatment variables across the different datasets. This work aims to establish a foundation for training large-scale multi-variate time series models on critical care data and to provide a benchmark for machine learning models in transfer learning across hospitals to study and address distribution shift challenges. We introduce a harmonized dataset for sequence modeling and transfer learning research, representing the first large-scale collection to include core treatment variables. Future plans involve expanding this dataset to support further advancements in transfer learning and the development of scalable, generalizable models for critical healthcare applications.","sentences":["Notable progress has been made in generalist medical large language models across various healthcare areas.","However, large-scale modeling of in-hospital time series data - such as vital signs, lab results, and treatments in critical care - remains underexplored.","Existing datasets are relatively small, but combining them can enhance patient diversity and improve model robustness.","To effectively utilize these combined datasets for large-scale modeling, it is essential to address the distribution shifts caused by varying treatment policies, necessitating the harmonization of treatment variables across the different datasets.","This work aims to establish a foundation for training large-scale multi-variate time series models on critical care data and to provide a benchmark for machine learning models in transfer learning across hospitals to study and address distribution shift challenges.","We introduce a harmonized dataset for sequence modeling and transfer learning research, representing the first large-scale collection to include core treatment variables.","Future plans involve expanding this dataset to support further advancements in transfer learning and the development of scalable, generalizable models for critical healthcare applications."],"url":"http://arxiv.org/abs/2411.16346v1"}
{"created":"2024-11-25 12:38:59","title":"A Data-Driven Approach to Dataflow-Aware Online Scheduling for Graph Neural Network Inference","abstract":"Graph Neural Networks (GNNs) have shown significant promise in various domains, such as recommendation systems, bioinformatics, and network analysis. However, the irregularity of graph data poses unique challenges for efficient computation, leading to the development of specialized GNN accelerator architectures that surpass traditional CPU and GPU performance. Despite this, the structural diversity of input graphs results in varying performance across different GNN accelerators, depending on their dataflows. This variability in performance due to differing dataflows and graph properties remains largely unexplored, limiting the adaptability of GNN accelerators. To address this, we propose a data-driven framework for dataflow-aware latency prediction in GNN inference. Our approach involves training regressors to predict the latency of executing specific graphs on particular dataflows, using simulations on synthetic graphs. Experimental results indicate that our regressors can predict the optimal dataflow for a given graph with up to 91.28% accuracy and a Mean Absolute Percentage Error (MAPE) of 3.78%. Additionally, we introduce an online scheduling algorithm that uses these regressors to enhance scheduling decisions. Our experiments demonstrate that this algorithm achieves up to $3.17\\times$ speedup in mean completion time and $6.26\\times$ speedup in mean execution time compared to the best feasible baseline across all datasets.","sentences":["Graph Neural Networks (GNNs) have shown significant promise in various domains, such as recommendation systems, bioinformatics, and network analysis.","However, the irregularity of graph data poses unique challenges for efficient computation, leading to the development of specialized GNN accelerator architectures that surpass traditional CPU and GPU performance.","Despite this, the structural diversity of input graphs results in varying performance across different GNN accelerators, depending on their dataflows.","This variability in performance due to differing dataflows and graph properties remains largely unexplored, limiting the adaptability of GNN accelerators.","To address this, we propose a data-driven framework for dataflow-aware latency prediction in GNN inference.","Our approach involves training regressors to predict the latency of executing specific graphs on particular dataflows, using simulations on synthetic graphs.","Experimental results indicate that our regressors can predict the optimal dataflow for a given graph with up to 91.28% accuracy and a Mean Absolute Percentage Error (MAPE) of 3.78%.","Additionally, we introduce an online scheduling algorithm that uses these regressors to enhance scheduling decisions.","Our experiments demonstrate that this algorithm achieves up to $3.17\\times$ speedup in mean completion time and $6.26\\times$ speedup in mean execution time compared to the best feasible baseline across all datasets."],"url":"http://arxiv.org/abs/2411.16342v1"}
{"created":"2024-11-25 12:36:25","title":"Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions (Extended Abstract)","abstract":"This paper explores the intersection of privacy, cybersecurity, and environmental impacts, specifically energy consumption and carbon emissions, in cloud-based office solutions. We hypothesise that solutions that emphasise privacy and security are typically \"greener\" than solutions that are financed through data collection and advertising. To test our hypothesis, we first investigate how the underlying architectures and business models of these services, e.g., monetisation through (personalised) advertising, contribute to the services' environmental impact. We then explore commonly used methodologies and identify tools that facilitate environmental assessments of software systems. By combining these tools, we develop an approach to systematically assess the environmental footprint of the user-side of online services, which we apply to investigate and compare the influence of service design and ad-blocking technology on the emissions of common web-mail services. Our measurements of a limited selection of such services does not yet conclusively support or falsify our hypothesis regarding primary impacts. However, we are already able to identify the greener web-mail services on the user-side and continue the investigation towards conclusive assessment strategies for online office solutions.","sentences":["This paper explores the intersection of privacy, cybersecurity, and environmental impacts, specifically energy consumption and carbon emissions, in cloud-based office solutions.","We hypothesise that solutions that emphasise privacy and security are typically \"greener\" than solutions that are financed through data collection and advertising.","To test our hypothesis, we first investigate how the underlying architectures and business models of these services, e.g., monetisation through (personalised) advertising, contribute to the services' environmental impact.","We then explore commonly used methodologies and identify tools that facilitate environmental assessments of software systems.","By combining these tools, we develop an approach to systematically assess the environmental footprint of the user-side of online services, which we apply to investigate and compare the influence of service design and ad-blocking technology on the emissions of common web-mail services.","Our measurements of a limited selection of such services does not yet conclusively support or falsify our hypothesis regarding primary impacts.","However, we are already able to identify the greener web-mail services on the user-side and continue the investigation towards conclusive assessment strategies for online office solutions."],"url":"http://arxiv.org/abs/2411.16340v1"}
{"created":"2024-11-25 12:26:48","title":"Cluster-based human-in-the-loop strategy for improving machine learning-based circulating tumor cell detection in liquid biopsy","abstract":"Detection and differentiation of circulating tumor cells (CTCs) and non-CTCs in blood draws of cancer patients pose multiple challenges. While the gold standard relies on tedious manual evaluation of an automatically generated selection of images, machine learning (ML) techniques offer the potential to automate these processes. However, human assessment remains indispensable when the ML system arrives at uncertain or wrong decisions due to an insufficient set of labeled training data. This study introduces a human-in-the-loop (HiL) strategy for improving ML-based CTC detection. We combine self-supervised deep learning and a conventional ML-based classifier and propose iterative targeted sampling and labeling of new unlabeled training samples by human experts. The sampling strategy is based on the classification performance of local latent space clusters. The advantages of the proposed approach compared to naive random sampling are demonstrated for liquid biopsy data from patients with metastatic breast cancer.","sentences":["Detection and differentiation of circulating tumor cells (CTCs) and non-CTCs in blood draws of cancer patients pose multiple challenges.","While the gold standard relies on tedious manual evaluation of an automatically generated selection of images, machine learning (ML) techniques offer the potential to automate these processes.","However, human assessment remains indispensable when the ML system arrives at uncertain or wrong decisions due to an insufficient set of labeled training data.","This study introduces a human-in-the-loop (HiL) strategy for improving ML-based CTC detection.","We combine self-supervised deep learning and a conventional ML-based classifier and propose iterative targeted sampling and labeling of new unlabeled training samples by human experts.","The sampling strategy is based on the classification performance of local latent space clusters.","The advantages of the proposed approach compared to naive random sampling are demonstrated for liquid biopsy data from patients with metastatic breast cancer."],"url":"http://arxiv.org/abs/2411.16332v1"}
{"created":"2024-11-25 12:23:14","title":"CapHDR2IR: Caption-Driven Transfer from Visible Light to Infrared Domain","abstract":"Infrared (IR) imaging offers advantages in several fields due to its unique ability of capturing content in extreme light conditions. However, the demanding hardware requirements of high-resolution IR sensors limit its widespread application. As an alternative, visible light can be used to synthesize IR images but this causes a loss of fidelity in image details and introduces inconsistencies due to lack of contextual awareness of the scene. This stems from a combination of using visible light with a standard dynamic range, especially under extreme lighting, and a lack of contextual awareness can result in pseudo-thermal-crossover artifacts. This occurs when multiple objects with similar temperatures appear indistinguishable in the training data, further exacerbating the loss of fidelity. To solve this challenge, this paper proposes CapHDR2IR, a novel framework incorporating vision-language models using high dynamic range (HDR) images as inputs to generate IR images. HDR images capture a wider range of luminance variations, ensuring reliable IR image generation in different light conditions. Additionally, a dense caption branch integrates semantic understanding, resulting in more meaningful and discernible IR outputs. Extensive experiments on the HDRT dataset show that the proposed CapHDR2IR achieves state-of-the-art performance compared with existing general domain transfer methods and those tailored for visible-to-infrared image translation.","sentences":["Infrared (IR) imaging offers advantages in several fields due to its unique ability of capturing content in extreme light conditions.","However, the demanding hardware requirements of high-resolution IR sensors limit its widespread application.","As an alternative, visible light can be used to synthesize IR images but this causes a loss of fidelity in image details and introduces inconsistencies due to lack of contextual awareness of the scene.","This stems from a combination of using visible light with a standard dynamic range, especially under extreme lighting, and a lack of contextual awareness can result in pseudo-thermal-crossover artifacts.","This occurs when multiple objects with similar temperatures appear indistinguishable in the training data, further exacerbating the loss of fidelity.","To solve this challenge, this paper proposes CapHDR2IR, a novel framework incorporating vision-language models using high dynamic range (HDR) images as inputs to generate IR images.","HDR images capture a wider range of luminance variations, ensuring reliable IR image generation in different light conditions.","Additionally, a dense caption branch integrates semantic understanding, resulting in more meaningful and discernible IR outputs.","Extensive experiments on the HDRT dataset show that the proposed CapHDR2IR achieves state-of-the-art performance compared with existing general domain transfer methods and those tailored for visible-to-infrared image translation."],"url":"http://arxiv.org/abs/2411.16327v1"}
{"created":"2024-11-25 12:11:27","title":"CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance Segmentation","abstract":"Traditionally, algorithms that learn to segment object instances in 2D images have heavily relied on large amounts of human-annotated data. Only recently, novel approaches have emerged tackling this problem in an unsupervised fashion. Generally, these approaches first generate pseudo-masks and then train a class-agnostic detector. While such methods deliver the current state of the art, they often fail to correctly separate instances overlapping in 2D image space since only semantics are considered. To tackle this issue, we instead propose to cut the semantic masks in 3D to obtain the final 2D instances by utilizing a point cloud representation of the scene. Furthermore, we derive a Spatial Importance function, which we use to resharpen the semantics along the 3D borders of instances. Nevertheless, these pseudo-masks are still subject to mask ambiguity. To address this issue, we further propose to augment the training of a class-agnostic detector with three Spatial Confidence components aiming to isolate a clean learning signal. With these contributions, our approach outperforms competing methods across multiple standard benchmarks for unsupervised instance segmentation and object detection.","sentences":["Traditionally, algorithms that learn to segment object instances in 2D images have heavily relied on large amounts of human-annotated data.","Only recently, novel approaches have emerged tackling this problem in an unsupervised fashion.","Generally, these approaches first generate pseudo-masks and then train a class-agnostic detector.","While such methods deliver the current state of the art, they often fail to correctly separate instances overlapping in 2D image space since only semantics are considered.","To tackle this issue, we instead propose to cut the semantic masks in 3D to obtain the final 2D instances by utilizing a point cloud representation of the scene.","Furthermore, we derive a Spatial Importance function, which we use to resharpen the semantics along the 3D borders of instances.","Nevertheless, these pseudo-masks are still subject to mask ambiguity.","To address this issue, we further propose to augment the training of a class-agnostic detector with three Spatial Confidence components aiming to isolate a clean learning signal.","With these contributions, our approach outperforms competing methods across multiple standard benchmarks for unsupervised instance segmentation and object detection."],"url":"http://arxiv.org/abs/2411.16319v1"}
{"created":"2024-11-25 12:08:54","title":"Local Learning for Covariate Selection in Nonparametric Causal Effect Estimation with Latent Variables","abstract":"Estimating causal effects from nonexperimental data is a fundamental problem in many fields of science. A key component of this task is selecting an appropriate set of covariates for confounding adjustment to avoid bias. Most existing methods for covariate selection often assume the absence of latent variables and rely on learning the global network structure among variables. However, identifying the global structure can be unnecessary and inefficient, especially when our primary interest lies in estimating the effect of a treatment variable on an outcome variable. To address this limitation, we propose a novel local learning approach for covariate selection in nonparametric causal effect estimation, which accounts for the presence of latent variables. Our approach leverages testable independence and dependence relationships among observed variables to identify a valid adjustment set for a target causal relationship, ensuring both soundness and completeness under standard assumptions. We validate the effectiveness of our algorithm through extensive experiments on both synthetic and real-world data.","sentences":["Estimating causal effects from nonexperimental data is a fundamental problem in many fields of science.","A key component of this task is selecting an appropriate set of covariates for confounding adjustment to avoid bias.","Most existing methods for covariate selection often assume the absence of latent variables and rely on learning the global network structure among variables.","However, identifying the global structure can be unnecessary and inefficient, especially when our primary interest lies in estimating the effect of a treatment variable on an outcome variable.","To address this limitation, we propose a novel local learning approach for covariate selection in nonparametric causal effect estimation, which accounts for the presence of latent variables.","Our approach leverages testable independence and dependence relationships among observed variables to identify a valid adjustment set for a target causal relationship, ensuring both soundness and completeness under standard assumptions.","We validate the effectiveness of our algorithm through extensive experiments on both synthetic and real-world data."],"url":"http://arxiv.org/abs/2411.16315v1"}
{"created":"2024-11-25 11:53:55","title":"An End-to-End Robust Point Cloud Semantic Segmentation Network with Single-Step Conditional Diffusion Models","abstract":"Existing conditional Denoising Diffusion Probabilistic Models (DDPMs) with a Noise-Conditional Framework (NCF) remain challenging for 3D scene understanding tasks, as the complex geometric details in scenes increase the difficulty of fitting the gradients of the data distribution (the scores) from semantic labels. This also results in longer training and inference time for DDPMs compared to non-DDPMs. From a different perspective, we delve deeply into the model paradigm dominated by the Conditional Network. In this paper, we propose an end-to-end robust semantic \\textbf{Seg}mentation \\textbf{Net}work based on a \\textbf{C}onditional-Noise Framework (CNF) of D\\textbf{D}PMs, named \\textbf{CDSegNet}. Specifically, CDSegNet models the Noise Network (NN) as a learnable noise-feature generator. This enables the Conditional Network (CN) to understand 3D scene semantics under multi-level feature perturbations, enhancing the generalization in unseen scenes. Meanwhile, benefiting from the noise system of DDPMs, CDSegNet exhibits strong noise and sparsity robustness in experiments. Moreover, thanks to CNF, CDSegNet can generate the semantic labels in a single-step inference like non-DDPMs, due to avoiding directly fitting the scores from semantic labels in the dominant network of CDSegNet. On public indoor and outdoor benchmarks, CDSegNet significantly outperforms existing methods, achieving state-of-the-art performance.","sentences":["Existing conditional Denoising Diffusion Probabilistic Models (DDPMs) with a Noise-Conditional Framework (NCF) remain challenging for 3D scene understanding tasks, as the complex geometric details in scenes increase the difficulty of fitting the gradients of the data distribution (the scores) from semantic labels.","This also results in longer training and inference time for DDPMs compared to non-DDPMs.","From a different perspective, we delve deeply into the model paradigm dominated by the Conditional Network.","In this paper, we propose an end-to-end robust semantic \\textbf{Seg}mentation \\textbf{Net}work based on a \\textbf{C}onditional-Noise Framework (CNF) of D\\textbf{D}PMs, named \\textbf{CDSegNet}.","Specifically, CDSegNet models the Noise Network (NN) as a learnable noise-feature generator.","This enables the Conditional Network (CN) to understand 3D scene semantics under multi-level feature perturbations, enhancing the generalization in unseen scenes.","Meanwhile, benefiting from the noise system of DDPMs, CDSegNet exhibits strong noise and sparsity robustness in experiments.","Moreover, thanks to CNF, CDSegNet can generate the semantic labels in a single-step inference like non-DDPMs, due to avoiding directly fitting the scores from semantic labels in the dominant network of CDSegNet.","On public indoor and outdoor benchmarks, CDSegNet significantly outperforms existing methods, achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2411.16308v1"}
{"created":"2024-11-25 11:47:31","title":"Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems","abstract":"Task-oriented Dialog (ToD) systems have to solve multiple subgoals to accomplish user goals, whereas feedback is often obtained only at the end of the dialog. In this work, we propose SUIT (SUbgoal-aware ITerative Training), an iterative training approach for improving ToD systems. We sample dialogs from the model we aim to improve and determine subgoals that contribute to dialog success using distant supervision to obtain high quality training samples. We show how this data improves supervised fine-tuning or, alternatively, preference learning results. SUIT is able to iteratively generate more data instead of relying on fixed static sets. SUIT reaches new state-of-the-art performance on a popular ToD benchmark.","sentences":["Task-oriented Dialog (ToD) systems have to solve multiple subgoals to accomplish user goals, whereas feedback is often obtained only at the end of the dialog.","In this work, we propose SUIT (SUbgoal-aware ITerative Training), an iterative training approach for improving ToD systems.","We sample dialogs from the model we aim to improve and determine subgoals that contribute to dialog success using distant supervision to obtain high quality training samples.","We show how this data improves supervised fine-tuning or, alternatively, preference learning results.","SUIT is able to iteratively generate more data instead of relying on fixed static sets.","SUIT reaches new state-of-the-art performance on a popular ToD benchmark."],"url":"http://arxiv.org/abs/2411.16305v1"}
{"created":"2024-11-25 11:43:22","title":"Understanding Generalization of Federated Learning: the Trade-off between Model Stability and Optimization","abstract":"Federated Learning (FL) is a distributed learning approach that trains neural networks across multiple devices while keeping their local data private. However, FL often faces challenges due to data heterogeneity, leading to inconsistent local optima among clients. These inconsistencies can cause unfavorable convergence behavior and generalization performance degradation. Existing studies mainly describe this issue through \\textit{convergence analysis}, focusing on how well a model fits training data, or through \\textit{algorithmic stability}, which examines the generalization gap. However, neither approach precisely captures the generalization performance of FL algorithms, especially for neural networks. In this paper, we introduce the first generalization dynamics analysis framework in federated optimization, highlighting the trade-offs between model stability and optimization. Through this framework, we show how the generalization of FL algorithms is affected by the interplay of algorithmic stability and optimization. This framework applies to standard federated optimization and its advanced versions, like server momentum. We find that fast convergence from large local steps or accelerated momentum enlarges stability but obtains better generalization performance. Our insights into these trade-offs can guide the practice of future algorithms for better generalization.","sentences":["Federated Learning (FL) is a distributed learning approach that trains neural networks across multiple devices while keeping their local data private.","However, FL often faces challenges due to data heterogeneity, leading to inconsistent local optima among clients.","These inconsistencies can cause unfavorable convergence behavior and generalization performance degradation.","Existing studies mainly describe this issue through \\textit{convergence analysis}, focusing on how well a model fits training data, or through \\textit{algorithmic stability}, which examines the generalization gap.","However, neither approach precisely captures the generalization performance of FL algorithms, especially for neural networks.","In this paper, we introduce the first generalization dynamics analysis framework in federated optimization, highlighting the trade-offs between model stability and optimization.","Through this framework, we show how the generalization of FL algorithms is affected by the interplay of algorithmic stability and optimization.","This framework applies to standard federated optimization and its advanced versions, like server momentum.","We find that fast convergence from large local steps or accelerated momentum enlarges stability but obtains better generalization performance.","Our insights into these trade-offs can guide the practice of future algorithms for better generalization."],"url":"http://arxiv.org/abs/2411.16303v1"}
{"created":"2024-11-25 11:35:08","title":"BayLing 2: A Multilingual Large Language Model with Efficient Language Alignment","abstract":"Large language models (LLMs), with their powerful generative capabilities and vast knowledge, empower various tasks in everyday life. However, these abilities are primarily concentrated in high-resource languages, leaving low-resource languages with weaker generative capabilities and relatively limited knowledge. Enhancing the multilingual capabilities of LLMs is therefore crucial for serving over 100 linguistic communities worldwide. An intuitive approach to enhance the multilingual capabilities would be to construct instruction data for various languages, but constructing instruction data for over 100 languages is prohibitively costly. In this paper, we introduce BayLing 2, which efficiently transfers generative capabilities and knowledge from high-resource languages to low-resource languages through language alignment. To achieve this, we constructed a dataset of 3.2 million instructions, comprising high-resource language instructions (Chinese and English) and cross-lingual instructions for 100+ languages and performed instruction tuning based on the dataset to facilitate the capability transfer between languages. Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B, and BayLing-3-8B, and conducted a comprehensive evaluation of BayLing. For multilingual translation across 100+ languages, BayLing shows superior performance compared to open-source models of similar scale. For multilingual knowledge and understanding benchmarks, BayLing achieves significant improvements across over 20 low-resource languages, demonstrating its capability of effective knowledge transfer from high-resource to low-resource languages. Furthermore, results on English benchmarks indicate that BayLing maintains high performance in highresource languages while enhancing the performance in low-resource languages. Demo, homepage, code and models of BayLing are available.","sentences":["Large language models (LLMs), with their powerful generative capabilities and vast knowledge, empower various tasks in everyday life.","However, these abilities are primarily concentrated in high-resource languages, leaving low-resource languages with weaker generative capabilities and relatively limited knowledge.","Enhancing the multilingual capabilities of LLMs is therefore crucial for serving over 100 linguistic communities worldwide.","An intuitive approach to enhance the multilingual capabilities would be to construct instruction data for various languages, but constructing instruction data for over 100 languages is prohibitively costly.","In this paper, we introduce BayLing 2, which efficiently transfers generative capabilities and knowledge from high-resource languages to low-resource languages through language alignment.","To achieve this, we constructed a dataset of 3.2 million instructions, comprising high-resource language instructions (Chinese and English) and cross-lingual instructions for 100+ languages and performed instruction tuning based on the dataset to facilitate the capability transfer between languages.","Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B, and BayLing-3-8B, and conducted a comprehensive evaluation of BayLing.","For multilingual translation across 100+ languages, BayLing shows superior performance compared to open-source models of similar scale.","For multilingual knowledge and understanding benchmarks, BayLing achieves significant improvements across over 20 low-resource languages, demonstrating its capability of effective knowledge transfer from high-resource to low-resource languages.","Furthermore, results on English benchmarks indicate that BayLing maintains high performance in highresource languages while enhancing the performance in low-resource languages.","Demo, homepage, code and models of BayLing are available."],"url":"http://arxiv.org/abs/2411.16300v1"}
{"created":"2024-11-25 11:31:53","title":"Evaluating Rank-N-Contrast: Continuous and Robust Representations for Regression","abstract":"This document is a replication of the original \"Rank-N-Contrast\" (arXiv:2210.01189v2) paper published in 2023. This evaluation is done for academic purposes. Deep regression models often fail to capture the continuous nature of sample orders, creating fragmented representations and suboptimal performance. To address this, we reproduced the Rank-N-Contrast (RNC) framework, which learns continuous representations by contrasting samples by their rankings in the target space. Our study validates RNC's theoretical and empirical benefits, including improved performance and robustness. We extended the evaluation to an additional regression dataset and conducted robustness tests using a holdout method, where a specific range of continuous data was excluded from the training set. This approach assessed the model's ability to generalise to unseen data and achieve state-of-the-art performance. This replication study validates the original findings and broadens the understanding of RNC's applicability and robustness.","sentences":["This document is a replication of the original \"Rank-N-Contrast\" (arXiv:2210.01189v2) paper published in 2023.","This evaluation is done for academic purposes.","Deep regression models often fail to capture the continuous nature of sample orders, creating fragmented representations and suboptimal performance.","To address this, we reproduced the Rank-N-Contrast (RNC) framework, which learns continuous representations by contrasting samples by their rankings in the target space.","Our study validates RNC's theoretical and empirical benefits, including improved performance and robustness.","We extended the evaluation to an additional regression dataset and conducted robustness tests using a holdout method, where a specific range of continuous data was excluded from the training set.","This approach assessed the model's ability to generalise to unseen data and achieve state-of-the-art performance.","This replication study validates the original findings and broadens the understanding of RNC's applicability and robustness."],"url":"http://arxiv.org/abs/2411.16298v1"}
{"created":"2024-11-25 11:02:44","title":"Dynamic Range Minimum Queries on the Ultra-Wide Word RAM","abstract":"We consider the dynamic range minimum problem on the ultra-wide word RAM model of computation. This model extends the classic $w$-bit word RAM model with special ultrawords of length $w^2$ bits that support standard arithmetic and boolean operation and scattered memory access operations that can access $w$ (non-contiguous) locations in memory. The ultra-wide word RAM model captures (and idealizes) modern vector processor architectures.   Our main result is a linear space data structure that supports range minimum queries and updates in $O(\\log \\log \\log n)$ time. This exponentially improves the time of existing techniques. Our result is based on a simple reduction to prefix minimum computations on sequences $O(\\log n)$ words combined with a new parallel, recursive implementation of these.","sentences":["We consider the dynamic range minimum problem on the ultra-wide word RAM model of computation.","This model extends the classic $w$-bit word RAM model with special ultrawords of length $w^2$ bits that support standard arithmetic and boolean operation and scattered memory access operations that can access $w$ (non-contiguous) locations in memory.","The ultra-wide word RAM model captures (and idealizes) modern vector processor architectures.   ","Our main result is a linear space data structure that supports range minimum queries and updates in $O(\\log \\log \\log","n)$ time.","This exponentially improves the time of existing techniques.","Our result is based on a simple reduction to prefix minimum computations on sequences $O(\\log n)$ words combined with a new parallel, recursive implementation of these."],"url":"http://arxiv.org/abs/2411.16281v1"}
{"created":"2024-11-25 10:23:11","title":"Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures","abstract":"Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmetic tasks. In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as \\emph{Commutativity} and \\emph{Identity} properties. Since these structures are observable through input-output relationships, they can generalize to unseen data. We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems. Our findings indicate that leveraging algebraic structures can enhance the LLMs' arithmetic capabilities, offering insights into improving their arithmetic performance.","sentences":["Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions.","This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH.","However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood.","Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmetic tasks.","In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as \\emph{Commutativity} and \\emph{Identity} properties.","Since these structures are observable through input-output relationships, they can generalize to unseen data.","We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems.","Our findings indicate that leveraging algebraic structures can enhance the LLMs' arithmetic capabilities, offering insights into improving their arithmetic performance."],"url":"http://arxiv.org/abs/2411.16260v1"}
{"created":"2024-11-25 10:17:49","title":"Scalable Fault-Tolerant MapReduce","abstract":"Supercomputers getting ever larger and energy-efficient is at odds with the reliability of the used hardware. Thus, the time intervals between component failures are decreasing. Contrarily, the latencies for individual operations of coarse-grained big-data tools grow with the number of processors. To overcome the resulting scalability limit, we need to go beyond the current practice of interoperation checkpointing. We give first results on how to achieve this for the popular MapReduce framework where huge multisets are processed by user-defined mapping and reducing functions. We observe that the full state of a MapReduce algorithm is described by its network communication. We present a low-overhead technique with no additional work during fault-free execution and the negligible expected relative communication overhead of $1/(p-1)$ on $p$ PEs. Recovery takes approximately the time of processing $1/p$ of the data on the surviving PEs. We achieve this by backing up self-messages and locally storing all messages sent through the network on the sending and receiving PEs until the next round of global communication. A prototypical implementation already indicates low overhead $<4\\,\\%$ during fault-free execution.","sentences":["Supercomputers getting ever larger and energy-efficient is at odds with the reliability of the used hardware.","Thus, the time intervals between component failures are decreasing.","Contrarily, the latencies for individual operations of coarse-grained big-data tools grow with the number of processors.","To overcome the resulting scalability limit, we need to go beyond the current practice of interoperation checkpointing.","We give first results on how to achieve this for the popular MapReduce framework where huge multisets are processed by user-defined mapping and reducing functions.","We observe that the full state of a MapReduce algorithm is described by its network communication.","We present a low-overhead technique with no additional work during fault-free execution and the negligible expected relative communication overhead of $1/(p-1)$ on $p$ PEs.","Recovery takes approximately the time of processing $1/p$ of the data on the surviving PEs.","We achieve this by backing up self-messages and locally storing all messages sent through the network on the sending and receiving PEs until the next round of global communication.","A prototypical implementation already indicates low overhead $<4\\,\\%$ during fault-free execution."],"url":"http://arxiv.org/abs/2411.16255v1"}
{"created":"2024-11-25 09:53:35","title":"UVLLM: An Automated Universal RTL Verification Framework using LLMs","abstract":"Verifying hardware designs in embedded systems is crucial but often labor-intensive and time-consuming. While existing solutions have improved automation, they frequently rely on unrealistic assumptions. To address these challenges, we introduce a novel framework, UVLLM, which combines Large Language Models (LLMs) with the Universal Verification Methodology (UVM) to relax these assumptions. UVLLM significantly enhances the automation of testing and repairing error-prone Register Transfer Level (RTL) codes, a critical aspect of verification development. Unlike existing methods, UVLLM ensures that all errors are triggered during verification, achieving a syntax error fix rate of 86.99% and a functional error fix rate of 71.92% on our proposed benchmark. These results demonstrate a substantial improvement in verification efficiency. Additionally, our study highlights the current limitations of LLM applications, particularly their reliance on extensive training data. We emphasize the transformative potential of LLMs in hardware design verification and suggest promising directions for future research in AI-driven hardware design methodologies. The Repo. of dataset and code: https://anonymous.4open.science/r/UVLLM/.","sentences":["Verifying hardware designs in embedded systems is crucial but often labor-intensive and time-consuming.","While existing solutions have improved automation, they frequently rely on unrealistic assumptions.","To address these challenges, we introduce a novel framework, UVLLM, which combines Large Language Models (LLMs) with the Universal Verification Methodology (UVM) to relax these assumptions.","UVLLM significantly enhances the automation of testing and repairing error-prone Register Transfer Level (RTL) codes, a critical aspect of verification development.","Unlike existing methods, UVLLM ensures that all errors are triggered during verification, achieving a syntax error fix rate of 86.99% and a functional error fix rate of 71.92% on our proposed benchmark.","These results demonstrate a substantial improvement in verification efficiency.","Additionally, our study highlights the current limitations of LLM applications, particularly their reliance on extensive training data.","We emphasize the transformative potential of LLMs in hardware design verification and suggest promising directions for future research in AI-driven hardware design methodologies.","The Repo. of dataset and code: https://anonymous.4open.science/r/UVLLM/."],"url":"http://arxiv.org/abs/2411.16238v1"}
{"created":"2024-11-25 09:28:58","title":"Weakly supervised image segmentation for defect-based grading of fresh produce","abstract":"Implementing image-based machine learning in agriculture is often limited by scarce data and annotations, making it hard to achieve high-quality model predictions. This study tackles the issue of postharvest quality assessment of bananas in decentralized supply chains. We propose a method to detect and segment surface defects in banana images using panoptic segmentation to quantify defect size and number. Instead of time-consuming pixel-level annotations, we use weak supervision with coarse labels. A dataset of 476 smartphone images of bananas was collected under real-world field conditions and annotated for bruises and scars. Using the Segment Anything Model (SAM), a recently published foundation model for image segmentation, we generated dense annotations from coarse bounding boxes to train a segmentation model, significantly reducing manual effort while achieving a panoptic quality score of 77.6%. This demonstrates SAM's potential for low-effort, accurate segmentation in agricultural settings with limited data.","sentences":["Implementing image-based machine learning in agriculture is often limited by scarce data and annotations, making it hard to achieve high-quality model predictions.","This study tackles the issue of postharvest quality assessment of bananas in decentralized supply chains.","We propose a method to detect and segment surface defects in banana images using panoptic segmentation to quantify defect size and number.","Instead of time-consuming pixel-level annotations, we use weak supervision with coarse labels.","A dataset of 476 smartphone images of bananas was collected under real-world field conditions and annotated for bruises and scars.","Using the Segment Anything Model (SAM), a recently published foundation model for image segmentation, we generated dense annotations from coarse bounding boxes to train a segmentation model, significantly reducing manual effort while achieving a panoptic quality score of 77.6%.","This demonstrates SAM's potential for low-effort, accurate segmentation in agricultural settings with limited data."],"url":"http://arxiv.org/abs/2411.16219v1"}
{"created":"2024-11-25 09:14:53","title":"Can Encrypted Images Still Train Neural Networks? Investigating Image Information and Random Vortex Transformation","abstract":"Vision is one of the essential sources through which humans acquire information. In this paper, we establish a novel framework for measuring image information content to evaluate the variation in information content during image transformations. Within this framework, we design a nonlinear function to calculate the neighboring information content of pixels at different distances, and then use this information to measure the overall information content of the image. Hence, we define a function to represent the variation in information content during image transformations. Additionally, we utilize this framework to prove the conclusion that swapping the positions of any two pixels reduces the image's information content. Furthermore, based on the aforementioned framework, we propose a novel image encryption algorithm called Random Vortex Transformation. This algorithm encrypts the image using random functions while preserving the neighboring information of the pixels. The encrypted images are difficult for the human eye to distinguish, yet they allow for direct training of the encrypted images using machine learning methods. Experimental verification demonstrates that training on the encrypted dataset using ResNet and Vision Transformers only results in a decrease in accuracy ranging from 0.3\\% to 6.5\\% compared to the original data, while ensuring the security of the data. Furthermore, there is a positive correlation between the rate of information loss in the images and the rate of accuracy loss, further supporting the validity of the proposed image information content measurement framework.","sentences":["Vision is one of the essential sources through which humans acquire information.","In this paper, we establish a novel framework for measuring image information content to evaluate the variation in information content during image transformations.","Within this framework, we design a nonlinear function to calculate the neighboring information content of pixels at different distances, and then use this information to measure the overall information content of the image.","Hence, we define a function to represent the variation in information content during image transformations.","Additionally, we utilize this framework to prove the conclusion that swapping the positions of any two pixels reduces the image's information content.","Furthermore, based on the aforementioned framework, we propose a novel image encryption algorithm called Random Vortex Transformation.","This algorithm encrypts the image using random functions while preserving the neighboring information of the pixels.","The encrypted images are difficult for the human eye to distinguish, yet they allow for direct training of the encrypted images using machine learning methods.","Experimental verification demonstrates that training on the encrypted dataset using ResNet and Vision Transformers only results in a decrease in accuracy ranging from 0.3\\% to 6.5\\% compared to the original data, while ensuring the security of the data.","Furthermore, there is a positive correlation between the rate of information loss in the images and the rate of accuracy loss, further supporting the validity of the proposed image information content measurement framework."],"url":"http://arxiv.org/abs/2411.16207v1"}
{"created":"2024-11-25 08:59:39","title":"Video-Text Dataset Construction from Multi-AI Feedback: Promoting Weak-to-Strong Preference Learning for Video Large Language Models","abstract":"High-quality video-text preference data is crucial for Multimodal Large Language Models (MLLMs) alignment. However, existing preference data is very scarce. Obtaining VQA preference data for preference training is costly, and manually annotating responses is highly unreliable, which could result in low-quality pairs. Meanwhile, AI-generated responses controlled by temperature adjustment lack diversity. To address these issues, we propose a high-quality VQA preference dataset, called \\textit{\\textbf{M}ultiple \\textbf{M}ultimodal \\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{P}reference Datasets in \\textbf{V}QA} (\\textbf{MMAIP-V}), which is constructed by sampling from the response distribution set and using an external scoring function for response evaluation. Furthermore, to fully leverage the preference knowledge in MMAIP-V and ensure sufficient optimization, we propose \\textit{\\textbf{Iter}ative \\textbf{W}eak-to-\\textbf{S}trong \\textbf{R}einforcement \\textbf{L}earning from \\textbf{AI} \\textbf{F}eedback for video MLLMs} (\\textbf{Iter-W2S-RLAIF}), a framework that gradually enhances MLLMs' alignment capabilities by iteratively updating the reference model and performing parameter extrapolation. Finally, we propose an unbiased and information-complete evaluation scheme in VQA evaluation. Experiments demonstrate that MMAIP-V is beneficial for MLLMs in preference learning and Iter-W2S-RLAIF fully exploits the alignment information in MMAIP-V. We believe that the proposed automatic VQA preference data generation pipeline based on AI feedback can greatly promote future work in the MLLMs alignment. \\textbf{Code and dataset are available} \\href{https://anonymous.4open.science/r/MMAIP-V_Iter-W2S-RLAIF-702F}{MMAIP-V\\_Iter-W2S-RLAIF-702F}.","sentences":["High-quality video-text preference data is crucial for Multimodal Large Language Models (MLLMs) alignment.","However, existing preference data is very scarce.","Obtaining VQA preference data for preference training is costly, and manually annotating responses is highly unreliable, which could result in low-quality pairs.","Meanwhile, AI-generated responses controlled by temperature adjustment lack diversity.","To address these issues, we propose a high-quality VQA preference dataset, called \\textit{\\textbf{M}ultiple \\textbf{M}ultimodal \\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{P}reference Datasets in \\textbf{V}QA} (\\textbf{MMAIP-V}), which is constructed by sampling from the response distribution set and using an external scoring function for response evaluation.","Furthermore, to fully leverage the preference knowledge in MMAIP-V and ensure sufficient optimization, we propose \\textit{\\textbf{Iter}ative \\textbf{W}eak-to-\\textbf{S}trong \\textbf{R}einforcement \\textbf{L}earning from \\textbf{AI} \\textbf{F}eedback for video MLLMs} (\\textbf{Iter-W2S-RLAIF}), a framework that gradually enhances MLLMs' alignment capabilities by iteratively updating the reference model and performing parameter extrapolation.","Finally, we propose an unbiased and information-complete evaluation scheme in VQA evaluation.","Experiments demonstrate that MMAIP-V is beneficial for MLLMs in preference learning and Iter-W2S-RLAIF fully exploits the alignment information in MMAIP-V. We believe that the proposed automatic VQA preference data generation pipeline based on AI feedback can greatly promote future work in the MLLMs alignment.","\\textbf{Code and dataset are available} \\href{https://anonymous.4open.science/r/MMAIP-V_Iter-W2S-RLAIF-702F}{MMAIP-V\\_Iter-W2S-RLAIF-702F}."],"url":"http://arxiv.org/abs/2411.16201v1"}
{"created":"2024-11-25 08:52:46","title":"Learn from Foundation Model: Fruit Detection Model without Manual Annotation","abstract":"Recent breakthroughs in large foundation models have enabled the possibility of transferring knowledge pre-trained on vast datasets to domains with limited data availability. Agriculture is one of the domains that lacks sufficient data. This study proposes a framework to train effective, domain-specific, small models from foundation models without manual annotation. Our approach begins with SDM (Segmentation-Description-Matching), a stage that leverages two foundation models: SAM2 (Segment Anything in Images and Videos) for segmentation and OpenCLIP (Open Contrastive Language-Image Pretraining) for zero-shot open-vocabulary classification. In the second stage, a novel knowledge distillation mechanism is utilized to distill compact, edge-deployable models from SDM, enhancing both inference speed and perception accuracy. The complete method, termed SDM-D (Segmentation-Description-Matching-Distilling), demonstrates strong performance across various fruit detection tasks object detection, semantic segmentation, and instance segmentation) without manual annotation. It nearly matches the performance of models trained with abundant labels. Notably, SDM-D outperforms open-set detection methods such as Grounding SAM and YOLO-World on all tested fruit detection datasets. Additionally, we introduce MegaFruits, a comprehensive fruit segmentation dataset encompassing over 25,000 images, and all code and datasets are made publicly available at https://github.com/AgRoboticsResearch/SDM-D.git.","sentences":["Recent breakthroughs in large foundation models have enabled the possibility of transferring knowledge pre-trained on vast datasets to domains with limited data availability.","Agriculture is one of the domains that lacks sufficient data.","This study proposes a framework to train effective, domain-specific, small models from foundation models without manual annotation.","Our approach begins with SDM (Segmentation-Description-Matching), a stage that leverages two foundation models: SAM2 (Segment Anything in Images and Videos) for segmentation and OpenCLIP (Open Contrastive Language-Image Pretraining) for zero-shot open-vocabulary classification.","In the second stage, a novel knowledge distillation mechanism is utilized to distill compact, edge-deployable models from SDM, enhancing both inference speed and perception accuracy.","The complete method, termed SDM-D (Segmentation-Description-Matching-Distilling), demonstrates strong performance across various fruit detection tasks object detection, semantic segmentation, and instance segmentation) without manual annotation.","It nearly matches the performance of models trained with abundant labels.","Notably, SDM-D outperforms open-set detection methods such as Grounding SAM and YOLO-World on all tested fruit detection datasets.","Additionally, we introduce MegaFruits, a comprehensive fruit segmentation dataset encompassing over 25,000 images, and all code and datasets are made publicly available at https://github.com/AgRoboticsResearch/SDM-D.git."],"url":"http://arxiv.org/abs/2411.16196v1"}
{"created":"2024-11-25 08:23:38","title":"Event-boosted Deformable 3D Gaussians for Fast Dynamic Scene Reconstruction","abstract":"3D Gaussian Splatting (3D-GS) enables real-time rendering but struggles with fast motion due to low temporal resolution of RGB cameras. To address this, we introduce the first approach combining event cameras, which capture high-temporal-resolution, continuous motion data, with deformable 3D-GS for fast dynamic scene reconstruction. We observe that threshold modeling for events plays a crucial role in achieving high-quality reconstruction. Therefore, we propose a GS-Threshold Joint Modeling (GTJM) strategy, creating a mutually reinforcing process that greatly improves both 3D reconstruction and threshold modeling. Moreover, we introduce a Dynamic-Static Decomposition (DSD) strategy that first identifies dynamic areas by exploiting the inability of static Gaussians to represent motions, then applies a buffer-based soft decomposition to separate dynamic and static areas. This strategy accelerates rendering by avoiding unnecessary deformation in static areas, and focuses on dynamic areas to enhance fidelity. Our approach achieves high-fidelity dynamic reconstruction at 156 FPS with a 400$\\times$400 resolution on an RTX 3090 GPU.","sentences":["3D Gaussian Splatting (3D-GS) enables real-time rendering but struggles with fast motion due to low temporal resolution of RGB cameras.","To address this, we introduce the first approach combining event cameras, which capture high-temporal-resolution, continuous motion data, with deformable 3D-GS for fast dynamic scene reconstruction.","We observe that threshold modeling for events plays a crucial role in achieving high-quality reconstruction.","Therefore, we propose a GS-Threshold Joint Modeling (GTJM) strategy, creating a mutually reinforcing process that greatly improves both 3D reconstruction and threshold modeling.","Moreover, we introduce a Dynamic-Static Decomposition (DSD) strategy that first identifies dynamic areas by exploiting the inability of static Gaussians to represent motions, then applies a buffer-based soft decomposition to separate dynamic and static areas.","This strategy accelerates rendering by avoiding unnecessary deformation in static areas, and focuses on dynamic areas to enhance fidelity.","Our approach achieves high-fidelity dynamic reconstruction at 156 FPS with a 400$\\times$400 resolution on an RTX 3090 GPU."],"url":"http://arxiv.org/abs/2411.16180v1"}
{"created":"2024-11-25 08:04:47","title":"SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis","abstract":"Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With the exponential growth of video data across web platforms, understanding long-form video is crucial for advancing generalized intelligence. In this paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel video-LLM framework designed to enhance the comprehension of lengthy video content through targeted retrieval process. We address two main challenges to achieve it: (i) We present the SceneWalk dataset, a high-quality collection of 87.8K long videos, each densely captioned at the segment level to enable models to capture scene continuity and maintain rich descriptive context. (ii) We develop robust architectural designs integrating dynamic routing mechanism and spatio-temporal projector to efficiently retrieve and process relevant video segments based on user queries. Our framework mitigates the limitations of current video-LMMs by allowing for precise identification and retrieval of relevant video segments in response to queries, thereby improving the contextual relevance of the generated responses. Through extensive experiments, SALOVA demonstrates enhanced capability in processing complex long-form videos, showing significant capability to maintain contextual integrity across extended sequences.","sentences":["Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead.","These constraints often lead to significant information loss and reduced relevance in the model responses.","With the exponential growth of video data across web platforms, understanding long-form video is crucial for advancing generalized intelligence.","In this paper, we introduce SALOVA:","Segment-Augmented LOng Video Assistant, a novel video-LLM framework designed to enhance the comprehension of lengthy video content through targeted retrieval process.","We address two main challenges to achieve it: (i) We present the SceneWalk dataset, a high-quality collection of 87.8K long videos, each densely captioned at the segment level to enable models to capture scene continuity and maintain rich descriptive context.","(ii) We develop robust architectural designs integrating dynamic routing mechanism and spatio-temporal projector to efficiently retrieve and process relevant video segments based on user queries.","Our framework mitigates the limitations of current video-LMMs by allowing for precise identification and retrieval of relevant video segments in response to queries, thereby improving the contextual relevance of the generated responses.","Through extensive experiments, SALOVA demonstrates enhanced capability in processing complex long-form videos, showing significant capability to maintain contextual integrity across extended sequences."],"url":"http://arxiv.org/abs/2411.16173v1"}
{"created":"2024-11-25 08:02:28","title":"U2NeRF: Unsupervised Underwater Image Restoration and Neural Radiance Fields","abstract":"Underwater images suffer from colour shifts, low contrast, and haziness due to light absorption, refraction, scattering and restoring these images has warranted much attention. In this work, we present Unsupervised Underwater Neural Radiance Field U2NeRF, a transformer-based architecture that learns to render and restore novel views conditioned on multi-view geometry simultaneously. Due to the absence of supervision, we attempt to implicitly bake restoring capabilities onto the NeRF pipeline and disentangle the predicted color into several components - scene radiance, direct transmission map, backscatter transmission map, and global background light, and when combined reconstruct the underwater image in a self-supervised manner. In addition, we release an Underwater View Synthesis UVS dataset consisting of 12 underwater scenes, containing both synthetically-generated and real-world data. Our experiments demonstrate that when optimized on a single scene, U2NeRF outperforms several baselines by as much LPIPS 11%, UIQM 5%, UCIQE 4% (on average) and showcases improved rendering and restoration capabilities. Code will be made available upon acceptance.","sentences":["Underwater images suffer from colour shifts, low contrast, and haziness due to light absorption, refraction, scattering and restoring these images has warranted much attention.","In this work, we present Unsupervised Underwater Neural Radiance Field U2NeRF, a transformer-based architecture that learns to render and restore novel views conditioned on multi-view geometry simultaneously.","Due to the absence of supervision, we attempt to implicitly bake restoring capabilities onto the NeRF pipeline and disentangle the predicted color into several components - scene radiance, direct transmission map, backscatter transmission map, and global background light, and when combined reconstruct the underwater image in a self-supervised manner.","In addition, we release an Underwater View Synthesis UVS dataset consisting of 12 underwater scenes, containing both synthetically-generated and real-world data.","Our experiments demonstrate that when optimized on a single scene, U2NeRF outperforms several baselines by as much LPIPS 11%, UIQM 5%, UCIQE 4% (on average) and showcases improved rendering and restoration capabilities.","Code will be made available upon acceptance."],"url":"http://arxiv.org/abs/2411.16172v1"}
{"created":"2024-11-25 08:00:21","title":"Image Generation Diversity Issues and How to Tame Them","abstract":"Generative methods now produce outputs nearly indistinguishable from real data but often fail to fully capture the data distribution. Unlike quality issues, diversity limitations in generative models are hard to detect visually, requiring specific metrics for assessment. In this paper, we draw attention to the current lack of diversity in generative models and the inability of common metrics to measure this. We achieve this by framing diversity as an image retrieval problem, where we measure how many real images can be retrieved using synthetic data as queries. This yields the Image Retrieval Score (IRS), an interpretable, hyperparameter-free metric that quantifies the diversity of a generative model's output. IRS requires only a subset of synthetic samples and provides a statistical measure of confidence. Our experiments indicate that current feature extractors commonly used in generative model assessment are inadequate for evaluating diversity effectively. Consequently, we perform an extensive search for the best feature extractors to assess diversity. Evaluation reveals that current diffusion models converge to limited subsets of the real distribution, with no current state-of-the-art models superpassing 77% of the diversity of the training data. To address this limitation, we introduce Diversity-Aware Diffusion Models (DiADM), a novel approach that improves diversity of unconditional diffusion models without loss of image quality. We do this by disentangling diversity from image quality by using a diversity aware module that uses pseudo-unconditional features as input. We provide a Python package offering unified feature extraction and metric computation to further facilitate the evaluation of generative models https://github.com/MischaD/beyondfid.","sentences":["Generative methods now produce outputs nearly indistinguishable from real data but often fail to fully capture the data distribution.","Unlike quality issues, diversity limitations in generative models are hard to detect visually, requiring specific metrics for assessment.","In this paper, we draw attention to the current lack of diversity in generative models and the inability of common metrics to measure this.","We achieve this by framing diversity as an image retrieval problem, where we measure how many real images can be retrieved using synthetic data as queries.","This yields the Image Retrieval Score (IRS), an interpretable, hyperparameter-free metric that quantifies the diversity of a generative model's output.","IRS requires only a subset of synthetic samples and provides a statistical measure of confidence.","Our experiments indicate that current feature extractors commonly used in generative model assessment are inadequate for evaluating diversity effectively.","Consequently, we perform an extensive search for the best feature extractors to assess diversity.","Evaluation reveals that current diffusion models converge to limited subsets of the real distribution, with no current state-of-the-art models superpassing 77% of the diversity of the training data.","To address this limitation, we introduce Diversity-Aware Diffusion Models (DiADM), a novel approach that improves diversity of unconditional diffusion models without loss of image quality.","We do this by disentangling diversity from image quality by using a diversity aware module that uses pseudo-unconditional features as input.","We provide a Python package offering unified feature extraction and metric computation to further facilitate the evaluation of generative models https://github.com/MischaD/beyondfid."],"url":"http://arxiv.org/abs/2411.16171v1"}
{"created":"2024-11-25 07:46:57","title":"BadSFL: Backdoor Attack against Scaffold Federated Learning","abstract":"Federated learning (FL) enables the training of deep learning models on distributed clients to preserve data privacy. However, this learning paradigm is vulnerable to backdoor attacks, where malicious clients can upload poisoned local models to embed backdoors into the global model, leading to attacker-desired predictions. Existing backdoor attacks mainly focus on FL with independently and identically distributed (IID) scenarios, while real-world FL training data are typically non-IID. Current strategies for non-IID backdoor attacks suffer from limitations in maintaining effectiveness and durability. To address these challenges, we propose a novel backdoor attack method, \\name, specifically designed for the FL framework using the scaffold aggregation algorithm in non-IID settings. \\name leverages a Generative Adversarial Network (GAN) based on the global model to complement the training set, achieving high accuracy on both backdoor and benign samples. It utilizes a specific feature as the backdoor trigger to ensure stealthiness, and exploits the Scaffold's control variate to predict the global model's convergence direction, ensuring the backdoor's persistence. Extensive experiments on three benchmark datasets demonstrate the high effectiveness, stealthiness, and durability of \\name. Notably, our attack remains effective over 60 rounds in the global model and up to 3 times longer than existing baseline attacks after stopping the injection of malicious updates.","sentences":["Federated learning (FL) enables the training of deep learning models on distributed clients to preserve data privacy.","However, this learning paradigm is vulnerable to backdoor attacks, where malicious clients can upload poisoned local models to embed backdoors into the global model, leading to attacker-desired predictions.","Existing backdoor attacks mainly focus on FL with independently and identically distributed (IID) scenarios, while real-world FL training data are typically non-IID.","Current strategies for non-IID backdoor attacks suffer from limitations in maintaining effectiveness and durability.","To address these challenges, we propose a novel backdoor attack method, \\name, specifically designed for the FL framework using the scaffold aggregation algorithm in non-IID settings.","\\name leverages a Generative Adversarial Network (GAN) based on the global model to complement the training set, achieving high accuracy on both backdoor and benign samples.","It utilizes a specific feature as the backdoor trigger to ensure stealthiness, and exploits the Scaffold's control variate to predict the global model's convergence direction, ensuring the backdoor's persistence.","Extensive experiments on three benchmark datasets demonstrate the high effectiveness, stealthiness, and durability of \\name.","Notably, our attack remains effective over 60 rounds in the global model and up to 3 times longer than existing baseline attacks after stopping the injection of malicious updates."],"url":"http://arxiv.org/abs/2411.16167v1"}
{"created":"2024-11-25 07:34:23","title":"MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors Enhanced Diffusion Model","abstract":"We introduce MVGenMaster, a multi-view diffusion model enhanced with 3D priors to address versatile Novel View Synthesis (NVS) tasks. MVGenMaster leverages 3D priors that are warped using metric depth and camera poses, significantly enhancing both generalization and 3D consistency in NVS. Our model features a simple yet effective pipeline that can generate up to 100 novel views conditioned on arbitrary reference views and camera poses with a single forward process. Additionally, we have developed a comprehensive large-scale multi-view image dataset comprising up to 1.2 million scenes, equipped with well-aligned metric depth. Moreover, we present several training and model modifications to strengthen the model with scaled-up datasets. Extensive evaluations across in- and out-of-domain benchmarks demonstrate the effectiveness of our proposed method and data formulation. Models and codes will be released at https://github.com/ewrfcas/MVGenMaster/.","sentences":["We introduce MVGenMaster, a multi-view diffusion model enhanced with 3D priors to address versatile Novel View Synthesis (NVS) tasks.","MVGenMaster leverages 3D priors that are warped using metric depth and camera poses, significantly enhancing both generalization and 3D consistency in NVS.","Our model features a simple yet effective pipeline that can generate up to 100 novel views conditioned on arbitrary reference views and camera poses with a single forward process.","Additionally, we have developed a comprehensive large-scale multi-view image dataset comprising up to 1.2 million scenes, equipped with well-aligned metric depth.","Moreover, we present several training and model modifications to strengthen the model with scaled-up datasets.","Extensive evaluations across in- and out-of-domain benchmarks demonstrate the effectiveness of our proposed method and data formulation.","Models and codes will be released at https://github.com/ewrfcas/MVGenMaster/."],"url":"http://arxiv.org/abs/2411.16157v1"}
{"created":"2024-11-25 07:32:02","title":"VideoOrion: Tokenizing Object Dynamics in Videos","abstract":"We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos--the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.","sentences":["We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos--the spatial-temporal dynamics of objects throughout the videos.","VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features.","Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs.","Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost.","Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks.","Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks."],"url":"http://arxiv.org/abs/2411.16156v1"}
{"created":"2024-11-25 07:30:52","title":"Graph Adapter of EEG Foundation Models for Parameter Efficient Fine Tuning","abstract":"In diagnosing mental diseases from electroencephalography (EEG) data, neural network models such as Transformers have been employed to capture temporal dynamics. Additionally, it is crucial to learn the spatial relationships between EEG sensors, for which Graph Neural Networks (GNNs) are commonly used. However, fine-tuning large-scale complex neural network models simultaneously to capture both temporal and spatial features increases computational costs due to the more significant number of trainable parameters. It causes the limited availability of EEG datasets for downstream tasks, making it challenging to fine-tune large models effectively. We propose EEG-GraphAdapter (EGA), a parameter-efficient fine-tuning (PEFT) approach to address these challenges. EGA is integrated into pre-trained temporal backbone models as a GNN-based module and fine-tuned itself alone while keeping the backbone model parameters frozen. This enables the acquisition of spatial representations of EEG signals for downstream tasks, significantly reducing computational overhead and data requirements. Experimental evaluations on healthcare-related downstream tasks of Major Depressive Disorder and Abnormality Detection demonstrate that our EGA improves performance by up to 16.1% in the F1-score compared with the backbone BENDR model.","sentences":["In diagnosing mental diseases from electroencephalography (EEG) data, neural network models such as Transformers have been employed to capture temporal dynamics.","Additionally, it is crucial to learn the spatial relationships between EEG sensors, for which Graph Neural Networks (GNNs) are commonly used.","However, fine-tuning large-scale complex neural network models simultaneously to capture both temporal and spatial features increases computational costs due to the more significant number of trainable parameters.","It causes the limited availability of EEG datasets for downstream tasks, making it challenging to fine-tune large models effectively.","We propose EEG-GraphAdapter (EGA), a parameter-efficient fine-tuning (PEFT) approach to address these challenges.","EGA is integrated into pre-trained temporal backbone models as a GNN-based module and fine-tuned itself alone while keeping the backbone model parameters frozen.","This enables the acquisition of spatial representations of EEG signals for downstream tasks, significantly reducing computational overhead and data requirements.","Experimental evaluations on healthcare-related downstream tasks of Major Depressive Disorder and Abnormality Detection demonstrate that our EGA improves performance by up to 16.1% in the F1-score compared with the backbone BENDR model."],"url":"http://arxiv.org/abs/2411.16155v1"}
{"created":"2024-11-25 07:26:22","title":"DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders","abstract":"Self-supervised learning (SSL) is pervasively exploited in training high-quality upstream encoders with a large amount of unlabeled data. However, it is found to be susceptible to backdoor attacks merely via polluting a small portion of training data. The victim encoders mismatch triggered inputs with target embeddings, e.g., match the triggered cat input to an airplane embedding, such that the downstream tasks are affected to misbehave when the trigger is activated. Emerging backdoor attacks have shown great threats in different SSL paradigms such as contrastive learning and CLIP, while few research is devoted to defending against such attacks. Besides, the existing ones fall short in detecting advanced stealthy backdoors. To address the limitations, we propose a novel detection mechanism, DeDe, which detects the activation of the backdoor mapping with the cooccurrence of victim encoder and trigger inputs. Specifically, DeDe trains a decoder for the SSL encoder on an auxiliary dataset (can be out-of-distribution or even slightly poisoned), such that for any triggered input that misleads to the target embedding, the decoder outputs an image significantly different from the input. We empirically evaluate DeDe on both contrastive learning and CLIP models against various types of backdoor attacks, and demonstrate its superior performance over SOTA detection methods in both upstream detection performance and ability of preventing backdoors in downstream tasks.","sentences":["Self-supervised learning (SSL) is pervasively exploited in training high-quality upstream encoders with a large amount of unlabeled data.","However, it is found to be susceptible to backdoor attacks merely via polluting a small portion of training data.","The victim encoders mismatch triggered inputs with target embeddings, e.g., match the triggered cat input to an airplane embedding, such that the downstream tasks are affected to misbehave when the trigger is activated.","Emerging backdoor attacks have shown great threats in different SSL paradigms such as contrastive learning and CLIP, while few research is devoted to defending against such attacks.","Besides, the existing ones fall short in detecting advanced stealthy backdoors.","To address the limitations, we propose a novel detection mechanism, DeDe, which detects the activation of the backdoor mapping with the cooccurrence of victim encoder and trigger inputs.","Specifically, DeDe trains a decoder for the SSL encoder on an auxiliary dataset (can be out-of-distribution or even slightly poisoned), such that for any triggered input that misleads to the target embedding, the decoder outputs an image significantly different from the input.","We empirically evaluate DeDe on both contrastive learning and CLIP models against various types of backdoor attacks, and demonstrate its superior performance over SOTA detection methods in both upstream detection performance and ability of preventing backdoors in downstream tasks."],"url":"http://arxiv.org/abs/2411.16154v1"}
{"created":"2024-11-25 07:22:20","title":"Optimizing Winograd Convolution on ARMv8 manycore processors","abstract":"As Convolutional Neural Networks (CNNs) become increasingly prevalent in deep learning applications, numerous algorithms, such as the Winograd algorithm, have been proposed to enhance their efficiency. However, existing implementations of Winograd Convolution based on General Matrix Multiplication (GEMM) exhibit certain limitations: the transformation tasks take up a significant portion of the process, computation efficiency is suboptimal, and a single parallel strategy leads to reduced parallel efficiency for certain layers. In this article, we present a novel fused Winograd Convolution algorithm specifically optimized for the three stages of Winograd Convolution - input and filter transformation, computation, and output transformation - carefully tailored for ARMv8 manycore CPUs. Our method maintains consecutive memory access as much as possible during the transformation stage and integrates data packing into a z-shape customized data layout, which is conducive for our meticulously optimized GEMM micro-kernel using a ping-pong technique. Moreover, we introduce a three-mode parallel strategy that adaptively switches based on the scale of convolutional layers, addressing the shortcomings of current methodologies. By manually optimizing each kernel at the assembly level and thoroughly analyzing the blocking parameters, we significantly reduce transformation time and enhance computational efficiency compared to state-of-the-art libraries. Experimental results demonstrate that our method achieves up to 2.35x and 2.39x speedup for single-thread execution and 1.66x and 2.06x geometric mean speedup for multi-thread execution compared to NCNN and NNPACK on the Kunpeng 920.","sentences":["As Convolutional Neural Networks (CNNs) become increasingly prevalent in deep learning applications, numerous algorithms, such as the Winograd algorithm, have been proposed to enhance their efficiency.","However, existing implementations of Winograd Convolution based on General Matrix Multiplication (GEMM) exhibit certain limitations: the transformation tasks take up a significant portion of the process, computation efficiency is suboptimal, and a single parallel strategy leads to reduced parallel efficiency for certain layers.","In this article, we present a novel fused Winograd Convolution algorithm specifically optimized for the three stages of Winograd Convolution - input and filter transformation, computation, and output transformation - carefully tailored for ARMv8 manycore CPUs.","Our method maintains consecutive memory access as much as possible during the transformation stage and integrates data packing into a z-shape customized data layout, which is conducive for our meticulously optimized GEMM micro-kernel using a ping-pong technique.","Moreover, we introduce a three-mode parallel strategy that adaptively switches based on the scale of convolutional layers, addressing the shortcomings of current methodologies.","By manually optimizing each kernel at the assembly level and thoroughly analyzing the blocking parameters, we significantly reduce transformation time and enhance computational efficiency compared to state-of-the-art libraries.","Experimental results demonstrate that our method achieves up to 2.35x and 2.39x speedup for single-thread execution and 1.66x and 2.06x geometric mean speedup for multi-thread execution compared to NCNN and NNPACK on the Kunpeng 920."],"url":"http://arxiv.org/abs/2411.16152v1"}
{"created":"2024-11-25 07:19:06","title":"Directed Token Sliding","abstract":"Reconfiguration problems involve determining whether two given configurations can be transformed into each other under specific rules. The Token Sliding problem asks whether, given two different set of tokens on vertices of a graph $G$, we can transform one into the other by sliding tokens step-by-step along edges of $G$ such that each resulting set of tokens forms an independent set in $G$. Recently, Ito et al. [MFCS 2022] introduced a directed variant of this problem. They showed that for general oriented graphs (i.e., graphs where no pair of vertices can have directed edges in both directions), the problem remains $\\mathsf{PSPACE}$-complete, and is solvable in polynomial time on oriented trees.   In this paper, we further investigate the Token Sliding problem on various oriented graph classes. We show that the problem remains $\\mathsf{PSPACE}$-complete for oriented planar graphs, split graphs, bipartite graphs and bounded treewidth graphs. Additionally, we present polynomial-time algorithms for solving the problem on oriented cycles and cographs.","sentences":["Reconfiguration problems involve determining whether two given configurations can be transformed into each other under specific rules.","The Token Sliding problem asks whether, given two different set of tokens on vertices of a graph $G$, we can transform one into the other by sliding tokens step-by-step along edges of $G$ such that each resulting set of tokens forms an independent set in $G$. Recently, Ito et al.","[MFCS 2022] introduced a directed variant of this problem.","They showed that for general oriented graphs (i.e., graphs where no pair of vertices can have directed edges in both directions), the problem remains $\\mathsf{PSPACE}$-complete, and is solvable in polynomial time on oriented trees.   ","In this paper, we further investigate the Token Sliding problem on various oriented graph classes.","We show that the problem remains $\\mathsf{PSPACE}$-complete for oriented planar graphs, split graphs, bipartite graphs and bounded treewidth graphs.","Additionally, we present polynomial-time algorithms for solving the problem on oriented cycles and cographs."],"url":"http://arxiv.org/abs/2411.16149v1"}
{"created":"2024-11-25 07:11:45","title":"Local Intrinsic Dimensionality for Dynamic Graph Embeddings","abstract":"The notion of local intrinsic dimensionality (LID) has important theoretical implications and practical applications in the fields of data mining and machine learning. Recent research efforts indicate that LID measures defined for graphs can improve graph representational learning methods based on random walks. In this paper, we discuss how NC-LID, a LID measure designed for static graphs, can be adapted for dynamic networks. Focusing on dynnode2vec as the most representative dynamic graph embedding method based on random walks, we examine correlations between NC-LID and the intrinsic quality of 10 real-world dynamic network embeddings. The obtained results show that NC-LID can be used as a good indicator of nodes whose embedding vectors do not tend to preserve temporal graph structure well. Thus, our empirical findings constitute the first step towards LID-aware dynamic graph embedding methods.","sentences":["The notion of local intrinsic dimensionality (LID) has important theoretical implications and practical applications in the fields of data mining and machine learning.","Recent research efforts indicate that LID measures defined for graphs can improve graph representational learning methods based on random walks.","In this paper, we discuss how NC-LID, a LID measure designed for static graphs, can be adapted for dynamic networks.","Focusing on dynnode2vec as the most representative dynamic graph embedding method based on random walks, we examine correlations between NC-LID and the intrinsic quality of 10 real-world dynamic network embeddings.","The obtained results show that NC-LID can be used as a good indicator of nodes whose embedding vectors do not tend to preserve temporal graph structure well.","Thus, our empirical findings constitute the first step towards LID-aware dynamic graph embedding methods."],"url":"http://arxiv.org/abs/2411.16145v1"}
{"created":"2024-11-25 07:10:03","title":"Using Drone Swarm to Stop Wildfire: A Predict-then-optimize Approach","abstract":"Drone swarms coupled with data intelligence can be the future of wildfire fighting. However, drone swarm firefighting faces enormous challenges, such as the highly complex environmental conditions in wildfire scenes, the highly dynamic nature of wildfire spread, and the significant computational complexity of drone swarm operations. We develop a predict-then-optimize approach to address these challenges to enable effective drone swarm firefighting. First, we construct wildfire spread prediction convex neural network (Convex-NN) models based on real wildfire data. Then, we propose a mixed-integer programming (MIP) model coupled with dynamic programming (DP) to enable efficient drone swarm task planning. We further use chance-constrained robust optimization (CCRO) to ensure robust firefighting performances under varying situations. The formulated model is solved efficiently using Benders Decomposition and Branch-and-Cut algorithms. After 75 simulated wildfire environments training, the MIP+CCRO approach shows the best performance among several testing sets, reducing movements by 37.3\\% compared to the plain MIP. It also significantly outperformed the GA baseline, which often failed to fully extinguish the fire. Eventually, we will conduct real-world fire spread and quenching experiments in the next stage for further validation.","sentences":["Drone swarms coupled with data intelligence can be the future of wildfire fighting.","However, drone swarm firefighting faces enormous challenges, such as the highly complex environmental conditions in wildfire scenes, the highly dynamic nature of wildfire spread, and the significant computational complexity of drone swarm operations.","We develop a predict-then-optimize approach to address these challenges to enable effective drone swarm firefighting.","First, we construct wildfire spread prediction convex neural network (Convex-NN) models based on real wildfire data.","Then, we propose a mixed-integer programming (MIP) model coupled with dynamic programming (DP) to enable efficient drone swarm task planning.","We further use chance-constrained robust optimization (CCRO) to ensure robust firefighting performances under varying situations.","The formulated model is solved efficiently using Benders Decomposition and Branch-and-Cut algorithms.","After 75 simulated wildfire environments training, the MIP+CCRO approach shows the best performance among several testing sets, reducing movements by 37.3\\% compared to the plain MIP.","It also significantly outperformed the GA baseline, which often failed to fully extinguish the fire.","Eventually, we will conduct real-world fire spread and quenching experiments in the next stage for further validation."],"url":"http://arxiv.org/abs/2411.16144v1"}
{"created":"2024-11-25 07:05:27","title":"Causal Adjacency Learning for Spatiotemporal Prediction Over Graphs","abstract":"Spatiotemporal prediction over graphs (STPG) is crucial for transportation systems. In existing STPG models, an adjacency matrix is an important component that captures the relations among nodes over graphs. However, most studies calculate the adjacency matrix by directly memorizing the data, such as distance- and correlation-based matrices. These adjacency matrices do not consider potential pattern shift for the test data, and may result in suboptimal performance if the test data has a different distribution from the training one. This issue is known as the Out-of-Distribution generalization problem. To address this issue, in this paper we propose a Causal Adjacency Learning (CAL) method to discover causal relations over graphs. The learned causal adjacency matrix is evaluated on a downstream spatiotemporal prediction task using real-world graph data. Results demonstrate that our proposed adjacency matrix can capture the causal relations, and using our learned adjacency matrix can enhance prediction performance on the OOD test data, even though causal learning is not conducted in the downstream task.","sentences":["Spatiotemporal prediction over graphs (STPG) is crucial for transportation systems.","In existing STPG models, an adjacency matrix is an important component that captures the relations among nodes over graphs.","However, most studies calculate the adjacency matrix by directly memorizing the data, such as distance- and correlation-based matrices.","These adjacency matrices do not consider potential pattern shift for the test data, and may result in suboptimal performance if the test data has a different distribution from the training one.","This issue is known as the Out-of-Distribution generalization problem.","To address this issue, in this paper we propose a Causal Adjacency Learning (CAL) method to discover causal relations over graphs.","The learned causal adjacency matrix is evaluated on a downstream spatiotemporal prediction task using real-world graph data.","Results demonstrate that our proposed adjacency matrix can capture the causal relations, and using our learned adjacency matrix can enhance prediction performance on the OOD test data, even though causal learning is not conducted in the downstream task."],"url":"http://arxiv.org/abs/2411.16142v1"}
{"created":"2024-11-25 06:48:38","title":"Context Awareness Gate For Retrieval Augmented Generation","abstract":"Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach to mitigate the limitations of large language models (LLMs) in answering domain-specific questions. Previous research has predominantly focused on improving the accuracy and quality of retrieved data chunks to enhance the overall performance of the generation pipeline. However, despite ongoing advancements, the critical issue of retrieving irrelevant information -- which can impair the ability of the model to utilize its internal knowledge effectively -- has received minimal attention. In this work, we investigate the impact of retrieving irrelevant information in open-domain question answering, highlighting its significant detrimental effect on the quality of LLM outputs. To address this challenge, we propose the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically adjusts the LLMs' input prompt based on whether the user query necessitates external context retrieval. Additionally, we introduce the Vector Candidates method, a core mathematical component of CAG that is statistical, LLM-independent, and highly scalable. We further examine the distributions of relationships between contexts and questions, presenting a statistical analysis of these distributions. This analysis can be leveraged to enhance the context retrieval process in Retrieval Augmented Generation (RAG) systems.","sentences":["Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach to mitigate the limitations of large language models (LLMs) in answering domain-specific questions.","Previous research has predominantly focused on improving the accuracy and quality of retrieved data chunks to enhance the overall performance of the generation pipeline.","However, despite ongoing advancements, the critical issue of retrieving irrelevant information -- which can impair the ability of the model to utilize its internal knowledge effectively -- has received minimal attention.","In this work, we investigate the impact of retrieving irrelevant information in open-domain question answering, highlighting its significant detrimental effect on the quality of LLM outputs.","To address this challenge, we propose the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically adjusts the LLMs' input prompt based on whether the user query necessitates external context retrieval.","Additionally, we introduce the Vector Candidates method, a core mathematical component of CAG that is statistical, LLM-independent, and highly scalable.","We further examine the distributions of relationships between contexts and questions, presenting a statistical analysis of these distributions.","This analysis can be leveraged to enhance the context retrieval process in Retrieval Augmented Generation (RAG) systems."],"url":"http://arxiv.org/abs/2411.16133v1"}
{"created":"2024-11-25 06:37:48","title":"End-to-End Steering for Autonomous Vehicles via Conditional Imitation Co-Learning","abstract":"Autonomous driving involves complex tasks such as data fusion, object and lane detection, behavior prediction, and path planning. As opposed to the modular approach which dedicates individual subsystems to tackle each of those tasks, the end-to-end approach treats the problem as a single learnable task using deep neural networks, reducing system complexity and minimizing dependency on heuristics. Conditional imitation learning (CIL) trains the end-to-end model to mimic a human expert considering the navigational commands guiding the vehicle to reach its destination, CIL adopts specialist network branches dedicated to learn the driving task for each navigational command. Nevertheless, the CIL model lacked generalization when deployed to unseen environments. This work introduces the conditional imitation co-learning (CIC) approach to address this issue by enabling the model to learn the relationships between CIL specialist branches via a co-learning matrix generated by gated hyperbolic tangent units (GTUs). Additionally, we propose posing the steering regression problem as classification, we use a classification-regression hybrid loss to bridge the gap between regression and classification, we also propose using co-existence probability to consider the spatial tendency between the steering classes. Our model is demonstrated to improve autonomous driving success rate in unseen environment by 62% on average compared to the CIL method.","sentences":["Autonomous driving involves complex tasks such as data fusion, object and lane detection, behavior prediction, and path planning.","As opposed to the modular approach which dedicates individual subsystems to tackle each of those tasks, the end-to-end approach treats the problem as a single learnable task using deep neural networks, reducing system complexity and minimizing dependency on heuristics.","Conditional imitation learning (CIL) trains the end-to-end model to mimic a human expert considering the navigational commands guiding the vehicle to reach its destination, CIL adopts specialist network branches dedicated to learn the driving task for each navigational command.","Nevertheless, the CIL model lacked generalization when deployed to unseen environments.","This work introduces the conditional imitation co-learning (CIC) approach to address this issue by enabling the model to learn the relationships between CIL specialist branches via a co-learning matrix generated by gated hyperbolic tangent units (GTUs).","Additionally, we propose posing the steering regression problem as classification, we use a classification-regression hybrid loss to bridge the gap between regression and classification, we also propose using co-existence probability to consider the spatial tendency between the steering classes.","Our model is demonstrated to improve autonomous driving success rate in unseen environment by 62% on average compared to the CIL method."],"url":"http://arxiv.org/abs/2411.16131v1"}
{"created":"2024-11-25 06:29:51","title":"CIA: Controllable Image Augmentation Framework Based on Stable Diffusion","abstract":"Computer vision tasks such as object detection and segmentation rely on the availability of extensive, accurately annotated datasets. In this work, We present CIA, a modular pipeline, for (1) generating synthetic images for dataset augmentation using Stable Diffusion, (2) filtering out low quality samples using defined quality metrics, (3) forcing the existence of specific patterns in generated images using accurate prompting and ControlNet. In order to show how CIA can be used to search for an optimal augmentation pipeline of training data, we study human object detection in a data constrained scenario, using YOLOv8n on COCO and Flickr30k datasets. We have recorded significant improvement using CIA-generated images, approaching the performances obtained when doubling the amount of real images in the dataset. Our findings suggest that our modular framework can significantly enhance object detection systems, and make it possible for future research to be done on data-constrained scenarios. The framework is available at: github.com/multitel-ai/CIA.","sentences":["Computer vision tasks such as object detection and segmentation rely on the availability of extensive, accurately annotated datasets.","In this work, We present CIA, a modular pipeline, for (1) generating synthetic images for dataset augmentation using Stable Diffusion, (2) filtering out low quality samples using defined quality metrics, (3) forcing the existence of specific patterns in generated images using accurate prompting and ControlNet.","In order to show how CIA can be used to search for an optimal augmentation pipeline of training data, we study human object detection in a data constrained scenario, using YOLOv8n on COCO and Flickr30k datasets.","We have recorded significant improvement using CIA-generated images, approaching the performances obtained when doubling the amount of real images in the dataset.","Our findings suggest that our modular framework can significantly enhance object detection systems, and make it possible for future research to be done on data-constrained scenarios.","The framework is available at: github.com/multitel-ai/CIA."],"url":"http://arxiv.org/abs/2411.16128v1"}
{"created":"2024-11-25 06:26:58","title":"DF-GNN: Dynamic Fusion Framework for Attention Graph Neural Networks on GPUs","abstract":"Attention Graph Neural Networks (AT-GNNs), such as GAT and Graph Transformer, have demonstrated superior performance compared to other GNNs. However, existing GNN systems struggle to efficiently train AT-GNNs on GPUs due to their intricate computation patterns. The execution of AT-GNN operations without kernel fusion results in heavy data movement and significant kernel launch overhead, while fixed thread scheduling in existing GNN kernel fusion strategies leads to sub-optimal performance, redundant computation and unbalanced workload. To address these challenges, we propose a dynamic kernel fusion framework, DF-GNN, for the AT-GNN family. DF-GNN introduces a dynamic bi-level thread scheduling strategy, enabling flexible adjustments to thread scheduling while retaining the benefits of shared memory within the fused kernel. DF-GNN tailors specific thread scheduling for operations in AT-GNNs and considers the performance bottleneck shift caused by the presence of super nodes. Additionally, DF-GNN is integrated with the PyTorch framework for high programmability. Evaluations across diverse GNN models and multiple datasets reveal that DF-GNN surpasses existing GNN kernel optimization works like cuGraph and dgNN, with speedups up to $7.0\\times$ over the state-of-the-art non-fusion DGL sparse library. Moreover, it achieves an average speedup of $2.16\\times$ in end-to-end training compared to the popular GNN computing framework DGL.","sentences":["Attention Graph Neural Networks (AT-GNNs), such as GAT and Graph Transformer, have demonstrated superior performance compared to other GNNs.","However, existing GNN systems struggle to efficiently train AT-GNNs on GPUs due to their intricate computation patterns.","The execution of AT-GNN operations without kernel fusion results in heavy data movement and significant kernel launch overhead, while fixed thread scheduling in existing GNN kernel fusion strategies leads to sub-optimal performance, redundant computation and unbalanced workload.","To address these challenges, we propose a dynamic kernel fusion framework, DF-GNN, for the AT-GNN family.","DF-GNN introduces a dynamic bi-level thread scheduling strategy, enabling flexible adjustments to thread scheduling while retaining the benefits of shared memory within the fused kernel.","DF-GNN tailors specific thread scheduling for operations in AT-GNNs and considers the performance bottleneck shift caused by the presence of super nodes.","Additionally, DF-GNN is integrated with the PyTorch framework for high programmability.","Evaluations across diverse GNN models and multiple datasets reveal that DF-GNN surpasses existing GNN kernel optimization works like cuGraph and dgNN, with speedups up to $7.0\\times$ over the state-of-the-art non-fusion DGL sparse library.","Moreover, it achieves an average speedup of $2.16\\times$ in end-to-end training compared to the popular GNN computing framework DGL."],"url":"http://arxiv.org/abs/2411.16127v1"}
{"created":"2024-11-25 06:00:42","title":"LLM Augmentations to support Analytical Reasoning over Multiple Documents","abstract":"Building on their demonstrated ability to perform a variety of tasks, we investigate the application of large language models (LLMs) to enhance in-depth analytical reasoning within the context of intelligence analysis. Intelligence analysts typically work with massive dossiers to draw connections between seemingly unrelated entities, and uncover adversaries' plans and motives. We explore if and how LLMs can be helpful to analysts for this task and develop an architecture to augment the capabilities of an LLM with a memory module called dynamic evidence trees (DETs) to develop and track multiple investigation threads. Through extensive experiments on multiple datasets, we highlight how LLMs, as-is, are still inadequate to support intelligence analysts and offer recommendations to improve LLMs for such intricate reasoning applications.","sentences":["Building on their demonstrated ability to perform a variety of tasks, we investigate the application of large language models (LLMs) to enhance in-depth analytical reasoning within the context of intelligence analysis.","Intelligence analysts typically work with massive dossiers to draw connections between seemingly unrelated entities, and uncover adversaries' plans and motives.","We explore if and how LLMs can be helpful to analysts for this task and develop an architecture to augment the capabilities of an LLM with a memory module called dynamic evidence trees (DETs) to develop and track multiple investigation threads.","Through extensive experiments on multiple datasets, we highlight how LLMs, as-is, are still inadequate to support intelligence analysts and offer recommendations to improve LLMs for such intricate reasoning applications."],"url":"http://arxiv.org/abs/2411.16116v1"}
{"created":"2024-11-25 05:51:38","title":"FUN-AD: Fully Unsupervised Learning for Anomaly Detection with Noisy Training Data","abstract":"While the mainstream research in anomaly detection has mainly followed the one-class classification, practical industrial environments often incur noisy training data due to annotation errors or lack of labels for new or refurbished products. To address these issues, we propose a novel learning-based approach for fully unsupervised anomaly detection with unlabeled and potentially contaminated training data. Our method is motivated by two observations, that i) the pairwise feature distances between the normal samples are on average likely to be smaller than those between the anomaly samples or heterogeneous samples and ii) pairs of features mutually closest to each other are likely to be homogeneous pairs, which hold if the normal data has smaller variance than the anomaly data. Building on the first observation that nearest-neighbor distances can distinguish between confident normal samples and anomalies, we propose a pseudo-labeling strategy using an iteratively reconstructed memory bank (IRMB). The second observation is utilized as a new loss function to promote class-homogeneity between mutually closest pairs thereby reducing the ill-posedness of the task. Experimental results on two public industrial anomaly benchmarks and semantic anomaly examples validate the effectiveness of FUN-AD across different scenarios and anomaly-to-normal ratios. Our code is available at https://github.com/HY-Vision-Lab/FUNAD.","sentences":["While the mainstream research in anomaly detection has mainly followed the one-class classification, practical industrial environments often incur noisy training data due to annotation errors or lack of labels for new or refurbished products.","To address these issues, we propose a novel learning-based approach for fully unsupervised anomaly detection with unlabeled and potentially contaminated training data.","Our method is motivated by two observations, that i) the pairwise feature distances between the normal samples are on average likely to be smaller than those between the anomaly samples or heterogeneous samples and ii) pairs of features mutually closest to each other are likely to be homogeneous pairs, which hold if the normal data has smaller variance than the anomaly data.","Building on the first observation that nearest-neighbor distances can distinguish between confident normal samples and anomalies, we propose a pseudo-labeling strategy using an iteratively reconstructed memory bank (IRMB).","The second observation is utilized as a new loss function to promote class-homogeneity between mutually closest pairs thereby reducing the ill-posedness of the task.","Experimental results on two public industrial anomaly benchmarks and semantic anomaly examples validate the effectiveness of FUN-AD across different scenarios and anomaly-to-normal ratios.","Our code is available at https://github.com/HY-Vision-Lab/FUNAD."],"url":"http://arxiv.org/abs/2411.16110v1"}
{"created":"2024-11-25 05:42:15","title":"Forest Biomass Mapping with Terrestrial Hyperspectral Imaging for Wildfire Risk Monitoring","abstract":"With the rapid increase in wildfires in the past decade, it has become necessary to detect and predict these disasters to mitigate losses to ecosystems and human lives. In this paper, we present a novel solution -- Hyper-Drive3D -- consisting of snapshot hyperspectral imaging and LiDAR, mounted on an Unmanned Ground Vehicle (UGV) that identifies areas inside forests at risk of becoming fuel for a forest fire. This system enables more accurate classification by analyzing the spectral signatures of forest vegetation. We conducted field trials in a controlled environment simulating forest conditions, yielding valuable insights into the system's effectiveness. Extensive data collection was also performed in a dense forest across varying environmental conditions and topographies to enhance the system's predictive capabilities for fire hazards and support a risk-informed, proactive forest management strategy. Additionally, we propose a framework for extracting moisture data from hyperspectral imagery and projecting it into 3D space.","sentences":["With the rapid increase in wildfires in the past decade, it has become necessary to detect and predict these disasters to mitigate losses to ecosystems and human lives.","In this paper, we present a novel solution -- Hyper-Drive3D -- consisting of snapshot hyperspectral imaging and LiDAR, mounted on an Unmanned Ground Vehicle (UGV) that identifies areas inside forests at risk of becoming fuel for a forest fire.","This system enables more accurate classification by analyzing the spectral signatures of forest vegetation.","We conducted field trials in a controlled environment simulating forest conditions, yielding valuable insights into the system's effectiveness.","Extensive data collection was also performed in a dense forest across varying environmental conditions and topographies to enhance the system's predictive capabilities for fire hazards and support a risk-informed, proactive forest management strategy.","Additionally, we propose a framework for extracting moisture data from hyperspectral imagery and projecting it into 3D space."],"url":"http://arxiv.org/abs/2411.16107v1"}
{"created":"2024-11-25 05:21:12","title":"An Empirical Study of Vulnerability Detection using Federated Learning","abstract":"Although Deep Learning (DL) methods becoming increasingly popular in vulnerability detection, their performance is seriously limited by insufficient training data. This is mainly because few existing software organizations can maintain a complete set of high-quality samples for DL-based vulnerability detection. Due to the concerns about privacy leakage, most of them are reluctant to share data, resulting in the data silo problem. Since enables collaboratively model training without data sharing, Federated Learning (FL) has been investigated as a promising means of addressing the data silo problem in DL-based vulnerability detection. However, since existing FL-based vulnerability detection methods focus on specific applications, it is still far unclear i) how well FL adapts to common vulnerability detection tasks and ii) how to design a high-performance FL solution for a specific vulnerability detection task. To answer these two questions, this paper first proposes VulFL, an effective evaluation framework for FL-based vulnerability detection. Then, based on VulFL, this paper conducts a comprehensive study to reveal the underlying capabilities of FL in dealing with different types of CWEs, especially when facing various data heterogeneity scenarios. Our experimental results show that, compared to independent training, FL can significantly improve the detection performance of common AI models on all investigated CWEs, though the performance of FL-based vulnerability detection is limited by heterogeneous data. To highlight the performance differences between different FL solutions for vulnerability detection, we extensively investigate the impacts of different configuration strategies for each framework component of VulFL. Our study sheds light on the potential of FL in vulnerability detection, which can be used to guide the design of FL-based solutions for vulnerability detection.","sentences":["Although Deep Learning (DL) methods becoming increasingly popular in vulnerability detection, their performance is seriously limited by insufficient training data.","This is mainly because few existing software organizations can maintain a complete set of high-quality samples for DL-based vulnerability detection.","Due to the concerns about privacy leakage, most of them are reluctant to share data, resulting in the data silo problem.","Since enables collaboratively model training without data sharing, Federated Learning (FL) has been investigated as a promising means of addressing the data silo problem in DL-based vulnerability detection.","However, since existing FL-based vulnerability detection methods focus on specific applications, it is still far unclear i) how well FL adapts to common vulnerability detection tasks and ii) how to design a high-performance FL solution for a specific vulnerability detection task.","To answer these two questions, this paper first proposes VulFL, an effective evaluation framework for FL-based vulnerability detection.","Then, based on VulFL, this paper conducts a comprehensive study to reveal the underlying capabilities of FL in dealing with different types of CWEs, especially when facing various data heterogeneity scenarios.","Our experimental results show that, compared to independent training, FL can significantly improve the detection performance of common AI models on all investigated CWEs, though the performance of FL-based vulnerability detection is limited by heterogeneous data.","To highlight the performance differences between different FL solutions for vulnerability detection, we extensively investigate the impacts of different configuration strategies for each framework component of VulFL.","Our study sheds light on the potential of FL in vulnerability detection, which can be used to guide the design of FL-based solutions for vulnerability detection."],"url":"http://arxiv.org/abs/2411.16099v1"}
{"created":"2024-11-25 05:15:38","title":"ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality Images","abstract":"Multimodal search has revolutionized the fashion industry, providing a seamless and intuitive way for users to discover and explore fashion items. Based on their preferences, style, or specific attributes, users can search for products by combining text and image information. Text-to-image searches enable users to find visually similar items or describe products using natural language. This paper presents an innovative approach called ENCLIP, for enhancing the performance of the Contrastive Language-Image Pretraining (CLIP) model, specifically in Multimodal Search targeted towards the domain of fashion intelligence. This method focuses on addressing the challenges posed by limited data availability and low-quality images. This paper proposes an algorithm that involves training and ensembling multiple instances of the CLIP model, and leveraging clustering techniques to group similar images together. The experimental findings presented in this study provide evidence of the effectiveness of the methodology. This approach unlocks the potential of CLIP in the domain of fashion intelligence, where data scarcity and image quality issues are prevalent. Overall, the ENCLIP method represents a valuable contribution to the field of fashion intelligence and provides a practical solution for optimizing the CLIP model in scenarios with limited data and low-quality images.","sentences":["Multimodal search has revolutionized the fashion industry, providing a seamless and intuitive way for users to discover and explore fashion items.","Based on their preferences, style, or specific attributes, users can search for products by combining text and image information.","Text-to-image searches enable users to find visually similar items or describe products using natural language.","This paper presents an innovative approach called ENCLIP, for enhancing the performance of the Contrastive Language-Image Pretraining (CLIP) model, specifically in Multimodal Search targeted towards the domain of fashion intelligence.","This method focuses on addressing the challenges posed by limited data availability and low-quality images.","This paper proposes an algorithm that involves training and ensembling multiple instances of the CLIP model, and leveraging clustering techniques to group similar images together.","The experimental findings presented in this study provide evidence of the effectiveness of the methodology.","This approach unlocks the potential of CLIP in the domain of fashion intelligence, where data scarcity and image quality issues are prevalent.","Overall, the ENCLIP method represents a valuable contribution to the field of fashion intelligence and provides a practical solution for optimizing the CLIP model in scenarios with limited data and low-quality images."],"url":"http://arxiv.org/abs/2411.16096v1"}
{"created":"2024-11-25 05:07:00","title":"LDACP: Long-Delayed Ad Conversions Prediction Model for Bidding Strategy","abstract":"In online advertising, once an ad campaign is deployed, the automated bidding system dynamically adjusts the bidding strategy to optimize Cost Per Action (CPA) based on the number of ad conversions. For ads with a long conversion delay, relying solely on the real-time tracked conversion number as a signal for bidding strategy can significantly overestimate the current CPA, leading to conservative bidding strategies. Therefore, it is crucial to predict the number of long-delayed conversions. Nonetheless, it is challenging to predict ad conversion numbers through traditional regression methods due to the wide range of ad conversion numbers. Previous regression works have addressed this challenge by transforming regression problems into bucket classification problems, achieving success in various scenarios. However, specific challenges arise when predicting the number of ad conversions: 1) The integer nature of ad conversion numbers exacerbates the discontinuity issue in one-hot hard labels; 2) The long-tail distribution of ad conversion numbers complicates tail data prediction. In this paper, we propose the Long-Delayed Ad Conversions Prediction model for bidding strategy (LDACP), which consists of two sub-modules. To alleviate the issue of discontinuity in one-hot hard labels, the Bucket Classification Module with label Smoothing method (BCMS) converts one-hot hard labels into non-normalized soft labels, then fits these soft labels by minimizing classification loss and regression loss. To address the challenge of predicting tail data, the Value Regression Module with Proxy labels (VRMP) uses the prediction bias of aggregated pCTCVR as proxy labels. Finally, a Mixture of Experts (MoE) structure integrates the predictions from BCMS and VRMP to obtain the final predicted ad conversion number.","sentences":["In online advertising, once an ad campaign is deployed, the automated bidding system dynamically adjusts the bidding strategy to optimize Cost Per Action (CPA) based on the number of ad conversions.","For ads with a long conversion delay, relying solely on the real-time tracked conversion number as a signal for bidding strategy can significantly overestimate the current CPA, leading to conservative bidding strategies.","Therefore, it is crucial to predict the number of long-delayed conversions.","Nonetheless, it is challenging to predict ad conversion numbers through traditional regression methods due to the wide range of ad conversion numbers.","Previous regression works have addressed this challenge by transforming regression problems into bucket classification problems, achieving success in various scenarios.","However, specific challenges arise when predicting the number of ad conversions: 1) The integer nature of ad conversion numbers exacerbates the discontinuity issue in one-hot hard labels; 2) The long-tail distribution of ad conversion numbers complicates tail data prediction.","In this paper, we propose the Long-Delayed Ad Conversions Prediction model for bidding strategy (LDACP), which consists of two sub-modules.","To alleviate the issue of discontinuity in one-hot hard labels, the Bucket Classification Module with label Smoothing method (BCMS) converts one-hot hard labels into non-normalized soft labels, then fits these soft labels by minimizing classification loss and regression loss.","To address the challenge of predicting tail data, the Value Regression Module with Proxy labels (VRMP) uses the prediction bias of aggregated pCTCVR as proxy labels.","Finally, a Mixture of Experts (MoE) structure integrates the predictions from BCMS and VRMP to obtain the final predicted ad conversion number."],"url":"http://arxiv.org/abs/2411.16095v1"}
{"created":"2024-11-25 04:22:33","title":"Leverage Task Context for Object Affordance Ranking","abstract":"Intelligent agents accomplish different tasks by utilizing various objects based on their affordance, but how to select appropriate objects according to task context is not well-explored. Current studies treat objects within the affordance category as equivalent, ignoring that object affordances vary in priority with different task contexts, hindering accurate decision-making in complex environments. To enable agents to develop a deeper understanding of the objects required to perform tasks, we propose to leverage task context for object affordance ranking, i.e., given image of a complex scene and the textual description of the affordance and task context, revealing task-object relationships and clarifying the priority rank of detected objects. To this end, we propose a novel Context-embed Group Ranking Framework with task relation mining module and graph group update module to deeply integrate task context and perform global relative relationship transmission. Due to the lack of such data, we construct the first large-scale task-oriented affordance ranking dataset with 25 common tasks, over 50k images and more than 661k objects. Experimental results demonstrate the feasibility of the task context based affordance learning paradigm and the superiority of our model over state-of-the-art models in the fields of saliency ranking and multimodal object detection. The source code and dataset will be made available to the public.","sentences":["Intelligent agents accomplish different tasks by utilizing various objects based on their affordance, but how to select appropriate objects according to task context is not well-explored.","Current studies treat objects within the affordance category as equivalent, ignoring that object affordances vary in priority with different task contexts, hindering accurate decision-making in complex environments.","To enable agents to develop a deeper understanding of the objects required to perform tasks, we propose to leverage task context for object affordance ranking, i.e., given image of a complex scene and the textual description of the affordance and task context, revealing task-object relationships and clarifying the priority rank of detected objects.","To this end, we propose a novel Context-embed Group Ranking Framework with task relation mining module and graph group update module to deeply integrate task context and perform global relative relationship transmission.","Due to the lack of such data, we construct the first large-scale task-oriented affordance ranking dataset with 25 common tasks, over 50k images and more than 661k objects.","Experimental results demonstrate the feasibility of the task context based affordance learning paradigm and the superiority of our model over state-of-the-art models in the fields of saliency ranking and multimodal object detection.","The source code and dataset will be made available to the public."],"url":"http://arxiv.org/abs/2411.16082v1"}
{"created":"2024-11-25 04:20:52","title":"Boosting 3D Object Generation through PBR Materials","abstract":"Automatic 3D content creation has gained increasing attention recently, due to its potential in various applications such as video games, film industry, and AR/VR. Recent advancements in diffusion models and multimodal models have notably improved the quality and efficiency of 3D object generation given a single RGB image. However, 3D objects generated even by state-of-the-art methods are still unsatisfactory compared to human-created assets. Considering only textures instead of materials makes these methods encounter challenges in photo-realistic rendering, relighting, and flexible appearance editing. And they also suffer from severe misalignment between geometry and high-frequency texture details. In this work, we propose a novel approach to boost the quality of generated 3D objects from the perspective of Physics-Based Rendering (PBR) materials. By analyzing the components of PBR materials, we choose to consider albedo, roughness, metalness, and bump maps. For albedo and bump maps, we leverage Stable Diffusion fine-tuned on synthetic data to extract these values, with novel usages of these fine-tuned models to obtain 3D consistent albedo UV and bump UV for generated objects. In terms of roughness and metalness maps, we adopt a semi-automatic process to provide room for interactive adjustment, which we believe is more practical. Extensive experiments demonstrate that our model is generally beneficial for various state-of-the-art generation methods, significantly boosting the quality and realism of their generated 3D objects, with natural relighting effects and substantially improved geometry.","sentences":["Automatic 3D content creation has gained increasing attention recently, due to its potential in various applications such as video games, film industry, and AR/VR.","Recent advancements in diffusion models and multimodal models have notably improved the quality and efficiency of 3D object generation given a single RGB image.","However, 3D objects generated even by state-of-the-art methods are still unsatisfactory compared to human-created assets.","Considering only textures instead of materials makes these methods encounter challenges in photo-realistic rendering, relighting, and flexible appearance editing.","And they also suffer from severe misalignment between geometry and high-frequency texture details.","In this work, we propose a novel approach to boost the quality of generated 3D objects from the perspective of Physics-Based Rendering (PBR) materials.","By analyzing the components of PBR materials, we choose to consider albedo, roughness, metalness, and bump maps.","For albedo and bump maps, we leverage Stable Diffusion fine-tuned on synthetic data to extract these values, with novel usages of these fine-tuned models to obtain 3D consistent albedo UV and bump UV for generated objects.","In terms of roughness and metalness maps, we adopt a semi-automatic process to provide room for interactive adjustment, which we believe is more practical.","Extensive experiments demonstrate that our model is generally beneficial for various state-of-the-art generation methods, significantly boosting the quality and realism of their generated 3D objects, with natural relighting effects and substantially improved geometry."],"url":"http://arxiv.org/abs/2411.16080v1"}
{"created":"2024-11-25 04:11:16","title":"Debiasing Classifiers by Amplifying Bias with Latent Diffusion and Large Language Models","abstract":"Neural networks struggle with image classification when biases are learned and misleads correlations, affecting their generalization and performance. Previous methods require attribute labels (e.g. background, color) or utilizes Generative Adversarial Networks (GANs) to mitigate biases. We introduce DiffuBias, a novel pipeline for text-to-image generation that enhances classifier robustness by generating bias-conflict samples, without requiring training during the generation phase. Utilizing pretrained diffusion and image captioning models, DiffuBias generates images that challenge the biases of classifiers, using the top-$K$ losses from a biased classifier ($f_B$) to create more representative data samples. This method not only debiases effectively but also boosts classifier generalization capabilities. To the best of our knowledge, DiffuBias is the first approach leveraging a stable diffusion model to generate bias-conflict samples in debiasing tasks. Our comprehensive experimental evaluations demonstrate that DiffuBias achieves state-of-the-art performance on benchmark datasets. We also conduct a comparative analysis of various generative models in terms of carbon emissions and energy consumption to highlight the significance of computational efficiency.","sentences":["Neural networks struggle with image classification when biases are learned and misleads correlations, affecting their generalization and performance.","Previous methods require attribute labels (e.g. background, color) or utilizes Generative Adversarial Networks (GANs) to mitigate biases.","We introduce DiffuBias, a novel pipeline for text-to-image generation that enhances classifier robustness by generating bias-conflict samples, without requiring training during the generation phase.","Utilizing pretrained diffusion and image captioning models, DiffuBias generates images that challenge the biases of classifiers, using the top-$K$ losses from a biased classifier ($f_B$) to create more representative data samples.","This method not only debiases effectively but also boosts classifier generalization capabilities.","To the best of our knowledge, DiffuBias is the first approach leveraging a stable diffusion model to generate bias-conflict samples in debiasing tasks.","Our comprehensive experimental evaluations demonstrate that DiffuBias achieves state-of-the-art performance on benchmark datasets.","We also conduct a comparative analysis of various generative models in terms of carbon emissions and energy consumption to highlight the significance of computational efficiency."],"url":"http://arxiv.org/abs/2411.16079v1"}
{"created":"2024-11-25 04:07:16","title":"SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for reference-free open-ended text","abstract":"Large Language Model (LLM) integrations into applications like Microsoft365 suite and Google Workspace for creating/processing documents, emails, presentations, etc. has led to considerable enhancements in productivity and time savings. But as these integrations become more more complex, it is paramount to ensure that the quality of output from the LLM-integrated applications are relevant and appropriate for use. Identifying the need to develop robust evaluation approaches for natural language generation, wherein references/ground labels doesn't exist or isn't amply available, this paper introduces a novel framework called \"SAGEval\" which utilizes a critiquing Agent to provide feedback on scores generated by LLM evaluators. We show that the critiquing Agent is able to rectify scores from LLM evaluators, in absence of references/ground-truth labels, thereby reducing the need for labeled data even for complex NLG evaluation scenarios, like the generation of JSON-structured forms/surveys with responses in different styles like multiple choice, likert ratings, single choice questions, etc.","sentences":["Large Language Model (LLM) integrations into applications like Microsoft365 suite and Google Workspace for creating/processing documents, emails, presentations, etc. has led to considerable enhancements in productivity and time savings.","But as these integrations become more more complex, it is paramount to ensure that the quality of output from the LLM-integrated applications are relevant and appropriate for use.","Identifying the need to develop robust evaluation approaches for natural language generation, wherein references/ground labels doesn't exist or isn't amply available, this paper introduces a novel framework called \"SAGEval\" which utilizes a critiquing Agent to provide feedback on scores generated by LLM evaluators.","We show that the critiquing Agent is able to rectify scores from LLM evaluators, in absence of references/ground-truth labels, thereby reducing the need for labeled data even for complex NLG evaluation scenarios, like the generation of JSON-structured forms/surveys with responses in different styles like multiple choice, likert ratings, single choice questions, etc."],"url":"http://arxiv.org/abs/2411.16077v1"}
{"created":"2024-11-25 04:06:48","title":"Geometry Distributions","abstract":"Neural representations of 3D data have been widely adopted across various applications, particularly in recent work leveraging coordinate-based networks to model scalar or vector fields. However, these approaches face inherent challenges, such as handling thin structures and non-watertight geometries, which limit their flexibility and accuracy. In contrast, we propose a novel geometric data representation that models geometry as distributions-a powerful representation that makes no assumptions about surface genus, connectivity, or boundary conditions. Our approach uses diffusion models with a novel network architecture to learn surface point distributions, capturing fine-grained geometric details. We evaluate our representation qualitatively and quantitatively across various object types, demonstrating its effectiveness in achieving high geometric fidelity. Additionally, we explore applications using our representation, such as textured mesh representation, neural surface compression, dynamic object modeling, and rendering, highlighting its potential to advance 3D geometric learning.","sentences":["Neural representations of 3D data have been widely adopted across various applications, particularly in recent work leveraging coordinate-based networks to model scalar or vector fields.","However, these approaches face inherent challenges, such as handling thin structures and non-watertight geometries, which limit their flexibility and accuracy.","In contrast, we propose a novel geometric data representation that models geometry as distributions-a powerful representation that makes no assumptions about surface genus, connectivity, or boundary conditions.","Our approach uses diffusion models with a novel network architecture to learn surface point distributions, capturing fine-grained geometric details.","We evaluate our representation qualitatively and quantitatively across various object types, demonstrating its effectiveness in achieving high geometric fidelity.","Additionally, we explore applications using our representation, such as textured mesh representation, neural surface compression, dynamic object modeling, and rendering, highlighting its potential to advance 3D geometric learning."],"url":"http://arxiv.org/abs/2411.16076v1"}
{"created":"2024-11-25 03:28:09","title":"Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation","abstract":"This paper explores the Class-Incremental Source-Free Unsupervised Domain Adaptation (CI-SFUDA) problem, where the unlabeled target data come incrementally without access to labeled source instances. This problem poses two challenges, the disturbances of similar source-class knowledge to target-class representation learning and the new target knowledge to old ones. To address them, we propose the Multi-Granularity Class Prototype Topology Distillation (GROTO) algorithm, which effectively transfers the source knowledge to the unlabeled class-incremental target domain. Concretely, we design the multi-granularity class prototype self-organization module and prototype topology distillation module. Firstly, the positive classes are mined by modeling two accumulation distributions. Then, we generate reliable pseudo-labels by introducing multi-granularity class prototypes, and use them to promote the positive-class target feature self-organization. Secondly, the positive-class prototypes are leveraged to construct the topological structures of source and target feature spaces. Then, we perform the topology distillation to continually mitigate the interferences of new target knowledge to old ones. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performances on three public datasets.","sentences":["This paper explores the Class-Incremental Source-Free Unsupervised Domain Adaptation (CI-SFUDA) problem, where the unlabeled target data come incrementally without access to labeled source instances.","This problem poses two challenges, the disturbances of similar source-class knowledge to target-class representation learning and the new target knowledge to old ones.","To address them, we propose the Multi-Granularity Class Prototype Topology Distillation (GROTO) algorithm, which effectively transfers the source knowledge to the unlabeled class-incremental target domain.","Concretely, we design the multi-granularity class prototype self-organization module and prototype topology distillation module.","Firstly, the positive classes are mined by modeling two accumulation distributions.","Then, we generate reliable pseudo-labels by introducing multi-granularity class prototypes, and use them to promote the positive-class target feature self-organization.","Secondly, the positive-class prototypes are leveraged to construct the topological structures of source and target feature spaces.","Then, we perform the topology distillation to continually mitigate the interferences of new target knowledge to old ones.","Extensive experiments demonstrate that our proposed method achieves state-of-the-art performances on three public datasets."],"url":"http://arxiv.org/abs/2411.16064v1"}
{"created":"2024-11-25 03:25:17","title":"VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics Prediction","abstract":"In-Context Operator Networks (ICONs) are models that learn operators across different types of PDEs using a few-shot, in-context approach. Although they show successful generalization to various PDEs, existing methods treat each data point as a single token, and suffer from computational inefficiency when processing dense data, limiting their application in higher spatial dimensions. In this work, we propose Vision In-Context Operator Networks (VICON), incorporating a vision transformer architecture that efficiently processes 2D functions through patch-wise operations. We evaluated our method on three fluid dynamics datasets, demonstrating both superior performance (reducing scaled $L^2$ error by $40\\%$ and $61.6\\%$ for two benchmark datasets for compressible flows, respectively) and computational efficiency (requiring only one-third of the inference time per frame) in long-term rollout predictions compared to the current state-of-the-art sequence-to-sequence model with fixed timestep prediction: Multiple Physics Pretraining (MPP). Compared to MPP, our method preserves the benefits of in-context operator learning, enabling flexible context formation when dealing with insufficient frame counts or varying timestep values.","sentences":["In-Context Operator Networks (ICONs) are models that learn operators across different types of PDEs using a few-shot, in-context approach.","Although they show successful generalization to various PDEs, existing methods treat each data point as a single token, and suffer from computational inefficiency when processing dense data, limiting their application in higher spatial dimensions.","In this work, we propose Vision In-Context Operator Networks (VICON), incorporating a vision transformer architecture that efficiently processes 2D functions through patch-wise operations.","We evaluated our method on three fluid dynamics datasets, demonstrating both superior performance (reducing scaled $L^2$ error by $40\\%$ and $61.6\\%$ for two benchmark datasets for compressible flows, respectively) and computational efficiency (requiring only one-third of the inference time per frame) in long-term rollout predictions compared to the current state-of-the-art sequence-to-sequence model with fixed timestep prediction: Multiple Physics Pretraining (MPP).","Compared to MPP, our method preserves the benefits of in-context operator learning, enabling flexible context formation when dealing with insufficient frame counts or varying timestep values."],"url":"http://arxiv.org/abs/2411.16063v1"}
{"created":"2024-11-25 01:48:09","title":"Predicting Emergent Capabilities by Finetuning","abstract":"A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., \"emergence laws\"). We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged. Finally, we present a case study of two realistic uses for emergence prediction.","sentences":["A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities.","In particular, language model pretraining loss is known to be highly predictable as a function of compute.","However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models.","In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task?","We then discover a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models.","To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., \"emergence laws\").","We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA).","Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged.","Finally, we present a case study of two realistic uses for emergence prediction."],"url":"http://arxiv.org/abs/2411.16035v1"}
{"created":"2024-11-25 01:15:31","title":"Binary Search with Distributional Predictions","abstract":"Algorithms with (machine-learned) predictions is a powerful framework for combining traditional worst-case algorithms with modern machine learning. However, the vast majority of work in this space assumes that the prediction itself is non-probabilistic, even if it is generated by some stochastic process (such as a machine learning system). This is a poor fit for modern ML, particularly modern neural networks, which naturally generate a distribution. We initiate the study of algorithms with distributional predictions, where the prediction itself is a distribution. We focus on one of the simplest yet fundamental settings: binary search (or searching a sorted array). This setting has one of the simplest algorithms with a point prediction, but what happens if the prediction is a distribution? We show that this is a richer setting: there are simple distributions where using the classical prediction-based algorithm with any single prediction does poorly. Motivated by this, as our main result, we give an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution $p$ and $\\eta$ is the earth mover's distance between $p$ and the predicted distribution $\\hat p$. This also yields the first distributionally-robust algorithm for the classical problem of computing an optimal binary search tree given a distribution over target keys. We complement this with a lower bound showing that this query complexity is essentially optimal (up to constants), and experiments validating the practical usefulness of our algorithm.","sentences":["Algorithms with (machine-learned) predictions is a powerful framework for combining traditional worst-case algorithms with modern machine learning.","However, the vast majority of work in this space assumes that the prediction itself is non-probabilistic, even if it is generated by some stochastic process (such as a machine learning system).","This is a poor fit for modern ML, particularly modern neural networks, which naturally generate a distribution.","We initiate the study of algorithms with distributional predictions, where the prediction itself is a distribution.","We focus on one of the simplest yet fundamental settings: binary search (or searching a sorted array).","This setting has one of the simplest algorithms with a point prediction, but what happens if the prediction is a distribution?","We show that this is a richer setting: there are simple distributions where using the classical prediction-based algorithm with any single prediction does poorly.","Motivated by this, as our main result, we give an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution $p$ and $\\eta$ is the earth mover's distance between $p$ and the predicted distribution $\\hat p$.","This also yields the first distributionally-robust algorithm for the classical problem of computing an optimal binary search tree given a distribution over target keys.","We complement this with a lower bound showing that this query complexity is essentially optimal (up to constants), and experiments validating the practical usefulness of our algorithm."],"url":"http://arxiv.org/abs/2411.16030v1"}
{"created":"2024-11-25 01:01:54","title":"From Dashcam Videos to Driving Simulations: Stress Testing Automated Vehicles against Rare Events","abstract":"Testing Automated Driving Systems (ADS) in simulation with realistic driving scenarios is important for verifying their performance. However, converting real-world driving videos into simulation scenarios is a significant challenge due to the complexity of interpreting high-dimensional video data and the time-consuming nature of precise manual scenario reconstruction. In this work, we propose a novel framework that automates the conversion of real-world car crash videos into detailed simulation scenarios for ADS testing. Our approach leverages prompt-engineered Video Language Models(VLM) to transform dashcam footage into SCENIC scripts, which define the environment and driving behaviors in the CARLA simulator, enabling the generation of realistic simulation scenarios. Importantly, rather than solely aiming for one-to-one scenario reconstruction, our framework focuses on capturing the essential driving behaviors from the original video while offering flexibility in parameters such as weather or road conditions to facilitate search-based testing. Additionally, we introduce a similarity metric that helps iteratively refine the generated scenario through feedback by comparing key features of driving behaviors between the real and simulated videos. Our preliminary results demonstrate substantial time efficiency, finishing the real-to-sim conversion in minutes with full automation and no human intervention, while maintaining high fidelity to the original driving events.","sentences":["Testing Automated Driving Systems (ADS) in simulation with realistic driving scenarios is important for verifying their performance.","However, converting real-world driving videos into simulation scenarios is a significant challenge due to the complexity of interpreting high-dimensional video data and the time-consuming nature of precise manual scenario reconstruction.","In this work, we propose a novel framework that automates the conversion of real-world car crash videos into detailed simulation scenarios for ADS testing.","Our approach leverages prompt-engineered Video Language Models(VLM) to transform dashcam footage into SCENIC scripts, which define the environment and driving behaviors in the CARLA simulator, enabling the generation of realistic simulation scenarios.","Importantly, rather than solely aiming for one-to-one scenario reconstruction, our framework focuses on capturing the essential driving behaviors from the original video while offering flexibility in parameters such as weather or road conditions to facilitate search-based testing.","Additionally, we introduce a similarity metric that helps iteratively refine the generated scenario through feedback by comparing key features of driving behaviors between the real and simulated videos.","Our preliminary results demonstrate substantial time efficiency, finishing the real-to-sim conversion in minutes with full automation and no human intervention, while maintaining high fidelity to the original driving events."],"url":"http://arxiv.org/abs/2411.16027v1"}
{"created":"2024-11-25 00:32:20","title":"TransCompressor: LLM-Powered Multimodal Data Compression for Smart Transportation","abstract":"The incorporation of Large Language Models (LLMs) into smart transportation systems has paved the way for improving data management and operational efficiency. This study introduces TransCompressor, a novel framework that leverages LLMs for efficient compression and decompression of multimodal transportation sensor data. TransCompressor has undergone thorough evaluation with diverse sensor data types, including barometer, speed, and altitude measurements, across various transportation modes like buses, taxis, and MTRs. Comprehensive evaluation illustrates the effectiveness of TransCompressor in reconstructing transportation sensor data at different compression ratios. The results highlight that, with well-crafted prompts, LLMs can utilize their vast knowledge base to contribute to data compression processes, enhancing data storage, analysis, and retrieval in smart transportation settings.","sentences":["The incorporation of Large Language Models (LLMs) into smart transportation systems has paved the way for improving data management and operational efficiency.","This study introduces TransCompressor, a novel framework that leverages LLMs for efficient compression and decompression of multimodal transportation sensor data.","TransCompressor has undergone thorough evaluation with diverse sensor data types, including barometer, speed, and altitude measurements, across various transportation modes like buses, taxis, and MTRs.","Comprehensive evaluation illustrates the effectiveness of TransCompressor in reconstructing transportation sensor data at different compression ratios.","The results highlight that, with well-crafted prompts, LLMs can utilize their vast knowledge base to contribute to data compression processes, enhancing data storage, analysis, and retrieval in smart transportation settings."],"url":"http://arxiv.org/abs/2411.16020v1"}
{"created":"2024-11-25 00:20:53","title":"Style-Pro: Style-Guided Prompt Learning for Generalizable Vision-Language Models","abstract":"Pre-trained Vision-language (VL) models, such as CLIP, have shown significant generalization ability to downstream tasks, even with minimal fine-tuning. While prompt learning has emerged as an effective strategy to adapt pre-trained VL models for downstream tasks, current approaches frequently encounter severe overfitting to specific downstream data distributions. This overfitting constrains the original behavior of the VL models to generalize to new domains or unseen classes, posing a critical challenge in enhancing the adaptability and generalization of VL models. To address this limitation, we propose Style-Pro, a novel style-guided prompt learning framework that mitigates overfitting and preserves the zero-shot generalization capabilities of CLIP. Style-Pro employs learnable style bases to synthesize diverse distribution shifts, guided by two specialized loss functions that ensure style diversity and content integrity. Then, to minimize discrepancies between unseen domains and the source domain, Style-Pro maps the unseen styles into the known style representation space as a weighted combination of style bases. Moreover, to maintain consistency between the style-shifted prompted model and the original frozen CLIP, Style-Pro introduces consistency constraints to preserve alignment in the learned embeddings, minimizing deviation during adaptation to downstream tasks. Extensive experiments across 11 benchmark datasets demonstrate the effectiveness of Style-Pro, consistently surpassing state-of-the-art methods in various settings, including base-to-new generalization, cross-dataset transfer, and domain generalization.","sentences":["Pre-trained Vision-language (VL) models, such as CLIP, have shown significant generalization ability to downstream tasks, even with minimal fine-tuning.","While prompt learning has emerged as an effective strategy to adapt pre-trained VL models for downstream tasks, current approaches frequently encounter severe overfitting to specific downstream data distributions.","This overfitting constrains the original behavior of the VL models to generalize to new domains or unseen classes, posing a critical challenge in enhancing the adaptability and generalization of VL models.","To address this limitation, we propose Style-Pro, a novel style-guided prompt learning framework that mitigates overfitting and preserves the zero-shot generalization capabilities of CLIP.","Style-Pro employs learnable style bases to synthesize diverse distribution shifts, guided by two specialized loss functions that ensure style diversity and content integrity.","Then, to minimize discrepancies between unseen domains and the source domain, Style-Pro maps the unseen styles into the known style representation space as a weighted combination of style bases.","Moreover, to maintain consistency between the style-shifted prompted model and the original frozen CLIP, Style-Pro introduces consistency constraints to preserve alignment in the learned embeddings, minimizing deviation during adaptation to downstream tasks.","Extensive experiments across 11 benchmark datasets demonstrate the effectiveness of Style-Pro, consistently surpassing state-of-the-art methods in various settings, including base-to-new generalization, cross-dataset transfer, and domain generalization."],"url":"http://arxiv.org/abs/2411.16018v1"}
