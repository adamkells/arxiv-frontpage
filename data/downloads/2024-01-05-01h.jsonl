{"created":"2024-01-03 18:24:18","title":"On the hardness of learning under symmetries","abstract":"We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries (\"equivariance\") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension. Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard.","sentences":["We study the problem of learning equivariant neural networks via gradient descent.","The incorporation of known symmetries (\"equivariance\") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision.","However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent.","In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent?","We answer this question in the negative.","In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension.","Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard."],"url":"http://arxiv.org/abs/2401.01869v1"}
{"created":"2024-01-03 17:44:17","title":"The Power of Training: How Different Neural Network Setups Influence the Energy Demand","abstract":"This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.","sentences":["This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption.","While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission.","Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer.","Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results.","Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning."],"url":"http://arxiv.org/abs/2401.01851v1"}
{"created":"2024-01-03 17:36:27","title":"DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction","abstract":"Forecasting future stock trends remains challenging for academia and industry due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics influencing stock prices. In recent years, graph neural networks have achieved remarkable performance in this problem by formulating multiple stocks as graph-structured data. However, most of these approaches rely on artificially defined factors to construct static stock graphs, which fail to capture the intrinsic interdependencies between stocks that rapidly evolve. In addition, these methods often ignore the hierarchical features of the stocks and lose distinctive information within. In this work, we propose a novel graph learning approach implemented without expert knowledge to address these issues. First, our approach automatically constructs dynamic stock graphs by entropy-driven edge generation from a signal processing perspective. Then, we further learn task-optimal dependencies between stocks via a generalized graph diffusion process on constructed stock graphs. Last, a decoupled representation learning scheme is adopted to capture distinctive hierarchical intra-stock features. Experimental results demonstrate substantial improvements over state-of-the-art baselines on real-world datasets. Moreover, the ablation study and sensitivity study further illustrate the effectiveness of the proposed method in modeling the time-evolving inter-stock and intra-stock dynamics.","sentences":["Forecasting future stock trends remains challenging for academia and industry due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics influencing stock prices.","In recent years, graph neural networks have achieved remarkable performance in this problem by formulating multiple stocks as graph-structured data.","However, most of these approaches rely on artificially defined factors to construct static stock graphs, which fail to capture the intrinsic interdependencies between stocks that rapidly evolve.","In addition, these methods often ignore the hierarchical features of the stocks and lose distinctive information within.","In this work, we propose a novel graph learning approach implemented without expert knowledge to address these issues.","First, our approach automatically constructs dynamic stock graphs by entropy-driven edge generation from a signal processing perspective.","Then, we further learn task-optimal dependencies between stocks via a generalized graph diffusion process on constructed stock graphs.","Last, a decoupled representation learning scheme is adopted to capture distinctive hierarchical intra-stock features.","Experimental results demonstrate substantial improvements over state-of-the-art baselines on real-world datasets.","Moreover, the ablation study and sensitivity study further illustrate the effectiveness of the proposed method in modeling the time-evolving inter-stock and intra-stock dynamics."],"url":"http://arxiv.org/abs/2401.01846v1"}
{"created":"2024-01-03 17:22:48","title":"Investigating Semi-Supervised Learning Algorithms in Text Datasets","abstract":"Using large training datasets enhances the generalization capabilities of neural networks. Semi-supervised learning (SSL) is useful when there are few labeled data and a lot of unlabeled data. SSL methods that use data augmentation are most successful for image datasets. In contrast, texts do not have consistent augmentation methods as images. Consequently, methods that use augmentation are not as effective in text data as they are in image data. In this study, we compared SSL algorithms that do not require augmentation; these are self-training, co-training, tri-training, and tri-training with disagreement. In the experiments, we used 4 different text datasets for different tasks. We examined the algorithms from a variety of perspectives by asking experiment questions and suggested several improvements. Among the algorithms, tri-training with disagreement showed the closest performance to the Oracle; however, performance gap shows that new semi-supervised algorithms or improvements in existing methods are needed.","sentences":["Using large training datasets enhances the generalization capabilities of neural networks.","Semi-supervised learning (SSL) is useful when there are few labeled data and a lot of unlabeled data.","SSL methods that use data augmentation are most successful for image datasets.","In contrast, texts do not have consistent augmentation methods as images.","Consequently, methods that use augmentation are not as effective in text data as they are in image data.","In this study, we compared SSL algorithms that do not require augmentation; these are self-training, co-training, tri-training, and tri-training with disagreement.","In the experiments, we used 4 different text datasets for different tasks.","We examined the algorithms from a variety of perspectives by asking experiment questions and suggested several improvements.","Among the algorithms, tri-training with disagreement showed the closest performance to the Oracle; however, performance gap shows that new semi-supervised algorithms or improvements in existing methods are needed."],"url":"http://arxiv.org/abs/2401.01843v1"}
{"created":"2024-01-03 17:20:27","title":"Wasserstein Nonnegative Tensor Factorization with Manifold Regularization","abstract":"Nonnegative tensor factorization (NTF) has become an important tool for feature extraction and part-based representation with preserved intrinsic structure information from nonnegative high-order data. However, the original NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss function which treats each feature equally leading to the neglect of the side-information of features. To utilize correlation information of features and manifold information of samples, we introduce Wasserstein manifold nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein distance between the distribution of input tensorial data and the distribution of reconstruction. Although some researches about Wasserstein distance have been proposed in nonnegative matrix factorization (NMF), they ignore the spatial structure information of higher-order data. We use Wasserstein distance (a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and add a graph regularizer to a latent factor. Experimental results demonstrate the effectiveness of the proposed method compared with other NMF and NTF methods.","sentences":["Nonnegative tensor factorization (NTF) has become an important tool for feature extraction and part-based representation with preserved intrinsic structure information from nonnegative high-order data.","However, the original NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss function which treats each feature equally leading to the neglect of the side-information of features.","To utilize correlation information of features and manifold information of samples, we introduce Wasserstein manifold nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein distance between the distribution of input tensorial data and the distribution of reconstruction.","Although some researches about Wasserstein distance have been proposed in nonnegative matrix factorization (NMF), they ignore the spatial structure information of higher-order data.","We use Wasserstein distance (a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and add a graph regularizer to a latent factor.","Experimental results demonstrate the effectiveness of the proposed method compared with other NMF and NTF methods."],"url":"http://arxiv.org/abs/2401.01842v1"}
{"created":"2024-01-03 17:05:17","title":"NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems","abstract":"Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknown dynamical system towards target states. Our experiments demonstrate the effectiveness and data efficiency of NODEC for learning optimal control of unknown dynamical systems.","sentences":["Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework.","For systems with unknown dynamics, an additional step of dynamics modeling is required.","However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function.","Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency.","To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model.","Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknown dynamical system towards target states.","Our experiments demonstrate the effectiveness and data efficiency of NODEC for learning optimal control of unknown dynamical systems."],"url":"http://arxiv.org/abs/2401.01836v1"}
{"created":"2024-01-03 16:47:13","title":"Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling","abstract":"Data augmentation is an effective technique for improving the performance of machine learning models. However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision. In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model. Our method involves iteratively masking words in a sentence and replacing them with language model predictions. We have tested our proposed method on various NLP tasks and found it to be effective in many cases. Our results are presented along with a comparison to existing augmentation methods. Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets.","sentences":["Data augmentation is an effective technique for improving the performance of machine learning models.","However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision.","In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model.","Our method involves iteratively masking words in a sentence and replacing them with language model predictions.","We have tested our proposed method on various NLP tasks and found it to be effective in many cases.","Our results are presented along with a comparison to existing augmentation methods.","Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets."],"url":"http://arxiv.org/abs/2401.01830v1"}
{"created":"2024-01-03 16:43:09","title":"Data-Driven Power Modeling and Monitoring via Hardware Performance Counters Tracking","abstract":"In the current high-performance and embedded computing era, full-stack energy-centric design is paramount. Use cases require increasingly high performance at an affordable power budget, often under real-time constraints. Extreme heterogeneity and parallelism address these issues but greatly complicate online power consumption assessment, which is essential for dynamic hardware and software stack adaptations. We introduce a novel architecture-agnostic power modeling methodology with state-of-the-art accuracy, low overhead, and high responsiveness. Our methodology identifies the best Performance Monitoring Counters (PMCs) to model the power consumption of each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS) state. The individual linear models are combined into a complete model that effectively describes the power consumption of the whole system, achieving high accuracy and low overhead. Our evaluation reports an average estimation error of 7.5 % for power consumption and 1.3 % for energy. Furthermore, we propose Runmeter, an open-source, PMC-based monitoring framework integrated into the Linux kernel. Runmeter manages PMC samples collection and manipulation, efficiently evaluating our power models at runtime. With a time overhead of only 0.7 % in the worst case, Runmeter provides responsive and accurate power measurements directly in the kernel, which can be employed for actuation policies such as Dynamic Power Management (DPM) and power-aware task scheduling.","sentences":["In the current high-performance and embedded computing era, full-stack energy-centric design is paramount.","Use cases require increasingly high performance at an affordable power budget, often under real-time constraints.","Extreme heterogeneity and parallelism address these issues but greatly complicate online power consumption assessment, which is essential for dynamic hardware and software stack adaptations.","We introduce a novel architecture-agnostic power modeling methodology with state-of-the-art accuracy, low overhead, and high responsiveness.","Our methodology identifies the best Performance Monitoring Counters (PMCs) to model the power consumption of each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS) state.","The individual linear models are combined into a complete model that effectively describes the power consumption of the whole system, achieving high accuracy and low overhead.","Our evaluation reports an average estimation error of 7.5 % for power consumption and 1.3 % for energy.","Furthermore, we propose Runmeter, an open-source, PMC-based monitoring framework integrated into the Linux kernel.","Runmeter manages PMC samples collection and manipulation, efficiently evaluating our power models at runtime.","With a time overhead of only 0.7 % in the worst case, Runmeter provides responsive and accurate power measurements directly in the kernel, which can be employed for actuation policies such as Dynamic Power Management (DPM) and power-aware task scheduling."],"url":"http://arxiv.org/abs/2401.01826v1"}
{"created":"2024-01-03 16:38:56","title":"HawkRover: An Autonomous mmWave Vehicular Communication Testbed with Multi-sensor Fusion and Deep Learning","abstract":"Connected and automated vehicles (CAVs) have become a transformative technology that can change our daily life. Currently, millimeter-wave (mmWave) bands are identified as the promising CAV connectivity solution. While it can provide high data rate, their realization faces many challenges such as high attenuation during mmWave signal propagation and mobility management. Existing solution has to initiate pilot signal to measure channel information, then apply signal processing to calculate the best narrow beam towards the receiver end to guarantee sufficient signal power. This process takes significant overhead and time, hence not suitable for vehicles. In this study, we propose an autonomous and low-cost testbed to collect extensive co-located mmWave signal and other sensors data such as LiDAR (Light Detection and Ranging), cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave vehicular communications. Intuitively, these sensors can build a 3D map around the vehicle and signal propagation path can be estimated, eliminating iterative the process via pilot signals. This multimodal data fusion, together with AI, is expected to bring significant advances in ``connected'' research.","sentences":["Connected and automated vehicles (CAVs) have become a transformative technology that can change our daily life.","Currently, millimeter-wave (mmWave) bands are identified as the promising CAV connectivity solution.","While it can provide high data rate, their realization faces many challenges such as high attenuation during mmWave signal propagation and mobility management.","Existing solution has to initiate pilot signal to measure channel information, then apply signal processing to calculate the best narrow beam towards the receiver end to guarantee sufficient signal power.","This process takes significant overhead and time, hence not suitable for vehicles.","In this study, we propose an autonomous and low-cost testbed to collect extensive co-located mmWave signal and other sensors data such as LiDAR (Light Detection and Ranging), cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave vehicular communications.","Intuitively, these sensors can build a 3D map around the vehicle and signal propagation path can be estimated, eliminating iterative the process via pilot signals.","This multimodal data fusion, together with AI, is expected to bring significant advances in ``connected'' research."],"url":"http://arxiv.org/abs/2401.01822v2"}
{"created":"2024-01-03 16:38:56","title":"Detours for Navigating Instructional Videos","abstract":"We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.","sentences":["We introduce the video detours problem for navigating instructional videos.","Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration.","To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries.","Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data.","We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques.","Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%."],"url":"http://arxiv.org/abs/2401.01823v1"}
{"created":"2024-01-03 16:21:46","title":"SENS3: Multisensory Database of Finger-Surface Interactions and Corresponding Sensations","abstract":"The growing demand for natural interactions with technology underscores the importance of achieving realistic touch sensations in digital environments. Realizing this goal highly depends on comprehensive databases of finger-surface interactions, which need further development. Here, we present SENS3, an extensive open-access repository of multisensory data acquired from fifty surfaces when two participants explored them with their fingertips through static contact, pressing, tapping, and sliding. SENS3 encompasses high-fidelity visual, audio, and haptic information recorded during these interactions, including videos, sounds, contact forces, torques, positions, accelerations, skin temperature, heat flux, and surface photographs. Additionally, it incorporates thirteen participants' psychophysical sensation ratings while exploring these surfaces freely. We anticipate that SENS3 will be valuable for advancing multisensory texture rendering, user experience development, and touch sensing in robotics.","sentences":["The growing demand for natural interactions with technology underscores the importance of achieving realistic touch sensations in digital environments.","Realizing this goal highly depends on comprehensive databases of finger-surface interactions, which need further development.","Here, we present SENS3, an extensive open-access repository of multisensory data acquired from fifty surfaces when two participants explored them with their fingertips through static contact, pressing, tapping, and sliding.","SENS3 encompasses high-fidelity visual, audio, and haptic information recorded during these interactions, including videos, sounds, contact forces, torques, positions, accelerations, skin temperature, heat flux, and surface photographs.","Additionally, it incorporates thirteen participants' psychophysical sensation ratings while exploring these surfaces freely.","We anticipate that SENS3 will be valuable for advancing multisensory texture rendering, user experience development, and touch sensing in robotics."],"url":"http://arxiv.org/abs/2401.01818v1"}
{"created":"2024-01-03 16:15:22","title":"Signal Processing in the Retina: Interpretable Graph Classifier to Predict Ganglion Cell Responses","abstract":"It is a popular hypothesis in neuroscience that ganglion cells in the retina are activated by selectively detecting visual features in an observed scene. While ganglion cell firings can be predicted via data-trained deep neural nets, the networks remain indecipherable, thus providing little understanding of the cells' underlying operations. To extract knowledge from the cell firings, in this paper we learn an interpretable graph-based classifier from data to predict the firings of ganglion cells in response to visual stimuli. Specifically, we learn a positive semi-definite (PSD) metric matrix $\\mathbf{M} \\succeq 0$ that defines Mahalanobis distances between graph nodes (visual events) endowed with pre-computed feature vectors; the computed inter-node distances lead to edge weights and a combinatorial graph that is amenable to binary classification. Mathematically, we define the objective of metric matrix $\\mathbf{M}$ optimization using a graph adaptation of large margin nearest neighbor (LMNN), which is rewritten as a semi-definite programming (SDP) problem. We solve it efficiently via a fast approximation called Gershgorin disc perfect alignment (GDPA) linearization. The learned metric matrix $\\mathbf{M}$ provides interpretability: important features are identified along $\\mathbf{M}$'s diagonal, and their mutual relationships are inferred from off-diagonal terms. Our fast metric learning framework can be applied to other biological systems with pre-chosen features that require interpretation.","sentences":["It is a popular hypothesis in neuroscience that ganglion cells in the retina are activated by selectively detecting visual features in an observed scene.","While ganglion cell firings can be predicted via data-trained deep neural nets, the networks remain indecipherable, thus providing little understanding of the cells' underlying operations.","To extract knowledge from the cell firings, in this paper we learn an interpretable graph-based classifier from data to predict the firings of ganglion cells in response to visual stimuli.","Specifically, we learn a positive semi-definite (PSD) metric matrix $\\mathbf{M} \\succeq 0$ that defines Mahalanobis distances between graph nodes (visual events) endowed with pre-computed feature vectors; the computed inter-node distances lead to edge weights and a combinatorial graph that is amenable to binary classification.","Mathematically, we define the objective of metric matrix $\\mathbf{M}$ optimization using a graph adaptation of large margin nearest neighbor (LMNN), which is rewritten as a semi-definite programming (SDP) problem.","We solve it efficiently via a fast approximation called Gershgorin disc perfect alignment (GDPA) linearization.","The learned metric matrix $\\mathbf{M}$ provides interpretability: important features are identified along $\\mathbf{M}$'s diagonal, and their mutual relationships are inferred from off-diagonal terms.","Our fast metric learning framework can be applied to other biological systems with pre-chosen features that require interpretation."],"url":"http://arxiv.org/abs/2401.01813v1"}
{"created":"2024-01-03 15:36:33","title":"Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review","abstract":"According to the World Health Organization (WHO), air pollution kills seven million people every year. Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries. In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction. The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used. Two research questions were formulated for this review. 1086 publications were collected in the initial PRISMA stage. After the screening and eligibility phases, 37 papers were selected for inclusion. A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction. Three methods of prediction were identified: time series, feature-based and spatio-temporal. This review's findings identify major limitations in applications found in the literature, namely lack of coverage, lack of diversity of data and lack of inclusion of context-specific features. This review proposes directions for future research and underlines practical implications in healthcare, urban planning, global synergy and smart cities.","sentences":["According to the World Health Organization (WHO), air pollution kills seven million people every year.","Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries.","In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction.","The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used.","Two research questions were formulated for this review.","1086 publications were collected in the initial PRISMA stage.","After the screening and eligibility phases, 37 papers were selected for inclusion.","A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction.","Three methods of prediction were identified: time series, feature-based and spatio-temporal.","This review's findings identify major limitations in applications found in the literature, namely lack of coverage, lack of diversity of data and lack of inclusion of context-specific features.","This review proposes directions for future research and underlines practical implications in healthcare, urban planning, global synergy and smart cities."],"url":"http://arxiv.org/abs/2401.01788v1"}
{"created":"2024-01-03 15:27:10","title":"An experimental sorting method for improving metagenomic data encoding","abstract":"Minimizing data storage poses a significant challenge in large-scale metagenomic projects. In this paper, we present a new method for improving the encoding of FASTQ files generated by metagenomic sequencing. This method incorporates metagenomic classification followed by a recursive filter for clustering reads by DNA sequence similarity to improve the overall reference-free compression. In the results, we show an overall improvement in the compression of several datasets. As hypothesized, we show a progressive compression gain for higher coverage depth and number of identified species. Additionally, we provide an implementation that is freely available at https://github.com/cobilab/mizar and can be customized to work with other FASTQ compression tools.","sentences":["Minimizing data storage poses a significant challenge in large-scale metagenomic projects.","In this paper, we present a new method for improving the encoding of FASTQ files generated by metagenomic sequencing.","This method incorporates metagenomic classification followed by a recursive filter for clustering reads by DNA sequence similarity to improve the overall reference-free compression.","In the results, we show an overall improvement in the compression of several datasets.","As hypothesized, we show a progressive compression gain for higher coverage depth and number of identified species.","Additionally, we provide an implementation that is freely available at https://github.com/cobilab/mizar and can be customized to work with other FASTQ compression tools."],"url":"http://arxiv.org/abs/2401.01786v1"}
{"created":"2024-01-03 15:12:42","title":"Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering","abstract":"While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data. Our model directly provides answers for $78.2\\%$ of the known queries and opts to search for $77.2\\%$ of the unknown ones. This results in the API being utilized only $62\\%$ of the time.","sentences":["While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination.","Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers.","Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information.","However, searching in a large collection of documents introduces additional computational/time costs.","An optimal behavior would be to query external resources only when the LLM is not confident about answers.","In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool.","We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task.","In addition, we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data.","Our model directly provides answers for $78.2\\%$ of the known queries and opts to search for $77.2\\%$ of the unknown ones.","This results in the API being utilized only $62\\%$ of the time."],"url":"http://arxiv.org/abs/2401.01780v1"}
{"created":"2024-01-03 14:52:18","title":"A Novel Paradigm for Neural Computation: X-Net with Learnable Neurons and Adaptable Structure","abstract":"Artificial neural networks (ANNs) have permeated various disciplinary domains, ranging from bioinformatics to financial analytics, where their application has become an indispensable facet of contemporary scientific research endeavors. However, the inherent limitations of traditional neural networks arise due to their relatively fixed network structures and activation functions. 1, The type of activation function is single and relatively fixed, which leads to poor \"unit representation ability\" of the network, and it is often used to solve simple problems with very complex networks; 2, the network structure is not adaptive, it is easy to cause network structure redundant or insufficient. To address the aforementioned issues, this study proposes a novel neural network called X-Net. By utilizing our designed Alternating Backpropagation mechanism, X-Net dynamically selects appropriate activation functions based on derivative information during training to enhance the network's representation capability for specific tasks. Simultaneously, it accurately adjusts the network structure at the neuron level to accommodate tasks of varying complexities and reduce computational costs. Through a series of experiments, we demonstrate the dual advantages of X-Net in terms of reducing model size and improving representation power. Specifically, in terms of the number of parameters, X-Net is only 3$\\%$ of baselines on average, and only 1.4$\\%$ under some tasks. In terms of representation ability, X-Net can achieve an average $R^2$=0.985 on the fitting task by only optimizing the activation function without introducing any parameters. Finally, we also tested the ability of X-Net to help scientific discovery on data from multiple disciplines such as society, energy, environment, and aerospace, and achieved concise and good results.","sentences":["Artificial neural networks (ANNs) have permeated various disciplinary domains, ranging from bioinformatics to financial analytics, where their application has become an indispensable facet of contemporary scientific research endeavors.","However, the inherent limitations of traditional neural networks arise due to their relatively fixed network structures and activation functions.","1, The type of activation function is single and relatively fixed, which leads to poor \"unit representation ability\" of the network, and it is often used to solve simple problems with very complex networks; 2, the network structure is not adaptive, it is easy to cause network structure redundant or insufficient.","To address the aforementioned issues, this study proposes a novel neural network called X-Net.","By utilizing our designed Alternating Backpropagation mechanism, X-Net dynamically selects appropriate activation functions based on derivative information during training to enhance the network's representation capability for specific tasks.","Simultaneously, it accurately adjusts the network structure at the neuron level to accommodate tasks of varying complexities and reduce computational costs.","Through a series of experiments, we demonstrate the dual advantages of X-Net in terms of reducing model size and improving representation power.","Specifically, in terms of the number of parameters, X-Net is only 3$\\%$ of baselines on average, and only 1.4$\\%$ under some tasks.","In terms of representation ability, X-Net can achieve an average $R^2$=0.985 on the fitting task by only optimizing the activation function without introducing any parameters.","Finally, we also tested the ability of X-Net to help scientific discovery on data from multiple disciplines such as society, energy, environment, and aerospace, and achieved concise and good results."],"url":"http://arxiv.org/abs/2401.01772v1"}
{"created":"2024-01-03 14:28:55","title":"Cross-target Stance Detection by Exploiting Target Analytical Perspectives","abstract":"Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target. One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets. However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge. In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge. First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM). Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments results demonstrate the superiority of MPPT against the state-of-the-art baseline methods.","sentences":["Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target.","One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets.","However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge.","In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge.","First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM).","Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor.","Extensive experiments results demonstrate the superiority of MPPT against the state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2401.01761v2"}
{"created":"2024-01-03 14:19:04","title":"Fuzzy Logic Controller Design for Mobile Robot Outdoor Navigation","abstract":"Many researchers around the world are researching to get control solutions that enhance robots' ability to navigate in dynamic environments autonomously. However, until these days robots have limited capability and many navigation tasks on Earth and other planets have been difficult so far. This paperwork presents the development of a control system for a differential drive-wheeled mobile robot that autonomously controls its position, heading, and speed based on destination information given and surrounding data gathered through mounted proximity and GPS sensors. The intelligence of this control system is implemented by using a fuzzy logic algorithm which is a very powerful tool to handle un-modeled systems like the dynamically changing environment dealt with in this research. The fuzzy controller is used to address the problems associated with navigation in an obstacle-strewn environment. Such issues include position estimation, path planning, and obstacle avoidance. In this study modeling, design, and simulation of the system have been done. The simulation result shows that the developed mobile robot travels successfully from any location to the destination location without colliding with obstacles.","sentences":["Many researchers around the world are researching to get control solutions that enhance robots' ability to navigate in dynamic environments autonomously.","However, until these days robots have limited capability and many navigation tasks on Earth and other planets have been difficult so far.","This paperwork presents the development of a control system for a differential drive-wheeled mobile robot that autonomously controls its position, heading, and speed based on destination information given and surrounding data gathered through mounted proximity and GPS sensors.","The intelligence of this control system is implemented by using a fuzzy logic algorithm which is a very powerful tool to handle un-modeled systems like the dynamically changing environment dealt with in this research.","The fuzzy controller is used to address the problems associated with navigation in an obstacle-strewn environment.","Such issues include position estimation, path planning, and obstacle avoidance.","In this study modeling, design, and simulation of the system have been done.","The simulation result shows that the developed mobile robot travels successfully from any location to the destination location without colliding with obstacles."],"url":"http://arxiv.org/abs/2401.01756v1"}
{"created":"2024-01-03 13:57:09","title":"Few-shot Image Generation via Information Transfer from the Built Geodesic Surface","abstract":"Images generated by most of generative models trained with limited data often exhibit deficiencies in either fidelity, diversity, or both. One effective solution to address the limitation is few-shot generative model adaption. However, the type of approaches typically rely on a large-scale pre-trained model, serving as a source domain, to facilitate information transfer to the target domain. In this paper, we propose a method called Information Transfer from the Built Geodesic Surface (ITBGS), which contains two module: Feature Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization (I\\&R). With the FAGS module, a pseudo-source domain is created by projecting image features from the training dataset into the Pre-Shape Space, subsequently generating new features on the Geodesic surface. Thus, no pre-trained models is needed for the adaption process during the training of generative models with FAGS. I\\&R module are introduced for supervising the interpolated images and regularizing their relative distances, respectively, to further enhance the quality of generated images. Through qualitative and quantitative experiments, we demonstrate that the proposed method consistently achieves optimal or comparable results across a diverse range of semantically distinct datasets, even in extremely few-shot scenarios.","sentences":["Images generated by most of generative models trained with limited data often exhibit deficiencies in either fidelity, diversity, or both.","One effective solution to address the limitation is few-shot generative model adaption.","However, the type of approaches typically rely on a large-scale pre-trained model, serving as a source domain, to facilitate information transfer to the target domain.","In this paper, we propose a method called Information Transfer from the Built Geodesic Surface (ITBGS), which contains two module: Feature Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization (I\\&R).","With the FAGS module, a pseudo-source domain is created by projecting image features from the training dataset into the Pre-Shape Space, subsequently generating new features on the Geodesic surface.","Thus, no pre-trained models is needed for the adaption process during the training of generative models with FAGS.","I\\&R module are introduced for supervising the interpolated images and regularizing their relative distances, respectively, to further enhance the quality of generated images.","Through qualitative and quantitative experiments, we demonstrate that the proposed method consistently achieves optimal or comparable results across a diverse range of semantically distinct datasets, even in extremely few-shot scenarios."],"url":"http://arxiv.org/abs/2401.01749v1"}
{"created":"2024-01-03 13:37:01","title":"Minimizing the Weighted Number of Tardy Jobs is W[1]-hard","abstract":"We consider the $1||\\sum w_J U_j$ problem, the problem of minimizing the weighted number of tardy jobs on a single machine. This problem is one of the most basic and fundamental problems in scheduling theory, with several different applications both in theory and practice. We prove that $1||\\sum w_J U_j$ is W[1]-hard with respect to the number $p_{\\#}$ of different processing times in the input, as well as with respect to the number $w_{\\#}$ of different weights in the input. This, along with previous work, provides a complete picture for $1||\\sum w_J U_j$ from the perspective of parameterized complexity, as well as almost tight complexity bounds for the problem under the Exponential Time Hypothesis (ETH).","sentences":["We consider the $1||\\sum w_J U_j$ problem, the problem of minimizing the weighted number of tardy jobs on a single machine.","This problem is one of the most basic and fundamental problems in scheduling theory, with several different applications both in theory and practice.","We prove that $1||\\sum w_J U_j$ is W[1]-hard with respect to the number $p_{\\#}$ of different processing times in the input, as well as with respect to the number $w_{\\#}$ of different weights in the input.","This, along with previous work, provides a complete picture for $1||\\sum w_J U_j$ from the perspective of parameterized complexity, as well as almost tight complexity bounds for the problem under the Exponential Time Hypothesis (ETH)."],"url":"http://arxiv.org/abs/2401.01740v1"}
{"created":"2024-01-03 13:16:38","title":"Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data","abstract":"Assistive robots should be able to wash, fold or iron clothes. However, due to the variety, deformability and self-occlusions of clothes, creating general-purpose robot systems for cloth manipulation is challenging. Synthetic data is a promising direction to improve generalization, though its usability is often limited by the sim-to-real gap. To advance the use of synthetic data for cloth manipulation and to enable tasks such as robotic folding, we present a synthetic data pipeline to train keypoint detectors for almost flattened cloth items. To test its performance, we have also collected a real-world dataset. We train detectors for both T-shirts, towels and shorts and obtain an average precision of 64.3%. Fine-tuning on real-world data improves performance to 74.2%. Additional insight is provided by discussing various failure modes of the keypoint detectors and by comparing different approaches to obtain cloth meshes and materials. We also quantify the remaining sim-to-real gap and argue that further improvements to the fidelity of cloth assets will be required to further reduce this gap. The code, dataset and trained models are available online.","sentences":["Assistive robots should be able to wash, fold or iron clothes.","However, due to the variety, deformability and self-occlusions of clothes, creating general-purpose robot systems for cloth manipulation is challenging.","Synthetic data is a promising direction to improve generalization, though its usability is often limited by the sim-to-real gap.","To advance the use of synthetic data for cloth manipulation and to enable tasks such as robotic folding, we present a synthetic data pipeline to train keypoint detectors for almost flattened cloth items.","To test its performance, we have also collected a real-world dataset.","We train detectors for both T-shirts, towels and shorts and obtain an average precision of 64.3%.","Fine-tuning on real-world data improves performance to 74.2%.","Additional insight is provided by discussing various failure modes of the keypoint detectors and by comparing different approaches to obtain cloth meshes and materials.","We also quantify the remaining sim-to-real gap and argue that further improvements to the fidelity of cloth assets will be required to further reduce this gap.","The code, dataset and trained models are available online."],"url":"http://arxiv.org/abs/2401.01734v1"}
{"created":"2024-01-03 13:12:04","title":"Investigating the Suitability of Concept Drift Detection for Detecting Leakages in Water Distribution Networks","abstract":"Leakages are a major risk in water distribution networks as they cause water loss and increase contamination risks. Leakage detection is a difficult task due to the complex dynamics of water distribution networks. In particular, small leakages are hard to detect. From a machine-learning perspective, leakages can be modeled as concept drift. Thus, a wide variety of drift detection schemes seems to be a suitable choice for detecting leakages. In this work, we explore the potential of model-loss-based and distribution-based drift detection methods to tackle leakage detection. We additionally discuss the issue of temporal dependencies in the data and propose a way to cope with it when applying distribution-based detection. We evaluate different methods systematically for leakages of different sizes and detection times. Additionally, we propose a first drift-detection-based technique for localizing leakages.","sentences":["Leakages are a major risk in water distribution networks as they cause water loss and increase contamination risks.","Leakage detection is a difficult task due to the complex dynamics of water distribution networks.","In particular, small leakages are hard to detect.","From a machine-learning perspective, leakages can be modeled as concept drift.","Thus, a wide variety of drift detection schemes seems to be a suitable choice for detecting leakages.","In this work, we explore the potential of model-loss-based and distribution-based drift detection methods to tackle leakage detection.","We additionally discuss the issue of temporal dependencies in the data and propose a way to cope with it when applying distribution-based detection.","We evaluate different methods systematically for leakages of different sizes and detection times.","Additionally, we propose a first drift-detection-based technique for localizing leakages."],"url":"http://arxiv.org/abs/2401.01733v1"}
{"created":"2024-01-03 13:07:07","title":"Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices","abstract":"Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively execute global parameter averaging across all clusters. We have framed our asynchronous SGD loss function as a block structured optimization problem with delayed updates and derived an optimal convergence rate of $O\\left(\\frac{1}{\\sqrt{K}}\\right)$. We further discuss linear speedup with respect to the number of participating clusters and the bound on the staleness parameter.","sentences":["Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets.","This trend is expected to continue.","However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales.","This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics.","Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model.","These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively execute global parameter averaging across all clusters.","We have framed our asynchronous SGD loss function as a block structured optimization problem with delayed updates and derived an optimal convergence rate of $O\\left(\\frac{1}{\\sqrt{K}}\\right)$. We further discuss linear speedup with respect to the number of participating clusters and the bound on the staleness parameter."],"url":"http://arxiv.org/abs/2401.01728v1"}
{"created":"2024-01-03 12:55:24","title":"Limited Feedback on Measurements: Sharing a Codebook or a Generative Model?","abstract":"Discrete Fourier transform (DFT) codebook-based solutions are well-established for limited feedback schemes in frequency division duplex (FDD) systems. In recent years, data-aided solutions have been shown to achieve higher performance, enabled by the adaptivity of the feedback scheme to the propagation environment of the base station (BS) cell. In particular, a versatile limited feedback scheme utilizing Gaussian mixture models (GMMs) was recently introduced. The scheme supports multi-user communications, exhibits low complexity, supports parallelization, and offers significant flexibility concerning various system parameters. Conceptually, a GMM captures environment knowledge and is subsequently transferred to the mobile terminals (MTs) for online inference of feedback information. Afterward, the BS designs precoders using either directional information or a generative modeling-based approach. A major shortcoming of recent works is that the assessed system performance is only evaluated through synthetic simulation data that is generally unable to fully characterize the features of real-world environments. It raises the question of how the GMM-based feedback scheme performs on real-world measurement data, especially compared to the well-established DFT-based solution. Our experiments reveal that the GMM-based feedback scheme tremendously improves the system performance measured in terms of sum-rate, allowing to deploy systems with fewer pilots or feedback bits.","sentences":["Discrete Fourier transform (DFT) codebook-based solutions are well-established for limited feedback schemes in frequency division duplex (FDD) systems.","In recent years, data-aided solutions have been shown to achieve higher performance, enabled by the adaptivity of the feedback scheme to the propagation environment of the base station (BS) cell.","In particular, a versatile limited feedback scheme utilizing Gaussian mixture models (GMMs) was recently introduced.","The scheme supports multi-user communications, exhibits low complexity, supports parallelization, and offers significant flexibility concerning various system parameters.","Conceptually, a GMM captures environment knowledge and is subsequently transferred to the mobile terminals (MTs) for online inference of feedback information.","Afterward, the BS designs precoders using either directional information or a generative modeling-based approach.","A major shortcoming of recent works is that the assessed system performance is only evaluated through synthetic simulation data that is generally unable to fully characterize the features of real-world environments.","It raises the question of how the GMM-based feedback scheme performs on real-world measurement data, especially compared to the well-established DFT-based solution.","Our experiments reveal that the GMM-based feedback scheme tremendously improves the system performance measured in terms of sum-rate, allowing to deploy systems with fewer pilots or feedback bits."],"url":"http://arxiv.org/abs/2401.01721v1"}
{"created":"2024-01-03 12:05:38","title":"Patterns of Persistence and Diffusibility across World's Languages","abstract":"Language similarities can be caused by genetic relatedness, areal contact, universality, or chance. Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored. In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility). We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages. We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones. Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another. Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics.","sentences":["Language similarities can be caused by genetic relatedness, areal contact, universality, or chance.","Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored.","In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility).","We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages.","We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones.","Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another.","Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics."],"url":"http://arxiv.org/abs/2401.01698v1"}
{"created":"2024-01-03 11:54:48","title":"AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with Detail-Preserving Model-based Deep Learning","abstract":"Deep learning has shown great potential in accelerating diffusion tensor imaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise and detail loss in reconstructing the DTI-derived parametric maps especially when sparsely sampled q-space data are used. This paper proposes a novel method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to facilitate fast and accurate DTI with only six measurements. AID-DTI is equipped with a newly designed Singular Value Decomposition (SVD)-based regularizer, which can effectively capture fine details while suppressing noise during network training. Experimental results on Human Connectome Project (HCP) data consistently demonstrate that the proposed method estimates DTI parameter maps with fine-grained details and outperforms three state-of-the-art methods both quantitatively and qualitatively.","sentences":["Deep learning has shown great potential in accelerating diffusion tensor imaging (DTI).","Nevertheless, existing methods tend to suffer from Rician noise and detail loss in reconstructing the DTI-derived parametric maps especially when sparsely sampled q-space data are used.","This paper proposes a novel method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to facilitate fast and accurate DTI with only six measurements.","AID-DTI is equipped with a newly designed Singular Value Decomposition (SVD)-based regularizer, which can effectively capture fine details while suppressing noise during network training.","Experimental results on Human Connectome Project (HCP) data consistently demonstrate that the proposed method estimates DTI parameter maps with fine-grained details and outperforms three state-of-the-art methods both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2401.01693v1"}
{"created":"2024-01-03 11:49:07","title":"Zero-shot Active Learning Using Self Supervised Learning","abstract":"Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.","sentences":["Deep learning algorithms are often said to be data hungry.","The performance of such algorithms generally improve as more and more annotated data is fed into the model.","While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task.","Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget.","In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process.","We aim to leverage self-supervised learnt features for the task of Active Learning.","The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation."],"url":"http://arxiv.org/abs/2401.01690v1"}
{"created":"2024-01-03 10:47:20","title":"Simultaneous q-Space Sampling Optimization and Reconstruction for Fast and High-fidelity Diffusion Magnetic Resonance Imaging","abstract":"Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the noninvasive investigation of tissue microstructural properties and structural connectivity in the \\textit{in vivo} human brain. However, to effectively capture the intricate characteristics of water diffusion at various directions and scales, it is important to employ comprehensive q-space sampling. Unfortunately, this requirement leads to long scan times, limiting the clinical applicability of dMRI. To address this challenge, we propose SSOR, a Simultaneous q-Space sampling Optimization and Reconstruction framework. We jointly optimize a subset of q-space samples using a continuous representation of spherical harmonic functions and a reconstruction network. Additionally, we integrate the unique properties of diffusion magnetic resonance imaging (dMRI) in both the q-space and image domains by applying $l1$-norm and total-variation regularization. The experiments conducted on HCP data demonstrate that SSOR has promising strengths both quantitatively and qualitatively and exhibits robustness to noise.","sentences":["Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the noninvasive investigation of tissue microstructural properties and structural connectivity in the \\textit{in vivo} human brain.","However, to effectively capture the intricate characteristics of water diffusion at various directions and scales, it is important to employ comprehensive q-space sampling.","Unfortunately, this requirement leads to long scan times, limiting the clinical applicability of dMRI.","To address this challenge, we propose SSOR, a Simultaneous q-Space sampling Optimization and Reconstruction framework.","We jointly optimize a subset of q-space samples using a continuous representation of spherical harmonic functions and a reconstruction network.","Additionally, we integrate the unique properties of diffusion magnetic resonance imaging (dMRI) in both the q-space and image domains by applying $l1$-norm and total-variation regularization.","The experiments conducted on HCP data demonstrate that SSOR has promising strengths both quantitatively and qualitatively and exhibits robustness to noise."],"url":"http://arxiv.org/abs/2401.01662v1"}
{"created":"2024-01-03 10:18:55","title":"Near Real-Time Data-Driven Control of Virtual Reality Traffic in Open Radio Access Network","abstract":"In mobile networks, Open Radio Access Network (ORAN) provides a framework for implementing network slicing that interacts with the resources at the lower layers. Both monitoring and Radio Access Network (RAN) control is feasible for both 4G and 5G systems. In this work, we consider how data-driven resource allocation in a 4G context can enable adaptive slice allocation to steer the experienced latency of Virtual Reality (VR) traffic towards a requested latency. We develop an xApp for the near real-time RAN Intelligent Controller (RIC) that embeds a heuristic algorithm for latency control, aiming to: (1) maintain latency of a VR stream around a requested value; and (2) improve the available RAN allocation to offer higher bit rate to another user. We have experimentally demonstrated the proposed approach in an ORAN testbed. Our results show that the data-driven approach can dynamically follow the variation of the traffic load while satisfying the required latency. This results in 15.8% more resources to secondary users than a latency-equivalent static allocation.","sentences":["In mobile networks, Open Radio Access Network (ORAN) provides a framework for implementing network slicing that interacts with the resources at the lower layers.","Both monitoring and Radio Access Network (RAN) control is feasible for both 4G and 5G systems.","In this work, we consider how data-driven resource allocation in a 4G context can enable adaptive slice allocation to steer the experienced latency of Virtual Reality (VR) traffic towards a requested latency.","We develop an xApp for the near real-time RAN Intelligent Controller (RIC) that embeds a heuristic algorithm for latency control, aiming to: (1) maintain latency of a VR stream around a requested value; and (2) improve the available RAN allocation to offer higher bit rate to another user.","We have experimentally demonstrated the proposed approach in an ORAN testbed.","Our results show that the data-driven approach can dynamically follow the variation of the traffic load while satisfying the required latency.","This results in 15.8% more resources to secondary users than a latency-equivalent static allocation."],"url":"http://arxiv.org/abs/2401.01652v1"}
{"created":"2024-01-03 10:07:11","title":"De-Confusing Pseudo-Labels in Source-Free Domain Adaptation","abstract":"Source-free domain adaptation (SFDA) aims to transfer knowledge learned from a source domain to an unlabeled target domain, where the source data is unavailable during adaptation. Existing approaches for SFDA focus on self-training usually including well-established entropy minimization and pseudo-labeling techniques. Recent work suggested a co-learning strategy to improve the quality of the generated target pseudo-labels using robust pretrained networks such as Swin-B. However, since the generated pseudo-labels depend on the source model, they may be noisy due to domain shift. In this paper, we view SFDA from the perspective of label noise learning and learn to de-confuse the pseudo-labels. More specifically, we learn a noise transition matrix of the pseudo-labels to capture the label corruption of each class and learn the underlying true label distribution. Estimating the noise transition matrix enables a better true class-posterior estimation results with better prediction accuracy. We demonstrate the effectiveness of our approach applied with several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art results on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome.","sentences":["Source-free domain adaptation (SFDA) aims to transfer knowledge learned from a source domain to an unlabeled target domain, where the source data is unavailable during adaptation.","Existing approaches for SFDA focus on self-training usually including well-established entropy minimization and pseudo-labeling techniques.","Recent work suggested a co-learning strategy to improve the quality of the generated target pseudo-labels using robust pretrained networks such as Swin-B. However, since the generated pseudo-labels depend on the source model, they may be noisy due to domain shift.","In this paper, we view SFDA from the perspective of label noise learning and learn to de-confuse the pseudo-labels.","More specifically, we learn a noise transition matrix of the pseudo-labels to capture the label corruption of each class and learn the underlying true label distribution.","Estimating the noise transition matrix enables a better true class-posterior estimation results with better prediction accuracy.","We demonstrate the effectiveness of our approach applied with several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art results on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome."],"url":"http://arxiv.org/abs/2401.01650v1"}
{"created":"2024-01-03 09:39:36","title":"Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction","abstract":"Multimodal learning significantly benefits cancer survival prediction, especially the integration of pathological images and genomic data. Despite advantages of multimodal learning for cancer survival prediction, massive redundancy in multimodal data prevents it from extracting discriminative and compact information: (1) An extensive amount of intra-modal task-unrelated information blurs discriminability, especially for gigapixel whole slide images (WSIs) with many patches in pathology and thousands of pathways in genomic data, leading to an ``intra-modal redundancy\" issue. (2) Duplicated information among modalities dominates the representation of multimodal data, which makes modality-specific information prone to being ignored, resulting in an ``inter-modal redundancy\" issue. To address these, we propose a new framework, Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and Prototypical Information Disentanglement (PID) module for inter-modal redundancy. Specifically, a variant of information bottleneck, PIB, is proposed to model prototypes approximating a bunch of instances for different risk levels, which can be used for selection of discriminative instances within modality. PID module decouples entangled multimodal data into compact distinct components: modality-common and modality-specific knowledge, under the guidance of the joint prototypical distribution. Extensive experiments on five cancer benchmark datasets demonstrated our superiority over other methods.","sentences":["Multimodal learning significantly benefits cancer survival prediction, especially the integration of pathological images and genomic data.","Despite advantages of multimodal learning for cancer survival prediction, massive redundancy in multimodal data prevents it from extracting discriminative and compact information: (1) An extensive amount of intra-modal task-unrelated information blurs discriminability, especially for gigapixel whole slide images (WSIs) with many patches in pathology and thousands of pathways in genomic data, leading to an ``intra-modal redundancy\" issue.","(2) Duplicated information among modalities dominates the representation of multimodal data, which makes modality-specific information prone to being ignored, resulting in an ``inter-modal redundancy\" issue.","To address these, we propose a new framework, Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and Prototypical Information Disentanglement (PID) module for inter-modal redundancy.","Specifically, a variant of information bottleneck, PIB, is proposed to model prototypes approximating a bunch of instances for different risk levels, which can be used for selection of discriminative instances within modality.","PID module decouples entangled multimodal data into compact distinct components: modality-common and modality-specific knowledge, under the guidance of the joint prototypical distribution.","Extensive experiments on five cancer benchmark datasets demonstrated our superiority over other methods."],"url":"http://arxiv.org/abs/2401.01646v1"}
{"created":"2024-01-03 09:32:48","title":"Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences","abstract":"Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly improves value detection rate at high precision thresholds and transfers well to out-of-domain distributions.","sentences":["Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction.","Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data.","Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions.","In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions.","Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks.","We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets.","The embedding model significantly improves value detection rate at high precision thresholds and transfers well to out-of-domain distributions."],"url":"http://arxiv.org/abs/2401.01641v2"}
{"created":"2024-01-03 09:31:43","title":"Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data","abstract":"Self-supervised learning (SSL) has become the de facto training paradigm of large models where pre-training is followed by supervised fine-tuning using domain-specific data and labels. Hypothesizing that SSL models would learn more generic, hence less biased, representations, this study explores the impact of pre-training and fine-tuning strategies on fairness (i.e., performing equally on different demographic breakdowns). Motivated by human-centric applications on real-world timeseries data, we interpret inductive biases on the model, layer, and metric levels by systematically comparing SSL models to their supervised counterparts. Our findings demonstrate that SSL has the capacity to achieve performance on par with supervised methods while significantly enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1% loss in performance through self-supervision. Ultimately, this work underscores SSL's potential in human-centric computing, particularly high-stakes, data-scarce application domains like healthcare.","sentences":["Self-supervised learning (SSL) has become the de facto training paradigm of large models where pre-training is followed by supervised fine-tuning using domain-specific data and labels.","Hypothesizing that SSL models would learn more generic, hence less biased, representations, this study explores the impact of pre-training and fine-tuning strategies on fairness (i.e., performing equally on different demographic breakdowns).","Motivated by human-centric applications on real-world timeseries data, we interpret inductive biases on the model, layer, and metric levels by systematically comparing SSL models to their supervised counterparts.","Our findings demonstrate that SSL has the capacity to achieve performance on par with supervised methods while significantly enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1% loss in performance through self-supervision.","Ultimately, this work underscores SSL's potential in human-centric computing, particularly high-stakes, data-scarce application domains like healthcare."],"url":"http://arxiv.org/abs/2401.01640v1"}
{"created":"2024-01-03 09:03:30","title":"Synthetic Data in AI: Challenges, Applications, and Ethical Implications","abstract":"In the rapidly evolving field of artificial intelligence, the creation and utilization of synthetic datasets have become increasingly significant. This report delves into the multifaceted aspects of synthetic data, particularly emphasizing the challenges and potential biases these datasets may harbor. It explores the methodologies behind synthetic data generation, spanning traditional statistical models to advanced deep learning techniques, and examines their applications across diverse domains. The report also critically addresses the ethical considerations and legal implications associated with synthetic datasets, highlighting the urgent need for mechanisms to ensure fairness, mitigate biases, and uphold ethical standards in AI development.","sentences":["In the rapidly evolving field of artificial intelligence, the creation and utilization of synthetic datasets have become increasingly significant.","This report delves into the multifaceted aspects of synthetic data, particularly emphasizing the challenges and potential biases these datasets may harbor.","It explores the methodologies behind synthetic data generation, spanning traditional statistical models to advanced deep learning techniques, and examines their applications across diverse domains.","The report also critically addresses the ethical considerations and legal implications associated with synthetic datasets, highlighting the urgent need for mechanisms to ensure fairness, mitigate biases, and uphold ethical standards in AI development."],"url":"http://arxiv.org/abs/2401.01629v1"}
{"created":"2024-01-03 08:54:56","title":"On the Expressive Power of Graph Neural Networks","abstract":"The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.   While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph structures, relating this to the graph isomorphism test. Besides, a GNN's ability to compute graph properties such as graph moments has been suggested as another form of expressiveness. All of these different definitions are complementary and have yielded different recommendations for GNN architecture choices. In this paper, we would like to give an overview of the notion of \"expressive power\" of GNNs and provide some valuable insights regarding the design choices of GNNs.","sentences":["The study of Graph Neural Networks has received considerable interest in the past few years.","By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine.","The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification.","However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.   ","While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives.","Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs.","Another direction focuses on the extent to which GNNs can distinguish between different graph structures, relating this to the graph isomorphism test.","Besides, a GNN's ability to compute graph properties such as graph moments has been suggested as another form of expressiveness.","All of these different definitions are complementary and have yielded different recommendations for GNN architecture choices.","In this paper, we would like to give an overview of the notion of \"expressive power\" of GNNs and provide some valuable insights regarding the design choices of GNNs."],"url":"http://arxiv.org/abs/2401.01626v1"}
{"created":"2024-01-03 08:51:18","title":"SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks","abstract":"Anomaly detection on attributed networks aims to find the nodes whose behaviors are significantly different from other majority nodes. Generally, network data contains information about relationships between entities, and the anomaly is usually embodied in these relationships. Therefore, how to comprehensively model complex interaction patterns in networks is still a major focus. It can be observed that anomalies in networks violate the homophily assumption. However, most existing studies only considered this phenomenon obliquely rather than explicitly. Besides, the node representation of normal entities can be perturbed easily by the noise relationships introduced by anomalous nodes. To address the above issues, we present a novel contrastive learning framework for anomaly detection on attributed networks, \\textbf{SCALA}, aiming to improve the embedding quality of the network and provide a new measurement of qualifying the anomaly score for each node by introducing sparsification into the conventional method. Extensive experiments are conducted on five benchmark real-world datasets and the results show that SCALA consistently outperforms all baseline methods significantly.","sentences":["Anomaly detection on attributed networks aims to find the nodes whose behaviors are significantly different from other majority nodes.","Generally, network data contains information about relationships between entities, and the anomaly is usually embodied in these relationships.","Therefore, how to comprehensively model complex interaction patterns in networks is still a major focus.","It can be observed that anomalies in networks violate the homophily assumption.","However, most existing studies only considered this phenomenon obliquely rather than explicitly.","Besides, the node representation of normal entities can be perturbed easily by the noise relationships introduced by anomalous nodes.","To address the above issues, we present a novel contrastive learning framework for anomaly detection on attributed networks, \\textbf{SCALA}, aiming to improve the embedding quality of the network and provide a new measurement of qualifying the anomaly score for each node by introducing sparsification into the conventional method.","Extensive experiments are conducted on five benchmark real-world datasets and the results show that SCALA consistently outperforms all baseline methods significantly."],"url":"http://arxiv.org/abs/2401.01625v1"}
{"created":"2024-01-03 08:41:10","title":"Several new classes of MDS symbol-pair codes derived from matrix-product codes","abstract":"In order to correct the pair-errors generated during the transmission of modern high-density data storage that the outputs of the channels consist of overlapping pairs of symbols, a new coding scheme named symbol-pair code is proposed. The error-correcting capability of the symbol-pair code is determined by its minimum symbol-pair distance. For such codes, the larger the minimum symbol-pair distance, the better. It is a challenging task to construct symbol-pair codes with optimal parameters, especially, maximum-distance-separable (MDS) symbol-pair codes. In this paper, the permutation equivalence codes of matrix-product codes with underlying matrixes of orders 3 and 4 are used to extend the minimum symbol-pair distance, and six new classes of MDS symbol-pair codes are derived.","sentences":["In order to correct the pair-errors generated during the transmission of modern high-density data storage that the outputs of the channels consist of overlapping pairs of symbols, a new coding scheme named symbol-pair code is proposed.","The error-correcting capability of the symbol-pair code is determined by its minimum symbol-pair distance.","For such codes, the larger the minimum symbol-pair distance, the better.","It is a challenging task to construct symbol-pair codes with optimal parameters, especially, maximum-distance-separable (MDS) symbol-pair codes.","In this paper, the permutation equivalence codes of matrix-product codes with underlying matrixes of orders 3 and 4 are used to extend the minimum symbol-pair distance, and six new classes of MDS symbol-pair codes are derived."],"url":"http://arxiv.org/abs/2401.01619v1"}
{"created":"2024-01-03 07:59:17","title":"Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning","abstract":"Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered. Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods. Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session. To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach. Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE. When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge. Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is publicly available at https://github.com/1170300714/LP-DiF.","sentences":["Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered.","Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF).","We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods.","Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session.","To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach.","Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE.","When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge.","Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL.","Code is publicly available at https://github.com/1170300714/LP-DiF."],"url":"http://arxiv.org/abs/2401.01598v1"}
{"created":"2024-01-03 07:58:25","title":"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries","abstract":"In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries. This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care. Our dataset, code, and pre-trained models will be made publicly available.","sentences":["In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making.","Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues.","Also prior works in the area of medical question summarisation have been limited to the English language.","This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting.","To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids.","This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective.","We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task.","By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries.","This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care.","Our dataset, code, and pre-trained models will be made publicly available."],"url":"http://arxiv.org/abs/2401.01596v1"}
{"created":"2024-01-03 07:54:13","title":"MLIP: Medical Language-Image Pre-training with Masked Local Representation Learning","abstract":"Existing contrastive language-image pre-training aims to learn a joint representation by matching abundant image-text pairs. However, the number of image-text pairs in medical datasets is usually orders of magnitude smaller than that in natural datasets. Besides, medical image-text pairs often involve numerous complex fine-grained correspondences. This paper aims to enhance the data efficiency by introducing multiple-to-multiple local relationship modeling to capture denser supervisions. More specifically, we propose a Medical Language-Image Pre-training (MLIP) framework, which exploits the limited image-text medical data more efficiently through patch-sentence matching. Furthermore, we introduce a masked contrastive learning strategy with semantic integrity estimation to reduce redundancy in images while preserving the underlying semantics. Our evaluation results show that MLIP outperforms previous work in zero/few-shot classification and few-shot segmentation tasks by a large margin.","sentences":["Existing contrastive language-image pre-training aims to learn a joint representation by matching abundant image-text pairs.","However, the number of image-text pairs in medical datasets is usually orders of magnitude smaller than that in natural datasets.","Besides, medical image-text pairs often involve numerous complex fine-grained correspondences.","This paper aims to enhance the data efficiency by introducing multiple-to-multiple local relationship modeling to capture denser supervisions.","More specifically, we propose a Medical Language-Image Pre-training (MLIP) framework, which exploits the limited image-text medical data more efficiently through patch-sentence matching.","Furthermore, we introduce a masked contrastive learning strategy with semantic integrity estimation to reduce redundancy in images while preserving the underlying semantics.","Our evaluation results show that MLIP outperforms previous work in zero/few-shot classification and few-shot segmentation tasks by a large margin."],"url":"http://arxiv.org/abs/2401.01591v1"}
{"created":"2024-01-03 07:47:22","title":"The Security and Privacy of Mobile Edge Computing: An Artificial Intelligence Perspective","abstract":"Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud computing and information technology (IT) services to be delivered at the network's edge. By shifting the load of cloud computing to individual local servers, MEC helps meet the requirements of ultralow latency, localized data processing, and extends the potential of Internet of Things (IoT) for end-users. However, the crosscutting nature of MEC and the multidisciplinary components necessary for its deployment have presented additional security and privacy concerns. Fortunately, Artificial Intelligence (AI) algorithms can cope with excessively unpredictable and complex data, which offers a distinct advantage in dealing with sophisticated and developing adversaries in the security industry. Hence, in this paper we comprehensively provide a survey of security and privacy in MEC from the perspective of AI. On the one hand, we use European Telecommunications Standards Institute (ETSI) MEC reference architecture as our based framework while merging the Software Defined Network (SDN) and Network Function Virtualization (NFV) to better illustrate a serviceable platform of MEC. On the other hand, we focus on new security and privacy issues, as well as potential solutions from the viewpoints of AI. Finally, we comprehensively discuss the opportunities and challenges associated with applying AI to MEC security and privacy as possible future research directions.","sentences":["Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud computing and information technology (IT) services to be delivered at the network's edge.","By shifting the load of cloud computing to individual local servers, MEC helps meet the requirements of ultralow latency, localized data processing, and extends the potential of Internet of Things (IoT) for end-users.","However, the crosscutting nature of MEC and the multidisciplinary components necessary for its deployment have presented additional security and privacy concerns.","Fortunately, Artificial Intelligence (AI) algorithms can cope with excessively unpredictable and complex data, which offers a distinct advantage in dealing with sophisticated and developing adversaries in the security industry.","Hence, in this paper we comprehensively provide a survey of security and privacy in MEC from the perspective of AI.","On the one hand, we use European Telecommunications Standards Institute (ETSI) MEC reference architecture as our based framework while merging the Software Defined Network (SDN) and Network Function Virtualization (NFV) to better illustrate a serviceable platform of MEC.","On the other hand, we focus on new security and privacy issues, as well as potential solutions from the viewpoints of AI.","Finally, we comprehensively discuss the opportunities and challenges associated with applying AI to MEC security and privacy as possible future research directions."],"url":"http://arxiv.org/abs/2401.01589v1"}
{"created":"2024-01-03 06:56:39","title":"CodeFuse-Query: A Data-Centric Static Code Analysis System for Large-Scale Organizations","abstract":"In the domain of large-scale software development, the demands for dynamic and multifaceted static code analysis exceed the capabilities of traditional tools. To bridge this gap, we present CodeFuse-Query, a system that redefines static code analysis through the fusion of Domain Optimized System Design and Logic Oriented Computation Design.   CodeFuse-Query reimagines code analysis as a data computation task, support scanning over 10 billion lines of code daily and more than 300 different tasks. It optimizes resource utilization, prioritizes data reusability, applies incremental code extraction, and introduces tasks types specially for Code Change, underscoring its domain-optimized design. The system's logic-oriented facet employs Datalog, utilizing a unique two-tiered schema, COREF, to convert source code into data facts. Through Godel, a distinctive language, CodeFuse-Query enables formulation of complex tasks as logical expressions, harnessing Datalog's declarative prowess.   This paper provides empirical evidence of CodeFuse-Query's transformative approach, demonstrating its robustness, scalability, and efficiency. We also highlight its real-world impact and diverse applications, emphasizing its potential to reshape the landscape of static code analysis in the context of large-scale software development.Furthermore, in the spirit of collaboration and advancing the field, our project is open-sourced and the repository is available for public access","sentences":["In the domain of large-scale software development, the demands for dynamic and multifaceted static code analysis exceed the capabilities of traditional tools.","To bridge this gap, we present CodeFuse-Query, a system that redefines static code analysis through the fusion of Domain Optimized System Design and Logic Oriented Computation Design.   ","CodeFuse-Query reimagines code analysis as a data computation task, support scanning over 10 billion lines of code daily and more than 300 different tasks.","It optimizes resource utilization, prioritizes data reusability, applies incremental code extraction, and introduces tasks types specially for Code Change, underscoring its domain-optimized design.","The system's logic-oriented facet employs Datalog, utilizing a unique two-tiered schema, COREF, to convert source code into data facts.","Through Godel, a distinctive language, CodeFuse-Query enables formulation of complex tasks as logical expressions, harnessing Datalog's declarative prowess.   ","This paper provides empirical evidence of CodeFuse-Query's transformative approach, demonstrating its robustness, scalability, and efficiency.","We also highlight its real-world impact and diverse applications, emphasizing its potential to reshape the landscape of static code analysis in the context of large-scale software development.","Furthermore, in the spirit of collaboration and advancing the field, our project is open-sourced and the repository is available for public access"],"url":"http://arxiv.org/abs/2401.01571v1"}
{"created":"2024-01-03 06:44:05","title":"Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval with Neural Rankers and Large Language Models","abstract":"We describe team ielab from CSIRO and The University of Queensland's approach to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers but to utilise Large Language Models to overcome the issue of lack of training data for such rankers. Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus. This synthetic dataset, combined with human-annotated training data from previous years, is used to train both dense and sparse retrievers based on PubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the system. To further enhance the effectiveness of our approach, we prompting GPT-4 as a TREC annotator to provide judgments on our run files. These judgments are subsequently employed to re-rank the results. This architecture tightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large Language Models, demonstrating a new approach to clinical trial retrieval.","sentences":["We describe team ielab from CSIRO and The University of Queensland's approach to the 2023 TREC Clinical Trials Track.","Our approach was to use neural rankers but to utilise Large Language Models to overcome the issue of lack of training data for such rankers.","Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus.","This synthetic dataset, combined with human-annotated training data from previous years, is used to train both dense and sparse retrievers based on PubmedBERT.","Additionally, a cross-encoder re-ranker is integrated into the system.","To further enhance the effectiveness of our approach, we prompting GPT-4 as a TREC annotator to provide judgments on our run files.","These judgments are subsequently employed to re-rank the results.","This architecture tightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large Language Models, demonstrating a new approach to clinical trial retrieval."],"url":"http://arxiv.org/abs/2401.01566v1"}
{"created":"2024-01-03 05:33:14","title":"Collaborative Perception for Connected and Autonomous Driving: Challenges, Possible Solutions and Opportunities","abstract":"Autonomous driving has attracted significant attention from both academia and industries, which is expected to offer a safer and more efficient driving system. However, current autonomous driving systems are mostly based on a single vehicle, which has significant limitations which still poses threats to driving safety. Collaborative perception with connected and autonomous vehicles (CAVs) shows a promising solution to overcoming these limitations. In this article, we first identify the challenges of collaborative perception, such as data sharing asynchrony, data volume, and pose errors. Then, we discuss the possible solutions to address these challenges with various technologies, where the research opportunities are also elaborated. Furthermore, we propose a scheme to deal with communication efficiency and latency problems, which is a channel-aware collaborative perception framework to dynamically adjust the communication graph and minimize latency, thereby improving perception performance while increasing communication efficiency. Finally, we conduct experiments to demonstrate the effectiveness of our proposed scheme.","sentences":["Autonomous driving has attracted significant attention from both academia and industries, which is expected to offer a safer and more efficient driving system.","However, current autonomous driving systems are mostly based on a single vehicle, which has significant limitations which still poses threats to driving safety.","Collaborative perception with connected and autonomous vehicles (CAVs) shows a promising solution to overcoming these limitations.","In this article, we first identify the challenges of collaborative perception, such as data sharing asynchrony, data volume, and pose errors.","Then, we discuss the possible solutions to address these challenges with various technologies, where the research opportunities are also elaborated.","Furthermore, we propose a scheme to deal with communication efficiency and latency problems, which is a channel-aware collaborative perception framework to dynamically adjust the communication graph and minimize latency, thereby improving perception performance while increasing communication efficiency.","Finally, we conduct experiments to demonstrate the effectiveness of our proposed scheme."],"url":"http://arxiv.org/abs/2401.01544v1"}
{"created":"2024-01-03 04:59:03","title":"Adversarial Machine Learning-Enabled Anonymization of OpenWiFi Data","abstract":"Data privacy and protection through anonymization is a critical issue for network operators or data owners before it is forwarded for other possible use of data. With the adoption of Artificial Intelligence (AI), data anonymization augments the likelihood of covering up necessary sensitive information; preventing data leakage and information loss. OpenWiFi networks are vulnerable to any adversary who is trying to gain access or knowledge on traffic regardless of the knowledge possessed by data owners. The odds for discovery of actual traffic information is addressed by applied conditional tabular generative adversarial network (CTGAN). CTGAN yields synthetic data; which disguises as actual data but fostering hidden acute information of actual data. In this paper, the similarity assessment of synthetic with actual data is showcased in terms of clustering algorithms followed by a comparison of performance for unsupervised cluster validation metrics. A well-known algorithm, K-means outperforms other algorithms in terms of similarity assessment of synthetic data over real data while achieving nearest scores 0.634, 23714.57, and 0.598 as Silhouette, Calinski and Harabasz and Davies Bouldin metric respectively. On exploiting a comparative analysis in validation scores among several algorithms, K-means forms the epitome of unsupervised clustering algorithms ensuring explicit usage of synthetic data at the same time a replacement for real data. Hence, the experimental results aim to show the viability of using CTGAN-generated synthetic data in lieu of publishing anonymized data to be utilized in various applications.","sentences":["Data privacy and protection through anonymization is a critical issue for network operators or data owners before it is forwarded for other possible use of data.","With the adoption of Artificial Intelligence (AI), data anonymization augments the likelihood of covering up necessary sensitive information; preventing data leakage and information loss.","OpenWiFi networks are vulnerable to any adversary who is trying to gain access or knowledge on traffic regardless of the knowledge possessed by data owners.","The odds for discovery of actual traffic information is addressed by applied conditional tabular generative adversarial network (CTGAN).","CTGAN yields synthetic data; which disguises as actual data but fostering hidden acute information of actual data.","In this paper, the similarity assessment of synthetic with actual data is showcased in terms of clustering algorithms followed by a comparison of performance for unsupervised cluster validation metrics.","A well-known algorithm, K-means outperforms other algorithms in terms of similarity assessment of synthetic data over real data while achieving nearest scores 0.634, 23714.57, and 0.598 as Silhouette, Calinski and Harabasz and Davies Bouldin metric respectively.","On exploiting a comparative analysis in validation scores among several algorithms, K-means forms the epitome of unsupervised clustering algorithms ensuring explicit usage of synthetic data at the same time a replacement for real data.","Hence, the experimental results aim to show the viability of using CTGAN-generated synthetic data in lieu of publishing anonymized data to be utilized in various applications."],"url":"http://arxiv.org/abs/2401.01542v1"}
{"created":"2024-01-03 03:40:53","title":"Poisoning Attacks against Recommender Systems: A Survey","abstract":"Modern recommender systems have seen substantial success, yet they remain vulnerable to malicious activities, notably poisoning attacks. These attacks involve injecting malicious data into the training datasets of RS, thereby compromising their integrity and manipulating recommendation outcomes for gaining illicit profits. This survey paper provides a systematic and up-to-date review of the research landscape on Poisoning Attacks against Recommendation (PAR). A novel and comprehensive taxonomy is proposed, categorizing existing PAR methodologies into three distinct categories: Component-Specific, Goal-Driven, and Capability Probing. For each category, we discuss its mechanism in detail, along with associated methods. Furthermore, this paper highlights potential future research avenues in this domain. Additionally, to facilitate and benchmark the empirical comparison of PAR, we introduce an open-source library, ARLib, which encompasses a comprehensive collection of PAR models and common datasets. The library is released at \\url{https://github.com/CoderWZW/ARLib}.","sentences":["Modern recommender systems have seen substantial success, yet they remain vulnerable to malicious activities, notably poisoning attacks.","These attacks involve injecting malicious data into the training datasets of RS, thereby compromising their integrity and manipulating recommendation outcomes for gaining illicit profits.","This survey paper provides a systematic and up-to-date review of the research landscape on Poisoning Attacks against Recommendation (PAR).","A novel and comprehensive taxonomy is proposed, categorizing existing PAR methodologies into three distinct categories: Component-Specific, Goal-Driven, and Capability Probing.","For each category, we discuss its mechanism in detail, along with associated methods.","Furthermore, this paper highlights potential future research avenues in this domain.","Additionally, to facilitate and benchmark the empirical comparison of PAR, we introduce an open-source library, ARLib, which encompasses a comprehensive collection of PAR models and common datasets.","The library is released at \\url{https://github.com/CoderWZW/ARLib}."],"url":"http://arxiv.org/abs/2401.01527v1"}
{"created":"2024-01-03 03:34:58","title":"Expected Transaction Value Optimization for Precise Marketing in FinTech Platforms","abstract":"FinTech platforms facilitated by digital payments are watching growth rapidly, which enable the distribution of mutual funds personalized to individual investors via mobile Apps. As the important intermediation of financial products investment, these platforms distribute thousands of mutual funds obtaining impressions under guaranteed delivery (GD) strategy required by fund companies. Driven by the profit from fund purchases of users, the platform aims to maximize each transaction amount of customers by promoting mutual funds to these investors who will be interested in. Different from the conversions in traditional advertising or e-commerce recommendations, the investment amount in each purchase varies greatly even for the same financial product, which provides a significant challenge for the promotion recommendation of mutual funds. In addition to predicting the click-through rate (CTR) or the conversion rate (CVR) as in traditional recommendations, it is essential for FinTech platforms to estimate the customers' purchase amount for each delivered fund and achieve an effective allocation of impressions based on the predicted results to optimize the total expected transaction value (ETV). In this paper, we propose an ETV optimized customer allocation framework (EOCA) that aims to maximize the total ETV of recommended funds, under the constraints of GD dealt with fund companies. To the best of our knowledge, it's the first attempt to solve the GD problem for financial product promotions based on customer purchase amount prediction. We conduct extensive experiments on large scale real-world datasets and online tests based on LiCaiTong, Tencent wealth management platform, to demonstrate the effectiveness of our proposed EOCA framework.","sentences":["FinTech platforms facilitated by digital payments are watching growth rapidly, which enable the distribution of mutual funds personalized to individual investors via mobile Apps.","As the important intermediation of financial products investment, these platforms distribute thousands of mutual funds obtaining impressions under guaranteed delivery (GD) strategy required by fund companies.","Driven by the profit from fund purchases of users, the platform aims to maximize each transaction amount of customers by promoting mutual funds to these investors who will be interested in.","Different from the conversions in traditional advertising or e-commerce recommendations, the investment amount in each purchase varies greatly even for the same financial product, which provides a significant challenge for the promotion recommendation of mutual funds.","In addition to predicting the click-through rate (CTR) or the conversion rate (CVR) as in traditional recommendations, it is essential for FinTech platforms to estimate the customers' purchase amount for each delivered fund and achieve an effective allocation of impressions based on the predicted results to optimize the total expected transaction value (ETV).","In this paper, we propose an ETV optimized customer allocation framework (EOCA) that aims to maximize the total ETV of recommended funds, under the constraints of GD dealt with fund companies.","To the best of our knowledge, it's the first attempt to solve the GD problem for financial product promotions based on customer purchase amount prediction.","We conduct extensive experiments on large scale real-world datasets and online tests based on LiCaiTong, Tencent wealth management platform, to demonstrate the effectiveness of our proposed EOCA framework."],"url":"http://arxiv.org/abs/2401.01525v1"}
{"created":"2024-01-03 03:14:46","title":"Quantum Leak: Timing Side-Channel Attacks on Cloud-Based Quantum Services","abstract":"Quantum computing offers significant acceleration capabilities over its classical counterpart in various application domains. Consequently, there has been substantial focus on improving quantum computing capabilities. However, to date, the security implications of these quantum computing platforms have been largely overlooked. With the emergence of cloud-based quantum computing services, it is critical to investigate the extension of classical computer security threats to the realm of quantum computing.   In this study, we investigated timing-based side-channel vulnerabilities within IBM's cloud-based quantum service. The proposed attack effectively subverts the confidentiality of the executed quantum algorithm, using a more realistic threat model compared to existing approaches. Our experimental results, conducted using IBM's quantum cloud service, demonstrate that with just 10 measurements, it is possible to identify the underlying quantum computer that executed the circuit. Moreover, when evaluated using the popular Grover circuit, we showcase the ability to leak the quantum oracle with a mere 500 measurements. These findings underline the pressing need to address timing-based vulnerabilities in quantum computing platforms and advocate for enhanced security measures to safeguard sensitive quantum algorithms and data.","sentences":["Quantum computing offers significant acceleration capabilities over its classical counterpart in various application domains.","Consequently, there has been substantial focus on improving quantum computing capabilities.","However, to date, the security implications of these quantum computing platforms have been largely overlooked.","With the emergence of cloud-based quantum computing services, it is critical to investigate the extension of classical computer security threats to the realm of quantum computing.   ","In this study, we investigated timing-based side-channel vulnerabilities within IBM's cloud-based quantum service.","The proposed attack effectively subverts the confidentiality of the executed quantum algorithm, using a more realistic threat model compared to existing approaches.","Our experimental results, conducted using IBM's quantum cloud service, demonstrate that with just 10 measurements, it is possible to identify the underlying quantum computer that executed the circuit.","Moreover, when evaluated using the popular Grover circuit, we showcase the ability to leak the quantum oracle with a mere 500 measurements.","These findings underline the pressing need to address timing-based vulnerabilities in quantum computing platforms and advocate for enhanced security measures to safeguard sensitive quantum algorithms and data."],"url":"http://arxiv.org/abs/2401.01521v1"}
{"created":"2024-01-03 03:01:29","title":"Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review","abstract":"This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologies in psychology, the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations. Researchers should responsibly use LLMs in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, the article provides a comprehensive overview of the current state of LLMs in psychology, exploring potential benefits and challenges. It serves as a call to action for researchers to leverage LLLs' advantages responsibly while addressing associated risks.","sentences":["This paper explores the frontiers of large language models (LLMs) in psychology applications.","Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions.","We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research.","It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior.","The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology.","While LLMs are essential in advancing research methodologies in psychology, the paper also cautions about their technical and ethical challenges.","There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations.","Researchers should responsibly use LLMs in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas.","Overall, the article provides a comprehensive overview of the current state of LLMs in psychology, exploring potential benefits and challenges.","It serves as a call to action for researchers to leverage LLLs' advantages responsibly while addressing associated risks."],"url":"http://arxiv.org/abs/2401.01519v1"}
{"created":"2024-01-03 02:44:02","title":"Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?","abstract":"This paper discusses the limitations of evaluating Masked Language Models (MLMs) in code completion tasks. We highlight that relying on accuracy-based measurements may lead to an overestimation of models' capabilities by neglecting the syntax rules of programming languages. To address these issues, we introduce a technique called SyntaxEval in which Syntactic Capabilities are used to enhance the evaluation of MLMs. SyntaxEval automates the process of masking elements in the model input based on their Abstract Syntax Trees (ASTs). We conducted a case study on two popular MLMs using data from GitHub repositories. Our results showed negative causal effects between the node types and MLMs' accuracy. We conclude that MLMs under study fail to predict some syntactic capabilities.","sentences":["This paper discusses the limitations of evaluating Masked Language Models (MLMs) in code completion tasks.","We highlight that relying on accuracy-based measurements may lead to an overestimation of models' capabilities by neglecting the syntax rules of programming languages.","To address these issues, we introduce a technique called SyntaxEval in which Syntactic Capabilities are used to enhance the evaluation of MLMs.","SyntaxEval automates the process of masking elements in the model input based on their Abstract Syntax Trees (ASTs).","We conducted a case study on two popular MLMs using data from GitHub repositories.","Our results showed negative causal effects between the node types and MLMs' accuracy.","We conclude that MLMs under study fail to predict some syntactic capabilities."],"url":"http://arxiv.org/abs/2401.01512v1"}
{"created":"2024-01-03 02:32:55","title":"Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model Implementation for Multicultural Enterprise","abstract":"The advent of Large Language Models has revolutionized information retrieval, ushering in a new era of expansive knowledge accessibility. While these models excel in providing open-world knowledge, effectively extracting answers in diverse linguistic environments with varying levels of literacy remains a formidable challenge. Retrieval Augmented Generation (RAG) emerges as a promising solution, bridging the gap between information availability and multilingual comprehension. However, deploying RAG models in real-world scenarios demands careful consideration of various factors. This paper addresses the critical challenges associated with implementing RAG models in multicultural environments. We delve into essential considerations, including data feeding strategies, timely updates, mitigation of hallucinations, prevention of erroneous responses, and optimization of delivery speed. Our work involves the integration of a diverse array of tools, meticulously combined to facilitate the seamless adoption of RAG models across languages and literacy levels within a multicultural organizational context. Through strategic tweaks in our approaches, we achieve not only effectiveness but also efficiency, ensuring the accelerated and accurate delivery of information in a manner that is tailored to the unique requirements of multilingual and multicultural settings.","sentences":["The advent of Large Language Models has revolutionized information retrieval, ushering in a new era of expansive knowledge accessibility.","While these models excel in providing open-world knowledge, effectively extracting answers in diverse linguistic environments with varying levels of literacy remains a formidable challenge.","Retrieval Augmented Generation (RAG) emerges as a promising solution, bridging the gap between information availability and multilingual comprehension.","However, deploying RAG models in real-world scenarios demands careful consideration of various factors.","This paper addresses the critical challenges associated with implementing RAG models in multicultural environments.","We delve into essential considerations, including data feeding strategies, timely updates, mitigation of hallucinations, prevention of erroneous responses, and optimization of delivery speed.","Our work involves the integration of a diverse array of tools, meticulously combined to facilitate the seamless adoption of RAG models across languages and literacy levels within a multicultural organizational context.","Through strategic tweaks in our approaches, we achieve not only effectiveness but also efficiency, ensuring the accelerated and accurate delivery of information in a manner that is tailored to the unique requirements of multilingual and multicultural settings."],"url":"http://arxiv.org/abs/2401.01511v1"}
{"created":"2024-01-03 02:29:34","title":"Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering","abstract":"While significant advancements have been made in video question answering (VideoQA), the potential benefits of enhancing model generalization through tailored difficulty scheduling have been largely overlooked in existing research. This paper seeks to bridge that gap by incorporating VideoQA into a curriculum learning (CL) framework that progressively trains models from simpler to more complex data. Recognizing that conventional self-paced CL methods rely on training loss for difficulty measurement, which might not accurately reflect the intricacies of video-question pairs, we introduce the concept of uncertainty-aware CL. Here, uncertainty serves as the guiding principle for dynamically adjusting the difficulty. Furthermore, we address the challenge posed by uncertainty by presenting a probabilistic modeling approach for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation graph, where the hidden representations are treated as stochastic variables. This yields two distinct types of uncertainty: one related to the inherent uncertainty in the data and another pertaining to the model's confidence. In practice, we seamlessly integrate the VideoQA model into our framework and conduct comprehensive experiments. The findings affirm that our approach not only achieves enhanced performance but also effectively quantifies uncertainty in the context of VideoQA.","sentences":["While significant advancements have been made in video question answering (VideoQA), the potential benefits of enhancing model generalization through tailored difficulty scheduling have been largely overlooked in existing research.","This paper seeks to bridge that gap by incorporating VideoQA into a curriculum learning (CL) framework that progressively trains models from simpler to more complex data.","Recognizing that conventional self-paced CL methods rely on training loss for difficulty measurement, which might not accurately reflect the intricacies of video-question pairs, we introduce the concept of uncertainty-aware CL.","Here, uncertainty serves as the guiding principle for dynamically adjusting the difficulty.","Furthermore, we address the challenge posed by uncertainty by presenting a probabilistic modeling approach for VideoQA.","Specifically, we conceptualize VideoQA as a stochastic computation graph, where the hidden representations are treated as stochastic variables.","This yields two distinct types of uncertainty: one related to the inherent uncertainty in the data and another pertaining to the model's confidence.","In practice, we seamlessly integrate the VideoQA model into our framework and conduct comprehensive experiments.","The findings affirm that our approach not only achieves enhanced performance but also effectively quantifies uncertainty in the context of VideoQA."],"url":"http://arxiv.org/abs/2401.01510v1"}
{"created":"2024-01-03 02:15:32","title":"Pontryagin Neural Operator for Solving Parametric General-Sum Differential Games","abstract":"The values of two-player general-sum differential games are viscosity solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy approximations for such games suffer from the curse of dimensionality (CoD). Alleviating CoD through physics-informed neural networks (PINN) encounters convergence issues when value discontinuity is present due to state constraints. On top of these challenges, it is often necessary to learn generalizable values and policies across a parametric space of games, e.g., for game parameter inference when information is incomplete. To address these challenges, we propose in this paper a Pontryagin-mode neural operator that outperforms existing state-of-the-art (SOTA) on safety performance across games with parametric state constraints. Our key contribution is the introduction of a costate loss defined on the discrepancy between forward and backward costate rollouts, which are computationally cheap. We show that the discontinuity of costate dynamics (in the presence of state constraints) effectively enables the learning of discontinuous values, without requiring manually supervised data as suggested by the current SOTA. More importantly, we show that the close relationship between costates and policies makes the former critical in learning feedback control policies with generalizable safety performance.","sentences":["The values of two-player general-sum differential games are viscosity solutions to Hamilton-Jacobi-Isaacs (HJI) equations.","Value and policy approximations for such games suffer from the curse of dimensionality (CoD).","Alleviating CoD through physics-informed neural networks (PINN) encounters convergence issues when value discontinuity is present due to state constraints.","On top of these challenges, it is often necessary to learn generalizable values and policies across a parametric space of games, e.g., for game parameter inference when information is incomplete.","To address these challenges, we propose in this paper a Pontryagin-mode neural operator that outperforms existing state-of-the-art (SOTA) on safety performance across games with parametric state constraints.","Our key contribution is the introduction of a costate loss defined on the discrepancy between forward and backward costate rollouts, which are computationally cheap.","We show that the discontinuity of costate dynamics (in the presence of state constraints) effectively enables the learning of discontinuous values, without requiring manually supervised data as suggested by the current SOTA.","More importantly, we show that the close relationship between costates and policies makes the former critical in learning feedback control policies with generalizable safety performance."],"url":"http://arxiv.org/abs/2401.01502v1"}
{"created":"2024-01-03 02:14:41","title":"Evaluation of automated driving system safety metrics with logged vehicle trajectory data","abstract":"Real-time safety metrics are important for the automated driving system (ADS) to assess the risk of driving situations and to assist the decision-making. Although a number of real-time safety metrics have been proposed in the literature, systematic performance evaluation of these safety metrics has been lacking. As different behavioral assumptions are adopted in different safety metrics, it is difficult to compare the safety metrics and evaluate their performance. To overcome this challenge, in this study, we propose an evaluation framework utilizing logged vehicle trajectory data, in that vehicle trajectories for both subject vehicle (SV) and background vehicles (BVs) are obtained and the prediction errors caused by behavioral assumptions can be eliminated. Specifically, we examine whether the SV is in a collision unavoidable situation at each moment, given all near-future trajectories of BVs. In this way, we level the ground for a fair comparison of different safety metrics, as a good safety metric should always alarm in advance to the collision unavoidable moment. When trajectory data from a large number of trips are available, we can systematically evaluate and compare different metrics' statistical performance. In the case study, three representative real-time safety metrics, including the time-to-collision (TTC), the PEGASUS Criticality Metric (PCM), and the Model Predictive Instantaneous Safety Metric (MPrISM), are evaluated using a large-scale simulated trajectory dataset. The proposed evaluation framework is important for researchers, practitioners, and regulators to characterize different metrics, and to select appropriate metrics for different applications. Moreover, by conducting failure analysis on moments when a safety metric failed, we can identify its potential weaknesses which are valuable for its potential refinements and improvements.","sentences":["Real-time safety metrics are important for the automated driving system (ADS) to assess the risk of driving situations and to assist the decision-making.","Although a number of real-time safety metrics have been proposed in the literature, systematic performance evaluation of these safety metrics has been lacking.","As different behavioral assumptions are adopted in different safety metrics, it is difficult to compare the safety metrics and evaluate their performance.","To overcome this challenge, in this study, we propose an evaluation framework utilizing logged vehicle trajectory data, in that vehicle trajectories for both subject vehicle (SV) and background vehicles (BVs) are obtained and the prediction errors caused by behavioral assumptions can be eliminated.","Specifically, we examine whether the SV is in a collision unavoidable situation at each moment, given all near-future trajectories of BVs.","In this way, we level the ground for a fair comparison of different safety metrics, as a good safety metric should always alarm in advance to the collision unavoidable moment.","When trajectory data from a large number of trips are available, we can systematically evaluate and compare different metrics' statistical performance.","In the case study, three representative real-time safety metrics, including the time-to-collision (TTC), the PEGASUS Criticality Metric (PCM), and the Model Predictive Instantaneous Safety Metric (MPrISM), are evaluated using a large-scale simulated trajectory dataset.","The proposed evaluation framework is important for researchers, practitioners, and regulators to characterize different metrics, and to select appropriate metrics for different applications.","Moreover, by conducting failure analysis on moments when a safety metric failed, we can identify its potential weaknesses which are valuable for its potential refinements and improvements."],"url":"http://arxiv.org/abs/2401.01501v1"}
{"created":"2024-01-03 01:58:31","title":"A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive Learning","abstract":"In terms of human-computer interaction, it is becoming more and more important to correctly understand the user's emotional state in a conversation, so the task of multimodal emotion recognition (MER) started to receive more attention. However, existing emotion classification methods usually perform classification only once. Sentences are likely to be misclassified in a single round of classification. Previous work usually ignores the similarities and differences between different morphological features in the fusion process. To address the above issues, we propose a two-stage emotion recognition model based on graph contrastive learning (TS-GCL). First, we encode the original dataset with different preprocessing modalities. Second, a graph contrastive learning (GCL) strategy is introduced for these three modal data with other structures to learn similarities and differences within and between modalities. Finally, we use MLP twice to achieve the final emotion classification. This staged classification method can help the model to better focus on different levels of emotional information, thereby improving the performance of the model. Extensive experiments show that TS-GCL has superior performance on IEMOCAP and MELD datasets compared with previous methods.","sentences":["In terms of human-computer interaction, it is becoming more and more important to correctly understand the user's emotional state in a conversation, so the task of multimodal emotion recognition (MER) started to receive more attention.","However, existing emotion classification methods usually perform classification only once.","Sentences are likely to be misclassified in a single round of classification.","Previous work usually ignores the similarities and differences between different morphological features in the fusion process.","To address the above issues, we propose a two-stage emotion recognition model based on graph contrastive learning (TS-GCL).","First, we encode the original dataset with different preprocessing modalities.","Second, a graph contrastive learning (GCL) strategy is introduced for these three modal data with other structures to learn similarities and differences within and between modalities.","Finally, we use MLP twice to achieve the final emotion classification.","This staged classification method can help the model to better focus on different levels of emotional information, thereby improving the performance of the model.","Extensive experiments show that TS-GCL has superior performance on IEMOCAP and MELD datasets compared with previous methods."],"url":"http://arxiv.org/abs/2401.01495v1"}
{"created":"2024-01-03 01:45:00","title":"Free Lunch for Federated Remote Sensing Target Fine-Grained Classification: A Parameter-Efficient Framework","abstract":"Remote Sensing Target Fine-grained Classification (TFGC) is of great significance in both military and civilian fields. Due to location differences, growth in data size, and centralized server storage constraints, these data are usually stored under different databases across regions/countries. However, privacy laws and national security concerns constrain researchers from accessing these sensitive remote sensing images for further analysis. Additionally, low-resource remote sensing devices encounter challenges in terms of communication overhead and efficiency when dealing with the ever-increasing data and model scales. To solve the above challenges, this paper proposes a novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed PRFL. The proposed framework allows each client to learn global and local knowledge to enhance the local representation of private data in environments with extreme statistical heterogeneity (non. Independent and Identically Distributed, IID). Thus, it provides highly customized models to clients with differentiated data distributions. Moreover, the framework minimizes communication overhead and improves efficiency while ensuring satisfactory performance, thereby enhancing robustness and practical applicability under resource-scarce conditions. We demonstrate the effectiveness of the proposed PRFL on the classical TFGC task by leveraging four public datasets.","sentences":["Remote Sensing Target Fine-grained Classification (TFGC) is of great significance in both military and civilian fields.","Due to location differences, growth in data size, and centralized server storage constraints, these data are usually stored under different databases across regions/countries.","However, privacy laws and national security concerns constrain researchers from accessing these sensitive remote sensing images for further analysis.","Additionally, low-resource remote sensing devices encounter challenges in terms of communication overhead and efficiency when dealing with the ever-increasing data and model scales.","To solve the above challenges, this paper proposes a novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed PRFL.","The proposed framework allows each client to learn global and local knowledge to enhance the local representation of private data in environments with extreme statistical heterogeneity (non.","Independent and Identically Distributed, IID).","Thus, it provides highly customized models to clients with differentiated data distributions.","Moreover, the framework minimizes communication overhead and improves efficiency while ensuring satisfactory performance, thereby enhancing robustness and practical applicability under resource-scarce conditions.","We demonstrate the effectiveness of the proposed PRFL on the classical TFGC task by leveraging four public datasets."],"url":"http://arxiv.org/abs/2401.01493v1"}
{"created":"2024-01-03 01:27:07","title":"A Hybrid Neural Network Model For Predicting The Nitrate Concentration In The Recirculating Aquaculture System","abstract":"This study was groundbreaking in its application of neural network models for nitrate management in the Recirculating Aquaculture System (RAS). A hybrid neural network model was proposed, which accurately predicted daily nitrate concentration and its trends using six water quality parameters. We conducted a 105-day aquaculture experiment, during which we collected 450 samples from five sets of RAS to train our model (C-L-A model) which incorporates Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and self-Attention. Furthermore, we obtained 90 samples from a standalone RAS as the testing data to evaluate the performance of the model in practical applications. The experimental results proved that the C-L-A model accurately predicted nitrate concentration in RAS and maintained good performance even with a reduced proportion of training data. We recommend using water quality parameters from the past 7 days to forecast future nitrate concentration, as this timeframe allows the model to achieve maximum generalization capability. Additionally, we compared the performance of the C-L-A model with three basic neural network models (CNN, LSTM, self-Attention) as well as three hybrid neural network models (CNN-LSTM, CNN-Attention, LSTM-Attention). The results demonstrated that the C-L-A model (R2=0.956) significantly outperformed the other neural network models (R2=0.901-0.927). Our study suggests that the utilization of neural network models, specifically the C-L-A model, could potentially assist the RAS industry in conserving resources for daily nitrate monitoring.","sentences":["This study was groundbreaking in its application of neural network models for nitrate management in the Recirculating Aquaculture System (RAS).","A hybrid neural network model was proposed, which accurately predicted daily nitrate concentration and its trends using six water quality parameters.","We conducted a 105-day aquaculture experiment, during which we collected 450 samples from five sets of RAS to train our model (C-L-A model) which incorporates Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and self-Attention.","Furthermore, we obtained 90 samples from a standalone RAS as the testing data to evaluate the performance of the model in practical applications.","The experimental results proved that the C-L-A model accurately predicted nitrate concentration in RAS and maintained good performance even with a reduced proportion of training data.","We recommend using water quality parameters from the past 7 days to forecast future nitrate concentration, as this timeframe allows the model to achieve maximum generalization capability.","Additionally, we compared the performance of the C-L-A model with three basic neural network models (CNN, LSTM, self-Attention) as well as three hybrid neural network models (CNN-LSTM, CNN-Attention, LSTM-Attention).","The results demonstrated that the C-L-A model (R2=0.956) significantly outperformed the other neural network models (R2=0.901-0.927).","Our study suggests that the utilization of neural network models, specifically the C-L-A model, could potentially assist the RAS industry in conserving resources for daily nitrate monitoring."],"url":"http://arxiv.org/abs/2401.01491v1"}
{"created":"2024-01-03 01:21:30","title":"Natural Language Processing and Multimodal Stock Price Prediction","abstract":"In the realm of financial decision-making, predicting stock prices is pivotal. Artificial intelligence techniques such as long short-term memory networks (LSTMs), support-vector machines (SVMs), and natural language processing (NLP) models are commonly employed to predict said prices. This paper utilizes stock percentage change as training data, in contrast to the traditional use of raw currency values, with a focus on analyzing publicly released news articles. The choice of percentage change aims to provide models with context regarding the significance of price fluctuations and overall price change impact on a given stock. The study employs specialized BERT natural language processing models to predict stock price trends, with a particular emphasis on various data modalities. The results showcase the capabilities of such strategies with a small natural language processing model to accurately predict overall stock trends, and highlight the effectiveness of certain data features and sector-specific data.","sentences":["In the realm of financial decision-making, predicting stock prices is pivotal.","Artificial intelligence techniques such as long short-term memory networks (LSTMs), support-vector machines (SVMs), and natural language processing (NLP) models are commonly employed to predict said prices.","This paper utilizes stock percentage change as training data, in contrast to the traditional use of raw currency values, with a focus on analyzing publicly released news articles.","The choice of percentage change aims to provide models with context regarding the significance of price fluctuations and overall price change impact on a given stock.","The study employs specialized BERT natural language processing models to predict stock price trends, with a particular emphasis on various data modalities.","The results showcase the capabilities of such strategies with a small natural language processing model to accurately predict overall stock trends, and highlight the effectiveness of certain data features and sector-specific data."],"url":"http://arxiv.org/abs/2401.01487v1"}
{"created":"2024-01-03 01:11:16","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition","abstract":"Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries from Africa, and +4.6 on the hardest classes. We further show competitive performance vs. few-shot target training, and provide insights into how descriptive knowledge captures geographical differences.","sentences":["Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context.","Class representations need to be adapted to more accurately reflect an object concept under these shifts.","In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness.","For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model.","In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies.","Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries from Africa, and +4.6 on the hardest classes.","We further show competitive performance vs. few-shot target training, and provide insights into how descriptive knowledge captures geographical differences."],"url":"http://arxiv.org/abs/2401.01482v1"}
{"created":"2024-01-03 00:22:41","title":"Demonstrating Mobile Manipulation in the Wild: A Metrics-Driven Approach","abstract":"We present our general-purpose mobile manipulation system consisting of a custom robot platform and key algorithms spanning perception and planning. To extensively test the system in the wild and benchmark its performance, we choose a grocery shopping scenario in an actual, unmodified grocery store. We derive key performance metrics from detailed robot log data collected during six week-long field tests, spread across 18 months. These objective metrics, gained from complex yet repeatable tests, drive the direction of our research efforts and let us continuously improve our system's performance. We find that thorough end-to-end system-level testing of a complex mobile manipulation system can serve as a reality-check for state-of-the-art methods in robotics. This effectively grounds robotics research efforts in real world needs and challenges, which we deem highly useful for the advancement of the field. To this end, we share our key insights and takeaways to inspire and accelerate similar system-level research projects.","sentences":["We present our general-purpose mobile manipulation system consisting of a custom robot platform and key algorithms spanning perception and planning.","To extensively test the system in the wild and benchmark its performance, we choose a grocery shopping scenario in an actual, unmodified grocery store.","We derive key performance metrics from detailed robot log data collected during six week-long field tests, spread across 18 months.","These objective metrics, gained from complex yet repeatable tests, drive the direction of our research efforts and let us continuously improve our system's performance.","We find that thorough end-to-end system-level testing of a complex mobile manipulation system can serve as a reality-check for state-of-the-art methods in robotics.","This effectively grounds robotics research efforts in real world needs and challenges, which we deem highly useful for the advancement of the field.","To this end, we share our key insights and takeaways to inspire and accelerate similar system-level research projects."],"url":"http://arxiv.org/abs/2401.01474v1"}
{"created":"2024-01-03 00:09:34","title":"Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation","abstract":"Summarization of electronic health records (EHRs) can substantially minimize 'screen time' for both patients as well as medical personnel. In recent years summarization of EHRs have employed machine learning pipelines using state of the art neural models. However, these models have produced less than adequate results that are attributed to the difficulty of obtaining sufficient annotated data for training. Moreover, the requirement to consider the entire content of an EHR in summarization has resulted in poor performance due to the fact that attention mechanisms in modern large language models (LLMs) adds a quadratic complexity in terms of the size of the input. We propose here a method that mitigates these shortcomings by combining semantic search, retrieval augmented generation (RAG) and question-answering using the latest LLMs. In our approach summarization is the extraction of answers to specific questions that are deemed important by subject-matter experts (SMEs). Our approach is quite efficient; requires minimal to no training; does not suffer from the 'hallucination' problem of LLMs; and it ensures diversity, since the summary will not have repeated content but diverse answers to specific questions.","sentences":["Summarization of electronic health records (EHRs) can substantially minimize 'screen time' for both patients as well as medical personnel.","In recent years summarization of EHRs have employed machine learning pipelines using state of the art neural models.","However, these models have produced less than adequate results that are attributed to the difficulty of obtaining sufficient annotated data for training.","Moreover, the requirement to consider the entire content of an EHR in summarization has resulted in poor performance due to the fact that attention mechanisms in modern large language models (LLMs) adds a quadratic complexity in terms of the size of the input.","We propose here a method that mitigates these shortcomings by combining semantic search, retrieval augmented generation (RAG) and question-answering using the latest LLMs.","In our approach summarization is the extraction of answers to specific questions that are deemed important by subject-matter experts (SMEs).","Our approach is quite efficient; requires minimal to no training; does not suffer from the 'hallucination' problem of LLMs; and it ensures diversity, since the summary will not have repeated content but diverse answers to specific questions."],"url":"http://arxiv.org/abs/2401.01469v1"}
{"created":"2024-01-02 23:08:49","title":"Outlier Ranking in Large-Scale Public Health Streams","abstract":"Disease control experts inspect public health data streams daily for outliers worth investigating, like those corresponding to data quality issues or disease outbreaks. However, they can only examine a few of the thousands of maximally-tied outliers returned by univariate outlier detection methods applied to large-scale public health data streams. To help experts distinguish the most important outliers from these thousands of tied outliers, we propose a new task for algorithms to rank the outputs of any univariate method applied to each of many streams. Our novel algorithm for this task, which leverages hierarchical networks and extreme value analysis, performed the best across traditional outlier detection metrics in a human-expert evaluation using public health data streams. Most importantly, experts have used our open-source Python implementation since April 2023 and report identifying outliers worth investigating 9.1x faster than their prior baseline. Other organizations can readily adapt this implementation to create rankings from the outputs of their tailored univariate methods across large-scale streams.","sentences":["Disease control experts inspect public health data streams daily for outliers worth investigating, like those corresponding to data quality issues or disease outbreaks.","However, they can only examine a few of the thousands of maximally-tied outliers returned by univariate outlier detection methods applied to large-scale public health data streams.","To help experts distinguish the most important outliers from these thousands of tied outliers, we propose a new task for algorithms to rank the outputs of any univariate method applied to each of many streams.","Our novel algorithm for this task, which leverages hierarchical networks and extreme value analysis, performed the best across traditional outlier detection metrics in a human-expert evaluation using public health data streams.","Most importantly, experts have used our open-source Python implementation since April 2023 and report identifying outliers worth investigating 9.1x faster than their prior baseline.","Other organizations can readily adapt this implementation to create rankings from the outputs of their tailored univariate methods across large-scale streams."],"url":"http://arxiv.org/abs/2401.01459v1"}
{"created":"2024-01-02 22:35:33","title":"A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook","abstract":"Autonomous driving has rapidly developed and shown promising performance with recent advances in hardware and deep learning methods. High-quality datasets are fundamental for developing reliable autonomous driving algorithms. Previous dataset surveys tried to review the datasets but either focused on a limited number or lacked detailed investigation of the characters of datasets. To this end, we present an exhaustive study of over 200 autonomous driving datasets from multiple perspectives, including sensor modalities, data size, tasks, and contextual conditions. We introduce a novel metric to evaluate the impact of each dataset, which can also be a guide for establishing new datasets. We further analyze the annotation process and quality of datasets. Additionally, we conduct an in-depth analysis of the data distribution of several vital datasets. Finally, we discuss the development trend of the future autonomous driving datasets.","sentences":["Autonomous driving has rapidly developed and shown promising performance with recent advances in hardware and deep learning methods.","High-quality datasets are fundamental for developing reliable autonomous driving algorithms.","Previous dataset surveys tried to review the datasets but either focused on a limited number or lacked detailed investigation of the characters of datasets.","To this end, we present an exhaustive study of over 200 autonomous driving datasets from multiple perspectives, including sensor modalities, data size, tasks, and contextual conditions.","We introduce a novel metric to evaluate the impact of each dataset, which can also be a guide for establishing new datasets.","We further analyze the annotation process and quality of datasets.","Additionally, we conduct an in-depth analysis of the data distribution of several vital datasets.","Finally, we discuss the development trend of the future autonomous driving datasets."],"url":"http://arxiv.org/abs/2401.01454v1"}
{"created":"2024-01-02 21:43:01","title":"Hierarchical Over-the-Air Federated Learning with Awareness of Interference and Data Heterogeneity","abstract":"When implementing hierarchical federated learning over wireless networks, scalability assurance and the ability to handle both interference and device data heterogeneity are crucial. This work introduces a learning method designed to address these challenges, along with a scalable transmission scheme that efficiently uses a single wireless resource through over-the-air computation. To provide resistance against data heterogeneity, we employ gradient aggregations. Meanwhile, the impact of interference is minimized through optimized receiver normalizing factors. For this, we model a multi-cluster wireless network using stochastic geometry, and characterize the mean squared error of the aggregation estimations as a function of the network parameters. We show that despite the interference and the data heterogeneity, the proposed scheme achieves high learning accuracy and can significantly outperform the conventional hierarchical algorithm.","sentences":["When implementing hierarchical federated learning over wireless networks, scalability assurance and the ability to handle both interference and device data heterogeneity are crucial.","This work introduces a learning method designed to address these challenges, along with a scalable transmission scheme that efficiently uses a single wireless resource through over-the-air computation.","To provide resistance against data heterogeneity, we employ gradient aggregations.","Meanwhile, the impact of interference is minimized through optimized receiver normalizing factors.","For this, we model a multi-cluster wireless network using stochastic geometry, and characterize the mean squared error of the aggregation estimations as a function of the network parameters.","We show that despite the interference and the data heterogeneity, the proposed scheme achieves high learning accuracy and can significantly outperform the conventional hierarchical algorithm."],"url":"http://arxiv.org/abs/2401.01442v1"}
{"created":"2024-01-02 21:27:43","title":"Off-Road LiDAR Intensity Based Semantic Segmentation","abstract":"LiDAR is used in autonomous driving to provide 3D spatial information and enable accurate perception in off-road environments, aiding in obstacle detection, mapping, and path planning. Learning-based LiDAR semantic segmentation utilizes machine learning techniques to automatically classify objects and regions in LiDAR point clouds. Learning-based models struggle in off-road environments due to the presence of diverse objects with varying colors, textures, and undefined boundaries, which can lead to difficulties in accurately classifying and segmenting objects using traditional geometric-based features. In this paper, we address this problem by harnessing the LiDAR intensity parameter to enhance object segmentation in off-road environments. Our approach was evaluated in the RELLIS-3D data set and yielded promising results as a preliminary analysis with improved mIoU for classes \"puddle\" and \"grass\" compared to more complex deep learning-based benchmarks. The methodology was evaluated for compatibility across both Velodyne and Ouster LiDAR systems, assuring its cross-platform applicability. This analysis advocates for the incorporation of calibrated intensity as a supplementary input, aiming to enhance the prediction accuracy of learning based semantic segmentation frameworks. https://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main","sentences":["LiDAR is used in autonomous driving to provide 3D spatial information and enable accurate perception in off-road environments, aiding in obstacle detection, mapping, and path planning.","Learning-based LiDAR semantic segmentation utilizes machine learning techniques to automatically classify objects and regions in LiDAR point clouds.","Learning-based models struggle in off-road environments due to the presence of diverse objects with varying colors, textures, and undefined boundaries, which can lead to difficulties in accurately classifying and segmenting objects using traditional geometric-based features.","In this paper, we address this problem by harnessing the LiDAR intensity parameter to enhance object segmentation in off-road environments.","Our approach was evaluated in the RELLIS-3D data set and yielded promising results as a preliminary analysis with improved mIoU for classes \"puddle\" and \"grass\" compared to more complex deep learning-based benchmarks.","The methodology was evaluated for compatibility across both Velodyne and Ouster LiDAR systems, assuring its cross-platform applicability.","This analysis advocates for the incorporation of calibrated intensity as a supplementary input, aiming to enhance the prediction accuracy of learning based semantic segmentation frameworks.","https://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main"],"url":"http://arxiv.org/abs/2401.01439v1"}
{"created":"2024-01-02 20:31:15","title":"Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference","abstract":"Pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions. Researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy. However, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images. On the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions. Especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data. To address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a deep causal generative model, which utilizes the pre-trained model and can provably sample from identifiable interventional and counterfactual distributions. Our algorithm, called Modular-DCM, uses adversarial training to learn the network weights, and to the best of our knowledge, is the first algorithm that can make use of pre-trained models and provably sample from any identifiable causal query in the presence of latent confounders with high-dimensional data. We demonstrate the utility of our algorithm using semi-synthetic and real-world datasets containing images as variables in the causal structure.","sentences":["Pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions.","Researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy.","However, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images.","On the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions.","Especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data.","To address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a deep causal generative model, which utilizes the pre-trained model and can provably sample from identifiable interventional and counterfactual distributions.","Our algorithm, called Modular-DCM, uses adversarial training to learn the network weights, and to the best of our knowledge, is the first algorithm that can make use of pre-trained models and provably sample from any identifiable causal query in the presence of latent confounders with high-dimensional data.","We demonstrate the utility of our algorithm using semi-synthetic and real-world datasets containing images as variables in the causal structure."],"url":"http://arxiv.org/abs/2401.01426v1"}
{"created":"2024-01-02 20:28:06","title":"SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset","abstract":"This paper investigates the high-level decision-making problem in highway scenarios regarding lane changing and over-taking other slower vehicles. In particular, this paper aims to improve the Travel Assist feature for automatic overtaking and lane changes on highways. About 9 million samples including lane images and other dynamic objects are collected in simulation. This data; Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this challenge. To solve this problem, an architecture called SwapTransformer is designed and implemented as an imitation learning approach on the OSHA dataset. Moreover, auxiliary tasks such as future points and car distance network predictions are proposed to aid the model in better understanding the surrounding environment. The performance of the proposed solution is compared with a multi-layer perceptron (MLP) and multi-head self-attention networks as baselines in a simulation environment. We also demonstrate the performance of the model with and without auxiliary tasks. All models are evaluated based on different metrics such as time to finish each lap, number of overtakes, and speed difference with speed limit. The evaluation shows that the SwapTransformer model outperforms other models in different traffic densities in the inference phase.","sentences":["This paper investigates the high-level decision-making problem in highway scenarios regarding lane changing and over-taking other slower vehicles.","In particular, this paper aims to improve the Travel Assist feature for automatic overtaking and lane changes on highways.","About 9 million samples including lane images and other dynamic objects are collected in simulation.","This data; Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this challenge.","To solve this problem, an architecture called SwapTransformer is designed and implemented as an imitation learning approach on the OSHA dataset.","Moreover, auxiliary tasks such as future points and car distance network predictions are proposed to aid the model in better understanding the surrounding environment.","The performance of the proposed solution is compared with a multi-layer perceptron (MLP) and multi-head self-attention networks as baselines in a simulation environment.","We also demonstrate the performance of the model with and without auxiliary tasks.","All models are evaluated based on different metrics such as time to finish each lap, number of overtakes, and speed difference with speed limit.","The evaluation shows that the SwapTransformer model outperforms other models in different traffic densities in the inference phase."],"url":"http://arxiv.org/abs/2401.01425v1"}
{"created":"2024-01-02 20:05:56","title":"To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation","abstract":"We conduct a large-scale fine-grained comparative analysis of machine translations (MT) against human translations (HT) through the lens of morphosyntactic divergence. Across three language pairs and two types of divergence defined as the structural difference between the source and the target, MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments. Through analysis on different decoding algorithms, we attribute this discrepancy to the use of beam search that biases MT towards more convergent patterns. This bias is most amplified when the convergent pattern appears around 50% of the time in training data. Lastly, we show that for a majority of morphosyntactic divergences, their presence in HT is correlated with decreased MT performance, presenting a greater challenge for MT systems.","sentences":["We conduct a large-scale fine-grained comparative analysis of machine translations (MT) against human translations (HT) through the lens of morphosyntactic divergence.","Across three language pairs and two types of divergence defined as the structural difference between the source and the target, MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments.","Through analysis on different decoding algorithms, we attribute this discrepancy to the use of beam search that biases MT towards more convergent patterns.","This bias is most amplified when the convergent pattern appears around 50% of the time in training data.","Lastly, we show that for a majority of morphosyntactic divergences, their presence in HT is correlated with decreased MT performance, presenting a greater challenge for MT systems."],"url":"http://arxiv.org/abs/2401.01419v1"}
{"created":"2024-01-02 19:56:50","title":"Flexible Control Flow Graph Alignment for Delivering Data-Driven Feedback to Novice Programming Learners","abstract":"Supporting learners in introductory programming assignments at scale is a necessity. This support includes automated feedback on what learners did incorrectly. Existing approaches cast the problem as automatically repairing learners' incorrect programs extrapolating the data from an existing correct program from other learners. However, such approaches are limited because they only compare programs with similar control flow and order of statements. A potentially valuable set of repair feedback from flexible comparisons is thus missing. In this paper, we present several modifications to CLARA, a data-driven automated repair approach that is open source, to deal with real-world introductory programs. We extend CLARA's abstract syntax tree processor to handle common introductory programming constructs. Additionally, we propose a flexible alignment algorithm over control flow graphs where we enrich nodes with semantic annotations extracted from programs using operations and calls. Using this alignment, we modify an incorrect program's control flow graph to match the correct programs to apply CLARA's original repair process. We evaluate our approach against a baseline on the twenty most popular programming problems in Codeforces. Our results indicate that flexible alignment has a significantly higher percentage of successful repairs at 46% compared to 5% for baseline CLARA. Our implementation is available at https://github.com/towhidabsar/clara.","sentences":["Supporting learners in introductory programming assignments at scale is a necessity.","This support includes automated feedback on what learners did incorrectly.","Existing approaches cast the problem as automatically repairing learners' incorrect programs extrapolating the data from an existing correct program from other learners.","However, such approaches are limited because they only compare programs with similar control flow and order of statements.","A potentially valuable set of repair feedback from flexible comparisons is thus missing.","In this paper, we present several modifications to CLARA, a data-driven automated repair approach that is open source, to deal with real-world introductory programs.","We extend CLARA's abstract syntax tree processor to handle common introductory programming constructs.","Additionally, we propose a flexible alignment algorithm over control flow graphs where we enrich nodes with semantic annotations extracted from programs using operations and calls.","Using this alignment, we modify an incorrect program's control flow graph to match the correct programs to apply CLARA's original repair process.","We evaluate our approach against a baseline on the twenty most popular programming problems in Codeforces.","Our results indicate that flexible alignment has a significantly higher percentage of successful repairs at 46% compared to 5% for baseline CLARA.","Our implementation is available at https://github.com/towhidabsar/clara."],"url":"http://arxiv.org/abs/2401.01416v1"}
{"created":"2024-01-02 19:00:13","title":"Scalable network reconstruction in subquadratic time","abstract":"Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\\log N)$, but with a more typical log-linear complexity of $O(N\\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. In practice, our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline, allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.","sentences":["Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model.","A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\\log N)$, but with a more typical log-linear complexity of $O(N\\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search.","In practice, our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline, allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges."],"url":"http://arxiv.org/abs/2401.01404v1"}
