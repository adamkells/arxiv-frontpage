{"created":"2025-01-22 18:59:46","title":"VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding","abstract":"In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation model for image and video understanding. The core design philosophy of VideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the vision-centric training paradigm and vision-centric framework design. The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding. Instead of preparing massive video-text datasets, we focus on constructing large-scale and high-quality image-text datasets. VideoLLaMA3 has four training stages: 1) vision-centric alignment stage, which warms up the vision encoder and projector; 2) vision-language pretraining stage, which jointly tunes the vision encoder, projector, and LLM with large-scale image-text data covering multiple types (including scene images, documents, charts) as well as text-only data. 3) multi-task fine-tuning stage, which incorporates image-text SFT data for downstream tasks and video-text data to establish a foundation for video understanding. 4) video-centric fine-tuning, which further improves the model's capability in video understanding. As for the framework design, to better capture fine-grained details in images, the pretrained vision encoder is adapted to encode images of varying sizes into vision tokens with corresponding numbers, rather than a fixed number of tokens. For video inputs, we reduce the number of vision tokens according to their similarity so that the representation of videos will be more precise and compact. Benefit from vision-centric designs, VideoLLaMA3 achieves compelling performances in both image and video understanding benchmarks.","sentences":["In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation model for image and video understanding.","The core design philosophy of VideoLLaMA3 is vision-centric.","The meaning of \"vision-centric\" is two-fold: the vision-centric training paradigm and vision-centric framework design.","The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding.","Instead of preparing massive video-text datasets, we focus on constructing large-scale and high-quality image-text datasets.","VideoLLaMA3 has four training stages: 1) vision-centric alignment stage, which warms up the vision encoder and projector; 2) vision-language pretraining stage, which jointly tunes the vision encoder, projector, and LLM with large-scale image-text data covering multiple types (including scene images, documents, charts) as well as text-only data.","3) multi-task fine-tuning stage, which incorporates image-text SFT data for downstream tasks and video-text data to establish a foundation for video understanding.","4) video-centric fine-tuning, which further improves the model's capability in video understanding.","As for the framework design, to better capture fine-grained details in images, the pretrained vision encoder is adapted to encode images of varying sizes into vision tokens with corresponding numbers, rather than a fixed number of tokens.","For video inputs, we reduce the number of vision tokens according to their similarity so that the representation of videos will be more precise and compact.","Benefit from vision-centric designs, VideoLLaMA3 achieves compelling performances in both image and video understanding benchmarks."],"url":"http://arxiv.org/abs/2501.13106v1"}
{"created":"2025-01-22 18:59:14","title":"On the Service Rate Region of Reed-Muller Codes","abstract":"We study the Service Rate Region (SRR) of Reed-Muller (RM) codes in the context of distributed storage systems. The SRR is a convex polytope comprising all achievable data access request rates under a given coding scheme. It represents a critical metric for evaluating system efficiency and scalability. Using the geometric properties of RM codes, we characterize recovery sets for data objects, including their existence, uniqueness, and enumeration. This analysis reveals a connection between recovery sets and minimum-weight codewords in the dual RM code, providing a framework for identifying small recovery sets. Using these results, we derive explicit and tight bounds for the maximal achievable demand for individual data objects, which define the maximal simplex within the service rate region.","sentences":["We study the Service Rate Region (SRR) of Reed-Muller (RM) codes in the context of distributed storage systems.","The SRR is a convex polytope comprising all achievable data access request rates under a given coding scheme.","It represents a critical metric for evaluating system efficiency and scalability.","Using the geometric properties of RM codes, we characterize recovery sets for data objects, including their existence, uniqueness, and enumeration.","This analysis reveals a connection between recovery sets and minimum-weight codewords in the dual RM code, providing a framework for identifying small recovery sets.","Using these results, we derive explicit and tight bounds for the maximal achievable demand for individual data objects, which define the maximal simplex within the service rate region."],"url":"http://arxiv.org/abs/2501.13105v1"}
{"created":"2025-01-22 18:57:14","title":"A Rate-Distortion Framework for Summarization","abstract":"This paper introduces an information-theoretic framework for text summarization. We define the summarizer rate-distortion function and show that it provides a fundamental lower bound on summarizer performance. We describe an iterative procedure, similar to Blahut-Arimoto algorithm, for computing this function. To handle real-world text datasets, we also propose a practical method that can calculate the summarizer rate-distortion function with limited data. Finally, we empirically confirm our theoretical results by comparing the summarizer rate-distortion function with the performances of different summarizers used in practice.","sentences":["This paper introduces an information-theoretic framework for text summarization.","We define the summarizer rate-distortion function and show that it provides a fundamental lower bound on summarizer performance.","We describe an iterative procedure, similar to Blahut-Arimoto algorithm, for computing this function.","To handle real-world text datasets, we also propose a practical method that can calculate the summarizer rate-distortion function with limited data.","Finally, we empirically confirm our theoretical results by comparing the summarizer rate-distortion function with the performances of different summarizers used in practice."],"url":"http://arxiv.org/abs/2501.13100v1"}
{"created":"2025-01-22 18:51:25","title":"Guaranteed Recovery of Unambiguous Clusters","abstract":"Clustering is often a challenging problem because of the inherent ambiguity in what the \"correct\" clustering should be. Even when the number of clusters $K$ is known, this ambiguity often still exists, particularly when there is variation in density among different clusters, and clusters have multiple relatively separated regions of high density. In this paper we propose an information-theoretic characterization of when a $K$-clustering is ambiguous, and design an algorithm that recovers the clustering whenever it is unambiguous. This characterization formalizes the situation when two high density regions within a cluster are separable enough that they look more like two distinct clusters than two truly distinct clusters in the clustering. The algorithm first identifies $K$ partial clusters (or \"seeds\") using a density-based approach, and then adds unclustered points to the initial $K$ partial clusters in a greedy manner to form a complete clustering. We implement and test a version of the algorithm that is modified to effectively handle overlapping clusters, and observe that it requires little parameter selection and displays improved performance on many datasets compared to widely used algorithms for non-convex cluster recovery.","sentences":["Clustering is often a challenging problem because of the inherent ambiguity in what the \"correct\" clustering should be.","Even when the number of clusters $K$ is known, this ambiguity often still exists, particularly when there is variation in density among different clusters, and clusters have multiple relatively separated regions of high density.","In this paper we propose an information-theoretic characterization of when a $K$-clustering is ambiguous, and design an algorithm that recovers the clustering whenever it is unambiguous.","This characterization formalizes the situation when two high density regions within a cluster are separable enough that they look more like two distinct clusters than two truly distinct clusters in the clustering.","The algorithm first identifies $K$ partial clusters (or \"seeds\") using a density-based approach, and then adds unclustered points to the initial $K$ partial clusters in a greedy manner to form a complete clustering.","We implement and test a version of the algorithm that is modified to effectively handle overlapping clusters, and observe that it requires little parameter selection and displays improved performance on many datasets compared to widely used algorithms for non-convex cluster recovery."],"url":"http://arxiv.org/abs/2501.13093v1"}
{"created":"2025-01-22 18:40:57","title":"Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment","abstract":"Large Language Models (LLMs) have demonstrated powerful capabilities that render them valuable in different applications, including conversational AI products. It is paramount to ensure the security and reliability of these products by mitigating their vulnerabilities towards malicious user interactions, which can lead to the exposure of great risks and reputational repercussions. In this work, we present a comprehensive study on the efficacy of fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs that serve as input moderation guardrails. We systematically explore various tuning methods by leveraging a small set of training data to adapt these models as proxy defense mechanisms to detect malicious inputs and provide a reasoning for their verdicts, thereby preventing the exploitation of conversational agents. We rigorously evaluate the efficacy and robustness of different tuning strategies to generalize across diverse adversarial and malicious query types. Our experimental results outline the potential of alignment processes tailored to a varied range of harmful input queries, even with constrained data resources. These techniques significantly enhance the safety of conversational AI systems and provide a feasible framework for deploying more secure and trustworthy AI-driven interactions.","sentences":["Large Language Models (LLMs) have demonstrated powerful capabilities that render them valuable in different applications, including conversational AI products.","It is paramount to ensure the security and reliability of these products by mitigating their vulnerabilities towards malicious user interactions, which can lead to the exposure of great risks and reputational repercussions.","In this work, we present a comprehensive study on the efficacy of fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs that serve as input moderation guardrails.","We systematically explore various tuning methods by leveraging a small set of training data to adapt these models as proxy defense mechanisms to detect malicious inputs and provide a reasoning for their verdicts, thereby preventing the exploitation of conversational agents.","We rigorously evaluate the efficacy and robustness of different tuning strategies to generalize across diverse adversarial and malicious query types.","Our experimental results outline the potential of alignment processes tailored to a varied range of harmful input queries, even with constrained data resources.","These techniques significantly enhance the safety of conversational AI systems and provide a feasible framework for deploying more secure and trustworthy AI-driven interactions."],"url":"http://arxiv.org/abs/2501.13080v1"}
{"created":"2025-01-22 18:28:18","title":"Beyond the Lungs: Extending the Field of View in Chest CT with Latent Diffusion Models","abstract":"The interconnection between the human lungs and other organs, such as the liver and kidneys, is crucial for understanding the underlying risks and effects of lung diseases and improving patient care. However, most research chest CT imaging is focused solely on the lungs due to considerations of cost and radiation dose. This restricted field of view (FOV) in the acquired images poses challenges to comprehensive analysis and hinders the ability to gain insights into the impact of lung diseases on other organs. To address this, we propose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel approach to capture the inter-organ relationships from CT images and extend the FOV of chest CT images. Our approach first trains a variational autoencoder (VAE) to encode 2D axial CT slices individually, then stacks the latent representations of the VAE to form a 3D context for training a latent diffusion model. Once trained, our approach extends the FOV of CT images in the z-direction by generating new axial slices in a zero-shot manner. We evaluated our approach on the National Lung Screening Trial (NLST) dataset, and results suggest that it effectively extends the FOV to include the liver and kidneys, which are not completely covered in the original NLST data acquisition. Quantitative results on a held-out whole-body dataset demonstrate that the generated slices exhibit high fidelity with acquired data, achieving an SSIM of 0.81.","sentences":["The interconnection between the human lungs and other organs, such as the liver and kidneys, is crucial for understanding the underlying risks and effects of lung diseases and improving patient care.","However, most research chest CT imaging is focused solely on the lungs due to considerations of cost and radiation dose.","This restricted field of view (FOV) in the acquired images poses challenges to comprehensive analysis and hinders the ability to gain insights into the impact of lung diseases on other organs.","To address this, we propose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel approach to capture the inter-organ relationships from CT images and extend the FOV of chest CT images.","Our approach first trains a variational autoencoder (VAE) to encode 2D axial CT slices individually, then stacks the latent representations of the VAE to form a 3D context for training a latent diffusion model.","Once trained, our approach extends the FOV of CT images in the z-direction by generating new axial slices in a zero-shot manner.","We evaluated our approach on the National Lung Screening Trial (NLST) dataset, and results suggest that it effectively extends the FOV to include the liver and kidneys, which are not completely covered in the original NLST data acquisition.","Quantitative results on a held-out whole-body dataset demonstrate that the generated slices exhibit high fidelity with acquired data, achieving an SSIM of 0.81."],"url":"http://arxiv.org/abs/2501.13068v1"}
{"created":"2025-01-22 18:21:55","title":"SMART-Vision: Survey of Modern Action Recognition Techniques in Vision","abstract":"Human Action Recognition (HAR) is a challenging domain in computer vision, involving recognizing complex patterns by analyzing the spatiotemporal dynamics of individuals' movements in videos. These patterns arise in sequential data, such as video frames, which are often essential to accurately distinguish actions that would be ambiguous in a single image. HAR has garnered considerable interest due to its broad applicability, ranging from robotics and surveillance systems to sports motion analysis, healthcare, and the burgeoning field of autonomous vehicles. While several taxonomies have been proposed to categorize HAR approaches in surveys, they often overlook hybrid methodologies and fail to demonstrate how different models incorporate various architectures and modalities. In this comprehensive survey, we present the novel SMART-Vision taxonomy, which illustrates how innovations in deep learning for HAR complement one another, leading to hybrid approaches beyond traditional categories. Our survey provides a clear roadmap from foundational HAR works to current state-of-the-art systems, highlighting emerging research directions and addressing unresolved challenges in discussion sections for architectures within the HAR domain. We provide details of the research datasets that various approaches used to measure and compare goodness HAR approaches. We also explore the rapidly emerging field of Open-HAR systems, which challenges HAR systems by presenting samples from unknown, novel classes during test time.","sentences":["Human Action Recognition (HAR) is a challenging domain in computer vision, involving recognizing complex patterns by analyzing the spatiotemporal dynamics of individuals' movements in videos.","These patterns arise in sequential data, such as video frames, which are often essential to accurately distinguish actions that would be ambiguous in a single image.","HAR has garnered considerable interest due to its broad applicability, ranging from robotics and surveillance systems to sports motion analysis, healthcare, and the burgeoning field of autonomous vehicles.","While several taxonomies have been proposed to categorize HAR approaches in surveys, they often overlook hybrid methodologies and fail to demonstrate how different models incorporate various architectures and modalities.","In this comprehensive survey, we present the novel SMART-Vision taxonomy, which illustrates how innovations in deep learning for HAR complement one another, leading to hybrid approaches beyond traditional categories.","Our survey provides a clear roadmap from foundational HAR works to current state-of-the-art systems, highlighting emerging research directions and addressing unresolved challenges in discussion sections for architectures within the HAR domain.","We provide details of the research datasets that various approaches used to measure and compare goodness HAR approaches.","We also explore the rapidly emerging field of Open-HAR systems, which challenges HAR systems by presenting samples from unknown, novel classes during test time."],"url":"http://arxiv.org/abs/2501.13066v1"}
{"created":"2025-01-22 18:01:24","title":"One-Class Domain Adaptation via Meta-Learning","abstract":"The deployment of IoT (Internet of Things) sensor-based machine learning models in industrial systems for anomaly classification tasks poses significant challenges due to distribution shifts, as the training data acquired in controlled laboratory settings may significantly differ from real-time data in production environments. Furthermore, many real-world applications cannot provide a substantial number of labeled examples for each anomalous class in every new environment. It is therefore crucial to develop adaptable machine learning models that can be effectively transferred from one environment to another, enabling rapid adaptation using normal operational data. We extended this problem setting to an arbitrary classification task and formulated the one-class domain adaptation (OC-DA) problem setting. We took a meta-learning approach to tackle the challenge of OC-DA, and proposed a task sampling strategy to adapt any bi-level meta-learning algorithm to OC-DA. We modified the well-established model-agnostic meta-learning (MAML) algorithm and introduced the OC-DA MAML algorithm. We provided a theoretical analysis showing that OC-DA MAML optimizes for meta-parameters that enable rapid one-class adaptation across domains. The OC-DA MAML algorithm is evaluated on the Rainbow-MNIST meta-learning benchmark and on a real-world dataset of vibration-based sensor readings. The results show that OC-DA MAML significantly improves the performance on the target domains and outperforms MAML using the standard task sampling strategy.","sentences":["The deployment of IoT (Internet of Things) sensor-based machine learning models in industrial systems for anomaly classification tasks poses significant challenges due to distribution shifts, as the training data acquired in controlled laboratory settings may significantly differ from real-time data in production environments.","Furthermore, many real-world applications cannot provide a substantial number of labeled examples for each anomalous class in every new environment.","It is therefore crucial to develop adaptable machine learning models that can be effectively transferred from one environment to another, enabling rapid adaptation using normal operational data.","We extended this problem setting to an arbitrary classification task and formulated the one-class domain adaptation (OC-DA) problem setting.","We took a meta-learning approach to tackle the challenge of OC-DA, and proposed a task sampling strategy to adapt any bi-level meta-learning algorithm to OC-DA.","We modified the well-established model-agnostic meta-learning (MAML) algorithm and introduced the OC-DA MAML algorithm.","We provided a theoretical analysis showing that OC-DA MAML optimizes for meta-parameters that enable rapid one-class adaptation across domains.","The OC-DA MAML algorithm is evaluated on the Rainbow-MNIST meta-learning benchmark and on a real-world dataset of vibration-based sensor readings.","The results show that OC-DA MAML significantly improves the performance on the target domains and outperforms MAML using the standard task sampling strategy."],"url":"http://arxiv.org/abs/2501.13052v1"}
{"created":"2025-01-22 17:59:26","title":"Column-Oriented Datalog on the GPU","abstract":"Datalog is a logic programming language widely used in knowledge representation and reasoning (KRR), program analysis, and social media mining due to its expressiveness and high performance. Traditionally, Datalog engines use either row-oriented or column-oriented storage. Engines like VLog and Nemo favor column-oriented storage for efficiency on limited-resource machines, while row-oriented engines like Souffle use advanced data structures with locking to perform better on multi-core CPUs. The advent of modern datacenter GPUs, such as the NVIDIA H100 with its ability to run over 16k threads simultaneously and high memory bandwidth, has reopened the debate on which storage layout is more effective. This paper presents the first column-oriented Datalog engines tailored to the strengths of modern GPUs. We present VFLog, a CUDA-based Datalog runtime library with a column-oriented GPU datastructure that supports all necessary relational algebra operations. Our results demonstrate over 200x performance gains over SOTA CPU-based column-oriented Datalog engines and a 2.5x speedup over GPU Datalog engines in various workloads, including KRR.","sentences":["Datalog is a logic programming language widely used in knowledge representation and reasoning (KRR), program analysis, and social media mining due to its expressiveness and high performance.","Traditionally, Datalog engines use either row-oriented or column-oriented storage.","Engines like VLog and Nemo favor column-oriented storage for efficiency on limited-resource machines, while row-oriented engines like Souffle use advanced data structures with locking to perform better on multi-core CPUs.","The advent of modern datacenter GPUs, such as the NVIDIA H100 with its ability to run over 16k threads simultaneously and high memory bandwidth, has reopened the debate on which storage layout is more effective.","This paper presents the first column-oriented Datalog engines tailored to the strengths of modern GPUs.","We present VFLog, a CUDA-based Datalog runtime library with a column-oriented GPU datastructure that supports all necessary relational algebra operations.","Our results demonstrate over 200x performance gains over SOTA CPU-based column-oriented Datalog engines and a 2.5x speedup over GPU Datalog engines in various workloads, including KRR."],"url":"http://arxiv.org/abs/2501.13051v1"}
{"created":"2025-01-22 17:44:01","title":"Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning","abstract":"Recent large language models (LLMs) have advanced table understanding capabilities but rely on converting tables into text sequences. While multimodal large language models (MLLMs) enable direct visual processing, they face limitations in handling scientific tables due to fixed input image resolutions and insufficient numerical reasoning capabilities. We present a comprehensive framework for multimodal scientific table understanding and reasoning with dynamic input image resolutions. Our framework consists of three key components: (1) MMSci-Pre, a domain-specific table structure learning dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins, an instruction tuning dataset with 12K samples across three table-based tasks, and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically designed to evaluate numerical reasoning capabilities. Extensive experiments demonstrate that our domain-specific approach with 52K scientific table images achieves superior performance compared to 150K general-domain tables, highlighting the importance of data quality over quantity. Our proposed table-based MLLMs with dynamic input resolutions show significant improvements in both general table understanding and numerical reasoning capabilities, with strong generalisation to held-out datasets. Our code and data are publicly available at https://github.com/Bernard-Yang/MMSci_Table.","sentences":["Recent large language models (LLMs) have advanced table understanding capabilities but rely on converting tables into text sequences.","While multimodal large language models (MLLMs) enable direct visual processing, they face limitations in handling scientific tables due to fixed input image resolutions and insufficient numerical reasoning capabilities.","We present a comprehensive framework for multimodal scientific table understanding and reasoning with dynamic input image resolutions.","Our framework consists of three key components: (1) MMSci-Pre, a domain-specific table structure learning dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins, an instruction tuning dataset with 12K samples across three table-based tasks, and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically designed to evaluate numerical reasoning capabilities.","Extensive experiments demonstrate that our domain-specific approach with 52K scientific table images achieves superior performance compared to 150K general-domain tables, highlighting the importance of data quality over quantity.","Our proposed table-based MLLMs with dynamic input resolutions show significant improvements in both general table understanding and numerical reasoning capabilities, with strong generalisation to held-out datasets.","Our code and data are publicly available at https://github.com/Bernard-Yang/MMSci_Table."],"url":"http://arxiv.org/abs/2501.13042v1"}
{"created":"2025-01-22 17:40:17","title":"TimeFilter: Patch-Specific Spatial-Temporal Graph Filtration for Time Series Forecasting","abstract":"Current time series forecasting methods can be broadly classified into two categories: Channel Independent (CI) and Channel Dependent (CD) strategies, both aiming to capture the complex dependencies within time series data. However, the CI strategy fails to exploit highly correlated covariate information, while the CD strategy integrates all dependencies, including irrelevant or noisy ones, thus compromising generalization. To mitigate these issues, recent works have introduced the Channel Clustering (CC) strategy by grouping channels with similar characteristics and applying different modeling techniques to each cluster. However, coarse-grained clustering cannot flexibly capture complex, time-varying interactions. Addressing the above challenges, we propose TimeFilter, a graph-based framework for adaptive and fine-grained dependency modeling. Specifically, after constructing the graph with the input sequence, TimeFilter filters out irrelevant correlations and preserves the most critical ones through patch-specific filtering. Extensive experiments on 13 real-world datasets from various application domains demonstrate the state-of-the-art performance of TimeFilter. The code is available at https://github.com/TROUBADOUR000/TimeFilter.","sentences":["Current time series forecasting methods can be broadly classified into two categories: Channel Independent (CI) and Channel Dependent (CD) strategies, both aiming to capture the complex dependencies within time series data.","However, the CI strategy fails to exploit highly correlated covariate information, while the CD strategy integrates all dependencies, including irrelevant or noisy ones, thus compromising generalization.","To mitigate these issues, recent works have introduced the Channel Clustering (CC) strategy by grouping channels with similar characteristics and applying different modeling techniques to each cluster.","However, coarse-grained clustering cannot flexibly capture complex, time-varying interactions.","Addressing the above challenges, we propose TimeFilter, a graph-based framework for adaptive and fine-grained dependency modeling.","Specifically, after constructing the graph with the input sequence, TimeFilter filters out irrelevant correlations and preserves the most critical ones through patch-specific filtering.","Extensive experiments on 13 real-world datasets from various application domains demonstrate the state-of-the-art performance of TimeFilter.","The code is available at https://github.com/TROUBADOUR000/TimeFilter."],"url":"http://arxiv.org/abs/2501.13041v1"}
{"created":"2025-01-22 17:29:26","title":"OLS4: A new Ontology Lookup Service for a growing interdisciplinary knowledge ecosystem","abstract":"The Ontology Lookup Service (OLS) is an open source search engine for ontologies which is used extensively in the bioinformatics and chemistry communities to annotate biological and biomedical data with ontology terms. Recently there has been a significant increase in the size and complexity of ontologies due to new scales of biological knowledge, such as spatial transcriptomics, new ontology development methodologies, and curation on an increased scale. Existing Web-based tools for ontology browsing such as BioPortal and OntoBee do not support the full range of definitions used by today's ontologies. In order to support the community going forward, we have developed OLS4, implementing the complete OWL2 specification, internationalization support for multiple languages, and a new user interface with UX enhancements such as links out to external databases. OLS4 has replaced OLS3 in production at EMBL-EBI and has a backwards compatible API supporting users of OLS3 to transition.","sentences":["The Ontology Lookup Service (OLS) is an open source search engine for ontologies which is used extensively in the bioinformatics and chemistry communities to annotate biological and biomedical data with ontology terms.","Recently there has been a significant increase in the size and complexity of ontologies due to new scales of biological knowledge, such as spatial transcriptomics, new ontology development methodologies, and curation on an increased scale.","Existing Web-based tools for ontology browsing such as BioPortal and OntoBee do not support the full range of definitions used by today's ontologies.","In order to support the community going forward, we have developed OLS4, implementing the complete OWL2 specification, internationalization support for multiple languages, and a new user interface with UX enhancements such as links out to external databases.","OLS4 has replaced OLS3 in production at EMBL-EBI and has a backwards compatible API supporting users of OLS3 to transition."],"url":"http://arxiv.org/abs/2501.13034v1"}
{"created":"2025-01-22 17:25:47","title":"A Probabilistic Model for Self-Supervised Learning","abstract":"Self-supervised learning (SSL) aims to find meaningful representations from unlabeled data by encoding semantic similarities through data augmentations. Despite its current popularity, theoretical insights about SSL are still scarce. For example, it is not yet known whether commonly used SSL loss functions can be related to a statistical model, much in the same as OLS, generalized linear models or PCA naturally emerge as maximum likelihood estimates of an underlying generative process. In this short paper, we consider a latent variable statistical model for SSL that exhibits an interesting property: Depending on the informativeness of the data augmentations, the MLE of the model either reduces to PCA, or approaches a simple non-contrastive loss. We analyze the model and also empirically illustrate our findings.","sentences":["Self-supervised learning (SSL) aims to find meaningful representations from unlabeled data by encoding semantic similarities through data augmentations.","Despite its current popularity, theoretical insights about SSL are still scarce.","For example, it is not yet known whether commonly used SSL loss functions can be related to a statistical model, much in the same as OLS, generalized linear models or PCA naturally emerge as maximum likelihood estimates of an underlying generative process.","In this short paper, we consider a latent variable statistical model for SSL that exhibits an interesting property: Depending on the informativeness of the data augmentations, the MLE of the model either reduces to PCA, or approaches a simple non-contrastive loss.","We analyze the model and also empirically illustrate our findings."],"url":"http://arxiv.org/abs/2501.13031v1"}
{"created":"2025-01-22 17:08:33","title":"Characterizing Collective Efforts in Content Sharing and Quality Control for ADHD-relevant Content on Video-sharing Platforms","abstract":"Video-sharing platforms (VSPs) have become increasingly important for individuals with ADHD to recognize symptoms, acquire knowledge, and receive support. While videos offer rich information and high engagement, they also present unique challenges, such as information quality and accessibility issues to users with ADHD. However, little work has thoroughly examined the video content quality and accessibility issues, the impact, and the control strategies in the ADHD community. We fill this gap by systematically collecting 373 ADHD-relevant videos with comments from YouTube and TikTok and analyzing the data with a mixed method. Our study identified the characteristics of ADHD-relevant videos on VSPs (e.g., creator types, video presentation forms, quality issues) and revealed the collective efforts of creators and viewers in video quality control, such as authority building, collective quality checking, and accessibility improvement. We further derive actionable design implications for VSPs to offer more reliable and ADHD-friendly contents.","sentences":["Video-sharing platforms (VSPs) have become increasingly important for individuals with ADHD to recognize symptoms, acquire knowledge, and receive support.","While videos offer rich information and high engagement, they also present unique challenges, such as information quality and accessibility issues to users with ADHD.","However, little work has thoroughly examined the video content quality and accessibility issues, the impact, and the control strategies in the ADHD community.","We fill this gap by systematically collecting 373 ADHD-relevant videos with comments from YouTube and TikTok and analyzing the data with a mixed method.","Our study identified the characteristics of ADHD-relevant videos on VSPs (e.g., creator types, video presentation forms, quality issues) and revealed the collective efforts of creators and viewers in video quality control, such as authority building, collective quality checking, and accessibility improvement.","We further derive actionable design implications for VSPs to offer more reliable and ADHD-friendly contents."],"url":"http://arxiv.org/abs/2501.13020v1"}
{"created":"2025-01-22 17:05:38","title":"Multi-Objective Hyperparameter Selection via Hypothesis Testing on Reliability Graphs","abstract":"In sensitive application domains, multi-objective hyperparameter selection can ensure the reliability of AI models prior to deployment, while optimizing auxiliary performance metrics. The state-of-the-art Pareto Testing (PT) method guarantees statistical reliability constraints by adopting a multiple hypothesis testing framework. In PT, hyperparameters are validated one at a time, following a data-driven order determined by expected reliability levels. This paper introduces a novel framework for multi-objective hyperparameter selection that captures the interdependencies among the reliability levels of different hyperparameter configurations using a directed acyclic graph (DAG), which is termed the reliability graph (RG). The RG is constructed based on prior information and data by using the Bradley-Terry model. The proposed approach, RG-based PT (RG-PT), leverages the RG to enable the efficient, parallel testing of multiple hyperparameters at the same reliability level. By integrating False Discovery Rate (FDR) control, RG-PT ensures robust statistical reliability guarantees and is shown via experiments across diverse domains to consistently yield superior solutions for multi-objective calibration problems.","sentences":["In sensitive application domains, multi-objective hyperparameter selection can ensure the reliability of AI models prior to deployment, while optimizing auxiliary performance metrics.","The state-of-the-art Pareto Testing (PT) method guarantees statistical reliability constraints by adopting a multiple hypothesis testing framework.","In PT, hyperparameters are validated one at a time, following a data-driven order determined by expected reliability levels.","This paper introduces a novel framework for multi-objective hyperparameter selection that captures the interdependencies among the reliability levels of different hyperparameter configurations using a directed acyclic graph (DAG), which is termed the reliability graph (RG).","The RG is constructed based on prior information and data by using the Bradley-Terry model.","The proposed approach, RG-based PT (RG-PT), leverages the RG to enable the efficient, parallel testing of multiple hyperparameters at the same reliability level.","By integrating False Discovery Rate (FDR) control, RG-PT ensures robust statistical reliability guarantees and is shown via experiments across diverse domains to consistently yield superior solutions for multi-objective calibration problems."],"url":"http://arxiv.org/abs/2501.13018v1"}
{"created":"2025-01-22 17:00:27","title":"Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review","abstract":"This study proposes a data-driven framework for enhancing the accuracy and efficiency of scientific peer review through an open, bottom-up process that estimates reviewer quality. Traditional closed peer review systems, while essential for quality control, are often slow, costly, and subject to biases that can impede scientific progress. Here, we introduce a method that evaluates individual reviewer reliability by quantifying agreement with community consensus scores and applying Bayesian weighting to refine paper quality assessments. We analyze open peer review data from two major scientific conferences, and demonstrate that reviewer-specific quality scores significantly improve the reliability of paper quality estimation. Perhaps surprisingly, we find that reviewer quality scores are unrelated to authorship quality. Our model incorporates incentive structures to recognize high-quality reviewers and encourage broader coverage of submitted papers, thereby mitigating the common \"rich-get-richer\" pitfall of social media. These findings suggest that open peer review, with mechanisms for estimating and incentivizing reviewer quality, offers a scalable and equitable alternative for scientific publishing, with potential to enhance the speed, fairness, and transparency of the peer review process.","sentences":["This study proposes a data-driven framework for enhancing the accuracy and efficiency of scientific peer review through an open, bottom-up process that estimates reviewer quality.","Traditional closed peer review systems, while essential for quality control, are often slow, costly, and subject to biases that can impede scientific progress.","Here, we introduce a method that evaluates individual reviewer reliability by quantifying agreement with community consensus scores and applying Bayesian weighting to refine paper quality assessments.","We analyze open peer review data from two major scientific conferences, and demonstrate that reviewer-specific quality scores significantly improve the reliability of paper quality estimation.","Perhaps surprisingly, we find that reviewer quality scores are unrelated to authorship quality.","Our model incorporates incentive structures to recognize high-quality reviewers and encourage broader coverage of submitted papers, thereby mitigating the common \"rich-get-richer\" pitfall of social media.","These findings suggest that open peer review, with mechanisms for estimating and incentivizing reviewer quality, offers a scalable and equitable alternative for scientific publishing, with potential to enhance the speed, fairness, and transparency of the peer review process."],"url":"http://arxiv.org/abs/2501.13014v1"}
{"created":"2025-01-22 16:50:58","title":"Deep Learning-Based Image Recovery and Pose Estimation for Resident Space Objects","abstract":"As the density of spacecraft in Earth's orbit increases, their recognition, pose and trajectory identification becomes crucial for averting potential collisions and executing debris removal operations. However, training models able to identify a spacecraft and its pose presents a significant challenge due to a lack of available image data for model training. This paper puts forth an innovative framework for generating realistic synthetic datasets of Resident Space Object (RSO) imagery. Using the International Space Station (ISS) as a test case, it goes on to combine image regression with image restoration methodologies to estimate pose from blurred images. An analysis of the proposed image recovery and regression techniques was undertaken, providing insights into the performance, potential enhancements and limitations when applied to real imagery of RSOs. The image recovery approach investigated involves first applying image deconvolution using an effective point spread function, followed by detail object extraction with a U-Net. Interestingly, using only U-Net for image reconstruction the best pose performance was attained, reducing the average Mean Squared Error in image recovery by 97.28% and the average angular error by 71.9%. The successful application of U-Net image restoration combined with the Resnet50 regression network for pose estimation of the International Space Station demonstrates the value of a diverse set of evaluation tools for effective solutions to real-world problems such as the analysis of distant objects in Earth's orbit.","sentences":["As the density of spacecraft in Earth's orbit increases, their recognition, pose and trajectory identification becomes crucial for averting potential collisions and executing debris removal operations.","However, training models able to identify a spacecraft and its pose presents a significant challenge due to a lack of available image data for model training.","This paper puts forth an innovative framework for generating realistic synthetic datasets of Resident Space Object (RSO) imagery.","Using the International Space Station (ISS) as a test case, it goes on to combine image regression with image restoration methodologies to estimate pose from blurred images.","An analysis of the proposed image recovery and regression techniques was undertaken, providing insights into the performance, potential enhancements and limitations when applied to real imagery of RSOs.","The image recovery approach investigated involves first applying image deconvolution using an effective point spread function, followed by detail object extraction with a U-Net.","Interestingly, using only U-Net for image reconstruction the best pose performance was attained, reducing the average Mean Squared Error in image recovery by 97.28% and the average angular error by 71.9%.","The successful application of U-Net image restoration combined with the Resnet50 regression network for pose estimation of the International Space Station demonstrates the value of a diverse set of evaluation tools for effective solutions to real-world problems such as the analysis of distant objects in Earth's orbit."],"url":"http://arxiv.org/abs/2501.13009v1"}
{"created":"2025-01-22 16:45:41","title":"Comparison of feature extraction tools for network traffic data","abstract":"The comparison analysis of the most popular tools to extract features from network traffic is conducted in this paper. Feature extraction plays a crucial role in Intrusion Detection Systems (IDS) because it helps to transform huge raw network data into meaningful and manageable features for analysis and detection of malicious activities. The good choice of feature extraction tool is an essential step in construction of Artificial Intelligence-based Intrusion Detection Systems (AI-IDS), which can help to enhance the efficiency, accuracy, and scalability of such systems.","sentences":["The comparison analysis of the most popular tools to extract features from network traffic is conducted in this paper.","Feature extraction plays a crucial role in Intrusion Detection Systems (IDS) because it helps to transform huge raw network data into meaningful and manageable features for analysis and detection of malicious activities.","The good choice of feature extraction tool is an essential step in construction of Artificial Intelligence-based Intrusion Detection Systems (AI-IDS), which can help to enhance the efficiency, accuracy, and scalability of such systems."],"url":"http://arxiv.org/abs/2501.13004v1"}
{"created":"2025-01-22 16:25:46","title":"An Offline Multi-Agent Reinforcement Learning Framework for Radio Resource Management","abstract":"Offline multi-agent reinforcement learning (MARL) addresses key limitations of online MARL, such as safety concerns, expensive data collection, extended training intervals, and high signaling overhead caused by online interactions with the environment. In this work, we propose an offline MARL algorithm for radio resource management (RRM), focusing on optimizing scheduling policies for multiple access points (APs) to jointly maximize the sum and tail rates of user equipment (UEs). We evaluate three training paradigms: centralized, independent, and centralized training with decentralized execution (CTDE). Our simulation results demonstrate that the proposed offline MARL framework outperforms conventional baseline approaches, achieving over a 15\\% improvement in a weighted combination of sum and tail rates. Additionally, the CTDE framework strikes an effective balance, reducing the computational complexity of centralized methods while addressing the inefficiencies of independent training. These results underscore the potential of offline MARL to deliver scalable, robust, and efficient solutions for resource management in dynamic wireless networks.","sentences":["Offline multi-agent reinforcement learning (MARL) addresses key limitations of online MARL, such as safety concerns, expensive data collection, extended training intervals, and high signaling overhead caused by online interactions with the environment.","In this work, we propose an offline MARL algorithm for radio resource management (RRM), focusing on optimizing scheduling policies for multiple access points (APs) to jointly maximize the sum and tail rates of user equipment (UEs).","We evaluate three training paradigms: centralized, independent, and centralized training with decentralized execution (CTDE).","Our simulation results demonstrate that the proposed offline MARL framework outperforms conventional baseline approaches, achieving over a 15\\% improvement in a weighted combination of sum and tail rates.","Additionally, the CTDE framework strikes an effective balance, reducing the computational complexity of centralized methods while addressing the inefficiencies of independent training.","These results underscore the potential of offline MARL to deliver scalable, robust, and efficient solutions for resource management in dynamic wireless networks."],"url":"http://arxiv.org/abs/2501.12991v1"}
{"created":"2025-01-22 16:07:24","title":"Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities","abstract":"In this paper, we compare data generated with mono- and multilingual LLMs spanning a range of model sizes with data provided by human participants in an experimental setting investigating well-established discourse biases. Beyond the comparison as such, we aim to develop a benchmark to assess the capabilities of LLMs with discourse biases as a robust proxy for more general discourse understanding capabilities. More specifically, we investigated Implicit Causality verbs, for which psycholinguistic research has found participants to display biases with regard to three phenomena:\\ the establishment of (i) coreference relations (Experiment 1), (ii) coherence relations (Experiment 2), and (iii) the use of particular referring expressions (Experiments 3 and 4). With regard to coreference biases we found only the largest monolingual LLM (German Bloom 6.4B) to display more human-like biases. For coherence relation, no LLM displayed the explanation bias usually found for humans. For referring expressions, all LLMs displayed a preference for referring to subject arguments with simpler forms than to objects. However, no bias effect on referring expression was found, as opposed to recent studies investigating human biases.","sentences":["In this paper, we compare data generated with mono- and multilingual LLMs spanning a range of model sizes with data provided by human participants in an experimental setting investigating well-established discourse biases.","Beyond the comparison as such, we aim to develop a benchmark to assess the capabilities of LLMs with discourse biases as a robust proxy for more general discourse understanding capabilities.","More specifically, we investigated Implicit Causality verbs, for which psycholinguistic research has found participants to display biases with regard to three phenomena:\\ the establishment of (i) coreference relations (Experiment 1), (ii) coherence relations (Experiment 2), and (iii) the use of particular referring expressions (Experiments 3 and 4).","With regard to coreference biases we found only the largest monolingual LLM (German Bloom 6.4B) to display more human-like biases.","For coherence relation, no LLM displayed the explanation bias usually found for humans.","For referring expressions, all LLMs displayed a preference for referring to subject arguments with simpler forms than to objects.","However, no bias effect on referring expression was found, as opposed to recent studies investigating human biases."],"url":"http://arxiv.org/abs/2501.12980v1"}
{"created":"2025-01-22 16:06:04","title":"FlanEC: Exploring Flan-T5 for Post-ASR Error Correction","abstract":"In this paper, we present an encoder-decoder model leveraging Flan-T5 for post-Automatic Speech Recognition (ASR) Generative Speech Error Correction (GenSEC), and we refer to it as FlanEC. We explore its application within the GenSEC framework to enhance ASR outputs by mapping n-best hypotheses into a single output sentence. By utilizing n-best lists from ASR models, we aim to improve the linguistic correctness, accuracy, and grammaticality of final ASR transcriptions. Specifically, we investigate whether scaling the training data and incorporating diverse datasets can lead to significant improvements in post-ASR error correction. We evaluate FlanEC using the HyPoradise dataset, providing a comprehensive analysis of the model's effectiveness in this domain. Furthermore, we assess the proposed approach under different settings to evaluate model scalability and efficiency, offering valuable insights into the potential of instruction-tuned encoder-decoder models for this task.","sentences":["In this paper, we present an encoder-decoder model leveraging Flan-T5 for post-Automatic Speech Recognition (ASR) Generative Speech Error Correction (GenSEC), and we refer to it as FlanEC.","We explore its application within the GenSEC framework to enhance ASR outputs by mapping n-best hypotheses into a single output sentence.","By utilizing n-best lists from ASR models, we aim to improve the linguistic correctness, accuracy, and grammaticality of final ASR transcriptions.","Specifically, we investigate whether scaling the training data and incorporating diverse datasets can lead to significant improvements in post-ASR error correction.","We evaluate FlanEC using the HyPoradise dataset, providing a comprehensive analysis of the model's effectiveness in this domain.","Furthermore, we assess the proposed approach under different settings to evaluate model scalability and efficiency, offering valuable insights into the potential of instruction-tuned encoder-decoder models for this task."],"url":"http://arxiv.org/abs/2501.12979v1"}
{"created":"2025-01-22 16:05:59","title":"Galois groups of polynomials and neurosymbolic networks","abstract":"This paper introduces a novel approach to understanding Galois theory, one of the foundational areas of algebra, through the lens of machine learning. By analyzing polynomial equations with machine learning techniques, we aim to streamline the process of determining solvability by radicals and explore broader applications within Galois theory. This summary encapsulates the background, methodology, potential applications, and challenges of using data science in Galois theory.   More specifically, we design a neurosymbolic network to classify Galois groups and show how this is more efficient than usual neural networks. We discover some very interesting distribution of polynomials for groups not isomorphic to the symmetric groups and alternating groups.","sentences":["This paper introduces a novel approach to understanding Galois theory, one of the foundational areas of algebra, through the lens of machine learning.","By analyzing polynomial equations with machine learning techniques, we aim to streamline the process of determining solvability by radicals and explore broader applications within Galois theory.","This summary encapsulates the background, methodology, potential applications, and challenges of using data science in Galois theory.   ","More specifically, we design a neurosymbolic network to classify Galois groups and show how this is more efficient than usual neural networks.","We discover some very interesting distribution of polynomials for groups not isomorphic to the symmetric groups and alternating groups."],"url":"http://arxiv.org/abs/2501.12978v1"}
{"created":"2025-01-22 15:58:11","title":"MorphoSkel3D: Morphological Skeletonization of 3D Point Clouds for Informed Sampling in Object Classification and Retrieval","abstract":"Point clouds are a set of data points in space to represent the 3D geometry of objects. A fundamental step in the processing is to identify a subset of points to represent the shape. While traditional sampling methods often ignore to incorporate geometrical information, recent developments in learning-based sampling models have achieved significant levels of performance. With the integration of geometrical priors, the ability to learn and preserve the underlying structure can be enhanced when sampling. To shed light into the shape, a qualitative skeleton serves as an effective descriptor to guide sampling for both local and global geometries. In this paper, we introduce MorphoSkel3D as a new technique based on morphology to facilitate an efficient skeletonization of shapes. With its low computational cost, MorphoSkel3D is a unique, rule-based algorithm to benchmark its quality and performance on two large datasets, ModelNet and ShapeNet, under different sampling ratios. The results show that training with MorphoSkel3D leads to an informed and more accurate sampling in the practical application of object classification and point cloud retrieval.","sentences":["Point clouds are a set of data points in space to represent the 3D geometry of objects.","A fundamental step in the processing is to identify a subset of points to represent the shape.","While traditional sampling methods often ignore to incorporate geometrical information, recent developments in learning-based sampling models have achieved significant levels of performance.","With the integration of geometrical priors, the ability to learn and preserve the underlying structure can be enhanced when sampling.","To shed light into the shape, a qualitative skeleton serves as an effective descriptor to guide sampling for both local and global geometries.","In this paper, we introduce MorphoSkel3D as a new technique based on morphology to facilitate an efficient skeletonization of shapes.","With its low computational cost, MorphoSkel3D is a unique, rule-based algorithm to benchmark its quality and performance on two large datasets, ModelNet and ShapeNet, under different sampling ratios.","The results show that training with MorphoSkel3D leads to an informed and more accurate sampling in the practical application of object classification and point cloud retrieval."],"url":"http://arxiv.org/abs/2501.12974v1"}
{"created":"2025-01-22 15:38:09","title":"It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act","abstract":"What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for AI models, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First a high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.), most non-discrimination regulations target only high-risk AI systems. (2.), the regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are often inconsistent and raise questions of computational feasibility. (3.) Regulations for General Purpose AI Models, such as Large Language Models that are not simultaneously classified as high-risk systems, currently lack specificity compared to other regulations. Based on these findings, we recommend developing more specific auditing and testing methodologies for AI systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems.","sentences":["What constitutes a fair decision?","This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used.","In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for AI models, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts.","This paper aims to bridge these two different concepts in the AI Act through: First a high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness.","Our analysis reveals three key findings: (1.), most non-discrimination regulations target only high-risk AI systems.","(2.), the regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are often inconsistent and raise questions of computational feasibility.","(3.)","Regulations for General Purpose AI Models, such as Large Language Models that are not simultaneously classified as high-risk systems, currently lack specificity compared to other regulations.","Based on these findings, we recommend developing more specific auditing and testing methodologies for AI systems.","This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems."],"url":"http://arxiv.org/abs/2501.12962v1"}
{"created":"2025-01-22 15:32:07","title":"A Novel Tracking Framework for Devices in X-ray Leveraging Supplementary Cue-Driven Self-Supervised Features","abstract":"To restore proper blood flow in blocked coronary arteries via angioplasty procedure, accurate placement of devices such as catheters, balloons, and stents under live fluoroscopy or diagnostic angiography is crucial. Identified balloon markers help in enhancing stent visibility in X-ray sequences, while the catheter tip aids in precise navigation and co-registering vessel structures, reducing the need for contrast in angiography. However, accurate detection of these devices in interventional X-ray sequences faces significant challenges, particularly due to occlusions from contrasted vessels and other devices and distractions from surrounding, resulting in the failure to track such small objects. While most tracking methods rely on spatial correlation of past and current appearance, they often lack strong motion comprehension essential for navigating through these challenging conditions, and fail to effectively detect multiple instances in the scene. To overcome these limitations, we propose a self-supervised learning approach that enhances its spatio-temporal understanding by incorporating supplementary cues and learning across multiple representation spaces on a large dataset. Followed by that, we introduce a generic real-time tracking framework that effectively leverages the pretrained spatio-temporal network and also takes the historical appearance and trajectory data into account. This results in enhanced localization of multiple instances of device landmarks. Our method outperforms state-of-the-art methods in interventional X-ray device tracking, especially stability and robustness, achieving an 87% reduction in max error for balloon marker detection and a 61% reduction in max error for catheter tip detection.","sentences":["To restore proper blood flow in blocked coronary arteries via angioplasty procedure, accurate placement of devices such as catheters, balloons, and stents under live fluoroscopy or diagnostic angiography is crucial.","Identified balloon markers help in enhancing stent visibility in X-ray sequences, while the catheter tip aids in precise navigation and co-registering vessel structures, reducing the need for contrast in angiography.","However, accurate detection of these devices in interventional X-ray sequences faces significant challenges, particularly due to occlusions from contrasted vessels and other devices and distractions from surrounding, resulting in the failure to track such small objects.","While most tracking methods rely on spatial correlation of past and current appearance, they often lack strong motion comprehension essential for navigating through these challenging conditions, and fail to effectively detect multiple instances in the scene.","To overcome these limitations, we propose a self-supervised learning approach that enhances its spatio-temporal understanding by incorporating supplementary cues and learning across multiple representation spaces on a large dataset.","Followed by that, we introduce a generic real-time tracking framework that effectively leverages the pretrained spatio-temporal network and also takes the historical appearance and trajectory data into account.","This results in enhanced localization of multiple instances of device landmarks.","Our method outperforms state-of-the-art methods in interventional X-ray device tracking, especially stability and robustness, achieving an 87% reduction in max error for balloon marker detection and a 61% reduction in max error for catheter tip detection."],"url":"http://arxiv.org/abs/2501.12958v1"}
{"created":"2025-01-22 15:19:35","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning","abstract":"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","sentences":["We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.","DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.","Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors.","However, it encounters challenges such as poor readability, and language mixing.","To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL.","DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.","To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."],"url":"http://arxiv.org/abs/2501.12948v1"}
{"created":"2025-01-22 15:17:35","title":"Less is More: Simple yet Effective Heuristic Community Detection with Graph Convolution Network","abstract":"Community detection is crucial in data mining. Traditional methods primarily focus on graph structure, often neglecting the significance of attribute features. In contrast, deep learning-based approaches incorporate attribute features and local structural information through contrastive learning, improving detection performance. However, existing algorithms' complex design and joint optimization make them difficult to train and reduce detection efficiency. Additionally, these methods require the number of communities to be predefined, making the results susceptible to artificial interference. To address these challenges, we propose a simple yet effective community detection algorithm that can adaptively detect communities without relying on data augmentation and contrastive optimization. The proposed algorithm first performs community pre-detection to extract global structural information adaptively. It then utilizes GCN to integrate local structures and attribute features. Subsequently, it combines global, local structures and attribute features in the feature space to discover community affiliations. Finally, a modularity maximization method is employed to optimize the communities based on these three types of information, thereby uncovering the community affiliation of each node. We conduct experimental comparisons across various graph datasets, evaluating the proposed algorithm against traditional methods and state-of-the-art community detection algorithms. The experimental results demonstrate that our algorithm achieves greater efficiency and accuracy in terms of both detection speed and effectiveness. The code is available at https://github.com/wuanghoong/Less-is-More.git.","sentences":["Community detection is crucial in data mining.","Traditional methods primarily focus on graph structure, often neglecting the significance of attribute features.","In contrast, deep learning-based approaches incorporate attribute features and local structural information through contrastive learning, improving detection performance.","However, existing algorithms' complex design and joint optimization make them difficult to train and reduce detection efficiency.","Additionally, these methods require the number of communities to be predefined, making the results susceptible to artificial interference.","To address these challenges, we propose a simple yet effective community detection algorithm that can adaptively detect communities without relying on data augmentation and contrastive optimization.","The proposed algorithm first performs community pre-detection to extract global structural information adaptively.","It then utilizes GCN to integrate local structures and attribute features.","Subsequently, it combines global, local structures and attribute features in the feature space to discover community affiliations.","Finally, a modularity maximization method is employed to optimize the communities based on these three types of information, thereby uncovering the community affiliation of each node.","We conduct experimental comparisons across various graph datasets, evaluating the proposed algorithm against traditional methods and state-of-the-art community detection algorithms.","The experimental results demonstrate that our algorithm achieves greater efficiency and accuracy in terms of both detection speed and effectiveness.","The code is available at https://github.com/wuanghoong/Less-is-More.git."],"url":"http://arxiv.org/abs/2501.12946v1"}
{"created":"2025-01-22 15:13:21","title":"Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained Scheduling","abstract":"Effective multi-user delay-constrained scheduling is crucial in various real-world applications, such as instant messaging, live streaming, and data center management. In these scenarios, schedulers must make real-time decisions to satisfy both delay and resource constraints without prior knowledge of system dynamics, which are often time-varying and challenging to estimate. Current learning-based methods typically require interactions with actual systems during the training stage, which can be difficult or impractical, as it is capable of significantly degrading system performance and incurring substantial service costs. To address these challenges, we propose a novel offline reinforcement learning-based algorithm, named \\underline{S}cheduling By \\underline{O}ffline Learning with \\underline{C}ritic Guidance and \\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies purely from pre-collected \\emph{offline data}. SOCD innovatively employs a diffusion-based policy network, complemented by a sampling-free critic network for policy guidance. By integrating the Lagrangian multiplier optimization into the offline reinforcement learning, SOCD effectively trains high-quality constraint-aware policies exclusively from available datasets, eliminating the need for online interactions with the system. Experimental results demonstrate that SOCD is resilient to various system dynamics, including partially observable and large-scale environments, and delivers superior performance compared to existing methods.","sentences":["Effective multi-user delay-constrained scheduling is crucial in various real-world applications, such as instant messaging, live streaming, and data center management.","In these scenarios, schedulers must make real-time decisions to satisfy both delay and resource constraints without prior knowledge of system dynamics, which are often time-varying and challenging to estimate.","Current learning-based methods typically require interactions with actual systems during the training stage, which can be difficult or impractical, as it is capable of significantly degrading system performance and incurring substantial service costs.","To address these challenges, we propose a novel offline reinforcement learning-based algorithm, named \\underline{S}cheduling By \\underline{O}ffline Learning with \\underline{C}ritic Guidance and \\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies purely from pre-collected \\emph{offline data}.","SOCD innovatively employs a diffusion-based policy network, complemented by a sampling-free critic network for policy guidance.","By integrating the Lagrangian multiplier optimization into the offline reinforcement learning, SOCD effectively trains high-quality constraint-aware policies exclusively from available datasets, eliminating the need for online interactions with the system.","Experimental results demonstrate that SOCD is resilient to various system dynamics, including partially observable and large-scale environments, and delivers superior performance compared to existing methods."],"url":"http://arxiv.org/abs/2501.12942v1"}
{"created":"2025-01-22 15:02:43","title":"DynamicEarth: How Far are We from Open-Vocabulary Change Detection?","abstract":"Monitoring Earth's evolving land covers requires methods capable of detecting changes across a wide range of categories and contexts. Existing change detection methods are hindered by their dependency on predefined classes, reducing their effectiveness in open-world applications. To address this issue, we introduce open-vocabulary change detection (OVCD), a novel task that bridges vision and language to detect changes across any category. Considering the lack of high-quality data and annotation, we propose two training-free frameworks, M-C-I and I-M-C, which leverage and integrate off-the-shelf foundation models for the OVCD task. The insight behind the M-C-I framework is to discover all potential changes and then classify these changes, while the insight of I-M-C framework is to identify all targets of interest and then determine whether their states have changed. Based on these two frameworks, we instantiate to obtain several methods, e.g., SAM-DINOv2-SegEarth-OV, Grounding-DINO-SAM2-DINO, etc. Extensive evaluations on 5 benchmark datasets demonstrate the superior generalization and robustness of our OVCD methods over existing supervised and unsupervised methods. To support continued exploration, we release DynamicEarth, a dedicated codebase designed to advance research and application of OVCD. https://likyoo.github.io/DynamicEarth","sentences":["Monitoring Earth's evolving land covers requires methods capable of detecting changes across a wide range of categories and contexts.","Existing change detection methods are hindered by their dependency on predefined classes, reducing their effectiveness in open-world applications.","To address this issue, we introduce open-vocabulary change detection (OVCD), a novel task that bridges vision and language to detect changes across any category.","Considering the lack of high-quality data and annotation, we propose two training-free frameworks, M-C-I and I-M-C, which leverage and integrate off-the-shelf foundation models for the OVCD task.","The insight behind the M-C-I framework is to discover all potential changes and then classify these changes, while the insight of I-M-C framework is to identify all targets of interest and then determine whether their states have changed.","Based on these two frameworks, we instantiate to obtain several methods, e.g., SAM-DINOv2-SegEarth-OV, Grounding-DINO-SAM2-DINO, etc.","Extensive evaluations on 5 benchmark datasets demonstrate the superior generalization and robustness of our OVCD methods over existing supervised and unsupervised methods.","To support continued exploration, we release DynamicEarth, a dedicated codebase designed to advance research and application of OVCD.","https://likyoo.github.io/DynamicEarth"],"url":"http://arxiv.org/abs/2501.12931v1"}
{"created":"2025-01-22 14:59:47","title":"QuaRs: A Transform for Better Lossless Compression of Integers","abstract":"The rise of integer-valued data, partly driven by the Internet of Things (IoT), has increased demand for efficient compression methods to reduce storage and transmission costs. Existing, speed-oriented methods rely on the ``smaller-numbers-less-bits'' principle, assuming unimodal distributions centered around zero. This assumption is often violated in practice, leading to suboptimal compression. We propose QuaRs, a transformation that reshapes arbitrary distributions into unimodal ones centered around zero, improving compatibility with fast integer compression methods. QuaRs remaps data based on quantiles, assigning smaller magnitudes to frequent values. The method is fast, invertible, and has sub-quadratic complexity. QuaRs enhances compression efficiency, even for challenging distributions, while integrating seamlessly with existing techniques.","sentences":["The rise of integer-valued data, partly driven by the Internet of Things (IoT), has increased demand for efficient compression methods to reduce storage and transmission costs.","Existing, speed-oriented methods rely on the ``smaller-numbers-less-bits'' principle, assuming unimodal distributions centered around zero.","This assumption is often violated in practice, leading to suboptimal compression.","We propose QuaRs, a transformation that reshapes arbitrary distributions into unimodal ones centered around zero, improving compatibility with fast integer compression methods.","QuaRs remaps data based on quantiles, assigning smaller magnitudes to frequent values.","The method is fast, invertible, and has sub-quadratic complexity.","QuaRs enhances compression efficiency, even for challenging distributions, while integrating seamlessly with existing techniques."],"url":"http://arxiv.org/abs/2501.12929v1"}
{"created":"2025-01-22 14:56:19","title":"Longitudinal Missing Data Imputation for Predicting Disability Stage of Patients with Multiple Sclerosis","abstract":"Multiple Sclerosis (MS) is a chronic disease characterized by progressive or alternate impairment of neurological functions (motor, sensory, visual, and cognitive). Predicting disease progression with a probabilistic and time-dependent approach might help in suggesting interventions that can delay the progression of the disease. However, extracting informative knowledge from irregularly collected longitudinal data is difficult, and missing data pose significant challenges. MS progression is measured through the Expanded Disability Status Scale (EDSS), which quantifies and monitors disability in MS over time. EDSS assesses impairment in eight functional systems (FS). Frequently, only the EDSS score assigned by clinicians is reported, while FS sub-scores are missing. Imputing these scores might be useful, especially to stratify patients according to their phenotype assessed over the disease progression. This study aimed at i) exploring different methodologies for imputing missing FS sub-scores, and ii) predicting the EDSS score using complete clinical data. Results show that Exponential Weighted Moving Average achieved the lowest error rate in the missing data imputation task; furthermore, the combination of Classification and Regression Trees for the imputation and SVM for the prediction task obtained the best accuracy.","sentences":["Multiple Sclerosis (MS) is a chronic disease characterized by progressive or alternate impairment of neurological functions (motor, sensory, visual, and cognitive).","Predicting disease progression with a probabilistic and time-dependent approach might help in suggesting interventions that can delay the progression of the disease.","However, extracting informative knowledge from irregularly collected longitudinal data is difficult, and missing data pose significant challenges.","MS progression is measured through the Expanded Disability Status Scale (EDSS), which quantifies and monitors disability in MS over time.","EDSS assesses impairment in eight functional systems (FS).","Frequently, only the EDSS score assigned by clinicians is reported, while FS sub-scores are missing.","Imputing these scores might be useful, especially to stratify patients according to their phenotype assessed over the disease progression.","This study aimed at i) exploring different methodologies for imputing missing FS sub-scores, and ii) predicting the EDSS score using complete clinical data.","Results show that Exponential Weighted Moving Average achieved the lowest error rate in the missing data imputation task; furthermore, the combination of Classification and Regression Trees for the imputation and SVM for the prediction task obtained the best accuracy."],"url":"http://arxiv.org/abs/2501.12927v1"}
{"created":"2025-01-22 14:37:44","title":"A Selective Homomorphic Encryption Approach for Faster Privacy-Preserving Federated Learning","abstract":"Federated learning is a machine learning method that supports training models on decentralized devices or servers, where each holds its local data, removing the need for data exchange. This approach is especially useful in healthcare, as it enables training on sensitive data without needing to share them. The nature of federated learning necessitates robust security precautions due to data leakage concerns during communication. To address this issue, we propose a new approach that employs selective encryption, homomorphic encryption, differential privacy, and bit-wise scrambling to minimize data leakage while achieving good execution performance. Our technique , FAS (fast and secure federated learning) is used to train deep learning models on medical imaging data. We implemented our technique using the Flower framework and compared with a state-of-the-art federated learning approach that also uses selective homomorphic encryption. Our experiments were run in a cluster of eleven physical machines to create a real-world federated learning scenario on different datasets. We observed that our approach is up to 90\\% faster than applying fully homomorphic encryption on the model weights. In addition, we can avoid the pretraining step that is required by our competitor and can save up to 20\\% in terms of total execution time. While our approach was faster, it obtained similar security results as the competitor.","sentences":["Federated learning is a machine learning method that supports training models on decentralized devices or servers, where each holds its local data, removing the need for data exchange.","This approach is especially useful in healthcare, as it enables training on sensitive data without needing to share them.","The nature of federated learning necessitates robust security precautions due to data leakage concerns during communication.","To address this issue, we propose a new approach that employs selective encryption, homomorphic encryption, differential privacy, and bit-wise scrambling to minimize data leakage while achieving good execution performance.","Our technique , FAS (fast and secure federated learning) is used to train deep learning models on medical imaging data.","We implemented our technique using the Flower framework and compared with a state-of-the-art federated learning approach that also uses selective homomorphic encryption.","Our experiments were run in a cluster of eleven physical machines to create a real-world federated learning scenario on different datasets.","We observed that our approach is up to 90\\% faster than applying fully homomorphic encryption on the model weights.","In addition, we can avoid the pretraining step that is required by our competitor and can save up to 20\\% in terms of total execution time.","While our approach was faster, it obtained similar security results as the competitor."],"url":"http://arxiv.org/abs/2501.12911v1"}
{"created":"2025-01-22 14:37:01","title":"PreciseCam: Precise Camera Control for Text-to-Image Generation","abstract":"Images as an artistic medium often rely on specific camera angles and lens distortions to convey ideas or emotions; however, such precise control is missing in current text-to-image models. We propose an efficient and general solution that allows precise control over the camera when generating both photographic and artistic images. Unlike prior methods that rely on predefined shots, we rely solely on four simple extrinsic and intrinsic camera parameters, removing the need for pre-existing geometry, reference 3D objects, and multi-view data. We also present a novel dataset with more than 57,000 images, along with their text prompts and ground-truth camera parameters. Our evaluation shows precise camera control in text-to-image generation, surpassing traditional prompt engineering approaches. Our data, model, and code are publicly available at https://graphics.unizar.es/projects/PreciseCam2024.","sentences":["Images as an artistic medium often rely on specific camera angles and lens distortions to convey ideas or emotions; however, such precise control is missing in current text-to-image models.","We propose an efficient and general solution that allows precise control over the camera when generating both photographic and artistic images.","Unlike prior methods that rely on predefined shots, we rely solely on four simple extrinsic and intrinsic camera parameters, removing the need for pre-existing geometry, reference 3D objects, and multi-view data.","We also present a novel dataset with more than 57,000 images, along with their text prompts and ground-truth camera parameters.","Our evaluation shows precise camera control in text-to-image generation, surpassing traditional prompt engineering approaches.","Our data, model, and code are publicly available at https://graphics.unizar.es/projects/PreciseCam2024."],"url":"http://arxiv.org/abs/2501.12910v1"}
{"created":"2025-01-22 14:21:04","title":"Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration","abstract":"Contextual Partitioning introduces an innovative approach to enhancing the architectural design of large-scale computational models through the dynamic segmentation of parameters into context-aware regions. This methodology emphasizes the importance of task-specific specialization, achieved through adaptive parameter allocation mechanisms that align with the linguistic features of input data. Experimental evaluations demonstrated substantial improvements in accuracy, perplexity, and contextual coherence across a variety of linguistic tasks, highlighting the adaptability and scalability of the proposed framework. By reducing redundancy and enhancing computational efficiency, Contextual Partitioning not only streamlines model operations but also expands the scope of applications for advanced language processing systems. The approach operates autonomously, requiring no external fine-tuning, thereby addressing a significant limitation in conventional parameter optimization techniques. Empirical results demonstrate the effectiveness of gradient-driven segmentation, enabling models to dynamically recalibrate and specialize in response to task-specific demands. Furthermore, resource utilization metrics reveal notable reductions in memory usage and training times, confirming the efficiency of the approach. Observations from qualitative analyses illustrate improved contextual coherence and logical flow in generated outputs, reinforcing the practical value of this technique. The findings collectively demonstrate the potential for Contextual Partitioning to redefine the scalability and adaptability of computational language architectures in diverse and complex domains.","sentences":["Contextual Partitioning introduces an innovative approach to enhancing the architectural design of large-scale computational models through the dynamic segmentation of parameters into context-aware regions.","This methodology emphasizes the importance of task-specific specialization, achieved through adaptive parameter allocation mechanisms that align with the linguistic features of input data.","Experimental evaluations demonstrated substantial improvements in accuracy, perplexity, and contextual coherence across a variety of linguistic tasks, highlighting the adaptability and scalability of the proposed framework.","By reducing redundancy and enhancing computational efficiency, Contextual Partitioning not only streamlines model operations but also expands the scope of applications for advanced language processing systems.","The approach operates autonomously, requiring no external fine-tuning, thereby addressing a significant limitation in conventional parameter optimization techniques.","Empirical results demonstrate the effectiveness of gradient-driven segmentation, enabling models to dynamically recalibrate and specialize in response to task-specific demands.","Furthermore, resource utilization metrics reveal notable reductions in memory usage and training times, confirming the efficiency of the approach.","Observations from qualitative analyses illustrate improved contextual coherence and logical flow in generated outputs, reinforcing the practical value of this technique.","The findings collectively demonstrate the potential for Contextual Partitioning to redefine the scalability and adaptability of computational language architectures in diverse and complex domains."],"url":"http://arxiv.org/abs/2501.12901v1"}
{"created":"2025-01-22 14:18:47","title":"Discrimination and AI in insurance: what do people find fair? Results from a survey","abstract":"Two modern trends in insurance are data-intensive underwriting and behavior-based insurance. Data-intensive underwriting means that insurers use and analyze more data for estimating the chance that a consumer files a claim and calculating the premium based on that estimation. Insurers analyze the new datasets with artificial intelligence (AI) to discover new correlations, with which they can estimate the policyholder's expected claims cost more precisely. Insurers also offer behavior-based insurance. For example, some car insurers use AI to follow the driving behavior of an individual policyholder in real-time and decide whether to offer that policyholder a discount. Similarly, a life insurer could track a policyholder's activity with a smart watch and offer a discount for an active lifestyle.   In this paper, we report on a survey of the Dutch population (N=999) in which we asked people's opinions about examples of data-intensive underwriting and behavior-based insurance. The main results include the following. First, if survey respondents find an insurance practice unfair, they also find the practice unacceptable. Second, respondents find almost all modern insurance practices that we described unfair. Third, respondents find practices fairer if they can influence the premium. For example, respondents find behavior-based car insurance with a car tracker relatively fair. Fourth, if respondents do not see the logic of using a certain consumer characteristic, then respondents find it unfair if an insurer calculates the premium based on the characteristic. Fifth, respondents find it unfair if an insurer offers an insurance product only to a specific group, such as car insurance specifically for family doctors. Sixth, respondents find it unfair if an insurance practice leads to higher prices for poorer people. We reflect on the policy implications of the findings.","sentences":["Two modern trends in insurance are data-intensive underwriting and behavior-based insurance.","Data-intensive underwriting means that insurers use and analyze more data for estimating the chance that a consumer files a claim and calculating the premium based on that estimation.","Insurers analyze the new datasets with artificial intelligence (AI) to discover new correlations, with which they can estimate the policyholder's expected claims cost more precisely.","Insurers also offer behavior-based insurance.","For example, some car insurers use AI to follow the driving behavior of an individual policyholder in real-time and decide whether to offer that policyholder a discount.","Similarly, a life insurer could track a policyholder's activity with a smart watch and offer a discount for an active lifestyle.   ","In this paper, we report on a survey of the Dutch population (N=999) in which we asked people's opinions about examples of data-intensive underwriting and behavior-based insurance.","The main results include the following.","First, if survey respondents find an insurance practice unfair, they also find the practice unacceptable.","Second, respondents find almost all modern insurance practices that we described unfair.","Third, respondents find practices fairer if they can influence the premium.","For example, respondents find behavior-based car insurance with a car tracker relatively fair.","Fourth, if respondents do not see the logic of using a certain consumer characteristic, then respondents find it unfair if an insurer calculates the premium based on the characteristic.","Fifth, respondents find it unfair if an insurer offers an insurance product only to a specific group, such as car insurance specifically for family doctors.","Sixth, respondents find it unfair if an insurance practice leads to higher prices for poorer people.","We reflect on the policy implications of the findings."],"url":"http://arxiv.org/abs/2501.12897v1"}
{"created":"2025-01-22 14:18:47","title":"DocTTT: Test-Time Training for Handwritten Document Recognition Using Meta-Auxiliary Learning","abstract":"Despite recent significant advancements in Handwritten Document Recognition (HDR), the efficient and accurate recognition of text against complex backgrounds, diverse handwriting styles, and varying document layouts remains a practical challenge. Moreover, this issue is seldom addressed in academic research, particularly in scenarios with minimal annotated data available. In this paper, we introduce the DocTTT framework to address these challenges. The key innovation of our approach is that it uses test-time training to adapt the model to each specific input during testing. We propose a novel Meta-Auxiliary learning approach that combines Meta-learning and self-supervised Masked Autoencoder~(MAE). During testing, we adapt the visual representation parameters using a self-supervised MAE loss. During training, we learn the model parameters using a meta-learning framework, so that the model parameters are learned to adapt to a new input effectively. Experimental results show that our proposed method significantly outperforms existing state-of-the-art approaches on benchmark datasets.","sentences":["Despite recent significant advancements in Handwritten Document Recognition (HDR), the efficient and accurate recognition of text against complex backgrounds, diverse handwriting styles, and varying document layouts remains a practical challenge.","Moreover, this issue is seldom addressed in academic research, particularly in scenarios with minimal annotated data available.","In this paper, we introduce the DocTTT framework to address these challenges.","The key innovation of our approach is that it uses test-time training to adapt the model to each specific input during testing.","We propose a novel Meta-Auxiliary learning approach that combines Meta-learning and self-supervised Masked Autoencoder~(MAE).","During testing, we adapt the visual representation parameters using a self-supervised MAE loss.","During training, we learn the model parameters using a meta-learning framework, so that the model parameters are learned to adapt to a new input effectively.","Experimental results show that our proposed method significantly outperforms existing state-of-the-art approaches on benchmark datasets."],"url":"http://arxiv.org/abs/2501.12898v1"}
{"created":"2025-01-22 14:13:44","title":"Statistical Privacy","abstract":"To analyze the privacy guarantee of personal data in a database that is subject to queries it is necessary to model the prior knowledge of a possible attacker. Differential privacy considers a worst-case scenario where he knows almost everything, which in many applications is unrealistic and requires a large utility loss.   This paper considers a situation called statistical privacy where an adversary knows the distribution by which the database is generated, but no exact data of all (or sufficient many) of its entries. We analyze in detail how the entropy of the distribution guarantes privacy for a large class of queries called property queries. Exact formulas are obtained for the privacy parameters. We analyze how they depend on the probability that an entry fulfills the property under investigation. These formulas turn out to be lengthy, but can be used for tight numerical approximations of the privacy parameters. Such estimations are necessary for applying privacy enhancing techniques in practice. For this statistical setting we further investigate the effect of adding noise or applying subsampling and the privacy utility tradeoff. The dependencies on the parameters are illustrated in detail by a series of plots. Finally, these results are compared to the differential privacy model.","sentences":["To analyze the privacy guarantee of personal data in a database that is subject to queries it is necessary to model the prior knowledge of a possible attacker.","Differential privacy considers a worst-case scenario where he knows almost everything, which in many applications is unrealistic and requires a large utility loss.   ","This paper considers a situation called statistical privacy where an adversary knows the distribution by which the database is generated, but no exact data of all (or sufficient many) of its entries.","We analyze in detail how the entropy of the distribution guarantes privacy for a large class of queries called property queries.","Exact formulas are obtained for the privacy parameters.","We analyze how they depend on the probability that an entry fulfills the property under investigation.","These formulas turn out to be lengthy, but can be used for tight numerical approximations of the privacy parameters.","Such estimations are necessary for applying privacy enhancing techniques in practice.","For this statistical setting we further investigate the effect of adding noise or applying subsampling and the privacy utility tradeoff.","The dependencies on the parameters are illustrated in detail by a series of plots.","Finally, these results are compared to the differential privacy model."],"url":"http://arxiv.org/abs/2501.12893v1"}
{"created":"2025-01-22 14:10:08","title":"Analyzing and Exploiting Branch Mispredictions in Microcode","abstract":"We present uSpectre, a new class of transient execution attacks that exploit microcode branch mispredictions to transiently leak sensitive data. We find that many long-known and recently-discovered transient execution attacks, which were previously categorized as Spectre or Meltdown variants, are actually instances of uSpectre on some Intel microarchitectures. Based on our observations, we discover multiple new uSpectre attacks and present a defense against uSpectre vulnerabilities, called uSLH.","sentences":["We present uSpectre, a new class of transient execution attacks that exploit microcode branch mispredictions to transiently leak sensitive data.","We find that many long-known and recently-discovered transient execution attacks, which were previously categorized as Spectre or Meltdown variants, are actually instances of uSpectre on some Intel microarchitectures.","Based on our observations, we discover multiple new uSpectre attacks and present a defense against uSpectre vulnerabilities, called uSLH."],"url":"http://arxiv.org/abs/2501.12890v1"}
{"created":"2025-01-22 14:02:11","title":"Multi-Platform Aggregated Dataset of Online Communities (MADOC)","abstract":"The Multi-platform Aggregated Dataset of Online Communities (MADOC) is a comprehensive dataset that facilitates computational social science research by providing FAIR-compliant standardized access to cross-platform analysis of online social dynamics. MADOC aggregates and standardizes data from Bluesky, Koo, Reddit, and Voat (2012-2024), containing 18.9 million posts, 236 million comments, and 23.1 million unique users. The dataset enables comparative studies of toxic behavior evolution across platforms through standardized interaction records and sentiment analysis. By providing UUID-anonymized user histories and temporal alignment of banned communities' activity patterns, MADOC supports research on content moderation impacts and platform migration trends. Distributed via Zenodo with persistent identifiers and Python/R toolkits, the dataset adheres to FAIR principles while addressing post-API-era research challenges through ethical aggregation of public social media archives.","sentences":["The Multi-platform Aggregated Dataset of Online Communities (MADOC) is a comprehensive dataset that facilitates computational social science research by providing FAIR-compliant standardized access to cross-platform analysis of online social dynamics.","MADOC aggregates and standardizes data from Bluesky, Koo, Reddit, and Voat (2012-2024), containing 18.9 million posts, 236 million comments, and 23.1 million unique users.","The dataset enables comparative studies of toxic behavior evolution across platforms through standardized interaction records and sentiment analysis.","By providing UUID-anonymized user histories and temporal alignment of banned communities' activity patterns, MADOC supports research on content moderation impacts and platform migration trends.","Distributed via Zenodo with persistent identifiers and Python/R toolkits, the dataset adheres to FAIR principles while addressing post-API-era research challenges through ethical aggregation of public social media archives."],"url":"http://arxiv.org/abs/2501.12886v1"}
{"created":"2025-01-22 13:51:33","title":"Learning Graph Node Embeddings by Smooth Pair Sampling","abstract":"Random walk-based node embedding algorithms have attracted a lot of attention due to their scalability and ease of implementation. Previous research has focused on different walk strategies, optimization objectives, and embedding learning models. Inspired by observations on real data, we take a different approach and propose a new regularization technique. More precisely, the frequencies of node pairs generated by the skip-gram model on random walk node sequences follow a highly skewed distribution which causes learning to be dominated by a fraction of the pairs. We address the issue by designing an efficient sampling procedure that generates node pairs according to their {\\em smoothed frequency}. Theoretical and experimental results demonstrate the advantages of our approach.","sentences":["Random walk-based node embedding algorithms have attracted a lot of attention due to their scalability and ease of implementation.","Previous research has focused on different walk strategies, optimization objectives, and embedding learning models.","Inspired by observations on real data, we take a different approach and propose a new regularization technique.","More precisely, the frequencies of node pairs generated by the skip-gram model on random walk node sequences follow a highly skewed distribution which causes learning to be dominated by a fraction of the pairs.","We address the issue by designing an efficient sampling procedure that generates node pairs according to their {\\em smoothed frequency}.","Theoretical and experimental results demonstrate the advantages of our approach."],"url":"http://arxiv.org/abs/2501.12884v1"}
{"created":"2025-01-22 13:36:46","title":"WisdomBot: Tuning Large Language Models with Artificial Intelligence Knowledge","abstract":"Large language models (LLMs) have emerged as powerful tools in natural language processing (NLP), showing a promising future of artificial generated intelligence (AGI). Despite their notable performance in the general domain, LLMs have remained suboptimal in the field of education, owing to the unique challenges presented by this domain, such as the need for more specialized knowledge, the requirement for personalized learning experiences, and the necessity for concise explanations of complex concepts. To address these issues, this paper presents a novel LLM for education named WisdomBot, which combines the power of LLMs with educational theories, enabling their seamless integration into educational contexts. To be specific, we harness self-instructed knowledge concepts and instructions under the guidance of Bloom's Taxonomy as training data. To further enhance the accuracy and professionalism of model's response on factual questions, we introduce two key enhancements during inference, i.e., local knowledge base retrieval augmentation and search engine retrieval augmentation during inference. We substantiate the effectiveness of our approach by applying it to several Chinese LLMs, thereby showcasing that the fine-tuned models can generate more reliable and professional responses.","sentences":["Large language models (LLMs) have emerged as powerful tools in natural language processing (NLP), showing a promising future of artificial generated intelligence (AGI).","Despite their notable performance in the general domain, LLMs have remained suboptimal in the field of education, owing to the unique challenges presented by this domain, such as the need for more specialized knowledge, the requirement for personalized learning experiences, and the necessity for concise explanations of complex concepts.","To address these issues, this paper presents a novel LLM for education named WisdomBot, which combines the power of LLMs with educational theories, enabling their seamless integration into educational contexts.","To be specific, we harness self-instructed knowledge concepts and instructions under the guidance of Bloom's Taxonomy as training data.","To further enhance the accuracy and professionalism of model's response on factual questions, we introduce two key enhancements during inference, i.e., local knowledge base retrieval augmentation and search engine retrieval augmentation during inference.","We substantiate the effectiveness of our approach by applying it to several Chinese LLMs, thereby showcasing that the fine-tuned models can generate more reliable and professional responses."],"url":"http://arxiv.org/abs/2501.12877v1"}
{"created":"2025-01-22 13:09:09","title":"HierPromptLM: A Pure PLM-based Framework for Representation Learning on Heterogeneous Text-rich Networks","abstract":"Representation learning on heterogeneous text-rich networks (HTRNs), which consist of multiple types of nodes and edges with each node associated with textual information, is essential for various real-world applications. Given the success of pretrained language models (PLMs) in processing text data, recent efforts have focused on integrating PLMs into HTRN representation learning. These methods typically handle textual and structural information separately, using both PLMs and heterogeneous graph neural networks (HGNNs). However, this separation fails to capture the critical interactions between these two types of information within HTRNs. Additionally, it necessitates an extra alignment step, which is challenging due to the fundamental differences between distinct embedding spaces generated by PLMs and HGNNs. To deal with it, we propose HierPromptLM, a novel pure PLM-based framework that seamlessly models both text data and graph structures without the need for separate processing. Firstly, we develop a Hierarchical Prompt module that employs prompt learning to integrate text data and heterogeneous graph structures at both the node and edge levels, within a unified textual space. Building upon this foundation, we further introduce two innovative HTRN-tailored pretraining tasks to fine-tune PLMs for representation learning by emphasizing the inherent heterogeneity and interactions between textual and structural information within HTRNs. Extensive experiments on two real-world HTRN datasets demonstrate HierPromptLM outperforms state-of-the-art methods, achieving significant improvements of up to 6.08% for node classification and 10.84% for link prediction.","sentences":["Representation learning on heterogeneous text-rich networks (HTRNs), which consist of multiple types of nodes and edges with each node associated with textual information, is essential for various real-world applications.","Given the success of pretrained language models (PLMs) in processing text data, recent efforts have focused on integrating PLMs into HTRN representation learning.","These methods typically handle textual and structural information separately, using both PLMs and heterogeneous graph neural networks (HGNNs).","However, this separation fails to capture the critical interactions between these two types of information within HTRNs.","Additionally, it necessitates an extra alignment step, which is challenging due to the fundamental differences between distinct embedding spaces generated by PLMs and HGNNs.","To deal with it, we propose HierPromptLM, a novel pure PLM-based framework that seamlessly models both text data and graph structures without the need for separate processing.","Firstly, we develop a Hierarchical Prompt module that employs prompt learning to integrate text data and heterogeneous graph structures at both the node and edge levels, within a unified textual space.","Building upon this foundation, we further introduce two innovative HTRN-tailored pretraining tasks to fine-tune PLMs for representation learning by emphasizing the inherent heterogeneity and interactions between textual and structural information within HTRNs.","Extensive experiments on two real-world HTRN datasets demonstrate HierPromptLM outperforms state-of-the-art methods, achieving significant improvements of up to 6.08% for node classification and 10.84% for link prediction."],"url":"http://arxiv.org/abs/2501.12857v1"}
{"created":"2025-01-22 13:03:38","title":"Data-and-Semantic Dual-Driven Spectrum Map Construction for 6G Spectrum Management","abstract":"Spectrum maps reflect the utilization and distribution of spectrum resources in the electromagnetic environment, serving as an effective approach to support spectrum management. However, the construction of spectrum maps in urban environments is challenging because of high-density connection and complex terrain. Moreover, the existing spectrum map construction methods are typically applied to a fixed frequency, which cannot cover the entire frequency band. To address the aforementioned challenges, a UNet-based data-and-semantic dual-driven method is proposed by introducing the semantic knowledge of binary city maps and binary sampling location maps to enhance the accuracy of spectrum map construction in complex urban environments with dense communications. Moreover, a joint frequency-space reasoning model is exploited to capture the correlation of spectrum data in terms of space and frequency, enabling the realization of complete spectrum map construction without sampling all frequencies of spectrum data. The simulation results demonstrate that the proposed method can infer the spectrum utilization status of missing frequencies and improve the completeness of the spectrum map construction. Furthermore, the accuracy of spectrum map construction achieved by the proposed data-and-semantic dual-driven method outperforms the benchmark schemes, especially in scenarios with low sampling density.","sentences":["Spectrum maps reflect the utilization and distribution of spectrum resources in the electromagnetic environment, serving as an effective approach to support spectrum management.","However, the construction of spectrum maps in urban environments is challenging because of high-density connection and complex terrain.","Moreover, the existing spectrum map construction methods are typically applied to a fixed frequency, which cannot cover the entire frequency band.","To address the aforementioned challenges, a UNet-based data-and-semantic dual-driven method is proposed by introducing the semantic knowledge of binary city maps and binary sampling location maps to enhance the accuracy of spectrum map construction in complex urban environments with dense communications.","Moreover, a joint frequency-space reasoning model is exploited to capture the correlation of spectrum data in terms of space and frequency, enabling the realization of complete spectrum map construction without sampling all frequencies of spectrum data.","The simulation results demonstrate that the proposed method can infer the spectrum utilization status of missing frequencies and improve the completeness of the spectrum map construction.","Furthermore, the accuracy of spectrum map construction achieved by the proposed data-and-semantic dual-driven method outperforms the benchmark schemes, especially in scenarios with low sampling density."],"url":"http://arxiv.org/abs/2501.12853v1"}
{"created":"2025-01-22 12:59:08","title":"ACEBench: Who Wins the Match Point in Tool Learning?","abstract":"Large language models (LLMs) have demonstrated significant potential in decision-making and reasoning, especially when combined with various tools to effectively solve complex problems. However, existing evaluation systems for assessing LLM function calling capabilities have several limitations: (1) limited evaluation scenarios, lacking assessments in real multi-turn dialogue contexts; (2) narrow evaluation dimensions, lacking detailed assessments for fine-grained function calls; (3) relying on LLMs or real API executions for result evaluation, which introduces significant overhead. To address these issues, we propose a comprehensive evaluation system named ACEBench. This system is meticulously designed to encompass a wide spectrum of function calling scenarios. Moreover, it categorizes these scenarios into three primary types according to the evaluation methodology: Normal, Special, and Agent. Normal evaluates function calls in basic scenarios; Special evaluates function calls in scenarios with vague or incomplete instructions; Agent introduces multi-agent interactions to simulate function calling evaluation in real-world multi-turn interactions. We conducted extensive experiments on ACEBench, analyzing various LLMs in-depth and performing a more granular analysis of error causes across different data types.","sentences":["Large language models (LLMs) have demonstrated significant potential in decision-making and reasoning, especially when combined with various tools to effectively solve complex problems.","However, existing evaluation systems for assessing LLM function calling capabilities have several limitations: (1) limited evaluation scenarios, lacking assessments in real multi-turn dialogue contexts; (2) narrow evaluation dimensions, lacking detailed assessments for fine-grained function calls; (3) relying on LLMs or real API executions for result evaluation, which introduces significant overhead.","To address these issues, we propose a comprehensive evaluation system named ACEBench.","This system is meticulously designed to encompass a wide spectrum of function calling scenarios.","Moreover, it categorizes these scenarios into three primary types according to the evaluation methodology: Normal, Special, and Agent.","Normal evaluates function calls in basic scenarios; Special evaluates function calls in scenarios with vague or incomplete instructions; Agent introduces multi-agent interactions to simulate function calling evaluation in real-world multi-turn interactions.","We conducted extensive experiments on ACEBench, analyzing various LLMs in-depth and performing a more granular analysis of error causes across different data types."],"url":"http://arxiv.org/abs/2501.12851v1"}
{"created":"2025-01-22 12:51:58","title":"A Note on Deterministic FPTAS for Partition","abstract":"We consider the Partition problem and propose a deterministic FPTAS (Fully Polynomial-Time Approximation Scheme) that runs in $\\widetilde{O}(n + 1/\\varepsilon)$-time. This is the best possible (up to a polylogarithmic factor) assuming the Strong Exponential Time Hypothesis~[Abboud, Bringmann, Hermelin, and Shabtay'22]. Prior to our work, only a randomized algorithm can achieve a running time of $\\widetilde{O}(n + 1/\\varepsilon)$~[Chen, Lian, Mao and Zhang '24], while the best deterministic algorithm runs in $\\widetilde{O}(n+1/\\varepsilon^{5/4})$ time~[Deng, Jin and Mao '23] and [Wu and Chen '22].","sentences":["We consider the Partition problem and propose a deterministic FPTAS (Fully Polynomial-Time Approximation Scheme) that runs in $\\widetilde{O}(n + 1/\\varepsilon)$-time.","This is the best possible (up to a polylogarithmic factor) assuming the Strong Exponential Time Hypothesis~[Abboud, Bringmann, Hermelin, and Shabtay'22].","Prior to our work, only a randomized algorithm can achieve a running time of $\\widetilde{O}(n + 1/\\varepsilon)$~[Chen, Lian, Mao and Zhang '24], while the best deterministic algorithm runs in $\\widetilde{O}(n+1/\\varepsilon^{5/4})$ time~[Deng, Jin and Mao '23] and","[Wu and Chen '22]."],"url":"http://arxiv.org/abs/2501.12848v1"}
{"created":"2025-01-22 12:29:33","title":"AMM-Diff: Adaptive Multi-Modality Diffusion Network for Missing Modality Imputation","abstract":"In clinical practice, full imaging is not always feasible, often due to complex acquisition protocols, stringent privacy regulations, or specific clinical needs. However, missing MR modalities pose significant challenges for tasks like brain tumor segmentation, especially in deep learning-based segmentation, as each modality provides complementary information crucial for improving accuracy. A promising solution is missing data imputation, where absent modalities are generated from available ones. While generative models have been widely used for this purpose, most state-of-the-art approaches are limited to single or dual target translations, lacking the adaptability to generate missing modalities based on varying input configurations. To address this, we propose an Adaptive Multi-Modality Diffusion Network (AMM-Diff), a novel diffusion-based generative model capable of handling any number of input modalities and generating the missing ones. We designed an Image-Frequency Fusion Network (IFFN) that learns a unified feature representation through a self-supervised pretext task across the full input modalities and their selected high-frequency Fourier components. The proposed diffusion model leverages this representation, encapsulating prior knowledge of the complete modalities, and combines it with an adaptive reconstruction strategy to achieve missing modality completion. Experimental results on the BraTS 2021 dataset demonstrate the effectiveness of our approach.","sentences":["In clinical practice, full imaging is not always feasible, often due to complex acquisition protocols, stringent privacy regulations, or specific clinical needs.","However, missing MR modalities pose significant challenges for tasks like brain tumor segmentation, especially in deep learning-based segmentation, as each modality provides complementary information crucial for improving accuracy.","A promising solution is missing data imputation, where absent modalities are generated from available ones.","While generative models have been widely used for this purpose, most state-of-the-art approaches are limited to single or dual target translations, lacking the adaptability to generate missing modalities based on varying input configurations.","To address this, we propose an Adaptive Multi-Modality Diffusion Network (AMM-Diff), a novel diffusion-based generative model capable of handling any number of input modalities and generating the missing ones.","We designed an Image-Frequency Fusion Network (IFFN) that learns a unified feature representation through a self-supervised pretext task across the full input modalities and their selected high-frequency Fourier components.","The proposed diffusion model leverages this representation, encapsulating prior knowledge of the complete modalities, and combines it with an adaptive reconstruction strategy to achieve missing modality completion.","Experimental results on the BraTS 2021 dataset demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2501.12840v1"}
{"created":"2025-01-22 12:21:17","title":"Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back Home","abstract":"Retrieval Augmented Generation (RAG) improves correctness of Question Answering (QA) and addresses hallucinations in Large Language Models (LLMs), yet greatly increase computational costs. Besides, RAG is not always needed as may introduce irrelevant information. Recent adaptive retrieval methods integrate LLMs' intrinsic knowledge with external information appealing to LLM self-knowledge, but they often neglect efficiency evaluations and comparisons with uncertainty estimation techniques. We bridge this gap by conducting a comprehensive analysis of 35 adaptive retrieval methods, including 8 recent approaches and 27 uncertainty estimation techniques, across 6 datasets using 10 metrics for QA performance, self-knowledge, and efficiency. Our findings show that uncertainty estimation techniques often outperform complex pipelines in terms of efficiency and self-knowledge, while maintaining comparable QA performance.","sentences":["Retrieval Augmented Generation (RAG) improves correctness of Question Answering (QA) and addresses hallucinations in Large Language Models (LLMs), yet greatly increase computational costs.","Besides, RAG is not always needed as may introduce irrelevant information.","Recent adaptive retrieval methods integrate LLMs' intrinsic knowledge with external information appealing to LLM self-knowledge, but they often neglect efficiency evaluations and comparisons with uncertainty estimation techniques.","We bridge this gap by conducting a comprehensive analysis of 35 adaptive retrieval methods, including 8 recent approaches and 27 uncertainty estimation techniques, across 6 datasets using 10 metrics for QA performance, self-knowledge, and efficiency.","Our findings show that uncertainty estimation techniques often outperform complex pipelines in terms of efficiency and self-knowledge, while maintaining comparable QA performance."],"url":"http://arxiv.org/abs/2501.12835v1"}
{"created":"2025-01-22 12:16:30","title":"A transformer-based deep q learning approach for dynamic load balancing in software-defined networks","abstract":"This study proposes a novel approach for dynamic load balancing in Software-Defined Networks (SDNs) using a Transformer-based Deep Q-Network (DQN). Traditional load balancing mechanisms, such as Round Robin (RR) and Weighted Round Robin (WRR), are static and often struggle to adapt to fluctuating traffic conditions, leading to inefficiencies in network performance. In contrast, SDNs offer centralized control and flexibility, providing an ideal platform for implementing machine learning-driven optimization strategies. The core of this research combines a Temporal Fusion Transformer (TFT) for accurate traffic prediction with a DQN model to perform real-time dynamic load balancing. The TFT model predicts future traffic loads, which the DQN uses as input, allowing it to make intelligent routing decisions that optimize throughput, minimize latency, and reduce packet loss. The proposed model was tested against RR and WRR in simulated environments with varying data rates, and the results demonstrate significant improvements in network performance. For the 500MB data rate, the DQN model achieved an average throughput of 0.275 compared to 0.202 and 0.205 for RR and WRR, respectively. Additionally, the DQN recorded lower average latency and packet loss. In the 1000MB simulation, the DQN model outperformed the traditional methods in throughput, latency, and packet loss, reinforcing its effectiveness in managing network loads dynamically. This research presents an important step towards enhancing network performance through the integration of machine learning models within SDNs, potentially paving the way for more adaptive, intelligent network management systems.","sentences":["This study proposes a novel approach for dynamic load balancing in Software-Defined Networks (SDNs) using a Transformer-based Deep Q-Network (DQN).","Traditional load balancing mechanisms, such as Round Robin (RR) and Weighted Round Robin (WRR), are static and often struggle to adapt to fluctuating traffic conditions, leading to inefficiencies in network performance.","In contrast, SDNs offer centralized control and flexibility, providing an ideal platform for implementing machine learning-driven optimization strategies.","The core of this research combines a Temporal Fusion Transformer (TFT) for accurate traffic prediction with a DQN model to perform real-time dynamic load balancing.","The TFT model predicts future traffic loads, which the DQN uses as input, allowing it to make intelligent routing decisions that optimize throughput, minimize latency, and reduce packet loss.","The proposed model was tested against RR and WRR in simulated environments with varying data rates, and the results demonstrate significant improvements in network performance.","For the 500MB data rate, the DQN model achieved an average throughput of 0.275 compared to 0.202 and 0.205 for RR and WRR, respectively.","Additionally, the DQN recorded lower average latency and packet loss.","In the 1000MB simulation, the DQN model outperformed the traditional methods in throughput, latency, and packet loss, reinforcing its effectiveness in managing network loads dynamically.","This research presents an important step towards enhancing network performance through the integration of machine learning models within SDNs, potentially paving the way for more adaptive, intelligent network management systems."],"url":"http://arxiv.org/abs/2501.12829v1"}
{"created":"2025-01-22 12:07:36","title":"Does multi-block MEV exist? Analysis of 2 years of MEV Data","abstract":"This study analyzes proposer-builder data and MEV-Boost payment data following the Ethereum merge in September 2022 to identify patterns of multi-block MEV. Our findings reveal fewer multi-slot sequences of builders than predicted by a random Monte Carlo simulation, with the longest observed sequence spanning 25 slots. Additionally, we observe that average MEV-Boost payments increase with the length of consecutive sequences, from approximately 0.05 ETH for single slots to 0.08 ETH for nine consecutive slots. Within longer sequences, payments per slot show a slight increase, suggesting that builders bid higher for longer sequences or the first slot after a longer sequence. A weak positive autocorrelation is found between subsequent MEV-Boost payments, challenging the hypothesis of alternating periods of low and high MEV. Finally, our comparison of builders during periods of low and high base fee volatility reveals minimal correlation, indicating the absence of builder specialization based on base fee volatility.","sentences":["This study analyzes proposer-builder data and MEV-Boost payment data following the Ethereum merge in September 2022 to identify patterns of multi-block MEV.","Our findings reveal fewer multi-slot sequences of builders than predicted by a random Monte Carlo simulation, with the longest observed sequence spanning 25 slots.","Additionally, we observe that average MEV-Boost payments increase with the length of consecutive sequences, from approximately 0.05 ETH for single slots to 0.08 ETH for nine consecutive slots.","Within longer sequences, payments per slot show a slight increase, suggesting that builders bid higher for longer sequences or the first slot after a longer sequence.","A weak positive autocorrelation is found between subsequent MEV-Boost payments, challenging the hypothesis of alternating periods of low and high MEV.","Finally, our comparison of builders during periods of low and high base fee volatility reveals minimal correlation, indicating the absence of builder specialization based on base fee volatility."],"url":"http://arxiv.org/abs/2501.12827v1"}
{"created":"2025-01-22 12:06:16","title":"Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek","abstract":"Natural Language Processing (NLP) for lesser-resourced languages faces persistent challenges, including limited datasets, inherited biases from high-resource languages, and the need for domain-specific solutions. This study addresses these gaps for Modern Greek through three key contributions. First, we evaluate the performance of open-source (Llama-70b) and closed-source (GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset availability, revealing task-specific strengths, weaknesses, and parity in their performance. Second, we expand the scope of Greek NLP by reframing Authorship Attribution as a tool to assess potential data usage by LLMs in pre-training, with high 0-shot accuracy suggesting ethical implications for data provenance. Third, we showcase a legal NLP case study, where a Summarize, Translate, and Embed (STE) methodology outperforms the traditional TF-IDF approach for clustering \\emph{long} legal texts. Together, these contributions provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps in model evaluation, task innovation, and real-world impact.","sentences":["Natural Language Processing (NLP) for lesser-resourced languages faces persistent challenges, including limited datasets, inherited biases from high-resource languages, and the need for domain-specific solutions.","This study addresses these gaps for Modern Greek through three key contributions.","First, we evaluate the performance of open-source (Llama-70b) and closed-source (GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset availability, revealing task-specific strengths, weaknesses, and parity in their performance.","Second, we expand the scope of Greek NLP by reframing Authorship Attribution as a tool to assess potential data usage by LLMs in pre-training, with high 0-shot accuracy suggesting ethical implications for data provenance.","Third, we showcase a legal NLP case study, where a Summarize, Translate, and Embed (STE) methodology outperforms the traditional TF-IDF approach for clustering \\emph{long} legal texts.","Together, these contributions provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps in model evaluation, task innovation, and real-world impact."],"url":"http://arxiv.org/abs/2501.12826v1"}
{"created":"2025-01-22 12:04:58","title":"Enhancing Monocular Depth Estimation with Multi-Source Auxiliary Tasks","abstract":"Monocular depth estimation (MDE) is a challenging task in computer vision, often hindered by the cost and scarcity of high-quality labeled datasets. We tackle this challenge using auxiliary datasets from related vision tasks for an alternating training scheme with a shared decoder built on top of a pre-trained vision foundation model, while giving a higher weight to MDE. Through extensive experiments we demonstrate the benefits of incorporating various in-domain auxiliary datasets and tasks to improve MDE quality on average by ~11%. Our experimental analysis shows that auxiliary tasks have different impacts, confirming the importance of task selection, highlighting that quality gains are not achieved by merely adding data. Remarkably, our study reveals that using semantic segmentation datasets as Multi-Label Dense Classification (MLDC) often results in additional quality gains. Lastly, our method significantly improves the data efficiency for the considered MDE datasets, enhancing their quality while reducing their size by at least 80%. This paves the way for using auxiliary data from related tasks to improve MDE quality despite limited availability of high-quality labeled data. Code is available at https://jugit.fz-juelich.de/ias-8/mdeaux.","sentences":["Monocular depth estimation (MDE) is a challenging task in computer vision, often hindered by the cost and scarcity of high-quality labeled datasets.","We tackle this challenge using auxiliary datasets from related vision tasks for an alternating training scheme with a shared decoder built on top of a pre-trained vision foundation model, while giving a higher weight to MDE.","Through extensive experiments we demonstrate the benefits of incorporating various in-domain auxiliary datasets and tasks to improve MDE quality on average by ~11%.","Our experimental analysis shows that auxiliary tasks have different impacts, confirming the importance of task selection, highlighting that quality gains are not achieved by merely adding data.","Remarkably, our study reveals that using semantic segmentation datasets as Multi-Label Dense Classification (MLDC) often results in additional quality gains.","Lastly, our method significantly improves the data efficiency for the considered MDE datasets, enhancing their quality while reducing their size by at least 80%.","This paves the way for using auxiliary data from related tasks to improve MDE quality despite limited availability of high-quality labeled data.","Code is available at https://jugit.fz-juelich.de/ias-8/mdeaux."],"url":"http://arxiv.org/abs/2501.12824v1"}
{"created":"2025-01-22 11:42:19","title":"PSGSL: A Probabilistic Framework Integrating Semantic Scene Understanding and Gas Sensing for Gas Source Localization","abstract":"Semantic scene understanding allows a robotic agent to reason about problems in complex ways, using information from multiple and varied sensors to make deductions about a particular matter. As a result, this form of intelligent robotics is capable of performing more complex tasks and achieving more precise results than simpler approaches based on single data sources. However, these improved capabilities come at the cost of higher complexity, both computational and in terms of design. Due to the increased design complexity, formal approaches for exploiting semantic understanding become necessary.   We present here a probabilistic formulation for integrating semantic knowledge into the process of gas source localization (GSL). The problem of GSL poses many unsolved challenges, and proposed solutions need to contend with the constraining limitations of sensing hardware. By exploiting semantic scene understanding, we can leverage other sources of information, such as vision, to improve the estimation of the source location. We show how our formulation can be applied to pre-existing GSL algorithms and the effect that including semantic data has on the produced estimations of the location of the source.","sentences":["Semantic scene understanding allows a robotic agent to reason about problems in complex ways, using information from multiple and varied sensors to make deductions about a particular matter.","As a result, this form of intelligent robotics is capable of performing more complex tasks and achieving more precise results than simpler approaches based on single data sources.","However, these improved capabilities come at the cost of higher complexity, both computational and in terms of design.","Due to the increased design complexity, formal approaches for exploiting semantic understanding become necessary.   ","We present here a probabilistic formulation for integrating semantic knowledge into the process of gas source localization (GSL).","The problem of GSL poses many unsolved challenges, and proposed solutions need to contend with the constraining limitations of sensing hardware.","By exploiting semantic scene understanding, we can leverage other sources of information, such as vision, to improve the estimation of the source location.","We show how our formulation can be applied to pre-existing GSL algorithms and the effect that including semantic data has on the produced estimations of the location of the source."],"url":"http://arxiv.org/abs/2501.12812v1"}
{"created":"2025-01-22 11:41:44","title":"Unveiling Zero-Space Detection: A Novel Framework for Autonomous Ransomware Identification in High-Velocity Environments","abstract":"Modern cybersecurity landscapes increasingly demand sophisticated detection frameworks capable of identifying evolving threats with precision and adaptability. The proposed Zero-Space Detection framework introduces a novel approach that dynamically identifies latent behavioral patterns through unsupervised clustering and advanced deep learning techniques. Designed to address the limitations of signature-based and heuristic methods, it operates effectively in high-velocity environments by integrating multi-phase filtering and ensemble learning for refined decision-making. Experimental evaluation reveals high detection rates across diverse ransomware families, including LockBit, Conti, REvil, and BlackMatter, while maintaining low false positive rates and scalable performance. Computational overhead remains minimal, with average processing times ensuring compatibility with real-time systems even under peak operational loads. The framework demonstrates resilience against adversarial strategies such as obfuscation and encryption speed variability, which frequently challenge conventional detection systems. Analysis across multiple data sources highlights its versatility in handling diverse file types and operational contexts. Comprehensive metrics, including detection probability, latency, and resource efficiency, validate its efficacy under real-world conditions. Through its modular architecture, the framework achieves seamless integration with existing cybersecurity infrastructures without significant reconfiguration. The results demonstrate its robustness and scalability, offering a transformative paradigm for ransomware identification in dynamic and resource-constrained environments.","sentences":["Modern cybersecurity landscapes increasingly demand sophisticated detection frameworks capable of identifying evolving threats with precision and adaptability.","The proposed Zero-Space Detection framework introduces a novel approach that dynamically identifies latent behavioral patterns through unsupervised clustering and advanced deep learning techniques.","Designed to address the limitations of signature-based and heuristic methods, it operates effectively in high-velocity environments by integrating multi-phase filtering and ensemble learning for refined decision-making.","Experimental evaluation reveals high detection rates across diverse ransomware families, including LockBit, Conti, REvil, and BlackMatter, while maintaining low false positive rates and scalable performance.","Computational overhead remains minimal, with average processing times ensuring compatibility with real-time systems even under peak operational loads.","The framework demonstrates resilience against adversarial strategies such as obfuscation and encryption speed variability, which frequently challenge conventional detection systems.","Analysis across multiple data sources highlights its versatility in handling diverse file types and operational contexts.","Comprehensive metrics, including detection probability, latency, and resource efficiency, validate its efficacy under real-world conditions.","Through its modular architecture, the framework achieves seamless integration with existing cybersecurity infrastructures without significant reconfiguration.","The results demonstrate its robustness and scalability, offering a transformative paradigm for ransomware identification in dynamic and resource-constrained environments."],"url":"http://arxiv.org/abs/2501.12811v1"}
{"created":"2025-01-22 10:58:04","title":"Hybrid Losses for Hierarchical Embedding Learning","abstract":"In traditional supervised learning, the cross-entropy loss treats all incorrect predictions equally, ignoring the relevance or proximity of wrong labels to the correct answer. By leveraging a tree hierarchy for fine-grained labels, we investigate hybrid losses, such as generalised triplet and cross-entropy losses, to enforce similarity between labels within a multi-task learning framework. We propose metrics to evaluate the embedding space structure and assess the model's ability to generalise to unseen classes, that is, to infer similar classes for data belonging to unseen categories. Our experiments on OrchideaSOL, a four-level hierarchical instrument sound dataset with nearly 200 detailed categories, demonstrate that the proposed hybrid losses outperform previous works in classification, retrieval, embedding space structure, and generalisation.","sentences":["In traditional supervised learning, the cross-entropy loss treats all incorrect predictions equally, ignoring the relevance or proximity of wrong labels to the correct answer.","By leveraging a tree hierarchy for fine-grained labels, we investigate hybrid losses, such as generalised triplet and cross-entropy losses, to enforce similarity between labels within a multi-task learning framework.","We propose metrics to evaluate the embedding space structure and assess the model's ability to generalise to unseen classes, that is, to infer similar classes for data belonging to unseen categories.","Our experiments on OrchideaSOL, a four-level hierarchical instrument sound dataset with nearly 200 detailed categories, demonstrate that the proposed hybrid losses outperform previous works in classification, retrieval, embedding space structure, and generalisation."],"url":"http://arxiv.org/abs/2501.12796v1"}
{"created":"2025-01-22 10:54:04","title":"Comparative Performance Evaluation of 5G-TSN Applications in Indoor Factory Environments","abstract":"While technologies such as Time-Sensitive Networking (TSN) enhance the determinism, real-time capabilities, and reliability of Ethernet, future industrial networks will not just have wired connections, but are increasingly using wireless communication links. Wireless networks enable mobility, have lower costs, and are easier to deploy. However, for many industrial applications, wired connections remain the preferred choice, particularly those requiring strict latency boundaries and ultra-reliable data flows, such as for controlling machinery or managing power electronics. The emergence of 5G, with its Ultra-Reliable Low-Latency Communication (URLLC) capabilities, presents a new opportunity for wireless industrial networks. 5G promises to enable high data rates, ultra-low latency, and minimal jitter. However, as 5G networks include wired links from the base station towards the core network, a combination of 5G with time-sensitive networking is needed to guarantee stringent QoS requirements. In this paper, we evaluate 5G-TSN performance for different indoor factory applications and environments through simulations. Our findings demonstrate that 5G-TSN can effectively address latency-sensitive scenarios in indoor factories environments.","sentences":["While technologies such as Time-Sensitive Networking (TSN) enhance the determinism, real-time capabilities, and reliability of Ethernet, future industrial networks will not just have wired connections, but are increasingly using wireless communication links.","Wireless networks enable mobility, have lower costs, and are easier to deploy.","However, for many industrial applications, wired connections remain the preferred choice, particularly those requiring strict latency boundaries and ultra-reliable data flows, such as for controlling machinery or managing power electronics.","The emergence of 5G, with its Ultra-Reliable Low-Latency Communication (URLLC) capabilities, presents a new opportunity for wireless industrial networks.","5G promises to enable high data rates, ultra-low latency, and minimal jitter.","However, as 5G networks include wired links from the base station towards the core network, a combination of 5G with time-sensitive networking is needed to guarantee stringent QoS requirements.","In this paper, we evaluate 5G-TSN performance for different indoor factory applications and environments through simulations.","Our findings demonstrate that 5G-TSN can effectively address latency-sensitive scenarios in indoor factories environments."],"url":"http://arxiv.org/abs/2501.12792v1"}
{"created":"2025-01-22 10:47:08","title":"Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana","abstract":"Evaluating Retrieval-Augmented Generation (RAG) systems, especially in domain-specific contexts, requires benchmarks that address the distinctive requirements of the applicative scenario. Since real data can be hard to obtain, a common strategy is to use LLM-based methods to generate synthetic data. Existing solutions are general purpose: given a document, they generate a question to build a Q&A pair. However, although the generated questions can be individually good, they are typically not diverse enough to reasonably cover the different ways real end-users can interact with the RAG system. We introduce here DataMorgana, a tool for generating highly customizable and diverse synthetic Q&A benchmarks tailored to RAG applications. DataMorgana enables detailed configurations of user and question categories and provides control over their distribution within the benchmark. It uses a lightweight two-stage process, ensuring efficiency and fast iterations, while generating benchmarks that reflect the expected traffic. We conduct a thorough line of experiments, showing quantitatively and qualitatively that DataMorgana surpasses existing tools and approaches in producing lexically, syntactically, and semantically diverse question sets across domain-specific and general-knowledge corpora. DataMorgana will be made available to selected teams in the research community, as first beta testers, in the context of the upcoming SIGIR'2025 LiveRAG challenge to be announced in early February 2025.","sentences":["Evaluating Retrieval-Augmented Generation (RAG) systems, especially in domain-specific contexts, requires benchmarks that address the distinctive requirements of the applicative scenario.","Since real data can be hard to obtain, a common strategy is to use LLM-based methods to generate synthetic data.","Existing solutions are general purpose: given a document, they generate a question to build a Q&A pair.","However, although the generated questions can be individually good, they are typically not diverse enough to reasonably cover the different ways real end-users can interact with the RAG system.","We introduce here DataMorgana, a tool for generating highly customizable and diverse synthetic Q&A benchmarks tailored to RAG applications.","DataMorgana enables detailed configurations of user and question categories and provides control over their distribution within the benchmark.","It uses a lightweight two-stage process, ensuring efficiency and fast iterations, while generating benchmarks that reflect the expected traffic.","We conduct a thorough line of experiments, showing quantitatively and qualitatively that DataMorgana surpasses existing tools and approaches in producing lexically, syntactically, and semantically diverse question sets across domain-specific and general-knowledge corpora.","DataMorgana will be made available to selected teams in the research community, as first beta testers, in the context of the upcoming SIGIR'2025 LiveRAG challenge to be announced in early February 2025."],"url":"http://arxiv.org/abs/2501.12789v1"}
{"created":"2025-01-22 10:37:55","title":"Preserving Culinary Traditions. A Crowdsourced Digital Collection of Cookbooks","abstract":"Recipes of popular origin and handwritten cookbooks are often overlooked by scholars. Rag\\`u is a pilot project that tries to fill in this gap by gathering and digitising a collection of cookbooks belonging to the Italian traditional cuisine, and making it accessible via a digital platform. The project aims at contributing to two research lines: a) to identify agile methods for publishing data in a low-cost crowdsourcing project, and b) to devise an effective storytelling journey for the Rag\\`u project.","sentences":["Recipes of popular origin and handwritten cookbooks are often overlooked by scholars.","Rag\\`u is a pilot project that tries to fill in this gap by gathering and digitising a collection of cookbooks belonging to the Italian traditional cuisine, and making it accessible via a digital platform.","The project aims at contributing to two research lines: a) to identify agile methods for publishing data in a low-cost crowdsourcing project, and b) to devise an effective storytelling journey for the Rag\\`u project."],"url":"http://arxiv.org/abs/2501.12786v1"}
{"created":"2025-01-22 10:16:53","title":"LLMs as Repositories of Factual Knowledge: Limitations and Solutions","abstract":"LLMs' sources of knowledge are data snapshots containing factual information about entities collected at different timestamps and from different media types (e.g. wikis, social media, etc.). Such unstructured knowledge is subject to change due to updates through time from past to present. Equally important are the inconsistencies and inaccuracies occurring in different information sources. Consequently, the model's knowledge about an entity may be perturbed while training over the sequence of snapshots or at inference time, resulting in inconsistent and inaccurate model performance. In this work, we study the appropriateness of Large Language Models (LLMs) as repositories of factual knowledge. We consider twenty-four state-of-the-art LLMs that are either closed-, partially (weights), or fully (weight and training data) open-source. We evaluate their reliability in responding to time-sensitive factual questions in terms of accuracy and consistency when prompts are perturbed. We further evaluate the effectiveness of state-of-the-art methods to improve LLMs' accuracy and consistency. We then propose \"ENtity-Aware Fine-tuning\" (ENAF), a soft neurosymbolic approach aimed at providing a structured representation of entities during fine-tuning to improve the model's performance.","sentences":["LLMs' sources of knowledge are data snapshots containing factual information about entities collected at different timestamps and from different media types (e.g. wikis, social media, etc.).","Such unstructured knowledge is subject to change due to updates through time from past to present.","Equally important are the inconsistencies and inaccuracies occurring in different information sources.","Consequently, the model's knowledge about an entity may be perturbed while training over the sequence of snapshots or at inference time, resulting in inconsistent and inaccurate model performance.","In this work, we study the appropriateness of Large Language Models (LLMs) as repositories of factual knowledge.","We consider twenty-four state-of-the-art LLMs that are either closed-, partially (weights), or fully (weight and training data) open-source.","We evaluate their reliability in responding to time-sensitive factual questions in terms of accuracy and consistency when prompts are perturbed.","We further evaluate the effectiveness of state-of-the-art methods to improve LLMs' accuracy and consistency.","We then propose \"ENtity-Aware Fine-tuning\" (ENAF), a soft neurosymbolic approach aimed at providing a structured representation of entities during fine-tuning to improve the model's performance."],"url":"http://arxiv.org/abs/2501.12774v1"}
{"created":"2025-01-22 10:14:27","title":"Non-adaptive Learning of Random Hypergraphs with Queries","abstract":"We study the problem of learning a hidden hypergraph $G=(V,E)$ by making a single batch of queries (non-adaptively). We consider the hyperedge detection model, in which every query must be of the form:   ``Does this set $S\\subseteq V$ contain at least one full hyperedge?''   In this model, it is known that there is no algorithm that allows to non-adaptively learn arbitrary hypergraphs by making fewer than $\\Omega(\\min\\{m^2\\log n, n^2\\})$ even when the hypergraph is constrained to be $2$-uniform (i.e. the hypergraph is simply a graph). Recently, Li et al. overcame this lower bound in the setting in which $G$ is a graph by assuming that the graph learned is sampled from an Erd\\H{o}s-R\\'enyi model. We generalize the result of Li et al. to the setting of random $k$-uniform hypergraphs. To achieve this result, we leverage a novel equivalence between the problem of learning a single hyperedge and the standard group testing problem. This latter result may also be of independent interest.","sentences":["We study the problem of learning a hidden hypergraph $G=(V,E)$ by making a single batch of queries (non-adaptively).","We consider the hyperedge detection model, in which every query must be of the form:   ``Does this set $S\\subseteq V$ contain at least one full hyperedge?''   ","In this model, it is known that there is no algorithm that allows to non-adaptively learn arbitrary hypergraphs by making fewer than $\\Omega(\\min\\{m^2\\log n, n^2\\})$ even when the hypergraph is constrained to be $2$-uniform (i.e. the hypergraph is simply a graph).","Recently, Li et al. overcame this lower bound in the setting in which $G$ is a graph by assuming that the graph learned is sampled from an Erd\\H{o}s-R\\'enyi model.","We generalize the result of Li et al. to the setting of random $k$-uniform hypergraphs.","To achieve this result, we leverage a novel equivalence between the problem of learning a single hyperedge and the standard group testing problem.","This latter result may also be of independent interest."],"url":"http://arxiv.org/abs/2501.12771v1"}
{"created":"2025-01-22 10:12:18","title":"On Tradeoffs in Learning-Augmented Algorithms","abstract":"The field of learning-augmented algorithms has gained significant attention in recent years. These algorithms, using potentially inaccurate predictions, must exhibit three key properties: consistency, robustness, and smoothness. In scenarios where distributional information about predictions is available, a strong expected performance is required. Typically, the design of these algorithms involves a natural tradeoff between consistency and robustness, and previous works aimed to achieve Pareto-optimal tradeoffs for specific problems. However, in some settings, this comes at the expense of smoothness. This paper demonstrates that certain problems involve multiple tradeoffs between consistency, robustness, smoothness, and average performance.","sentences":["The field of learning-augmented algorithms has gained significant attention in recent years.","These algorithms, using potentially inaccurate predictions, must exhibit three key properties: consistency, robustness, and smoothness.","In scenarios where distributional information about predictions is available, a strong expected performance is required.","Typically, the design of these algorithms involves a natural tradeoff between consistency and robustness, and previous works aimed to achieve Pareto-optimal tradeoffs for specific problems.","However, in some settings, this comes at the expense of smoothness.","This paper demonstrates that certain problems involve multiple tradeoffs between consistency, robustness, smoothness, and average performance."],"url":"http://arxiv.org/abs/2501.12770v1"}
{"created":"2025-01-22 10:01:54","title":"NExtLong: Toward Effective Long-Context Training without Long Documents","abstract":"Large language models (LLMs) with extended context windows have made significant strides yet remain a challenge due to the scarcity of long documents. Existing methods tend to synthesize long-context data but lack a clear mechanism to reinforce the long-range dependency modeling. To address this limitation, we propose NExtLong, a novel framework for synthesizing long-context data through Negative document Extension. NExtLong decomposes a document into multiple meta-chunks and extends the context by interleaving hard negative distractors retrieved from pretraining corpora. This approach compels the model to discriminate long-range dependent context from distracting content, enhancing its ability to model long-range dependencies. Extensive experiments demonstrate that NExtLong achieves significant performance improvements on the HELMET and RULER benchmarks compared to existing long-context synthesis approaches and leading models, which are trained on non-synthetic long documents. These findings highlight NExtLong's ability to reduce reliance on non-synthetic long documents, making it an effective framework for developing advanced long-context LLMs.","sentences":["Large language models (LLMs) with extended context windows have made significant strides yet remain a challenge due to the scarcity of long documents.","Existing methods tend to synthesize long-context data but lack a clear mechanism to reinforce the long-range dependency modeling.","To address this limitation, we propose NExtLong, a novel framework for synthesizing long-context data through Negative document Extension.","NExtLong decomposes a document into multiple meta-chunks and extends the context by interleaving hard negative distractors retrieved from pretraining corpora.","This approach compels the model to discriminate long-range dependent context from distracting content, enhancing its ability to model long-range dependencies.","Extensive experiments demonstrate that NExtLong achieves significant performance improvements on the HELMET and RULER benchmarks compared to existing long-context synthesis approaches and leading models, which are trained on non-synthetic long documents.","These findings highlight NExtLong's ability to reduce reliance on non-synthetic long documents, making it an effective framework for developing advanced long-context LLMs."],"url":"http://arxiv.org/abs/2501.12766v1"}
{"created":"2025-01-22 09:42:57","title":"A topology optimisation framework to design test specimens for one-shot identification or discovery of material models","abstract":"The increasing availability of full-field displacement data from imaging techniques in experimental mechanics is determining a gradual shift in the paradigm of material model calibration and discovery, from using several simple-geometry tests towards a few, or even one single test with complicated geometry. The feasibility of such a \"one-shot\" calibration or discovery heavily relies upon the richness of the measured displacement data, i.e., their ability to probe the space of the state variables and the stress space (whereby the stresses depend on the constitutive law being sought) to an extent sufficient for an accurate and robust calibration or discovery process. The richness of the displacement data is in turn directly governed by the specimen geometry. In this paper, we propose a density-based topology optimisation framework to optimally design the geometry of the target specimen for calibration of an anisotropic elastic material model. To this end, we perform automatic, high-resolution specimen design by maximising the robustness of the solution of the inverse problem, i.e., the identified material parameters, given noisy displacement measurements from digital image correlation. We discuss the choice of the cost function and the design of the topology optimisation framework, and we analyse a range of optimised topologies generated for the identification of isotropic and anisotropic elastic responses.","sentences":["The increasing availability of full-field displacement data from imaging techniques in experimental mechanics is determining a gradual shift in the paradigm of material model calibration and discovery, from using several simple-geometry tests towards a few, or even one single test with complicated geometry.","The feasibility of such a \"one-shot\" calibration or discovery heavily relies upon the richness of the measured displacement data, i.e., their ability to probe the space of the state variables and the stress space (whereby the stresses depend on the constitutive law being sought) to an extent sufficient for an accurate and robust calibration or discovery process.","The richness of the displacement data is in turn directly governed by the specimen geometry.","In this paper, we propose a density-based topology optimisation framework to optimally design the geometry of the target specimen for calibration of an anisotropic elastic material model.","To this end, we perform automatic, high-resolution specimen design by maximising the robustness of the solution of the inverse problem, i.e., the identified material parameters, given noisy displacement measurements from digital image correlation.","We discuss the choice of the cost function and the design of the topology optimisation framework, and we analyse a range of optimised topologies generated for the identification of isotropic and anisotropic elastic responses."],"url":"http://arxiv.org/abs/2501.12756v1"}
{"created":"2025-01-22 09:35:58","title":"Estimating the Conformal Prediction Threshold from Noisy Labels","abstract":"Conformal Prediction (CP) is a method to control prediction uncertainty by producing a small prediction set, ensuring a predetermined probability that the true class lies within this set. This is commonly done by defining a score, based on the model predictions, and setting a threshold on this score using a validation set. In this study, we address the problem of CP calibration when we only have access to a validation set with noisy labels. We show how we can estimate the noise-free conformal threshold based on the noisy labeled data. Our solution is flexible and can accommodate various modeling assumptions regarding the label contamination process, without needing any information about the underlying data distribution or the internal mechanisms of the machine learning classifier. We develop a coverage guarantee for uniform noise that is effective even in tasks with a large number of classes. We dub our approach Noise-Aware Conformal Prediction (NACP) and show on several natural and medical image classification datasets, including ImageNet, that it significantly outperforms current noisy label methods and achieves results comparable to those obtained with a clean validation set.","sentences":["Conformal Prediction (CP) is a method to control prediction uncertainty by producing a small prediction set, ensuring a predetermined probability that the true class lies within this set.","This is commonly done by defining a score, based on the model predictions, and setting a threshold on this score using a validation set.","In this study, we address the problem of CP calibration when we only have access to a validation set with noisy labels.","We show how we can estimate the noise-free conformal threshold based on the noisy labeled data.","Our solution is flexible and can accommodate various modeling assumptions regarding the label contamination process, without needing any information about the underlying data distribution or the internal mechanisms of the machine learning classifier.","We develop a coverage guarantee for uniform noise that is effective even in tasks with a large number of classes.","We dub our approach Noise-Aware Conformal Prediction (NACP) and show on several natural and medical image classification datasets, including ImageNet, that it significantly outperforms current noisy label methods and achieves results comparable to those obtained with a clean validation set."],"url":"http://arxiv.org/abs/2501.12749v1"}
{"created":"2025-01-22 09:12:16","title":"Bad-PFL: Exploring Backdoor Attacks against Personalized Federated Learning","abstract":"Data heterogeneity and backdoor attacks rank among the most significant challenges facing federated learning (FL). For data heterogeneity, personalized federated learning (PFL) enables each client to maintain a private personalized model to cater to client-specific knowledge. Meanwhile, vanilla FL has proven vulnerable to backdoor attacks. However, recent advancements in PFL community have demonstrated a potential immunity against such attacks. This paper explores this intersection further, revealing that existing federated backdoor attacks fail in PFL because backdoors about manually designed triggers struggle to survive in personalized models. To tackle this, we design Bad-PFL, which employs features from natural data as our trigger. As long as the model is trained on natural data, it inevitably embeds the backdoor associated with our trigger, ensuring its longevity in personalized models. Moreover, our trigger undergoes mutual reinforcement training with the model, further solidifying the backdoor's durability and enhancing attack effectiveness. The large-scale experiments across three benchmark datasets demonstrate the superior performance of our attack against various PFL methods, even when equipped with state-of-the-art defense mechanisms.","sentences":["Data heterogeneity and backdoor attacks rank among the most significant challenges facing federated learning (FL).","For data heterogeneity, personalized federated learning (PFL) enables each client to maintain a private personalized model to cater to client-specific knowledge.","Meanwhile, vanilla FL has proven vulnerable to backdoor attacks.","However, recent advancements in PFL community have demonstrated a potential immunity against such attacks.","This paper explores this intersection further, revealing that existing federated backdoor attacks fail in PFL because backdoors about manually designed triggers struggle to survive in personalized models.","To tackle this, we design Bad-PFL, which employs features from natural data as our trigger.","As long as the model is trained on natural data, it inevitably embeds the backdoor associated with our trigger, ensuring its longevity in personalized models.","Moreover, our trigger undergoes mutual reinforcement training with the model, further solidifying the backdoor's durability and enhancing attack effectiveness.","The large-scale experiments across three benchmark datasets demonstrate the superior performance of our attack against various PFL methods, even when equipped with state-of-the-art defense mechanisms."],"url":"http://arxiv.org/abs/2501.12736v1"}
{"created":"2025-01-22 09:12:09","title":"Online Preference Alignment for Language Models via Count-based Exploration","abstract":"Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage, and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e. \\emph{how to explore} for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named \\emph{Count-based Online Preference Optimization (COPO)}, which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences.","Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage, and the resulting reward model is hard to generalize in out-of-distribution responses.","Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs.","In this paper, we study the fundamental problem in online RLHF, i.e. \\emph{how to explore} for LLM.","We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy.","Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus.","We further propose a practical algorithm, named \\emph{Count-based Online Preference Optimization (COPO)}, which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data.","COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies.","We conduct online RLHF experiments on Zephyr and Llama-3 models.","The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance."],"url":"http://arxiv.org/abs/2501.12735v1"}
{"created":"2025-01-22 09:09:17","title":"GRAMA: Adaptive Graph Autoregressive Moving Average Models","abstract":"Graph State Space Models (SSMs) have recently been introduced to enhance Graph Neural Networks (GNNs) in modeling long-range interactions. Despite their success, existing methods either compromise on permutation equivariance or limit their focus to pairwise interactions rather than sequences. Building on the connection between Autoregressive Moving Average (ARMA) and SSM, in this paper, we introduce GRAMA, a Graph Adaptive method based on a learnable Autoregressive Moving Average (ARMA) framework that addresses these limitations. By transforming from static to sequential graph data, GRAMA leverages the strengths of the ARMA framework, while preserving permutation equivariance. Moreover, GRAMA incorporates a selective attention mechanism for dynamic learning of ARMA coefficients, enabling efficient and flexible long-range information propagation. We also establish theoretical connections between GRAMA and Selective SSMs, providing insights into its ability to capture long-range dependencies. Extensive experiments on 14 synthetic and real-world datasets demonstrate that GRAMA consistently outperforms backbone models and performs competitively with state-of-the-art methods.","sentences":["Graph State Space Models (SSMs) have recently been introduced to enhance Graph Neural Networks (GNNs) in modeling long-range interactions.","Despite their success, existing methods either compromise on permutation equivariance or limit their focus to pairwise interactions rather than sequences.","Building on the connection between Autoregressive Moving Average (ARMA) and SSM, in this paper, we introduce GRAMA, a Graph Adaptive method based on a learnable Autoregressive Moving Average (ARMA) framework that addresses these limitations.","By transforming from static to sequential graph data, GRAMA leverages the strengths of the ARMA framework, while preserving permutation equivariance.","Moreover, GRAMA incorporates a selective attention mechanism for dynamic learning of ARMA coefficients, enabling efficient and flexible long-range information propagation.","We also establish theoretical connections between GRAMA and Selective SSMs, providing insights into its ability to capture long-range dependencies.","Extensive experiments on 14 synthetic and real-world datasets demonstrate that GRAMA consistently outperforms backbone models and performs competitively with state-of-the-art methods."],"url":"http://arxiv.org/abs/2501.12732v1"}
{"created":"2025-01-22 09:05:01","title":"A Call for Critically Rethinking and Reforming Data Analysis in Empirical Software Engineering","abstract":"Context: Empirical Software Engineering (ESE) drives innovation in SE through qualitative and quantitative studies. However, concerns about the correct application of empirical methodologies have existed since the 2006 Dagstuhl seminar on SE. Objective: To analyze three decades of SE research, identify mistakes in statistical methods, and evaluate experts' ability to detect and address these issues. Methods: We conducted a literature survey of ~27,000 empirical studies, using LLMs to classify statistical methodologies as adequate or inadequate. Additionally, we selected 30 primary studies and held a workshop with 33 ESE experts to assess their ability to identify and resolve statistical issues. Results: Significant statistical issues were found in the primary studies, and experts showed limited ability to detect and correct these methodological problems, raising concerns about the broader ESE community's proficiency in this area. Conclusions. Despite our study's eventual limitations, its results shed light on recurring issues from promoting information copy-and-paste from past authors' works and the continuous publication of inadequate approaches that promote dubious results and jeopardize the spread of the correct statistical strategies among researchers. Besides, it justifies further investigation into empirical rigor in software engineering to expose these recurring issues and establish a framework for reassessing our field's foundation of statistical methodology application. Therefore, this work calls for critically rethinking and reforming data analysis in empirical software engineering, paving the way for our work soon.","sentences":["Context: Empirical Software Engineering (ESE) drives innovation in SE through qualitative and quantitative studies.","However, concerns about the correct application of empirical methodologies have existed since the 2006 Dagstuhl seminar on SE.","Objective: To analyze three decades of SE research, identify mistakes in statistical methods, and evaluate experts' ability to detect and address these issues.","Methods: We conducted a literature survey of ~27,000 empirical studies, using LLMs to classify statistical methodologies as adequate or inadequate.","Additionally, we selected 30 primary studies and held a workshop with 33 ESE experts to assess their ability to identify and resolve statistical issues.","Results:","Significant statistical issues were found in the primary studies, and experts showed limited ability to detect and correct these methodological problems, raising concerns about the broader ESE community's proficiency in this area.","Conclusions.","Despite our study's eventual limitations, its results shed light on recurring issues from promoting information copy-and-paste from past authors' works and the continuous publication of inadequate approaches that promote dubious results and jeopardize the spread of the correct statistical strategies among researchers.","Besides, it justifies further investigation into empirical rigor in software engineering to expose these recurring issues and establish a framework for reassessing our field's foundation of statistical methodology application.","Therefore, this work calls for critically rethinking and reforming data analysis in empirical software engineering, paving the way for our work soon."],"url":"http://arxiv.org/abs/2501.12728v1"}
{"created":"2025-01-22 08:53:12","title":"Anomaly Detection in Double-entry Bookkeeping Data by Federated Learning System with Non-model Sharing Approach","abstract":"Anomaly detection is crucial in financial auditing and effective detection often requires obtaining large volumes of data from multiple organizations. However, confidentiality concerns hinder data sharing among audit firms. Although the federated learning (FL)-based approach, FedAvg, has been proposed to address this challenge, its use of mutiple communication rounds increases its overhead, limiting its practicality. In this study, we propose a novel framework employing Data Collaboration (DC) analysis -- a non-model share-type FL method -- to streamline model training into a single communication round. Our method first encodes journal entry data via dimensionality reduction to obtain secure intermediate representations, then transforms them into collaboration representations for building an autoencoder that detects anomalies. We evaluate our approach on a synthetic dataset and real journal entry data from multiple organizations. The results show that our method not only outperforms single-organization baselines but also exceeds FedAvg in non-i.i.d. experiments on real journal entry data that closely mirror real-world conditions. By preserving data confidentiality and reducing iterative communication, this study addresses a key auditing challenge -- ensuring data confidentiality while integrating knowledge from multiple audit firms. Our findings represent a significant advance in artificial intelligence-driven auditing and underscore the potential of FL methods in high-security domains.","sentences":["Anomaly detection is crucial in financial auditing and effective detection often requires obtaining large volumes of data from multiple organizations.","However, confidentiality concerns hinder data sharing among audit firms.","Although the federated learning (FL)-based approach, FedAvg, has been proposed to address this challenge, its use of mutiple communication rounds increases its overhead, limiting its practicality.","In this study, we propose a novel framework employing Data Collaboration (DC) analysis -- a non-model share-type FL method -- to streamline model training into a single communication round.","Our method first encodes journal entry data via dimensionality reduction to obtain secure intermediate representations, then transforms them into collaboration representations for building an autoencoder that detects anomalies.","We evaluate our approach on a synthetic dataset and real journal entry data from multiple organizations.","The results show that our method not only outperforms single-organization baselines but also exceeds FedAvg in non-i.i.d. experiments on real journal entry data that closely mirror real-world conditions.","By preserving data confidentiality and reducing iterative communication, this study addresses a key auditing challenge -- ensuring data confidentiality while integrating knowledge from multiple audit firms.","Our findings represent a significant advance in artificial intelligence-driven auditing and underscore the potential of FL methods in high-security domains."],"url":"http://arxiv.org/abs/2501.12723v1"}
{"created":"2025-01-22 08:49:44","title":"A systematic data characteristic understanding framework towards physical-sensor big data challenges","abstract":"Big data present new opportunities for modern society while posing challenges for data scientists. Recent advancements in sensor networks and the widespread adoption of IoT have led to the collection of physical-sensor data on an enormous scale. However, significant challenges arise in high-quality big data analytics. To uncover big data challenges and enhance data quality, it is essential to quantitatively unveil data characteristics. Furthermore, the existing studies lack analysis of the specific time-related characteristics. Enhancing the efficiency and precision of data analytics through the big data lifecycle requires a comprehensive understanding of data characteristics to address the hidden big data challenges. To fill in the research gap, this paper proposes a systematic data characteristic framework based on a 6Vs model. The framework aims to unveil the data characteristics in terms of data volume, variety, velocity, veracity, value, and variability through a set of statistical indicators. This model improves the objectivity of data characteristic understanding by relying solely on data-driven indicators. The indicators related to time-related characteristics in physical-sensor data are also included. Furthermore, the big data challenges are linked to each dimension of the 6Vs model to gain a quantitative understanding of the data challenges. Finally, a pipeline is developed to implement the proposed framework, and two case studies are conducted to illustrate the process of understanding the physical-sensor data characteristics and making recommendations for data preprocessing to address the big data challenges. The proposed framework is able to analyze the characteristics of all physical-sensor data, therefore, identifying potential challenges in subsequent analytics, and providing recommendations for data preprocessing.","sentences":["Big data present new opportunities for modern society while posing challenges for data scientists.","Recent advancements in sensor networks and the widespread adoption of IoT have led to the collection of physical-sensor data on an enormous scale.","However, significant challenges arise in high-quality big data analytics.","To uncover big data challenges and enhance data quality, it is essential to quantitatively unveil data characteristics.","Furthermore, the existing studies lack analysis of the specific time-related characteristics.","Enhancing the efficiency and precision of data analytics through the big data lifecycle requires a comprehensive understanding of data characteristics to address the hidden big data challenges.","To fill in the research gap, this paper proposes a systematic data characteristic framework based on a 6Vs model.","The framework aims to unveil the data characteristics in terms of data volume, variety, velocity, veracity, value, and variability through a set of statistical indicators.","This model improves the objectivity of data characteristic understanding by relying solely on data-driven indicators.","The indicators related to time-related characteristics in physical-sensor data are also included.","Furthermore, the big data challenges are linked to each dimension of the 6Vs model to gain a quantitative understanding of the data challenges.","Finally, a pipeline is developed to implement the proposed framework, and two case studies are conducted to illustrate the process of understanding the physical-sensor data characteristics and making recommendations for data preprocessing to address the big data challenges.","The proposed framework is able to analyze the characteristics of all physical-sensor data, therefore, identifying potential challenges in subsequent analytics, and providing recommendations for data preprocessing."],"url":"http://arxiv.org/abs/2501.12720v1"}
{"created":"2025-01-22 08:24:22","title":"Making Temporal Betweenness Computation Faster and Restless","abstract":"Bu{\\ss} et al [KDD 2020] recently proved that the problem of computing the betweenness of all nodes of a temporal graph is computationally hard in the case of foremost and fastest paths, while it is solvable in time O(n 3 T 2 ) in the case of shortest and shortest foremost paths, where n is the number of nodes and T is the number of distinct time steps. A new algorithm for temporal betweenness computation is introduced in this paper. In the case of shortest and shortest foremost paths, it requires O(n + M ) space and runs in time where M is the number of temporal edges, thus significantly improving the algorithm of Bu{\\ss} et al in terms of time complexity (note that T is usually large). Experimental evidence is provided that our algorithm performs between twice and almost 250 times better than the algorithm of Bu{\\ss} et al. Moreover, we were able to compute the exact temporal betweenness values of several large temporal graphs with over a million of temporal edges. For such size, only approximate computation was possible by using the algorithm of Santoro and Sarpe [WWW 2022]. Maybe more importantly, our algorithm extends to the case of restless walks (that is, walks with waiting constraints in each node), thus providing a polynomial-time algorithm (with complexity O(nM )) for computing the temporal betweenness in the case of several different optimality criteria. Such restless computation was known only for the shortest criterion (Rymar et al [JGAA 2023]), with complexity O(n 2 M T 2 ). We performed an extensive experimental validation by comparing different waiting constraints and different optimisation criteria. Moreover, as a case study, we investigate six public transit networks including Berlin, Rome, and Paris. Overall we find a general consistency between the different variants of betweenness centrality. However, we do measure a sensible influence of waiting constraints, and note some cases of low correlation for certain pairs of criteria in some networks.","sentences":["Bu{\\ss} et","al [KDD 2020] recently proved that the problem of computing the betweenness of all nodes of a temporal graph is computationally hard in the case of foremost and fastest paths, while it is solvable in time O(n 3 T 2 ) in the case of shortest and shortest foremost paths, where n is the number of nodes and T is the number of distinct time steps.","A new algorithm for temporal betweenness computation is introduced in this paper.","In the case of shortest and shortest foremost paths, it requires O(n","+ M ) space and runs in time where M is the number of temporal edges, thus significantly improving the algorithm of Bu{\\ss} et al in terms of time complexity (note that T is usually large).","Experimental evidence is provided that our algorithm performs between twice and almost 250 times better than the algorithm of Bu{\\ss} et al.","Moreover, we were able to compute the exact temporal betweenness values of several large temporal graphs with over a million of temporal edges.","For such size, only approximate computation was possible by using the algorithm of Santoro and Sarpe [WWW 2022].","Maybe more importantly, our algorithm extends to the case of restless walks (that is, walks with waiting constraints in each node), thus providing a polynomial-time algorithm (with complexity O(nM )) for computing the temporal betweenness in the case of several different optimality criteria.","Such restless computation was known only for the shortest criterion (Rymar et al [JGAA 2023]), with complexity O(n","2 M T 2 ).","We performed an extensive experimental validation by comparing different waiting constraints and different optimisation criteria.","Moreover, as a case study, we investigate six public transit networks including Berlin, Rome, and Paris.","Overall we find a general consistency between the different variants of betweenness centrality.","However, we do measure a sensible influence of waiting constraints, and note some cases of low correlation for certain pairs of criteria in some networks."],"url":"http://arxiv.org/abs/2501.12708v1"}
{"created":"2025-01-22 08:23:10","title":"REX: Causal Discovery based on Machine Learning and Explainability techniques","abstract":"Explainability techniques hold significant potential for enhancing the causal discovery process, which is crucial for understanding complex systems in areas like healthcare, economics, and artificial intelligence. However, no causal discovery methods currently incorporate explainability into their models to derive causal graphs. Thus, in this paper we explore this innovative approach, as it offers substantial potential and represents a promising new direction worth investigating. Specifically, we introduce REX, a causal discovery method that leverages machine learning (ML) models coupled with explainability techniques, specifically Shapley values, to identify and interpret significant causal relationships among variables.   Comparative evaluations on synthetic datasets comprising continuous tabular data reveal that REX outperforms state-of-the-art causal discovery methods across diverse data generation processes, including non-linear and additive noise models. Moreover, REX was tested on the Sachs single-cell protein-signaling dataset, achieving a precision of 0.952 and recovering key causal relationships with no incorrect edges. Taking together, these results showcase REX's effectiveness in accurately recovering true causal structures while minimizing false positive predictions, its robustness across diverse datasets, and its applicability to real-world problems. By combining ML and explainability techniques with causal discovery, REX bridges the gap between predictive modeling and causal inference, offering an effective tool for understanding complex causal structures. REX is publicly available at https://github.com/renero/causalgraph.","sentences":["Explainability techniques hold significant potential for enhancing the causal discovery process, which is crucial for understanding complex systems in areas like healthcare, economics, and artificial intelligence.","However, no causal discovery methods currently incorporate explainability into their models to derive causal graphs.","Thus, in this paper we explore this innovative approach, as it offers substantial potential and represents a promising new direction worth investigating.","Specifically, we introduce REX, a causal discovery method that leverages machine learning (ML) models coupled with explainability techniques, specifically Shapley values, to identify and interpret significant causal relationships among variables.   ","Comparative evaluations on synthetic datasets comprising continuous tabular data reveal that REX outperforms state-of-the-art causal discovery methods across diverse data generation processes, including non-linear and additive noise models.","Moreover, REX was tested on the Sachs single-cell protein-signaling dataset, achieving a precision of 0.952 and recovering key causal relationships with no incorrect edges.","Taking together, these results showcase REX's effectiveness in accurately recovering true causal structures while minimizing false positive predictions, its robustness across diverse datasets, and its applicability to real-world problems.","By combining ML and explainability techniques with causal discovery, REX bridges the gap between predictive modeling and causal inference, offering an effective tool for understanding complex causal structures.","REX is publicly available at https://github.com/renero/causalgraph."],"url":"http://arxiv.org/abs/2501.12706v1"}
{"created":"2025-01-22 08:14:11","title":"Combining Knowledge Graph and LLMs for Enhanced Zero-shot Visual Question Answering","abstract":"Zero-shot visual question answering (ZS-VQA), an emerged critical research area, intends to answer visual questions without providing training samples. Existing research in ZS-VQA has proposed to leverage knowledge graphs or large language models (LLMs), respectively, as external information sources to help VQA model comprehend images and questions. However, LLMs often struggle in accurately interpreting specific question meanings. Meanwhile, although knowledge graph has rich entity relationships, it is challenging to effectively connect entities to individual image content for visual question answers. In this paper, we propose a novel design to combine knowledge graph and LLMs for zero-shot visual question answer. Our approach uses LLMs' powerful understanding capabilities to accurately interpret image content through a strategic question search mechanism. Meanwhile, the knowledge graph is used to expand and connect users' queries to the image content for better visual question answering. An optimization algorithm is further used to determine the optimal weights for the loss functions derived from different information sources, towards a globally optimal set of candidate answers. Experimental results on two benchmark datasets demonstrate that our model achieves state-of-the-art (SOTA) performance. Both source code and benchmark data will be released for public access.","sentences":["Zero-shot visual question answering (ZS-VQA), an emerged critical research area, intends to answer visual questions without providing training samples.","Existing research in ZS-VQA has proposed to leverage knowledge graphs or large language models (LLMs), respectively, as external information sources to help VQA model comprehend images and questions.","However, LLMs often struggle in accurately interpreting specific question meanings.","Meanwhile, although knowledge graph has rich entity relationships, it is challenging to effectively connect entities to individual image content for visual question answers.","In this paper, we propose a novel design to combine knowledge graph and LLMs for zero-shot visual question answer.","Our approach uses LLMs' powerful understanding capabilities to accurately interpret image content through a strategic question search mechanism.","Meanwhile, the knowledge graph is used to expand and connect users' queries to the image content for better visual question answering.","An optimization algorithm is further used to determine the optimal weights for the loss functions derived from different information sources, towards a globally optimal set of candidate answers.","Experimental results on two benchmark datasets demonstrate that our model achieves state-of-the-art (SOTA) performance.","Both source code and benchmark data will be released for public access."],"url":"http://arxiv.org/abs/2501.12697v1"}
{"created":"2025-01-22 06:42:12","title":"Manifold learning and optimization using tangent space proxies","abstract":"We present a framework for efficiently approximating differential-geometric primitives on arbitrary manifolds via construction of an atlas graph representation, which leverages the canonical characterization of a manifold as a finite collection, or atlas, of overlapping coordinate charts. We first show the utility of this framework in a setting where the manifold is expressed in closed form, specifically, a runtime advantage, compared with state-of-the-art approaches, for first-order optimization over the Grassmann manifold. Moreover, using point cloud data for which a complex manifold structure was previously established, i.e., high-contrast image patches, we show that an atlas graph with the correct geometry can be directly learned from the point cloud. Finally, we demonstrate that learning an atlas graph enables downstream key machine learning tasks. In particular, we implement a Riemannian generalization of support vector machines that uses the learned atlas graph to approximate complex differential-geometric primitives, including Riemannian logarithms and vector transports. These settings suggest the potential of this framework for even more complex settings, where ambient dimension and noise levels may be much higher.","sentences":["We present a framework for efficiently approximating differential-geometric primitives on arbitrary manifolds via construction of an atlas graph representation, which leverages the canonical characterization of a manifold as a finite collection, or atlas, of overlapping coordinate charts.","We first show the utility of this framework in a setting where the manifold is expressed in closed form, specifically, a runtime advantage, compared with state-of-the-art approaches, for first-order optimization over the Grassmann manifold.","Moreover, using point cloud data for which a complex manifold structure was previously established, i.e., high-contrast image patches, we show that an atlas graph with the correct geometry can be directly learned from the point cloud.","Finally, we demonstrate that learning an atlas graph enables downstream key machine learning tasks.","In particular, we implement a Riemannian generalization of support vector machines that uses the learned atlas graph to approximate complex differential-geometric primitives, including Riemannian logarithms and vector transports.","These settings suggest the potential of this framework for even more complex settings, where ambient dimension and noise levels may be much higher."],"url":"http://arxiv.org/abs/2501.12678v1"}
{"created":"2025-01-22 06:08:15","title":"NBDI: A Simple and Efficient Termination Condition for Skill Extraction from Task-Agnostic Demonstrations","abstract":"Intelligent agents are able to make decisions based on different levels of granularity and duration. Recent advances in skill learning enabled the agent to solve complex, long-horizon tasks by effectively guiding the agent in choosing appropriate skills. However, the practice of using fixed-length skills can easily result in skipping valuable decision points, which ultimately limits the potential for further exploration and faster policy learning. In this work, we propose to learn a simple and efficient termination condition that identifies decision points through a state-action novelty module that leverages agent experience data. Our approach, Novelty-based Decision Point Identification (NBDI), outperforms previous baselines in complex, long-horizon tasks, and remains effective even in the presence of significant variations in the environment configurations of downstream tasks, highlighting the importance of decision point identification in skill learning.","sentences":["Intelligent agents are able to make decisions based on different levels of granularity and duration.","Recent advances in skill learning enabled the agent to solve complex, long-horizon tasks by effectively guiding the agent in choosing appropriate skills.","However, the practice of using fixed-length skills can easily result in skipping valuable decision points, which ultimately limits the potential for further exploration and faster policy learning.","In this work, we propose to learn a simple and efficient termination condition that identifies decision points through a state-action novelty module that leverages agent experience data.","Our approach, Novelty-based Decision Point Identification (NBDI), outperforms previous baselines in complex, long-horizon tasks, and remains effective even in the presence of significant variations in the environment configurations of downstream tasks, highlighting the importance of decision point identification in skill learning."],"url":"http://arxiv.org/abs/2501.12668v1"}
{"created":"2025-01-22 05:44:02","title":"A Multi-Stakeholder Perspective on Self-Managing Networks","abstract":"Modern telecommunication networks face an increasing complexity due to the rapidly growing number of networked devices and rising amounts of data. The literature advocates for self-managing networks as a means to tackle the resulting challenges. While self-managing networks provide potential solutions to these challenges, current research solely focuses on the perspective of network operators. However, modern telecommunication networks involve various stakeholders, such as service providers and end users, and necessitate interactions between them. By transitioning from a single-stakeholder to a multi-stakeholder perspective, we address the preferences of all involved parties, acknowledging potential conflicts of interest and constraints like information asymmetries. This broader perspective facilitates the development of more effective self-managing networks, significantly enhancing their performance metrics compared to approaches that solely prioritize the concerns of network operators.","sentences":["Modern telecommunication networks face an increasing complexity due to the rapidly growing number of networked devices and rising amounts of data.","The literature advocates for self-managing networks as a means to tackle the resulting challenges.","While self-managing networks provide potential solutions to these challenges, current research solely focuses on the perspective of network operators.","However, modern telecommunication networks involve various stakeholders, such as service providers and end users, and necessitate interactions between them.","By transitioning from a single-stakeholder to a multi-stakeholder perspective, we address the preferences of all involved parties, acknowledging potential conflicts of interest and constraints like information asymmetries.","This broader perspective facilitates the development of more effective self-managing networks, significantly enhancing their performance metrics compared to approaches that solely prioritize the concerns of network operators."],"url":"http://arxiv.org/abs/2501.12659v1"}
{"created":"2025-01-22 05:33:05","title":"AnyNav: Visual Neuro-Symbolic Friction Learning for Off-road Navigation","abstract":"Off-road navigation is essential for a wide range of applications in field robotics such as planetary exploration and disaster response. However, it remains an unresolved challenge due to the unstructured environments and inherent complexity of terrain-vehicle interactions. Traditional physics-based methods struggle to accurately model the nonlinear dynamics of these interactions, while data-driven approaches often suffer from overfitting to specific motion patterns, vehicle sizes, and types, limiting their generalizability. To overcome these challenges, we introduce a vision-based friction estimation framework grounded in neuro-symbolic principles, integrating neural networks for visual perception with symbolic reasoning for physical modeling. This enables significantly improved generalization abilities through explicit physical reasoning incorporating the predicted friction. Additionally, we develop a physics-informed planner that leverages the learned friction coefficient to generate physically feasible and efficient paths, along with corresponding speed profiles. We refer to our approach as AnyNav and evaluate it in both simulation and real-world experiments, demonstrating its utility and robustness across various off-road scenarios and multiple types of four-wheeled vehicles. These results mark an important step toward developing neuro-symbolic spatial intelligence to reason about complex, unstructured environments and enable autonomous off-road navigation in challenging scenarios. Video demonstrations are available at https://sairlab.org/anynav/, where the source code will also be released.","sentences":["Off-road navigation is essential for a wide range of applications in field robotics such as planetary exploration and disaster response.","However, it remains an unresolved challenge due to the unstructured environments and inherent complexity of terrain-vehicle interactions.","Traditional physics-based methods struggle to accurately model the nonlinear dynamics of these interactions, while data-driven approaches often suffer from overfitting to specific motion patterns, vehicle sizes, and types, limiting their generalizability.","To overcome these challenges, we introduce a vision-based friction estimation framework grounded in neuro-symbolic principles, integrating neural networks for visual perception with symbolic reasoning for physical modeling.","This enables significantly improved generalization abilities through explicit physical reasoning incorporating the predicted friction.","Additionally, we develop a physics-informed planner that leverages the learned friction coefficient to generate physically feasible and efficient paths, along with corresponding speed profiles.","We refer to our approach as AnyNav and evaluate it in both simulation and real-world experiments, demonstrating its utility and robustness across various off-road scenarios and multiple types of four-wheeled vehicles.","These results mark an important step toward developing neuro-symbolic spatial intelligence to reason about complex, unstructured environments and enable autonomous off-road navigation in challenging scenarios.","Video demonstrations are available at https://sairlab.org/anynav/, where the source code will also be released."],"url":"http://arxiv.org/abs/2501.12654v1"}
{"created":"2025-01-22 05:24:23","title":"The potential -- and the pitfalls -- of using pre-trained language models as cognitive science theories","abstract":"Many studies have evaluated the cognitive alignment of Pre-trained Language Models (PLMs), i.e., their correspondence to adult performance across a range of cognitive domains. Recently, the focus has expanded to the developmental alignment of these models: identifying phases during training where improvements in model performance track improvements in children's thinking over development. However, there are many challenges to the use of PLMs as cognitive science theories, including different architectures, different training data modalities and scales, and limited model interpretability. In this paper, we distill lessons learned from treating PLMs, not as engineering artifacts but as cognitive science and developmental science models. We review assumptions used by researchers to map measures of PLM performance to measures of human performance. We identify potential pitfalls of this approach to understanding human thinking, and we end by enumerating criteria for using PLMs as credible accounts of cognition and cognitive development.","sentences":["Many studies have evaluated the cognitive alignment of Pre-trained Language Models (PLMs), i.e., their correspondence to adult performance across a range of cognitive domains.","Recently, the focus has expanded to the developmental alignment of these models: identifying phases during training where improvements in model performance track improvements in children's thinking over development.","However, there are many challenges to the use of PLMs as cognitive science theories, including different architectures, different training data modalities and scales, and limited model interpretability.","In this paper, we distill lessons learned from treating PLMs, not as engineering artifacts but as cognitive science and developmental science models.","We review assumptions used by researchers to map measures of PLM performance to measures of human performance.","We identify potential pitfalls of this approach to understanding human thinking, and we end by enumerating criteria for using PLMs as credible accounts of cognition and cognitive development."],"url":"http://arxiv.org/abs/2501.12651v1"}
{"created":"2025-01-22 05:03:51","title":"Training Data Attribution (TDA): Examining Its Adoption & Use Cases","abstract":"This report investigates Training Data Attribution (TDA) and its potential importance to and tractability for reducing extreme risks from AI. First, we discuss the plausibility and amount of effort it would take to bring existing TDA research efforts from their current state, to an efficient and accurate tool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss the numerous research benefits AI labs will expect to see from using such TDA tooling. Then, we discuss a key outstanding bottleneck that would limit such TDA tooling from being accessible publicly: AI labs' willingness to disclose their training data. We suggest ways AI labs may work around these limitations, and discuss the willingness of governments to mandate such access. Assuming that AI labs willingly provide access to TDA inference, we then discuss what high-level societal benefits you might see. We list and discuss a series of policies and systems that may be enabled by TDA. Finally, we present an evaluation of TDA's potential impact on mitigating large-scale risks from AI systems.","sentences":["This report investigates Training Data Attribution (TDA) and its potential importance to and tractability for reducing extreme risks from AI.","First, we discuss the plausibility and amount of effort it would take to bring existing TDA research efforts from their current state, to an efficient and accurate tool for TDA inference that can be run on frontier-scale LLMs.","Next, we discuss the numerous research benefits AI labs will expect to see from using such TDA tooling.","Then, we discuss a key outstanding bottleneck that would limit such TDA tooling from being accessible publicly: AI labs' willingness to disclose their training data.","We suggest ways AI labs may work around these limitations, and discuss the willingness of governments to mandate such access.","Assuming that AI labs willingly provide access to TDA inference, we then discuss what high-level societal benefits you might see.","We list and discuss a series of policies and systems that may be enabled by TDA.","Finally, we present an evaluation of TDA's potential impact on mitigating large-scale risks from AI systems."],"url":"http://arxiv.org/abs/2501.12642v1"}
{"created":"2025-01-22 04:58:50","title":"Dynamics of Toxicity in Political Podcasts","abstract":"Toxicity in digital media poses significant challenges, yet little attention has been given to its dynamics within the rapidly growing medium of podcasts. This paper addresses this gap by analyzing political podcast data to study the emergence and propagation of toxicity, focusing on conversation chains-structured reply patterns within podcast transcripts. Leveraging state-of-the-art transcription models and advanced conversational analysis techniques, we systematically examine toxic discourse in over 30 popular political podcasts in the United States. Our key contributions include: (1) creating a comprehensive dataset of transcribed and diarized political podcasts, identifying thousands of toxic instances using Google's Perspective API, (2) uncovering concerning trends where a majority of episodes contain at least one toxic instance, (3) introducing toxic conversation chains and analyzing their structural and linguistic properties, revealing characteristics such as longer durations, repetitive patterns, figurative language, and emotional cues tied to anger and annoyance, (4) identifying demand-related words like 'want', 'like', and 'know' as precursors to toxicity, and (5) developing predictive models to anticipate toxicity shifts based on annotated change points. Our findings provide critical insights into podcast toxicity and establish a foundation for future research on real-time monitoring and intervention mechanisms to foster healthier discourse in this influential medium.","sentences":["Toxicity in digital media poses significant challenges, yet little attention has been given to its dynamics within the rapidly growing medium of podcasts.","This paper addresses this gap by analyzing political podcast data to study the emergence and propagation of toxicity, focusing on conversation chains-structured reply patterns within podcast transcripts.","Leveraging state-of-the-art transcription models and advanced conversational analysis techniques, we systematically examine toxic discourse in over 30 popular political podcasts in the United States.","Our key contributions include: (1) creating a comprehensive dataset of transcribed and diarized political podcasts, identifying thousands of toxic instances using Google's Perspective API, (2) uncovering concerning trends where a majority of episodes contain at least one toxic instance, (3) introducing toxic conversation chains and analyzing their structural and linguistic properties, revealing characteristics such as longer durations, repetitive patterns, figurative language, and emotional cues tied to anger and annoyance, (4) identifying demand-related words like 'want', 'like', and 'know' as precursors to toxicity, and (5) developing predictive models to anticipate toxicity shifts based on annotated change points.","Our findings provide critical insights into podcast toxicity and establish a foundation for future research on real-time monitoring and intervention mechanisms to foster healthier discourse in this influential medium."],"url":"http://arxiv.org/abs/2501.12640v1"}
{"created":"2025-01-22 04:43:21","title":"Multiple Queries with Multiple Keys: A Precise Prompt Matching Paradigm for Prompt-based Continual Learning","abstract":"Continual learning requires machine learning models to continuously acquire new knowledge in dynamic environments while avoiding the forgetting of previous knowledge. Prompt-based continual learning methods effectively address the issue of catastrophic forgetting through prompt expansion and selection. However, existing approaches often suffer from low accuracy in prompt selection, which can result in the model receiving biased knowledge and making biased predictions. To address this issue, we propose the Multiple Queries with Multiple Keys (MQMK) prompt matching paradigm for precise prompt selection. The goal of MQMK is to select the prompts whose training data distribution most closely matches that of the test sample. Specifically, Multiple Queries enable precise breadth search by introducing task-specific knowledge, while Multiple Keys perform deep search by representing the feature distribution of training samples at a fine-grained level. Experiments show that MQMK enhances the prompt matching rate by over 30% in challenging scenarios and achieves state-of-the-art performance on three widely adopted continual learning benchmarks. Once this paper is accepted, we will release the code.","sentences":["Continual learning requires machine learning models to continuously acquire new knowledge in dynamic environments while avoiding the forgetting of previous knowledge.","Prompt-based continual learning methods effectively address the issue of catastrophic forgetting through prompt expansion and selection.","However, existing approaches often suffer from low accuracy in prompt selection, which can result in the model receiving biased knowledge and making biased predictions.","To address this issue, we propose the Multiple Queries with Multiple Keys (MQMK) prompt matching paradigm for precise prompt selection.","The goal of MQMK is to select the prompts whose training data distribution most closely matches that of the test sample.","Specifically, Multiple Queries enable precise breadth search by introducing task-specific knowledge, while Multiple Keys perform deep search by representing the feature distribution of training samples at a fine-grained level.","Experiments show that MQMK enhances the prompt matching rate by over 30% in challenging scenarios and achieves state-of-the-art performance on three widely adopted continual learning benchmarks.","Once this paper is accepted, we will release the code."],"url":"http://arxiv.org/abs/2501.12635v1"}
{"created":"2025-01-22 04:42:19","title":"SoMa: Identifying, Exploring, and Understanding the DRAM Communication Scheduling Space for DNN Accelerators","abstract":"Modern Deep Neural Network (DNN) accelerators are equipped with increasingly larger on-chip buffers to provide more opportunities to alleviate the increasingly severe DRAM bandwidth pressure. However, most existing research on buffer utilization still primarily focuses on single-layer dataflow scheduling optimization. As buffers grow large enough to accommodate most single-layer weights in most networks, the impact of single-layer dataflow optimization on DRAM communication diminishes significantly. Therefore, developing new paradigms that fuse multiple layers to fully leverage the increasingly abundant on-chip buffer resources to reduce DRAM accesses has become particularly important, yet remains an open challenge. To address this challenge, we first identify the optimization opportunities in DRAM communication scheduling by analyzing the drawbacks of existing works on the layer fusion paradigm and recognizing the vast optimization potential in scheduling the timing of data prefetching from and storing to DRAM. To fully exploit these optimization opportunities, we develop a Tensor-centric Notation and its corresponding parsing method to represent different DRAM communication scheduling schemes and depict the overall space of DRAM communication scheduling. Then, to thoroughly and efficiently explore the space of DRAM communication scheduling for diverse accelerators and workloads, we develop an end-to-end scheduling framework, SoMa, which has already been developed into a compiler for our commercial accelerator product. Compared with the state-of-the-art (SOTA) Cocco framework, SoMa achieves, on average, a 2.11x performance improvement and a 37.3% reduction in energy cost simultaneously. Then, we leverage SoMa to study optimizations for LLM, perform design space exploration (DSE), and analyze the DRAM communication scheduling space through a practical example, yielding some..(more)","sentences":["Modern Deep Neural Network (DNN) accelerators are equipped with increasingly larger on-chip buffers to provide more opportunities to alleviate the increasingly severe DRAM bandwidth pressure.","However, most existing research on buffer utilization still primarily focuses on single-layer dataflow scheduling optimization.","As buffers grow large enough to accommodate most single-layer weights in most networks, the impact of single-layer dataflow optimization on DRAM communication diminishes significantly.","Therefore, developing new paradigms that fuse multiple layers to fully leverage the increasingly abundant on-chip buffer resources to reduce DRAM accesses has become particularly important, yet remains an open challenge.","To address this challenge, we first identify the optimization opportunities in DRAM communication scheduling by analyzing the drawbacks of existing works on the layer fusion paradigm and recognizing the vast optimization potential in scheduling the timing of data prefetching from and storing to DRAM.","To fully exploit these optimization opportunities, we develop a Tensor-centric Notation and its corresponding parsing method to represent different DRAM communication scheduling schemes and depict the overall space of DRAM communication scheduling.","Then, to thoroughly and efficiently explore the space of DRAM communication scheduling for diverse accelerators and workloads, we develop an end-to-end scheduling framework, SoMa, which has already been developed into a compiler for our commercial accelerator product.","Compared with the state-of-the-art (SOTA) Cocco framework, SoMa achieves, on average, a 2.11x performance improvement and a 37.3% reduction in energy cost simultaneously.","Then, we leverage SoMa to study optimizations for LLM, perform design space exploration (DSE), and analyze the DRAM communication scheduling space through a practical example, yielding some..(more)"],"url":"http://arxiv.org/abs/2501.12634v1"}
{"created":"2025-01-22 04:12:32","title":"Toward Model-centric Heterogeneous Federated Graph Learning: A Knowledge-driven Approach","abstract":"Federated graph learning (FGL) has emerged as a promising paradigm for collaborative machine learning, enabling multiple parties to jointly train models while preserving the privacy of raw graph data. However, existing FGL methods often overlook the model-centric heterogeneous FGL (MHtFGL) problem, which arises in real-world applications, such as the aggregation of models from different companies with varying scales and architectures. MHtFGL presents an additional challenge: the diversity of client model architectures hampers common learning and integration of graph representations. To address this issue, we propose the Federated Graph Knowledge Collaboration (FedGKC) framework, comprising two key components: Client-side Self-Mutual Knowledge Distillation, which fosters effective knowledge sharing among clients through copilot models; and Server-side Knowledge-Aware Model Aggregation, which enhances model integration by accounting for the knowledge acquired by clients. Experiments on eight benchmark datasets demonstrate that FedGKC achieves an average accuracy improvement of 3.74% over baseline models in MHtFGL scenarios, while also maintaining excellent performance in homogeneous settings.","sentences":["Federated graph learning (FGL) has emerged as a promising paradigm for collaborative machine learning, enabling multiple parties to jointly train models while preserving the privacy of raw graph data.","However, existing FGL methods often overlook the model-centric heterogeneous FGL (MHtFGL) problem, which arises in real-world applications, such as the aggregation of models from different companies with varying scales and architectures.","MHtFGL presents an additional challenge: the diversity of client model architectures hampers common learning and integration of graph representations.","To address this issue, we propose the Federated Graph Knowledge Collaboration (FedGKC) framework, comprising two key components: Client-side Self-Mutual Knowledge Distillation, which fosters effective knowledge sharing among clients through copilot models; and Server-side Knowledge-Aware Model Aggregation, which enhances model integration by accounting for the knowledge acquired by clients.","Experiments on eight benchmark datasets demonstrate that FedGKC achieves an average accuracy improvement of 3.74% over baseline models in MHtFGL scenarios, while also maintaining excellent performance in homogeneous settings."],"url":"http://arxiv.org/abs/2501.12624v1"}
{"created":"2025-01-22 04:01:17","title":"Adaptive Data Exploitation in Deep Reinforcement Learning","abstract":"We introduce ADEPT: Adaptive Data ExPloiTation, a simple yet powerful framework to enhance the **data efficiency** and **generalization** in deep reinforcement learning (RL). Specifically, ADEPT adaptively manages the use of sampled data across different learning stages via multi-armed bandit (MAB) algorithms, optimizing data utilization while mitigating overfitting. Moreover, ADEPT can significantly reduce the computational overhead and accelerate a wide range of RL algorithms. We test ADEPT on benchmarks including Procgen, MiniGrid, and PyBullet. Extensive simulation demonstrates that ADEPT can achieve superior performance with remarkable computational efficiency, offering a practical solution to data-efficient RL. Our code is available at https://github.com/yuanmingqi/ADEPT.","sentences":["We introduce ADEPT:","Adaptive Data ExPloiTation, a simple yet powerful framework to enhance the **data efficiency** and **generalization** in deep reinforcement learning (RL).","Specifically, ADEPT adaptively manages the use of sampled data across different learning stages via multi-armed bandit (MAB) algorithms, optimizing data utilization while mitigating overfitting.","Moreover, ADEPT can significantly reduce the computational overhead and accelerate a wide range of RL algorithms.","We test ADEPT on benchmarks including Procgen, MiniGrid, and PyBullet.","Extensive simulation demonstrates that ADEPT can achieve superior performance with remarkable computational efficiency, offering a practical solution to data-efficient RL.","Our code is available at https://github.com/yuanmingqi/ADEPT."],"url":"http://arxiv.org/abs/2501.12620v1"}
{"created":"2025-01-22 03:57:52","title":"Distillation Quantification for Large Language Models","abstract":"Model distillation is a technique for transferring knowledge from large language models (LLMs) to smaller ones, aiming to create resource-efficient yet high-performing models. However, excessive distillation can lead to homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks. These limitations underscore the need to systematically quantify the distillation process and its impact. In this work, we propose a framework to evaluate and quantify model distillation. Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization. Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees compared to aligned LLMs. By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs' robustness and safety. The code and data are available under https://github.com/Aegis1863/LLMs-Distillation-Quantification.","sentences":["Model distillation is a technique for transferring knowledge from large language models (LLMs) to smaller ones, aiming to create resource-efficient yet high-performing models.","However, excessive distillation can lead to homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks.","These limitations underscore the need to systematically quantify the distillation process and its impact.","In this work, we propose a framework to evaluate and quantify model distillation.","Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization.","Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini.","(2) Base LLMs show higher distillation degrees compared to aligned LLMs.","By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs' robustness and safety.","The code and data are available under https://github.com/Aegis1863/LLMs-Distillation-Quantification."],"url":"http://arxiv.org/abs/2501.12619v1"}
{"created":"2025-01-22 03:56:55","title":"Fray: An Efficient General-Purpose Concurrency Testing Platform for the JVM","abstract":"Concurrency bugs are hard to discover and reproduce. Prior work has developed sophisticated algorithms to search for concurrency bugs, such as partial order sampling (POS); however, fundamental limitations with existing platforms for concurrency control hinder effective testing of real-world software. We observe that the design space for concurrency control on managed code involves complex trade-offs between expressibility, applicability, and maintainability on the one hand, and bug-finding efficiency on the other hand.   This paper presents Fray, a platform for performing push-button concurrency testing of data-race-free JVM programs. The key insight behind Fray is that effective controlled concurrency testing requires orchestrating thread interleavings without replacing existing concurrency primitives, while encoding their semantics for faithfully expressing the set of all possible program behaviors. Fray incorporates a novel concurrency control mechanism called shadow locking, designed to make controlled concurrency testing practical and efficient for JVM programs. In an empirical evaluation on 53 benchmark programs with known bugs (SCTBench and JaConTeBe), Fray with random search finds 70% more bugs than JPF and 77% more bugs than RR's chaos mode. We also demonstrate Fray's push-button applicability on 2,655 tests from Apache Kafka, Lucene, and Google Guava. In these mature projects, Fray successfully discovered 18 real-world concurrency bugs that can cause 363 tests to fail reproducibly.","sentences":["Concurrency bugs are hard to discover and reproduce.","Prior work has developed sophisticated algorithms to search for concurrency bugs, such as partial order sampling (POS); however, fundamental limitations with existing platforms for concurrency control hinder effective testing of real-world software.","We observe that the design space for concurrency control on managed code involves complex trade-offs between expressibility, applicability, and maintainability on the one hand, and bug-finding efficiency on the other hand.   ","This paper presents Fray, a platform for performing push-button concurrency testing of data-race-free JVM programs.","The key insight behind Fray is that effective controlled concurrency testing requires orchestrating thread interleavings without replacing existing concurrency primitives, while encoding their semantics for faithfully expressing the set of all possible program behaviors.","Fray incorporates a novel concurrency control mechanism called shadow locking, designed to make controlled concurrency testing practical and efficient for JVM programs.","In an empirical evaluation on 53 benchmark programs with known bugs (SCTBench and JaConTeBe), Fray with random search finds 70% more bugs than JPF and 77% more bugs than RR's chaos mode.","We also demonstrate Fray's push-button applicability on 2,655 tests from Apache Kafka, Lucene, and Google Guava.","In these mature projects, Fray successfully discovered 18 real-world concurrency bugs that can cause 363 tests to fail reproducibly."],"url":"http://arxiv.org/abs/2501.12618v1"}
{"created":"2025-01-22 03:29:43","title":"T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation","abstract":"Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content. Current research on assessing T2I safety remains in its early stages. While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored. To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias. We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts. Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing. Data and evaluator are released under https://github.com/adwardlee/t2i_safety.","sentences":["Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains.","However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content.","Current research on assessing T2I safety remains in its early stages.","While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored.","To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias.","We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts.","Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect.","We evaluate 12 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing.","Data and evaluator are released under https://github.com/adwardlee/t2i_safety."],"url":"http://arxiv.org/abs/2501.12612v1"}
{"created":"2025-01-22 03:28:40","title":"Exploring Wikipedia Gender Diversity Over Time $\\unicode{x2013}$ The Wikipedia Gender Dashboard (WGD)","abstract":"The Wikipedia editors' community has been actively pursuing the intent of achieving gender equality. To that end, it is important to explore the historical evolution of underlying gender disparities in Wikipedia articles. This paper presents the Wikipedia Gender Dashboard (WGD), a tool designed to enable the interaction with gender distribution data, including the average age in every subclass of individuals (i.e. Astronauts, Politicians, etc.) over the years. Wikipedia APIs, DBpedia, and Wikidata endpoints were used to query the data to ensure persistent data collection. The WGD was then created with Microsoft Power BI before being embedded on a public website. The analysis of the data available in the WGD found that female articles only represent around 17% of English Wikipedia, but it has been growing steadily over the last 20 years. Meanwhile, the average age across genders decreased over time. WGD also shows that most subclasses of `Person' are male-dominated. Wikipedia editors can make use of WGD to locate areas with marginalized genders in Wikipedia, and increase their efforts to produce more content providing coverage for those genders to achieve better gender equality in Wikipedia.","sentences":["The Wikipedia editors' community has been actively pursuing the intent of achieving gender equality.","To that end, it is important to explore the historical evolution of underlying gender disparities in Wikipedia articles.","This paper presents the Wikipedia Gender Dashboard (WGD), a tool designed to enable the interaction with gender distribution data, including the average age in every subclass of individuals (i.e. Astronauts, Politicians, etc.) over the years.","Wikipedia APIs, DBpedia, and Wikidata endpoints were used to query the data to ensure persistent data collection.","The WGD was then created with Microsoft Power BI before being embedded on a public website.","The analysis of the data available in the WGD found that female articles only represent around 17% of English Wikipedia, but it has been growing steadily over the last 20 years.","Meanwhile, the average age across genders decreased over time.","WGD also shows that most subclasses of `Person' are male-dominated.","Wikipedia editors can make use of WGD to locate areas with marginalized genders in Wikipedia, and increase their efforts to produce more content providing coverage for those genders to achieve better gender equality in Wikipedia."],"url":"http://arxiv.org/abs/2501.12610v1"}
{"created":"2025-01-22 03:19:03","title":"Low-Dimensional Representation-Driven TSK Fuzzy System for Feature Selection","abstract":"Feature selection can select important features to address dimensional curses. Subspace learning, a widely used dimensionality reduction method, can project the original data into a low-dimensional space. However, the low-dimensional representation is often transformed back into the original space, resulting in information loss. Additionally, gate function-based methods in Takagi-Sugeno-Kang fuzzy system (TSK-FS) are commonly less discrimination. To address these issues, this paper proposes a novel feature selection method that integrates subspace learning with TSK-FS. Specifically, a projection matrix is used to fit the intrinsic low-dimensional representation. Subsequently, the low-dimensional representation is fed to TSK-FS to measure its availability. The firing strength is slacked so that TSK-FS is not limited by numerical underflow. Finally, the $\\ell _{2,1}$-norm is introduced to select significant features and the connection to related works is discussed. The proposed method is evaluated against six state-of-the-art methods on eighteen datasets, and the results demonstrate the superiority of the proposed method.","sentences":["Feature selection can select important features to address dimensional curses.","Subspace learning, a widely used dimensionality reduction method, can project the original data into a low-dimensional space.","However, the low-dimensional representation is often transformed back into the original space, resulting in information loss.","Additionally, gate function-based methods in Takagi-Sugeno-Kang fuzzy system (TSK-FS) are commonly less discrimination.","To address these issues, this paper proposes a novel feature selection method that integrates subspace learning with TSK-FS.","Specifically, a projection matrix is used to fit the intrinsic low-dimensional representation.","Subsequently, the low-dimensional representation is fed to TSK-FS to measure its availability.","The firing strength is slacked so that TSK-FS is not limited by numerical underflow.","Finally, the $\\ell _{2,1}$-norm is introduced to select significant features and the connection to related works is discussed.","The proposed method is evaluated against six state-of-the-art methods on eighteen datasets, and the results demonstrate the superiority of the proposed method."],"url":"http://arxiv.org/abs/2501.12607v1"}
{"created":"2025-01-22 02:56:25","title":"Bridging the Digital Divide: Approach to Documenting Early Computing Artifacts Using Established Standards for Cross-Collection Knowledge Integration Ontology","abstract":"In this paper we address the challenges of documenting early digital artifacts in collections built to offer historical context for future generations. Through insights from active community members (N=20), we examine current archival needs and obstacles. We assess the potential of the CIDOC Conceptual Reference Model (CRM) for categorizing fragmented digital data. Despite its complexity, CIDOC-CRM proves logical, human-readable, and adaptable, enabling archivists to select minimal yet effective building blocks set to empower community-led heritage projects.","sentences":["In this paper we address the challenges of documenting early digital artifacts in collections built to offer historical context for future generations.","Through insights from active community members (N=20), we examine current archival needs and obstacles.","We assess the potential of the CIDOC Conceptual Reference Model (CRM) for categorizing fragmented digital data.","Despite its complexity, CIDOC-CRM proves logical, human-readable, and adaptable, enabling archivists to select minimal yet effective building blocks set to empower community-led heritage projects."],"url":"http://arxiv.org/abs/2501.12603v1"}
{"created":"2025-01-22 02:48:14","title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","abstract":"Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).","sentences":["Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data.","Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards.","However, prior published work has not produced competitive results.","In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization.","Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models.","Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1.","Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%)."],"url":"http://arxiv.org/abs/2501.12599v1"}
{"created":"2025-01-22 02:48:07","title":"On Accelerating Deep Neural Network Mutation Analysis by Neuron and Mutant Clustering","abstract":"Mutation analysis of deep neural networks (DNNs) is a promising method for effective evaluation of test data quality and model robustness, but it can be computationally expensive, especially for large models. To alleviate this, we present DEEPMAACC, a technique and a tool that speeds up DNN mutation analysis through neuron and mutant clustering. DEEPMAACC implements two methods: (1) neuron clustering to reduce the number of generated mutants and (2) mutant clustering to reduce the number of mutants to be tested by selecting representative mutants for testing. Both use hierarchical agglomerative clustering to group neurons and mutants with similar weights, with the goal of improving efficiency while maintaining mutation score. DEEPMAACC has been evaluated on 8 DNN models across 4 popular classification datasets and two DNN architectures. When compared to exhaustive, or vanilla, mutation analysis, the results provide empirical evidence that neuron clustering approach, on average, accelerates mutation analysis by 69.77%, with an average -26.84% error in mutation score. Meanwhile, mutant clustering approach, on average, accelerates mutation analysis by 35.31%, with an average 1.96% error in mutation score. Our results demonstrate that a trade-off can be made between mutation testing speed and mutation score error.","sentences":["Mutation analysis of deep neural networks (DNNs) is a promising method for effective evaluation of test data quality and model robustness, but it can be computationally expensive, especially for large models.","To alleviate this, we present DEEPMAACC, a technique and a tool that speeds up DNN mutation analysis through neuron and mutant clustering.","DEEPMAACC implements two methods: (1) neuron clustering to reduce the number of generated mutants and (2) mutant clustering to reduce the number of mutants to be tested by selecting representative mutants for testing.","Both use hierarchical agglomerative clustering to group neurons and mutants with similar weights, with the goal of improving efficiency while maintaining mutation score.","DEEPMAACC has been evaluated on 8 DNN models across 4 popular classification datasets and two DNN architectures.","When compared to exhaustive, or vanilla, mutation analysis, the results provide empirical evidence that neuron clustering approach, on average, accelerates mutation analysis by 69.77%, with an average -26.84% error in mutation score.","Meanwhile, mutant clustering approach, on average, accelerates mutation analysis by 35.31%, with an average 1.96% error in mutation score.","Our results demonstrate that a trade-off can be made between mutation testing speed and mutation score error."],"url":"http://arxiv.org/abs/2501.12598v1"}
{"created":"2025-01-22 02:45:30","title":"Adapting OpenAI's CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control: An Expository Case Study with Multiple Application Examples","abstract":"This expository paper introduces a simplified approach to image-based quality inspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image Pretraining) model adapted for few-shot learning. While CLIP has demonstrated impressive capabilities in general computer vision tasks, its direct application to manufacturing inspection presents challenges due to the domain gap between its training data and industrial applications. We evaluate CLIP's effectiveness through five case studies: metallic pan surface inspection, 3D printing extrusion profile analysis, stochastic textured surface evaluation, automotive assembly inspection, and microstructure image classification. Our results show that CLIP can achieve high classification accuracy with relatively small learning sets (50-100 examples per class) for single-component and texture-based applications. However, the performance degrades with complex multi-component scenes. We provide a practical implementation framework that enables quality engineers to quickly assess CLIP's suitability for their specific applications before pursuing more complex solutions. This work establishes CLIP-based few-shot learning as an effective baseline approach that balances implementation simplicity with robust performance, demonstrated in several manufacturing quality control applications.","sentences":["This expository paper introduces a simplified approach to image-based quality inspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image Pretraining) model adapted for few-shot learning.","While CLIP has demonstrated impressive capabilities in general computer vision tasks, its direct application to manufacturing inspection presents challenges due to the domain gap between its training data and industrial applications.","We evaluate CLIP's effectiveness through five case studies: metallic pan surface inspection, 3D printing extrusion profile analysis, stochastic textured surface evaluation, automotive assembly inspection, and microstructure image classification.","Our results show that CLIP can achieve high classification accuracy with relatively small learning sets (50-100 examples per class) for single-component and texture-based applications.","However, the performance degrades with complex multi-component scenes.","We provide a practical implementation framework that enables quality engineers to quickly assess CLIP's suitability for their specific applications before pursuing more complex solutions.","This work establishes CLIP-based few-shot learning as an effective baseline approach that balances implementation simplicity with robust performance, demonstrated in several manufacturing quality control applications."],"url":"http://arxiv.org/abs/2501.12596v1"}
{"created":"2025-01-22 02:45:21","title":"A Unified Invariant Learning Framework for Graph Classification","abstract":"Invariant learning demonstrates substantial potential for enhancing the generalization of graph neural networks (GNNs) with out-of-distribution (OOD) data. It aims to recognize stable features in graph data for classification, based on the premise that these features causally determine the target label, and their influence is invariant to changes in distribution. Along this line, most studies have attempted to pinpoint these stable features by emphasizing explicit substructures in the graph, such as masked or attentive subgraphs, and primarily enforcing the invariance principle in the semantic space, i.e., graph representations. However, we argue that focusing only on the semantic space may not accurately identify these stable features. To address this, we introduce the Unified Invariant Learning (UIL) framework for graph classification. It provides a unified perspective on invariant graph learning, emphasizing both structural and semantic invariance principles to identify more robust stable features. In the graph space, UIL adheres to the structural invariance principle by reducing the distance between graphons over a set of stable features across different environments. Simultaneously, to confirm semantic invariance, UIL underscores that the acquired graph representations should demonstrate exemplary performance across diverse environments. We present both theoretical and empirical evidence to confirm our method's ability to recognize superior stable features. Moreover, through a series of comprehensive experiments complemented by in-depth analyses, we demonstrate that UIL considerably enhances OOD generalization, surpassing the performance of leading baseline methods. Our codes are available at https://github.com/yongduosui/UIL.","sentences":["Invariant learning demonstrates substantial potential for enhancing the generalization of graph neural networks (GNNs) with out-of-distribution (OOD) data.","It aims to recognize stable features in graph data for classification, based on the premise that these features causally determine the target label, and their influence is invariant to changes in distribution.","Along this line, most studies have attempted to pinpoint these stable features by emphasizing explicit substructures in the graph, such as masked or attentive subgraphs, and primarily enforcing the invariance principle in the semantic space, i.e., graph representations.","However, we argue that focusing only on the semantic space may not accurately identify these stable features.","To address this, we introduce the Unified Invariant Learning (UIL) framework for graph classification.","It provides a unified perspective on invariant graph learning, emphasizing both structural and semantic invariance principles to identify more robust stable features.","In the graph space, UIL adheres to the structural invariance principle by reducing the distance between graphons over a set of stable features across different environments.","Simultaneously, to confirm semantic invariance, UIL underscores that the acquired graph representations should demonstrate exemplary performance across diverse environments.","We present both theoretical and empirical evidence to confirm our method's ability to recognize superior stable features.","Moreover, through a series of comprehensive experiments complemented by in-depth analyses, we demonstrate that UIL considerably enhances OOD generalization, surpassing the performance of leading baseline methods.","Our codes are available at https://github.com/yongduosui/UIL."],"url":"http://arxiv.org/abs/2501.12595v1"}
{"created":"2025-01-22 02:35:20","title":"FedGrAINS: Personalized SubGraph Federated Learning with Adaptive Neighbor Sampling","abstract":"Graphs are crucial for modeling relational and biological data. As datasets grow larger in real-world scenarios, the risk of exposing sensitive information increases, making privacy-preserving training methods like federated learning (FL) essential to ensure data security and compliance with privacy regulations. Recently proposed personalized subgraph FL methods have become the de-facto standard for training personalized Graph Neural Networks (GNNs) in a federated manner while dealing with the missing links across clients' subgraphs due to privacy restrictions. However, personalized subgraph FL faces significant challenges due to the heterogeneity in client subgraphs, such as degree distributions among the nodes, which complicate federated training of graph models. To address these challenges, we propose \\textit{FedGrAINS}, a novel data-adaptive and sampling-based regularization method for subgraph FL. FedGrAINS leverages generative flow networks (GFlowNets) to evaluate node importance concerning clients' tasks, dynamically adjusting the message-passing step in clients' GNNs. This adaptation reflects task-optimized sampling aligned with a trajectory balance objective. Experimental results demonstrate that the inclusion of \\textit{FedGrAINS} as a regularizer consistently improves the FL performance compared to baselines that do not leverage such regularization.","sentences":["Graphs are crucial for modeling relational and biological data.","As datasets grow larger in real-world scenarios, the risk of exposing sensitive information increases, making privacy-preserving training methods like federated learning (FL) essential to ensure data security and compliance with privacy regulations.","Recently proposed personalized subgraph FL methods have become the de-facto standard for training personalized Graph Neural Networks (GNNs) in a federated manner while dealing with the missing links across clients' subgraphs due to privacy restrictions.","However, personalized subgraph FL faces significant challenges due to the heterogeneity in client subgraphs, such as degree distributions among the nodes, which complicate federated training of graph models.","To address these challenges, we propose \\textit{FedGrAINS}, a novel data-adaptive and sampling-based regularization method for subgraph FL. FedGrAINS leverages generative flow networks (GFlowNets) to evaluate node importance concerning clients' tasks, dynamically adjusting the message-passing step in clients' GNNs.","This adaptation reflects task-optimized sampling aligned with a trajectory balance objective.","Experimental results demonstrate that the inclusion of \\textit{FedGrAINS} as a regularizer consistently improves the FL performance compared to baselines that do not leverage such regularization."],"url":"http://arxiv.org/abs/2501.12592v1"}
{"created":"2025-01-22 02:16:57","title":"Entropy Polarization-Based Data Compression Without Frozen Set Construction","abstract":"Classical source polar codes require the construction of frozen sets for given sources. While this scheme offers excellent theoretical performance, it faces challenges in practical data compression systems, including sensitivity to the accuracy and computational complexity of the construction algorithm. In this letter, we explore the feasibility of construction-free polar compression schemes. By optimally selecting output symbols based on the decoder's behavior, the proposed scheme not only enhances flexibility but also achieves significant improvements in compression rates. Several enhancements are introduced to facilitate the practical implementation of the proposed scheme. Numerical results demonstrate the superior performance compared to existing polar compression approaches.","sentences":["Classical source polar codes require the construction of frozen sets for given sources.","While this scheme offers excellent theoretical performance, it faces challenges in practical data compression systems, including sensitivity to the accuracy and computational complexity of the construction algorithm.","In this letter, we explore the feasibility of construction-free polar compression schemes.","By optimally selecting output symbols based on the decoder's behavior, the proposed scheme not only enhances flexibility but also achieves significant improvements in compression rates.","Several enhancements are introduced to facilitate the practical implementation of the proposed scheme.","Numerical results demonstrate the superior performance compared to existing polar compression approaches."],"url":"http://arxiv.org/abs/2501.12584v1"}
{"created":"2025-01-22 02:04:12","title":"Approximate Puzzlepiece Compositing","abstract":"The increasing demand for larger and higher fidelity simulations has made Adaptive Mesh Refinement (AMR) and unstructured mesh techniques essential to focus compute effort and memory cost on just the areas of interest in the simulation domain. The distribution of these meshes over the compute nodes is often determined by balancing compute, memory, and network costs, leading to distributions with jagged nonconvex boundaries that fit together much like puzzle pieces. It is expensive, and sometimes impossible, to re-partition the data posing a challenge for in situ and post hoc visualization as the data cannot be rendered using standard sort-last compositing techniques that require a convex and disjoint data partitioning. We present a new distributed volume rendering and compositing algorithm, Approximate Puzzlepiece Compositing, that enables fast and high-accuracy in-place rendering of AMR and unstructured meshes. Our approach builds on Moment-Based Ordered-Independent Transparency to achieve a scalable, order-independent compositing algorithm that requires little communication and does not impose requirements on the data partitioning. We evaluate the image quality and scalability of our approach on synthetic data and two large-scale unstructured meshes on HPC systems by comparing to state-of-the-art sort-last compositing techniques, highlighting our approach's minimal overhead at higher core counts. We demonstrate that Approximate Puzzlepiece Compositing provides a scalable, high-performance, and high-quality distributed rendering approach applicable to the complex data distributions encountered in large-scale CFD simulations.","sentences":["The increasing demand for larger and higher fidelity simulations has made Adaptive Mesh Refinement (AMR) and unstructured mesh techniques essential to focus compute effort and memory cost on just the areas of interest in the simulation domain.","The distribution of these meshes over the compute nodes is often determined by balancing compute, memory, and network costs, leading to distributions with jagged nonconvex boundaries that fit together much like puzzle pieces.","It is expensive, and sometimes impossible, to re-partition the data posing a challenge for in situ and post hoc visualization as the data cannot be rendered using standard sort-last compositing techniques that require a convex and disjoint data partitioning.","We present a new distributed volume rendering and compositing algorithm, Approximate Puzzlepiece Compositing, that enables fast and high-accuracy in-place rendering of AMR and unstructured meshes.","Our approach builds on Moment-Based Ordered-Independent Transparency to achieve a scalable, order-independent compositing algorithm that requires little communication and does not impose requirements on the data partitioning.","We evaluate the image quality and scalability of our approach on synthetic data and two large-scale unstructured meshes on HPC systems by comparing to state-of-the-art sort-last compositing techniques, highlighting our approach's minimal overhead at higher core counts.","We demonstrate that Approximate Puzzlepiece Compositing provides a scalable, high-performance, and high-quality distributed rendering approach applicable to the complex data distributions encountered in large-scale CFD simulations."],"url":"http://arxiv.org/abs/2501.12581v1"}
{"created":"2025-01-22 00:20:26","title":"Generalization Performance of Hypergraph Neural Networks","abstract":"Hypergraph neural networks have been promising tools for handling learning tasks involving higher-order data, with notable applications in web graphs, such as modeling multi-way hyperlink structures and complex user interactions. Yet, their generalization abilities in theory are less clear to us. In this paper, we seek to develop margin-based generalization bounds for four representative classes of hypergraph neural networks, including convolutional-based methods (UniGCN), set-based aggregation (AllDeepSets), invariant and equivariant transformations (M-IGN), and tensor-based approaches (T-MPHN). Through the PAC-Bayes framework, our results reveal the manner in which hypergraph structure and spectral norms of the learned weights can affect the generalization bounds, where the key technical challenge lies in developing new perturbation analysis for hypergraph neural networks, which offers a rigorous understanding of how variations in the model's weights and hypergraph structure impact its generalization behavior. Our empirical study examines the relationship between the practical performance and theoretical bounds of the models over synthetic and real-world datasets. One of our primary observations is the strong correlation between the theoretical bounds and empirical loss, with statistically significant consistency in most cases.","sentences":["Hypergraph neural networks have been promising tools for handling learning tasks involving higher-order data, with notable applications in web graphs, such as modeling multi-way hyperlink structures and complex user interactions.","Yet, their generalization abilities in theory are less clear to us.","In this paper, we seek to develop margin-based generalization bounds for four representative classes of hypergraph neural networks, including convolutional-based methods (UniGCN), set-based aggregation (AllDeepSets), invariant and equivariant transformations (M-IGN), and tensor-based approaches (T-MPHN).","Through the PAC-Bayes framework, our results reveal the manner in which hypergraph structure and spectral norms of the learned weights can affect the generalization bounds, where the key technical challenge lies in developing new perturbation analysis for hypergraph neural networks, which offers a rigorous understanding of how variations in the model's weights and hypergraph structure impact its generalization behavior.","Our empirical study examines the relationship between the practical performance and theoretical bounds of the models over synthetic and real-world datasets.","One of our primary observations is the strong correlation between the theoretical bounds and empirical loss, with statistically significant consistency in most cases."],"url":"http://arxiv.org/abs/2501.12554v1"}
{"created":"2025-01-22 00:03:25","title":"An O(log n)-Approximation Algorithm for (p,q)-Flexible Graph Connectivity via Independent Rounding","abstract":"In the $(p,q)$-Flexible Graph Connectivity problem, the input is a graph $G = (V,E)$ with the edge set $E = \\mathscr{S} \\cup \\mathscr{U}$ partitioned into safe and unsafe edges, and the goal is to find a minimum cost set of edges $F$ such that the subgraph $(V,F)$ remains $p$-edge-connected after removing any $q$ unsafe edges from $F$. We give a new integer programming formulation for the problem, by adding knapsack cover constraints to the $p(p+q)$-connected capacitated edge-connectivity formulation studied in previous work, and show that the corresponding linear relaxation can be solved in polynomial time by giving an efficient separation oracle. Further, we show that independent randomized rounding yields an $O(\\log n)$-approximation for arbitrary values of $p$ and $q$, improving the state-of-the-art $O(q\\log n)$. For both separation and rounding, a key insight is to use Karger's bound on the number of near-minimum cuts.","sentences":["In the $(p,q)$-Flexible Graph Connectivity problem, the input is a graph $G = (V,E)$ with the edge set $E = \\mathscr{S} \\cup \\mathscr{U}$ partitioned into safe and unsafe edges, and the goal is to find a minimum cost set of edges $F$ such that the subgraph $(V,F)$ remains $p$-edge-connected after removing any $q$ unsafe edges from $F$. We give a new integer programming formulation for the problem, by adding knapsack cover constraints to the $p(p+q)$-connected capacitated edge-connectivity formulation studied in previous work, and show that the corresponding linear relaxation can be solved in polynomial time by giving an efficient separation oracle.","Further, we show that independent randomized rounding yields an $O(\\log n)$-approximation for arbitrary values of $p$ and $q$, improving the state-of-the-art $O(q\\log n)$. For both separation and rounding, a key insight is to use Karger's bound on the number of near-minimum cuts."],"url":"http://arxiv.org/abs/2501.12549v1"}
{"created":"2025-01-21 23:05:12","title":"Academic Case Reports Lack Diversity: Assessing the Presence and Diversity of Sociodemographic and Behavioral Factors related with Post COVID-19 Condition","abstract":"Understanding the prevalence, disparities, and symptom variations of Post COVID-19 Condition (PCC) for vulnerable populations is crucial to improving care and addressing intersecting inequities. This study aims to develop a comprehensive framework for integrating social determinants of health (SDOH) into PCC research by leveraging NLP techniques to analyze disparities and variations in SDOH representation within PCC case reports. Following construction of a PCC Case Report Corpus, comprising over 7,000 case reports from the LitCOVID repository, a subset of 709 reports were annotated with 26 core SDOH-related entity types using pre-trained named entity recognition (NER) models, human review, and data augmentation to improve quality, diversity and representation of entity types. An NLP pipeline integrating NER, natural language inference (NLI), trigram and frequency analyses was developed to extract and analyze these entities. Both encoder-only transformer models and RNN-based models were assessed for the NER objective.   Fine-tuned encoder-only BERT models outperformed traditional RNN-based models in generalizability to distinct sentence structures and greater class sparsity. Exploratory analysis revealed variability in entity richness, with prevalent entities like condition, age, and access to care, and underrepresentation of sensitive categories like race and housing status. Trigram analysis highlighted frequent co-occurrences among entities, including age, gender, and condition. The NLI objective (entailment and contradiction analysis) showed attributes like \"Experienced violence or abuse\" and \"Has medical insurance\" had high entailment rates (82.4%-80.3%), while attributes such as \"Is female-identifying,\" \"Is married,\" and \"Has a terminal condition\" exhibited high contradiction rates (70.8%-98.5%).","sentences":["Understanding the prevalence, disparities, and symptom variations of Post COVID-19 Condition (PCC) for vulnerable populations is crucial to improving care and addressing intersecting inequities.","This study aims to develop a comprehensive framework for integrating social determinants of health (SDOH) into PCC research by leveraging NLP techniques to analyze disparities and variations in SDOH representation within PCC case reports.","Following construction of a PCC Case Report Corpus, comprising over 7,000 case reports from the LitCOVID repository, a subset of 709 reports were annotated with 26 core SDOH-related entity types using pre-trained named entity recognition (NER) models, human review, and data augmentation to improve quality, diversity and representation of entity types.","An NLP pipeline integrating NER, natural language inference (NLI), trigram and frequency analyses was developed to extract and analyze these entities.","Both encoder-only transformer models and RNN-based models were assessed for the NER objective.   ","Fine-tuned encoder-only BERT models outperformed traditional RNN-based models in generalizability to distinct sentence structures and greater class sparsity.","Exploratory analysis revealed variability in entity richness, with prevalent entities like condition, age, and access to care, and underrepresentation of sensitive categories like race and housing status.","Trigram analysis highlighted frequent co-occurrences among entities, including age, gender, and condition.","The NLI objective (entailment and contradiction analysis) showed attributes like \"Experienced violence or abuse\" and \"Has medical insurance\" had high entailment rates (82.4%-80.3%), while attributes such as \"Is female-identifying,\" \"Is married,\" and \"Has a terminal condition\" exhibited high contradiction rates (70.8%-98.5%)."],"url":"http://arxiv.org/abs/2501.12538v1"}
{"created":"2025-01-21 23:01:21","title":"Enhancing Privacy in the Early Detection of Sexual Predators Through Federated Learning and Differential Privacy","abstract":"The increased screen time and isolation caused by the COVID-19 pandemic have led to a significant surge in cases of online grooming, which is the use of strategies by predators to lure children into sexual exploitation. Previous efforts to detect grooming in industry and academia have involved accessing and monitoring private conversations through centrally-trained models or sending private conversations to a global server. In this work, we implement a privacy-preserving pipeline for the early detection of sexual predators. We leverage federated learning and differential privacy in order to create safer online spaces for children while respecting their privacy. We investigate various privacy-preserving implementations and discuss their benefits and shortcomings. Our extensive evaluation using real-world data proves that privacy and utility can coexist with only a slight reduction in utility.","sentences":["The increased screen time and isolation caused by the COVID-19 pandemic have led to a significant surge in cases of online grooming, which is the use of strategies by predators to lure children into sexual exploitation.","Previous efforts to detect grooming in industry and academia have involved accessing and monitoring private conversations through centrally-trained models or sending private conversations to a global server.","In this work, we implement a privacy-preserving pipeline for the early detection of sexual predators.","We leverage federated learning and differential privacy in order to create safer online spaces for children while respecting their privacy.","We investigate various privacy-preserving implementations and discuss their benefits and shortcomings.","Our extensive evaluation using real-world data proves that privacy and utility can coexist with only a slight reduction in utility."],"url":"http://arxiv.org/abs/2501.12537v1"}
{"created":"2025-01-21 22:59:50","title":"Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs","abstract":"This paper presents the development of a comprehensive dataset capturing interactions between Autonomous Vehicles (AVs) and traffic control devices, specifically traffic lights and stop signs. Derived from the Waymo Motion dataset, our work addresses a critical gap in the existing literature by providing real-world trajectory data on how AVs navigate these traffic control devices. We propose a methodology for identifying and extracting relevant interaction trajectory data from the Waymo Motion dataset, incorporating over 37,000 instances with traffic lights and 44,000 with stop signs. Our methodology includes defining rules to identify various interaction types, extracting trajectory data, and applying a wavelet-based denoising method to smooth the acceleration and speed profiles and eliminate anomalous values, thereby enhancing the trajectory quality. Quality assessment metrics indicate that trajectories obtained in this study have anomaly proportions in acceleration and jerk profiles reduced to near-zero levels across all interaction categories. By making this dataset publicly available, we aim to address the current gap in datasets containing AV interaction behaviors with traffic lights and signs. Based on the organized and published dataset, we can gain a more in-depth understanding of AVs' behavior when interacting with traffic lights and signs. This will facilitate research on AV integration into existing transportation infrastructures and networks, supporting the development of more accurate behavioral models and simulation tools.","sentences":["This paper presents the development of a comprehensive dataset capturing interactions between Autonomous Vehicles (AVs) and traffic control devices, specifically traffic lights and stop signs.","Derived from the Waymo Motion dataset, our work addresses a critical gap in the existing literature by providing real-world trajectory data on how AVs navigate these traffic control devices.","We propose a methodology for identifying and extracting relevant interaction trajectory data from the Waymo Motion dataset, incorporating over 37,000 instances with traffic lights and 44,000 with stop signs.","Our methodology includes defining rules to identify various interaction types, extracting trajectory data, and applying a wavelet-based denoising method to smooth the acceleration and speed profiles and eliminate anomalous values, thereby enhancing the trajectory quality.","Quality assessment metrics indicate that trajectories obtained in this study have anomaly proportions in acceleration and jerk profiles reduced to near-zero levels across all interaction categories.","By making this dataset publicly available, we aim to address the current gap in datasets containing AV interaction behaviors with traffic lights and signs.","Based on the organized and published dataset, we can gain a more in-depth understanding of AVs' behavior when interacting with traffic lights and signs.","This will facilitate research on AV integration into existing transportation infrastructures and networks, supporting the development of more accurate behavioral models and simulation tools."],"url":"http://arxiv.org/abs/2501.12536v1"}
{"created":"2025-01-21 22:57:09","title":"How Does the Spatial Distribution of Pre-training Data Affect Geospatial Foundation Models?","abstract":"Foundation models have made rapid advances in many domains including Earth observation, where Geospatial Foundation Models (GFMs) can help address global challenges such as climate change, agriculture, and disaster response. Previous work on GFMs focused on tailoring model architecture and pre-text tasks, and did not investigate the impact of pre-training data selection on model performance. However, recent works from other domains show that the pre-training data distribution is an important factor influencing the performance of the foundation models. With this motivation, our research explores how the geographic distribution of pre-training data affects the performance of GFMs. We evaluated several pre-training data distributions by sampling different compositions from a global data pool. Our experiments with two GFMs on downstream tasks indicate that balanced and globally representative data compositions often outperform region-specific sampling, highlighting the importance of diversity and global coverage in pre-training data. Our results suggest that the most appropriate data sampling technique may depend on the specific GFM architecture. These findings will support the development of robust GFMs by incorporating quality pre-training data distributions, ultimately improving machine learning solutions for Earth observation.","sentences":["Foundation models have made rapid advances in many domains including Earth observation, where Geospatial Foundation Models (GFMs) can help address global challenges such as climate change, agriculture, and disaster response.","Previous work on GFMs focused on tailoring model architecture and pre-text tasks, and did not investigate the impact of pre-training data selection on model performance.","However, recent works from other domains show that the pre-training data distribution is an important factor influencing the performance of the foundation models.","With this motivation, our research explores how the geographic distribution of pre-training data affects the performance of GFMs.","We evaluated several pre-training data distributions by sampling different compositions from a global data pool.","Our experiments with two GFMs on downstream tasks indicate that balanced and globally representative data compositions often outperform region-specific sampling, highlighting the importance of diversity and global coverage in pre-training data.","Our results suggest that the most appropriate data sampling technique may depend on the specific GFM architecture.","These findings will support the development of robust GFMs by incorporating quality pre-training data distributions, ultimately improving machine learning solutions for Earth observation."],"url":"http://arxiv.org/abs/2501.12535v1"}
{"created":"2025-01-21 22:26:47","title":"Federated Discrete Denoising Diffusion Model for Molecular Generation with OpenFL","abstract":"Generating unique molecules with biochemically desired properties to serve as viable drug candidates is a difficult task that requires specialized domain expertise. In recent years, diffusion models have shown promising results in accelerating the drug design process through AI-driven molecular generation. However, training these models requires massive amounts of data, which are often isolated in proprietary silos. OpenFL is a federated learning framework that enables privacy-preserving collaborative training across these decentralized data sites. In this work, we present a federated discrete denoising diffusion model that was trained using OpenFL. The federated model achieves comparable performance with a model trained on centralized data when evaluating the uniqueness and validity of the generated molecules. This demonstrates the utility of federated learning in the drug design process.   OpenFL is available at: https://github.com/securefederatedai/openfl","sentences":["Generating unique molecules with biochemically desired properties to serve as viable drug candidates is a difficult task that requires specialized domain expertise.","In recent years, diffusion models have shown promising results in accelerating the drug design process through AI-driven molecular generation.","However, training these models requires massive amounts of data, which are often isolated in proprietary silos.","OpenFL is a federated learning framework that enables privacy-preserving collaborative training across these decentralized data sites.","In this work, we present a federated discrete denoising diffusion model that was trained using OpenFL.","The federated model achieves comparable performance with a model trained on centralized data when evaluating the uniqueness and validity of the generated molecules.","This demonstrates the utility of federated learning in the drug design process.   ","OpenFL is available at: https://github.com/securefederatedai/openfl"],"url":"http://arxiv.org/abs/2501.12523v1"}
{"created":"2025-01-21 22:25:56","title":"Topology of Out-of-Distribution Examples in Deep Neural Networks","abstract":"As deep neural networks (DNNs) become increasingly common, concerns about their robustness do as well. A longstanding problem for deployed DNNs is their behavior in the face of unfamiliar inputs; specifically, these models tend to be overconfident and incorrect when encountering out-of-distribution (OOD) examples. In this work, we present a topological approach to characterizing OOD examples using latent layer embeddings from DNNs. Our goal is to identify topological features, referred to as landmarks, that indicate OOD examples. We conduct extensive experiments on benchmark datasets and a realistic DNN model, revealing a key insight for OOD detection. Well-trained DNNs have been shown to induce a topological simplification on training data for simple models and datasets; we show that this property holds for realistic, large-scale test and training data, but does not hold for OOD examples. More specifically, we find that the average lifetime (or persistence) of OOD examples is statistically longer than that of training or test examples. This indicates that DNNs struggle to induce topological simplification on unfamiliar inputs. Our empirical results provide novel evidence of topological simplification in realistic DNNs and lay the groundwork for topologically-informed OOD detection strategies.","sentences":["As deep neural networks (DNNs) become increasingly common, concerns about their robustness do as well.","A longstanding problem for deployed DNNs is their behavior in the face of unfamiliar inputs; specifically, these models tend to be overconfident and incorrect when encountering out-of-distribution (OOD) examples.","In this work, we present a topological approach to characterizing OOD examples using latent layer embeddings from DNNs.","Our goal is to identify topological features, referred to as landmarks, that indicate OOD examples.","We conduct extensive experiments on benchmark datasets and a realistic DNN model, revealing a key insight for OOD detection.","Well-trained DNNs have been shown to induce a topological simplification on training data for simple models and datasets; we show that this property holds for realistic, large-scale test and training data, but does not hold for OOD examples.","More specifically, we find that the average lifetime (or persistence) of OOD examples is statistically longer than that of training or test examples.","This indicates that DNNs struggle to induce topological simplification on unfamiliar inputs.","Our empirical results provide novel evidence of topological simplification in realistic DNNs and lay the groundwork for topologically-informed OOD detection strategies."],"url":"http://arxiv.org/abs/2501.12522v1"}
{"created":"2025-01-21 22:24:03","title":"An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts","abstract":"The tidal wave of advancements in Large Language Models (LLMs) has led to their swift integration into application-level logic. Many software systems now use prompts to interact with these black-box models, combining natural language with dynamic values interpolated at runtime, to perform tasks ranging from sentiment analysis to question answering. Due to the programmatic and structured natural language aspects of these prompts, we refer to them as Developer Prompts. Unlike traditional software artifacts, Dev Prompts blend natural language instructions with artificial languages such as programming and markup languages, thus requiring specialized tools for analysis, distinct from classical software evaluation methods.   In response to this need, we introduce PromptDoctor, a tool explicitly designed to detect and correct issues of Dev Prompts. PromptDoctor identifies and addresses problems related to bias, vulnerability, and sub-optimal performance in Dev Prompts, helping mitigate their possible harms. In our analysis of 2,173 Dev Prompts, selected as a representative sample of 40,573 Dev Prompts, we found that 3.46% contained one or more forms of bias, 10.75% were vulnerable to prompt injection attacks. Additionally, 3,310 were amenable to automated prompt optimization. To address these issues, we applied PromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased 68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev Prompts, and improved the performance of 37.1% sub-optimal Dev Prompts. Finally, we developed a PromptDoctor VSCode extension, enabling developers to easily enhance Dev Prompts in their existing development workflows. The data and source code for this work are available at","sentences":["The tidal wave of advancements in Large Language Models (LLMs) has led to their swift integration into application-level logic.","Many software systems now use prompts to interact with these black-box models, combining natural language with dynamic values interpolated at runtime, to perform tasks ranging from sentiment analysis to question answering.","Due to the programmatic and structured natural language aspects of these prompts, we refer to them as Developer Prompts.","Unlike traditional software artifacts, Dev Prompts blend natural language instructions with artificial languages such as programming and markup languages, thus requiring specialized tools for analysis, distinct from classical software evaluation methods.   ","In response to this need, we introduce PromptDoctor, a tool explicitly designed to detect and correct issues of Dev Prompts.","PromptDoctor identifies and addresses problems related to bias, vulnerability, and sub-optimal performance in Dev Prompts, helping mitigate their possible harms.","In our analysis of 2,173 Dev Prompts, selected as a representative sample of 40,573 Dev Prompts, we found that 3.46% contained one or more forms of bias, 10.75% were vulnerable to prompt injection attacks.","Additionally, 3,310 were amenable to automated prompt optimization.","To address these issues, we applied PromptDoctor to the flawed Dev Prompts we discovered.","PromptDoctor de-biased 68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev Prompts, and improved the performance of 37.1% sub-optimal Dev Prompts.","Finally, we developed a PromptDoctor VSCode extension, enabling developers to easily enhance Dev Prompts in their existing development workflows.","The data and source code for this work are available at"],"url":"http://arxiv.org/abs/2501.12521v1"}
{"created":"2025-01-21 22:00:54","title":"Robustness of Selected Learning Models under Label-Flipping Attack","abstract":"In this paper we compare traditional machine learning and deep learning models trained on a malware dataset when subjected to adversarial attack based on label-flipping. Specifically, we investigate the robustness of Support Vector Machines (SVM), Random Forest, Gaussian Naive Bayes (GNB), Gradient Boosting Machine (GBM), LightGBM, XGBoost, Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), MobileNet, and DenseNet models when facing varying percentages of misleading labels. We empirically assess the the accuracy of each of these models under such an adversarial attack on the training data. This research aims to provide insights into which models are inherently more robust, in the sense of being better able to resist intentional disruptions to the training data. We find wide variation in the robustness of the models tested to adversarial attack, with our MLP model achieving the best combination of initial accuracy and robustness.","sentences":["In this paper we compare traditional machine learning and deep learning models trained on a malware dataset when subjected to adversarial attack based on label-flipping.","Specifically, we investigate the robustness of Support Vector Machines (SVM), Random Forest, Gaussian Naive Bayes (GNB), Gradient Boosting Machine (GBM), LightGBM, XGBoost, Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), MobileNet, and DenseNet models when facing varying percentages of misleading labels.","We empirically assess the the accuracy of each of these models under such an adversarial attack on the training data.","This research aims to provide insights into which models are inherently more robust, in the sense of being better able to resist intentional disruptions to the training data.","We find wide variation in the robustness of the models tested to adversarial attack, with our MLP model achieving the best combination of initial accuracy and robustness."],"url":"http://arxiv.org/abs/2501.12516v1"}
{"created":"2025-01-21 21:15:42","title":"Stable Matching with Interviews","abstract":"In several two-sided markets, including labor and dating, agents typically have limited information about their preferences prior to mutual interactions. This issue can result in matching frictions, as arising in the labor market for medical residencies, where high application rates are followed by a large number of interviews. Yet, the extensive literature on two-sided matching primarily focuses on models where agents know their preferences, leaving the interactions necessary for preference discovery largely overlooked. This paper studies this problem using an algorithmic approach, extending Gale-Shapley's deferred acceptance to this context.   Two algorithms are proposed. The first is an adaptive algorithm that expands upon Gale-Shapley's deferred acceptance by incorporating interviews between applicants and positions. Similar to deferred acceptance, one side sequentially proposes to the other. However, the order of proposals is carefully chosen to ensure an interim stable matching is found. Furthermore, with high probability, the number of interviews conducted by each applicant or position is limited to $O(\\log^2 n)$.   In many seasonal markets, interactions occur more simultaneously, consisting of an initial interview phase followed by a clearing stage. We present a non-adaptive algorithm for generating a single stage set of in tiered random markets. The algorithm finds an interim stable matching in such markets while assigning no more than $O(\\log^3 n)$ interviews to each applicant or position.","sentences":["In several two-sided markets, including labor and dating, agents typically have limited information about their preferences prior to mutual interactions.","This issue can result in matching frictions, as arising in the labor market for medical residencies, where high application rates are followed by a large number of interviews.","Yet, the extensive literature on two-sided matching primarily focuses on models where agents know their preferences, leaving the interactions necessary for preference discovery largely overlooked.","This paper studies this problem using an algorithmic approach, extending Gale-Shapley's deferred acceptance to this context.   ","Two algorithms are proposed.","The first is an adaptive algorithm that expands upon Gale-Shapley's deferred acceptance by incorporating interviews between applicants and positions.","Similar to deferred acceptance, one side sequentially proposes to the other.","However, the order of proposals is carefully chosen to ensure an interim stable matching is found.","Furthermore, with high probability, the number of interviews conducted by each applicant or position is limited to $O(\\log^2 n)$.   In many seasonal markets, interactions occur more simultaneously, consisting of an initial interview phase followed by a clearing stage.","We present a non-adaptive algorithm for generating a single stage set of in tiered random markets.","The algorithm finds an interim stable matching in such markets while assigning no more than $O(\\log^3 n)$ interviews to each applicant or position."],"url":"http://arxiv.org/abs/2501.12503v1"}
{"created":"2025-01-21 21:04:08","title":"Identification of Nonparametric Dynamic Causal Structure and Latent Process in Climate System","abstract":"The study of learning causal structure with latent variables has advanced the understanding of the world by uncovering causal relationships and latent factors, e.g., Causal Representation Learning (CRL). However, in real-world scenarios, such as those in climate systems, causal relationships are often nonparametric, dynamic, and exist among both observed variables and latent variables. These challenges motivate us to consider a general setting in which causal relations are nonparametric and unrestricted in their occurrence, which is unconventional to current methods. To solve this problem, with the aid of 3-measurement in temporal structure, we theoretically show that both latent variables and processes can be identified up to minor indeterminacy under mild assumptions. Moreover, we tackle the general nonlinear Causal Discovery (CD) from observations, e.g., temperature, as a specific task of learning independent representation, through the principle of functional equivalence. Based on these insights, we develop an estimation approach simultaneously recovering both the observed causal structure and latent causal process in a nontrivial manner. Simulation studies validate the theoretical foundations and demonstrate the effectiveness of the proposed methodology. In the experiments involving climate data, this approach offers a powerful and in-depth understanding of the climate system.","sentences":["The study of learning causal structure with latent variables has advanced the understanding of the world by uncovering causal relationships and latent factors, e.g., Causal Representation Learning (CRL).","However, in real-world scenarios, such as those in climate systems, causal relationships are often nonparametric, dynamic, and exist among both observed variables and latent variables.","These challenges motivate us to consider a general setting in which causal relations are nonparametric and unrestricted in their occurrence, which is unconventional to current methods.","To solve this problem, with the aid of 3-measurement in temporal structure, we theoretically show that both latent variables and processes can be identified up to minor indeterminacy under mild assumptions.","Moreover, we tackle the general nonlinear Causal Discovery (CD) from observations, e.g., temperature, as a specific task of learning independent representation, through the principle of functional equivalence.","Based on these insights, we develop an estimation approach simultaneously recovering both the observed causal structure and latent causal process in a nontrivial manner.","Simulation studies validate the theoretical foundations and demonstrate the effectiveness of the proposed methodology.","In the experiments involving climate data, this approach offers a powerful and in-depth understanding of the climate system."],"url":"http://arxiv.org/abs/2501.12500v1"}
{"created":"2025-01-21 20:34:38","title":"Optimizing Blockchain Analysis: Tackling Temporality and Scalability with an Incremental Approach with Metropolis-Hastings Random Walks","abstract":"Blockchain technology, with implications in the financial domain, offers data in the form of large-scale transaction networks. Analyzing transaction networks facilitates fraud detection, market analysis, and supports government regulation. Despite many graph representation learning methods for transaction network analysis, we pinpoint two salient limitations that merit more investigation. Existing methods predominantly focus on the snapshots of transaction networks, sidelining the evolving nature of blockchain transaction networks. Existing methodologies may not sufficiently emphasize efficient, incremental learning capabilities, which are essential for addressing the scalability challenges in ever-expanding large-scale transaction networks. To address these challenges, we employed an incremental approach for random walk-based node representation learning in transaction networks. Further, we proposed a Metropolis-Hastings-based random walk mechanism for improved efficiency. The empirical evaluation conducted on blockchain transaction datasets reveals comparable performance in node classification tasks while reducing computational overhead. Potential applications include transaction network monitoring, the efficient classification of blockchain addresses for fraud detection or the identification of specialized address types within the network.","sentences":["Blockchain technology, with implications in the financial domain, offers data in the form of large-scale transaction networks.","Analyzing transaction networks facilitates fraud detection, market analysis, and supports government regulation.","Despite many graph representation learning methods for transaction network analysis, we pinpoint two salient limitations that merit more investigation.","Existing methods predominantly focus on the snapshots of transaction networks, sidelining the evolving nature of blockchain transaction networks.","Existing methodologies may not sufficiently emphasize efficient, incremental learning capabilities, which are essential for addressing the scalability challenges in ever-expanding large-scale transaction networks.","To address these challenges, we employed an incremental approach for random walk-based node representation learning in transaction networks.","Further, we proposed a Metropolis-Hastings-based random walk mechanism for improved efficiency.","The empirical evaluation conducted on blockchain transaction datasets reveals comparable performance in node classification tasks while reducing computational overhead.","Potential applications include transaction network monitoring, the efficient classification of blockchain addresses for fraud detection or the identification of specialized address types within the network."],"url":"http://arxiv.org/abs/2501.12491v1"}
{"created":"2025-01-21 20:34:15","title":"A Fast Counting-Free Algorithm for Computing Atomic Sets in Feature Models","abstract":"In the context of product-line engineering and feature models, atomic sets are sets of features that must always be selected together in order for a configuration to be valid. For many analyses and applications, these features may be condensed into one feature, without affecting, for instance, satisfiability, model counting, sampling, or knowledge compilation. However, the performance of current approaches tends to be insufficient in practice. This is especially true but not limited to approaches based on model counting. In this work, we present a counting-free algorithm for computing atomic sets that only relies on SAT solving. Our evaluation shows that it scales with ease to hard real-world systems and even succeeds for a contemporary version of the Linux kernel.","sentences":["In the context of product-line engineering and feature models, atomic sets are sets of features that must always be selected together in order for a configuration to be valid.","For many analyses and applications, these features may be condensed into one feature, without affecting, for instance, satisfiability, model counting, sampling, or knowledge compilation.","However, the performance of current approaches tends to be insufficient in practice.","This is especially true but not limited to approaches based on model counting.","In this work, we present a counting-free algorithm for computing atomic sets that only relies on SAT solving.","Our evaluation shows that it scales with ease to hard real-world systems and even succeeds for a contemporary version of the Linux kernel."],"url":"http://arxiv.org/abs/2501.12490v1"}
{"created":"2025-01-21 20:20:36","title":"A Smart IoT Framework for Climate-Resilient and Sustainable Maize Farming In Uganda","abstract":"This study provides a framework that incorporates the Internet of Things (IoT) technology into maize farming activities in Central Uganda as a solution to various challenges including climate change, sub-optimal resource use and low crop yields. Using IoT-based modeling and simulation, the presented solution recommends cost-effective and efficient approaches to irrigation, crop yield improvement enhancement and prevention of drinking water loss while being practical for smallholder farmers. The framework is developed in a manner that is appropriate for low resource use regions by using local strategies that are easily understandable and actionable for the farmers thus solving the issue of technology access and social economic constraints. Research in this area brought to light the promise that the IoT holds for the evolution of agriculture into a more data-informed, climate-smart sector, contributes to the much-needed food in the world, is economically viable, facilitates sustainable rural development and is a huge step for the agriculture modernization of Uganda.","sentences":["This study provides a framework that incorporates the Internet of Things (IoT) technology into maize farming activities in Central Uganda as a solution to various challenges including climate change, sub-optimal resource use and low crop yields.","Using IoT-based modeling and simulation, the presented solution recommends cost-effective and efficient approaches to irrigation, crop yield improvement enhancement and prevention of drinking water loss while being practical for smallholder farmers.","The framework is developed in a manner that is appropriate for low resource use regions by using local strategies that are easily understandable and actionable for the farmers thus solving the issue of technology access and social economic constraints.","Research in this area brought to light the promise that the IoT holds for the evolution of agriculture into a more data-informed, climate-smart sector, contributes to the much-needed food in the world, is economically viable, facilitates sustainable rural development and is a huge step for the agriculture modernization of Uganda."],"url":"http://arxiv.org/abs/2501.12483v1"}
{"created":"2025-01-21 19:37:48","title":"An Empirical Characterization of Outages and Incidents in Public Services for Large Language Models","abstract":"People and businesses increasingly rely on public LLM services, such as ChatGPT, DALLE, and Claude. Understanding their outages, and particularly measuring their failure-recovery processes, is becoming a stringent problem. However, only limited studies exist in this emerging area. Addressing this problem, in this work we conduct an empirical characterization of outages and failure-recovery in public LLM services. We collect and prepare datasets for 8 commonly used LLM services across 3 major LLM providers, including market-leads OpenAI and Anthropic. We conduct a detailed analysis of failure recovery statistical properties, temporal patterns, co-occurrence, and the impact range of outage-causing incidents. We make over 10 observations, among which: (1) Failures in OpenAI's ChatGPT take longer to resolve but occur less frequently than those in Anthropic's Claude;(2) OpenAI and Anthropic service failures exhibit strong weekly and monthly periodicity; and (3) OpenAI services offer better failure-isolation than Anthropic services. Our research explains LLM failure characteristics and thus enables optimization in building and using LLM systems. FAIR data and code are publicly available on https://zenodo.org/records/14018219 and https://github.com/atlarge-research/llm-service-analysis.","sentences":["People and businesses increasingly rely on public LLM services, such as ChatGPT, DALLE, and Claude.","Understanding their outages, and particularly measuring their failure-recovery processes, is becoming a stringent problem.","However, only limited studies exist in this emerging area.","Addressing this problem, in this work we conduct an empirical characterization of outages and failure-recovery in public LLM services.","We collect and prepare datasets for 8 commonly used LLM services across 3 major LLM providers, including market-leads OpenAI and Anthropic.","We conduct a detailed analysis of failure recovery statistical properties, temporal patterns, co-occurrence, and the impact range of outage-causing incidents.","We make over 10 observations, among which: (1) Failures in OpenAI's ChatGPT take longer to resolve but occur less frequently than those in Anthropic's Claude;(2) OpenAI and Anthropic service failures exhibit strong weekly and monthly periodicity; and (3) OpenAI services offer better failure-isolation than Anthropic services.","Our research explains LLM failure characteristics and thus enables optimization in building and using LLM systems.","FAIR data and code are publicly available on https://zenodo.org/records/14018219 and https://github.com/atlarge-research/llm-service-analysis."],"url":"http://arxiv.org/abs/2501.12469v1"}
{"created":"2025-01-21 19:22:45","title":"Adaptive PII Mitigation Framework for Large Language Models","abstract":"Artificial Intelligence (AI) faces growing challenges from evolving data protection laws and enforcement practices worldwide. Regulations like GDPR and CCPA impose strict compliance requirements on Machine Learning (ML) models, especially concerning personal data use. These laws grant individuals rights such as data correction and deletion, complicating the training and deployment of Large Language Models (LLMs) that rely on extensive datasets. Public data availability does not guarantee its lawful use for ML, amplifying these challenges.   This paper introduces an adaptive system for mitigating risk of Personally Identifiable Information (PII) and Sensitive Personal Information (SPI) in LLMs. It dynamically aligns with diverse regulatory frameworks and integrates seamlessly into Governance, Risk, and Compliance (GRC) systems. The system uses advanced NLP techniques, context-aware analysis, and policy-driven masking to ensure regulatory compliance.   Benchmarks highlight the system's effectiveness, with an F1 score of 0.95 for Passport Numbers, outperforming tools like Microsoft Presidio (0.33) and Amazon Comprehend (0.54). In human evaluations, the system achieved an average user trust score of 4.6/5, with participants acknowledging its accuracy and transparency. Observations demonstrate stricter anonymization under GDPR compared to CCPA, which permits pseudonymization and user opt-outs. These results validate the system as a scalable and robust solution for enterprise privacy compliance.","sentences":["Artificial Intelligence (AI) faces growing challenges from evolving data protection laws and enforcement practices worldwide.","Regulations like GDPR and CCPA impose strict compliance requirements on Machine Learning (ML) models, especially concerning personal data use.","These laws grant individuals rights such as data correction and deletion, complicating the training and deployment of Large Language Models (LLMs) that rely on extensive datasets.","Public data availability does not guarantee its lawful use for ML, amplifying these challenges.   ","This paper introduces an adaptive system for mitigating risk of Personally Identifiable Information (PII) and Sensitive Personal Information (SPI) in LLMs.","It dynamically aligns with diverse regulatory frameworks and integrates seamlessly into Governance, Risk, and Compliance (GRC) systems.","The system uses advanced NLP techniques, context-aware analysis, and policy-driven masking to ensure regulatory compliance.   ","Benchmarks highlight the system's effectiveness, with an F1 score of 0.95 for Passport Numbers, outperforming tools like Microsoft Presidio (0.33) and Amazon Comprehend (0.54).","In human evaluations, the system achieved an average user trust score of 4.6/5, with participants acknowledging its accuracy and transparency.","Observations demonstrate stricter anonymization under GDPR compared to CCPA, which permits pseudonymization and user opt-outs.","These results validate the system as a scalable and robust solution for enterprise privacy compliance."],"url":"http://arxiv.org/abs/2501.12465v1"}
{"created":"2025-01-21 19:17:46","title":"Empowering AIOps: Leveraging Large Language Models for IT Operations ManagementOperations Management","abstract":"The integration of Artificial Intelligence (AI) into IT Operations Management (ITOM), commonly referred to as AIOps, offers substantial potential for automating workflows, enhancing efficiency, and supporting informed decision-making. However, implementing AI within IT operations is not without its challenges, including issues related to data quality, the complexity of IT environments, and skill gaps within teams. The advent of Large Language Models (LLMs) presents an opportunity to address some of these challenges, particularly through their advanced natural language understanding capabilities. These features enable organizations to process and analyze vast amounts of unstructured data, such as system logs, incident reports, and technical documentation. This ability aligns with the motivation behind our research, where we aim to integrate traditional predictive machine learning models with generative AI technologies like LLMs. By combining these approaches, we propose innovative methods to tackle persistent challenges in AIOps and enhance the capabilities of IT operations management.","sentences":["The integration of Artificial Intelligence (AI) into IT Operations Management (ITOM), commonly referred to as AIOps, offers substantial potential for automating workflows, enhancing efficiency, and supporting informed decision-making.","However, implementing AI within IT operations is not without its challenges, including issues related to data quality, the complexity of IT environments, and skill gaps within teams.","The advent of Large Language Models (LLMs) presents an opportunity to address some of these challenges, particularly through their advanced natural language understanding capabilities.","These features enable organizations to process and analyze vast amounts of unstructured data, such as system logs, incident reports, and technical documentation.","This ability aligns with the motivation behind our research, where we aim to integrate traditional predictive machine learning models with generative AI technologies like LLMs.","By combining these approaches, we propose innovative methods to tackle persistent challenges in AIOps and enhance the capabilities of IT operations management."],"url":"http://arxiv.org/abs/2501.12461v1"}
{"created":"2025-01-21 19:04:53","title":"Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications","abstract":"The adoption of Large Language Models (LLMs) has revolutionized AI applications but poses significant challenges in safeguarding user privacy. Ensuring compliance with privacy regulations such as GDPR and CCPA while addressing nuanced privacy risks requires robust and scalable frameworks. This paper presents a detailed study of OneShield Privacy Guard, a framework designed to mitigate privacy risks in user inputs and LLM outputs across enterprise and open-source settings. We analyze two real-world deployments:(1) a multilingual privacy-preserving system integrated with Data and Model Factory, focusing on enterprise-scale data governance; and (2) PR Insights, an open-source repository emphasizing automated triaging and community-driven refinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting sensitive entities like dates, names, and phone numbers across 26 languages, outperforming state-of-the-art tool such as StarPII and Presidio by up to 12\\%. Deployment 2, with an average F1 score of 0.86, reduced manual effort by over 300 hours in three months, accurately flagging 8.25\\% of 1,256 pull requests for privacy risks with enhanced context sensitivity. These results demonstrate OneShield's adaptability and efficacy in diverse environments, offering actionable insights for context-aware entity recognition, automated compliance, and ethical AI adoption. This work advances privacy-preserving frameworks, supporting user trust and compliance across operational contexts.","sentences":["The adoption of Large Language Models (LLMs) has revolutionized AI applications but poses significant challenges in safeguarding user privacy.","Ensuring compliance with privacy regulations such as GDPR and CCPA while addressing nuanced privacy risks requires robust and scalable frameworks.","This paper presents a detailed study of OneShield Privacy Guard, a framework designed to mitigate privacy risks in user inputs and LLM outputs across enterprise and open-source settings.","We analyze two real-world deployments:(1) a multilingual privacy-preserving system integrated with Data and Model Factory, focusing on enterprise-scale data governance; and (2) PR Insights, an open-source repository emphasizing automated triaging and community-driven refinements.","In Deployment 1, OneShield achieved a 0.95 F1 score in detecting sensitive entities like dates, names, and phone numbers across 26 languages, outperforming state-of-the-art tool such as StarPII and Presidio by up to 12\\%.","Deployment 2, with an average F1 score of 0.86, reduced manual effort by over 300 hours in three months, accurately flagging 8.25\\% of 1,256 pull requests for privacy risks with enhanced context sensitivity.","These results demonstrate OneShield's adaptability and efficacy in diverse environments, offering actionable insights for context-aware entity recognition, automated compliance, and ethical AI adoption.","This work advances privacy-preserving frameworks, supporting user trust and compliance across operational contexts."],"url":"http://arxiv.org/abs/2501.12456v1"}
{"created":"2025-01-21 18:54:16","title":"Enhancing Retrosynthesis with Conformer: A Template-Free Method","abstract":"Retrosynthesis plays a crucial role in the fields of organic synthesis and drug development, where the goal is to identify suitable reactants that can yield a target product molecule. Although existing methods have achieved notable success, they typically overlook the 3D conformational details and internal spatial organization of molecules. This oversight makes it challenging to predict reactants that conform to genuine chemical principles, particularly when dealing with complex molecular structures, such as polycyclic and heteroaromatic compounds. In response to this challenge, we introduce a novel transformer-based, template-free approach that incorporates 3D conformer data and spatial information. Our approach includes an Atom-align Fusion module that integrates 3D positional data at the input stage, ensuring correct alignment between atom tokens and their respective 3D coordinates. Additionally, we propose a Distance-weighted Attention mechanism that refines the self-attention process, constricting the model s focus to relevant atom pairs in 3D space. Extensive experiments on the USPTO-50K dataset demonstrate that our model outperforms previous template-free methods, setting a new benchmark for the field. A case study further highlights our method s ability to predict reasonable and accurate reactants.","sentences":["Retrosynthesis plays a crucial role in the fields of organic synthesis and drug development, where the goal is to identify suitable reactants that can yield a target product molecule.","Although existing methods have achieved notable success, they typically overlook the 3D conformational details and internal spatial organization of molecules.","This oversight makes it challenging to predict reactants that conform to genuine chemical principles, particularly when dealing with complex molecular structures, such as polycyclic and heteroaromatic compounds.","In response to this challenge, we introduce a novel transformer-based, template-free approach that incorporates 3D conformer data and spatial information.","Our approach includes an Atom-align Fusion module that integrates 3D positional data at the input stage, ensuring correct alignment between atom tokens and their respective 3D coordinates.","Additionally, we propose a Distance-weighted Attention mechanism that refines the self-attention process, constricting the model s focus to relevant atom pairs in 3D space.","Extensive experiments on the USPTO-50K dataset demonstrate that our model outperforms previous template-free methods, setting a new benchmark for the field.","A case study further highlights our method s ability to predict reasonable and accurate reactants."],"url":"http://arxiv.org/abs/2501.12434v1"}
