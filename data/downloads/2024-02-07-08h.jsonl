{"created":"2024-02-06 18:43:48","title":"CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations","abstract":"Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance. The code and data are publicly available at https://github.com/THUDM/CogCoM.","sentences":["Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers.","However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses.","In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in).","This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths.","We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism.","Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance.","The code and data are publicly available at https://github.com/THUDM/CogCoM."],"url":"http://arxiv.org/abs/2402.04236v1"}
{"created":"2024-02-06 18:39:43","title":"Can Generative Agents Predict Emotion?","abstract":"Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents.","sentences":["Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans.","In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories.","Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation.","First, the agent perceives new experiences as time series text data.","After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm.","Through this comparison we can analyse how the agent reacts to the new experience in context.","The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event.","Finally, the new experience is then added to the agents memory to be used in the creation of future norms.","By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios.","The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary.","We hope that this paper is another step towards the alignment of generative agents."],"url":"http://arxiv.org/abs/2402.04232v1"}
{"created":"2024-02-06 18:17:02","title":"Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks","abstract":"Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive the convergence bound of the proposed algorithm. Based on this bound, we minimize a weighted utility function for jointly configuring the controllable parameters to train the RawHFL energy efficiently under practical resource constraints. Our extensive simulation results validate the proposed algorithm's superiority, in terms of test accuracy and energy cost, over existing baselines.","sentences":["Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks.","Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs).","As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands.","Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests.","A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content.","Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive the convergence bound of the proposed algorithm.","Based on this bound, we minimize a weighted utility function for jointly configuring the controllable parameters to train the RawHFL energy efficiently under practical resource constraints.","Our extensive simulation results validate the proposed algorithm's superiority, in terms of test accuracy and energy cost, over existing baselines."],"url":"http://arxiv.org/abs/2402.04216v1"}
{"created":"2024-02-06 18:09:05","title":"Variational Shapley Network: A Probabilistic Approach to Self-Explaining Shapley values with Uncertainty Quantification","abstract":"Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes. Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations. We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass. Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations. Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture. Evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance.","sentences":["Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes.","Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations.","We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass.","Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations.","Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture.","Evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance."],"url":"http://arxiv.org/abs/2402.04211v1"}
{"created":"2024-02-06 18:01:29","title":"Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models","abstract":"This paper introduces a system designed to generate explanations for the actions performed by an autonomous robot in Human-Robot Interaction (HRI). Explainability in robotics, encapsulated within the concept of an eXplainable Autonomous Robot (XAR), is a growing research area. The work described in this paper aims to take advantage of the capabilities of Large Language Models (LLMs) in performing natural language processing tasks. This study focuses on the possibility of generating explanations using such models in combination with a Retrieval Augmented Generation (RAG) method to interpret data gathered from the logs of autonomous systems. In addition, this work also presents a formalization of the proposed explanation system. It has been evaluated through a navigation test from the European Robotics League (ERL), a Europe-wide social robotics competition. Regarding the obtained results, a validation questionnaire has been conducted to measure the quality of the explanations from the perspective of technical users. The results obtained during the experiment highlight the potential utility of LLMs in achieving explanatory capabilities in robots.","sentences":["This paper introduces a system designed to generate explanations for the actions performed by an autonomous robot in Human-Robot Interaction (HRI).","Explainability in robotics, encapsulated within the concept of an eXplainable Autonomous Robot (XAR), is a growing research area.","The work described in this paper aims to take advantage of the capabilities of Large Language Models (LLMs) in performing natural language processing tasks.","This study focuses on the possibility of generating explanations using such models in combination with a Retrieval Augmented Generation (RAG) method to interpret data gathered from the logs of autonomous systems.","In addition, this work also presents a formalization of the proposed explanation system.","It has been evaluated through a navigation test from the European Robotics League (ERL), a Europe-wide social robotics competition.","Regarding the obtained results, a validation questionnaire has been conducted to measure the quality of the explanations from the perspective of technical users.","The results obtained during the experiment highlight the potential utility of LLMs in achieving explanatory capabilities in robots."],"url":"http://arxiv.org/abs/2402.04206v1"}
{"created":"2024-02-06 17:59:46","title":"Human-Like Geometric Abstraction in Large Pre-trained Neural Networks","abstract":"Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry. Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations. However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data. In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations. We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing.","sentences":["Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry.","Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations.","However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data.","In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations.","We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing."],"url":"http://arxiv.org/abs/2402.04203v1"}
{"created":"2024-02-06 17:49:02","title":"Gradient Coding in Decentralized Learning for Evading Stragglers","abstract":"In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.","sentences":["In this paper, we consider a decentralized learning problem in the presence of stragglers.","Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios.","To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO).","In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner.","We analyze the convergence performance of GOCO for strongly convex loss functions.","And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods."],"url":"http://arxiv.org/abs/2402.04193v1"}
{"created":"2024-02-06 17:46:41","title":"Start Stop Bit Method for Efficient Data Communication in 6G Mobile Radio Systems","abstract":"In this article, a novel approach for mobile radio communications is proposed and analysed, which is promising for future 6G cooperative distributed MIMO systems. The fundamental idea is a new mechanism namely start stop bit method, which transmits bit sequences as the start/stop bits of a synchronized counter instead of transmitting the full encoded bit sequence itself. In that way, theoretically, we can transmit infinitely long data messages with only one bit for starting and one bit for stopping the counter. The value of the counter, as identified by the stop bit, is then used to reconstruct and remap the one and unique transmitted bit sequence. The start stop bit method is characterized by a high signal sparsity as only two bits are transmitted, independently of the bit sequence length for the message. Among the benefits of the start stop bit method are energy efficient data transmission, and effective distributed MIMO systems, which exploit the sparse inter cooperation area interference as well as the low processing complexity for the sparse precoder calculation. Moreover, for the next mobile wireless generation, we propose an advanced scheme of the start stop bit method which enhances its resource usage. We call the resulting method a sparse dMIMO system.","sentences":["In this article, a novel approach for mobile radio communications is proposed and analysed, which is promising for future 6G cooperative distributed MIMO systems.","The fundamental idea is a new mechanism namely start stop bit method, which transmits bit sequences as the start/stop bits of a synchronized counter instead of transmitting the full encoded bit sequence itself.","In that way, theoretically, we can transmit infinitely long data messages with only one bit for starting and one bit for stopping the counter.","The value of the counter, as identified by the stop bit, is then used to reconstruct and remap the one and unique transmitted bit sequence.","The start stop bit method is characterized by a high signal sparsity as only two bits are transmitted, independently of the bit sequence length for the message.","Among the benefits of the start stop bit method are energy efficient data transmission, and effective distributed MIMO systems, which exploit the sparse inter cooperation area interference as well as the low processing complexity for the sparse precoder calculation.","Moreover, for the next mobile wireless generation, we propose an advanced scheme of the start stop bit method which enhances its resource usage.","We call the resulting method a sparse dMIMO system."],"url":"http://arxiv.org/abs/2402.04187v1"}
{"created":"2024-02-06 17:42:39","title":"Reinforcement Learning with Ensemble Model Predictive Safety Certification","abstract":"Reinforcement learning algorithms need exploration to learn. However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment. In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning. Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller. Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods.","sentences":["Reinforcement learning algorithms need exploration to learn.","However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment.","In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning.","Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller.","Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods."],"url":"http://arxiv.org/abs/2402.04182v1"}
{"created":"2024-02-06 17:38:41","title":"Deep-Learning Estimation of Weight Distribution Using Joint Kinematics for Lower-Limb Exoskeleton Control","abstract":"In the control of lower-limb exoskeletons with feet, the phase in the gait cycle can be identified by monitoring the weight distribution at the feet. This phase information can be used in the exoskeleton's controller to compensate the dynamics of the exoskeleton and to assign impedance parameters. Typically the weight distribution is calculated using data from sensors such as treadmill force plates or insole force sensors. However, these solutions increase both the setup complexity and cost. For this reason, we propose a deep-learning approach that uses a short time window of joint kinematics to predict the weight distribution of an exoskeleton in real time. The model was trained on treadmill walking data from six users wearing a four-degree-of-freedom exoskeleton and tested in real time on three different users wearing the same device. This test set includes two users not present in the training set to demonstrate the model's ability to generalize across individuals. Results show that the proposed method is able to fit the actual weight distribution with R2=0.9 and is suitable for real-time control with prediction times less than 1 ms. Experiments in closed-loop exoskeleton control show that deep-learning-based weight distribution estimation can be used to replace force sensors in overground and treadmill walking.","sentences":["In the control of lower-limb exoskeletons with feet, the phase in the gait cycle can be identified by monitoring the weight distribution at the feet.","This phase information can be used in the exoskeleton's controller to compensate the dynamics of the exoskeleton and to assign impedance parameters.","Typically the weight distribution is calculated using data from sensors such as treadmill force plates or insole force sensors.","However, these solutions increase both the setup complexity and cost.","For this reason, we propose a deep-learning approach that uses a short time window of joint kinematics to predict the weight distribution of an exoskeleton in real time.","The model was trained on treadmill walking data from six users wearing a four-degree-of-freedom exoskeleton and tested in real time on three different users wearing the same device.","This test set includes two users not present in the training set to demonstrate the model's ability to generalize across individuals.","Results show that the proposed method is able to fit the actual weight distribution with R2=0.9 and is suitable for real-time control with prediction times less than 1 ms.","Experiments in closed-loop exoskeleton control show that deep-learning-based weight distribution estimation can be used to replace force sensors in overground and treadmill walking."],"url":"http://arxiv.org/abs/2402.04180v1"}
{"created":"2024-02-06 17:31:36","title":"SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models","abstract":"Multimodal large language models (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and grounding) based on strong visual semantic representation and language reasoning ability. However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored. In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to evaluate multimodal face data in these two face security tasks. For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask). For the face forgery detection task, we evaluate GAN-based and diffusion-based data with both visual and acoustic modalities. Each question is subjected to both zero-shot and few-shot tests under standard and chain of thought (COT) settings. The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, multimodal flexible reasoning, and joint face spoof and forgery detection. Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining. Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT. The project is available at https$:$//github.com/laiyingxin2/SHIELD","sentences":["Multimodal large language models (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and grounding) based on strong visual semantic representation and language reasoning ability.","However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored.","In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection.","Specifically, we design true/false and multiple-choice questions to evaluate multimodal face data in these two face security tasks.","For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask).","For the face forgery detection task, we evaluate GAN-based and diffusion-based data with both visual and acoustic modalities.","Each question is subjected to both zero-shot and few-shot tests under standard and chain of thought (COT) settings.","The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, multimodal flexible reasoning, and joint face spoof and forgery detection.","Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining.","Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT.","The project is available at https$:$//github.com/laiyingxin2/SHIELD"],"url":"http://arxiv.org/abs/2402.04178v1"}
{"created":"2024-02-06 17:31:20","title":"Scaling Laws for Downstream Task Performance of Large Language Models","abstract":"Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with more pretraining data. In such cases, we show that it is possible to predict the downstream BLEU score with good accuracy using a log-law. However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves. By analyzing these observations, we provide new practical insights for choosing appropriate pretraining data.","sentences":["Scaling laws provide important insights that can guide the design of large language models (LLMs).","Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss.","However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance.","In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks.","Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score.","Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior.","With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with more pretraining data.","In such cases, we show that it is possible to predict the downstream BLEU score with good accuracy using a log-law.","However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves.","By analyzing these observations, we provide new practical insights for choosing appropriate pretraining data."],"url":"http://arxiv.org/abs/2402.04177v1"}
{"created":"2024-02-06 17:22:45","title":"Mind the Gap: Securely modeling cyber risk based on security deviations from a peer group","abstract":"There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers? Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share. Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed. As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group. This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations. We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data. We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers.","sentences":["There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers?","Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share.","Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed.","As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group.","This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations.","We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data.","We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers."],"url":"http://arxiv.org/abs/2402.04166v1"}
{"created":"2024-02-06 17:18:59","title":"Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains","abstract":"In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the transformer architecture. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. We further investigate these findings in the broader context of higher order Markov chains and deeper architectures, and outline open problems in this arena. Code is available at \\url{https://github.com/Bond1995/Markov}.","sentences":["In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages.","A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner.","To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains.","Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance.","In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the transformer architecture.","Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results.","We further investigate these findings in the broader context of higher order Markov chains and deeper architectures, and outline open problems in this arena.","Code is available at \\url{https://github.com/Bond1995/Markov}."],"url":"http://arxiv.org/abs/2402.04161v1"}
{"created":"2024-02-06 16:48:58","title":"Human Emotions Analysis and Recognition Using EEG Signals in Response to 360$^\\circ$ Videos","abstract":"Emotion recognition (ER) technology is an integral part for developing innovative applications such as drowsiness detection and health monitoring that plays a pivotal role in contemporary society. This study delves into ER using electroencephalography (EEG), within immersive virtual reality (VR) environments. There are four main stages in our proposed methodology including data acquisition, pre-processing, feature extraction, and emotion classification. Acknowledging the limitations of existing 2D datasets, we introduce a groundbreaking 3D VR dataset to elevate the precision of emotion elicitation. Leveraging the Interaxon Muse headband for EEG recording and Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40 participants, prioritizing subjects without reported mental illnesses. Pre-processing entails rigorous cleaning, uniform truncation, and the application of a Savitzky-Golay filter to the EEG data. Feature extraction encompasses a comprehensive analysis of metrics such as power spectral density, correlation, rational and divisional asymmetry, and power spectrum. To ensure the robustness of our model, we employed a 10-fold cross-validation, revealing an average validation accuracy of 85.54\\%, with a noteworthy maximum accuracy of 90.20\\% in the best fold. Subsequently, the trained model demonstrated a commendable test accuracy of 82.03\\%, promising favorable outcomes.","sentences":["Emotion recognition (ER) technology is an integral part for developing innovative applications such as drowsiness detection and health monitoring that plays a pivotal role in contemporary society.","This study delves into ER using electroencephalography (EEG), within immersive virtual reality (VR) environments.","There are four main stages in our proposed methodology including data acquisition, pre-processing, feature extraction, and emotion classification.","Acknowledging the limitations of existing 2D datasets, we introduce a groundbreaking 3D VR dataset to elevate the precision of emotion elicitation.","Leveraging the Interaxon Muse headband for EEG recording and Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40 participants, prioritizing subjects without reported mental illnesses.","Pre-processing entails rigorous cleaning, uniform truncation, and the application of a Savitzky-Golay filter to the EEG data.","Feature extraction encompasses a comprehensive analysis of metrics such as power spectral density, correlation, rational and divisional asymmetry, and power spectrum.","To ensure the robustness of our model, we employed a 10-fold cross-validation, revealing an average validation accuracy of 85.54\\%, with a noteworthy maximum accuracy of 90.20\\% in the best fold.","Subsequently, the trained model demonstrated a commendable test accuracy of 82.03\\%, promising favorable outcomes."],"url":"http://arxiv.org/abs/2402.04142v1"}
{"created":"2024-02-06 16:12:36","title":"Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science","abstract":"Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science.","sentences":["Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field.","In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM).","However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models.","To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition.","Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks.","Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering.","Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science."],"url":"http://arxiv.org/abs/2402.04119v1"}
{"created":"2024-02-06 16:03:57","title":"Behind the Screen: Investigating ChatGPT's Dark Personality Traits and Conspiracy Beliefs","abstract":"ChatGPT is notorious for its intransparent behavior. This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale. The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4. For traits that have shown to be interdependent in human studies, correlations were considered. Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses. Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between GPT-3.5 and GPT-4. However, GPT-4 showed a pronounced tendency to believe in information withholding. This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5. Apparently, in this case an increased data exposure correlates with a greater belief in the control of information. An assignment of extreme political affiliations increased the belief in conspiracy theories. Test sequencing affected the models' responses and the observed correlations, indicating a form of contextual memory.","sentences":["ChatGPT is notorious for its intransparent behavior.","This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4.","Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale.","The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4.","For traits that have shown to be interdependent in human studies, correlations were considered.","Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses.","Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between GPT-3.5 and GPT-4.","However, GPT-4 showed a pronounced tendency to believe in information withholding.","This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5.","Apparently, in this case an increased data exposure correlates with a greater belief in the control of information.","An assignment of extreme political affiliations increased the belief in conspiracy theories.","Test sequencing affected the models' responses and the observed correlations, indicating a form of contextual memory."],"url":"http://arxiv.org/abs/2402.04110v1"}
{"created":"2024-02-06 15:55:46","title":"VRMM: A Volumetric Relightable Morphable Head Model","abstract":"In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables accurate 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.","sentences":["In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling.","While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions.","Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations.","This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice.","The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions.","We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation.","Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM.","Such an approach enables accurate 3D face reconstruction from even a single portrait input.","Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling."],"url":"http://arxiv.org/abs/2402.04101v1"}
{"created":"2024-02-06 15:52:23","title":"Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction","abstract":"The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI). However, conventional DIP suffers from severe overfitting and spectral bias effects.In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.This study sheds light on important underlying properties for DIP-based recovery.Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs. However, obtaining suitable reference images requires supervision, and raises practical difficulties. In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data. Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting.","sentences":["The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI).","However, conventional DIP suffers from severe overfitting and spectral bias effects.","In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.","This study sheds light on important underlying properties for DIP-based recovery.","Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs.","However, obtaining suitable reference images requires supervision, and raises practical difficulties.","In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data.","Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.","We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting."],"url":"http://arxiv.org/abs/2402.04097v1"}
{"created":"2024-02-06 15:46:48","title":"Acceleration and energy consumption optimization in cascading classifiers for face detection on low-cost ARM big.LITTLE asymmetric architectures","abstract":"This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget. A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features. The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing object detection algorithms based on cascading classifiers. Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements. Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture.","sentences":["This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget.","A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.","LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features.","The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing object detection algorithms based on cascading classifiers.","Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements.","Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture."],"url":"http://arxiv.org/abs/2402.04090v1"}
{"created":"2024-02-06 15:45:27","title":"A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation","abstract":"Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches. Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}.","sentences":["Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity.","Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks.","However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources.","In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP.","Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance.","By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training.","To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP.","Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization.","In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches.","Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}."],"url":"http://arxiv.org/abs/2402.04087v1"}
{"created":"2024-02-06 15:39:09","title":"Provably learning a multi-head attention layer","abstract":"The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and projection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns $F$ to small error given random labeled examples drawn uniformly from $\\{\\pm 1\\}^{k\\times d}$.   - We prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable.   We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian. Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the Gaussian distribution. In contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and \"slices\" thereof.","sentences":["The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models.","Given a sequence length $k$, attention matrices $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and projection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X})","\\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$.","In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns $F$ to small error given random labeled examples drawn uniformly from $\\{\\pm 1\\}^{k\\times d}$.   - We prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable.   ","We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian.","Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the Gaussian distribution.","In contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and \"slices\" thereof."],"url":"http://arxiv.org/abs/2402.04084v1"}
{"created":"2024-02-06 15:34:44","title":"Improved Generalization of Weight Space Networks via Augmentations","abstract":"Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substantial 5-10% gains in downstream classification.","sentences":["Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks.","Unfortunately, weight space models tend to suffer from substantial overfitting.","We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets.","While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object.","To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces.","We demonstrate the effectiveness of these methods in two setups.","In classification, they improve performance similarly to having up to 10 times more data.","In self-supervised contrastive learning, they yield substantial 5-10% gains in downstream classification."],"url":"http://arxiv.org/abs/2402.04081v1"}
{"created":"2024-02-06 15:34:30","title":"Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning","abstract":"This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}.","sentences":["This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL).","At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy.","We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets.","To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement.","By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks.","Code is available at \\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}."],"url":"http://arxiv.org/abs/2402.04080v1"}
{"created":"2024-02-06 15:13:17","title":"Retrieve to Explain: Evidence-driven Predictions with Language Models","abstract":"Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.","sentences":["Machine learning models, particularly language models, are notoriously difficult to introspect.","Black-box models can mask both issues in model training and harmful biases.","For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively.","To address these issues, we introduce Retrieve to Explain (R2E).","R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction.","R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language.","We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes."],"url":"http://arxiv.org/abs/2402.04068v1"}
{"created":"2024-02-06 15:03:53","title":"Deep Learning for Multivariate Time Series Imputation: A Survey","abstract":"The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}.","sentences":["The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis.","Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks.","In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods.","First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations.","We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks.","Finally, the open issues for future research on multivariate time series imputation are pointed out.","All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}."],"url":"http://arxiv.org/abs/2402.04059v1"}
{"created":"2024-02-06 14:43:31","title":"Mission Planning and Safety Assessment for Pipeline Inspection Using Autonomous Underwater Vehicles: A Framework based on Behavior Trees","abstract":"The recent advance in autonomous underwater robotics facilitates autonomous inspection tasks of offshore infrastructure. However, current inspection missions rely on predefined plans created offline, hampering the flexibility and autonomy of the inspection vehicle and the mission's success in case of unexpected events. In this work, we address these challenges by proposing a framework encompassing the modeling and verification of mission plans through Behavior Trees (BTs). This framework leverages the modularity of BTs to model onboard reactive behaviors, thus enabling autonomous plan executions, and uses BehaVerify to verify the mission's safety. Moreover, as a use case of this framework, we present a novel AI-enabled algorithm that aims for efficient, autonomous pipeline camera data collection. In a simulated environment, we demonstrate the framework's application to our proposed pipeline inspection algorithm. Our framework marks a significant step forward in the field of autonomous underwater robotics, promising to enhance the safety and success of underwater missions in practical, real-world applications. https://github.com/remaro-network/pipe_inspection_mission","sentences":["The recent advance in autonomous underwater robotics facilitates autonomous inspection tasks of offshore infrastructure.","However, current inspection missions rely on predefined plans created offline, hampering the flexibility and autonomy of the inspection vehicle and the mission's success in case of unexpected events.","In this work, we address these challenges by proposing a framework encompassing the modeling and verification of mission plans through Behavior Trees (BTs).","This framework leverages the modularity of BTs to model onboard reactive behaviors, thus enabling autonomous plan executions, and uses BehaVerify to verify the mission's safety.","Moreover, as a use case of this framework, we present a novel AI-enabled algorithm that aims for efficient, autonomous pipeline camera data collection.","In a simulated environment, we demonstrate the framework's application to our proposed pipeline inspection algorithm.","Our framework marks a significant step forward in the field of autonomous underwater robotics, promising to enhance the safety and success of underwater missions in practical, real-world applications.","https://github.com/remaro-network/pipe_inspection_mission"],"url":"http://arxiv.org/abs/2402.04045v1"}
{"created":"2024-02-06 14:26:22","title":"HEAM : Hashed Embedding Acceleration using Processing-In-Memory","abstract":"In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized recommendation systems. Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is utilized-a technique aimed at reducing the size of embedding tables. The architecture is organized into a three-tier memory hierarchy consisting of conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory (PIM), and a bank group-level PIM incorporating a Look-Up-Table. This setup is specifically designed to accommodate the unique aspects of compositional embedding, such as temporal locality and embedding table capacity. This design effectively reduces bank access, improves access efficiency, and enhances overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings compared to the baseline.","sentences":["In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations.","Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth.","However, these solutions fall short when dealing with the expanding size of personalized recommendation systems.","Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers.","Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources.","This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is utilized-a technique aimed at reducing the size of embedding tables.","The architecture is organized into a three-tier memory hierarchy consisting of conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory (PIM), and a bank group-level PIM incorporating a Look-Up-Table.","This setup is specifically designed to accommodate the unique aspects of compositional embedding, such as temporal locality and embedding table capacity.","This design effectively reduces bank access, improves access efficiency, and enhances overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings compared to the baseline."],"url":"http://arxiv.org/abs/2402.04032v1"}
{"created":"2024-02-06 14:26:02","title":"Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation","abstract":"This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm.","sentences":["This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps.","Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images.","By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data).","Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models.","The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm."],"url":"http://arxiv.org/abs/2402.04031v1"}
{"created":"2024-02-06 14:25:09","title":"Reducing the Cost of Quantum Chemical Data By Backpropagating Through Density Functional Theory","abstract":"Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\\text{electrons}}^3)$. Sch\\\"utt et al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN). Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels. For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week. DFT labels molecules by minimizing energy $E(\\cdot )$ as a \"loss function.\" We bypass dataset creation by directly training NNs with $E(\\cdot )$ as a loss function. For comparison, Sch\\\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h.","sentences":["Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\\text{electrons}}^3)$. Sch\\\"utt et al.","(2019) successfully approximate DFT 1000x faster with Neural Networks (NN).","Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels.","For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week.","DFT labels molecules by minimizing energy $E(\\cdot )$ as a \"loss function.\"","We bypass dataset creation by directly training NNs with $E(\\cdot )$ as a loss function.","For comparison, Sch\\\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h."],"url":"http://arxiv.org/abs/2402.04030v1"}
{"created":"2024-02-06 14:24:28","title":"AlbNews: A Corpus of Headlines for Topic Modeling in Albanian","abstract":"The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.","sentences":["The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks.","This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian.","The data can be freely used for conducting topic modeling research.","We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples.","These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments."],"url":"http://arxiv.org/abs/2402.04028v1"}
{"created":"2024-02-06 14:06:23","title":"Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses","abstract":"Model Inversion (MI) attacks aim to disclose private information about the training data by abusing access to the pre-trained models. These attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training data, which has raised significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this field and presents a holistic survey. Firstly, our work briefly reviews the traditional MI on machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on \\textbf{D}eep \\textbf{N}eural \\textbf{N}etworks (DNNs) across multiple modalities and learning tasks.","sentences":["Model Inversion (MI) attacks aim to disclose private information about the training data by abusing access to the pre-trained models.","These attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training data, which has raised significant privacy concerns.","Despite the rapid advances in the field, we lack a comprehensive overview of existing MI attacks and defenses.","To fill this gap, this paper thoroughly investigates this field and presents a holistic survey.","Firstly, our work briefly reviews the traditional MI on machine learning scenarios.","We then elaborately analyze and compare numerous recent attacks and defenses on \\textbf{D}eep \\textbf{N}eural \\textbf{N}etworks (DNNs) across multiple modalities and learning tasks."],"url":"http://arxiv.org/abs/2402.04013v1"}
{"created":"2024-02-06 14:05:05","title":"Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously","abstract":"Availability attacks can prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and making unlearnable examples before release. Ideally, the obtained unlearnability prevents algorithms from training usable models. When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection. Through evaluation, we have found that most of the existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection. Different from recent methods based on contrastive error minimization, we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks to obtain attacks effective for both SL and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in real-world applications.","sentences":["Availability attacks can prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and making unlearnable examples before release.","Ideally, the obtained unlearnability prevents algorithms from training usable models.","When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection.","Through evaluation, we have found that most of the existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection.","Different from recent methods based on contrastive error minimization, we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks to obtain attacks effective for both SL and CL.","Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in real-world applications."],"url":"http://arxiv.org/abs/2402.04010v1"}
{"created":"2024-02-06 13:59:56","title":"Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought","abstract":"During both pretraining and fine-tuning, Large Language Models (\\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality'' or \\textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \\textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \\textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained models both prompted and fine-tuned on noised datasets with varying levels of dataset contamination and intensity. We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise. In contrast, few-shot prompted models appear more sensitive to even static noise. We conclude with a discussion of how our findings impact noise filtering best-practices, in particular emphasizing the importance of removing samples containing destructive dynamic noise with global errors.","sentences":["During both pretraining and fine-tuning, Large Language Models (\\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality.","Both phases of training typically involve heuristically filtering out ``low-quality'' or \\textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance.","In this work, we study how noise in chain of thought (\\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks.","First, we develop the Traced Integer (\\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers.","We then define two types of noise: \\textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \\textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed.","We then evaluate the test performance of pretrained models both prompted and fine-tuned on noised datasets with varying levels of dataset contamination and intensity.","We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise.","In contrast, few-shot prompted models appear more sensitive to even static noise.","We conclude with a discussion of how our findings impact noise filtering best-practices, in particular emphasizing the importance of removing samples containing destructive dynamic noise with global errors."],"url":"http://arxiv.org/abs/2402.04004v1"}
{"created":"2024-02-06 13:47:12","title":"Gradient Sketches for Training Data Attribution and Studying the Loss Landscape","abstract":"Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products. While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-trained language models.","sentences":["Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry.","Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products.","While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks.","Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms.","We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-trained language models."],"url":"http://arxiv.org/abs/2402.03994v1"}
{"created":"2024-02-06 13:24:36","title":"Tail-Erasure-Correcting Codes","abstract":"The increasing demand for data storage has prompted the exploration of new techniques, with molecular data storage being a promising alternative. In this work, we develop coding schemes for a new storage paradigm that can be represented as a collection of two-dimensional arrays. Motivated by error patterns observed in recent prototype architectures, our study focuses on correcting erasures in the last few symbols of each row, and also correcting arbitrary deletions across rows. We present code constructions and explicit encoders and decoders that are shown to be nearly optimal in many scenarios. We show that the new coding schemes are capable of effectively mitigating these errors, making these emerging storage platforms potentially promising solutions.","sentences":["The increasing demand for data storage has prompted the exploration of new techniques, with molecular data storage being a promising alternative.","In this work, we develop coding schemes for a new storage paradigm that can be represented as a collection of two-dimensional arrays.","Motivated by error patterns observed in recent prototype architectures, our study focuses on correcting erasures in the last few symbols of each row, and also correcting arbitrary deletions across rows.","We present code constructions and explicit encoders and decoders that are shown to be nearly optimal in many scenarios.","We show that the new coding schemes are capable of effectively mitigating these errors, making these emerging storage platforms potentially promising solutions."],"url":"http://arxiv.org/abs/2402.03987v1"}
{"created":"2024-02-06 13:16:54","title":"Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting","abstract":"In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities. Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents' stochastic behavior bounded by scene constraints. However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision. These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints. To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a Transformer-based conditional denoising diffusion model to guide the prediction of future trajectories. To ensure multimodality, we incorporate behavioral tokens to direct the trajectory's modes, such as going straight, turning right or left. Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories. Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings.","sentences":["In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities.","Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents' stochastic behavior bounded by scene constraints.","However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision.","These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints.","To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a Transformer-based conditional denoising diffusion model to guide the prediction of future trajectories.","To ensure multimodality, we incorporate behavioral tokens to direct the trajectory's modes, such as going straight, turning right or left.","Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories.","Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings."],"url":"http://arxiv.org/abs/2402.03981v1"}
{"created":"2024-02-06 12:59:02","title":"Tabular Data: Is Attention All You Need?","abstract":"Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.","sentences":["Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data.","Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data.","In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections.","In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees.","Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets.","As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications."],"url":"http://arxiv.org/abs/2402.03970v1"}
{"created":"2024-02-06 12:20:35","title":"Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding","abstract":"This work contributes a novel deep navigation policy that enables collision-free flight of aerial robots based on a modular approach exploiting deep collision encoding and reinforcement learning. The proposed solution builds upon a deep collision encoder that is trained on both simulated and real depth images using supervised learning such that it compresses the high-dimensional depth data to a low-dimensional latent space encoding collision information while accounting for the robot size. This compressed encoding is combined with an estimate of the robot's odometry and the desired target location to train a deep reinforcement learning navigation policy that offers low-latency computation and robust sim2real performance. A set of simulation and experimental studies in diverse environments are conducted and demonstrate the efficiency of the emerged behavior and its resilience in real-life deployments.","sentences":["This work contributes a novel deep navigation policy that enables collision-free flight of aerial robots based on a modular approach exploiting deep collision encoding and reinforcement learning.","The proposed solution builds upon a deep collision encoder that is trained on both simulated and real depth images using supervised learning such that it compresses the high-dimensional depth data to a low-dimensional latent space encoding collision information while accounting for the robot size.","This compressed encoding is combined with an estimate of the robot's odometry and the desired target location to train a deep reinforcement learning navigation policy that offers low-latency computation and robust sim2real performance.","A set of simulation and experimental studies in diverse environments are conducted and demonstrate the efficiency of the emerged behavior and its resilience in real-life deployments."],"url":"http://arxiv.org/abs/2402.03947v1"}
{"created":"2024-02-06 12:19:46","title":"Using metaheuristics for the location of bicycle stations","abstract":"In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles. To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization. The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station). We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages. We use irace to parameterize each algorithm automatically, to contribute with a methodology to fine-tune algorithms automatically. We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application. We have compared our results with the implemented solution in Malaga. Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations.","sentences":["In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles.","To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization.","The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station).","We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages.","We use irace to parameterize each algorithm automatically, to contribute with a methodology to fine-tune algorithms automatically.","We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application.","We have compared our results with the implemented solution in Malaga.","Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations."],"url":"http://arxiv.org/abs/2402.03945v1"}
{"created":"2024-02-06 12:18:54","title":"Discovery of the Hidden World with Large Language Models","abstract":"Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a causal learning module (e.g., the FCI algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by LLMs. We verify the effectiveness of COAT in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis.","sentences":["Science originates with discovering new causal knowledge from a combination of known facts and observations.","Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations.","However, the causal variables are usually unavailable in a wide range of real-world applications.","The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data.","Therefore, we introduce COAT:","Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data.","Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data.","The annotated data will be fed to a causal learning module (e.g., the FCI algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by LLMs.","We verify the effectiveness of COAT in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis."],"url":"http://arxiv.org/abs/2402.03941v1"}
{"created":"2024-02-06 11:54:23","title":"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs","abstract":"Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been globally exposed to $\\sim$4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.","sentences":["Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source.","The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers.","Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error.","Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users.","In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination.","By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release.","We report that these models have been globally exposed to $\\sim$4.7M samples from 263 benchmarks.","At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues.","We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts."],"url":"http://arxiv.org/abs/2402.03927v1"}
{"created":"2024-02-06 11:44:50","title":"Competitive advantage of URLLC vs. eMBB for supporting timeliness-relevant services","abstract":"5G specifications promise a common and flexible-enough network infrastructure capable of satisfying diverse requirements of both current and future use cases. Two service types standardized in 5G are eMBB, without stringent delay guarantee, and URLLC, with stringent delay guarantee. We focus on a use case where data timeliness is the relevant quality parameter. We provide an economic rationale for the support of data-based services, that is, from the point of view of the profits attained by the service providers and operators (SP). More specifically, we focus on data-based services the quality of which is related to the Age of Information, and we assess two alternatives for the support of this sort of services by means of a 5G network: one that is based on the eMBB service type, and one that is based on the URLLC service type. These assessment is conducted in a duopoly scenario. We conclude that URLLC support provides a competitive advantage to an SP against a competitor SP that supports its service offering on eMBB. And that there is a slightly better situation for the users when the URLLC QoS constraint is stringent.","sentences":["5G specifications promise a common and flexible-enough network infrastructure capable of satisfying diverse requirements of both current and future use cases.","Two service types standardized in 5G are eMBB, without stringent delay guarantee, and URLLC, with stringent delay guarantee.","We focus on a use case where data timeliness is the relevant quality parameter.","We provide an economic rationale for the support of data-based services, that is, from the point of view of the profits attained by the service providers and operators (SP).","More specifically, we focus on data-based services the quality of which is related to the Age of Information, and we assess two alternatives for the support of this sort of services by means of a 5G network: one that is based on the eMBB service type, and one that is based on the URLLC service type.","These assessment is conducted in a duopoly scenario.","We conclude that URLLC support provides a competitive advantage to an SP against a competitor SP that supports its service offering on eMBB.","And that there is a slightly better situation for the users when the URLLC QoS constraint is stringent."],"url":"http://arxiv.org/abs/2402.03922v1"}
{"created":"2024-02-06 11:35:02","title":"Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning","abstract":"Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prototypes used in a novel asymmetric cross entropy loss which effectively balances prototype rehearsal with data from new tasks. Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to learn new tasks by maintaining model plasticity and significantly outperform the state-of-the-art.","sentences":["Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data.","In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone.","This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting.","To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias.","Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM).","The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prototypes used in a novel asymmetric cross entropy loss which effectively balances prototype rehearsal with data from new tasks.","Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to learn new tasks by maintaining model plasticity and significantly outperform the state-of-the-art."],"url":"http://arxiv.org/abs/2402.03917v1"}
{"created":"2024-02-06 11:33:57","title":"Can Large Language Models Detect Rumors on Social Media?","abstract":"In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.","sentences":["In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media.","However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information.","Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden.","We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%.","Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios."],"url":"http://arxiv.org/abs/2402.03916v1"}
{"created":"2024-02-06 11:22:39","title":"Understanding Trends, Patterns, and Dynamics in Global Acquisitions: A Network Perspective","abstract":"Studying acquisitions offers invaluable insights into startup trends, aiding informed investment decisions for businesses. However, the scarcity of studies in this domain prompts our focus on shedding light in this area. Employing Crunchbase data, our study delves into the global network of company acquisitions using diverse network analysis techniques. Our findings unveil an acquisition network characterized by a primarily sparse structure comprising localized dense connections. We reveal a prevalent tendency among organizations to acquire companies within their own country and industry. Furthermore, our temporal analysis indicates a growth in network communities over time, accompanied by a trend toward a sparser network. Through centrality metrics computation in the cross-city acquisition network, we identify New York, London, and San Francisco as pivotal and central hubs in the global economic landscape. Finally, we show that the United States, United Kingdom, and Germany are predominant countries in international acquisitions.","sentences":["Studying acquisitions offers invaluable insights into startup trends, aiding informed investment decisions for businesses.","However, the scarcity of studies in this domain prompts our focus on shedding light in this area.","Employing Crunchbase data, our study delves into the global network of company acquisitions using diverse network analysis techniques.","Our findings unveil an acquisition network characterized by a primarily sparse structure comprising localized dense connections.","We reveal a prevalent tendency among organizations to acquire companies within their own country and industry.","Furthermore, our temporal analysis indicates a growth in network communities over time, accompanied by a trend toward a sparser network.","Through centrality metrics computation in the cross-city acquisition network, we identify New York, London, and San Francisco as pivotal and central hubs in the global economic landscape.","Finally, we show that the United States, United Kingdom, and Germany are predominant countries in international acquisitions."],"url":"http://arxiv.org/abs/2402.03910v1"}
{"created":"2024-02-06 11:19:40","title":"Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy","abstract":"Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive. While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques. In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes. We argue that such inclusion will facilitate diversity for XR use. In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life. Lastly, we speculate that combining the information provided to LLM-powered environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions. While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated. In summary, despite some challenges, embedding LLMs into XR is a promising and novel research area with several opportunities.","sentences":["Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive.","While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques.","In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes.","We argue that such inclusion will facilitate diversity for XR use.","In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life.","Lastly, we speculate that combining the information provided to LLM-powered environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions.","While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated.","In summary, despite some challenges, embedding LLMs into XR is a promising and novel research area with several opportunities."],"url":"http://arxiv.org/abs/2402.03907v1"}
{"created":"2024-02-06 11:13:54","title":"A phase transition between positional and semantic learning in a solvable model of dot-product attention","abstract":"We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product attention layer to linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data.","sentences":["We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning).","For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism.","On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices.","In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape.","We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity.","Finally, we compare the dot-product attention layer to linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data."],"url":"http://arxiv.org/abs/2402.03902v1"}
{"created":"2024-02-06 11:10:32","title":"Robust Data-EnablEd Predictive Leading Cruise Control via Reachability Analysis","abstract":"Data-driven predictive control promises modelfree wave-dampening strategies for Connected and Autonomous Vehicles (CAVs) in mixed traffic flow. However, the performance suffers from unknown noise and disturbances, which could occur in offline data collection and online predictive control. In this paper, we propose a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC) method based on reachability analysis, aiming to achieve safe and optimal control of CAVs under bounded process noise and external disturbances. Precisely, we decouple the mixed platoon system into an error system and a nominal system, and tighten the constraint via the data-driven reachable set technique. Then, the enhanced safety constraint is integrated with the data-driven predictive control formulation to achieve stronger robust control performance for CAVs. Simulations validate the effectiveness of the proposed method in mitigating traffic waves with better robustness.","sentences":["Data-driven predictive control promises modelfree wave-dampening strategies for Connected and Autonomous Vehicles (CAVs) in mixed traffic flow.","However, the performance suffers from unknown noise and disturbances, which could occur in offline data collection and online predictive control.","In this paper, we propose a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC) method based on reachability analysis, aiming to achieve safe and optimal control of CAVs under bounded process noise and external disturbances.","Precisely, we decouple the mixed platoon system into an error system and a nominal system, and tighten the constraint via the data-driven reachable set technique.","Then, the enhanced safety constraint is integrated with the data-driven predictive control formulation to achieve stronger robust control performance for CAVs.","Simulations validate the effectiveness of the proposed method in mitigating traffic waves with better robustness."],"url":"http://arxiv.org/abs/2402.03897v1"}
{"created":"2024-02-06 10:48:59","title":"Full-Duplex Millimeter Wave MIMO Channel Estimation: A Neural Network Approach","abstract":"Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality with great potential for further improvement. We study full-duplex transmissions as an effective way to improve mmWave MIMO systems. Compared to half-duplex systems, full-duplex transmissions may offer higher data rates and lower latency. However, full-duplex transmission is hindered by self-interference (SI) at the receive antennas, and SI channel estimation becomes a crucial step to make the full-duplex systems feasible. In this paper, we address the problem of channel estimation in full-duplex mmWave MIMO systems using neural networks (NNs). Our approach involves sharing pilot resources between user equipments (UEs) and transmit antennas at the base station (BS), aiming to reduce the pilot overhead in full-duplex systems and to achieve a comparable level to that of a half-duplex system. Additionally, in the case of separate antenna configurations in a full-duplex BS, providing channel estimates of transmit antenna (TX) arrays to the downlink UEs poses another challenge, as the TX arrays are not capable of receiving pilot signals. To address this, we employ an NN to map the channel from the downlink UEs to the receive antenna (RX) arrays to the channel from the TX arrays to the downlink UEs. We further elaborate on how NNs perform the estimation with different architectures, (e.g., different numbers of hidden layers), the introduction of non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)), and different channel conditions (e.g., low-correlated and high-correlated channels). Our work provides novel insights into NN-based channel estimators.","sentences":["Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality with great potential for further improvement.","We study full-duplex transmissions as an effective way to improve mmWave MIMO systems.","Compared to half-duplex systems, full-duplex transmissions may offer higher data rates and lower latency.","However, full-duplex transmission is hindered by self-interference (SI) at the receive antennas, and SI channel estimation becomes a crucial step to make the full-duplex systems feasible.","In this paper, we address the problem of channel estimation in full-duplex mmWave MIMO systems using neural networks (NNs).","Our approach involves sharing pilot resources between user equipments (UEs) and transmit antennas at the base station (BS), aiming to reduce the pilot overhead in full-duplex systems and to achieve a comparable level to that of a half-duplex system.","Additionally, in the case of separate antenna configurations in a full-duplex BS, providing channel estimates of transmit antenna (TX) arrays to the downlink UEs poses another challenge, as the TX arrays are not capable of receiving pilot signals.","To address this, we employ an NN to map the channel from the downlink UEs to the receive antenna (RX) arrays to the channel from the TX arrays to the downlink UEs.","We further elaborate on how NNs perform the estimation with different architectures, (e.g., different numbers of hidden layers), the introduction of non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)), and different channel conditions (e.g., low-correlated and high-correlated channels).","Our work provides novel insights into NN-based channel estimators."],"url":"http://arxiv.org/abs/2402.03886v1"}
{"created":"2024-02-06 10:48:46","title":"MOMENT: A Family of Open Time-series Foundation Models","abstract":"We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time-series models. Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.","sentences":["We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis.","Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous.","Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages.","To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training.","Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings.","Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning.","Finally, we present several interesting empirical observations about large pre-trained time-series models.","Our code is available anonymously at anonymous.4open.science/r/BETT-773F/."],"url":"http://arxiv.org/abs/2402.03885v1"}
{"created":"2024-02-06 10:28:07","title":"Binaural sound source localization using a hybrid time and frequency domain model","abstract":"This paper introduces a new approach to sound source localization using head-related transfer function (HRTF) characteristics, which enable precise full-sphere localization from raw data. While previous research focused primarily on using extensive microphone arrays in the frontal plane, this arrangement often encountered limitations in accuracy and robustness when dealing with smaller microphone arrays. Our model proposes using both time and frequency domain for sound source localization while utilizing Deep Learning (DL) approach. The performance of our proposed model, surpasses the current state-of-the-art results. Specifically, it boasts an average angular error of $0.24 degrees and an average Euclidean distance of 0.01 meters, while the known state-of-the-art gives average angular error of 19.07 degrees and average Euclidean distance of 1.08 meters. This level of accuracy is of paramount importance for a wide range of applications, including robotics, virtual reality, and aiding individuals with cochlear implants (CI).","sentences":["This paper introduces a new approach to sound source localization using head-related transfer function (HRTF) characteristics, which enable precise full-sphere localization from raw data.","While previous research focused primarily on using extensive microphone arrays in the frontal plane, this arrangement often encountered limitations in accuracy and robustness when dealing with smaller microphone arrays.","Our model proposes using both time and frequency domain for sound source localization while utilizing Deep Learning (DL) approach.","The performance of our proposed model, surpasses the current state-of-the-art results.","Specifically, it boasts an average angular error of $0.24 degrees and an average Euclidean distance of 0.01 meters, while the known state-of-the-art gives average angular error of 19.07 degrees and average Euclidean distance of 1.08 meters.","This level of accuracy is of paramount importance for a wide range of applications, including robotics, virtual reality, and aiding individuals with cochlear implants (CI)."],"url":"http://arxiv.org/abs/2402.03867v1"}
{"created":"2024-02-06 09:50:15","title":"Global certification via perfect hashing","abstract":"In this work, we provide an upper bound for global certification of graph homomorphism, a generalization of graph coloring. In certification, the nodes of a network should decide if the network satisfies a given property, thanks to small pieces of information called certificates. Here, there is only one global certificate which is shared by all the nodes, and the property we want to certify is the existence of a graph homomorphism to a given graph.   For bipartiteness, a special case of graph homomorphism, Feuilloley and Hirvonen proved in~\\cite{FeuilloleyH18} some upper and lower bounds on the size of the optimal certificate, and made the conjecture that their lower bound could be improved to match their upper bound. We prove that this conjecture is false: their lower bound was in fact optimal, and we prove it by providing the matching upper bound using a known result of perfect hashing.","sentences":["In this work, we provide an upper bound for global certification of graph homomorphism, a generalization of graph coloring.","In certification, the nodes of a network should decide if the network satisfies a given property, thanks to small pieces of information called certificates.","Here, there is only one global certificate which is shared by all the nodes, and the property we want to certify is the existence of a graph homomorphism to a given graph.   ","For bipartiteness, a special case of graph homomorphism, Feuilloley and Hirvonen proved in~\\cite{FeuilloleyH18} some upper and lower bounds on the size of the optimal certificate, and made the conjecture that their lower bound could be improved to match their upper bound.","We prove that this conjecture is false: their lower bound was in fact optimal, and we prove it by providing the matching upper bound using a known result of perfect hashing."],"url":"http://arxiv.org/abs/2402.03849v1"}
{"created":"2024-02-06 09:41:43","title":"On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models","abstract":"Diffusion models are generative models that have recently demonstrated impressive performances in terms of sampling quality and density estimation in high dimensions. They rely on a forward continuous diffusion process and a backward continuous denoising process, which can be described by a time-dependent vector field and is used as a generative model. In the original formulation of the diffusion model, this vector field is assumed to be the score function (i.e. it is the gradient of the log-probability at a given time in the diffusion process). Curiously, on the practical side, most studies on diffusion models implement this vector field as a neural network function and do not constrain it be the gradient of some energy function (that is, most studies do not constrain the vector field to be conservative). Even though some studies investigated empirically whether such a constraint will lead to a performance gain, they lead to contradicting results and failed to provide analytical results. Here, we provide three analytical results regarding the extent of the modeling freedom of this vector field. {Firstly, we propose a novel decomposition of vector fields into a conservative component and an orthogonal component which satisfies a given (gauge) freedom. Secondly, from this orthogonal decomposition, we show that exact density estimation and exact sampling is achieved when the conservative component is exactly equals to the true score and therefore conservativity is neither necessary nor sufficient to obtain exact density estimation and exact sampling. Finally, we show that when it comes to inferring local information of the data manifold, constraining the vector field to be conservative is desirable.","sentences":["Diffusion models are generative models that have recently demonstrated impressive performances in terms of sampling quality and density estimation in high dimensions.","They rely on a forward continuous diffusion process and a backward continuous denoising process, which can be described by a time-dependent vector field and is used as a generative model.","In the original formulation of the diffusion model, this vector field is assumed to be the score function (i.e. it is the gradient of the log-probability at a given time in the diffusion process).","Curiously, on the practical side, most studies on diffusion models implement this vector field as a neural network function and do not constrain it be the gradient of some energy function (that is, most studies do not constrain the vector field to be conservative).","Even though some studies investigated empirically whether such a constraint will lead to a performance gain, they lead to contradicting results and failed to provide analytical results.","Here, we provide three analytical results regarding the extent of the modeling freedom of this vector field.","{Firstly, we propose a novel decomposition of vector fields into a conservative component and an orthogonal component which satisfies a given (gauge) freedom.","Secondly, from this orthogonal decomposition, we show that exact density estimation and exact sampling is achieved when the conservative component is exactly equals to the true score and therefore conservativity is neither necessary nor sufficient to obtain exact density estimation and exact sampling.","Finally, we show that when it comes to inferring local information of the data manifold, constraining the vector field to be conservative is desirable."],"url":"http://arxiv.org/abs/2402.03845v1"}
{"created":"2024-02-06 09:37:42","title":"Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation","abstract":"In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information. We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant for a robotic mission. We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data. A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs. As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces. The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method. We establish the novel concept of \\textit{Belief Scene Graphs} (BSG), as a core component to integrate expectations into abstract representations. This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for the task planning and optimization of a variety of robotics missions. The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested on a real-life experiment to emulate human common sense of unseen-objects.","sentences":["In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information.","We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant for a robotic mission.","We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data.","A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs.","As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces.","The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method.","We establish the novel concept of \\textit{Belief Scene Graphs} (BSG), as a core component to integrate expectations into abstract representations.","This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for the task planning and optimization of a variety of robotics missions.","The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested on a real-life experiment to emulate human common sense of unseen-objects."],"url":"http://arxiv.org/abs/2402.03840v1"}
{"created":"2024-02-06 09:26:46","title":"Enhanced Security and Efficiency in Blockchain with Aggregated Zero-Knowledge Proof Mechanisms","abstract":"Blockchain technology has emerged as a revolutionary tool in ensuring data integrity and security in digital transactions. However, the current approaches to data verification in blockchain systems, particularly in Ethereum, face challenges in terms of efficiency and computational overhead. The traditional use of Merkle Trees and cryptographic hash functions, while effective, leads to significant resource consumption, especially for large datasets. This highlights a gap in existing research: the need for more efficient methods of data verification in blockchain networks. Our study addresses this gap by proposing an innovative aggregation scheme for Zero-Knowledge Proofs within the structure of Merkle Trees. We develop a system that significantly reduces the size of the proof and the computational resources needed for its generation and verification. Our approach represents a paradigm shift in blockchain data verification, balancing security with efficiency. We conducted extensive experimental evaluations using real Ethereum block data to validate the effectiveness of our proposed scheme. The results demonstrate a drastic reduction in proof size and computational requirements compared to traditional methods, making the verification process more efficient and economically viable. Our contribution fills a critical research void, offering a scalable and secure solution for blockchain data verification. The implications of our work are far-reaching, enhancing the overall performance and adaptability of blockchain technology in various applications, from financial transactions to supply chain management.","sentences":["Blockchain technology has emerged as a revolutionary tool in ensuring data integrity and security in digital transactions.","However, the current approaches to data verification in blockchain systems, particularly in Ethereum, face challenges in terms of efficiency and computational overhead.","The traditional use of Merkle Trees and cryptographic hash functions, while effective, leads to significant resource consumption, especially for large datasets.","This highlights a gap in existing research: the need for more efficient methods of data verification in blockchain networks.","Our study addresses this gap by proposing an innovative aggregation scheme for Zero-Knowledge Proofs within the structure of Merkle Trees.","We develop a system that significantly reduces the size of the proof and the computational resources needed for its generation and verification.","Our approach represents a paradigm shift in blockchain data verification, balancing security with efficiency.","We conducted extensive experimental evaluations using real Ethereum block data to validate the effectiveness of our proposed scheme.","The results demonstrate a drastic reduction in proof size and computational requirements compared to traditional methods, making the verification process more efficient and economically viable.","Our contribution fills a critical research void, offering a scalable and secure solution for blockchain data verification.","The implications of our work are far-reaching, enhancing the overall performance and adaptability of blockchain technology in various applications, from financial transactions to supply chain management."],"url":"http://arxiv.org/abs/2402.03834v1"}
{"created":"2024-02-06 09:23:26","title":"Rethinking Skill Extraction in the Job Market Domain using Large Language Models","abstract":"Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes. The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags. However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions. In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets. Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences. We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.","sentences":["Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes.","The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags.","However, the reliance on manually annotated data limits the generalizability of such approaches.","Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions.","In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets.","Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences.","We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks."],"url":"http://arxiv.org/abs/2402.03832v1"}
{"created":"2024-02-06 09:19:44","title":"OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving","abstract":"With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency. The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years. Data plays a core role in the algorithm closed-loop system. However, collecting real-world data is expensive, time-consuming, and unsafe. With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering. It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology. (2) Trajectory editing of the ego vehicle and participating vehicles. (3) Rich vehicle model library that can be freely selected and inserted into the scene. (4) Rich sensors model library where you can select specified sensors to generate data. (5) A highly customizable data generation system can generate data according to user needs. We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition. Code is available at https://github.com/PJLab-ADG/OASim.","sentences":["With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency.","The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years.","Data plays a core role in the algorithm closed-loop system.","However, collecting real-world data is expensive, time-consuming, and unsafe.","With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering.","It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology.","(2) Trajectory editing of the ego vehicle and participating vehicles.","(3) Rich vehicle model library that can be freely selected and inserted into the scene.","(4) Rich sensors model library where you can select specified sensors to generate data.","(5) A highly customizable data generation system can generate data according to user needs.","We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition.","Code is available at https://github.com/PJLab-ADG/OASim."],"url":"http://arxiv.org/abs/2402.03830v1"}
{"created":"2024-02-06 09:17:07","title":"Estimating Barycenters of Distributions with Neural Optimal Transport","abstract":"Given a collection of probability measures, a practitioner sometimes needs to find an \"average\" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.","sentences":["Given a collection of probability measures, a practitioner sometimes needs to find an \"average\" distribution which adequately aggregates reference distributions.","A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work.","By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem.","Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions.","These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost.","We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups."],"url":"http://arxiv.org/abs/2402.03828v1"}
{"created":"2024-02-06 09:07:26","title":"Asymptotic generalization error of a single-layer graph convolutional network","abstract":"While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.","sentences":["While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks.","In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit.","Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM.","We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases."],"url":"http://arxiv.org/abs/2402.03818v1"}
{"created":"2024-02-06 09:00:05","title":"Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression","abstract":"Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy. To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet. To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients. The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS. To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating. In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates. In the latter phase, clients upload global significant model updates to the PS for aggregation. FediAC consumes much less memory space and communication traffic than existing works because the first phase can guarantee consensus compression across clients. The PS easily aligns model update indices to swiftly complete aggregation in the second phase. Finally, we conduct extensive experiments by using public datasets to demonstrate that FediAC remarkably surpasses the state-of-the-art baselines in terms of model accuracy and communication traffic.","sentences":["Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy.","To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet.","To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients.","The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS.","To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating.","In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates.","In the latter phase, clients upload global significant model updates to the PS for aggregation.","FediAC consumes much less memory space and communication traffic than existing works because the first phase can guarantee consensus compression across clients.","The PS easily aligns model update indices to swiftly complete aggregation in the second phase.","Finally, we conduct extensive experiments by using public datasets to demonstrate that FediAC remarkably surpasses the state-of-the-art baselines in terms of model accuracy and communication traffic."],"url":"http://arxiv.org/abs/2402.03815v1"}
{"created":"2024-02-06 08:57:12","title":"FDO Manager: Minimum Viable FAIR Digital Object Implementation","abstract":"The concept of FAIR Digital Objects (FDOs) aims to revolutionise the field of digital preservation and accessibility in the next few years. Central to this revolution is the alignment of FDOs with the FAIR (Findable, Accessible, Interoperable, Reusable) Principles, particularly emphasizing machine-actionability and interoperability across diverse data ecosystems. This abstract introduces the \"FDO Manager\", a Minimum Viable Implementation, designed to optimize the management of FDOs following these principles and the FDO specifications. The FDO Manager is tailored to manage research artefacts such as datasets, codes, and publications, to foster increased transparency and reproducibility in research. The abstract presents the implementation details of the FDO Manager, its underlying architecture, and the metadata schemas it employs, thereby offering a clear and comprehensive understanding of its functionalities and impact on the research domain.","sentences":["The concept of FAIR Digital Objects (FDOs) aims to revolutionise the field of digital preservation and accessibility in the next few years.","Central to this revolution is the alignment of FDOs with the FAIR (Findable, Accessible, Interoperable, Reusable) Principles, particularly emphasizing machine-actionability and interoperability across diverse data ecosystems.","This abstract introduces the \"FDO Manager\", a Minimum Viable Implementation, designed to optimize the management of FDOs following these principles and the FDO specifications.","The FDO Manager is tailored to manage research artefacts such as datasets, codes, and publications, to foster increased transparency and reproducibility in research.","The abstract presents the implementation details of the FDO Manager, its underlying architecture, and the metadata schemas it employs, thereby offering a clear and comprehensive understanding of its functionalities and impact on the research domain."],"url":"http://arxiv.org/abs/2402.03812v1"}
{"created":"2024-02-06 08:48:01","title":"SEABO: A Simple Search-Based Method for Offline Imitation Learning","abstract":"Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL algorithms with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and offline IL methods across many tasks. Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations. Our code is publicly available at https://github.com/dmksjfl/SEABO.","sentences":["Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment.","Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels.","In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient.","To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data.","To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO.","SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner.","Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL algorithms with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and offline IL methods across many tasks.","Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations.","Our code is publicly available at https://github.com/dmksjfl/SEABO."],"url":"http://arxiv.org/abs/2402.03807v1"}
{"created":"2024-02-06 08:14:56","title":"Adaptive Blockwise Task-interleaved Pipeline Parallelism","abstract":"Efficient distributed training serves as a powerful catalyst and an essential foundation for the development of large-scale neural networks. In distributed training scenarios, various pipeline parallelism methods are cleverly designed and widely employed. In this paper, we propose ZeroPP, a highly efficient and flexible pipeline parallelism method that trades off pipeline bubbles, memory usage, and communication through adaptive scheduling units. ZeroPP achieves minimal pipeline bubbles by carefully staggering the computation tasks of forward, input gradient, and weight gradient within a scheduling unit. Additionally, ZeroPP optimizes the combination of pipeline parallelism and fully sharded data parallelism using a blockwise schedule. We conduct experiments with popular GPT-style models and observe up to a 30% increase in throughput compared to the state-of-the-art breath-first pipeline parallelism. Besides, our evaluation also demonstrates up to a 68% increase in throughput and a 10% reduction in memory consumption compared to the memory-efficient 1F1B method.","sentences":["Efficient distributed training serves as a powerful catalyst and an essential foundation for the development of large-scale neural networks.","In distributed training scenarios, various pipeline parallelism methods are cleverly designed and widely employed.","In this paper, we propose ZeroPP, a highly efficient and flexible pipeline parallelism method that trades off pipeline bubbles, memory usage, and communication through adaptive scheduling units.","ZeroPP achieves minimal pipeline bubbles by carefully staggering the computation tasks of forward, input gradient, and weight gradient within a scheduling unit.","Additionally, ZeroPP optimizes the combination of pipeline parallelism and fully sharded data parallelism using a blockwise schedule.","We conduct experiments with popular GPT-style models and observe up to a 30% increase in throughput compared to the state-of-the-art breath-first pipeline parallelism.","Besides, our evaluation also demonstrates up to a 68% increase in throughput and a 10% reduction in memory consumption compared to the memory-efficient 1F1B method."],"url":"http://arxiv.org/abs/2402.03791v1"}
{"created":"2024-02-06 07:57:13","title":"Weakly Supervised Anomaly Detection via Knowledge-Data Alignment","abstract":"Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types.","sentences":["Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis.","Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels.","Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance.","Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies.","In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data.","Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data.","To facilitate this alignment, we employ the Optimal Transport (OT) technique.","We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies.","Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types."],"url":"http://arxiv.org/abs/2402.03785v1"}
{"created":"2024-02-06 07:55:54","title":"AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction","abstract":"Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships within the air quality data. Experiments on two real-world benchmark datasets demonstrate that AirPhyNet outperforms state-of-the-art models for different testing scenarios including different lead time (24h, 48h, 72h), sparse data and sudden change prediction, achieving reduction in prediction errors up to 10%. Moreover, a case study further validates that our model captures underlying physical processes of particle movement and generates accurate predictions with real physical meaning.","sentences":["Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions.","Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions.","To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet).","Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks.","Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships within the air quality data.","Experiments on two real-world benchmark datasets demonstrate that AirPhyNet outperforms state-of-the-art models for different testing scenarios including different lead time (24h, 48h, 72h), sparse data and sudden change prediction, achieving reduction in prediction errors up to 10%.","Moreover, a case study further validates that our model captures underlying physical processes of particle movement and generates accurate predictions with real physical meaning."],"url":"http://arxiv.org/abs/2402.03784v1"}
{"created":"2024-02-06 07:48:22","title":"Improving Automated Code Reviews: Learning from Experience","abstract":"Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments. This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers. To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would. A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus. However, such techniques did not fully utilise quality reviews amongst the training data. Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others. In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique. Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data. The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies. This work sheds light on resource-efficient ways to boost automated code review models.","sentences":["Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments.","This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers.","To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would.","A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus.","However, such techniques did not fully utilise quality reviews amongst the training data.","Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others.","In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique.","Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data.","The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies.","This work sheds light on resource-efficient ways to boost automated code review models."],"url":"http://arxiv.org/abs/2402.03777v1"}
{"created":"2024-02-06 07:40:53","title":"Learning a Decision Tree Algorithm with Transformers","abstract":"Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.","sentences":["Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data.","Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree.","However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization.","To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification.","Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets.","We then train MetaTree to produce the trees that achieve strong generalization performance.","This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance."],"url":"http://arxiv.org/abs/2402.03774v1"}
{"created":"2024-02-06 07:35:36","title":"Encoding Version History Context for Better Code Representation","abstract":"With the exponential growth of AI tools that generate source code, understanding software has become crucial. When developers comprehend a program, they may refer to additional contexts to look for information, e.g. program documentation or historical code versions. Therefore, we argue that encoding this additional contextual information could also benefit code representation for deep learning. Recent papers incorporate contextual data (e.g. call hierarchy) into vector representation to address program comprehension problems. This motivates further studies to explore additional contexts, such as version history, to enhance models' understanding of programs. That is, insights from version history enable recognition of patterns in code evolution over time, recurring issues, and the effectiveness of past solutions. Our paper presents preliminary evidence of the potential benefit of encoding contextual information from the version history to predict code clones and perform code classification. We experiment with two representative deep learning models, ASTNN and CodeBERT, to investigate whether combining additional contexts with different aggregations may benefit downstream activities. The experimental result affirms the positive impact of combining version history into source code representation in all scenarios; however, to ensure the technique performs consistently, we need to conduct a holistic investigation on a larger code base using different combinations of contexts, aggregation, and models. Therefore, we propose a research agenda aimed at exploring various aspects of encoding additional context to improve code representation and its optimal utilisation in specific situations.","sentences":["With the exponential growth of AI tools that generate source code, understanding software has become crucial.","When developers comprehend a program, they may refer to additional contexts to look for information, e.g. program documentation or historical code versions.","Therefore, we argue that encoding this additional contextual information could also benefit code representation for deep learning.","Recent papers incorporate contextual data (e.g. call hierarchy) into vector representation to address program comprehension problems.","This motivates further studies to explore additional contexts, such as version history, to enhance models' understanding of programs.","That is, insights from version history enable recognition of patterns in code evolution over time, recurring issues, and the effectiveness of past solutions.","Our paper presents preliminary evidence of the potential benefit of encoding contextual information from the version history to predict code clones and perform code classification.","We experiment with two representative deep learning models, ASTNN and CodeBERT, to investigate whether combining additional contexts with different aggregations may benefit downstream activities.","The experimental result affirms the positive impact of combining version history into source code representation in all scenarios; however, to ensure the technique performs consistently, we need to conduct a holistic investigation on a larger code base using different combinations of contexts, aggregation, and models.","Therefore, we propose a research agenda aimed at exploring various aspects of encoding additional context to improve code representation and its optimal utilisation in specific situations."],"url":"http://arxiv.org/abs/2402.03773v1"}
{"created":"2024-02-06 07:25:21","title":"Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes","abstract":"In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates. In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates. We develop optimal tuning strategy that minimizes the loss function (equivalent to maximizing the model utility) subject to the budget for communication. We further demonstrate that Fed-CVLC is indeed a general compression design that bridges quantization and sparsification, with greater flexibility. Extensive experiments have been conducted with public datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art baselines, improving model utility by 1.50%-5.44%, or shrinking communication traffic by 16.67%-41.61%.","sentences":["In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients.","FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck.","Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates.","In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL.","We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates.","We develop optimal tuning strategy that minimizes the loss function (equivalent to maximizing the model utility) subject to the budget for communication.","We further demonstrate that Fed-CVLC is indeed a general compression design that bridges quantization and sparsification, with greater flexibility.","Extensive experiments have been conducted with public datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art baselines, improving model utility by 1.50%-5.44%, or shrinking communication traffic by 16.67%-41.61%."],"url":"http://arxiv.org/abs/2402.03770v1"}
{"created":"2024-02-06 06:42:51","title":"Enhanced sampling of robust molecular datasets with uncertainty-based collective variables","abstract":"Generating a data set that is representative of the accessible configuration space of a molecular system is crucial for the robustness of machine learned interatomic potentials (MLIP). However, the complexity of molecular systems, characterized by intricate potential energy surfaces (PESs) with numerous local minima and energy barriers, presents a significant challenge. Traditional methods of data generation, such as random sampling or exhaustive exploration, are either intractable or may not capture rare, but highly informative configurations. In this study, we propose a method that leverages uncertainty as the collective variable (CV) to guide the acquisition of chemically-relevant data points, focusing on regions of the configuration space where ML model predictions are most uncertain. This approach employs a Gaussian Mixture Model-based uncertainty metric from a single model as the CV for biased molecular dynamics simulations. The effectiveness of our approach in overcoming energy barriers and exploring unseen energy minima, thereby enhancing the data set in an active learning framework, is demonstrated on the alanine dipeptide benchmark system.","sentences":["Generating a data set that is representative of the accessible configuration space of a molecular system is crucial for the robustness of machine learned interatomic potentials (MLIP).","However, the complexity of molecular systems, characterized by intricate potential energy surfaces (PESs) with numerous local minima and energy barriers, presents a significant challenge.","Traditional methods of data generation, such as random sampling or exhaustive exploration, are either intractable or may not capture rare, but highly informative configurations.","In this study, we propose a method that leverages uncertainty as the collective variable (CV) to guide the acquisition of chemically-relevant data points, focusing on regions of the configuration space where ML model predictions are most uncertain.","This approach employs a Gaussian Mixture Model-based uncertainty metric from a single model as the CV for biased molecular dynamics simulations.","The effectiveness of our approach in overcoming energy barriers and exploring unseen energy minima, thereby enhancing the data set in an active learning framework, is demonstrated on the alanine dipeptide benchmark system."],"url":"http://arxiv.org/abs/2402.03753v1"}
{"created":"2024-02-06 06:37:43","title":"Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach","abstract":"With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system. Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications. However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges. Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour. In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment convolution network (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions. We propose a digital twin mobility profiling (DTMP) framework to learn node profiles on a mobility network DT model. Extensive experiments have been conducted upon three real-world datasets. Experimental results demonstrate the effectiveness of DTMP.","sentences":["With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system.","Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications.","However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges.","Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour.","In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment convolution network (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions.","We propose a digital twin mobility profiling (DTMP) framework to learn node profiles on a mobility network DT model.","Extensive experiments have been conducted upon three real-world datasets.","Experimental results demonstrate the effectiveness of DTMP."],"url":"http://arxiv.org/abs/2402.03750v1"}
{"created":"2024-02-06 06:28:30","title":"Succinct Data Structure for Chordal Graphs with Bounded Vertex Leafage","abstract":"Chordal graphs is a well-studied large graph class that is also a strict super-class of path graphs. Munro and Wu (ISAAC 2018) have given an $(n^2/4+o(n^2))-$bit succinct representation for $n-$vertex unlabeled chordal graphs. A chordal graph $G=(V,E)$ is the intersection graph of sub-trees of a tree $T$. Based on this characterization, the two parameters of chordal graphs which we consider in this work are \\textit{leafage}, introduced by Lin, McKee and West (Discussiones Mathematicae Graph Theory 1998) and \\textit{vertex leafage}, introduced by Chaplick and Stacho (Discret. Appl. Math. 2014). Leafage is the minimum number of leaves in any possible tree $T$ characterizing $G$. Let $L(u)$ denote the number of leaves of the sub-tree in $T$ corresponding to $u \\in V$ and $k=\\max\\limits_{u \\in V} L(u)$. The smallest $k$ for which there exists a tree $T$ for $G$ is called its vertex leafage.   In this work, we improve the worst-case information theoretic lower bound of Munro and Wu (ISAAC 2018) for chordal graphs when vertex leafage is bounded and leafage is unbounded. The class of unlabeled $k-$vertex leafage chordal graphs that consists of all chordal graphs with vertex leafage at most $k$ and unbounded leafage, denoted $\\mathcal{G}_k$, is introduced for the first time. For $k>1$ in $o(n/\\log n)$, we obtain a lower bound of $((k-1)n \\log n - kn \\log k - O(\\log n))-$bits on the size of any data structure that encodes a graph in $\\mathcal{G}_k$. Further, for every $k-$vertex leafage chordal graph $G$ such that $k>1$ in $o(n/\\log n)$, we present a $((k-1)n \\log n + o(kn \\log n))-$bit data structure, constructed using the succinct data structure for path graphs with $kn/2$ vertices. Our data structure supports adjacency query in $O(k \\log n)$ time and using additional $2n \\log n$ bits, an $O(k^2 d_v \\log n + \\log^2 n)$ time neighbourhood query where $d_v$ is degree of $v \\in V$.","sentences":["Chordal graphs is a well-studied large graph class that is also a strict super-class of path graphs.","Munro and Wu (ISAAC 2018) have given an $(n^2/4+o(n^2))-$bit succinct representation for $n-$vertex unlabeled chordal graphs.","A chordal graph $G=(V,E)$ is the intersection graph of sub-trees of a tree $T$. Based on this characterization, the two parameters of chordal graphs which we consider in this work are \\textit{leafage}, introduced by Lin, McKee and West (Discussiones Mathematicae Graph Theory 1998) and \\textit{vertex leafage}, introduced by Chaplick and Stacho (Discret.","Appl.","Math. 2014).","Leafage is the minimum number of leaves in any possible tree $T$ characterizing $G$. Let $L(u)$ denote the number of leaves of the sub-tree in $T$ corresponding to $u \\in V$ and $k=\\max\\limits_{u \\in V} L(u)$.","The smallest $k$ for which there exists a tree $T$ for $G$ is called its vertex leafage.   ","In this work, we improve the worst-case information theoretic lower bound of Munro and Wu (ISAAC 2018) for chordal graphs when vertex leafage is bounded and leafage is unbounded.","The class of unlabeled $k-$vertex leafage chordal graphs that consists of all chordal graphs with vertex leafage at most $k$ and unbounded leafage, denoted $\\mathcal{G}_k$, is introduced for the first time.","For $k>1$ in $o(n/\\log n)$, we obtain a lower bound of $((k-1)n \\log n - kn \\log k - O(\\log n))-$bits on the size of any data structure that encodes a graph in $\\mathcal{G}_k$. Further, for every $k-$vertex leafage chordal graph $G$ such that $k>1$ in $o(n/\\log n)$, we present a $((k-1)n \\log n + o(kn \\log n))-$bit data structure, constructed using the succinct data structure for path graphs with $kn/2$ vertices.","Our data structure supports adjacency query in $O(k \\log n)$ time and using additional $2n \\log n$ bits, an $O(k^2 d_v \\log n + \\log^2 n)$ time neighbourhood query where $d_v$ is degree of $v \\in V$."],"url":"http://arxiv.org/abs/2402.03748v1"}
{"created":"2024-02-06 06:28:17","title":"An invariance constrained deep learning network for PDE discovery","abstract":"The discovery of partial differential equations (PDEs) from datasets has attracted increased attention. However, the discovery of governing equations from sparse data with high noise is still very challenging due to the difficulty of derivatives computation and the disturbance of noise. Moreover, the selection principles for the candidate library to meet physical laws need to be further studied. The invariance is one of the fundamental laws for governing equations. In this study, we propose an invariance constrained deep learning network (ICNet) for the discovery of PDEs. Considering that temporal and spatial translation invariance (Galilean invariance) is a fundamental property of physical laws, we filter the candidates that cannot meet the requirement of the Galilean transformations. Subsequently, we embedded the fixed and possible terms into the loss function of neural network, significantly countering the effect of sparse data with high noise. Then, by filtering out redundant terms without fixing learnable parameters during the training process, the governing equations discovered by the ICNet method can effectively approximate the real governing equations. We select the 2D Burgers equation, the equation of 2D channel flow over an obstacle, and the equation of 3D intracranial aneurysm as examples to verify the superiority of the ICNet for fluid mechanics. Furthermore, we extend similar invariance methods to the discovery of wave equation (Lorentz Invariance) and verify it through Single and Coupled Klein-Gordon equation. The results show that the ICNet method with physical constraints exhibits excellent performance in governing equations discovery from sparse and noisy data.","sentences":["The discovery of partial differential equations (PDEs) from datasets has attracted increased attention.","However, the discovery of governing equations from sparse data with high noise is still very challenging due to the difficulty of derivatives computation and the disturbance of noise.","Moreover, the selection principles for the candidate library to meet physical laws need to be further studied.","The invariance is one of the fundamental laws for governing equations.","In this study, we propose an invariance constrained deep learning network (ICNet) for the discovery of PDEs.","Considering that temporal and spatial translation invariance (Galilean invariance) is a fundamental property of physical laws, we filter the candidates that cannot meet the requirement of the Galilean transformations.","Subsequently, we embedded the fixed and possible terms into the loss function of neural network, significantly countering the effect of sparse data with high noise.","Then, by filtering out redundant terms without fixing learnable parameters during the training process, the governing equations discovered by the ICNet method can effectively approximate the real governing equations.","We select the 2D Burgers equation, the equation of 2D channel flow over an obstacle, and the equation of 3D intracranial aneurysm as examples to verify the superiority of the ICNet for fluid mechanics.","Furthermore, we extend similar invariance methods to the discovery of wave equation (Lorentz Invariance) and verify it through Single and Coupled Klein-Gordon equation.","The results show that the ICNet method with physical constraints exhibits excellent performance in governing equations discovery from sparse and noisy data."],"url":"http://arxiv.org/abs/2402.03747v1"}
{"created":"2024-02-06 06:27:40","title":"Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback","abstract":"Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). The previous approaches for VLMMs involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and adding additional learnable modules. Video and text multimodal alignment remains challenging, primarily due to the deficient volume and quality of multimodal instruction-tune data compared to text-only data. We present a novel alignment strategy that employs multimodal AI system to oversee itself called Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content. Demonstrating enhanced performance across diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.","sentences":["Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs).","The previous approaches for VLMMs involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and adding additional learnable modules.","Video and text multimodal alignment remains challenging, primarily due to the deficient volume and quality of multimodal instruction-tune data compared to text-only data.","We present a novel alignment strategy that employs multimodal AI system to oversee itself called Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities.","In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content.","Demonstrating enhanced performance across diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model.","We commit to open-sourcing our code, models, and datasets to foster further research in this area."],"url":"http://arxiv.org/abs/2402.03746v1"}
{"created":"2024-02-06 06:13:13","title":"BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning","abstract":"The detection of automated accounts, also known as \"social bots\", has been an increasingly important concern for online social networks (OSNs). While several methods have been proposed for detecting social bots, significant research gaps remain. First, current models exhibit limitations in detecting sophisticated bots that aim to mimic genuine OSN users. Second, these methods often rely on simplistic profile features, which are susceptible to manipulation. In addition to their vulnerability to adversarial manipulations, these models lack generalizability, resulting in subpar performance when trained on one dataset and tested on another.   To address these challenges, we propose a novel framework for social Bot detection with Self-Supervised Contrastive Learning (BotSSCL). Our framework leverages contrastive learning to distinguish between social bots and humans in the embedding space to improve linear separability. The high-level representations derived by BotSSCL enhance its resilience to variations in data distribution and ensure generalizability. We evaluate BotSSCL's robustness against adversarial attempts to manipulate bot accounts to evade detection. Experiments on two datasets featuring sophisticated bots demonstrate that BotSSCL outperforms other supervised, unsupervised, and self-supervised baseline methods. We achieve approx. 6% and approx. 8% higher (F1) performance than SOTA on both datasets. In addition, BotSSCL also achieves 67% F1 when trained on one dataset and tested with another, demonstrating its generalizability. Lastly, BotSSCL increases adversarial complexity and only allows 4% success to the adversary in evading detection.","sentences":["The detection of automated accounts, also known as \"social bots\", has been an increasingly important concern for online social networks (OSNs).","While several methods have been proposed for detecting social bots, significant research gaps remain.","First, current models exhibit limitations in detecting sophisticated bots that aim to mimic genuine OSN users.","Second, these methods often rely on simplistic profile features, which are susceptible to manipulation.","In addition to their vulnerability to adversarial manipulations, these models lack generalizability, resulting in subpar performance when trained on one dataset and tested on another.   ","To address these challenges, we propose a novel framework for social Bot detection with Self-Supervised Contrastive Learning (BotSSCL).","Our framework leverages contrastive learning to distinguish between social bots and humans in the embedding space to improve linear separability.","The high-level representations derived by BotSSCL enhance its resilience to variations in data distribution and ensure generalizability.","We evaluate BotSSCL's robustness against adversarial attempts to manipulate bot accounts to evade detection.","Experiments on two datasets featuring sophisticated bots demonstrate that BotSSCL outperforms other supervised, unsupervised, and self-supervised baseline methods.","We achieve approx.","6% and approx.","8% higher (F1) performance than SOTA on both datasets.","In addition, BotSSCL also achieves 67% F1 when trained on one dataset and tested with another, demonstrating its generalizability.","Lastly, BotSSCL increases adversarial complexity and only allows 4% success to the adversary in evading detection."],"url":"http://arxiv.org/abs/2402.03740v1"}
{"created":"2024-02-06 06:05:11","title":"An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem","abstract":"The Maximum s-Bundle Problem (MBP) addresses the task of identifying a maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if its vertex connectivity is at least |V|-s, where the vertex connectivity equals the minimum number of vertices whose deletion yields a disconnected or trivial graph. MBP is NP-hard and holds relevance in numerous realworld scenarios emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the branch-and-bound (BnB) framework, whose performance heavily depends on the quality of the upper bound on the cardinality of a maximum s-bundle and the initial lower bound with graph reduction. In this work, we introduce a novel Partition-based Upper Bound (PUB) that leverages the graph partitioning technique to achieve a tighter upper bound compared to existing ones. To increase the lower bound, we propose to do short random walks on a clique to generate larger initial solutions. Then, we propose a new BnB algorithm that uses the initial lower bound and PUB in preprocessing for graph reduction, and uses PUB in the BnB search process for branch pruning. Extensive experiments with diverse s values demonstrate the significant progress of our algorithm over state-of-the-art BnB MBP algorithms. Moreover, our initial lower bound can also be generalized to other relaxation clique problems.","sentences":["The Maximum s-Bundle Problem (MBP) addresses the task of identifying a maximum s-bundle in a given graph.","A graph G=(V, E) is called an s-bundle if its vertex connectivity is at least |V|-s, where the vertex connectivity equals the minimum number of vertices whose deletion yields a disconnected or trivial graph.","MBP is NP-hard and holds relevance in numerous realworld scenarios emphasizing the vertex connectivity.","Exact algorithms for MBP mainly follow the branch-and-bound (BnB) framework, whose performance heavily depends on the quality of the upper bound on the cardinality of a maximum s-bundle and the initial lower bound with graph reduction.","In this work, we introduce a novel Partition-based Upper Bound (PUB) that leverages the graph partitioning technique to achieve a tighter upper bound compared to existing ones.","To increase the lower bound, we propose to do short random walks on a clique to generate larger initial solutions.","Then, we propose a new BnB algorithm that uses the initial lower bound and PUB in preprocessing for graph reduction, and uses PUB in the BnB search process for branch pruning.","Extensive experiments with diverse s values demonstrate the significant progress of our algorithm over state-of-the-art BnB MBP algorithms.","Moreover, our initial lower bound can also be generalized to other relaxation clique problems."],"url":"http://arxiv.org/abs/2402.03736v1"}
{"created":"2024-02-06 05:58:15","title":"Deep Outdated Fact Detection in Knowledge Graphs","abstract":"Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains. However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves. Existing solutions for outdated fact detection often rely on manual recognition. In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs. DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations. To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities. Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods.","sentences":["Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains.","However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves.","Existing solutions for outdated fact detection often rely on manual recognition.","In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs.","DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations.","To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities.","Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2402.03732v1"}
{"created":"2024-02-06 05:32:49","title":"Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory","abstract":"Deep-learning and large scale language-image training have produced image object detectors that generalise well to diverse environments and semantic classes. However, single-image object detectors trained on internet data are not optimally tailored for the embodied conditions inherent in robotics. Instead, robots must detect objects from complex multi-modal data streams involving depth, localisation and temporal correlation, a task termed embodied object detection. Paradigms such as Video Object Detection (VOD) and Semantic Mapping have been proposed to leverage such embodied data streams, but existing work fails to enhance performance using language-image training. In response, we investigate how an image object detector pre-trained using language-image data can be extended to perform embodied object detection. We propose a novel implicit object memory that uses projective geometry to aggregate the features of detected objects across long temporal horizons. The spatial and temporal information accumulated in memory is then used to enhance the image features of the base detector. When tested on embodied data streams sampled from diverse indoor scenes, our approach improves the base object detector by 3.09 mAP, outperforming alternative external memories designed for VOD and Semantic Mapping. Our method also shows a significant improvement of 16.90 mAP relative to baselines that perform embodied object detection without first training on language-image data, and is robust to sensor noise and domain shift experienced in real-world deployment.","sentences":["Deep-learning and large scale language-image training have produced image object detectors that generalise well to diverse environments and semantic classes.","However, single-image object detectors trained on internet data are not optimally tailored for the embodied conditions inherent in robotics.","Instead, robots must detect objects from complex multi-modal data streams involving depth, localisation and temporal correlation, a task termed embodied object detection.","Paradigms such as Video Object Detection (VOD) and Semantic Mapping have been proposed to leverage such embodied data streams, but existing work fails to enhance performance using language-image training.","In response, we investigate how an image object detector pre-trained using language-image data can be extended to perform embodied object detection.","We propose a novel implicit object memory that uses projective geometry to aggregate the features of detected objects across long temporal horizons.","The spatial and temporal information accumulated in memory is then used to enhance the image features of the base detector.","When tested on embodied data streams sampled from diverse indoor scenes, our approach improves the base object detector by 3.09 mAP, outperforming alternative external memories designed for VOD and Semantic Mapping.","Our method also shows a significant improvement of 16.90 mAP relative to baselines that perform embodied object detection without first training on language-image data, and is robust to sensor noise and domain shift experienced in real-world deployment."],"url":"http://arxiv.org/abs/2402.03721v1"}
{"created":"2024-02-06 05:11:38","title":"Clarify: Improving Model Robustness With Natural Language Corrections","abstract":"In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use such descriptions to improve the training process by reweighting the training data or gathering additional targeted data. Our user studies show that non-expert users can successfully describe model misconceptions via Clarify, improving worst-group accuracy by an average of 17.1% in two datasets. Additionally, we use Clarify to find and rectify 31 novel hard subpopulations in the ImageNet dataset, improving minority-split accuracy from 21.1% to 28.7%.","sentences":["In supervised learning, models are trained to extract correlations from a static dataset.","This often leads to models that rely on high-level misconceptions.","To prevent such misconceptions, we must necessarily provide additional information beyond the training data.","Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution.","Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data.","We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision.","We introduce Clarify, a novel interface and method for interactively correcting model misconceptions.","Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns.","Then, in an entirely automated way, we use such descriptions to improve the training process by reweighting the training data or gathering additional targeted data.","Our user studies show that non-expert users can successfully describe model misconceptions via Clarify, improving worst-group accuracy by an average of 17.1% in two datasets.","Additionally, we use Clarify to find and rectify 31 novel hard subpopulations in the ImageNet dataset, improving minority-split accuracy from 21.1% to 28.7%."],"url":"http://arxiv.org/abs/2402.03715v1"}
{"created":"2024-02-06 05:10:00","title":"Advancing Location-Invariant and Device-Agnostic Motion Activity Recognition on Wearable Devices","abstract":"Wearable sensors have permeated into people's lives, ushering impactful applications in interactive systems and activity recognition. However, practitioners face significant obstacles when dealing with sensing heterogeneities, requiring custom models for different platforms. In this paper, we conduct a comprehensive evaluation of the generalizability of motion models across sensor locations. Our analysis highlights this challenge and identifies key on-body locations for building location-invariant models that can be integrated on any device. For this, we introduce the largest multi-location activity dataset (N=50, 200 cumulative hours), which we make publicly available. We also present deployable on-device motion models reaching 91.41% frame-level F1-score from a single model irrespective of sensor placements. Lastly, we investigate cross-location data synthesis, aiming to alleviate the laborious data collection tasks by synthesizing data in one location given data from another. These contributions advance our vision of low-barrier, location-invariant activity recognition systems, catalyzing research in HCI and ubiquitous computing.","sentences":["Wearable sensors have permeated into people's lives, ushering impactful applications in interactive systems and activity recognition.","However, practitioners face significant obstacles when dealing with sensing heterogeneities, requiring custom models for different platforms.","In this paper, we conduct a comprehensive evaluation of the generalizability of motion models across sensor locations.","Our analysis highlights this challenge and identifies key on-body locations for building location-invariant models that can be integrated on any device.","For this, we introduce the largest multi-location activity dataset (N=50, 200 cumulative hours), which we make publicly available.","We also present deployable on-device motion models reaching 91.41% frame-level F1-score from a single model irrespective of sensor placements.","Lastly, we investigate cross-location data synthesis, aiming to alleviate the laborious data collection tasks by synthesizing data in one location given data from another.","These contributions advance our vision of low-barrier, location-invariant activity recognition systems, catalyzing research in HCI and ubiquitous computing."],"url":"http://arxiv.org/abs/2402.03714v1"}
{"created":"2024-02-06 04:57:07","title":"MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats","abstract":"In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset's applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://github.com/ntu-aris/MMAUD.","sentences":["In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset.","MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation.","MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays.","It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB.","Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets.","Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions.","Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools.","Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds.","This approach enhances the dataset's applicability, capturing the exact challenges faced during proximate vehicular operations.","It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond.","Our dataset, codes, and designs will be available in https://github.com/ntu-aris/MMAUD."],"url":"http://arxiv.org/abs/2402.03706v1"}
{"created":"2024-02-06 04:56:43","title":"FoolSDEdit: Deceptively Steering Your Edits Towards Targeted Attribute-aware Distribution","abstract":"Guided image synthesis methods, like SDEdit based on the diffusion model, excel at creating realistic images from user inputs such as stroke paintings. However, existing efforts mainly focus on image quality, often overlooking a key point: the diffusion model represents a data distribution, not individual images. This introduces a low but critical chance of generating images that contradict user intentions, raising ethical concerns. For example, a user inputting a stroke painting with female characteristics might, with some probability, get male faces from SDEdit. To expose this potential vulnerability, we aim to build an adversarial attack forcing SDEdit to generate a specific data distribution aligned with a specified attribute (e.g., female), without changing the input's attribute characteristics. We propose the Targeted Attribute Generative Attack (TAGA), using an attribute-aware objective function and optimizing the adversarial noise added to the input stroke painting. Empirical studies reveal that traditional adversarial noise struggles with TAGA, while natural perturbations like exposure and motion blur easily alter generated images' attributes. To execute effective attacks, we introduce FoolSDEdit: We design a joint adversarial exposure and blur attack, adding exposure and motion blur to the stroke painting and optimizing them together. We optimize the execution strategy of various perturbations, framing it as a network architecture search problem. We create the SuperPert, a graph representing diverse execution strategies for different perturbations. After training, we obtain the optimized execution strategy for effective TAGA against SDEdit. Comprehensive experiments on two datasets show our method compelling SDEdit to generate a targeted attribute-aware data distribution, significantly outperforming baselines.","sentences":["Guided image synthesis methods, like SDEdit based on the diffusion model, excel at creating realistic images from user inputs such as stroke paintings.","However, existing efforts mainly focus on image quality, often overlooking a key point: the diffusion model represents a data distribution, not individual images.","This introduces a low but critical chance of generating images that contradict user intentions, raising ethical concerns.","For example, a user inputting a stroke painting with female characteristics might, with some probability, get male faces from SDEdit.","To expose this potential vulnerability, we aim to build an adversarial attack forcing SDEdit to generate a specific data distribution aligned with a specified attribute (e.g., female), without changing the input's attribute characteristics.","We propose the Targeted Attribute Generative Attack (TAGA), using an attribute-aware objective function and optimizing the adversarial noise added to the input stroke painting.","Empirical studies reveal that traditional adversarial noise struggles with TAGA, while natural perturbations like exposure and motion blur easily alter generated images' attributes.","To execute effective attacks, we introduce FoolSDEdit: We design a joint adversarial exposure and blur attack, adding exposure and motion blur to the stroke painting and optimizing them together.","We optimize the execution strategy of various perturbations, framing it as a network architecture search problem.","We create the SuperPert, a graph representing diverse execution strategies for different perturbations.","After training, we obtain the optimized execution strategy for effective TAGA against SDEdit.","Comprehensive experiments on two datasets show our method compelling SDEdit to generate a targeted attribute-aware data distribution, significantly outperforming baselines."],"url":"http://arxiv.org/abs/2402.03705v1"}
{"created":"2024-02-06 04:42:36","title":"Improving and Unifying Discrete&Continuous-time Discrete Denoising Diffusion","abstract":"Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects. Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets. We open-source our unified code at https://github.com/LingxiaoShawn/USD3.","sentences":["Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs.","Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion.","However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability.","In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion.","In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion.","Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects.","Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets.","We open-source our unified code at https://github.com/LingxiaoShawn/USD3."],"url":"http://arxiv.org/abs/2402.03701v1"}
{"created":"2024-02-06 04:37:09","title":"Estimating the Local Learning Coefficient at Scale","abstract":"The \\textit{local learning coefficient} (LLC) is a principled way of quantifying model complexity, originally derived in the context of Bayesian statistics using singular learning theory (SLT). Several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets. Using a method developed in {\\tt arXiv:2308.12108 [stat.ML]} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters. We also show that the estimated LLC has the rescaling invariance that holds for the theoretical quantity.","sentences":["The \\textit{local learning coefficient} (LLC) is a principled way of quantifying model complexity, originally derived in the context of Bayesian statistics using singular learning theory (SLT).","Several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets.","Using a method developed in {\\tt arXiv:2308.12108","[stat.","ML]} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters.","We also show that the estimated LLC has the rescaling invariance that holds for the theoretical quantity."],"url":"http://arxiv.org/abs/2402.03698v1"}
{"created":"2024-02-06 04:22:44","title":"A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective","abstract":"Vertical Federated Learning (VFL) is a federated learning paradigm where multiple participants, who share the same set of samples but hold different features, jointly train machine learning models. Although VFL enables collaborative machine learning without sharing raw data, it is still susceptible to various privacy threats. In this paper, we conduct the first comprehensive survey of the state-of-the-art in privacy attacks and defenses in VFL. We provide taxonomies for both attacks and defenses, based on their characterizations, and discuss open challenges and future research directions. Specifically, our discussion is structured around the model's life cycle, by delving into the privacy threats encountered during different stages of machine learning and their corresponding countermeasures. This survey not only serves as a resource for the research community but also offers clear guidance and actionable insights for practitioners to safeguard data privacy throughout the model's life cycle.","sentences":["Vertical Federated Learning (VFL) is a federated learning paradigm where multiple participants, who share the same set of samples but hold different features, jointly train machine learning models.","Although VFL enables collaborative machine learning without sharing raw data, it is still susceptible to various privacy threats.","In this paper, we conduct the first comprehensive survey of the state-of-the-art in privacy attacks and defenses in VFL.","We provide taxonomies for both attacks and defenses, based on their characterizations, and discuss open challenges and future research directions.","Specifically, our discussion is structured around the model's life cycle, by delving into the privacy threats encountered during different stages of machine learning and their corresponding countermeasures.","This survey not only serves as a resource for the research community but also offers clear guidance and actionable insights for practitioners to safeguard data privacy throughout the model's life cycle."],"url":"http://arxiv.org/abs/2402.03688v1"}
{"created":"2024-02-06 03:47:49","title":"ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor","abstract":"As Graph Neural Networks (GNNs) become popular, libraries like PyTorch-Geometric (PyG) and Deep Graph Library (DGL) are proposed; these libraries have emerged as the de facto standard for implementing GNNs because they provide graph-oriented APIs and are purposefully designed to manage the inherent sparsity and irregularity in graph structures. However, these libraries show poor scalability on multi-core processors, which under-utilizes the available platform resources and limits the performance. This is because GNN training is a resource-intensive workload with high volume of irregular data accessing, and existing libraries fail to utilize the memory bandwidth efficiently. To address this challenge, we propose ARGO, a novel runtime system for GNN training that offers scalable performance. ARGO exploits multi-processing and core-binding techniques to improve platform resource utilization. We further develop an auto-tuner that searches for the optimal configuration for multi-processing and core-binding. The auto-tuner works automatically, making it completely transparent from the user. Furthermore, the auto-tuner allows ARGO to adapt to various platforms, GNN models, datasets, etc. We evaluate ARGO on two representative GNN models and four widely-used datasets on two platforms. With the proposed autotuner, ARGO is able to select a near-optimal configuration by exploring only 5% of the design space. ARGO speeds up state-of-the-art GNN libraries by up to 5.06x and 4.54x on a four-socket Ice Lake machine with 112 cores and a two-socket Sapphire Rapids machine with 64 cores, respectively. Finally, ARGO can seamlessly integrate into widely-used GNN libraries (e.g., DGL, PyG) with few lines of code and speed up GNN training.","sentences":["As Graph Neural Networks (GNNs) become popular, libraries like PyTorch-Geometric (PyG) and Deep Graph Library (DGL) are proposed; these libraries have emerged as the de facto standard for implementing GNNs because they provide graph-oriented APIs and are purposefully designed to manage the inherent sparsity and irregularity in graph structures.","However, these libraries show poor scalability on multi-core processors, which under-utilizes the available platform resources and limits the performance.","This is because GNN training is a resource-intensive workload with high volume of irregular data accessing, and existing libraries fail to utilize the memory bandwidth efficiently.","To address this challenge, we propose ARGO, a novel runtime system for GNN training that offers scalable performance.","ARGO exploits multi-processing and core-binding techniques to improve platform resource utilization.","We further develop an auto-tuner that searches for the optimal configuration for multi-processing and core-binding.","The auto-tuner works automatically, making it completely transparent from the user.","Furthermore, the auto-tuner allows ARGO to adapt to various platforms, GNN models, datasets, etc.","We evaluate ARGO on two representative GNN models and four widely-used datasets on two platforms.","With the proposed autotuner, ARGO is able to select a near-optimal configuration by exploring only 5% of the design space.","ARGO speeds up state-of-the-art GNN libraries by up to 5.06x and 4.54x on a four-socket Ice Lake machine with 112 cores and a two-socket Sapphire Rapids machine with 64 cores, respectively.","Finally, ARGO can seamlessly integrate into widely-used GNN libraries (e.g., DGL, PyG) with few lines of code and speed up GNN training."],"url":"http://arxiv.org/abs/2402.03671v1"}
{"created":"2024-02-06 03:41:12","title":"Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning","abstract":"Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the reasoning abilities of LLMs. The experimental results on popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method enhances the overall accuracy of factual reasoning by 27.33% and mathematical proof by 31.43%, when compared with traditional DR methods. Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy.","sentences":["Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning.","However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR.","Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof.","Specifically, our methodology comprises two steps.","Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs.","Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process.","Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the reasoning abilities of LLMs.","The experimental results on popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method enhances the overall accuracy of factual reasoning by 27.33% and mathematical proof by 31.43%, when compared with traditional DR methods.","Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy."],"url":"http://arxiv.org/abs/2402.03667v1"}
{"created":"2024-02-06 03:33:50","title":"Symbol Correctness in Deep Neural Networks Containing Symbolic Layers","abstract":"To handle AI tasks that combine perception and logical reasoning, recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in addition to traditional neural layers -- symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference. We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data. We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for). Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs faced by NS-DNN training algorithms. In doing so, we both identify significant points of ambiguity in prior work, and provide a framework to support further NS-DNN developments.","sentences":["To handle AI tasks that combine perception and logical reasoning, recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in addition to traditional neural layers -- symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference.","We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data.","We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for).","Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs faced by NS-DNN training algorithms.","In doing so, we both identify significant points of ambiguity in prior work, and provide a framework to support further NS-DNN developments."],"url":"http://arxiv.org/abs/2402.03663v1"}
{"created":"2024-02-06 03:31:28","title":"Transductive Reward Inference on Graph","abstract":"In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data. Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum. Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach. The application of our inferred rewards improves the performance in offline reinforcement learning tasks.","sentences":["In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning.","Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics.","Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data.","We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards.","Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data.","Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum.","Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach.","The application of our inferred rewards improves the performance in offline reinforcement learning tasks."],"url":"http://arxiv.org/abs/2402.03661v1"}
{"created":"2024-02-06 02:56:07","title":"Temporal Graph Analysis with TGX","abstract":"Real-world networks, with their evolving relations, are best captured as temporal graphs. However, existing software libraries are largely designed for static graphs where the dynamic nature of temporal graphs is ignored. Bridging this gap, we introduce TGX, a Python package specially designed for analysis of temporal networks that encompasses an automated pipeline for data loading, data processing, and analysis of evolving graphs. TGX provides access to eleven built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as well as any novel datasets in the .csv format. Beyond data loading, TGX facilitates data processing functionalities such as discretization of temporal graphs and node subsampling to accelerate working with larger datasets. For comprehensive investigation, TGX offers network analysis by providing a diverse set of measures, including average node degree and the evolving number of nodes and edges per timestamp. Additionally, the package consolidates meaningful visualization plots indicating the evolution of temporal patterns, such as Temporal Edge Appearance (TEA) and Temporal Edge Trafficc (TET) plots. The TGX package is a robust tool for examining the features of temporal graphs and can be used in various areas like studying social networks, citation networks, and tracking user interactions. We plan to continuously support and update TGX based on community feedback. TGX is publicly available on: https://github.com/ComplexData-MILA/TGX.","sentences":["Real-world networks, with their evolving relations, are best captured as temporal graphs.","However, existing software libraries are largely designed for static graphs where the dynamic nature of temporal graphs is ignored.","Bridging this gap, we introduce TGX, a Python package specially designed for analysis of temporal networks that encompasses an automated pipeline for data loading, data processing, and analysis of evolving graphs.","TGX provides access to eleven built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as well as any novel datasets in the .csv format.","Beyond data loading, TGX facilitates data processing functionalities such as discretization of temporal graphs and node subsampling to accelerate working with larger datasets.","For comprehensive investigation, TGX offers network analysis by providing a diverse set of measures, including average node degree and the evolving number of nodes and edges per timestamp.","Additionally, the package consolidates meaningful visualization plots indicating the evolution of temporal patterns, such as Temporal Edge Appearance (TEA) and Temporal Edge Trafficc (TET) plots.","The TGX package is a robust tool for examining the features of temporal graphs and can be used in various areas like studying social networks, citation networks, and tracking user interactions.","We plan to continuously support and update TGX based on community feedback.","TGX is publicly available on: https://github.com/ComplexData-MILA/TGX."],"url":"http://arxiv.org/abs/2402.03651v1"}
{"created":"2024-02-06 02:47:16","title":"CAMBranch: Contrastive Learning with Augmented MILPs for Branching","abstract":"Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\\&B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. To address this challenge, we propose \\textbf{C}ontrastive Learning with \\textbf{A}ugmented \\textbf{M}ILPs for \\textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs. This approach enables the acquisition of a considerable number of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions. Experimental results demonstrate that CAMBranch, trained with only 10\\% of the complete dataset, exhibits superior performance. Ablation studies further validate the effectiveness of our method.","sentences":["Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\\&B) branching policies for solving Mixed Integer Linear Programming (MILP).","These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance.","However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor.","To address this challenge, we propose \\textbf{C}ontrastive Learning with \\textbf{A}ugmented \\textbf{M}ILPs for \\textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs.","This approach enables the acquisition of a considerable number of labeled expert samples.","CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions.","Experimental results demonstrate that CAMBranch, trained with only 10\\% of the complete dataset, exhibits superior performance.","Ablation studies further validate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.03647v1"}
{"created":"2024-02-06 02:45:13","title":"Lens: A Foundation Model for Network Traffic","abstract":"Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from large-scale network traffic. To further enhance pre-training performance, we design a novel loss that integrates three distinct tasks, namely Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation results on multiple benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and traffic generation. Notably, it also requires considerably less labeled data for fine-tuning compared to current methods.","sentences":["Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers.","Analyzing and understanding network traffic is vital for improving network security and management.","However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics.","To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data.","However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks.","To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data.","Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from large-scale network traffic.","To further enhance pre-training performance, we design a novel loss that integrates three distinct tasks, namely Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP).","Evaluation results on multiple benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and traffic generation.","Notably, it also requires considerably less labeled data for fine-tuning compared to current methods."],"url":"http://arxiv.org/abs/2402.03646v1"}
{"created":"2024-02-06 02:39:59","title":"Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation","abstract":"The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish. The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine. The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms. By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues. To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages. This underlines the viability of stance classification as an effective tool for identifying multicultural misinformation.","sentences":["The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation.","In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish.","The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine.","The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms.","By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues.","To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages.","This underlines the viability of stance classification as an effective tool for identifying multicultural misinformation."],"url":"http://arxiv.org/abs/2402.03642v1"}
{"created":"2024-02-06 02:33:00","title":"torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability Problem","abstract":"The remarkable achievements of machine learning techniques in analyzing discrete structures have drawn significant attention towards their integration into combinatorial optimization algorithms. Typically, these methodologies improve existing solvers by injecting learned models within the solving loop to enhance the efficiency of the search process. In this work, we derive a single differentiable function capable of approximating solutions for the Maximum Satisfiability Problem (MaxSAT). Then, we present a novel neural network architecture to model our differentiable function, and progressively solve MaxSAT using backpropagation. This approach eliminates the need for labeled data or a neural network training phase, as the training process functions as the solving algorithm. Additionally, we leverage the computational power of GPUs to accelerate these computations. Experimental results on challenging MaxSAT instances show that our proposed methodology outperforms two existing MaxSAT solvers, and is on par with another in terms of solution cost, without necessitating any training or access to an underlying SAT solver. Given that numerous NP-hard problems can be reduced to MaxSAT, our novel technique paves the way for a new generation of solvers poised to benefit from neural network GPU acceleration.","sentences":["The remarkable achievements of machine learning techniques in analyzing discrete structures have drawn significant attention towards their integration into combinatorial optimization algorithms.","Typically, these methodologies improve existing solvers by injecting learned models within the solving loop to enhance the efficiency of the search process.","In this work, we derive a single differentiable function capable of approximating solutions for the Maximum Satisfiability Problem (MaxSAT).","Then, we present a novel neural network architecture to model our differentiable function, and progressively solve MaxSAT using backpropagation.","This approach eliminates the need for labeled data or a neural network training phase, as the training process functions as the solving algorithm.","Additionally, we leverage the computational power of GPUs to accelerate these computations.","Experimental results on challenging MaxSAT instances show that our proposed methodology outperforms two existing MaxSAT solvers, and is on par with another in terms of solution cost, without necessitating any training or access to an underlying SAT solver.","Given that numerous NP-hard problems can be reduced to MaxSAT, our novel technique paves the way for a new generation of solvers poised to benefit from neural network GPU acceleration."],"url":"http://arxiv.org/abs/2402.03640v1"}
{"created":"2024-02-06 02:28:59","title":"Online Informative Sampling using Semantic Features in Underwater Environments","abstract":"The underwater world remains largely unexplored, with Autonomous Underwater Vehicles (AUVs) playing a crucial role in sub-sea explorations. However, continuous monitoring of underwater environments using AUVs can generate a significant amount of data. In addition, sending live data feed from an underwater environment requires dedicated on-board data storage options for AUVs which can hinder requirements of other higher priority tasks. Informative sampling techniques offer a solution by condensing observations. In this paper, we present a semantically-aware online informative sampling (ON-IS) approach which samples an AUV's visual experience in real-time. Specifically, we obtain visual features from a fine-tuned object detection model to align the sampling outcomes with the desired semantic information. Our contributions are (a) a novel Semantic Online Informative Sampling (SON-IS) algorithm, (b) a user study to validate the proposed approach and (c) a novel evaluation metric to score our proposed algorithm with respect to the suggested samples by human subjects","sentences":["The underwater world remains largely unexplored, with Autonomous Underwater Vehicles (AUVs) playing a crucial role in sub-sea explorations.","However, continuous monitoring of underwater environments using AUVs can generate a significant amount of data.","In addition, sending live data feed from an underwater environment requires dedicated on-board data storage options for AUVs which can hinder requirements of other higher priority tasks.","Informative sampling techniques offer a solution by condensing observations.","In this paper, we present a semantically-aware online informative sampling (ON-IS) approach which samples an AUV's visual experience in real-time.","Specifically, we obtain visual features from a fine-tuned object detection model to align the sampling outcomes with the desired semantic information.","Our contributions are (a) a novel Semantic Online Informative Sampling (SON-IS) algorithm, (b) a user study to validate the proposed approach and (c) a novel evaluation metric to score our proposed algorithm with respect to the suggested samples by human subjects"],"url":"http://arxiv.org/abs/2402.03636v1"}
{"created":"2024-02-06 01:56:29","title":"Disparate Impact on Group Accuracy of Linearization for Private Inference","abstract":"Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the fine-tuning step for linearized models can serve as an effective mitigation strategy.","sentences":["Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge.","To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks.","This technique results in significantly reduced runtimes with often negligible impacts on accuracy.","In this paper, we demonstrate that such computational benefits may lead to increased fairness costs.","Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups.","To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures.","Finally, we show how a simple procedure altering the fine-tuning step for linearized models can serve as an effective mitigation strategy."],"url":"http://arxiv.org/abs/2402.03629v1"}
{"created":"2024-02-06 01:29:35","title":"Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time","abstract":"In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\\sqrt{\\log n})$, where $n$ is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well.","sentences":["In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations.","We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\\sqrt{\\log n})$, where $n$ is the number of training samples.","A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor.","Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss.","Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well."],"url":"http://arxiv.org/abs/2402.03625v1"}
{"created":"2024-02-06 01:07:56","title":"Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction","abstract":"Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.","sentences":["Humans extract useful abstractions of the world from noisy sensory data.","Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions.","Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language.","To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa.","We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's.","This suggests human visual and linguistic representations are more dissociable than those of GPT-4."],"url":"http://arxiv.org/abs/2402.03618v1"}
{"created":"2024-02-06 01:06:17","title":"Environment-Centric Learning Approach for Gait Synthesis in Terrestrial Soft Robots","abstract":"Locomotion gaits are fundamental for control of soft terrestrial robots. However, synthesis of these gaits is challenging due to modeling of robot-environment interaction and lack of a mathematical framework. This work presents an environment-centric, data-driven and fault-tolerant probabilistic Model-Free Control (pMFC) framework that allows for soft multi-limb robots to learn from their environment and synthesize diverse sets of locomotion gaits for realizing open-loop control. Here, discretization of factors dominating robot-environment interactions enables an environment-specific graphical representation where the edges encode experimental locomotion data corresponding to the robot motion primitives. In this graph, locomotion gaits are defined as simple cycles that are transformation invariant, i.e., the locomotion is independent of the starting vertex of these periodic cycles. Gait synthesis, the problem of finding optimal locomotion gaits for a given substrate, is formulated as Binary Integer Linear Programming (BILP) problems with a linearized cost function, linear constraints, and iterative simple cycle detection. Experimentally, gaits are synthesized for varying robot-environment interactions. Variables include robot morphology - three-limb and four-limb robots, TerreSoRo-III and TerreSoRo-IV; substrate - rubber mat, whiteboard and carpet; and actuator functionality - simulated loss of robot limb actuation. On an average, gait synthesis improves the translation and rotation speeds by 82% and 97% respectively. The results highlight that data-driven methods are vital to soft robot locomotion control due to the significant influence of unexpected asymmetries in the system and the dependence of optimal gait sequences on the experimental robot-environment interaction.","sentences":["Locomotion gaits are fundamental for control of soft terrestrial robots.","However, synthesis of these gaits is challenging due to modeling of robot-environment interaction and lack of a mathematical framework.","This work presents an environment-centric, data-driven and fault-tolerant probabilistic Model-Free Control (pMFC) framework that allows for soft multi-limb robots to learn from their environment and synthesize diverse sets of locomotion gaits for realizing open-loop control.","Here, discretization of factors dominating robot-environment interactions enables an environment-specific graphical representation where the edges encode experimental locomotion data corresponding to the robot motion primitives.","In this graph, locomotion gaits are defined as simple cycles that are transformation invariant, i.e., the locomotion is independent of the starting vertex of these periodic cycles.","Gait synthesis, the problem of finding optimal locomotion gaits for a given substrate, is formulated as Binary Integer Linear Programming (BILP) problems with a linearized cost function, linear constraints, and iterative simple cycle detection.","Experimentally, gaits are synthesized for varying robot-environment interactions.","Variables include robot morphology - three-limb and four-limb robots, TerreSoRo-III and TerreSoRo-IV; substrate - rubber mat, whiteboard and carpet; and actuator functionality - simulated loss of robot limb actuation.","On an average, gait synthesis improves the translation and rotation speeds by 82% and 97% respectively.","The results highlight that data-driven methods are vital to soft robot locomotion control due to the significant influence of unexpected asymmetries in the system and the dependence of optimal gait sequences on the experimental robot-environment interaction."],"url":"http://arxiv.org/abs/2402.03617v1"}
{"created":"2024-02-06 01:01:23","title":"Bayesian Factorised Granger-Causal Graphs For Multivariate Time-series Data","abstract":"We study the problem of automatically discovering Granger causal relations from observational multivariate time-series data. Vector autoregressive (VAR) models have been time-tested for this problem, including Bayesian variants and more recent developments using deep neural networks. Most existing VAR methods for Granger causality use sparsity-inducing penalties/priors or post-hoc thresholds to interpret their coefficients as Granger causal graphs. Instead, we propose a new Bayesian VAR model with a hierarchical graph prior over binary Granger causal graphs, separately from the VAR coefficients. We develop an efficient algorithm to infer the posterior over binary Granger causal graphs. Our method provides better uncertainty quantification, has less hyperparameters, and achieves better performance than competing approaches, especially on sparse multivariate time-series data.","sentences":["We study the problem of automatically discovering Granger causal relations from observational multivariate time-series data.","Vector autoregressive (VAR) models have been time-tested for this problem, including Bayesian variants and more recent developments using deep neural networks.","Most existing VAR methods for Granger causality use sparsity-inducing penalties/priors or post-hoc thresholds to interpret their coefficients as Granger causal graphs.","Instead, we propose a new Bayesian VAR model with a hierarchical graph prior over binary Granger causal graphs, separately from the VAR coefficients.","We develop an efficient algorithm to infer the posterior over binary Granger causal graphs.","Our method provides better uncertainty quantification, has less hyperparameters, and achieves better performance than competing approaches, especially on sparse multivariate time-series data."],"url":"http://arxiv.org/abs/2402.03614v1"}
{"created":"2024-02-06 00:55:06","title":"Privacy risk in GeoData: A survey","abstract":"With the ubiquitous use of location-based services, large-scale individual-level location data has been widely collected through location-awareness devices. The exposure of location data constitutes a significant privacy risk to users as it can lead to de-anonymisation, the inference of sensitive information, and even physical threats. Geoprivacy concerns arise on the issues of user identity de-anonymisation and location exposure. In this survey, we analyse different geomasking techniques that have been proposed to protect the privacy of individuals in geodata. We present a taxonomy to characterise these techniques along different dimensions, and conduct a survey of geomasking techniques. We then highlight shortcomings of current techniques and discuss avenues for future research.","sentences":["With the ubiquitous use of location-based services, large-scale individual-level location data has been widely collected through location-awareness devices.","The exposure of location data constitutes a significant privacy risk to users as it can lead to de-anonymisation, the inference of sensitive information, and even physical threats.","Geoprivacy concerns arise on the issues of user identity de-anonymisation and location exposure.","In this survey, we analyse different geomasking techniques that have been proposed to protect the privacy of individuals in geodata.","We present a taxonomy to characterise these techniques along different dimensions, and conduct a survey of geomasking techniques.","We then highlight shortcomings of current techniques and discuss avenues for future research."],"url":"http://arxiv.org/abs/2402.03612v1"}
{"created":"2024-02-06 00:20:49","title":"Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction","abstract":"Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users. While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models. To address this research gap, our study performs a top-down analysis on representative CTR models. Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias. We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them. Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias. Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models. Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods. The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets.","sentences":["Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users.","While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models.","To address this research gap, our study performs a top-down analysis on representative CTR models.","Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias.","We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them.","Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias.","Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models.","Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods.","The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets."],"url":"http://arxiv.org/abs/2402.03600v1"}
{"created":"2024-02-06 00:14:53","title":"Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models","abstract":"Prescription contraceptives play a critical role in supporting women's reproductive health. With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest. However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract. Here, we evaluate the zero-shot abilities of a recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset. We demonstrate that GPT-4 can accurately extract reasons for contraceptive switching, outperforming baseline BERT-based models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively. Human evaluation of GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal hallucinations. Using extracted reasons, we identified patient preference, adverse events, and insurance as key reasons for switching using unsupervised topic modeling approaches. Notably, we also showed using our approach that \"weight gain/mood change\" and \"insurance coverage\" are disproportionately found as reasons for contraceptive switching in specific demographic populations. Our code and supplemental data are available at https://github.com/BMiao10/contraceptive-switching.","sentences":["Prescription contraceptives play a critical role in supporting women's reproductive health.","With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest.","However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract.","Here, we evaluate the zero-shot abilities of a recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset.","We demonstrate that GPT-4 can accurately extract reasons for contraceptive switching, outperforming baseline BERT-based models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively.","Human evaluation of GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal hallucinations.","Using extracted reasons, we identified patient preference, adverse events, and insurance as key reasons for switching using unsupervised topic modeling approaches.","Notably, we also showed using our approach that \"weight gain/mood change\" and \"insurance coverage\" are disproportionately found as reasons for contraceptive switching in specific demographic populations.","Our code and supplemental data are available at https://github.com/BMiao10/contraceptive-switching."],"url":"http://arxiv.org/abs/2402.03597v1"}
{"created":"2024-02-05 23:46:42","title":"A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System","abstract":"Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the alleviation of traffic congestion and to healthier lifestyles. Efficiently operating such systems and maintaining high customer satisfaction is challenging due to the stochastic nature of trip demand, leading to full or empty stations. Devising effective rebalancing strategies using vehicles to redistribute bikes among stations is therefore of uttermost importance for operators. As a promising alternative to classical mathematical optimization, reinforcement learning is gaining ground to solve sequential decision-making problems. This paper introduces a spatio-temporal reinforcement learning algorithm for the dynamic rebalancing problem with multiple vehicles. We first formulate the problem as a Multi-agent Markov Decision Process in a continuous time framework. This allows for independent and cooperative vehicle rebalancing, eliminating the impractical restriction of time-discretized models where vehicle departures are synchronized. A comprehensive simulator under the first-arrive-first-serve rule is then developed to facilitate the learning process by computing immediate rewards under diverse demand scenarios. To estimate the value function and learn the rebalancing policy, various Deep Q-Network configurations are tested, minimizing the lost demand. Experiments are carried out on various datasets generated from historical data, affected by both temporal and weather factors. The proposed algorithms outperform benchmarks, including a multi-period Mixed-Integer Programming model, in terms of lost demand. Once trained, it yields immediate decisions, making it suitable for real-time applications. Our work offers practical insights for operators and enriches the integration of reinforcement learning into dynamic rebalancing problems, paving the way for more intelligent and robust urban mobility solutions.","sentences":["Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the alleviation of traffic congestion and to healthier lifestyles.","Efficiently operating such systems and maintaining high customer satisfaction is challenging due to the stochastic nature of trip demand, leading to full or empty stations.","Devising effective rebalancing strategies using vehicles to redistribute bikes among stations is therefore of uttermost importance for operators.","As a promising alternative to classical mathematical optimization, reinforcement learning is gaining ground to solve sequential decision-making problems.","This paper introduces a spatio-temporal reinforcement learning algorithm for the dynamic rebalancing problem with multiple vehicles.","We first formulate the problem as a Multi-agent Markov Decision Process in a continuous time framework.","This allows for independent and cooperative vehicle rebalancing, eliminating the impractical restriction of time-discretized models where vehicle departures are synchronized.","A comprehensive simulator under the first-arrive-first-serve rule is then developed to facilitate the learning process by computing immediate rewards under diverse demand scenarios.","To estimate the value function and learn the rebalancing policy, various Deep Q-Network configurations are tested, minimizing the lost demand.","Experiments are carried out on various datasets generated from historical data, affected by both temporal and weather factors.","The proposed algorithms outperform benchmarks, including a multi-period Mixed-Integer Programming model, in terms of lost demand.","Once trained, it yields immediate decisions, making it suitable for real-time applications.","Our work offers practical insights for operators and enriches the integration of reinforcement learning into dynamic rebalancing problems, paving the way for more intelligent and robust urban mobility solutions."],"url":"http://arxiv.org/abs/2402.03589v1"}
{"created":"2024-02-05 23:46:03","title":"Continual Domain Adversarial Adaptation via Double-Head Discriminators","abstract":"Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data. Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches. This limitation arises from the erroneous empirical estimation of $\\gH$-divergence with few source domain samples. To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase. We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\\gH$-divergence related adversarial loss is reduced from the source domain side. Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\\%$ improvement on all categories of target domain adaptation task while significantly mitigating the forgetting on source domain.","sentences":["Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data.","Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches.","This limitation arises from the erroneous empirical estimation of $\\gH$-divergence with few source domain samples.","To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase.","We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\\gH$-divergence related adversarial loss is reduced from the source domain side.","Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\\%$ improvement on all categories of target domain adaptation task while significantly mitigating the forgetting on source domain."],"url":"http://arxiv.org/abs/2402.03588v1"}
{"created":"2024-02-05 23:17:08","title":"Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels","abstract":"Apple and Google introduced their versions of privacy nutrition labels to the mobile app stores to better inform users of the apps' data practices. However, these labels are self-reported by developers and have been found to contain many inaccuracies due to misunderstandings of the label taxonomy. In this work, we present Matcha, an IDE plugin that uses automated code analysis to help developers create accurate Google Play data safety labels. Developers can benefit from Matcha's ability to detect user data accesses and transmissions while staying in control of the generated label by adding custom Java annotations and modifying an auto-generated XML specification. Our evaluation with 12 developers showed that Matcha helped our participants improved the accuracy of a label they created with Google's official tool for a real-world app they developed. We found that participants preferred Matcha for its accuracy benefits. Drawing on Matcha, we discuss general design recommendations for developer tools used to create accurate standardized privacy notices.","sentences":["Apple and Google introduced their versions of privacy nutrition labels to the mobile app stores to better inform users of the apps' data practices.","However, these labels are self-reported by developers and have been found to contain many inaccuracies due to misunderstandings of the label taxonomy.","In this work, we present Matcha, an IDE plugin that uses automated code analysis to help developers create accurate Google Play data safety labels.","Developers can benefit from Matcha's ability to detect user data accesses and transmissions while staying in control of the generated label by adding custom Java annotations and modifying an auto-generated XML specification.","Our evaluation with 12 developers showed that Matcha helped our participants improved the accuracy of a label they created with Google's official tool for a real-world app they developed.","We found that participants preferred Matcha for its accuracy benefits.","Drawing on Matcha, we discuss general design recommendations for developer tools used to create accurate standardized privacy notices."],"url":"http://arxiv.org/abs/2402.03582v1"}
{"created":"2024-02-05 22:58:06","title":"Revisiting the Dataset Bias Problem from a Statistical Perspective","abstract":"In this paper, we study the \"dataset bias\" problem from a statistical standpoint, and identify the main cause of the problem as the strong correlation between a class attribute u and a non-class attribute b in the input x, represented by p(u|b) differing significantly from p(u). Since p(u|b) appears as part of the sampling distributions in the standard maximum log-likelihood (MLL) objective, a model trained on a biased dataset via MLL inherently incorporates such correlation into its parameters, leading to poor generalization to unbiased test data. From this observation, we propose to mitigate dataset bias via either weighting the objective of each sample n by \\frac{1}{p(u_{n}|b_{n})} or sampling that sample with a weight proportional to \\frac{1}{p(u_{n}|b_{n})}. While both methods are statistically equivalent, the former proves more stable and effective in practice. Additionally, we establish a connection between our debiasing approach and causal reasoning, reinforcing our method's theoretical foundation. However, when the bias label is unavailable, computing p(u|b) exactly is difficult. To overcome this challenge, we propose to approximate \\frac{1}{p(u|b)} using a biased classifier trained with \"bias amplification\" losses. Extensive experiments on various biased datasets demonstrate the superiority of our method over existing debiasing techniques in most settings, validating our theoretical analysis.","sentences":["In this paper, we study the \"dataset bias\" problem from a statistical standpoint, and identify the main cause of the problem as the strong correlation between a class attribute u and a non-class attribute b in the input x, represented by p(u|b) differing significantly from p(u).","Since p(u|b) appears as part of the sampling distributions in the standard maximum log-likelihood (MLL) objective, a model trained on a biased dataset via MLL inherently incorporates such correlation into its parameters, leading to poor generalization to unbiased test data.","From this observation, we propose to mitigate dataset bias via either weighting the objective of each sample n by \\frac{1}{p(u_{n}|b_{n})} or sampling that sample with a weight proportional to \\frac{1}{p(u_{n}|b_{n})}.","While both methods are statistically equivalent, the former proves more stable and effective in practice.","Additionally, we establish a connection between our debiasing approach and causal reasoning, reinforcing our method's theoretical foundation.","However, when the bias label is unavailable, computing p(u|b) exactly is difficult.","To overcome this challenge, we propose to approximate \\frac{1}{p(u|b)} using a biased classifier trained with \"bias amplification\" losses.","Extensive experiments on various biased datasets demonstrate the superiority of our method over existing debiasing techniques in most settings, validating our theoretical analysis."],"url":"http://arxiv.org/abs/2402.03577v1"}
{"created":"2024-02-05 22:55:33","title":"Toward Human-AI Alignment in Large-Scale Multi-Player Games","abstract":"Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity. Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns. These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications. Our study advances the alignment discussion in AI and especially generative AI research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming.","sentences":["Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay.","We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies.","Our approach has three components.","First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space.","This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent.","Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior.","Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast.","This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity.","Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns.","These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications.","Our study advances the alignment discussion in AI and especially generative AI research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming."],"url":"http://arxiv.org/abs/2402.03575v1"}
{"created":"2024-02-05 22:43:57","title":"Diffusion World Model","abstract":"We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\\%$ performance gain, and achieves state-of-the-art performance.","sentences":["We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently.","As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires.","We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM.","In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling.","Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data.","Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation.","In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\\%$ performance gain, and achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.03570v1"}
{"created":"2024-02-05 22:20:19","title":"VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation","abstract":"Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks: Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations. The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset. Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset.","sentences":["Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions.","The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data.","To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance.","VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models.","We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks:","Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations.","The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset.","Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset."],"url":"http://arxiv.org/abs/2402.03561v1"}
{"created":"2024-02-05 22:18:18","title":"Dynamic flux surrogate-based partitioned methods for interface problems","abstract":"Partitioned methods for coupled problems rely on data transfers between subdomains to synchronize the subdomain equations and enable their independent solution. By treating each subproblem as a separate entity, these methods enable code reuse, increase concurrency and provide a convenient framework for plug-and-play multiphysics simulations. However, accuracy and stability of partitioned methods depends critically on the type of information exchanged between the subproblems. The exchange mechanisms can vary from minimally intrusive remap across interfaces to more accurate but also more intrusive and expensive estimates of the necessary information based on monolithic formulations of the coupled system. These transfer mechanisms are separated by accuracy, performance and intrusiveness gaps that tend to limit the scope of the resulting partitioned methods to specific simulation scenarios. Data-driven system identification techniques provide an opportunity to close these gaps by enabling the construction of accurate, computationally efficient and minimally intrusive data transfer surrogates. This approach shifts the principal computational burden to an offline phase, leaving the application of the surrogate as the sole additional cost during the online simulation phase. In this paper we formulate and demonstrate such a \\emph{dynamic flux surrogate-based} partitioned method for a model advection-diffusion transmission problem by using Dynamic Mode Decomposition (DMD) to learn the dynamics of the interface flux from data. The accuracy of the resulting DMD flux surrogate is comparable to that of a dual Schur complement reconstruction, yet its application cost is significantly lower. Numerical results confirm the attractive properties of the new partitioned approach.","sentences":["Partitioned methods for coupled problems rely on data transfers between subdomains to synchronize the subdomain equations and enable their independent solution.","By treating each subproblem as a separate entity, these methods enable code reuse, increase concurrency and provide a convenient framework for plug-and-play multiphysics simulations.","However, accuracy and stability of partitioned methods depends critically on the type of information exchanged between the subproblems.","The exchange mechanisms can vary from minimally intrusive remap across interfaces to more accurate but also more intrusive and expensive estimates of the necessary information based on monolithic formulations of the coupled system.","These transfer mechanisms are separated by accuracy, performance and intrusiveness gaps that tend to limit the scope of the resulting partitioned methods to specific simulation scenarios.","Data-driven system identification techniques provide an opportunity to close these gaps by enabling the construction of accurate, computationally efficient and minimally intrusive data transfer surrogates.","This approach shifts the principal computational burden to an offline phase, leaving the application of the surrogate as the sole additional cost during the online simulation phase.","In this paper we formulate and demonstrate such a \\emph{dynamic flux surrogate-based} partitioned method for a model advection-diffusion transmission problem by using Dynamic Mode Decomposition (DMD) to learn the dynamics of the interface flux from data.","The accuracy of the resulting DMD flux surrogate is comparable to that of a dual Schur complement reconstruction, yet its application cost is significantly lower.","Numerical results confirm the attractive properties of the new partitioned approach."],"url":"http://arxiv.org/abs/2402.03560v1"}
{"created":"2024-02-05 22:18:16","title":"Projected Generative Diffusion Models for Constraint Satisfaction","abstract":"Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generation, trajectory optimization in path planning, and morphometric properties adherence in material science.","sentences":["Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process.","However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges.","This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem.","This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles.","This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations.","These capabilities are demonstrated by physics-informed motion in video generation, trajectory optimization in path planning, and morphometric properties adherence in material science."],"url":"http://arxiv.org/abs/2402.03559v1"}
{"created":"2024-02-05 22:16:05","title":"Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?","abstract":"The path signature, having enjoyed recent success in the machine learning community, is a theoretically-driven method for engineering features from irregular paths. On the other hand, graph neural networks (GNN), neural architectures for processing data on graphs, excel on tasks with irregular domains, such as sensor networks. In this paper, we introduce a novel approach, Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path signatures into graph convolutional neural networks (GCNN), and leveraging the strengths of both path signatures, for feature extraction, and GCNNs, for handling spatial interactions. We apply our method to analyze slow earthquake sequences, also called slow slip events (SSE), utilizing data from GPS timeseries, with a case study on a GPS sensor network on the east coast of New Zealand's north island. We also establish benchmarks for our method on simulated stochastic differential equations, which model similar reaction-diffusion phenomenon. Our methodology shows promise for future advancement in earthquake prediction and sensor network analysis.","sentences":["The path signature, having enjoyed recent success in the machine learning community, is a theoretically-driven method for engineering features from irregular paths.","On the other hand, graph neural networks (GNN), neural architectures for processing data on graphs, excel on tasks with irregular domains, such as sensor networks.","In this paper, we introduce a novel approach, Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path signatures into graph convolutional neural networks (GCNN), and leveraging the strengths of both path signatures, for feature extraction, and GCNNs, for handling spatial interactions.","We apply our method to analyze slow earthquake sequences, also called slow slip events (SSE), utilizing data from GPS timeseries, with a case study on a GPS sensor network on the east coast of New Zealand's north island.","We also establish benchmarks for our method on simulated stochastic differential equations, which model similar reaction-diffusion phenomenon.","Our methodology shows promise for future advancement in earthquake prediction and sensor network analysis."],"url":"http://arxiv.org/abs/2402.03558v1"}
{"created":"2024-02-05 22:14:21","title":"A security framework for Ethereum smart contracts","abstract":"The use of blockchain and smart contracts have not stopped growing in recent years. Like all software that begins to expand its use, it is also beginning to be targeted by hackers who will try to exploit vulnerabilities in both the underlying technology and the smart contract code itself. While many tools already exist for analyzing vulnerabilities in smart contracts, the heterogeneity and variety of approaches and differences in providing the analysis data makes the learning curve for the smart contract developer steep. In this article the authors present ESAF (Ethereum Security Analysis Framework), a framework for analysis of smart contracts that aims to unify and facilitate the task of analyzing smart contract vulnerabilities which can be used as a persistent security monitoring tool for a set of target contracts as well as a classic vulnerability analysis tool among other uses.","sentences":["The use of blockchain and smart contracts have not stopped growing in recent years.","Like all software that begins to expand its use, it is also beginning to be targeted by hackers who will try to exploit vulnerabilities in both the underlying technology and the smart contract code itself.","While many tools already exist for analyzing vulnerabilities in smart contracts, the heterogeneity and variety of approaches and differences in providing the analysis data makes the learning curve for the smart contract developer steep.","In this article the authors present ESAF (Ethereum Security Analysis Framework), a framework for analysis of smart contracts that aims to unify and facilitate the task of analyzing smart contract vulnerabilities which can be used as a persistent security monitoring tool for a set of target contracts as well as a classic vulnerability analysis tool among other uses."],"url":"http://arxiv.org/abs/2402.03555v1"}
{"created":"2024-02-05 22:11:37","title":"A retrospective analysis of Montana's 2020 congressional redistricting map","abstract":"The 2020 decennial census data resulted in an increase from one to two congressional representatives in the state of Montana. The state underwent its redistricting process in 2021 in time for the November 2022 congressional elections, carving the state into two districts. This paper analyzes the redistricting process and compares the adopted congressional map to the space of all other possible maps. In particular, we look at the population deviation, compactness and political outcomes of these maps. We also consider how well two popular sampling techniques, that sample from the space of possible maps, approximate the true distributions of these measures.","sentences":["The 2020 decennial census data resulted in an increase from one to two congressional representatives in the state of Montana.","The state underwent its redistricting process in 2021 in time for the November 2022 congressional elections, carving the state into two districts.","This paper analyzes the redistricting process and compares the adopted congressional map to the space of all other possible maps.","In particular, we look at the population deviation, compactness and political outcomes of these maps.","We also consider how well two popular sampling techniques, that sample from the space of possible maps, approximate the true distributions of these measures."],"url":"http://arxiv.org/abs/2402.03551v1"}
{"created":"2024-02-05 22:03:25","title":"Online Feature Updates Improve Online (Generalized) Label Shift Adaptation","abstract":"This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.","sentences":["This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging.","While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time.","Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model.","Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement.","Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts."],"url":"http://arxiv.org/abs/2402.03545v1"}
{"created":"2024-02-05 21:55:24","title":"HAMLET: Graph Transformer Neural Operator for Partial Differential Equations","abstract":"We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.","sentences":["We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks.","The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process.","This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats.","Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness.","HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains.","Moreover, it boosts model resilience and performance, especially in scenarios with limited data.","We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs."],"url":"http://arxiv.org/abs/2402.03541v1"}
{"created":"2024-02-05 21:43:30","title":"A 0.5V, 6.2$\u03bc$W, 0.059mm$^{2}$ Sinusoidal Current Generator IC with 0.088% THD for Bio-Impedance Sensing","abstract":"This paper presents the first sub-10$\\mu$W, sub-0.1% total harmonic distortion (THD) sinusoidal current generator (CG) integrated circuit (IC) that is capable of 20kHz output for the bio-impedance (Bio-Z) sensing applications. To benefit from the ultra-low-power nature of near-threshold operation, a 9b pseudo-sine lookup table (LUT) is 3b $\\Delta\\Sigma$ modulated in the digital domain, thus linearity burden of the digital-to-analog converter (DAC) is avoided and only a 1.29$\\mu$W of logic power is consumed, from a 0.5V supply and a 2.56MHz clock frequency. A half-period (HP) reset is introduced in the capacitive DAC, leading to around 30dB reduction of in-band noise by avoiding the sampling of data-dependent glitches and attenuating the kT/C noise and the non-idealities of reset switches (SW).","sentences":["This paper presents the first sub-10$\\mu$W, sub-0.1% total harmonic distortion (THD) sinusoidal current generator (CG) integrated circuit (IC) that is capable of 20kHz output for the bio-impedance (Bio-Z) sensing applications.","To benefit from the ultra-low-power nature of near-threshold operation, a 9b pseudo-sine lookup table (LUT) is 3b $\\Delta\\Sigma$ modulated in the digital domain, thus linearity burden of the digital-to-analog converter (DAC) is avoided and only a 1.29$\\mu$W of logic power is consumed, from a 0.5V supply and a 2.56MHz clock frequency.","A half-period (HP) reset is introduced in the capacitive DAC, leading to around 30dB reduction of in-band noise by avoiding the sampling of data-dependent glitches and attenuating the kT/C noise and the non-idealities of reset switches (SW)."],"url":"http://arxiv.org/abs/2402.03533v1"}
{"created":"2024-02-05 21:03:34","title":"Spatially Consistent Air-to-Ground Channel Modeling via Generative Neural Networks","abstract":"This article proposes a generative neural network architecture for spatially consistent air-to-ground channel modeling. The approach considers the trajectories of uncrewed aerial vehicles along typical urban paths, capturing spatial dependencies within received signal strength (RSS) sequences from multiple cellular base stations (gNBs). Through the incorporation of conditioning data, the model accurately discriminates between gNBs and drives the correlation matrix distance between real and generated sequences to minimal values. This enables evaluating performance and mobility management metrics with spatially (and by extension temporally) consistent RSS values, rather than independent snapshots. For some tasks underpinned by these metrics, say handovers, consistency is essential.","sentences":["This article proposes a generative neural network architecture for spatially consistent air-to-ground channel modeling.","The approach considers the trajectories of uncrewed aerial vehicles along typical urban paths, capturing spatial dependencies within received signal strength (RSS) sequences from multiple cellular base stations (gNBs).","Through the incorporation of conditioning data, the model accurately discriminates between gNBs and drives the correlation matrix distance between real and generated sequences to minimal values.","This enables evaluating performance and mobility management metrics with spatially (and by extension temporally) consistent RSS values, rather than independent snapshots.","For some tasks underpinned by these metrics, say handovers, consistency is essential."],"url":"http://arxiv.org/abs/2402.03517v1"}
{"created":"2024-02-05 20:48:57","title":"Neural networks for abstraction and reasoning: Towards broad generalization in machines","abstract":"For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition. We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation.We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks. LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches. We perform an ensemble analysis, combining models to achieve better results than any system alone. Finally, we publish the arckit Python library to make future research on ARC easier.","sentences":["For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy.","While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.","In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization.","Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all.","We revisit whether recent advances in neural networks allow progress on this task.   ","First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.","DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition.","We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation.","We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks.","LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches.","We perform an ensemble analysis, combining models to achieve better results than any system alone.","Finally, we publish the arckit Python library to make future research on ARC easier."],"url":"http://arxiv.org/abs/2402.03507v1"}
{"created":"2024-02-05 20:36:33","title":"How Does Unlabeled Data Provably Help Out-of-Distribution Detection?","abstract":"Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our theory shows that SAL can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned OOD classifier. Empirically, SAL achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights. Code is publicly available at https://github.com/deeplearning-wisc/sal.","sentences":["Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data.","Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data.","This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier.","Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection.","This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness.","The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data.","Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm.","Our theory shows that SAL can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned OOD classifier.","Empirically, SAL achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights.","Code is publicly available at https://github.com/deeplearning-wisc/sal."],"url":"http://arxiv.org/abs/2402.03502v1"}
{"created":"2024-02-05 19:58:40","title":"Early prediction of onset of sepsis in Clinical Setting","abstract":"This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA. A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\\% of the train dataset, encompassing 107 features (including the original and derived features). Subsequently, the model was evaluated on the remaining 20\\% of the test data. The model was validated on prospective data that was entirely unseen during the training phase. To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised. The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3. The F1 scores were 80.8\\% and 67.1\\% respectively for the test data and the prospective data for the same threshold, highlighting its potential to be integrated into clinical decision-making processes effectively. These results bear testament to the model's robust predictive capabilities and its potential to substantially impact clinical decision-making processes.","sentences":["This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA.","A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\\% of the train dataset, encompassing 107 features (including the original and derived features).","Subsequently, the model was evaluated on the remaining 20\\% of the test data.","The model was validated on prospective data that was entirely unseen during the training phase.","To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper.","Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised.","The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3.","The F1 scores were 80.8\\% and 67.1\\% respectively for the test data and the prospective data for the same threshold, highlighting its potential to be integrated into clinical decision-making processes effectively.","These results bear testament to the model's robust predictive capabilities and its potential to substantially impact clinical decision-making processes."],"url":"http://arxiv.org/abs/2402.03486v1"}
