{"created":"2025-04-21 17:59:55","title":"StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians","abstract":"3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art.","sentences":["3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics.","We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement.","Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment.","StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony.","Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering.","This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art."],"url":"http://arxiv.org/abs/2504.15281v1"}
{"created":"2025-04-21 17:59:53","title":"VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models","abstract":"Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning. Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress.","sentences":["Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models.","Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning.","To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons).","These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives.","We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes.","Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning.","Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress."],"url":"http://arxiv.org/abs/2504.15279v1"}
{"created":"2025-04-21 17:59:49","title":"DRAWER: Digital Reconstruction and Articulation With Environment Realism","abstract":"Creating virtual digital replicas from real-world data unlocks significant potential across domains like gaming and robotics. In this paper, we present DRAWER, a novel framework that converts a video of a static indoor scene into a photorealistic and interactive digital environment. Our approach centers on two main contributions: (i) a reconstruction module based on a dual scene representation that reconstructs the scene with fine-grained geometric details, and (ii) an articulation module that identifies articulation types and hinge positions, reconstructs simulatable shapes and appearances and integrates them into the scene. The resulting virtual environment is photorealistic, interactive, and runs in real time, with compatibility for game engines and robotic simulation platforms. We demonstrate the potential of DRAWER by using it to automatically create an interactive game in Unreal Engine and to enable real-to-sim-to-real transfer for robotics applications.","sentences":["Creating virtual digital replicas from real-world data unlocks significant potential across domains like gaming and robotics.","In this paper, we present DRAWER, a novel framework that converts a video of a static indoor scene into a photorealistic and interactive digital environment.","Our approach centers on two main contributions: (i) a reconstruction module based on a dual scene representation that reconstructs the scene with fine-grained geometric details, and (ii) an articulation module that identifies articulation types and hinge positions, reconstructs simulatable shapes and appearances and integrates them into the scene.","The resulting virtual environment is photorealistic, interactive, and runs in real time, with compatibility for game engines and robotic simulation platforms.","We demonstrate the potential of DRAWER by using it to automatically create an interactive game in Unreal Engine and to enable real-to-sim-to-real transfer for robotics applications."],"url":"http://arxiv.org/abs/2504.15278v1"}
{"created":"2025-04-21 17:57:28","title":"Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models","abstract":"We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B.","sentences":["We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning.","Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks.","The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details.","The framework also includes numerous efficiency optimizations in the pipeline for long-context data training.","Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding.","Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs.","Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B."],"url":"http://arxiv.org/abs/2504.15271v1"}
{"created":"2025-04-21 17:49:06","title":"Diffusion Bridge Models for 3D Medical Image Translation","abstract":"Diffusion tensor imaging (DTI) provides crucial insights into the microstructure of the human brain, but it can be time-consuming to acquire compared to more readily available T1-weighted (T1w) magnetic resonance imaging (MRI). To address this challenge, we propose a diffusion bridge model for 3D brain image translation between T1w MRI and DTI modalities. Our model learns to generate high-quality DTI fractional anisotropy (FA) images from T1w images and vice versa, enabling cross-modality data augmentation and reducing the need for extensive DTI acquisition. We evaluate our approach using perceptual similarity, pixel-level agreement, and distributional consistency metrics, demonstrating strong performance in capturing anatomical structures and preserving information on white matter integrity. The practical utility of the synthetic data is validated through sex classification and Alzheimer's disease classification tasks, where the generated images achieve comparable performance to real data. Our diffusion bridge model offers a promising solution for improving neuroimaging datasets and supporting clinical decision-making, with the potential to significantly impact neuroimaging research and clinical practice.","sentences":["Diffusion tensor imaging (DTI) provides crucial insights into the microstructure of the human brain, but it can be time-consuming to acquire compared to more readily available T1-weighted (T1w) magnetic resonance imaging (MRI).","To address this challenge, we propose a diffusion bridge model for 3D brain image translation between T1w MRI and DTI modalities.","Our model learns to generate high-quality DTI fractional anisotropy (FA) images from T1w images and vice versa, enabling cross-modality data augmentation and reducing the need for extensive DTI acquisition.","We evaluate our approach using perceptual similarity, pixel-level agreement, and distributional consistency metrics, demonstrating strong performance in capturing anatomical structures and preserving information on white matter integrity.","The practical utility of the synthetic data is validated through sex classification and Alzheimer's disease classification tasks, where the generated images achieve comparable performance to real data.","Our diffusion bridge model offers a promising solution for improving neuroimaging datasets and supporting clinical decision-making, with the potential to significantly impact neuroimaging research and clinical practice."],"url":"http://arxiv.org/abs/2504.15267v1"}
{"created":"2025-04-21 17:45:21","title":"Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning","abstract":"Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries.","sentences":["Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers.","Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes.","This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings.","Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively.","Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands.","Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.","This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries."],"url":"http://arxiv.org/abs/2504.15263v1"}
{"created":"2025-04-21 17:41:15","title":"Leveraging Language Models for Automated Patient Record Linkage","abstract":"Objective: Healthcare data fragmentation presents a major challenge for linking patient data, necessitating robust record linkage to integrate patient records from diverse sources. This study investigates the feasibility of leveraging language models for automated patient record linkage, focusing on two key tasks: blocking and matching. Materials and Methods: We utilized real-world healthcare data from the Missouri Cancer Registry and Research Center, linking patient records from two independent sources using probabilistic linkage as a baseline. A transformer-based model, RoBERTa, was fine-tuned for blocking using sentence embeddings. For matching, several language models were experimented under fine-tuned and zero-shot settings, assessing their performance against ground truth labels. Results: The fine-tuned blocking model achieved a 92% reduction in the number of candidate pairs while maintaining near-perfect recall. In the matching task, fine-tuned Mistral-7B achieved the best performance with only 6 incorrect predictions. Among zero-shot models, Mistral-Small-24B performed best, with a total of 55 incorrect predictions. Discussion: Fine-tuned language models achieved strong performance in patient record blocking and matching with minimal errors. However, they remain less accurate and efficient than a hybrid rule-based and probabilistic approach for blocking. Additionally, reasoning models like DeepSeek-R1 are impractical for large-scale record linkage due to high computational costs. Conclusion: This study highlights the potential of language models for automating patient record linkage, offering improved efficiency by eliminating the manual efforts required to perform patient record linkage. Overall, language models offer a scalable solution that can enhance data integration, reduce manual effort, and support disease surveillance and research.","sentences":["Objective: Healthcare data fragmentation presents a major challenge for linking patient data, necessitating robust record linkage to integrate patient records from diverse sources.","This study investigates the feasibility of leveraging language models for automated patient record linkage, focusing on two key tasks: blocking and matching.","Materials and Methods: We utilized real-world healthcare data from the Missouri Cancer Registry and Research Center, linking patient records from two independent sources using probabilistic linkage as a baseline.","A transformer-based model, RoBERTa, was fine-tuned for blocking using sentence embeddings.","For matching, several language models were experimented under fine-tuned and zero-shot settings, assessing their performance against ground truth labels.","Results:","The fine-tuned blocking model achieved a 92% reduction in the number of candidate pairs while maintaining near-perfect recall.","In the matching task, fine-tuned Mistral-7B achieved the best performance with only 6 incorrect predictions.","Among zero-shot models, Mistral-Small-24B performed best, with a total of 55 incorrect predictions.","Discussion: Fine-tuned language models achieved strong performance in patient record blocking and matching with minimal errors.","However, they remain less accurate and efficient than a hybrid rule-based and probabilistic approach for blocking.","Additionally, reasoning models like DeepSeek-R1 are impractical for large-scale record linkage due to high computational costs.","Conclusion:","This study highlights the potential of language models for automating patient record linkage, offering improved efficiency by eliminating the manual efforts required to perform patient record linkage.","Overall, language models offer a scalable solution that can enhance data integration, reduce manual effort, and support disease surveillance and research."],"url":"http://arxiv.org/abs/2504.15261v1"}
{"created":"2025-04-21 17:38:50","title":"Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation","abstract":"Digital modeling and reconstruction of human faces serve various applications. However, its availability is often hindered by the requirements of data capturing devices, manual labor, and suitable actors. This situation restricts the diversity, expressiveness, and control over the resulting models. This work aims to demonstrate that a semantically controllable generative network can provide enhanced control over the digital face modeling process. To enhance diversity beyond the limited human faces scanned in a controlled setting, we introduce a novel data generation pipeline that creates a high-quality 3D face database using a pre-trained diffusion model. Our proposed normalization module converts synthesized data from the diffusion model into high-quality scanned data. Using the 44,000 face models we obtained, we further developed an efficient GAN-based generator. This generator accepts semantic attributes as input, and generates geometry and albedo. It also allows continuous post-editing of attributes in the latent space. Our asset refinement component subsequently creates physically-based facial assets. We introduce a comprehensive system designed for creating and editing high-quality face assets. Our proposed model has undergone extensive experiment, comparison and evaluation. We also integrate everything into a web-based interactive tool. We aim to make this tool publicly available with the release of the paper.","sentences":["Digital modeling and reconstruction of human faces serve various applications.","However, its availability is often hindered by the requirements of data capturing devices, manual labor, and suitable actors.","This situation restricts the diversity, expressiveness, and control over the resulting models.","This work aims to demonstrate that a semantically controllable generative network can provide enhanced control over the digital face modeling process.","To enhance diversity beyond the limited human faces scanned in a controlled setting, we introduce a novel data generation pipeline that creates a high-quality 3D face database using a pre-trained diffusion model.","Our proposed normalization module converts synthesized data from the diffusion model into high-quality scanned data.","Using the 44,000 face models we obtained, we further developed an efficient GAN-based generator.","This generator accepts semantic attributes as input, and generates geometry and albedo.","It also allows continuous post-editing of attributes in the latent space.","Our asset refinement component subsequently creates physically-based facial assets.","We introduce a comprehensive system designed for creating and editing high-quality face assets.","Our proposed model has undergone extensive experiment, comparison and evaluation.","We also integrate everything into a web-based interactive tool.","We aim to make this tool publicly available with the release of the paper."],"url":"http://arxiv.org/abs/2504.15259v1"}
{"created":"2025-04-21 17:33:02","title":"SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam","abstract":"Understanding and monitoring aquatic biodiversity is critical for ecological health and conservation efforts. This paper proposes SuoiAI, an end-to-end pipeline for building a dataset of aquatic invertebrates in Vietnam and employing machine learning (ML) techniques for species classification. We outline the methods for data collection, annotation, and model training, focusing on reducing annotation effort through semi-supervised learning and leveraging state-of-the-art object detection and classification models. Our approach aims to overcome challenges such as data scarcity, fine-grained classification, and deployment in diverse environmental conditions.","sentences":["Understanding and monitoring aquatic biodiversity is critical for ecological health and conservation efforts.","This paper proposes SuoiAI, an end-to-end pipeline for building a dataset of aquatic invertebrates in Vietnam and employing machine learning (ML) techniques for species classification.","We outline the methods for data collection, annotation, and model training, focusing on reducing annotation effort through semi-supervised learning and leveraging state-of-the-art object detection and classification models.","Our approach aims to overcome challenges such as data scarcity, fine-grained classification, and deployment in diverse environmental conditions."],"url":"http://arxiv.org/abs/2504.15252v1"}
{"created":"2025-04-21 17:31:55","title":"On Learning Parallel Pancakes with Mostly Uniform Weights","abstract":"We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on $\\mathbb{R}^d$. This task is known to have complexity $d^{\\Omega(k)}$ in full generality. To circumvent this exponential lower bound on the number of components, research has focused on learning families of GMMs satisfying additional structural properties. A natural assumption posits that the component weights are not exponentially small and that the components have the same unknown covariance. Recent work gave a $d^{O(\\log(1/w_{\\min}))}$-time algorithm for this class of GMMs, where $w_{\\min}$ is the minimum weight. Our first main result is a Statistical Query (SQ) lower bound showing that this quasi-polynomial upper bound is essentially best possible, even for the special case of uniform weights. Specifically, we show that it is SQ-hard to distinguish between such a mixture and the standard Gaussian. We further explore how the distribution of weights affects the complexity of this task. Our second main result is a quasi-polynomial upper bound for the aforementioned testing task when most of the weights are uniform while a small fraction of the weights are potentially arbitrary.","sentences":["We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on $\\mathbb{R}^d$. This task is known to have complexity $d^{\\Omega(k)}$ in full generality.","To circumvent this exponential lower bound on the number of components, research has focused on learning families of GMMs satisfying additional structural properties.","A natural assumption posits that the component weights are not exponentially small and that the components have the same unknown covariance.","Recent work gave a $d^{O(\\log(1/w_{\\min}))}$-time algorithm for this class of GMMs, where $w_{\\min}$ is the minimum weight.","Our first main result is a Statistical Query (SQ) lower bound showing that this quasi-polynomial upper bound is essentially best possible, even for the special case of uniform weights.","Specifically, we show that it is SQ-hard to distinguish between such a mixture and the standard Gaussian.","We further explore how the distribution of weights affects the complexity of this task.","Our second main result is a quasi-polynomial upper bound for the aforementioned testing task when most of the weights are uniform while a small fraction of the weights are potentially arbitrary."],"url":"http://arxiv.org/abs/2504.15251v1"}
{"created":"2025-04-21 17:22:18","title":"Lance: Efficient Random Access in Columnar Storage through Adaptive Structural Encodings","abstract":"The growing interest in artificial intelligence has created workloads that require both sequential and random access. At the same time, NVMe-backed storage solutions have emerged, providing caching capability for large columnar datasets in cloud storage. Current columnar storage libraries fall short of effectively utilizing an NVMe device's capabilities, especially when it comes to random access. Historically, this has been assumed an implicit weakness in columnar storage formats, but this has not been sufficiently explored. In this paper, we examine the effectiveness of popular columnar formats such as Apache Arrow, Apache Parquet, and Lance in both random access and full scan tasks against NVMe storage.   We argue that effective encoding of a column's structure, such as the repetition and validity information, is the key to unlocking the disk's performance. We show that Parquet, when configured correctly, can achieve over 60x better random access performance than default settings. We also show that this high random access performance requires making minor trade-offs in scan performance and RAM utilization. We then describe the Lance structural encoding scheme, which alternates between two different structural encodings based on data width, and achieves better random access performance without making trade-offs in scan performance or RAM utilization.","sentences":["The growing interest in artificial intelligence has created workloads that require both sequential and random access.","At the same time, NVMe-backed storage solutions have emerged, providing caching capability for large columnar datasets in cloud storage.","Current columnar storage libraries fall short of effectively utilizing an NVMe device's capabilities, especially when it comes to random access.","Historically, this has been assumed an implicit weakness in columnar storage formats, but this has not been sufficiently explored.","In this paper, we examine the effectiveness of popular columnar formats such as Apache Arrow, Apache Parquet, and Lance in both random access and full scan tasks against NVMe storage.   ","We argue that effective encoding of a column's structure, such as the repetition and validity information, is the key to unlocking the disk's performance.","We show that Parquet, when configured correctly, can achieve over 60x better random access performance than default settings.","We also show that this high random access performance requires making minor trade-offs in scan performance and RAM utilization.","We then describe the Lance structural encoding scheme, which alternates between two different structural encodings based on data width, and achieves better random access performance without making trade-offs in scan performance or RAM utilization."],"url":"http://arxiv.org/abs/2504.15247v1"}
{"created":"2025-04-21 17:19:57","title":"A Refreshment Stirred, Not Shaken (III): Can Swapping Be Differentially Private?","abstract":"The quest for a precise and contextually grounded answer to the question in the present paper's title resulted in this stirred-not-shaken triptych, a phrase that reflects our desire to deepen the theoretical basis, broaden the practical applicability, and reduce the misperception of differential privacy (DP)$\\unicode{x2014}$all without shaking its core foundations. Indeed, given the existence of more than 200 formulations of DP (and counting), before even attempting to answer the titular question one must first precisely specify what it actually means to be DP. Motivated by this observation, a theoretical investigation into DP's fundamental essence resulted in Part I of this trio, which introduces a five-building-block system explicating the who, where, what, how and how much aspects of DP. Instantiating this system in the context of the United States Decennial Census, Part II then demonstrates the broader applicability and relevance of DP by comparing a swapping strategy like that used in 2010 with the TopDown Algorithm$\\unicode{x2014}$a DP method adopted in the 2020 Census. This paper provides nontechnical summaries of the preceding two parts as well as new discussion$\\unicode{x2014}$for example, on how greater awareness of the five building blocks can thwart privacy theatrics; how our results bridging traditional SDC and DP allow a data custodian to reap the benefits of both these fields; how invariants impact disclosure risk; and how removing the implicit reliance on aleatoric uncertainty could lead to new generalizations of DP.","sentences":["The quest for a precise and contextually grounded answer to the question in the present paper's title resulted in this stirred-not-shaken triptych, a phrase that reflects our desire to deepen the theoretical basis, broaden the practical applicability, and reduce the misperception of differential privacy (DP)$\\unicode{x2014}$all without shaking its core foundations.","Indeed, given the existence of more than 200 formulations of DP (and counting), before even attempting to answer the titular question one must first precisely specify what it actually means to be DP.","Motivated by this observation, a theoretical investigation into DP's fundamental essence resulted in Part I of this trio, which introduces a five-building-block system explicating the who, where, what, how and how much aspects of DP.","Instantiating this system in the context of the United States Decennial Census, Part II then demonstrates the broader applicability and relevance of DP by comparing a swapping strategy like that used in 2010 with the TopDown Algorithm$\\unicode{x2014}$a DP method adopted in the 2020 Census.","This paper provides nontechnical summaries of the preceding two parts as well as new discussion$\\unicode{x2014}$for example, on how greater awareness of the five building blocks can thwart privacy theatrics; how our results bridging traditional SDC and DP allow a data custodian to reap the benefits of both these fields; how invariants impact disclosure risk; and how removing the implicit reliance on aleatoric uncertainty could lead to new generalizations of DP."],"url":"http://arxiv.org/abs/2504.15246v1"}
{"created":"2025-04-21 17:16:14","title":"Faster Algorithms for Agnostically Learning Disjunctions and their Implications","abstract":"We study the algorithmic task of learning Boolean disjunctions in the distribution-free agnostic PAC model. The best known agnostic learner for the class of disjunctions over $\\{0, 1\\}^n$ is the $L_1$-polynomial regression algorithm, achieving complexity $2^{\\tilde{O}(n^{1/2})}$. This complexity bound is known to be nearly best possible within the class of Correlational Statistical Query (CSQ) algorithms. In this work, we develop an agnostic learner for this concept class with complexity $2^{\\tilde{O}(n^{1/3})}$. Our algorithm can be implemented in the Statistical Query (SQ) model, providing the first separation between the SQ and CSQ models in distribution-free agnostic learning.","sentences":["We study the algorithmic task of learning Boolean disjunctions in the distribution-free agnostic PAC model.","The best known agnostic learner for the class of disjunctions over $\\{0, 1\\}^n$ is the $L_1$-polynomial regression algorithm, achieving complexity $2^{\\tilde{O}(n^{1/2})}$. This complexity bound is known to be nearly best possible within the class of Correlational Statistical Query (CSQ) algorithms.","In this work, we develop an agnostic learner for this concept class with complexity $2^{\\tilde{O}(n^{1/3})}$. Our algorithm can be implemented in the Statistical Query (SQ) model, providing the first separation between the SQ and CSQ models in distribution-free agnostic learning."],"url":"http://arxiv.org/abs/2504.15244v1"}
{"created":"2025-04-21 17:15:06","title":"MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning","abstract":"Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation.","sentences":["Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors.","This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited.","Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications.","In this work, we propose an approach to build a multilingual guardrail with reasoning.","Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance.","Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages.","The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation."],"url":"http://arxiv.org/abs/2504.15241v1"}
{"created":"2025-04-21 16:57:46","title":"M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global Scoring and Calibrated Thresholding","abstract":"With the widespread availability of sensor data across industrial and operational systems, we frequently encounter heterogeneous time series from multiple systems. Anomaly detection is crucial for such systems to facilitate predictive maintenance. However, most existing anomaly detection methods are designed for either univariate or single-system multivariate data, making them insufficient for these complex scenarios. To address this, we introduce M$^2$AD, a framework for unsupervised anomaly detection in multivariate time series data from multiple systems. M$^2$AD employs deep models to capture expected behavior under normal conditions, using the residuals as indicators of potential anomalies. These residuals are then aggregated into a global anomaly score through a Gaussian Mixture Model and Gamma calibration. We theoretically demonstrate that this framework can effectively address heterogeneity and dependencies across sensors and systems. Empirically, M$^2$AD outperforms existing methods in extensive evaluations by 21% on average, and its effectiveness is demonstrated on a large-scale real-world case study on 130 assets in Amazon Fulfillment Centers. Our code and results are available at https://github.com/sarahmish/M2AD.","sentences":["With the widespread availability of sensor data across industrial and operational systems, we frequently encounter heterogeneous time series from multiple systems.","Anomaly detection is crucial for such systems to facilitate predictive maintenance.","However, most existing anomaly detection methods are designed for either univariate or single-system multivariate data, making them insufficient for these complex scenarios.","To address this, we introduce M$^2$AD, a framework for unsupervised anomaly detection in multivariate time series data from multiple systems.","M$^2$AD employs deep models to capture expected behavior under normal conditions, using the residuals as indicators of potential anomalies.","These residuals are then aggregated into a global anomaly score through a Gaussian Mixture Model and Gamma calibration.","We theoretically demonstrate that this framework can effectively address heterogeneity and dependencies across sensors and systems.","Empirically, M$^2$AD outperforms existing methods in extensive evaluations by 21% on average, and its effectiveness is demonstrated on a large-scale real-world case study on 130 assets in Amazon Fulfillment Centers.","Our code and results are available at https://github.com/sarahmish/M2AD."],"url":"http://arxiv.org/abs/2504.15225v1"}
{"created":"2025-04-21 16:53:02","title":"A Deep Learning Framework for Sequence Mining with Bidirectional LSTM and Multi-Scale Attention","abstract":"This paper addresses the challenges of mining latent patterns and modeling contextual dependencies in complex sequence data. A sequence pattern mining algorithm is proposed by integrating Bidirectional Long Short-Term Memory (BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both forward and backward dependencies in sequences, enhancing the model's ability to perceive global contextual structures. At the same time, the multi-scale attention module assigns adaptive weights to key feature regions under different window sizes. This improves the model's responsiveness to both local and global important information. Extensive experiments are conducted on a publicly available multivariate time series dataset. The proposed model is compared with several mainstream sequence modeling methods. Results show that it outperforms existing models in terms of accuracy, precision, and recall. This confirms the effectiveness and robustness of the proposed architecture in complex pattern recognition tasks. Further ablation studies and sensitivity analyses are carried out to investigate the effects of attention scale and input sequence length on model performance. These results provide empirical support for structural optimization of the model.","sentences":["This paper addresses the challenges of mining latent patterns and modeling contextual dependencies in complex sequence data.","A sequence pattern mining algorithm is proposed by integrating Bidirectional Long Short-Term Memory (BiLSTM) with a multi-scale attention mechanism.","The BiLSTM captures both forward and backward dependencies in sequences, enhancing the model's ability to perceive global contextual structures.","At the same time, the multi-scale attention module assigns adaptive weights to key feature regions under different window sizes.","This improves the model's responsiveness to both local and global important information.","Extensive experiments are conducted on a publicly available multivariate time series dataset.","The proposed model is compared with several mainstream sequence modeling methods.","Results show that it outperforms existing models in terms of accuracy, precision, and recall.","This confirms the effectiveness and robustness of the proposed architecture in complex pattern recognition tasks.","Further ablation studies and sensitivity analyses are carried out to investigate the effects of attention scale and input sequence length on model performance.","These results provide empirical support for structural optimization of the model."],"url":"http://arxiv.org/abs/2504.15223v1"}
{"created":"2025-04-21 16:31:15","title":"Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI","abstract":"The evaluation of Generative AI (GenAI) systems plays a critical role in public policy and decision-making, yet existing methods are often limited by reliance on benchmark-driven, point-estimate comparisons that fail to capture uncertainty and broader societal impacts. This paper argues for the use of Bayesian statistics as a principled framework to address these challenges. Bayesian methods enable the integration of domain expertise through prior elicitation, allow for continuous learning from new data, and provide robust uncertainty quantification via posterior inference. We demonstrate how Bayesian inference can be applied to GenAI evaluation, particularly in incorporating stakeholder perspectives to enhance fairness, transparency, and reliability. Furthermore, we discuss Bayesian workflows as an iterative process for model validation and refinement, ensuring robust assessments of GenAI systems in dynamic, real-world contexts.","sentences":["The evaluation of Generative AI (GenAI) systems plays a critical role in public policy and decision-making, yet existing methods are often limited by reliance on benchmark-driven, point-estimate comparisons that fail to capture uncertainty and broader societal impacts.","This paper argues for the use of Bayesian statistics as a principled framework to address these challenges.","Bayesian methods enable the integration of domain expertise through prior elicitation, allow for continuous learning from new data, and provide robust uncertainty quantification via posterior inference.","We demonstrate how Bayesian inference can be applied to GenAI evaluation, particularly in incorporating stakeholder perspectives to enhance fairness, transparency, and reliability.","Furthermore, we discuss Bayesian workflows as an iterative process for model validation and refinement, ensuring robust assessments of GenAI systems in dynamic, real-world contexts."],"url":"http://arxiv.org/abs/2504.15211v1"}
{"created":"2025-04-21 16:29:07","title":"Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs","abstract":"Code-generating Large Language Models (LLMs) have become essential tools in modern software development, enhancing productivity and accelerating development. This paper aims to investigate the fine-tuning of code-generating LLMs using Reinforcement Learning and Direct Preference Optimization, further improving their performance. To achieve this, we enhance the training data for the reward model with the help of symbolic execution techniques, ensuring more comprehensive and objective data. With symbolic execution, we create a custom dataset that better captures the nuances in code evaluation. Our reward models, fine-tuned on this dataset, demonstrate significant improvements over the baseline, CodeRL, in estimating the quality of generated code. Our code-generating LLMs, trained with the help of reward model feedback, achieve similar results compared to the CodeRL benchmark.","sentences":["Code-generating Large Language Models (LLMs) have become essential tools in modern software development, enhancing productivity and accelerating development.","This paper aims to investigate the fine-tuning of code-generating LLMs using Reinforcement Learning and Direct Preference Optimization, further improving their performance.","To achieve this, we enhance the training data for the reward model with the help of symbolic execution techniques, ensuring more comprehensive and objective data.","With symbolic execution, we create a custom dataset that better captures the nuances in code evaluation.","Our reward models, fine-tuned on this dataset, demonstrate significant improvements over the baseline, CodeRL, in estimating the quality of generated code.","Our code-generating LLMs, trained with the help of reward model feedback, achieve similar results compared to the CodeRL benchmark."],"url":"http://arxiv.org/abs/2504.15210v1"}
{"created":"2025-04-21 16:27:16","title":"A Causal Convolutional Low-rank Representation Model for Imputation of Water Quality Data","abstract":"The monitoring of water quality is a crucial part of environmental protection, and a large number of monitors are widely deployed to monitor water quality. Due to unavoidable factors such as data acquisition breakdowns, sensors and communication failures, water quality monitoring data suffers from missing values over time, resulting in High-Dimensional and Sparse (HDS) Water Quality Data (WQD). The simple and rough filling of the missing values leads to inaccurate results and affects the implementation of relevant measures. Therefore, this paper proposes a Causal convolutional Low-rank Representation (CLR) model for imputing missing WQD to improve the completeness of the WQD, which employs a two-fold idea: a) applying causal convolutional operation to consider the temporal dependence of the low-rank representation, thus incorporating temporal information to improve the imputation accuracy; and b) implementing a hyperparameters adaptation scheme to automatically adjust the best hyperparameters during model training, thereby reducing the tedious manual adjustment of hyper-parameters. Experimental studies on three real-world water quality datasets demonstrate that the proposed CLR model is superior to some of the existing state-of-the-art imputation models in terms of imputation accuracy and time cost, as well as indicating that the proposed model provides more reliable decision support for environmental monitoring.","sentences":["The monitoring of water quality is a crucial part of environmental protection, and a large number of monitors are widely deployed to monitor water quality.","Due to unavoidable factors such as data acquisition breakdowns, sensors and communication failures, water quality monitoring data suffers from missing values over time, resulting in High-Dimensional and Sparse (HDS)","Water Quality Data (WQD).","The simple and rough filling of the missing values leads to inaccurate results and affects the implementation of relevant measures.","Therefore, this paper proposes a Causal convolutional Low-rank Representation (CLR) model for imputing missing WQD to improve the completeness of the WQD, which employs a two-fold idea: a) applying causal convolutional operation to consider the temporal dependence of the low-rank representation, thus incorporating temporal information to improve the imputation accuracy; and b) implementing a hyperparameters adaptation scheme to automatically adjust the best hyperparameters during model training, thereby reducing the tedious manual adjustment of hyper-parameters.","Experimental studies on three real-world water quality datasets demonstrate that the proposed CLR model is superior to some of the existing state-of-the-art imputation models in terms of imputation accuracy and time cost, as well as indicating that the proposed model provides more reliable decision support for environmental monitoring."],"url":"http://arxiv.org/abs/2504.15209v1"}
{"created":"2025-04-21 16:26:56","title":"Compute-Optimal LLMs Provably Generalize Better With Scale","abstract":"Why do larger language models generalize better? To investigate this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. This generalization bound can be decomposed into three interpretable components: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As compute-optimal language models are scaled up, the number of parameters per data point remains constant; however, both the loss variance and the quantization error decrease, implying that larger models should have smaller generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows more slowly than their capacity on the compute-optimal frontier. From these findings we produce a scaling law for the generalization gap, with bounds that become predictably stronger with scale.","sentences":["Why do larger language models generalize better?","To investigate this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws.","We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function.","This generalization bound can be decomposed into three interpretable components: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate.","As compute-optimal language models are scaled up, the number of parameters per data point remains constant; however, both the loss variance and the quantization error decrease, implying that larger models should have smaller generalization gaps.","We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows more slowly than their capacity on the compute-optimal frontier.","From these findings we produce a scaling law for the generalization gap, with bounds that become predictably stronger with scale."],"url":"http://arxiv.org/abs/2504.15208v1"}
{"created":"2025-04-21 16:02:26","title":"Automated Measurement of Eczema Severity with Self-Supervised Learning","abstract":"Automated diagnosis of eczema using images acquired from digital camera can enable individuals to self-monitor their recovery. The process entails first segmenting out the eczema region from the image and then measuring the severity of eczema in the segmented region. The state-of-the-art methods for automated eczema diagnosis rely on deep neural networks such as convolutional neural network (CNN) and have shown impressive performance in accurately measuring the severity of eczema. However, these methods require massive volume of annotated data to train which can be hard to obtain. In this paper, we propose a self-supervised learning framework for automated eczema diagnosis under limited training data regime. Our framework consists of two stages: i) Segmentation, where we use an in-context learning based algorithm called SegGPT for few-shot segmentation of eczema region from the image; ii) Feature extraction and classification, where we extract DINO features from the segmented regions and feed it to a multi-layered perceptron (MLP) for 4-class classification of eczema severity. When evaluated on a dataset of annotated \"in-the-wild\" eczema images, we show that our method outperforms (Weighted F1: 0.67 $\\pm$ 0.01) the state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted F1: 0.44 $\\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\\pm$ 0.22). Our results show that self-supervised learning can be a viable solution for automated skin diagnosis where labeled data is scarce.","sentences":["Automated diagnosis of eczema using images acquired from digital camera can enable individuals to self-monitor their recovery.","The process entails first segmenting out the eczema region from the image and then measuring the severity of eczema in the segmented region.","The state-of-the-art methods for automated eczema diagnosis rely on deep neural networks such as convolutional neural network (CNN) and have shown impressive performance in accurately measuring the severity of eczema.","However, these methods require massive volume of annotated data to train which can be hard to obtain.","In this paper, we propose a self-supervised learning framework for automated eczema diagnosis under limited training data regime.","Our framework consists of two stages: i) Segmentation, where we use an in-context learning based algorithm called SegGPT for few-shot segmentation of eczema region from the image; ii) Feature extraction and classification, where we extract DINO features from the segmented regions and feed it to a multi-layered perceptron (MLP) for 4-class classification of eczema severity.","When evaluated on a dataset of annotated \"in-the-wild\" eczema images, we show that our method outperforms (Weighted F1: 0.67 $\\pm$ 0.01) the state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted F1: 0.44 $\\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\\pm$ 0.22).","Our results show that self-supervised learning can be a viable solution for automated skin diagnosis where labeled data is scarce."],"url":"http://arxiv.org/abs/2504.15193v1"}
{"created":"2025-04-21 15:44:06","title":"Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform","abstract":"The recent surge in open-source text-to-video generation models has significantly energized the research community, yet their dependence on proprietary training datasets remains a key constraint. While existing open datasets like Koala-36M employ algorithmic filtering of web-scraped videos from early platforms, they still lack the quality required for fine-tuning advanced video generation models. We present Tiger200K, a manually curated high visual quality video dataset sourced from User-Generated Content (UGC) platforms. By prioritizing visual fidelity and aesthetic quality, Tiger200K underscores the critical role of human expertise in data curation, and providing high-quality, temporally consistent video-text pairs for fine-tuning and optimizing video generation architectures through a simple but effective pipeline including shot boundary detection, OCR, border detecting, motion filter and fine bilingual caption. The dataset will undergo ongoing expansion and be released as an open-source initiative to advance research and applications in video generative models. Project page: https://tinytigerpan.github.io/tiger200k/","sentences":["The recent surge in open-source text-to-video generation models has significantly energized the research community, yet their dependence on proprietary training datasets remains a key constraint.","While existing open datasets like Koala-36M employ algorithmic filtering of web-scraped videos from early platforms, they still lack the quality required for fine-tuning advanced video generation models.","We present Tiger200K, a manually curated high visual quality video dataset sourced from User-Generated Content (UGC) platforms.","By prioritizing visual fidelity and aesthetic quality, Tiger200K underscores the critical role of human expertise in data curation, and providing high-quality, temporally consistent video-text pairs for fine-tuning and optimizing video generation architectures through a simple but effective pipeline including shot boundary detection, OCR, border detecting, motion filter and fine bilingual caption.","The dataset will undergo ongoing expansion and be released as an open-source initiative to advance research and applications in video generative models.","Project page: https://tinytigerpan.github.io/tiger200k/"],"url":"http://arxiv.org/abs/2504.15182v1"}
{"created":"2025-04-21 15:40:14","title":"FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image","abstract":"We present a novel framework for generating high-quality, animatable 4D avatar from a single image. While recent advances have shown promising results in 4D avatar creation, existing methods either require extensive multiview data or struggle with shape accuracy and identity consistency. To address these limitations, we propose a comprehensive system that leverages shape, image, and video priors to create full-view, animatable avatars. Our approach first obtains initial coarse shape through 3D-GAN inversion. Then, it enhances multiview textures using depth-guided warping signals for cross-view consistency with the help of the image diffusion model. To handle expression animation, we incorporate a video prior with synchronized driving signals across viewpoints. We further introduce a Consistent-Inconsistent training to effectively handle data inconsistencies during 4D reconstruction. Experimental results demonstrate that our method achieves superior quality compared to the prior art, while maintaining consistency across different viewpoints and expressions.","sentences":["We present a novel framework for generating high-quality, animatable 4D avatar from a single image.","While recent advances have shown promising results in 4D avatar creation, existing methods either require extensive multiview data or struggle with shape accuracy and identity consistency.","To address these limitations, we propose a comprehensive system that leverages shape, image, and video priors to create full-view, animatable avatars.","Our approach first obtains initial coarse shape through 3D-GAN inversion.","Then, it enhances multiview textures using depth-guided warping signals for cross-view consistency with the help of the image diffusion model.","To handle expression animation, we incorporate a video prior with synchronized driving signals across viewpoints.","We further introduce a Consistent-Inconsistent training to effectively handle data inconsistencies during 4D reconstruction.","Experimental results demonstrate that our method achieves superior quality compared to the prior art, while maintaining consistency across different viewpoints and expressions."],"url":"http://arxiv.org/abs/2504.15179v1"}
{"created":"2025-04-21 15:24:34","title":"Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment in Aquaculture","abstract":"Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture management. Recent multi-modal approaches have shown promise in improving FFIA robustness and efficiency. However, these methods face significant challenges when adapting to new fish species or environments due to catastrophic forgetting and the lack of suitable datasets. To address these limitations, we first introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled audio-visual clips capturing feeding intensities across six different fish species in real aquaculture environments. Then, we pioneer audio-visual class incremental learning (CIL) for FFIA and demonstrate through benchmarking on AV-CIL-FFIA that it significantly outperforms single-modality methods. Existing CIL methods rely heavily on historical data. Exemplar-based approaches store raw samples, creating storage challenges, while exemplar-free methods avoid data storage but struggle to distinguish subtle feeding intensity variations across different fish species. To overcome these limitations, we introduce HAIL-FFIA, a novel audio-visual class-incremental learning framework that bridges this gap with a prototype-based approach that achieves exemplar-free efficiency while preserving essential knowledge through compact feature representations. Specifically, HAIL-FFIA employs hierarchical representation learning with a dual-path knowledge preservation mechanism that separates general intensity knowledge from fish-specific characteristics. Additionally, it features a dynamic modality balancing system that adaptively adjusts the importance of audio versus visual information based on feeding behaviour stages. Experimental results show that HAIL-FFIA is superior to SOTA methods on AV-CIL-FFIA, achieving higher accuracy with lower storage needs while effectively mitigating catastrophic forgetting in incremental fish species learning.","sentences":["Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture management.","Recent multi-modal approaches have shown promise in improving FFIA robustness and efficiency.","However, these methods face significant challenges when adapting to new fish species or environments due to catastrophic forgetting and the lack of suitable datasets.","To address these limitations, we first introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled audio-visual clips capturing feeding intensities across six different fish species in real aquaculture environments.","Then, we pioneer audio-visual class incremental learning (CIL) for FFIA and demonstrate through benchmarking on AV-CIL-FFIA that it significantly outperforms single-modality methods.","Existing CIL methods rely heavily on historical data.","Exemplar-based approaches store raw samples, creating storage challenges, while exemplar-free methods avoid data storage but struggle to distinguish subtle feeding intensity variations across different fish species.","To overcome these limitations, we introduce HAIL-FFIA, a novel audio-visual class-incremental learning framework that bridges this gap with a prototype-based approach that achieves exemplar-free efficiency while preserving essential knowledge through compact feature representations.","Specifically, HAIL-FFIA employs hierarchical representation learning with a dual-path knowledge preservation mechanism that separates general intensity knowledge from fish-specific characteristics.","Additionally, it features a dynamic modality balancing system that adaptively adjusts the importance of audio versus visual information based on feeding behaviour stages.","Experimental results show that HAIL-FFIA is superior to SOTA methods on AV-CIL-FFIA, achieving higher accuracy with lower storage needs while effectively mitigating catastrophic forgetting in incremental fish species learning."],"url":"http://arxiv.org/abs/2504.15171v1"}
{"created":"2025-04-21 15:09:40","title":"Survey of Loss Augmented Knowledge Tracing","abstract":"The training of artificial neural networks is heavily dependent on the careful selection of an appropriate loss function. While commonly used loss functions, such as cross-entropy and mean squared error (MSE), generally suffice for a broad range of tasks, challenges often emerge due to limitations in data quality or inefficiencies within the learning process. In such circumstances, the integration of supplementary terms into the loss function can serve to address these challenges, enhancing both model performance and robustness. Two prominent techniques, loss regularization and contrastive learning, have been identified as effective strategies for augmenting the capacity of loss functions in artificial neural networks.   Knowledge tracing is a compelling area of research that leverages predictive artificial intelligence to facilitate the automation of personalized and efficient educational experiences for students. In this paper, we provide a comprehensive review of the deep learning-based knowledge tracing (DKT) algorithms trained using advanced loss functions and discuss their improvements over prior techniques. We discuss contrastive knowledge tracing algorithms, such as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT, providing performance benchmarks and insights into real-world deployment challenges. The survey concludes with future research directions, including hybrid loss strategies and context-aware modeling.","sentences":["The training of artificial neural networks is heavily dependent on the careful selection of an appropriate loss function.","While commonly used loss functions, such as cross-entropy and mean squared error (MSE), generally suffice for a broad range of tasks, challenges often emerge due to limitations in data quality or inefficiencies within the learning process.","In such circumstances, the integration of supplementary terms into the loss function can serve to address these challenges, enhancing both model performance and robustness.","Two prominent techniques, loss regularization and contrastive learning, have been identified as effective strategies for augmenting the capacity of loss functions in artificial neural networks.   ","Knowledge tracing is a compelling area of research that leverages predictive artificial intelligence to facilitate the automation of personalized and efficient educational experiences for students.","In this paper, we provide a comprehensive review of the deep learning-based knowledge tracing (DKT) algorithms trained using advanced loss functions and discuss their improvements over prior techniques.","We discuss contrastive knowledge tracing algorithms, such as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT, providing performance benchmarks and insights into real-world deployment challenges.","The survey concludes with future research directions, including hybrid loss strategies and context-aware modeling."],"url":"http://arxiv.org/abs/2504.15163v1"}
{"created":"2025-04-21 15:07:26","title":"The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks","abstract":"Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa, require that all categories in an annotation task be sufficiently represented in the training data for optimal performance. However, it is often difficult to find sufficient examples for all categories in a task when building a high-quality training set. In this article, I describe this problem and propose a solution, the synthetic imputation approach. Leveraging a generative LLM (GPT-4o), this approach generates synthetic texts based on careful prompting and five original examples drawn randomly with replacement from the sample. This approach ensures that new synthetic texts are sufficiently different from the original texts to reduce overfitting, but retain the underlying substantive meaning of the examples to maximize out-of-sample performance. With 75 original examples or more, synthetic imputation's performance is on par with a full sample of original texts, and overfitting remains low, predictable and correctable with 50 original samples. The synthetic imputation approach provides a novel role for generative LLMs in research and allows applied researchers to balance their datasets for best performance.","sentences":["Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa, require that all categories in an annotation task be sufficiently represented in the training data for optimal performance.","However, it is often difficult to find sufficient examples for all categories in a task when building a high-quality training set.","In this article, I describe this problem and propose a solution, the synthetic imputation approach.","Leveraging a generative LLM (GPT-4o), this approach generates synthetic texts based on careful prompting and five original examples drawn randomly with replacement from the sample.","This approach ensures that new synthetic texts are sufficiently different from the original texts to reduce overfitting, but retain the underlying substantive meaning of the examples to maximize out-of-sample performance.","With 75 original examples or more, synthetic imputation's performance is on par with a full sample of original texts, and overfitting remains low, predictable and correctable with 50 original samples.","The synthetic imputation approach provides a novel role for generative LLMs in research and allows applied researchers to balance their datasets for best performance."],"url":"http://arxiv.org/abs/2504.15160v1"}
{"created":"2025-04-21 15:05:22","title":"Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration","abstract":"Recently, pre-trained text-to-image (T2I) models have been extensively adopted for real-world image restoration because of their powerful generative prior. However, controlling these large models for image restoration usually requires a large number of high-quality images and immense computational resources for training, which is costly and not privacy-friendly. In this paper, we find that the well-trained large T2I model (i.e., Flux) is able to produce a variety of high-quality images aligned with real-world distributions, offering an unlimited supply of training samples to mitigate the above issue. Specifically, we proposed a training data construction pipeline for image restoration, namely FluxGen, which includes unconditional image generation, image selection, and degraded image simulation. A novel light-weighted adapter (FluxIR) with squeeze-and-excitation layers is also carefully designed to control the large Diffusion Transformer (DiT)-based T2I model so that reasonable details can be restored. Experiments demonstrate that our proposed method enables the Flux model to adapt effectively to real-world image restoration tasks, achieving superior scores and visual quality on both synthetic and real-world degradation datasets - at only about 8.5\\% of the training cost compared to current approaches.","sentences":["Recently, pre-trained text-to-image (T2I) models have been extensively adopted for real-world image restoration because of their powerful generative prior.","However, controlling these large models for image restoration usually requires a large number of high-quality images and immense computational resources for training, which is costly and not privacy-friendly.","In this paper, we find that the well-trained large T2I model (i.e., Flux) is able to produce a variety of high-quality images aligned with real-world distributions, offering an unlimited supply of training samples to mitigate the above issue.","Specifically, we proposed a training data construction pipeline for image restoration, namely FluxGen, which includes unconditional image generation, image selection, and degraded image simulation.","A novel light-weighted adapter (FluxIR) with squeeze-and-excitation layers is also carefully designed to control the large Diffusion Transformer (DiT)-based T2I model so that reasonable details can be restored.","Experiments demonstrate that our proposed method enables the Flux model to adapt effectively to real-world image restoration tasks, achieving superior scores and visual quality on both synthetic and real-world degradation datasets - at only about 8.5\\% of the training cost compared to current approaches."],"url":"http://arxiv.org/abs/2504.15159v1"}
{"created":"2025-04-21 14:57:48","title":"Dynamic 3D KAN Convolution with Adaptive Grid Optimization for Hyperspectral Image Classification","abstract":"Deep neural networks face several challenges in hyperspectral image classification, including high-dimensional data, sparse distribution of ground objects, and spectral redundancy, which often lead to classification overfitting and limited generalization capability. To more efficiently adapt to ground object distributions while extracting image features without introducing excessive parameters and skipping redundant information, this paper proposes KANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an adaptive grid update mechanism. By introducing learnable univariate B-spline functions on network edges, specifically by flattening three-dimensional neighborhoods into vectors and applying B-spline-parameterized nonlinear activation functions to replace the fixed linear weights of traditional 3D convolutional kernels, we precisely capture complex spectral-spatial nonlinear relationships in hyperspectral data. Simultaneously, through a dynamic grid adjustment mechanism, we adaptively update the grid point positions of B-splines based on the statistical characteristics of input data, optimizing the resolution of spline functions to match the non-uniform distribution of spectral features, significantly improving the model's accuracy in high-dimensional data modeling and parameter efficiency, effectively alleviating the curse of dimensionality. This characteristic demonstrates superior neural scaling laws compared to traditional convolutional neural networks and reduces overfitting risks in small-sample and high-noise scenarios. KANet enhances model representation capability through a 3D dynamic expert convolution system without increasing network depth or width. The proposed method demonstrates superior performance on IN, UP, and KSC datasets, outperforming mainstream hyperspectral image classification approaches.","sentences":["Deep neural networks face several challenges in hyperspectral image classification, including high-dimensional data, sparse distribution of ground objects, and spectral redundancy, which often lead to classification overfitting and limited generalization capability.","To more efficiently adapt to ground object distributions while extracting image features without introducing excessive parameters and skipping redundant information, this paper proposes KANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an adaptive grid update mechanism.","By introducing learnable univariate B-spline functions on network edges, specifically by flattening three-dimensional neighborhoods into vectors and applying B-spline-parameterized nonlinear activation functions to replace the fixed linear weights of traditional 3D convolutional kernels, we precisely capture complex spectral-spatial nonlinear relationships in hyperspectral data.","Simultaneously, through a dynamic grid adjustment mechanism, we adaptively update the grid point positions of B-splines based on the statistical characteristics of input data, optimizing the resolution of spline functions to match the non-uniform distribution of spectral features, significantly improving the model's accuracy in high-dimensional data modeling and parameter efficiency, effectively alleviating the curse of dimensionality.","This characteristic demonstrates superior neural scaling laws compared to traditional convolutional neural networks and reduces overfitting risks in small-sample and high-noise scenarios.","KANet enhances model representation capability through a 3D dynamic expert convolution system without increasing network depth or width.","The proposed method demonstrates superior performance on IN, UP, and KSC datasets, outperforming mainstream hyperspectral image classification approaches."],"url":"http://arxiv.org/abs/2504.15155v1"}
{"created":"2025-04-21 14:56:29","title":"Distribution Testing Meets Sum Estimation","abstract":"We study the problem of estimating the sum of $n$ elements, each with weight $w(i)$, in a structured universe. Our goal is to estimate $W = \\sum_{i=1}^n w(i)$ within a $(1 \\pm \\epsilon)$ factor using a sublinear number of samples, assuming weights are non-increasing, i.e., $w(1) \\geq w(2) \\geq \\dots \\geq w(n)$. The sum estimation problem is well-studied under different access models to the universe $U$. However, to the best of our knowledge, nothing is known about the sum estimation problem using non-adaptive conditional sampling. In this work, we explore the sum estimation problem using non-adaptive conditional weighted and non-adaptive conditional uniform samples, assuming that the underlying distribution ($D(i)=w(i)/W$) is monotone. We also extend our approach to to the case where the underlying distribution of $U$ is unimodal. Additionally, we consider support size estimation when $w(i) = 0$ or $w(i) \\geq W/n$, using hybrid sampling (both weighted and uniform) to access $U$. We propose an algorithm to estimate $W$ under the non-increasing weight assumption, using $O(\\frac{1}{\\epsilon^3} \\log{n} + \\frac{1}{\\epsilon^6})$ non-adaptive weighted conditional samples and $O(\\frac{1}{\\epsilon^3} \\log{n})$ uniform conditional samples. Our algorithm matches the $\\Omega(\\log{n})$ lower bound by \\cite{ACK15}. For unimodal distributions, the sample complexity remains similar, with an additional $O(\\log{n})$ evaluation queries to locate the minimum weighted point in the domain. For estimating the support size $k$ of $U$, where weights are either $0$ or at least $W/n$, our algorithm uses $O\\big( \\frac{\\log^3(n/\\epsilon)}{\\epsilon^8} \\cdot \\log^4 \\frac{\\log(n/\\epsilon)}{\\epsilon} \\big)$ uniform samples and $O\\big( \\frac{\\log(n/\\epsilon)}{\\epsilon^2} \\cdot \\log \\frac{\\log(n/\\epsilon)}{\\epsilon} \\big)$ weighted samples to output $\\hat{k}$ satisfying $k - 2\\epsilon n \\leq \\hat{k} \\leq k + \\epsilon n$.","sentences":["We study the problem of estimating the sum of $n$ elements, each with weight $w(i)$, in a structured universe.","Our goal is to estimate $W = \\sum_{i=1}^n w(i)$ within a $(1 \\pm \\epsilon)$ factor using a sublinear number of samples, assuming weights are non-increasing, i.e., $w(1) \\geq w(2)","\\geq \\dots \\geq w(n)$.","The sum estimation problem is well-studied under different access models to the universe $U$. However, to the best of our knowledge, nothing is known about the sum estimation problem using non-adaptive conditional sampling.","In this work, we explore the sum estimation problem using non-adaptive conditional weighted and non-adaptive conditional uniform samples, assuming that the underlying distribution ($D(i)=w(i)/W$) is monotone.","We also extend our approach to to the case where the underlying distribution of $U$ is unimodal.","Additionally, we consider support size estimation when $w(i) = 0$ or $w(i)","\\geq W/n$, using hybrid sampling (both weighted and uniform) to access $U$.","We propose an algorithm to estimate $W$ under the non-increasing weight assumption, using $O(\\frac{1}{\\epsilon^3} \\log{n} + \\frac{1}{\\epsilon^6})$ non-adaptive weighted conditional samples and $O(\\frac{1}{\\epsilon^3} \\log{n})$ uniform conditional samples.","Our algorithm matches the $\\Omega(\\log{n})$ lower bound by \\cite{ACK15}.","For unimodal distributions, the sample complexity remains similar, with an additional $O(\\log{n})$ evaluation queries to locate the minimum weighted point in the domain.","For estimating the support size $k$ of $U$, where weights are either $0$ or at least $W/n$, our algorithm uses $O\\big( \\frac{\\log^3(n/\\epsilon)}{\\epsilon^8} \\cdot \\log^4 \\frac{\\log(n/\\epsilon)}{\\epsilon} \\big)$ uniform samples and $O\\big( \\frac{\\log(n/\\epsilon)}{\\epsilon^2} \\cdot \\log \\frac{\\log(n/\\epsilon)}{\\epsilon} \\big)$ weighted samples to output $\\hat{k}$ satisfying $k - 2\\epsilon n \\leq \\hat{k} \\leq k + \\epsilon n$."],"url":"http://arxiv.org/abs/2504.15153v1"}
{"created":"2025-04-21 14:30:16","title":"Investigating Youth's Technical and Ethical Understanding of Generative Language Models When Engaging in Construction and Deconstruction Activities","abstract":"The widespread adoption of generative artificial intelligence/machine learning (AI/ML) technologies has increased the need to support youth in developing AI/ML literacies. However, most work has centered on preparing young people to use these systems, with less attention to how they can participate in designing and evaluating them. This study investigates how engaging young people in the design and auditing of generative language models (GLMs) may foster the development of their understanding of how these systems work from both technical and ethical perspectives. The study takes an in-pieces approach to investigate novices' conceptions of GLMs. Such an approach supports the analysis of how technical and ethical conceptions evolve and relate to each other. I am currently conducting a series of participatory design workshops with sixteen ninth graders (ages 14-15) in which they will (a) build GLMs from a data-driven perspective that glassboxes how data shapes model performance and (b) audit commercial GLMs by repeatedly and systematically querying them to draw inferences about their behaviors. I will analyze participants' interactions to identify ethical and technical conceptions they may exhibit while designing and auditing GLMs. I will also conduct clinical interviews and use microgenetic knowledge analysis and ordered network analysis to investigate how participants' ethical and technical conceptions of GLMs relate to each other and change after the workshop. The study will contribute (a) evidence of how engaging youth in design and auditing activities may support the development of ethical and technical understanding of GLMs and (b) an inventory of novice design and auditing practices that may support youth's technical and ethical understanding of GLMs.","sentences":["The widespread adoption of generative artificial intelligence/machine learning (AI/ML) technologies has increased the need to support youth in developing AI/ML literacies.","However, most work has centered on preparing young people to use these systems, with less attention to how they can participate in designing and evaluating them.","This study investigates how engaging young people in the design and auditing of generative language models (GLMs) may foster the development of their understanding of how these systems work from both technical and ethical perspectives.","The study takes an in-pieces approach to investigate novices' conceptions of GLMs.","Such an approach supports the analysis of how technical and ethical conceptions evolve and relate to each other.","I am currently conducting a series of participatory design workshops with sixteen ninth graders (ages 14-15) in which they will (a) build GLMs from a data-driven perspective that glassboxes how data shapes model performance and (b) audit commercial GLMs by repeatedly and systematically querying them to draw inferences about their behaviors.","I will analyze participants' interactions to identify ethical and technical conceptions they may exhibit while designing and auditing GLMs.","I will also conduct clinical interviews and use microgenetic knowledge analysis and ordered network analysis to investigate how participants' ethical and technical conceptions of GLMs relate to each other and change after the workshop.","The study will contribute (a) evidence of how engaging youth in design and auditing activities may support the development of ethical and technical understanding of GLMs and (b) an inventory of novice design and auditing practices that may support youth's technical and ethical understanding of GLMs."],"url":"http://arxiv.org/abs/2504.15132v1"}
{"created":"2025-04-21 14:25:32","title":"Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning","abstract":"Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics, particularly in applications such as warehouse automation and logistics. Existing solutions often face challenges in scalability, adaptability, and efficiency, limiting their applicability in dynamic environments with real-time planning requirements. This paper presents Neural ATTF (Adaptive Task Token Framework), a new algorithm that combines a Priority Guided Task Matching (PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning method. Neural STA* enhances path planning by enabling rapid exploration of the search space through guided learned heuristics and ensures collision avoidance under dynamic constraints. PGTM prioritizes delayed agents and dynamically assigns tasks by prioritizing agents nearest to these tasks, optimizing both continuity and system throughput. Experimental evaluations against state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and LNS-wPBS, demonstrate the superior scalability, solution quality, and computational efficiency of Neural ATTF. These results highlight the framework's potential for addressing the critical demands of complex, real-world multi-agent systems operating in high-demand, unpredictable settings.","sentences":["Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics, particularly in applications such as warehouse automation and logistics.","Existing solutions often face challenges in scalability, adaptability, and efficiency, limiting their applicability in dynamic environments with real-time planning requirements.","This paper presents Neural ATTF (Adaptive Task Token Framework), a new algorithm that combines a Priority Guided Task Matching (PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning method.","Neural STA* enhances path planning by enabling rapid exploration of the search space through guided learned heuristics and ensures collision avoidance under dynamic constraints.","PGTM prioritizes delayed agents and dynamically assigns tasks by prioritizing agents nearest to these tasks, optimizing both continuity and system throughput.","Experimental evaluations against state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and LNS-wPBS, demonstrate the superior scalability, solution quality, and computational efficiency of Neural ATTF.","These results highlight the framework's potential for addressing the critical demands of complex, real-world multi-agent systems operating in high-demand, unpredictable settings."],"url":"http://arxiv.org/abs/2504.15130v1"}
{"created":"2025-04-21 14:25:23","title":"A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment","abstract":"Deploying robot learning methods to a quadrotor in unstructured outdoor environments is an exciting task. Quadrotors operating in real-world environments by learning-based methods encounter several challenges: a large amount of simulator generated data required for training, strict demands for real-time processing onboard, and the sim-to-real gap caused by dynamic and noisy conditions. Current works have made a great breakthrough in applying learning-based methods to end-to-end control of quadrotors, but rarely mention the infrastructure system training from scratch and deploying to reality, which makes it difficult to reproduce methods and applications. To bridge this gap, we propose a platform that enables the seamless transfer of end-to-end deep reinforcement learning (DRL) policies. We integrate the training environment, flight dynamics control, DRL algorithms, the MAVROS middleware stack, and hardware into a comprehensive workflow and architecture that enables quadrotors' policies to be trained from scratch to real-world deployment in several minutes. Our platform provides rich types of environments including hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and planning in unknown environments, as a physical experiment benchmark. Through extensive empirical validation, we demonstrate the efficiency of proposed sim-to-real platform, and robust outdoor flight performance under real-world perturbations. Details can be found from our website https://emnavi.tech/AirGym/.","sentences":["Deploying robot learning methods to a quadrotor in unstructured outdoor environments is an exciting task.","Quadrotors operating in real-world environments by learning-based methods encounter several challenges: a large amount of simulator generated data required for training, strict demands for real-time processing onboard, and the sim-to-real gap caused by dynamic and noisy conditions.","Current works have made a great breakthrough in applying learning-based methods to end-to-end control of quadrotors, but rarely mention the infrastructure system training from scratch and deploying to reality, which makes it difficult to reproduce methods and applications.","To bridge this gap, we propose a platform that enables the seamless transfer of end-to-end deep reinforcement learning (DRL) policies.","We integrate the training environment, flight dynamics control, DRL algorithms, the MAVROS middleware stack, and hardware into a comprehensive workflow and architecture that enables quadrotors' policies to be trained from scratch to real-world deployment in several minutes.","Our platform provides rich types of environments including hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and planning in unknown environments, as a physical experiment benchmark.","Through extensive empirical validation, we demonstrate the efficiency of proposed sim-to-real platform, and robust outdoor flight performance under real-world perturbations.","Details can be found from our website https://emnavi.tech/AirGym/."],"url":"http://arxiv.org/abs/2504.15129v1"}
{"created":"2025-04-21 14:19:00","title":"Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations","abstract":"This work introduces a novel method for surface normal estimation from rectified stereo image pairs, leveraging affine transformations derived from disparity values to achieve fast and accurate results. We demonstrate how the rectification of stereo image pairs simplifies the process of surface normal estimation by reducing computational complexity. To address noise reduction, we develop a custom algorithm inspired by convolutional operations, tailored to process disparity data efficiently. We also introduce adaptive heuristic techniques for efficiently detecting connected surface components within the images, further improving the robustness of the method. By integrating these methods, we construct a surface normal estimator that is both fast and accurate, producing a dense, oriented point cloud as the final output. Our method is validated using both simulated environments and real-world stereo images from the Middlebury and Cityscapes datasets, demonstrating significant improvements in real-time performance and accuracy when implemented on a GPU. Upon acceptance, the shader source code will be made publicly available to facilitate further research and reproducibility.","sentences":["This work introduces a novel method for surface normal estimation from rectified stereo image pairs, leveraging affine transformations derived from disparity values to achieve fast and accurate results.","We demonstrate how the rectification of stereo image pairs simplifies the process of surface normal estimation by reducing computational complexity.","To address noise reduction, we develop a custom algorithm inspired by convolutional operations, tailored to process disparity data efficiently.","We also introduce adaptive heuristic techniques for efficiently detecting connected surface components within the images, further improving the robustness of the method.","By integrating these methods, we construct a surface normal estimator that is both fast and accurate, producing a dense, oriented point cloud as the final output.","Our method is validated using both simulated environments and real-world stereo images from the Middlebury and Cityscapes datasets, demonstrating significant improvements in real-time performance and accuracy when implemented on a GPU.","Upon acceptance, the shader source code will be made publicly available to facilitate further research and reproducibility."],"url":"http://arxiv.org/abs/2504.15121v1"}
{"created":"2025-04-21 14:17:25","title":"Kuwain 1.5B: An Arabic SLM via Language Injection","abstract":"Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model's existing knowledge with a minimum amount of the original model's data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes.","sentences":["Enhancing existing models with new knowledge is a crucial aspect of AI development.","This paper introduces a novel method for integrating a new language into a large language model (LLM).","Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge.","We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English.","Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model's existing knowledge with a minimum amount of the original model's data.","This offers a cost-effective alternative to training a comprehensive model in both English and Arabic.","The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes."],"url":"http://arxiv.org/abs/2504.15120v1"}
{"created":"2025-04-21 14:10:20","title":"Deterministic $k$-Median Clustering in Near-Optimal Time","abstract":"The metric $k$-median problem is a textbook clustering problem. As input, we are given a metric space $V$ of size $n$ and an integer $k$, and our task is to find a subset $S \\subseteq V$ of at most $k$ `centers' that minimizes the total distance from each point in $V$ to its nearest center in $S$.   Mettu and Plaxton [UAI'02] gave a randomized algorithm for $k$-median that computes a $O(1)$-approximation in $\\tilde O(nk)$ time. They also showed that any algorithm for this problem with a bounded approximation ratio must have a running time of $\\Omega(nk)$. Thus, the running time of their algorithm is optimal up to polylogarithmic factors.   For deterministic $k$-median, Guha et al.~[FOCS'00] gave an algorithm that computes a $\\text{poly}(\\log (n/k))$-approximation in $\\tilde O(nk)$ time, where the degree of the polynomial in the approximation is unspecified. To the best of our knowledge, this remains the state-of-the-art approximation of any deterministic $k$-median algorithm with this running time.   This leads us to the following natural question: What is the best approximation of a deterministic $k$-median algorithm with near-optimal running time? We make progress in answering this question by giving a deterministic algorithm that computes a $O(\\log(n/k))$-approximation in $\\tilde O(nk)$ time. We also provide a lower bound showing that any deterministic algorithm with this running time must have an approximation ratio of $\\Omega(\\log n/(\\log k + \\log \\log n))$, establishing a gap between the randomized and deterministic settings for $k$-median.","sentences":["The metric $k$-median problem is a textbook clustering problem.","As input, we are given a metric space $V$ of size $n$ and an integer $k$, and our task is to find a subset $S \\subseteq V$ of at most $k$ `centers' that minimizes the total distance from each point in $V$ to its nearest center in $S$.   Mettu and Plaxton [UAI'02] gave a randomized algorithm for $k$-median that computes a $O(1)$-approximation in $\\tilde O(nk)$ time.","They also showed that any algorithm for this problem with a bounded approximation ratio must have a running time of $\\Omega(nk)$. Thus, the running time of their algorithm is optimal up to polylogarithmic factors.   ","For deterministic $k$-median, Guha et al.~[FOCS'00] gave an algorithm that computes a $\\text{poly}(\\log (n/k))$-approximation in $\\tilde O(nk)$ time, where the degree of the polynomial in the approximation is unspecified.","To the best of our knowledge, this remains the state-of-the-art approximation of any deterministic $k$-median algorithm with this running time.   ","This leads us to the following natural question: What is the best approximation of a deterministic $k$-median algorithm with near-optimal running time?","We make progress in answering this question by giving a deterministic algorithm that computes a $O(\\log(n/k))$-approximation in $\\tilde O(nk)$ time.","We also provide a lower bound showing that any deterministic algorithm with this running time must have an approximation ratio of $\\Omega(\\log n/(\\log k + \\log \\log n))$, establishing a gap between the randomized and deterministic settings for $k$-median."],"url":"http://arxiv.org/abs/2504.15115v1"}
{"created":"2025-04-21 13:41:09","title":"Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN","abstract":"Up to now, the training processes of typical Generative Adversarial Networks (GANs) are still particularly sensitive to data properties and hyperparameters, which may lead to severe oscillations, difficulties in convergence, or even failures to converge, especially when the overall variances of the training sets are large. These phenomena are often attributed to the training characteristics of such networks. Aiming at the problem, this paper develops a new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which employs reinforcement learning in the training process of GANs to make training easier. Specifically, this paper allows the training step size to be controlled by an agent to improve training stability, and makes the training process more intelligent with variable learning rates, making GANs less sensitive to step size. Experiments have been conducted on three benchmark datasets to verify the effectiveness of the developed FSCO.","sentences":["Up to now, the training processes of typical Generative Adversarial Networks (GANs) are still particularly sensitive to data properties and hyperparameters, which may lead to severe oscillations, difficulties in convergence, or even failures to converge, especially when the overall variances of the training sets are large.","These phenomena are often attributed to the training characteristics of such networks.","Aiming at the problem, this paper develops a new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which employs reinforcement learning in the training process of GANs to make training easier.","Specifically, this paper allows the training step size to be controlled by an agent to improve training stability, and makes the training process more intelligent with variable learning rates, making GANs less sensitive to step size.","Experiments have been conducted on three benchmark datasets to verify the effectiveness of the developed FSCO."],"url":"http://arxiv.org/abs/2504.15099v1"}
{"created":"2025-04-21 13:25:55","title":"Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models","abstract":"Detecting collaborative and problem-solving behaviours from digital traces to interpret students' collaborative problem solving (CPS) competency is a long-term goal in the Artificial Intelligence in Education (AIEd) field. Although multimodal data and advanced models are argued to have the potential to detect complex CPS behaviours, empirical evidence on their value remains limited with some contrasting evidence. In this study, we investigated the potential of multimodal data to improve model performance in diagnosing 78 secondary school students' CPS subskills and indicators in authentic educational settings. In particular, text embeddings from verbal data and acoustic embeddings from audio data were used in a multimodal classification model for CPS diagnosis. Both unimodal and multimodal transformer-based models outperformed traditional models in detecting CPS classes. Although the inclusion of multimodality did not improve the performance of traditional unimodal models, its integration into transformer-based models demonstrated improved performance for diagnosing social-cognitive CPS classes compared to unimodal transformer-based models. Based on the results, the paper argues that multimodality and the selection of a particular modelling technique should not be taken for granted to achieve the best performance in the automated detection of every CPS subskill and indicator. Rather, their value is limited to certain types of CPS indicators, affected by the complexity of the labels, and dependent on the composition of indicators in the dataset. We conclude the paper by discussing the required nuance when considering the value of LLMs and multimodality in automated CPS diagnosis, highlighting the need for human-AI complementarity, and proposing the exploration of relevant model architectures and techniques to improve CPS diagnosis in authentic educational contexts.","sentences":["Detecting collaborative and problem-solving behaviours from digital traces to interpret students' collaborative problem solving (CPS) competency is a long-term goal in the Artificial Intelligence in Education (AIEd) field.","Although multimodal data and advanced models are argued to have the potential to detect complex CPS behaviours, empirical evidence on their value remains limited with some contrasting evidence.","In this study, we investigated the potential of multimodal data to improve model performance in diagnosing 78 secondary school students' CPS subskills and indicators in authentic educational settings.","In particular, text embeddings from verbal data and acoustic embeddings from audio data were used in a multimodal classification model for CPS diagnosis.","Both unimodal and multimodal transformer-based models outperformed traditional models in detecting CPS classes.","Although the inclusion of multimodality did not improve the performance of traditional unimodal models, its integration into transformer-based models demonstrated improved performance for diagnosing social-cognitive CPS classes compared to unimodal transformer-based models.","Based on the results, the paper argues that multimodality and the selection of a particular modelling technique should not be taken for granted to achieve the best performance in the automated detection of every CPS subskill and indicator.","Rather, their value is limited to certain types of CPS indicators, affected by the complexity of the labels, and dependent on the composition of indicators in the dataset.","We conclude the paper by discussing the required nuance when considering the value of LLMs and multimodality in automated CPS diagnosis, highlighting the need for human-AI complementarity, and proposing the exploration of relevant model architectures and techniques to improve CPS diagnosis in authentic educational contexts."],"url":"http://arxiv.org/abs/2504.15093v1"}
{"created":"2025-04-21 13:24:30","title":"Federated Latent Factor Model for Bias-Aware Recommendation with Privacy-Preserving","abstract":"A recommender system (RS) aims to provide users with personalized item recommendations, enhancing their overall experience. Traditional RSs collect and process all user data on a central server. However, this centralized approach raises significant privacy concerns, as it increases the risk of data breaches and privacy leakages, which are becoming increasingly unacceptable to privacy-sensitive users. To address these privacy challenges, federated learning has been integrated into RSs, ensuring that user data remains secure. In centralized RSs, the issue of rating bias is effectively addressed by jointly analyzing all users' raw interaction data. However, this becomes a significant challenge in federated RSs, as raw data is no longer accessible due to privacy-preserving constraints. To overcome this problem, we propose a Federated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is explicitly incorporated into every local model's loss function, allowing for the effective elimination of rating bias without compromising data privacy. Extensive experiments conducted on three real-world datasets demonstrate that FBALF achieves significantly higher recommendation accuracy compared to other state-of-the-art federated RSs.","sentences":["A recommender system (RS) aims to provide users with personalized item recommendations, enhancing their overall experience.","Traditional RSs collect and process all user data on a central server.","However, this centralized approach raises significant privacy concerns, as it increases the risk of data breaches and privacy leakages, which are becoming increasingly unacceptable to privacy-sensitive users.","To address these privacy challenges, federated learning has been integrated into RSs, ensuring that user data remains secure.","In centralized RSs, the issue of rating bias is effectively addressed by jointly analyzing all users' raw interaction data.","However, this becomes a significant challenge in federated RSs, as raw data is no longer accessible due to privacy-preserving constraints.","To overcome this problem, we propose a Federated Bias-Aware Latent Factor (FBALF) model.","In FBALF, training bias is explicitly incorporated into every local model's loss function, allowing for the effective elimination of rating bias without compromising data privacy.","Extensive experiments conducted on three real-world datasets demonstrate that FBALF achieves significantly higher recommendation accuracy compared to other state-of-the-art federated RSs."],"url":"http://arxiv.org/abs/2504.15090v1"}
{"created":"2025-04-21 13:18:54","title":"Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation","abstract":"Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by leveraging historical interactions across multiple domains, focusing on modeling cross-domain preferences through intra- and inter-sequence item relationships. Inspired by human cognitive processes, we propose Hierarchical Attention Fusion of Visual and Textual Representations (HAF-VT), a novel approach integrating visual and textual data to enhance cognitive modeling. Using the frozen CLIP model, we generate image and text embeddings, enriching item representations with multimodal data. A hierarchical attention mechanism jointly learns single-domain and cross-domain preferences, mimicking human information integration. Evaluated on four e-commerce datasets, HAF-VT outperforms existing methods in capturing cross-domain user interests, bridging cognitive principles with computational models and highlighting the role of multimodal data in sequential decision-making.","sentences":["Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by leveraging historical interactions across multiple domains, focusing on modeling cross-domain preferences through intra- and inter-sequence item relationships.","Inspired by human cognitive processes, we propose Hierarchical Attention Fusion of Visual and Textual Representations (HAF-VT), a novel approach integrating visual and textual data to enhance cognitive modeling.","Using the frozen CLIP model, we generate image and text embeddings, enriching item representations with multimodal data.","A hierarchical attention mechanism jointly learns single-domain and cross-domain preferences, mimicking human information integration.","Evaluated on four e-commerce datasets, HAF-VT outperforms existing methods in capturing cross-domain user interests, bridging cognitive principles with computational models and highlighting the role of multimodal data in sequential decision-making."],"url":"http://arxiv.org/abs/2504.15085v1"}
{"created":"2025-04-21 12:59:40","title":"Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling","abstract":"We introduce an extensive new dataset of MIDI files, created by transcribing audio recordings of piano performances into their constituent notes. The data pipeline we use is multi-stage, employing a language model to autonomously crawl and score audio recordings from the internet based on their metadata, followed by a stage of pruning and segmentation using an audio classifier. The resulting dataset contains over one million distinct MIDI files, comprising roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of our techniques, offering statistical insights, and investigate the content by extracting metadata tags, which we also provide. Dataset available at https://github.com/loubbrad/aria-midi.","sentences":["We introduce an extensive new dataset of MIDI files, created by transcribing audio recordings of piano performances into their constituent notes.","The data pipeline we use is multi-stage, employing a language model to autonomously crawl and score audio recordings from the internet based on their metadata, followed by a stage of pruning and segmentation using an audio classifier.","The resulting dataset contains over one million distinct MIDI files, comprising roughly 100,000 hours of transcribed audio.","We provide an in-depth analysis of our techniques, offering statistical insights, and investigate the content by extracting metadata tags, which we also provide.","Dataset available at https://github.com/loubbrad/aria-midi."],"url":"http://arxiv.org/abs/2504.15071v1"}
{"created":"2025-04-21 12:12:43","title":"ScanEdit: Hierarchically-Guided Functional 3D Scan Editing","abstract":"With the fast pace of 3D capture technology and resulting abundance of 3D data, effective 3D scene editing becomes essential for a variety of graphics applications. In this work we present ScanEdit, an instruction-driven method for functional editing of complex, real-world 3D scans. To model large and interdependent sets of ob- jectswe propose a hierarchically-guided approach. Given a 3D scan decomposed into its object instances, we first construct a hierarchical scene graph representation to enable effective, tractable editing. We then leverage reason- ing capabilities of Large Language Models (LLMs) and translate high-level language instructions into actionable commands applied hierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based guidance with ex- plicit physical constraints and generates realistic scenes where object arrangements obey both physics and common sense. In our extensive experimental evaluation ScanEdit outperforms state of the art and demonstrates excellent re- sults for a variety of real-world scenes and input instruc- tions.","sentences":["With the fast pace of 3D capture technology and resulting abundance of 3D data, effective 3D scene editing becomes essential for a variety of graphics applications.","In this work we present ScanEdit, an instruction-driven method for functional editing of complex, real-world 3D scans.","To model large and interdependent sets of ob- jectswe propose a hierarchically-guided approach.","Given a 3D scan decomposed into its object instances, we first construct a hierarchical scene graph representation to enable effective, tractable editing.","We then leverage reason-","ing capabilities of Large Language Models (LLMs) and translate high-level language instructions into actionable commands applied hierarchically to the scene graph.","Fi-","nally, ScanEdit integrates LLM-based guidance with ex- plicit physical constraints and generates realistic scenes where object arrangements obey both physics and common sense.","In our extensive experimental evaluation ScanEdit outperforms state of the art and demonstrates excellent re- sults for a variety of real-world scenes and input instruc- tions."],"url":"http://arxiv.org/abs/2504.15049v1"}
{"created":"2025-04-21 12:00:20","title":"Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision","abstract":"RL systems usually tackle generalization by inferring task beliefs from high-quality samples or warmup explorations. The restricted form limits their generality and usability since these supervision signals are expensive and even infeasible to acquire in advance for unseen tasks. Learning directly from the raw text about decision tasks is a promising alternative to leverage a much broader source of supervision. In the paper, we propose Text-to-Decision Agent (T2DA), a simple and scalable framework that supervises generalist policy learning with natural language. We first introduce a generalized world model to encode multi-task decision data into a dynamics-aware embedding space. Then, inspired by CLIP, we predict which textual description goes with which decision embedding, effectively bridging their semantic gap via contrastive language-decision pre-training and aligning the text embeddings to comprehend the environment dynamics. After training the text-conditioned generalist policy, the agent can directly realize zero-shot text-to-decision generation in response to language instructions. Comprehensive experiments on MuJoCo and Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot generalization and outperforms various types of baselines.","sentences":["RL systems usually tackle generalization by inferring task beliefs from high-quality samples or warmup explorations.","The restricted form limits their generality and usability since these supervision signals are expensive and even infeasible to acquire in advance for unseen tasks.","Learning directly from the raw text about decision tasks is a promising alternative to leverage a much broader source of supervision.","In the paper, we propose Text-to-Decision Agent (T2DA), a simple and scalable framework that supervises generalist policy learning with natural language.","We first introduce a generalized world model to encode multi-task decision data into a dynamics-aware embedding space.","Then, inspired by CLIP, we predict which textual description goes with which decision embedding, effectively bridging their semantic gap via contrastive language-decision pre-training and aligning the text embeddings to comprehend the environment dynamics.","After training the text-conditioned generalist policy, the agent can directly realize zero-shot text-to-decision generation in response to language instructions.","Comprehensive experiments on MuJoCo and Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot generalization and outperforms various types of baselines."],"url":"http://arxiv.org/abs/2504.15046v1"}
{"created":"2025-04-21 11:48:39","title":"A Call for New Recipes to Enhance Spatial Reasoning in MLLMs","abstract":"Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in general vision-language tasks. However, recent studies have exposed critical limitations in their spatial reasoning capabilities. This deficiency in spatial reasoning significantly constrains MLLMs' ability to interact effectively with the physical world, thereby limiting their broader applications. We argue that spatial reasoning capabilities will not naturally emerge from merely scaling existing architectures and training methodologies. Instead, this challenge demands dedicated attention to fundamental modifications in the current MLLM development approach. In this position paper, we first establish a comprehensive framework for spatial reasoning within the context of MLLMs. We then elaborate on its pivotal role in real-world applications. Through systematic analysis, we examine how individual components of the current methodology-from training data to reasoning mechanisms-influence spatial reasoning capabilities. This examination reveals critical limitations while simultaneously identifying promising avenues for advancement. Our work aims to direct the AI research community's attention toward these crucial yet underexplored aspects. By highlighting these challenges and opportunities, we seek to catalyze progress toward achieving human-like spatial reasoning capabilities in MLLMs.","sentences":["Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in general vision-language tasks.","However, recent studies have exposed critical limitations in their spatial reasoning capabilities.","This deficiency in spatial reasoning significantly constrains MLLMs' ability to interact effectively with the physical world, thereby limiting their broader applications.","We argue that spatial reasoning capabilities will not naturally emerge from merely scaling existing architectures and training methodologies.","Instead, this challenge demands dedicated attention to fundamental modifications in the current MLLM development approach.","In this position paper, we first establish a comprehensive framework for spatial reasoning within the context of MLLMs.","We then elaborate on its pivotal role in real-world applications.","Through systematic analysis, we examine how individual components of the current methodology-from training data to reasoning mechanisms-influence spatial reasoning capabilities.","This examination reveals critical limitations while simultaneously identifying promising avenues for advancement.","Our work aims to direct the AI research community's attention toward these crucial yet underexplored aspects.","By highlighting these challenges and opportunities, we seek to catalyze progress toward achieving human-like spatial reasoning capabilities in MLLMs."],"url":"http://arxiv.org/abs/2504.15037v1"}
{"created":"2025-04-21 11:48:39","title":"Estimating transformative agreement impact on hybrid open access: A comparative large-scale study using Scopus, Web of Science and open metadata","abstract":"This study compares open metadata from hoaddata, an openly available dataset based on Crossref, OpenAlex and the cOAlition S Journal Checker Tool, with proprietary bibliometric databases Scopus and Web of Science to estimate the impact of transformative agreements on hybrid open access publishing. Analysing over 13,000 hybrid journals between 2019-2023, the research found substantial growth in open access due to these agreements, although most articles remain paywalled. The results were consistent across all three data sources, showing strong correlations in country-level metrics despite differences in journal coverage and metadata availability. By 2023, transformative agreements enabled the majority of open access in hybrid journals, with particularly high adoption in European countries. The analysis revealed strong alignment between first and corresponding authorship when measuring agreement uptake by publisher and country. This comparative approach supports the use of open metadata for large-scale hybrid open access studies, while using multiple data sources together provides a more robust understanding of hybrid open access adoption than any single database can offer, overcoming individual limitations in coverage and metadata quality.","sentences":["This study compares open metadata from hoaddata, an openly available dataset based on Crossref, OpenAlex and the cOAlition S Journal Checker Tool, with proprietary bibliometric databases Scopus and Web of Science to estimate the impact of transformative agreements on hybrid open access publishing.","Analysing over 13,000 hybrid journals between 2019-2023, the research found substantial growth in open access due to these agreements, although most articles remain paywalled.","The results were consistent across all three data sources, showing strong correlations in country-level metrics despite differences in journal coverage and metadata availability.","By 2023, transformative agreements enabled the majority of open access in hybrid journals, with particularly high adoption in European countries.","The analysis revealed strong alignment between first and corresponding authorship when measuring agreement uptake by publisher and country.","This comparative approach supports the use of open metadata for large-scale hybrid open access studies, while using multiple data sources together provides a more robust understanding of hybrid open access adoption than any single database can offer, overcoming individual limitations in coverage and metadata quality."],"url":"http://arxiv.org/abs/2504.15038v1"}
{"created":"2025-04-21 11:44:58","title":"Dynamic Robustness Verification Against Weak Memory (Extended Version)","abstract":"Dynamic race detection is a highly effective runtime verification technique for identifying data races by instrumenting and monitoring concurrent program runs. However, standard dynamic race detection is incompatible with practical weak memory models; the added instrumentation introduces extra synchronization, which masks weakly consistent behaviors and inherently misses certain data races. In response, we propose to dynamically verify program robustness-a property ensuring that a program exhibits only strongly consistent behaviors. Building on an existing static decision procedures, we develop an algorithm for dynamic robustness verification under a C11-style memory model. The algorithm is based on \"location clocks\", a variant of vector clocks used in standard race detection. It allows effective and easy-to-apply defense against weak memory on a per-program basis, which can be combined with race detection that assumes strong consistency. We implement our algorithm in a tool, called RSAN, and evaluate it across various settings. To our knowledge, this work is the first to propose and develop dynamic verification of robustness against weak memory models.","sentences":["Dynamic race detection is a highly effective runtime verification technique for identifying data races by instrumenting and monitoring concurrent program runs.","However, standard dynamic race detection is incompatible with practical weak memory models; the added instrumentation introduces extra synchronization, which masks weakly consistent behaviors and inherently misses certain data races.","In response, we propose to dynamically verify program robustness-a property ensuring that a program exhibits only strongly consistent behaviors.","Building on an existing static decision procedures, we develop an algorithm for dynamic robustness verification under a C11-style memory model.","The algorithm is based on \"location clocks\", a variant of vector clocks used in standard race detection.","It allows effective and easy-to-apply defense against weak memory on a per-program basis, which can be combined with race detection that assumes strong consistency.","We implement our algorithm in a tool, called RSAN, and evaluate it across various settings.","To our knowledge, this work is the first to propose and develop dynamic verification of robustness against weak memory models."],"url":"http://arxiv.org/abs/2504.15036v1"}
{"created":"2025-04-21 11:11:07","title":"LLMs as Data Annotators: How Close Are We to Human Performance","abstract":"In NLP, fine-tuning LLMs is effective for various applications but requires high-quality annotated data. However, manual annotation of data is labor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly used to automate the process, often employing in-context learning (ICL) in which some examples related to the task are given in the prompt for better performance. However, manually selecting context examples can lead to inefficiencies and suboptimal model performance. This paper presents comprehensive experiments comparing several LLMs, considering different embedding models, across various datasets for the Named Entity Recognition (NER) task. The evaluation encompasses models with approximately $7$B and $70$B parameters, including both proprietary and non-proprietary models. Furthermore, leveraging the success of Retrieval-Augmented Generation (RAG), it also considers a method that addresses the limitations of ICL by automatically retrieving contextual examples, thereby enhancing performance. The results highlight the importance of selecting the appropriate LLM and embedding model, understanding the trade-offs between LLM sizes and desired performance, and the necessity to direct research efforts towards more challenging datasets.","sentences":["In NLP, fine-tuning LLMs is effective for various applications but requires high-quality annotated data.","However, manual annotation of data is labor-intensive, time-consuming, and costly.","Therefore, LLMs are increasingly used to automate the process, often employing in-context learning (ICL) in which some examples related to the task are given in the prompt for better performance.","However, manually selecting context examples can lead to inefficiencies and suboptimal model performance.","This paper presents comprehensive experiments comparing several LLMs, considering different embedding models, across various datasets for the Named Entity Recognition (NER) task.","The evaluation encompasses models with approximately $7$B and $70$B parameters, including both proprietary and non-proprietary models.","Furthermore, leveraging the success of Retrieval-Augmented Generation (RAG), it also considers a method that addresses the limitations of ICL by automatically retrieving contextual examples, thereby enhancing performance.","The results highlight the importance of selecting the appropriate LLM and embedding model, understanding the trade-offs between LLM sizes and desired performance, and the necessity to direct research efforts towards more challenging datasets."],"url":"http://arxiv.org/abs/2504.15022v1"}
{"created":"2025-04-21 10:00:13","title":"Weakly Approximating Knapsack in Subquadratic Time","abstract":"We consider the classic Knapsack problem. Let $t$ and $\\mathrm{OPT}$ be the capacity and the optimal value, respectively. If one seeks a solution with total profit at least $\\mathrm{OPT}/(1 + \\varepsilon)$ and total weight at most $t$, then Knapsack can be solved in $\\tilde{O}(n + (\\frac{1}{\\varepsilon})^2)$ time [Chen, Lian, Mao, and Zhang '24][Mao '24]. This running time is the best possible (up to a logarithmic factor), assuming that $(\\min,+)$-convolution cannot be solved in truly subquadratic time [K\\\"unnemann, Paturi, and Schneider '17][Cygan, Mucha, W\\k{e}grzycki, and W{\\l}odarczyk '19]. The same upper and lower bounds hold if one seeks a solution with total profit at least $\\mathrm{OPT}$ and total weight at most $(1 + \\varepsilon)t$. Therefore, it is natural to ask the following question.   If one seeks a solution with total profit at least $\\mathrm{OPT}/(1+\\varepsilon)$ and total weight at most $(1 + \\varepsilon)t$, can Knsapck be solved in $\\tilde{O}(n + (\\frac{1}{\\varepsilon})^{2-\\delta})$ time for some constant $\\delta > 0$?   We answer this open question affirmatively by proposing an $\\tilde{O}(n + (\\frac{1}{\\varepsilon})^{7/4})$-time algorithm.","sentences":["We consider the classic Knapsack problem.","Let $t$ and $\\mathrm{OPT}$ be the capacity and the optimal value, respectively.","If one seeks a solution with total profit at least $\\mathrm{OPT}/(1 + \\varepsilon)$ and total weight at most $t$, then Knapsack can be solved in $\\tilde{O}(n + (\\frac{1}{\\varepsilon})^2)$ time","[Chen, Lian, Mao, and Zhang '24][Mao '24].","This running time is the best possible (up to a logarithmic factor), assuming that $(\\min,+)$-convolution cannot be solved in truly subquadratic time [K\\\"unnemann, Paturi, and Schneider '17][Cygan, Mucha, W\\k{e}grzycki, and W{\\l}odarczyk '19].","The same upper and lower bounds hold if one seeks a solution with total profit at least $\\mathrm{OPT}$ and total weight at most $(1 + \\varepsilon)t$.","Therefore, it is natural to ask the following question.   ","If one seeks a solution with total profit at least $\\mathrm{OPT}/(1+\\varepsilon)$ and total weight at most $(1 + \\varepsilon)t$, can Knsapck be solved in $\\tilde{O}(n + (\\frac{1}{\\varepsilon})^{2-\\delta})$ time for some constant $\\delta > 0$?   ","We answer this open question affirmatively by proposing an $\\tilde{O}(n + (\\frac{1}{\\varepsilon})^{7/4})$-time algorithm."],"url":"http://arxiv.org/abs/2504.15001v1"}
{"created":"2025-04-21 09:51:24","title":"Learning Compositional Transferability of Time Series for Source-Free Domain Adaptation","abstract":"Domain adaptation is challenging for time series classification due to the highly dynamic nature. This study tackles the most difficult subtask when both target labels and source data are inaccessible, namely, source-free domain adaptation. To reuse the classification backbone pre-trained on source data, time series reconstruction is a sound solution that aligns target and source time series by minimizing the reconstruction errors of both. However, simply fine-tuning the source pre-trained reconstruction model on target data may lose the learnt priori, and it struggles to accommodate domain varying temporal patterns in a single encoder-decoder. Therefore, this paper tries to disentangle the composition of domain transferability by using a compositional architecture for time series reconstruction. Here, the preceding component is a U-net frozen since pre-trained, the output of which during adaptation is the initial reconstruction of a given target time series, acting as a coarse step to prompt the subsequent finer adaptation. The following pipeline for finer adaptation includes two parallel branches: The source replay branch using a residual link to preserve the output of U-net, and the offset compensation branch that applies an additional autoencoder (AE) to further warp U-net's output. By deploying a learnable factor on either branch to scale their composition in the final output of reconstruction, the data transferability is disentangled and the learnt reconstructive capability from source data is retained. During inference, aside from the batch-level optimization in the training, we search at test time stability-aware rescaling of source replay branch to tolerate instance-wise variation. The experimental results show that such compositional architecture of time series reconstruction leads to SOTA performance on 3 widely used benchmarks.","sentences":["Domain adaptation is challenging for time series classification due to the highly dynamic nature.","This study tackles the most difficult subtask when both target labels and source data are inaccessible, namely, source-free domain adaptation.","To reuse the classification backbone pre-trained on source data, time series reconstruction is a sound solution that aligns target and source time series by minimizing the reconstruction errors of both.","However, simply fine-tuning the source pre-trained reconstruction model on target data may lose the learnt priori, and it struggles to accommodate domain varying temporal patterns in a single encoder-decoder.","Therefore, this paper tries to disentangle the composition of domain transferability by using a compositional architecture for time series reconstruction.","Here, the preceding component is a U-net frozen since pre-trained, the output of which during adaptation is the initial reconstruction of a given target time series, acting as a coarse step to prompt the subsequent finer adaptation.","The following pipeline for finer adaptation includes two parallel branches: The source replay branch using a residual link to preserve the output of U-net, and the offset compensation branch that applies an additional autoencoder (AE) to further warp U-net's output.","By deploying a learnable factor on either branch to scale their composition in the final output of reconstruction, the data transferability is disentangled and the learnt reconstructive capability from source data is retained.","During inference, aside from the batch-level optimization in the training, we search at test time stability-aware rescaling of source replay branch to tolerate instance-wise variation.","The experimental results show that such compositional architecture of time series reconstruction leads to SOTA performance on 3 widely used benchmarks."],"url":"http://arxiv.org/abs/2504.14994v1"}
{"created":"2025-04-21 09:51:18","title":"Dual Utilization of Perturbation for Stream Data Publication under Local Differential Privacy","abstract":"Stream data from real-time distributed systems such as IoT, tele-health, and crowdsourcing has become an important data source. However, the collection and analysis of user-generated stream data raise privacy concerns due to the potential exposure of sensitive information. To address these concerns, local differential privacy (LDP) has emerged as a promising standard. Nevertheless, applying LDP to stream data presents significant challenges, as stream data often involves a large or even infinite number of values. Allocating a given privacy budget across these data points would introduce overwhelming LDP noise to the original stream data.   Beyond existing approaches that merely use perturbed values for estimating statistics, our design leverages them for both perturbation and estimation. This dual utilization arises from a key observation: each user knows their own ground truth and perturbed values, enabling a precise computation of the deviation error caused by perturbation. By incorporating this deviation into the perturbation process of subsequent values, the previous noise can be calibrated. Following this insight, we introduce the Iterative Perturbation Parameterization (IPP) method, which utilizes current perturbed results to calibrate the subsequent perturbation process. To enhance the robustness of calibration and reduce sensitivity, two algorithms, namely Accumulated Perturbation Parameterization (APP) and Clipped Accumulated Perturbation Parameterization (CAPP) are further developed. We prove that these three algorithms satisfy $w$-event differential privacy while significantly improving utility. Experimental results demonstrate that our techniques outperform state-of-the-art LDP stream publishing solutions in terms of utility, while retaining the same privacy guarantee.","sentences":["Stream data from real-time distributed systems such as IoT, tele-health, and crowdsourcing has become an important data source.","However, the collection and analysis of user-generated stream data raise privacy concerns due to the potential exposure of sensitive information.","To address these concerns, local differential privacy (LDP) has emerged as a promising standard.","Nevertheless, applying LDP to stream data presents significant challenges, as stream data often involves a large or even infinite number of values.","Allocating a given privacy budget across these data points would introduce overwhelming LDP noise to the original stream data.   ","Beyond existing approaches that merely use perturbed values for estimating statistics, our design leverages them for both perturbation and estimation.","This dual utilization arises from a key observation: each user knows their own ground truth and perturbed values, enabling a precise computation of the deviation error caused by perturbation.","By incorporating this deviation into the perturbation process of subsequent values, the previous noise can be calibrated.","Following this insight, we introduce the Iterative Perturbation Parameterization (IPP) method, which utilizes current perturbed results to calibrate the subsequent perturbation process.","To enhance the robustness of calibration and reduce sensitivity, two algorithms, namely Accumulated Perturbation Parameterization (APP) and Clipped Accumulated Perturbation Parameterization (CAPP) are further developed.","We prove that these three algorithms satisfy $w$-event differential privacy while significantly improving utility.","Experimental results demonstrate that our techniques outperform state-of-the-art LDP stream publishing solutions in terms of utility, while retaining the same privacy guarantee."],"url":"http://arxiv.org/abs/2504.14993v1"}
{"created":"2025-04-21 09:38:38","title":"Dynamic Legged Ball Manipulation on Rugged Terrains with Hierarchical Reinforcement Learning","abstract":"Advancing the dynamic loco-manipulation capabilities of quadruped robots in complex terrains is crucial for performing diverse tasks. Specifically, dynamic ball manipulation in rugged environments presents two key challenges. The first is coordinating distinct motion modalities to integrate terrain traversal and ball control seamlessly. The second is overcoming sparse rewards in end-to-end deep reinforcement learning, which impedes efficient policy convergence. To address these challenges, we propose a hierarchical reinforcement learning framework. A high-level policy, informed by proprioceptive data and ball position, adaptively switches between pre-trained low-level skills such as ball dribbling and rough terrain navigation. We further propose Dynamic Skill-Focused Policy Optimization to suppress gradients from inactive skills and enhance critical skill learning. Both simulation and real-world experiments validate that our methods outperform baseline approaches in dynamic ball manipulation across rugged terrains, highlighting its effectiveness in challenging environments. Videos are on our website: dribble-hrl.github.io.","sentences":["Advancing the dynamic loco-manipulation capabilities of quadruped robots in complex terrains is crucial for performing diverse tasks.","Specifically, dynamic ball manipulation in rugged environments presents two key challenges.","The first is coordinating distinct motion modalities to integrate terrain traversal and ball control seamlessly.","The second is overcoming sparse rewards in end-to-end deep reinforcement learning, which impedes efficient policy convergence.","To address these challenges, we propose a hierarchical reinforcement learning framework.","A high-level policy, informed by proprioceptive data and ball position, adaptively switches between pre-trained low-level skills such as ball dribbling and rough terrain navigation.","We further propose Dynamic Skill-Focused Policy Optimization to suppress gradients from inactive skills and enhance critical skill learning.","Both simulation and real-world experiments validate that our methods outperform baseline approaches in dynamic ball manipulation across rugged terrains, highlighting its effectiveness in challenging environments.","Videos are on our website: dribble-hrl.github.io."],"url":"http://arxiv.org/abs/2504.14989v1"}
{"created":"2025-04-21 09:30:41","title":"Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation","abstract":"Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal perception capabilities, garnering significant attention. While numerous evaluation studies have emerged, assessing LVLMs both holistically and on specialized tasks, fine-grained image tasks-fundamental to computer vision-remain largely unexplored. To fill this gap, we introduce a comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49 million questions and 3.32 million images. Our evaluation systematically examines LVLMs from both human-oriented and machine-oriented perspectives, focusing on their semantic recognition and fine-grained feature representation capabilities. Through extensive experiments on eight representative LVLMs/VLMs, we uncover key findings regarding the influence of training paradigms, modality alignment, perturbation susceptibility, and fine-grained category reasoning on task performance. This work provides critical insights into the limitations of current LVLMs and offers guidance for future data construction and model design in the development of more advanced LVLMs. Our code is open-source and available at https://github.com/SEU-VIPGroup/FG-BMK.","sentences":["Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal perception capabilities, garnering significant attention.","While numerous evaluation studies have emerged, assessing LVLMs both holistically and on specialized tasks, fine-grained image tasks-fundamental to computer vision-remain largely unexplored.","To fill this gap, we introduce a comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49 million questions and 3.32 million images.","Our evaluation systematically examines LVLMs from both human-oriented and machine-oriented perspectives, focusing on their semantic recognition and fine-grained feature representation capabilities.","Through extensive experiments on eight representative LVLMs/VLMs, we uncover key findings regarding the influence of training paradigms, modality alignment, perturbation susceptibility, and fine-grained category reasoning on task performance.","This work provides critical insights into the limitations of current LVLMs and offers guidance for future data construction and model design in the development of more advanced LVLMs.","Our code is open-source and available at https://github.com/SEU-VIPGroup/FG-BMK."],"url":"http://arxiv.org/abs/2504.14988v1"}
{"created":"2025-04-21 09:26:05","title":"aiXamine: LLM Safety and Security Simplified","abstract":"Evaluating Large Language Models (LLMs) for safety and security remains a complex task, often requiring users to navigate a fragmented landscape of ad hoc benchmarks, datasets, metrics, and reporting formats. To address this challenge, we present aiXamine, a comprehensive black-box evaluation platform for LLM safety and security. aiXamine integrates over 40 tests (i.e., benchmarks) organized into eight key services targeting specific dimensions of safety and security: adversarial robustness, code security, fairness and bias, hallucination, model and data privacy, out-of-distribution (OOD) robustness, over-refusal, and safety alignment. The platform aggregates the evaluation results into a single detailed report per model, providing a detailed breakdown of model performance, test examples, and rich visualizations. We used aiXamine to assess over 50 publicly available and proprietary LLMs, conducting over 2K examinations. Our findings reveal notable vulnerabilities in leading models, including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0. Additionally, we observe that open-source models can match or exceed proprietary models in specific services such as safety alignment, fairness and bias, and OOD robustness. Finally, we identify trade-offs between distillation strategies, model size, training methods, and architectural choices.","sentences":["Evaluating Large Language Models (LLMs) for safety and security remains a complex task, often requiring users to navigate a fragmented landscape of ad hoc benchmarks, datasets, metrics, and reporting formats.","To address this challenge, we present aiXamine, a comprehensive black-box evaluation platform for LLM safety and security.","aiXamine integrates over 40 tests (i.e., benchmarks) organized into eight key services targeting specific dimensions of safety and security: adversarial robustness, code security, fairness and bias, hallucination, model and data privacy, out-of-distribution (OOD) robustness, over-refusal, and safety alignment.","The platform aggregates the evaluation results into a single detailed report per model, providing a detailed breakdown of model performance, test examples, and rich visualizations.","We used aiXamine to assess over 50 publicly available and proprietary LLMs, conducting over 2K examinations.","Our findings reveal notable vulnerabilities in leading models, including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.","Additionally, we observe that open-source models can match or exceed proprietary models in specific services such as safety alignment, fairness and bias, and OOD robustness.","Finally, we identify trade-offs between distillation strategies, model size, training methods, and architectural choices."],"url":"http://arxiv.org/abs/2504.14985v1"}
{"created":"2025-04-21 08:48:24","title":"A Security Framework for General Blockchain Layer 2 Protocols","abstract":"Layer 2 (L2) solutions are the cornerstone of blockchain scalability, enabling high-throughput and low-cost interactions by shifting execution off-chain while maintaining security through interactions with the underlying ledger. Despite their common goals, the principal L2 paradigms -- payment channels, rollups, and sidechains -- differ substantially in architecture and assumptions, making it difficult to comparatively analyze their security and trade-offs.   To address this, we present the first general security framework for L2 protocols. Our framework is based on the IITM-based Universal Composability (iUC) framework, in which L2 protocols are modeled as stateful machines interacting with higher-level protocol users and the underlying ledger. The methodology defines a generic execution environment that captures ledger events, message passing, and adversarial scheduling, and characterizes security through trace-based predicates parameterized by adversarial capabilities and timing assumptions. By abstracting away from protocol-specific details while preserving critical interface and execution behavior, the framework enables modular, protocol-agnostic reasoning and composable security proofs across a wide range of L2 constructions.   To demonstrate its applicability, we analyze an example from each of the three dominant L2 scaling paradigms: a payment channel (Brick), a sidechain (Liquid Network), and a rollup (Arbitrum). By instantiating each within our framework, we derive their security properties and expose trade-offs. These include the time for dispute resolution, distribution of off-chain storage and computation, and varying trust assumptions (e.g., reliance on honest parties or data availability). Our framework unifies the analysis of diverse L2 designs and pinpoints their strengths and limitations, providing a foundation for secure, systematic L2 development.","sentences":["Layer 2 (L2) solutions are the cornerstone of blockchain scalability, enabling high-throughput and low-cost interactions by shifting execution off-chain while maintaining security through interactions with the underlying ledger.","Despite their common goals, the principal L2 paradigms -- payment channels, rollups, and sidechains -- differ substantially in architecture and assumptions, making it difficult to comparatively analyze their security and trade-offs.   ","To address this, we present the first general security framework for L2 protocols.","Our framework is based on the IITM-based Universal Composability (iUC) framework, in which L2 protocols are modeled as stateful machines interacting with higher-level protocol users and the underlying ledger.","The methodology defines a generic execution environment that captures ledger events, message passing, and adversarial scheduling, and characterizes security through trace-based predicates parameterized by adversarial capabilities and timing assumptions.","By abstracting away from protocol-specific details while preserving critical interface and execution behavior, the framework enables modular, protocol-agnostic reasoning and composable security proofs across a wide range of L2 constructions.   ","To demonstrate its applicability, we analyze an example from each of the three dominant L2 scaling paradigms: a payment channel (Brick), a sidechain (Liquid Network), and a rollup (Arbitrum).","By instantiating each within our framework, we derive their security properties and expose trade-offs.","These include the time for dispute resolution, distribution of off-chain storage and computation, and varying trust assumptions (e.g., reliance on honest parties or data availability).","Our framework unifies the analysis of diverse L2 designs and pinpoints their strengths and limitations, providing a foundation for secure, systematic L2 development."],"url":"http://arxiv.org/abs/2504.14965v1"}
{"created":"2025-04-21 08:44:33","title":"Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues","abstract":"Speaker identification using voice recordings leverages unique acoustic features, but this approach fails when only textual data is available. Few approaches have attempted to tackle the problem of identifying speakers solely from text, and the existing ones have primarily relied on traditional methods. In this work, we explore the use of fuzzy fingerprints from large pre-trained models to improve text-based speaker identification. We integrate speaker-specific tokens and context-aware modeling, demonstrating that conversational context significantly boosts accuracy, reaching 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering improved interpretability. Finally, we analyze ambiguous utterances and propose a mechanism to detect speaker-agnostic lines. Our findings highlight key challenges and provide insights for future improvements in text-based speaker identification.","sentences":["Speaker identification using voice recordings leverages unique acoustic features, but this approach fails when only textual data is available.","Few approaches have attempted to tackle the problem of identifying speakers solely from text, and the existing ones have primarily relied on traditional methods.","In this work, we explore the use of fuzzy fingerprints from large pre-trained models to improve text-based speaker identification.","We integrate speaker-specific tokens and context-aware modeling, demonstrating that conversational context significantly boosts accuracy, reaching 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset.","Additionally, we show that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering improved interpretability.","Finally, we analyze ambiguous utterances and propose a mechanism to detect speaker-agnostic lines.","Our findings highlight key challenges and provide insights for future improvements in text-based speaker identification."],"url":"http://arxiv.org/abs/2504.14963v1"}
{"created":"2025-04-21 08:39:47","title":"MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core","abstract":"Mixture of Experts (MoE) models enhance neural network scalability by dynamically selecting relevant experts per input token, enabling larger model sizes while maintaining manageable computation costs. However, efficient training of large-scale MoE models across thousands of GPUs presents significant challenges due to limitations in existing parallelism strategies. We introduce an end-to-end training framework for large-scale MoE models that utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism. Central to our approach is MoE Parallel Folding, a novel strategy that decouples the parallelization of attention and MoE layers in Transformer models, allowing each layer type to adopt optimal parallel configurations. Additionally, we develop a flexible token-level dispatcher that supports both token-dropping and token-dropless MoE training across all five dimensions of parallelism. This dispatcher accommodates dynamic tensor shapes and coordinates different parallelism schemes for Attention and MoE layers, facilitating complex parallelism implementations. Our experiments demonstrate significant improvements in training efficiency and scalability. We achieve up to 49.3% Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The framework scales efficiently up to 1,024 GPUs and maintains high performance with sequence lengths up to 128K tokens, validating its effectiveness for large-scale MoE model training. The code is available in Megatron-Core.","sentences":["Mixture of Experts (MoE) models enhance neural network scalability by dynamically selecting relevant experts per input token, enabling larger model sizes while maintaining manageable computation costs.","However, efficient training of large-scale MoE models across thousands of GPUs presents significant challenges due to limitations in existing parallelism strategies.","We introduce an end-to-end training framework for large-scale MoE models that utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.","Central to our approach is MoE Parallel Folding, a novel strategy that decouples the parallelization of attention and MoE layers in Transformer models, allowing each layer type to adopt optimal parallel configurations.","Additionally, we develop a flexible token-level dispatcher that supports both token-dropping and token-dropless MoE training across all five dimensions of parallelism.","This dispatcher accommodates dynamic tensor shapes and coordinates different parallelism schemes for Attention and MoE layers, facilitating complex parallelism implementations.","Our experiments demonstrate significant improvements in training efficiency and scalability.","We achieve up to 49.3% Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods.","The framework scales efficiently up to 1,024 GPUs and maintains high performance with sequence lengths up to 128K tokens, validating its effectiveness for large-scale MoE model training.","The code is available in Megatron-Core."],"url":"http://arxiv.org/abs/2504.14960v1"}
{"created":"2025-04-21 08:38:52","title":"ScaleGuard: Rational and Scalable Configuration Privacy Protection with Topology Expansion","abstract":"As networks grow in size and complexity, safeguarding sensitive data while sharing configuration files is critical for network management and research. Existing anonymization tools primarily hide fields like IP addresses or AS numbers to mitigate direct data exposure. However, they often lack mechanisms to preserve privacy around network scale, an increasingly sensitive aspect that can reveal organizational size or resource distribution. We propose ScaleGuard, which preserves network functional equivalence while adding fake routers and hosts to conceal network scale, and generating complete router configurations that resemble the originals. Our system introduces a graph embedding-based expansion method and k-degree mapping anonymity, reducing unnecessary topology modifications when adversaries only know the original degree sequence. For routing repair, ScaleGuard designs a network repair framework combining SMT and iterative methods, delivering stable performance under randomized link costs and complex cross-protocol routing. Experiment results show that ScaleGuard expands network scale effectively, providing consistent anonymization of topology, scale, and routing, while achieving strong topological rationality, configuration fidelity, and repairing efficiency.","sentences":["As networks grow in size and complexity, safeguarding sensitive data while sharing configuration files is critical for network management and research.","Existing anonymization tools primarily hide fields like IP addresses or AS numbers to mitigate direct data exposure.","However, they often lack mechanisms to preserve privacy around network scale, an increasingly sensitive aspect that can reveal organizational size or resource distribution.","We propose ScaleGuard, which preserves network functional equivalence while adding fake routers and hosts to conceal network scale, and generating complete router configurations that resemble the originals.","Our system introduces a graph embedding-based expansion method and k-degree mapping anonymity, reducing unnecessary topology modifications when adversaries only know the original degree sequence.","For routing repair, ScaleGuard designs a network repair framework combining SMT and iterative methods, delivering stable performance under randomized link costs and complex cross-protocol routing.","Experiment results show that ScaleGuard expands network scale effectively, providing consistent anonymization of topology, scale, and routing, while achieving strong topological rationality, configuration fidelity, and repairing efficiency."],"url":"http://arxiv.org/abs/2504.14959v1"}
{"created":"2025-04-21 08:27:26","title":"Efficient Document Retrieval with G-Retriever","abstract":"Textual data question answering has gained significant attention due to its growing applicability. Recently, a novel approach leveraging the Retrieval-Augmented Generation (RAG) method was introduced, utilizing the Prize-Collecting Steiner Tree (PCST) optimization for sub-graph construction. However, this method focused solely on node attributes, leading to incomplete contextual understanding. In this paper, we propose an enhanced approach that replaces the PCST method with an attention-based sub-graph construction technique, enabling more efficient and context-aware retrieval. Additionally, we encode both node and edge attributes, leading to richer graph representations. Our method also incorporates an improved projection layer and multi-head attention pooling for better alignment with Large Language Models (LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our approach is competitive and achieves marginally better results compared to the original method, underscoring its potential for more accurate question answering.","sentences":["Textual data question answering has gained significant attention due to its growing applicability.","Recently, a novel approach leveraging the Retrieval-Augmented Generation (RAG) method was introduced, utilizing the Prize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.","However, this method focused solely on node attributes, leading to incomplete contextual understanding.","In this paper, we propose an enhanced approach that replaces the PCST method with an attention-based sub-graph construction technique, enabling more efficient and context-aware retrieval.","Additionally, we encode both node and edge attributes, leading to richer graph representations.","Our method also incorporates an improved projection layer and multi-head attention pooling for better alignment with Large Language Models (LLMs).","Experimental evaluations on the WebQSP dataset demonstrate that our approach is competitive and achieves marginally better results compared to the original method, underscoring its potential for more accurate question answering."],"url":"http://arxiv.org/abs/2504.14955v1"}
{"created":"2025-04-21 08:22:58","title":"PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for PIV","abstract":"Deep learning algorithms have significantly reduced the computational time and improved the spatial resolution of particle image velocimetry~(PIV). However, the models trained on synthetic datasets might have a degraded performance on practical particle images due to domain gaps. As a result, special residual patterns are often observed for the vector fields of deep learning-based estimators. To reduce the special noise step-by-step, we employ a denoising diffusion model~(FlowDiffuser) for PIV analysis. And the data-hungry iterative denoising diffusion model is trained via a transfer learning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1) pre-training a FlowDiffuser model with multiple optical flow datasets of the computer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the pre-trained model on synthetic PIV datasets. Note that the PIV images are upsampled by a factor of two to resolve the small-scale turbulent flow structures. The visualized results indicate that our PIV-FlowDiffuser effectively suppresses the noise patterns. Therefore, the denoising diffusion model reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV baseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits enhanced generalization performance on unseen particle images due to transfer learning. Overall, this study highlights the transfer-learning-based denoising diffusion models for PIV. And a detailed implementation is recommended for interested readers in the repository https://github.com/Zhu-Qianyu/PIV-FlowDiffuser.","sentences":["Deep learning algorithms have significantly reduced the computational time and improved the spatial resolution of particle image velocimetry~(PIV).","However, the models trained on synthetic datasets might have a degraded performance on practical particle images due to domain gaps.","As a result, special residual patterns are often observed for the vector fields of deep learning-based estimators.","To reduce the special noise step-by-step, we employ a denoising diffusion model~(FlowDiffuser) for PIV analysis.","And the data-hungry iterative denoising diffusion model is trained via a transfer learning strategy, resulting in our PIV-FlowDiffuser method.","Specifically, (1) pre-training a FlowDiffuser model with multiple optical flow datasets of the computer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the pre-trained model on synthetic PIV datasets.","Note that the PIV images are upsampled by a factor of two to resolve the small-scale turbulent flow structures.","The visualized results indicate that our PIV-FlowDiffuser effectively suppresses the noise patterns.","Therefore, the denoising diffusion model reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV baseline on the classic Cai's dataset.","Besides, PIV-FlowDiffuser exhibits enhanced generalization performance on unseen particle images due to transfer learning.","Overall, this study highlights the transfer-learning-based denoising diffusion models for PIV.","And a detailed implementation is recommended for interested readers in the repository https://github.com/Zhu-Qianyu/PIV-FlowDiffuser."],"url":"http://arxiv.org/abs/2504.14952v1"}
{"created":"2025-04-21 08:10:59","title":"Generative Semantic Communications: Principles and Practices","abstract":"Semantic communication leverages artificial intelligence (AI) technologies to extract semantic information from data for efficient transmission, theraby significantly reducing communication cost. With the evolution towards artificial general intelligence (AGI), the increasing demands for AGI services pose new challenges to semantic communication. In response, we propose a new paradigm for AGI-driven communications, called generative semantic communication (GSC), which utilizes advanced AI technologies such as foundation models and generative models. We first describe the basic concept of GSC and its difference from existing semantic communications, and then introduce a general framework of GSC, followed by two case studies to verify the advantages of GSC in AGI-driven applications. Finally, open challenges and new research directions are discussed to stimulate this line of research and pave the way for practical applications.","sentences":["Semantic communication leverages artificial intelligence (AI) technologies to extract semantic information from data for efficient transmission, theraby significantly reducing communication cost.","With the evolution towards artificial general intelligence (AGI), the increasing demands for AGI services pose new challenges to semantic communication.","In response, we propose a new paradigm for AGI-driven communications, called generative semantic communication (GSC), which utilizes advanced AI technologies such as foundation models and generative models.","We first describe the basic concept of GSC and its difference from existing semantic communications, and then introduce a general framework of GSC, followed by two case studies to verify the advantages of GSC in AGI-driven applications.","Finally, open challenges and new research directions are discussed to stimulate this line of research and pave the way for practical applications."],"url":"http://arxiv.org/abs/2504.14947v1"}
{"created":"2025-04-21 08:01:32","title":"Causal DAG Summarization (Full Version)","abstract":"Causal inference aids researchers in discovering cause-and-effect relationships, leading to scientific insights. Accurate causal estimation requires identifying confounding variables to avoid false discoveries. Pearl's causal model uses causal DAGs to identify confounding variables, but incorrect DAGs can lead to unreliable causal conclusions. However, for high dimensional data, the causal DAGs are often complex beyond human verifiability. Graph summarization is a logical next step, but current methods for general-purpose graph summarization are inadequate for causal DAG summarization. This paper addresses these challenges by proposing a causal graph summarization objective that balances graph simplification for better understanding while retaining essential causal information for reliable inference. We develop an efficient greedy algorithm and show that summary causal DAGs can be directly used for inference and are more robust to misspecification of assumptions, enhancing robustness for causal inference. Experimenting with six real-life datasets, we compared our algorithm to three existing solutions, showing its effectiveness in handling high-dimensional data and its ability to generate summary DAGs that ensure both reliable causal inference and robustness against misspecifications.","sentences":["Causal inference aids researchers in discovering cause-and-effect relationships, leading to scientific insights.","Accurate causal estimation requires identifying confounding variables to avoid false discoveries.","Pearl's causal model uses causal DAGs to identify confounding variables, but incorrect DAGs can lead to unreliable causal conclusions.","However, for high dimensional data, the causal DAGs are often complex beyond human verifiability.","Graph summarization is a logical next step, but current methods for general-purpose graph summarization are inadequate for causal DAG summarization.","This paper addresses these challenges by proposing a causal graph summarization objective that balances graph simplification for better understanding while retaining essential causal information for reliable inference.","We develop an efficient greedy algorithm and show that summary causal DAGs can be directly used for inference and are more robust to misspecification of assumptions, enhancing robustness for causal inference.","Experimenting with six real-life datasets, we compared our algorithm to three existing solutions, showing its effectiveness in handling high-dimensional data and its ability to generate summary DAGs that ensure both reliable causal inference and robustness against misspecifications."],"url":"http://arxiv.org/abs/2504.14937v1"}
{"created":"2025-04-21 07:53:58","title":"TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models","abstract":"In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard. To address this issue, we propose a novel two-step image generation model inspired by the conditional diffusion model. The first step involves creating an image segmentation mask for some prompt-based generated images. This mask embodies the shape of the image. Thereafter, the diffusion model is asked to generate the image anew while avoiding the shape in question. This approach shows a decrease in structural similarity from the training image, i.e. we are able to avoid the source copying problem using this approach without expensive retraining of the model or user-centered prompt generation techniques. This makes our approach the most computationally inexpensive approach to avoiding both copyright infringement and source copying for diffusion model-based image generation.","sentences":["In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images.","Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use.","As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights.","Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying.","Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard.","To address this issue, we propose a novel two-step image generation model inspired by the conditional diffusion model.","The first step involves creating an image segmentation mask for some prompt-based generated images.","This mask embodies the shape of the image.","Thereafter, the diffusion model is asked to generate the image anew while avoiding the shape in question.","This approach shows a decrease in structural similarity from the training image, i.e. we are able to avoid the source copying problem using this approach without expensive retraining of the model or user-centered prompt generation techniques.","This makes our approach the most computationally inexpensive approach to avoiding both copyright infringement and source copying for diffusion model-based image generation."],"url":"http://arxiv.org/abs/2504.14933v1"}
{"created":"2025-04-21 07:40:35","title":"Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos","abstract":"Adversarial Training (AT) has been shown to significantly enhance adversarial robustness via a min-max optimization approach. However, its effectiveness in video recognition tasks is hampered by two main challenges. First, fast adversarial training for video models remains largely unexplored, which severely impedes its practical applications. Specifically, most video adversarial training methods are computationally costly, with long training times and high expenses. Second, existing methods struggle with the trade-off between clean accuracy and adversarial robustness. To address these challenges, we introduce Video Fast Adversarial Training with Weak-to-Strong consistency (VFAT-WS), the first fast adversarial training method for video data. Specifically, VFAT-WS incorporates the following key designs: First, it integrates a straightforward yet effective temporal frequency augmentation (TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a single-step PGD attack to boost training efficiency and robustness. Second, it devises a weak-to-strong spatial-temporal consistency regularization, which seamlessly integrates the simpler TF-AUG and the more complex STF-AUG. Leveraging the consistency regularization, it steers the learning process from simple to complex augmentations. Both of them work together to achieve a better trade-off between clean accuracy and robustness. Extensive experiments on UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that VFAT-WS achieves great improvements in adversarial robustness and corruption robustness, while accelerating training by nearly 490%.","sentences":["Adversarial Training (AT) has been shown to significantly enhance adversarial robustness via a min-max optimization approach.","However, its effectiveness in video recognition tasks is hampered by two main challenges.","First, fast adversarial training for video models remains largely unexplored, which severely impedes its practical applications.","Specifically, most video adversarial training methods are computationally costly, with long training times and high expenses.","Second, existing methods struggle with the trade-off between clean accuracy and adversarial robustness.","To address these challenges, we introduce Video Fast Adversarial Training with Weak-to-Strong consistency (VFAT-WS), the first fast adversarial training method for video data.","Specifically, VFAT-WS incorporates the following key designs: First, it integrates a straightforward yet effective temporal frequency augmentation (TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a single-step PGD attack to boost training efficiency and robustness.","Second, it devises a weak-to-strong spatial-temporal consistency regularization, which seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.","Leveraging the consistency regularization, it steers the learning process from simple to complex augmentations.","Both of them work together to achieve a better trade-off between clean accuracy and robustness.","Extensive experiments on UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that VFAT-WS achieves great improvements in adversarial robustness and corruption robustness, while accelerating training by nearly 490%."],"url":"http://arxiv.org/abs/2504.14921v1"}
{"created":"2025-04-21 07:39:29","title":"DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding","abstract":"Humans can effortlessly locate desired objects in cluttered environments, relying on a cognitive mechanism known as visual search to efficiently filter out irrelevant information and focus on task-related regions. Inspired by this process, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing visual search method that enhances fine-grained visual understanding in large multimodal models (LMMs). Unlike existing approaches which require additional modules or data collection, Dyfo leverages a bidirectional interaction between LMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to simulate human-like focus adjustments. This enables LMMs to focus on key visual regions while filtering out irrelevant content, without introducing additional training caused by vocabulary expansion or the integration of specialized localization modules. Experimental results demonstrate that Dyfo significantly improves fine-grained visual understanding and reduces hallucination issues in LMMs, achieving superior performance across both fixed and dynamic resolution models. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025","sentences":["Humans can effortlessly locate desired objects in cluttered environments, relying on a cognitive mechanism known as visual search to efficiently filter out irrelevant information and focus on task-related regions.","Inspired by this process, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing visual search method that enhances fine-grained visual understanding in large multimodal models (LMMs).","Unlike existing approaches which require additional modules or data collection, Dyfo leverages a bidirectional interaction between LMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to simulate human-like focus adjustments.","This enables LMMs to focus on key visual regions while filtering out irrelevant content, without introducing additional training caused by vocabulary expansion or the integration of specialized localization modules.","Experimental results demonstrate that Dyfo significantly improves fine-grained visual understanding and reduces hallucination issues in LMMs, achieving superior performance across both fixed and dynamic resolution models.","The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025"],"url":"http://arxiv.org/abs/2504.14920v1"}
{"created":"2025-04-21 07:22:11","title":"Dynamic Graph-Like Learning with Contrastive Clustering on Temporally-Factored Ship Motion Data for Imbalanced Sea State Estimation in Autonomous Vessel","abstract":"Accurate sea state estimation is crucial for the real-time control and future state prediction of autonomous vessels. However, traditional methods struggle with challenges such as data imbalance and feature redundancy in ship motion data, limiting their effectiveness. To address these challenges, we propose the Temporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE), a novel deep learning model that combines three key components: a time dimension factorization module to reduce data redundancy, a dynamic graph-like learning module to capture complex variable interactions, and a contrastive clustering loss function to effectively manage class imbalance. Our experiments demonstrate that TGC-SSE significantly outperforms existing methods across 14 public datasets, achieving the highest accuracy in 9 datasets, with a 20.79% improvement over EDI. Furthermore, in the field of sea state estimation, TGC-SSE surpasses five benchmark methods and seven deep learning models. Ablation studies confirm the effectiveness of each module, demonstrating their respective roles in enhancing overall model performance. Overall, TGC-SSE not only improves the accuracy of sea state estimation but also exhibits strong generalization capabilities, providing reliable support for autonomous vessel operations.","sentences":["Accurate sea state estimation is crucial for the real-time control and future state prediction of autonomous vessels.","However, traditional methods struggle with challenges such as data imbalance and feature redundancy in ship motion data, limiting their effectiveness.","To address these challenges, we propose the Temporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE), a novel deep learning model that combines three key components: a time dimension factorization module to reduce data redundancy, a dynamic graph-like learning module to capture complex variable interactions, and a contrastive clustering loss function to effectively manage class imbalance.","Our experiments demonstrate that TGC-SSE significantly outperforms existing methods across 14 public datasets, achieving the highest accuracy in 9 datasets, with a 20.79% improvement over EDI.","Furthermore, in the field of sea state estimation, TGC-SSE surpasses five benchmark methods and seven deep learning models.","Ablation studies confirm the effectiveness of each module, demonstrating their respective roles in enhancing overall model performance.","Overall, TGC-SSE not only improves the accuracy of sea state estimation but also exhibits strong generalization capabilities, providing reliable support for autonomous vessel operations."],"url":"http://arxiv.org/abs/2504.14907v1"}
{"created":"2025-04-21 07:20:19","title":"VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform","abstract":"Exponentially growing short video platforms (SVPs) face significant challenges in moderating content detrimental to users' mental health, particularly for minors. The dissemination of such content on SVPs can lead to catastrophic societal consequences. Although substantial efforts have been dedicated to moderating such content, existing methods suffer from critical limitations: (1) Manual review is prone to human bias and incurs high operational costs. (2) Automated methods, though efficient, lack nuanced content understanding, resulting in lower accuracy. (3) Industrial moderation regulations struggle to adapt to rapidly evolving trends due to long update cycles. In this paper, we annotate the first SVP content moderation benchmark with authentic user/reviewer feedback to fill the absence of benchmark in this field. Then we evaluate various methods on the benchmark to verify the existence of the aforementioned limitations. We further propose our common-law content moderation framework named KuaiMod to address these challenges. KuaiMod consists of three components: training data construction, offline adaptation, and online deployment & refinement. Leveraging large vision language model (VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video toxicity based on sparse user feedback and fosters dynamic moderation policy with rapid update speed and high accuracy. Offline experiments and large-scale online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the best moderation performance on our benchmark. The deployment of KuaiMod reduces the user reporting rate by 20% and its application in video recommendation increases both Daily Active User (DAU) and APP Usage Time (AUT) on several Kuaishou scenarios. We have open-sourced our benchmark at https://kuaimod.github.io.","sentences":["Exponentially growing short video platforms (SVPs) face significant challenges in moderating content detrimental to users' mental health, particularly for minors.","The dissemination of such content on SVPs can lead to catastrophic societal consequences.","Although substantial efforts have been dedicated to moderating such content, existing methods suffer from critical limitations: (1) Manual review is prone to human bias and incurs high operational costs.","(2) Automated methods, though efficient, lack nuanced content understanding, resulting in lower accuracy.","(3) Industrial moderation regulations struggle to adapt to rapidly evolving trends due to long update cycles.","In this paper, we annotate the first SVP content moderation benchmark with authentic user/reviewer feedback to fill the absence of benchmark in this field.","Then we evaluate various methods on the benchmark to verify the existence of the aforementioned limitations.","We further propose our common-law content moderation framework named KuaiMod to address these challenges.","KuaiMod consists of three components: training data construction, offline adaptation, and online deployment & refinement.","Leveraging large vision language model (VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video toxicity based on sparse user feedback and fosters dynamic moderation policy with rapid update speed and high accuracy.","Offline experiments and large-scale online A/B test demonstrates the superiority of KuaiMod:","KuaiMod achieves the best moderation performance on our benchmark.","The deployment of KuaiMod reduces the user reporting rate by 20% and its application in video recommendation increases both Daily Active User (DAU) and APP Usage Time (AUT) on several Kuaishou scenarios.","We have open-sourced our benchmark at https://kuaimod.github.io."],"url":"http://arxiv.org/abs/2504.14904v1"}
{"created":"2025-04-21 07:10:41","title":"Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation","abstract":"Camera and human motion controls have been extensively studied for video generation, but existing approaches typically address them separately, suffering from limited data with high-quality annotations for both aspects. To overcome this, we present Uni3C, a unified 3D-enhanced framework for precise control of both camera and human motion in video generation. Uni3C includes two key contributions. First, we propose a plug-and-play control module trained with a frozen video generative backbone, PCDController, which utilizes unprojected point clouds from monocular depth to achieve accurate camera control. By leveraging the strong 3D priors of point clouds and the powerful capacities of video foundational models, PCDController shows impressive generalization, performing well regardless of whether the inference backbone is frozen or fine-tuned. This flexibility enables different modules of Uni3C to be trained in specific domains, i.e., either camera control or human motion control, reducing the dependency on jointly annotated data. Second, we propose a jointly aligned 3D world guidance for the inference phase that seamlessly integrates both scenic point clouds and SMPL-X characters to unify the control signals for camera and human motion, respectively. Extensive experiments confirm that PCDController enjoys strong robustness in driving camera motion for fine-tuned backbones of video generation. Uni3C substantially outperforms competitors in both camera controllability and human motion quality. Additionally, we collect tailored validation sets featuring challenging camera movements and human actions to validate the effectiveness of our method.","sentences":["Camera and human motion controls have been extensively studied for video generation, but existing approaches typically address them separately, suffering from limited data with high-quality annotations for both aspects.","To overcome this, we present Uni3C, a unified 3D-enhanced framework for precise control of both camera and human motion in video generation.","Uni3C includes two key contributions.","First, we propose a plug-and-play control module trained with a frozen video generative backbone, PCDController, which utilizes unprojected point clouds from monocular depth to achieve accurate camera control.","By leveraging the strong 3D priors of point clouds and the powerful capacities of video foundational models, PCDController shows impressive generalization, performing well regardless of whether the inference backbone is frozen or fine-tuned.","This flexibility enables different modules of Uni3C to be trained in specific domains, i.e., either camera control or human motion control, reducing the dependency on jointly annotated data.","Second, we propose a jointly aligned 3D world guidance for the inference phase that seamlessly integrates both scenic point clouds and SMPL-X characters to unify the control signals for camera and human motion, respectively.","Extensive experiments confirm that PCDController enjoys strong robustness in driving camera motion for fine-tuned backbones of video generation.","Uni3C substantially outperforms competitors in both camera controllability and human motion quality.","Additionally, we collect tailored validation sets featuring challenging camera movements and human actions to validate the effectiveness of our method."],"url":"http://arxiv.org/abs/2504.14899v1"}
{"created":"2025-04-21 07:07:49","title":"Physics-Aware Compression of Plasma Distribution Functions with GPU-Accelerated Gaussian Mixture Models","abstract":"Data compression is a critical technology for large-scale plasma simulations. Storing complete particle information requires Terabyte-scale data storage, and analysis requires ad-hoc scalable post-processing tools. We propose a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) to approximate electron and ion velocity distribution functions with a number of Gaussian components. This GMM-based method allows us to capture plasma features such as mean velocity and temperature, and it enables us to identify heating processes and generate beams. We first construct a histogram to reduce computational overhead and apply GPU-accelerated, in-situ GMM fitting within \\texttt{iPIC3D}, a large-scale implicit Particle-in-Cell simulator, ensuring real-time compression. The compressed representation is stored using the \\texttt{ADIOS 2} library, thus optimizing the I/O process. The GPU and histogramming implementation provides a significant speed-up with respect to GMM on particles (both in time and required memory at run-time), enabling real-time compression. Compared to algorithms like SZ, MGARD, and BLOSC2, our GMM-based method has a physics-based approach, retaining the physical interpretation of plasma phenomena such as beam formation, acceleration, and heating mechanisms. Our GMM algorithm achieves a compression ratio of up to $10^4$, requiring a processing time comparable to, or even lower than, standard compression engines.","sentences":["Data compression is a critical technology for large-scale plasma simulations.","Storing complete particle information requires Terabyte-scale data storage, and analysis requires ad-hoc scalable post-processing tools.","We propose a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) to approximate electron and ion velocity distribution functions with a number of Gaussian components.","This GMM-based method allows us to capture plasma features such as mean velocity and temperature, and it enables us to identify heating processes and generate beams.","We first construct a histogram to reduce computational overhead and apply GPU-accelerated, in-situ GMM fitting within \\texttt{iPIC3D}, a large-scale implicit Particle-in-Cell simulator, ensuring real-time compression.","The compressed representation is stored using the \\texttt{ADIOS 2} library, thus optimizing the I/O process.","The GPU and histogramming implementation provides a significant speed-up with respect to GMM on particles (both in time and required memory at run-time), enabling real-time compression.","Compared to algorithms like SZ, MGARD, and BLOSC2, our GMM-based method has a physics-based approach, retaining the physical interpretation of plasma phenomena such as beam formation, acceleration, and heating mechanisms.","Our GMM algorithm achieves a compression ratio of up to $10^4$, requiring a processing time comparable to, or even lower than, standard compression engines."],"url":"http://arxiv.org/abs/2504.14897v1"}
{"created":"2025-04-21 06:47:46","title":"Never too Cocky to Cooperate: An FIM and RL-based USV-AUV Collaborative System for Underwater Tasks in Extreme Sea Conditions","abstract":"This paper develops a novel unmanned surface vehicle (USV)-autonomous underwater vehicle (AUV) collaborative system designed to enhance underwater task performance in extreme sea conditions. The system integrates a dual strategy: (1) high-precision multi-AUV localization enabled by Fisher information matrix-optimized USV path planning, and (2) reinforcement learning-based cooperative planning and control method for multi-AUV task execution. Extensive experimental evaluations in the underwater data collection task demonstrate the system's operational feasibility, with quantitative results showing significant performance improvements over baseline methods. The proposed system exhibits robust coordination capabilities between USV and AUVs while maintaining stability in extreme sea conditions. To facilitate reproducibility and community advancement, we provide an open-source simulation toolkit available at: https://github.com/360ZMEM/USV-AUV-colab .","sentences":["This paper develops a novel unmanned surface vehicle (USV)-autonomous underwater vehicle (AUV) collaborative system designed to enhance underwater task performance in extreme sea conditions.","The system integrates a dual strategy: (1) high-precision multi-AUV localization enabled by Fisher information matrix-optimized USV path planning, and (2) reinforcement learning-based cooperative planning and control method for multi-AUV task execution.","Extensive experimental evaluations in the underwater data collection task demonstrate the system's operational feasibility, with quantitative results showing significant performance improvements over baseline methods.","The proposed system exhibits robust coordination capabilities between USV and AUVs while maintaining stability in extreme sea conditions.","To facilitate reproducibility and community advancement, we provide an open-source simulation toolkit available at: https://github.com/360ZMEM/USV-AUV-colab ."],"url":"http://arxiv.org/abs/2504.14894v1"}
{"created":"2025-04-21 06:36:09","title":"Latent Bayesian Optimization via Autoregressive Normalizing Flows","abstract":"Bayesian Optimization (BO) has been recognized for its effectiveness in optimizing expensive and complex objective functions. Recent advancements in Latent Bayesian Optimization (LBO) have shown promise by integrating generative models such as variational autoencoders (VAEs) to manage the complexity of high-dimensional and structured data spaces. However, existing LBO approaches often suffer from the value discrepancy problem, which arises from the reconstruction gap between input and latent spaces. This value discrepancy problem propagates errors throughout the optimization process, leading to suboptimal outcomes. To address this issue, we propose a Normalizing Flow-based Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative model to establish one-to-one encoding function from the input space to the latent space, along with its left-inverse decoding function, eliminating the reconstruction gap. Specifically, we introduce SeqFlow, an autoregressive normalizing flow for sequence data. In addition, we develop a new candidate sampling strategy that dynamically adjusts the exploration probability for each token based on its importance. Through extensive experiments, our NF-BO method demonstrates superior performance in molecule generation tasks, significantly outperforming both traditional and recent LBO approaches.","sentences":["Bayesian Optimization (BO) has been recognized for its effectiveness in optimizing expensive and complex objective functions.","Recent advancements in Latent Bayesian Optimization (LBO) have shown promise by integrating generative models such as variational autoencoders (VAEs) to manage the complexity of high-dimensional and structured data spaces.","However, existing LBO approaches often suffer from the value discrepancy problem, which arises from the reconstruction gap between input and latent spaces.","This value discrepancy problem propagates errors throughout the optimization process, leading to suboptimal outcomes.","To address this issue, we propose a Normalizing Flow-based Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative model to establish one-to-one encoding function from the input space to the latent space, along with its left-inverse decoding function, eliminating the reconstruction gap.","Specifically, we introduce SeqFlow, an autoregressive normalizing flow for sequence data.","In addition, we develop a new candidate sampling strategy that dynamically adjusts the exploration probability for each token based on its importance.","Through extensive experiments, our NF-BO method demonstrates superior performance in molecule generation tasks, significantly outperforming both traditional and recent LBO approaches."],"url":"http://arxiv.org/abs/2504.14889v1"}
{"created":"2025-04-21 06:30:21","title":"Zero Day Malware Detection with Alpha: Fast DBI with Transformer Models for Real World Application","abstract":"The effectiveness of an AI model in accurately classifying novel malware hinges on the quality of the features it is trained on, which in turn depends on the effectiveness of the analysis tool used. Peekaboo, a Dynamic Binary Instrumentation (DBI) tool, defeats malware evasion techniques to capture authentic behavior at the Assembly (ASM) instruction level. This behavior exhibits patterns consistent with Zipf's law, a distribution commonly seen in natural languages, making Transformer models particularly effective for binary classification tasks. We introduce Alpha, a framework for zero day malware detection that leverages Transformer models and ASM language. Alpha is trained on malware and benign software data collected through Peekaboo, enabling it to identify entirely new samples with exceptional accuracy. Alpha eliminates any common functions from the test samples that are in the training dataset. This forces the model to rely on contextual patterns and novel ASM instruction combinations to detect malicious behavior, rather than memorizing familiar features. By combining the strengths of DBI, ASM analysis, and Transformer architectures, Alpha offers a powerful approach to proactively addressing the evolving threat of malware. Alpha demonstrates perfect accuracy for Ransomware, Worms and APTs with flawless classification for both malicious and benign samples. The results highlight the model's exceptional performance in detecting truly new malware samples.","sentences":["The effectiveness of an AI model in accurately classifying novel malware hinges on the quality of the features it is trained on, which in turn depends on the effectiveness of the analysis tool used.","Peekaboo, a Dynamic Binary Instrumentation (DBI) tool, defeats malware evasion techniques to capture authentic behavior at the Assembly (ASM) instruction level.","This behavior exhibits patterns consistent with Zipf's law, a distribution commonly seen in natural languages, making Transformer models particularly effective for binary classification tasks.","We introduce Alpha, a framework for zero day malware detection that leverages Transformer models and ASM language.","Alpha is trained on malware and benign software data collected through Peekaboo, enabling it to identify entirely new samples with exceptional accuracy.","Alpha eliminates any common functions from the test samples that are in the training dataset.","This forces the model to rely on contextual patterns and novel ASM instruction combinations to detect malicious behavior, rather than memorizing familiar features.","By combining the strengths of DBI, ASM analysis, and Transformer architectures, Alpha offers a powerful approach to proactively addressing the evolving threat of malware.","Alpha demonstrates perfect accuracy for Ransomware, Worms and APTs with flawless classification for both malicious and benign samples.","The results highlight the model's exceptional performance in detecting truly new malware samples."],"url":"http://arxiv.org/abs/2504.14886v1"}
{"created":"2025-04-21 06:07:32","title":"Collaborative Enhancement Network for Low-quality Multi-spectral Vehicle Re-identification","abstract":"The performance of multi-spectral vehicle Re-identification (ReID) is significantly degraded when some important discriminative cues in visible, near infrared and thermal infrared spectra are lost. Existing methods generate or enhance missing details in low-quality spectra data using the high-quality one, generally called the primary spectrum, but how to justify the primary spectrum is a challenging problem. In addition, when the quality of the primary spectrum is low, the enhancement effect would be greatly degraded, thus limiting the performance of multi-spectral vehicle ReID. To address these problems, we propose the Collaborative Enhancement Network (CoEN), which generates a high-quality proxy from all spectra data and leverages it to supervise the selection of primary spectrum and enhance all spectra features in a collaborative manner, for robust multi-spectral vehicle ReID. First, to integrate the rich cues from all spectra data, we design the Proxy Generator (PG) to progressively aggregate multi-spectral features. Second, we design the Dynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring their correlations with the proxy, to accurately select the primary spectra with the highest correlation. Finally, we design the Collaborative Enhancement Module (CEM) to effectively compensate for missing contents of all spectra by collaborating the primary spectra and the proxy, thereby mitigating the impact of low-quality primary spectra. Extensive experiments on three benchmark datasets are conducted to validate the efficacy of the proposed approach against other multi-spectral vehicle ReID methods. The codes will be released at https://github.com/yongqisun/CoEN.","sentences":["The performance of multi-spectral vehicle Re-identification (ReID) is significantly degraded when some important discriminative cues in visible, near infrared and thermal infrared spectra are lost.","Existing methods generate or enhance missing details in low-quality spectra data using the high-quality one, generally called the primary spectrum, but how to justify the primary spectrum is a challenging problem.","In addition, when the quality of the primary spectrum is low, the enhancement effect would be greatly degraded, thus limiting the performance of multi-spectral vehicle ReID.","To address these problems, we propose the Collaborative Enhancement Network (CoEN), which generates a high-quality proxy from all spectra data and leverages it to supervise the selection of primary spectrum and enhance all spectra features in a collaborative manner, for robust multi-spectral vehicle ReID.","First, to integrate the rich cues from all spectra data, we design the Proxy Generator (PG) to progressively aggregate multi-spectral features.","Second, we design the Dynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring their correlations with the proxy, to accurately select the primary spectra with the highest correlation.","Finally, we design the Collaborative Enhancement Module (CEM) to effectively compensate for missing contents of all spectra by collaborating the primary spectra and the proxy, thereby mitigating the impact of low-quality primary spectra.","Extensive experiments on three benchmark datasets are conducted to validate the efficacy of the proposed approach against other multi-spectral vehicle ReID methods.","The codes will be released at https://github.com/yongqisun/CoEN."],"url":"http://arxiv.org/abs/2504.14877v1"}
{"created":"2025-04-21 06:02:03","title":"ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams","abstract":"The rapid growth of video-text data presents challenges in storage and computation during training. Online learning, which processes streaming data in real-time, offers a promising solution to these issues while also allowing swift adaptations in scenarios demanding real-time responsiveness. One strategy to enhance the efficiency and effectiveness of learning involves identifying and prioritizing data that enhances performance on target downstream tasks. We propose Relevance and Specificity-based online filtering framework (ReSpec) that selects data based on four criteria: (i) modality alignment for clean data, (ii) task relevance for target focused data, (iii) specificity for informative and detailed data, and (iv) efficiency for low-latency processing. Relevance is determined by the probabilistic alignment of incoming data with downstream tasks, while specificity employs the distance to a root embedding representing the least specific data as an efficient proxy for informativeness. By establishing reference points from target task data, ReSpec filters incoming data in real-time, eliminating the need for extensive storage and compute. Evaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains state-of-the-art performance on five zeroshot video retrieval tasks, using as little as 5% of the data while incurring minimal compute. The source code is available at https://github.com/cdjkim/ReSpec.","sentences":["The rapid growth of video-text data presents challenges in storage and computation during training.","Online learning, which processes streaming data in real-time, offers a promising solution to these issues while also allowing swift adaptations in scenarios demanding real-time responsiveness.","One strategy to enhance the efficiency and effectiveness of learning involves identifying and prioritizing data that enhances performance on target downstream tasks.","We propose Relevance and Specificity-based online filtering framework (ReSpec) that selects data based on four criteria: (i) modality alignment for clean data, (ii) task relevance for target focused data, (iii) specificity for informative and detailed data, and (iv) efficiency for low-latency processing.","Relevance is determined by the probabilistic alignment of incoming data with downstream tasks, while specificity employs the distance to a root embedding representing the least specific data as an efficient proxy for informativeness.","By establishing reference points from target task data, ReSpec filters incoming data in real-time, eliminating the need for extensive storage and compute.","Evaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains state-of-the-art performance on five zeroshot video retrieval tasks, using as little as 5% of the data while incurring minimal compute.","The source code is available at https://github.com/cdjkim/ReSpec."],"url":"http://arxiv.org/abs/2504.14875v1"}
{"created":"2025-04-21 05:57:34","title":"Efficient Function Orchestration for Large Language Models","abstract":"Function calling is a fundamental capability of today's large language models, but sequential function calling posed efficiency problems. Recent studies have proposed to request function calls with parallelism support in order to alleviate this issue. However, they either delegate the concurrent function calls to users for execution which are conversely executed sequentially, or overlook the relations among various function calls, rending limited efficiency. This paper introduces LLMOrch, an advanced framework for automated, parallel function calling in large language models. The key principle behind LLMOrch is to identify an available processor to execute a function call while preventing any single processor from becoming overburdened. To this end, LLMOrch models the data relations (i.e., def-use) among different function calls and coordinates their executions by their control relations (i.e., mutual-exclusion) as well as the working status of the underlying processors. When comparing with state-of-the-art techniques, LLMOrch demonstrated comparable efficiency improvements in orchestrating I/O-intensive functions, while significantly outperforming (2$\\times$) them with compute-intensive functions. LLMOrch's performance even showed a linear correlation to the number of allocated processors. We believe that these results highlight the potential of LLMOrch as an efficient solution for parallel function orchestration in the context of large language models.","sentences":["Function calling is a fundamental capability of today's large language models, but sequential function calling posed efficiency problems.","Recent studies have proposed to request function calls with parallelism support in order to alleviate this issue.","However, they either delegate the concurrent function calls to users for execution which are conversely executed sequentially, or overlook the relations among various function calls, rending limited efficiency.","This paper introduces LLMOrch, an advanced framework for automated, parallel function calling in large language models.","The key principle behind LLMOrch is to identify an available processor to execute a function call while preventing any single processor from becoming overburdened.","To this end, LLMOrch models the data relations (i.e., def-use) among different function calls and coordinates their executions by their control relations (i.e., mutual-exclusion) as well as the working status of the underlying processors.","When comparing with state-of-the-art techniques, LLMOrch demonstrated comparable efficiency improvements in orchestrating I/O-intensive functions, while significantly outperforming (2$\\times$) them with compute-intensive functions.","LLMOrch's performance even showed a linear correlation to the number of allocated processors.","We believe that these results highlight the potential of LLMOrch as an efficient solution for parallel function orchestration in the context of large language models."],"url":"http://arxiv.org/abs/2504.14872v1"}
{"created":"2025-04-21 05:48:52","title":"Natural Fingerprints of Large Language Models","abstract":"Large language models (LLMs) often exhibit biases -- systematic deviations from expected norms -- in their outputs. These range from overt issues, such as unfair responses, to subtler patterns that can reveal which model produced them. We investigate the factors that give rise to identifiable characteristics in LLMs. Since LLMs model training data distribution, it is reasonable that differences in training data naturally lead to the characteristics. However, our findings reveal that even when LLMs are trained on the exact same data, it is still possible to distinguish the source model based on its generated text. We refer to these unintended, distinctive characteristics as natural fingerprints. By systematically controlling training conditions, we show that the natural fingerprints can emerge from subtle differences in the training process, such as parameter sizes, optimization settings, and even random seeds. We believe that understanding natural fingerprints offers new insights into the origins of unintended bias and ways for improving control over LLM behavior.","sentences":["Large language models (LLMs) often exhibit biases -- systematic deviations from expected norms -- in their outputs.","These range from overt issues, such as unfair responses, to subtler patterns that can reveal which model produced them.","We investigate the factors that give rise to identifiable characteristics in LLMs.","Since LLMs model training data distribution, it is reasonable that differences in training data naturally lead to the characteristics.","However, our findings reveal that even when LLMs are trained on the exact same data, it is still possible to distinguish the source model based on its generated text.","We refer to these unintended, distinctive characteristics as natural fingerprints.","By systematically controlling training conditions, we show that the natural fingerprints can emerge from subtle differences in the training process, such as parameter sizes, optimization settings, and even random seeds.","We believe that understanding natural fingerprints offers new insights into the origins of unintended bias and ways for improving control over LLM behavior."],"url":"http://arxiv.org/abs/2504.14871v1"}
{"created":"2025-04-21 05:27:33","title":"GainSight: Application-Guided Profiling for Composing Heterogeneous On-Chip Memories in AI Hardware Accelerators","abstract":"As AI workloads drive soaring memory requirements, there is a need for higher-density on-chip memory for domain-specific accelerators that goes beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, there has been little work in factoring dynamic application profiles into such design decisions. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and computes data lifetimes in domain-specific accelerators. By combining instrumentation and simulation across retargetable hardware backends, GainSight aligns heterogeneous memory designs with workload-specific traffic and lifetime metrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA H100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3) Up to 90% of GPU cache fetches are never reused, highlighting inefficiencies in terms of cache pollution. These insights that GainSight provides can be used to better understand the design spaces of both emerging on-chip memories and software algorithmic optimizations for the next generation of AI accelerators.","sentences":["As AI workloads drive soaring memory requirements, there is a need for higher-density on-chip memory for domain-specific accelerators that goes beyond what current SRAM technology can provide.","We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories.","However, there has been little work in factoring dynamic application profiles into such design decisions.","We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and computes data lifetimes in domain-specific accelerators.","By combining instrumentation and simulation across retargetable hardware backends, GainSight aligns heterogeneous memory designs with workload-specific traffic and lifetime metrics.","Case studies on MLPerf Inference and PolyBench workloads using NVIDIA H100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3) Up to 90% of GPU cache fetches are never reused, highlighting inefficiencies in terms of cache pollution.","These insights that GainSight provides can be used to better understand the design spaces of both emerging on-chip memories and software algorithmic optimizations for the next generation of AI accelerators."],"url":"http://arxiv.org/abs/2504.14866v1"}
{"created":"2025-04-21 05:03:19","title":"FERMI: Flexible Radio Mapping with a Hybrid Propagation Model and Scalable Autonomous Data Collection","abstract":"Communication is fundamental for multi-robot collaboration, with accurate radio mapping playing a crucial role in predicting signal strength between robots. However, modeling radio signal propagation in large and occluded environments is challenging due to complex interactions between signals and obstacles. Existing methods face two key limitations: they struggle to predict signal strength for transmitter-receiver pairs not present in the training set, while also requiring extensive manual data collection for modeling, making them impractical for large, obstacle-rich scenarios. To overcome these limitations, we propose FERMI, a flexible radio mapping framework. FERMI combines physics-based modeling of direct signal paths with a neural network to capture environmental interactions with radio signals. This hybrid model learns radio signal propagation more efficiently, requiring only sparse training data. Additionally, FERMI introduces a scalable planning method for autonomous data collection using a multi-robot team. By increasing parallelism in data collection and minimizing robot travel costs between regions, overall data collection efficiency is significantly improved. Experiments in both simulation and real-world scenarios demonstrate that FERMI enables accurate signal prediction and generalizes well to unseen positions in complex environments. It also supports fully autonomous data collection and scales to different team sizes, offering a flexible solution for creating radio maps. Our code is open-sourced at https://github.com/ymLuo1214/Flexible-Radio-Mapping.","sentences":["Communication is fundamental for multi-robot collaboration, with accurate radio mapping playing a crucial role in predicting signal strength between robots.","However, modeling radio signal propagation in large and occluded environments is challenging due to complex interactions between signals and obstacles.","Existing methods face two key limitations: they struggle to predict signal strength for transmitter-receiver pairs not present in the training set, while also requiring extensive manual data collection for modeling, making them impractical for large, obstacle-rich scenarios.","To overcome these limitations, we propose FERMI, a flexible radio mapping framework.","FERMI combines physics-based modeling of direct signal paths with a neural network to capture environmental interactions with radio signals.","This hybrid model learns radio signal propagation more efficiently, requiring only sparse training data.","Additionally, FERMI introduces a scalable planning method for autonomous data collection using a multi-robot team.","By increasing parallelism in data collection and minimizing robot travel costs between regions, overall data collection efficiency is significantly improved.","Experiments in both simulation and real-world scenarios demonstrate that FERMI enables accurate signal prediction and generalizes well to unseen positions in complex environments.","It also supports fully autonomous data collection and scales to different team sizes, offering a flexible solution for creating radio maps.","Our code is open-sourced at https://github.com/ymLuo1214/Flexible-Radio-Mapping."],"url":"http://arxiv.org/abs/2504.14862v1"}
{"created":"2025-04-21 05:01:58","title":"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum Inner Product Search","abstract":"Maximum Inner Product Search (MIPS) is a fundamental challenge in machine learning and information retrieval, particularly in high-dimensional data applications. Existing approaches to MIPS either rely solely on Inner Product (IP) similarity, which faces issues with local optima and redundant computations, or reduce the MIPS problem to the Nearest Neighbor Search under the Euclidean metric via space projection, leading to topology destruction and information loss. Despite the divergence of the two paradigms, we argue that there is no inherent binary opposition between IP and Euclidean metrics. By stitching IP and Euclidean in the design of indexing and search algorithms, we can significantly enhance MIPS performance. Specifically, this paper explores the theoretical and empirical connections between these two metrics from the MIPS perspective. Our investigation, grounded in graph-based search, reveals that different indexing and search strategies offer distinct advantages for MIPS, depending on the underlying data topology. Building on these insights, we introduce a novel graph-based index called Metric-Amphibious Graph (MAG) and a corresponding search algorithm, Adaptive Navigation with Metric Switch (ANMS). To facilitate parameter tuning for optimal performance, we identify three statistical indicators that capture essential data topology properties and correlate strongly with parameter tuning. Extensive experiments on 12 real-world datasets demonstrate that MAG outperforms existing state-of-the-art methods, achieving up to 4x search speedup while maintaining adaptability and scalability.","sentences":["Maximum Inner Product Search (MIPS) is a fundamental challenge in machine learning and information retrieval, particularly in high-dimensional data applications.","Existing approaches to MIPS either rely solely on Inner Product (IP) similarity, which faces issues with local optima and redundant computations, or reduce the MIPS problem to the Nearest Neighbor Search under the Euclidean metric via space projection, leading to topology destruction and information loss.","Despite the divergence of the two paradigms, we argue that there is no inherent binary opposition between IP and Euclidean metrics.","By stitching IP and Euclidean in the design of indexing and search algorithms, we can significantly enhance MIPS performance.","Specifically, this paper explores the theoretical and empirical connections between these two metrics from the MIPS perspective.","Our investigation, grounded in graph-based search, reveals that different indexing and search strategies offer distinct advantages for MIPS, depending on the underlying data topology.","Building on these insights, we introduce a novel graph-based index called Metric-Amphibious Graph (MAG) and a corresponding search algorithm, Adaptive Navigation with Metric Switch (ANMS).","To facilitate parameter tuning for optimal performance, we identify three statistical indicators that capture essential data topology properties and correlate strongly with parameter tuning.","Extensive experiments on 12 real-world datasets demonstrate that MAG outperforms existing state-of-the-art methods, achieving up to 4x search speedup while maintaining adaptability and scalability."],"url":"http://arxiv.org/abs/2504.14861v1"}
{"created":"2025-04-21 04:50:24","title":"SuFIA-BC: Generating High Quality Demonstration Data for Visuomotor Policy Learning in Surgical Subtasks","abstract":"Behavior cloning facilitates the learning of dexterous manipulation skills, yet the complexity of surgical environments, the difficulty and expense of obtaining patient data, and robot calibration errors present unique challenges for surgical robot learning. We provide an enhanced surgical digital twin with photorealistic human anatomical organs, integrated into a comprehensive simulator designed to generate high-quality synthetic data to solve fundamental tasks in surgical autonomy. We present SuFIA-BC: visual Behavior Cloning policies for Surgical First Interactive Autonomy Assistants. We investigate visual observation spaces including multi-view cameras and 3D visual representations extracted from a single endoscopic camera view. Through systematic evaluation, we find that the diverse set of photorealistic surgical tasks introduced in this work enables a comprehensive evaluation of prospective behavior cloning models for the unique challenges posed by surgical environments. We observe that current state-of-the-art behavior cloning techniques struggle to solve the contact-rich and complex tasks evaluated in this work, regardless of their underlying perception or control architectures. These findings highlight the importance of customizing perception pipelines and control architectures, as well as curating larger-scale synthetic datasets that meet the specific demands of surgical tasks. Project website: https://orbit-surgical.github.io/sufia-bc/","sentences":["Behavior cloning facilitates the learning of dexterous manipulation skills, yet the complexity of surgical environments, the difficulty and expense of obtaining patient data, and robot calibration errors present unique challenges for surgical robot learning.","We provide an enhanced surgical digital twin with photorealistic human anatomical organs, integrated into a comprehensive simulator designed to generate high-quality synthetic data to solve fundamental tasks in surgical autonomy.","We present SuFIA-BC: visual Behavior Cloning policies for Surgical First Interactive Autonomy Assistants.","We investigate visual observation spaces including multi-view cameras and 3D visual representations extracted from a single endoscopic camera view.","Through systematic evaluation, we find that the diverse set of photorealistic surgical tasks introduced in this work enables a comprehensive evaluation of prospective behavior cloning models for the unique challenges posed by surgical environments.","We observe that current state-of-the-art behavior cloning techniques struggle to solve the contact-rich and complex tasks evaluated in this work, regardless of their underlying perception or control architectures.","These findings highlight the importance of customizing perception pipelines and control architectures, as well as curating larger-scale synthetic datasets that meet the specific demands of surgical tasks.","Project website: https://orbit-surgical.github.io/sufia-bc/"],"url":"http://arxiv.org/abs/2504.14857v1"}
{"created":"2025-04-21 04:50:16","title":"Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation","abstract":"While hallucinations of large language models could been alleviated through retrieval-augmented generation and citation generation, how the model utilizes internal knowledge is still opaque, and the trustworthiness of its generated answers remains questionable. In this work, we introduce Context-Prior Augmented Citation Generation task, requiring models to generate citations considering both external and internal knowledge while providing trustworthy references, with 5 evaluation metrics focusing on 3 aspects: answer helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the paradigm for our task, and also design INTRALIGN, an integrated method containing customary data generation and an alignment algorithm. Our experimental results show that our method achieves a better cross-scenario performance with regard to other baselines. Our extended experiments further reveal that retrieval quality, question types, and model knowledge have considerable influence on the trustworthiness in citation generation.","sentences":["While hallucinations of large language models could been alleviated through retrieval-augmented generation and citation generation, how the model utilizes internal knowledge is still opaque, and the trustworthiness of its generated answers remains questionable.","In this work, we introduce Context-Prior Augmented Citation Generation task, requiring models to generate citations considering both external and internal knowledge while providing trustworthy references, with 5 evaluation metrics focusing on 3 aspects: answer helpfulness, citation faithfulness, and trustworthiness.","We introduce RAEL, the paradigm for our task, and also design INTRALIGN, an integrated method containing customary data generation and an alignment algorithm.","Our experimental results show that our method achieves a better cross-scenario performance with regard to other baselines.","Our extended experiments further reveal that retrieval quality, question types, and model knowledge have considerable influence on the trustworthiness in citation generation."],"url":"http://arxiv.org/abs/2504.14856v1"}
{"created":"2025-04-21 04:45:40","title":"Uncertainty quantification of neural network models of evolving processes via Langevin sampling","abstract":"We propose a scalable, approximate inference hypernetwork framework for a general model of history-dependent processes. The flexible data model is based on a neural ordinary differential equation (NODE) representing the evolution of internal states together with a trainable observation model subcomponent. The posterior distribution corresponding to the data model parameters (weights and biases) follows a stochastic differential equation with a drift term related to the score of the posterior that is learned jointly with the data model parameters. This Langevin sampling approach offers flexibility in balancing the computational budget between the evaluation cost of the data model and the approximation of the posterior density of its parameters. We demonstrate performance of the hypernetwork on chemical reaction and material physics data and compare it to mean-field variational inference.","sentences":["We propose a scalable, approximate inference hypernetwork framework for a general model of history-dependent processes.","The flexible data model is based on a neural ordinary differential equation (NODE) representing the evolution of internal states together with a trainable observation model subcomponent.","The posterior distribution corresponding to the data model parameters (weights and biases) follows a stochastic differential equation with a drift term related to the score of the posterior that is learned jointly with the data model parameters.","This Langevin sampling approach offers flexibility in balancing the computational budget between the evaluation cost of the data model and the approximation of the posterior density of its parameters.","We demonstrate performance of the hypernetwork on chemical reaction and material physics data and compare it to mean-field variational inference."],"url":"http://arxiv.org/abs/2504.14854v1"}
{"created":"2025-04-21 03:58:40","title":"Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph Reasoning","abstract":"Multi-modal data provides abundant and diverse object information, crucial for effective modal interactions in Re-Identification (ReID) tasks. However, existing approaches often overlook the quality variations in local features and fail to fully leverage the complementary information across modalities, particularly in the case of low-quality features. In this paper, we propose to address this issue by leveraging a novel graph reasoning model, termed the Modality-aware Graph Reasoning Network (MGRNet). Specifically, we first construct modality-aware graphs to enhance the extraction of fine-grained local details by effectively capturing and modeling the relationships between patches. Subsequently, the selective graph nodes swap operation is employed to alleviate the adverse effects of low-quality local features by considering both local and global information, enhancing the representation of discriminative information. Finally, the swapped modality-aware graphs are fed into the local-aware graph reasoning module, which propagates multi-modal information to yield a reliable feature representation. Another advantage of the proposed graph reasoning approach is its ability to reconstruct missing modal information by exploiting inherent structural relationships, thereby minimizing disparities between different modalities. Experimental results on four benchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the proposed method achieves state-of-the-art performance in multi-modal object ReID. The code for our method will be available upon acceptance.","sentences":["Multi-modal data provides abundant and diverse object information, crucial for effective modal interactions in Re-Identification (ReID) tasks.","However, existing approaches often overlook the quality variations in local features and fail to fully leverage the complementary information across modalities, particularly in the case of low-quality features.","In this paper, we propose to address this issue by leveraging a novel graph reasoning model, termed the Modality-aware Graph Reasoning Network (MGRNet).","Specifically, we first construct modality-aware graphs to enhance the extraction of fine-grained local details by effectively capturing and modeling the relationships between patches.","Subsequently, the selective graph nodes swap operation is employed to alleviate the adverse effects of low-quality local features by considering both local and global information, enhancing the representation of discriminative information.","Finally, the swapped modality-aware graphs are fed into the local-aware graph reasoning module, which propagates multi-modal information to yield a reliable feature representation.","Another advantage of the proposed graph reasoning approach is its ability to reconstruct missing modal information by exploiting inherent structural relationships, thereby minimizing disparities between different modalities.","Experimental results on four benchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the proposed method achieves state-of-the-art performance in multi-modal object ReID.","The code for our method will be available upon acceptance."],"url":"http://arxiv.org/abs/2504.14847v1"}
{"created":"2025-04-21 03:56:56","title":"Enhancing the Patent Matching Capability of Large Language Models via the Memory Graph","abstract":"Intellectual Property (IP) management involves strategically protecting and utilizing intellectual assets to enhance organizational innovation, competitiveness, and value creation. Patent matching is a crucial task in intellectual property management, which facilitates the organization and utilization of patents. Existing models often rely on the emergent capabilities of Large Language Models (LLMs) and leverage them to identify related patents directly. However, these methods usually depend on matching keywords and overlook the hierarchical classification and categorical relationships of patents. In this paper, we propose MemGraph, a method that augments the patent matching capabilities of LLMs by incorporating a memory graph derived from their parametric memory. Specifically, MemGraph prompts LLMs to traverse their memory to identify relevant entities within patents, followed by attributing these entities to corresponding ontologies. After traversing the memory graph, we utilize extracted entities and ontologies to improve the capability of LLM in comprehending the semantics of patents. Experimental results on the PatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a 17.68% performance improvement over baseline LLMs. The further analysis highlights the generalization ability of MemGraph across various LLMs, both in-domain and out-of-domain, and its capacity to enhance the internal reasoning processes of LLMs during patent matching. All data and codes are available at https://github.com/NEUIR/MemGraph.","sentences":["Intellectual Property (IP) management involves strategically protecting and utilizing intellectual assets to enhance organizational innovation, competitiveness, and value creation.","Patent matching is a crucial task in intellectual property management, which facilitates the organization and utilization of patents.","Existing models often rely on the emergent capabilities of Large Language Models (LLMs) and leverage them to identify related patents directly.","However, these methods usually depend on matching keywords and overlook the hierarchical classification and categorical relationships of patents.","In this paper, we propose MemGraph, a method that augments the patent matching capabilities of LLMs by incorporating a memory graph derived from their parametric memory.","Specifically, MemGraph prompts LLMs to traverse their memory to identify relevant entities within patents, followed by attributing these entities to corresponding ontologies.","After traversing the memory graph, we utilize extracted entities and ontologies to improve the capability of LLM in comprehending the semantics of patents.","Experimental results on the PatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a 17.68% performance improvement over baseline LLMs.","The further analysis highlights the generalization ability of MemGraph across various LLMs, both in-domain and out-of-domain, and its capacity to enhance the internal reasoning processes of LLMs during patent matching.","All data and codes are available at https://github.com/NEUIR/MemGraph."],"url":"http://arxiv.org/abs/2504.14845v1"}
{"created":"2025-04-21 03:00:18","title":"Distribution-aware Dataset Distillation for Efficient Image Restoration","abstract":"With the exponential increase in image data, training an image restoration model is laborious. Dataset distillation is a potential solution to this problem, yet current distillation techniques are a blank canvas in the field of image restoration. To fill this gap, we propose the Distribution-aware Dataset Distillation method (TripleD), a new framework that extends the principles of dataset distillation to image restoration. Specifically, TripleD uses a pre-trained vision Transformer to extract features from images for complexity evaluation, and the subset (the number of samples is much smaller than the original training set) is selected based on complexity. The selected subset is then fed through a lightweight CNN that fine-tunes the image distribution to align with the distribution of the original dataset at the feature level. To efficiently condense knowledge, the training is divided into two stages. Early stages focus on simpler, low-complexity samples to build foundational knowledge, while later stages select more complex and uncertain samples as the model matures. Our method achieves promising performance on multiple image restoration tasks, including multi-task image restoration, all-in-one image restoration, and ultra-high-definition image restoration tasks. Note that we can train a state-of-the-art image restoration model on an ultra-high-definition (4K resolution) dataset using only one consumer-grade GPU in less than 8 hours (500 savings in computing resources and immeasurable training time).","sentences":["With the exponential increase in image data, training an image restoration model is laborious.","Dataset distillation is a potential solution to this problem, yet current distillation techniques are a blank canvas in the field of image restoration.","To fill this gap, we propose the Distribution-aware Dataset Distillation method (TripleD), a new framework that extends the principles of dataset distillation to image restoration.","Specifically, TripleD uses a pre-trained vision Transformer to extract features from images for complexity evaluation, and the subset (the number of samples is much smaller than the original training set) is selected based on complexity.","The selected subset is then fed through a lightweight CNN that fine-tunes the image distribution to align with the distribution of the original dataset at the feature level.","To efficiently condense knowledge, the training is divided into two stages.","Early stages focus on simpler, low-complexity samples to build foundational knowledge, while later stages select more complex and uncertain samples as the model matures.","Our method achieves promising performance on multiple image restoration tasks, including multi-task image restoration, all-in-one image restoration, and ultra-high-definition image restoration tasks.","Note that we can train a state-of-the-art image restoration model on an ultra-high-definition (4K resolution) dataset using only one consumer-grade GPU in less than 8 hours (500 savings in computing resources and immeasurable training time)."],"url":"http://arxiv.org/abs/2504.14826v1"}
{"created":"2025-04-21 03:00:17","title":"ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages","abstract":"Vision Transformers (ViTs) have revolutionized computer vision by leveraging self-attention to model long-range dependencies. However, ViTs face challenges such as high computational costs due to the quadratic scaling of self-attention and the requirement of a large amount of training data. To address these limitations, we propose the Efficient Convolutional Vision Transformer (ECViT), a hybrid architecture that effectively combines the strengths of CNNs and Transformers. ECViT introduces inductive biases such as locality and translation invariance, inherent to Convolutional Neural Networks (CNNs) into the Transformer framework by extracting patches from low-level features and enhancing the encoder with convolutional operations. Additionally, it incorporates local-attention and a pyramid structure to enable efficient multi-scale feature extraction and representation. Experimental results demonstrate that ECViT achieves an optimal balance between performance and efficiency, outperforming state-of-the-art models on various image classification tasks while maintaining low computational and storage requirements. ECViT offers an ideal solution for applications that prioritize high efficiency without compromising performance.","sentences":["Vision Transformers (ViTs) have revolutionized computer vision by leveraging self-attention to model long-range dependencies.","However, ViTs face challenges such as high computational costs due to the quadratic scaling of self-attention and the requirement of a large amount of training data.","To address these limitations, we propose the Efficient Convolutional Vision Transformer (ECViT), a hybrid architecture that effectively combines the strengths of CNNs and Transformers.","ECViT introduces inductive biases such as locality and translation invariance, inherent to Convolutional Neural Networks (CNNs) into the Transformer framework by extracting patches from low-level features and enhancing the encoder with convolutional operations.","Additionally, it incorporates local-attention and a pyramid structure to enable efficient multi-scale feature extraction and representation.","Experimental results demonstrate that ECViT achieves an optimal balance between performance and efficiency, outperforming state-of-the-art models on various image classification tasks while maintaining low computational and storage requirements.","ECViT offers an ideal solution for applications that prioritize high efficiency without compromising performance."],"url":"http://arxiv.org/abs/2504.14825v1"}
{"created":"2025-04-21 02:31:19","title":"CSI2Dig: Recovering Digit Content from Smartphone Loudspeakers Using Channel State Information","abstract":"Eavesdropping on sounds emitted by mobile device loudspeakers can capture sensitive digital information, such as SMS verification codes, credit card numbers, and withdrawal passwords, which poses significant security risks. Existing schemes either require expensive specialized equipment, rely on spyware, or are limited to close-range signal acquisition. In this paper, we propose a scheme, CSI2Dig, for recovering digit content from Channel State Information (CSI) when digits are played through a smartphone loudspeaker. We observe that the electromagnetic interference caused by the audio signals from the loudspeaker affects the WiFi signals emitted by the phone's WiFi antenna. Building upon contrastive learning and denoising autoencoders, we develop a two-branch autoencoder network designed to amplify the impact of this electromagnetic interference on CSI. For feature extraction, we introduce the TS-Net, a model that captures relevant features from both the temporal and spatial dimensions of the CSI data. We evaluate our scheme across various devices, distances, volumes, and other settings. Experimental results demonstrate that our scheme can achieve an accuracy of 72.97%.","sentences":["Eavesdropping on sounds emitted by mobile device loudspeakers can capture sensitive digital information, such as SMS verification codes, credit card numbers, and withdrawal passwords, which poses significant security risks.","Existing schemes either require expensive specialized equipment, rely on spyware, or are limited to close-range signal acquisition.","In this paper, we propose a scheme, CSI2Dig, for recovering digit content from Channel State Information (CSI) when digits are played through a smartphone loudspeaker.","We observe that the electromagnetic interference caused by the audio signals from the loudspeaker affects the WiFi signals emitted by the phone's WiFi antenna.","Building upon contrastive learning and denoising autoencoders, we develop a two-branch autoencoder network designed to amplify the impact of this electromagnetic interference on CSI.","For feature extraction, we introduce the TS-Net, a model that captures relevant features from both the temporal and spatial dimensions of the CSI data.","We evaluate our scheme across various devices, distances, volumes, and other settings.","Experimental results demonstrate that our scheme can achieve an accuracy of 72.97%."],"url":"http://arxiv.org/abs/2504.14812v1"}
{"created":"2025-04-21 02:25:03","title":"DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning","abstract":"Ad-hoc instruction fine-tuning of large language models (LLMs) is widely adopted for domain-specific adaptation. While domain-specific supervised fine-tuning (SFT) is effective and efficient, it often weakens cross-domain generalization and struggles with noisy training data. To address these challenges, we propose DONOD, a lightweight model-intrinsic data pruning method. Our approach evaluates data using two model-parameter-based metrics: Delta of Norm (DON), which captures the cumulative influence on model weights, and Norm of Delta (NOD), which quantifies weight instability. Moreover, by employing the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) algorithm, we effectively filter noisy, unlearnable, and generalization-harming samples without relying on auxiliary models during the SFT process. Experiments on mathematical tasks demonstrate that data selected by DONOD achieve superior fine-tuning efficiency and improved robustness against noisy data. By filtering out 70% of the full dataset, we improve target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile, our selected data present superior cross-architecture generalization. Data pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD demonstrates comparable or superior performance while remaining dataset-agnostic, enabling broader applicability.","sentences":["Ad-hoc instruction fine-tuning of large language models (LLMs) is widely adopted for domain-specific adaptation.","While domain-specific supervised fine-tuning (SFT) is effective and efficient, it often weakens cross-domain generalization and struggles with noisy training data.","To address these challenges, we propose DONOD, a lightweight model-intrinsic data pruning method.","Our approach evaluates data using two model-parameter-based metrics: Delta of Norm (DON), which captures the cumulative influence on model weights, and Norm of Delta (NOD), which quantifies weight instability.","Moreover, by employing the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) algorithm, we effectively filter noisy, unlearnable, and generalization-harming samples without relying on auxiliary models during the SFT process.","Experiments on mathematical tasks demonstrate that data selected by DONOD achieve superior fine-tuning efficiency and improved robustness against noisy data.","By filtering out 70% of the full dataset, we improve target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%.","Meanwhile, our selected data present superior cross-architecture generalization.","Data pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger models (e.g., Llama 2-13B).","Compared to existing related methodologies, DONOD demonstrates comparable or superior performance while remaining dataset-agnostic, enabling broader applicability."],"url":"http://arxiv.org/abs/2504.14810v1"}
{"created":"2025-04-21 02:15:27","title":"An Iterative Task-Driven Framework for Resilient LiDAR Place Recognition in Adverse Weather","abstract":"LiDAR place recognition (LPR) plays a vital role in autonomous navigation. However, existing LPR methods struggle to maintain robustness under adverse weather conditions such as rain, snow, and fog, where weather-induced noise and point cloud degradation impair LiDAR reliability and perception accuracy. To tackle these challenges, we propose an Iterative Task-Driven Framework (ITDNet), which integrates a LiDAR Data Restoration (LDR) module and a LiDAR Place Recognition (LPR) module through an iterative learning strategy. These modules are jointly trained end-to-end, with alternating optimization to enhance performance. The core rationale of ITDNet is to leverage the LDR module to recover the corrupted point clouds while preserving structural consistency with clean data, thereby improving LPR accuracy in adverse weather. Simultaneously, the LPR task provides feature pseudo-labels to guide the LDR module's training, aligning it more effectively with the LPR task. To achieve this, we first design a task-driven LPR loss and a reconstruction loss to jointly supervise the optimization of the LDR module. Furthermore, for the LDR module, we propose a Dual-Domain Mixer (DDM) block for frequency-spatial feature fusion and a Semantic-Aware Generator (SAG) block for semantic-guided restoration. In addition, for the LPR module, we introduce a Multi-Frequency Transformer (MFT) block and a Wavelet Pyramid NetVLAD (WPN) block to aggregate multi-scale, robust global descriptors. Finally, extensive experiments on the Weather-KITTI, Boreas, and our proposed Weather-Apollo datasets demonstrate that, demonstrate that ITDNet outperforms existing LPR methods, achieving state-of-the-art performance in adverse weather. The datasets and code will be made publicly available at https://github.com/Grandzxw/ITDNet.","sentences":["LiDAR place recognition (LPR) plays a vital role in autonomous navigation.","However, existing LPR methods struggle to maintain robustness under adverse weather conditions such as rain, snow, and fog, where weather-induced noise and point cloud degradation impair LiDAR reliability and perception accuracy.","To tackle these challenges, we propose an Iterative Task-Driven Framework (ITDNet), which integrates a LiDAR Data Restoration (LDR) module and a LiDAR Place Recognition (LPR) module through an iterative learning strategy.","These modules are jointly trained end-to-end, with alternating optimization to enhance performance.","The core rationale of ITDNet is to leverage the LDR module to recover the corrupted point clouds while preserving structural consistency with clean data, thereby improving LPR accuracy in adverse weather.","Simultaneously, the LPR task provides feature pseudo-labels to guide the LDR module's training, aligning it more effectively with the LPR task.","To achieve this, we first design a task-driven LPR loss and a reconstruction loss to jointly supervise the optimization of the LDR module.","Furthermore, for the LDR module, we propose a Dual-Domain Mixer (DDM) block for frequency-spatial feature fusion and a Semantic-Aware Generator (SAG) block for semantic-guided restoration.","In addition, for the LPR module, we introduce a Multi-Frequency Transformer (MFT) block and a Wavelet Pyramid NetVLAD (WPN) block to aggregate multi-scale, robust global descriptors.","Finally, extensive experiments on the Weather-KITTI, Boreas, and our proposed Weather-Apollo datasets demonstrate that, demonstrate that ITDNet outperforms existing LPR methods, achieving state-of-the-art performance in adverse weather.","The datasets and code will be made publicly available at https://github.com/Grandzxw/ITDNet."],"url":"http://arxiv.org/abs/2504.14806v1"}
{"created":"2025-04-21 02:05:53","title":"The k-Center Problem of Uncertain Points on Graphs","abstract":"In this paper, we study the $k$-center problem of uncertain points on a graph. Given are an undirected graph $G = (V, E)$ and a set $\\mathcal{P}$ of $n$ uncertain points where each uncertain point with a non-negative weight has $m$ possible locations on $G$ each associated with a probability. The problem aims to find $k$ centers (points) on $G$ so as to minimize the maximum weighted expected distance of uncertain points to their expected closest centers. No previous work exist for the $k$-center problem of uncertain points on undirected graphs. We propose exact algorithms that solve respectively the case of $k=2$ in $O(|E|^2m^2n\\log |E|mn\\log mn )$ time and the problem with $k\\geq 3$ in $O(\\min\\{|E|^km^kn^{k+1}k\\log |E|mn\\log m, |E|^kn^\\frac{k}{2}m^\\frac{k^2}{2}\\log |E|mn\\})$ time, provided with the distance matrix of $G$. In addition, an $O(|E|mn\\log mn)$-time algorithmic approach is given for the one-center case.","sentences":["In this paper, we study the $k$-center problem of uncertain points on a graph.","Given are an undirected graph $G = (V, E)$ and a set $\\mathcal{P}$ of $n$ uncertain points where each uncertain point with a non-negative weight has $m$ possible locations on $G$ each associated with a probability.","The problem aims to find $k$ centers (points) on $G$ so as to minimize the maximum weighted expected distance of uncertain points to their expected closest centers.","No previous work exist for the $k$-center problem of uncertain points on undirected graphs.","We propose exact algorithms that solve respectively the case of $k=2$ in $O(|E|^2m^2n\\log |E|mn\\log mn )$ time and the problem with $k\\geq 3$ in $O(\\min\\{|E|^km^kn^{k+1}k\\log |E|mn\\log m, |E|^kn^\\frac{k}{2}m^\\frac{k^2}{2}\\log |E|mn\\})$ time, provided with the distance matrix of $G$. In addition, an $O(|E|mn\\log mn)$-time algorithmic approach is given for the one-center case."],"url":"http://arxiv.org/abs/2504.14803v1"}
{"created":"2025-04-21 01:58:29","title":"A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions","abstract":"The small sample imbalance (S&I) problem is a major challenge in machine learning and data analysis. It is characterized by a small number of samples and an imbalanced class distribution, which leads to poor model performance. In addition, indistinct inter-class feature distributions further complicate classification tasks. Existing methods often rely on algorithmic heuristics without sufficiently analyzing the underlying data characteristics. We argue that a detailed analysis from the data perspective is essential before developing an appropriate solution. Therefore, this paper proposes a systematic analytical framework for the S\\&I problem. We first summarize imbalance metrics and complexity analysis methods, highlighting the need for interpretable benchmarks to characterize S&I problems. Second, we review recent solutions for conventional, complexity-based, and extreme S&I problems, revealing methodological differences in handling various data distributions. Our summary finds that resampling remains a widely adopted solution. However, we conduct experiments on binary and multiclass datasets, revealing that classifier performance differences significantly exceed the improvements achieved through resampling. Finally, this paper highlights open questions and discusses future trends.","sentences":["The small sample imbalance (S&I) problem is a major challenge in machine learning and data analysis.","It is characterized by a small number of samples and an imbalanced class distribution, which leads to poor model performance.","In addition, indistinct inter-class feature distributions further complicate classification tasks.","Existing methods often rely on algorithmic heuristics without sufficiently analyzing the underlying data characteristics.","We argue that a detailed analysis from the data perspective is essential before developing an appropriate solution.","Therefore, this paper proposes a systematic analytical framework for the S\\&I problem.","We first summarize imbalance metrics and complexity analysis methods, highlighting the need for interpretable benchmarks to characterize S&I problems.","Second, we review recent solutions for conventional, complexity-based, and extreme S&I problems, revealing methodological differences in handling various data distributions.","Our summary finds that resampling remains a widely adopted solution.","However, we conduct experiments on binary and multiclass datasets, revealing that classifier performance differences significantly exceed the improvements achieved through resampling.","Finally, this paper highlights open questions and discusses future trends."],"url":"http://arxiv.org/abs/2504.14800v1"}
{"created":"2025-04-21 01:55:54","title":"Automated Duplicate Bug Report Detection in Large Open Bug Repositories","abstract":"Many users and contributors of large open-source projects report software defects or enhancement requests (known as bug reports) to the issue-tracking systems. However, they sometimes report issues that have already been reported. First, they may not have time to do sufficient research on existing bug reports. Second, they may not possess the right expertise in that specific area to realize that an existing bug report is essentially elaborating on the same matter, perhaps with a different wording. In this paper, we propose a novel approach based on machine learning methods that can automatically detect duplicate bug reports in an open bug repository based on the textual data in the reports. We present six alternative methods: Topic modeling, Gaussian Naive Bayes, deep learning, time-based organization, clustering, and summarization using a generative pre-trained transformer large language model. Additionally, we introduce a novel threshold-based approach for duplicate identification, in contrast to the conventional top-k selection method that has been widely used in the literature. Our approach demonstrates promising results across all the proposed methods, achieving accuracy rates ranging from the high 70%'s to the low 90%'s. We evaluated our methods on a public dataset of issues belonging to an Eclipse open-source project.","sentences":["Many users and contributors of large open-source projects report software defects or enhancement requests (known as bug reports) to the issue-tracking systems.","However, they sometimes report issues that have already been reported.","First, they may not have time to do sufficient research on existing bug reports.","Second, they may not possess the right expertise in that specific area to realize that an existing bug report is essentially elaborating on the same matter, perhaps with a different wording.","In this paper, we propose a novel approach based on machine learning methods that can automatically detect duplicate bug reports in an open bug repository based on the textual data in the reports.","We present six alternative methods: Topic modeling, Gaussian Naive Bayes, deep learning, time-based organization, clustering, and summarization using a generative pre-trained transformer large language model.","Additionally, we introduce a novel threshold-based approach for duplicate identification, in contrast to the conventional top-k selection method that has been widely used in the literature.","Our approach demonstrates promising results across all the proposed methods, achieving accuracy rates ranging from the high 70%'s to the low 90%'s.","We evaluated our methods on a public dataset of issues belonging to an Eclipse open-source project."],"url":"http://arxiv.org/abs/2504.14797v1"}
{"created":"2025-04-21 01:33:56","title":"Enhanced Data-driven Topology Design Methodology with Multi-level Mesh and Correlation-based Mutation for Stress-related Multi-objective Optimization","abstract":"Topology optimization (TO) serves as a widely applied structural design approach to tackle various engineering problems. Nevertheless, sensitivity-based TO methods usually struggle with solving strongly nonlinear optimization problems. By leveraging high capacity of deep generative model, which is an influential machine learning technique, the sensitivity-free data-driven topology design (DDTD) methodology is regarded as an effective means of overcoming these issues. The DDTD methodology depends on initial dataset with a certain regularity, making its results highly sensitive to initial dataset quality. This limits its effectiveness and generalizability, especially for optimization problems without priori information. In this research, we proposed a multi-level mesh DDTD-based method with correlation-based mutation module to escape from the limitation of the quality of the initial dataset on the results and enhance computational efficiency. The core is to employ a correlation-based mutation module to assign new geometric features with physical meaning to the generated data, while utilizing a multi-level mesh strategy to progressively enhance the refinement of the structural representation, thus avoiding the maintenance of a high degree-of-freedom (DOF) representation throughout the iterative process. The proposed multi-level mesh DDTD-based method can be driven by a low quality initial dataset without the need for time-consuming construction of a specific dataset, thus significantly increasing generality and reducing application difficulty, while further lowering computational cost of DDTD methodology. Various comparison experiments with the traditional sensitivity-based TO methods on stress-related strongly nonlinear problems demonstrate the generality and effectiveness of the proposed method.","sentences":["Topology optimization (TO) serves as a widely applied structural design approach to tackle various engineering problems.","Nevertheless, sensitivity-based TO methods usually struggle with solving strongly nonlinear optimization problems.","By leveraging high capacity of deep generative model, which is an influential machine learning technique, the sensitivity-free data-driven topology design (DDTD) methodology is regarded as an effective means of overcoming these issues.","The DDTD methodology depends on initial dataset with a certain regularity, making its results highly sensitive to initial dataset quality.","This limits its effectiveness and generalizability, especially for optimization problems without priori information.","In this research, we proposed a multi-level mesh DDTD-based method with correlation-based mutation module to escape from the limitation of the quality of the initial dataset on the results and enhance computational efficiency.","The core is to employ a correlation-based mutation module to assign new geometric features with physical meaning to the generated data, while utilizing a multi-level mesh strategy to progressively enhance the refinement of the structural representation, thus avoiding the maintenance of a high degree-of-freedom (DOF) representation throughout the iterative process.","The proposed multi-level mesh DDTD-based method can be driven by a low quality initial dataset without the need for time-consuming construction of a specific dataset, thus significantly increasing generality and reducing application difficulty, while further lowering computational cost of DDTD methodology.","Various comparison experiments with the traditional sensitivity-based TO methods on stress-related strongly nonlinear problems demonstrate the generality and effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2504.14790v1"}
{"created":"2025-04-21 01:01:18","title":"Cultivating Multidisciplinary Research and Education on GPU Infrastructure for Mid-South Institutions at the University of Memphis: Practice and Challenge","abstract":"To support rapid scientific advancement and promote access to large-scale computing resources for under-resourced institutions at the Mid-South region, the University of Memphis (UofM) established the first regional mid-scale GPU cluster, iTiger, a valuable high-performance computing (HPC) infrastructure. In this study, we present our continuous efforts to manage the critical cyberinfrastructure and provide essential computing supports for educators, students, and researchers in AI, data sciences, and related scientific fields in the Mid-South region, such as precision agriculture, smart transportation, and health informatics. We outline our initiatives to broaden CI adoptions across regional computing-related scientific and engineering fields, such as seed grant, workshop trainings, course integration, and other outreach activities. While we've observed promising outcomes of regional CI adoptions, we will discuss insights and challenges of Mid-South CI users, which can inspire other institutions to implement similar programs.","sentences":["To support rapid scientific advancement and promote access to large-scale computing resources for under-resourced institutions at the Mid-South region, the University of Memphis (UofM) established the first regional mid-scale GPU cluster, iTiger, a valuable high-performance computing (HPC) infrastructure.","In this study, we present our continuous efforts to manage the critical cyberinfrastructure and provide essential computing supports for educators, students, and researchers in AI, data sciences, and related scientific fields in the Mid-South region, such as precision agriculture, smart transportation, and health informatics.","We outline our initiatives to broaden CI adoptions across regional computing-related scientific and engineering fields, such as seed grant, workshop trainings, course integration, and other outreach activities.","While we've observed promising outcomes of regional CI adoptions, we will discuss insights and challenges of Mid-South CI users, which can inspire other institutions to implement similar programs."],"url":"http://arxiv.org/abs/2504.14786v1"}
{"created":"2025-04-21 00:46:28","title":"Novel Concept-Oriented Synthetic Data approach for Training Generative AI-Driven Crystal Grain Analysis Using Diffusion Model","abstract":"The traditional techniques for extracting polycrystalline grain structures from microscopy images, such as transmission electron microscopy (TEM) and scanning electron microscopy (SEM), are labour-intensive, subjective, and time-consuming, limiting their scalability for high-throughput analysis. In this study, we present an automated methodology integrating edge detection with generative diffusion models to effectively identify grains, eliminate noise, and connect broken segments in alignment with predicted grain boundaries. Due to the limited availability of adequate images preventing the training of deep machine learning models, a new seven-stage methodology is employed to generate synthetic TEM images for training. This concept-oriented synthetic data approach can be extended to any field of interest where the scarcity of data is a challenge. The presented model was applied to various metals with average grain sizes down to the nanoscale, producing grain morphologies from low-resolution TEM images that are comparable to those obtained from advanced and demanding experimental techniques with an average accuracy of 97.23%.","sentences":["The traditional techniques for extracting polycrystalline grain structures from microscopy images, such as transmission electron microscopy (TEM) and scanning electron microscopy (SEM), are labour-intensive, subjective, and time-consuming, limiting their scalability for high-throughput analysis.","In this study, we present an automated methodology integrating edge detection with generative diffusion models to effectively identify grains, eliminate noise, and connect broken segments in alignment with predicted grain boundaries.","Due to the limited availability of adequate images preventing the training of deep machine learning models, a new seven-stage methodology is employed to generate synthetic TEM images for training.","This concept-oriented synthetic data approach can be extended to any field of interest where the scarcity of data is a challenge.","The presented model was applied to various metals with average grain sizes down to the nanoscale, producing grain morphologies from low-resolution TEM images that are comparable to those obtained from advanced and demanding experimental techniques with an average accuracy of 97.23%."],"url":"http://arxiv.org/abs/2504.14782v1"}
{"created":"2025-04-20 23:50:23","title":"Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions","abstract":"The exponential growth of Large Language Models (LLMs) continues to highlight the need for efficient strategies to meet ever-expanding computational and data demands. This survey provides a comprehensive analysis of two complementary paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both aimed at compressing LLMs while preserving their advanced reasoning capabilities and linguistic diversity. We first examine key methodologies in KD, such as task-specific alignment, rationale-based training, and multi-teacher frameworks, alongside DD techniques that synthesize compact, high-impact datasets through optimization-based gradient matching, latent space regularization, and generative synthesis. Building on these foundations, we explore how integrating KD and DD can produce more effective and scalable compression strategies. Together, these approaches address persistent challenges in model scalability, architectural heterogeneity, and the preservation of emergent LLM abilities. We further highlight applications across domains such as healthcare and education, where distillation enables efficient deployment without sacrificing performance. Despite substantial progress, open challenges remain in preserving emergent reasoning and linguistic diversity, enabling efficient adaptation to continually evolving teacher models and datasets, and establishing comprehensive evaluation protocols. By synthesizing methodological innovations, theoretical foundations, and practical insights, our survey charts a path toward sustainable, resource-efficient LLMs through the tighter integration of KD and DD principles.","sentences":["The exponential growth of Large Language Models (LLMs) continues to highlight the need for efficient strategies to meet ever-expanding computational and data demands.","This survey provides a comprehensive analysis of two complementary paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both aimed at compressing LLMs while preserving their advanced reasoning capabilities and linguistic diversity.","We first examine key methodologies in KD, such as task-specific alignment, rationale-based training, and multi-teacher frameworks, alongside DD techniques that synthesize compact, high-impact datasets through optimization-based gradient matching, latent space regularization, and generative synthesis.","Building on these foundations, we explore how integrating KD and DD can produce more effective and scalable compression strategies.","Together, these approaches address persistent challenges in model scalability, architectural heterogeneity, and the preservation of emergent LLM abilities.","We further highlight applications across domains such as healthcare and education, where distillation enables efficient deployment without sacrificing performance.","Despite substantial progress, open challenges remain in preserving emergent reasoning and linguistic diversity, enabling efficient adaptation to continually evolving teacher models and datasets, and establishing comprehensive evaluation protocols.","By synthesizing methodological innovations, theoretical foundations, and practical insights, our survey charts a path toward sustainable, resource-efficient LLMs through the tighter integration of KD and DD principles."],"url":"http://arxiv.org/abs/2504.14772v1"}
{"created":"2025-04-20 23:40:29","title":"Building babyGPTs: Youth Engaging in Data Practices and Ethical Considerations through the Construction of Generative Language Models","abstract":"As generative language models (GLMs) have gained popularity, youth are increasingly using them in their everyday lives. As such, most research has centered on supporting youth as users of GLM-powered systems. However, we know little of how to engage youth in the design of these models. Building on the rich legacy of child-computer interaction research that positions youth as designers of computing systems, we explore how to support young people in designing GLMs. Through a case study of three teenagers (ages 14-15) building a babyGPT screenplay generator, we illustrate how the team developed a model while engaging in artificial intelligence/machine learning-relevant data practices and addressing ethical issues. This paper contributes a case study that demonstrates the feasibility of engaging youth in building GLMs.","sentences":["As generative language models (GLMs) have gained popularity, youth are increasingly using them in their everyday lives.","As such, most research has centered on supporting youth as users of GLM-powered systems.","However, we know little of how to engage youth in the design of these models.","Building on the rich legacy of child-computer interaction research that positions youth as designers of computing systems, we explore how to support young people in designing GLMs.","Through a case study of three teenagers (ages 14-15) building a babyGPT screenplay generator, we illustrate how the team developed a model while engaging in artificial intelligence/machine learning-relevant data practices and addressing ethical issues.","This paper contributes a case study that demonstrates the feasibility of engaging youth in building GLMs."],"url":"http://arxiv.org/abs/2504.14769v1"}
{"created":"2025-04-20 23:35:41","title":"Steering Semantic Data Processing With DocWrangler","abstract":"Unstructured text has long been difficult to automatically analyze at scale. Large language models (LLMs) now offer a way forward by enabling {\\em semantic data processing}, where familiar data processing operators (e.g., map, reduce, filter) are powered by LLMs instead of code. However, building effective semantic data processing pipelines presents a departure from traditional data pipelines: users need to understand their data to write effective pipelines, yet they need to construct pipelines to extract the data necessary for that understanding -- all while navigating LLM idiosyncrasies and inconsistencies. We present \\docwrangler, a mixed-initiative integrated development environment (IDE) for semantic data processing with three novel features to address the gaps between the user, their data, and their pipeline: {\\em (i) In-Situ User Notes} that allows users to inspect, annotate, and track observations across documents and LLM outputs, {\\em (ii) LLM-Assisted Prompt Refinement} that transforms user notes into improved operations, and {\\em (iii) LLM-Assisted Operation Decomposition} that identifies when operations or documents are too complex for the LLM to correctly process and suggests decompositions. Our evaluation combines a think-aloud study with 10 participants and a public-facing deployment (available at \\href{https://docetl.org/playground}{docetl.org/playground}) with 1,500+ recorded sessions, revealing how users develop systematic strategies for their semantic data processing tasks; e.g., transforming open-ended operations into classifiers for easier validation and intentionally using vague prompts to learn more about their data or LLM capabilities.","sentences":["Unstructured text has long been difficult to automatically analyze at scale.","Large language models (LLMs) now offer a way forward by enabling {\\em semantic data processing}, where familiar data processing operators (e.g., map, reduce, filter) are powered by LLMs instead of code.","However, building effective semantic data processing pipelines presents a departure from traditional data pipelines: users need to understand their data to write effective pipelines, yet they need to construct pipelines to extract the data necessary for that understanding -- all while navigating LLM idiosyncrasies and inconsistencies.","We present \\docwrangler, a mixed-initiative integrated development environment (IDE) for semantic data processing with three novel features to address the gaps between the user, their data, and their pipeline: {\\em (i) In-Situ User Notes} that allows users to inspect, annotate, and track observations across documents and LLM outputs, {\\em (ii) LLM-Assisted Prompt Refinement} that transforms user notes into improved operations, and {\\em (iii) LLM-Assisted Operation Decomposition} that identifies when operations or documents are too complex for the LLM to correctly process and suggests decompositions.","Our evaluation combines a think-aloud study with 10 participants and a public-facing deployment (available at \\href{https://docetl.org/playground}{docetl.org/playground}) with 1,500+ recorded sessions, revealing how users develop systematic strategies for their semantic data processing tasks; e.g., transforming open-ended operations into classifiers for easier validation and intentionally using vague prompts to learn more about their data or LLM capabilities."],"url":"http://arxiv.org/abs/2504.14764v1"}
{"created":"2025-04-20 22:37:43","title":"SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs","abstract":"Large language models (LLMs) are transforming automated program repair (APR) through agent-based approaches that localize bugs, generate patches, and verify fixes. However, the lack of high-quality, scalable training datasets, especially those with verifiable outputs and intermediate reasoning traces-limits progress, particularly for open-source models. In this work, we present SWE-Synth, a framework for synthesizing realistic, verifiable, and process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM agents to simulate debugging workflows, producing not only bug-fix pairs but also test cases and structured repair trajectories. Compared to manually curated datasets, our method scales with minimal human effort while preserving contextual richness and correctness. Experiments show that models trained on SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench Lite. Our results highlight the potential of synthetic, agent-generated data to advance the state of the art in APR and software engineering automation.","sentences":["Large language models (LLMs) are transforming automated program repair (APR) through agent-based approaches that localize bugs, generate patches, and verify fixes.","However, the lack of high-quality, scalable training datasets, especially those with verifiable outputs and intermediate reasoning traces-limits progress, particularly for open-source models.","In this work, we present SWE-Synth, a framework for synthesizing realistic, verifiable, and process-aware bug-fix datasets at the repository level.","SWE-Synth leverages LLM agents to simulate debugging workflows, producing not only bug-fix pairs but also test cases and structured repair trajectories.","Compared to manually curated datasets, our method scales with minimal human effort while preserving contextual richness and correctness.","Experiments show that models trained on SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench Lite.","Our results highlight the potential of synthetic, agent-generated data to advance the state of the art in APR and software engineering automation."],"url":"http://arxiv.org/abs/2504.14757v1"}
{"created":"2025-04-20 21:42:25","title":"PPO-EPO: Energy and Performance Optimization for O-RAN Using Reinforcement Learning","abstract":"Energy consumption in mobile communication networks has become a significant challenge due to its direct impact on Capital Expenditure (CAPEX) and Operational Expenditure (OPEX). The introduction of Open RAN (O-RAN) enables telecommunication providers to leverage network intelligence to optimize energy efficiency while maintaining Quality of Service (QoS). One promising approach involves traffic-aware cell shutdown strategies, where underutilized cells are selectively deactivated without compromising overall network performance. However, achieving this balance requires precise traffic steering mechanisms that account for throughput performance, power efficiency, and network interference constraints.   This work proposes a reinforcement learning (RL) model based on the Proximal Policy Optimization (PPO) algorithm to optimize traffic steering and energy efficiency. The objective is to maximize energy efficiency and performance gains while strategically shutting down underutilized cells. The proposed RL model learns adaptive policies to make optimal shutdown decisions by considering throughput degradation constraints, interference thresholds, and PRB utilization balance. Experimental validation using TeraVM Viavi RIC tester data demonstrates that our method significantly improves the network's energy efficiency and downlink throughput.","sentences":["Energy consumption in mobile communication networks has become a significant challenge due to its direct impact on Capital Expenditure (CAPEX) and Operational Expenditure (OPEX).","The introduction of Open RAN (O-RAN) enables telecommunication providers to leverage network intelligence to optimize energy efficiency while maintaining Quality of Service (QoS).","One promising approach involves traffic-aware cell shutdown strategies, where underutilized cells are selectively deactivated without compromising overall network performance.","However, achieving this balance requires precise traffic steering mechanisms that account for throughput performance, power efficiency, and network interference constraints.   ","This work proposes a reinforcement learning (RL) model based on the Proximal Policy Optimization (PPO) algorithm to optimize traffic steering and energy efficiency.","The objective is to maximize energy efficiency and performance gains while strategically shutting down underutilized cells.","The proposed RL model learns adaptive policies to make optimal shutdown decisions by considering throughput degradation constraints, interference thresholds, and PRB utilization balance.","Experimental validation using TeraVM Viavi RIC tester data demonstrates that our method significantly improves the network's energy efficiency and downlink throughput."],"url":"http://arxiv.org/abs/2504.14749v1"}
{"created":"2025-04-20 21:04:23","title":"PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines","abstract":"Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains -- such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using our open-source LLM pipeline tools. This dataset is 5x larger than previous collections. Using a hold-out test split of PROMPTEVALS as a benchmark, we evaluated closed- and open-source models in generating relevant assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. We believe our dataset can spur further research in LLM reliability, alignment, and prompt engineering.","sentences":["Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains -- such as finance, marketing, and e-commerce.","However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations.","To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential.","Yet, determining the right set of assertions that capture developer requirements for a task is challenging.","In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using our open-source LLM pipeline tools.","This dataset is 5x larger than previous collections.","Using a hold-out test split of PROMPTEVALS as a benchmark, we evaluated closed- and open-source models in generating relevant assertions.","Notably, our fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance.","We believe our dataset can spur further research in LLM reliability, alignment, and prompt engineering."],"url":"http://arxiv.org/abs/2504.14738v1"}
{"created":"2025-04-20 19:36:04","title":"Approximate all-pairs Hamming distances and 0-1 matrix multiplication","abstract":"Arslan showed that computing all-pairs Hamming distances is easily   reducible to arithmetic 0-1 matrix multiplication (IPL 2018). We   provide a reverse, linear-time reduction of arithmetic 0-1 matrix   multiplication to computing all-pairs distances in a Hamming space.   On the other hand, we present a fast randomized algorithm for   approximate all-pairs distances in a Hamming space. By combining it   with our reduction, we obtain also a fast randomized algorithm for   approximate 0-1 matrix multiplication. Next, we present an   output-sensitive randomized algorithm for a minimum spanning tree of   a set of points in a generalized Hamming space, the lower is the   cost of the minimum spanning tree the faster is our   algorithm. Finally, we provide $(2+\\epsilon)$- approximation   algorithms for the $\\ell$-center clustering and minimum-diameter   $\\ell$-clustering problems in a Hamming space $\\{0,1\\}^d$ that are   substantially faster than the known $2$-approximation ones when both   $\\ell$ and $d$ are super-logarithmic.","sentences":["Arslan showed that computing all-pairs Hamming distances is easily   reducible to arithmetic 0-1 matrix multiplication (IPL 2018).","We   provide a reverse, linear-time reduction of arithmetic 0-1 matrix   multiplication to computing all-pairs distances in a Hamming space.   ","On the other hand, we present a fast randomized algorithm for   approximate all-pairs distances in a Hamming space.","By combining it   with our reduction, we obtain also a fast randomized algorithm for   approximate 0-1 matrix multiplication.","Next, we present an   output-sensitive randomized algorithm for a minimum spanning tree of   a set of points in a generalized Hamming space, the lower is the   cost of the minimum spanning tree the faster is our   algorithm.","Finally, we provide $(2+\\epsilon)$- approximation   algorithms for the $\\ell$-center clustering and minimum-diameter   $\\ell$-clustering problems in a Hamming space $\\{0,1\\}^d$ that are   substantially faster than the known $2$-approximation ones when both   $\\ell$ and $d$ are super-logarithmic."],"url":"http://arxiv.org/abs/2504.14723v1"}
{"created":"2025-04-20 19:18:13","title":"Video QoE Metrics from Encrypted Traffic: Application-agnostic Methodology","abstract":"Instant Messaging-Based Video Call Applications (IMVCAs) and Video Conferencing Applications (VCAs) have become integral to modern communication. Ensuring a high Quality of Experience (QoE) for users in this context is critical for network operators, as network conditions significantly impact user QoE. However, network operators lack access to end-device QoE metrics due to encrypted traffic. Existing solutions estimate QoE metrics from encrypted traffic traversing the network, with the most advanced approaches leveraging machine learning models. Subsequently, the need for ground truth QoE metrics for training and validation poses a challenge, as not all video applications provide these metrics. To address this challenge, we propose an application-agnostic approach for objective QoE estimation from encrypted traffic. Independent of the video application, we obtained key video QoE metrics, enabling broad applicability to various proprietary IMVCAs and VCAs. To validate our solution, we created a diverse dataset from WhatsApp video sessions under various network conditions, comprising 25,680 seconds of traffic data and QoE metrics. Our evaluation shows high performance across the entire dataset, with 85.2% accuracy for FPS predictions within an error margin of two FPS, and 90.2% accuracy for PIQE-based quality rating classification.","sentences":["Instant Messaging-Based Video Call Applications (IMVCAs) and Video Conferencing Applications (VCAs) have become integral to modern communication.","Ensuring a high Quality of Experience (QoE) for users in this context is critical for network operators, as network conditions significantly impact user QoE.","However, network operators lack access to end-device QoE metrics due to encrypted traffic.","Existing solutions estimate QoE metrics from encrypted traffic traversing the network, with the most advanced approaches leveraging machine learning models.","Subsequently, the need for ground truth QoE metrics for training and validation poses a challenge, as not all video applications provide these metrics.","To address this challenge, we propose an application-agnostic approach for objective QoE estimation from encrypted traffic.","Independent of the video application, we obtained key video QoE metrics, enabling broad applicability to various proprietary IMVCAs and VCAs.","To validate our solution, we created a diverse dataset from WhatsApp video sessions under various network conditions, comprising 25,680 seconds of traffic data and QoE metrics.","Our evaluation shows high performance across the entire dataset, with 85.2% accuracy for FPS predictions within an error margin of two FPS, and 90.2% accuracy for PIQE-based quality rating classification."],"url":"http://arxiv.org/abs/2504.14720v1"}
{"created":"2025-04-20 18:51:26","title":"Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline","abstract":"Machine learning (ML)-based planners have recently gained significant attention. They offer advantages over traditional optimization-based planning algorithms. These advantages include fewer manually selected parameters and faster development. Within ML-based planning, imitation learning (IL) is a common algorithm. It primarily learns driving policies directly from supervised trajectory data. While IL has demonstrated strong performance on many open-loop benchmarks, it remains challenging to determine if the learned policy truly understands fundamental driving principles, rather than simply extrapolating from the ego-vehicle's initial state. Several studies have identified this limitation and proposed algorithms to address it. However, these methods often use original datasets for evaluation. In these datasets, future trajectories are heavily dependent on initial conditions. Furthermore, IL often overfits to the most common scenarios. It struggles to generalize to rare or unseen situations.   To address these challenges, this work proposes: 1) a novel closed-loop simulator supporting both imitation and reinforcement learning, 2) a causal benchmark derived from the Waymo Open Dataset to rigorously assess the impact of the copycat problem, and 3) a novel framework integrating imitation learning and reinforcement learning to overcome the limitations of purely imitative approaches. The code for this work will be released soon.","sentences":["Machine learning (ML)-based planners have recently gained significant attention.","They offer advantages over traditional optimization-based planning algorithms.","These advantages include fewer manually selected parameters and faster development.","Within ML-based planning, imitation learning (IL) is a common algorithm.","It primarily learns driving policies directly from supervised trajectory data.","While IL has demonstrated strong performance on many open-loop benchmarks, it remains challenging to determine if the learned policy truly understands fundamental driving principles, rather than simply extrapolating from the ego-vehicle's initial state.","Several studies have identified this limitation and proposed algorithms to address it.","However, these methods often use original datasets for evaluation.","In these datasets, future trajectories are heavily dependent on initial conditions.","Furthermore, IL often overfits to the most common scenarios.","It struggles to generalize to rare or unseen situations.   ","To address these challenges, this work proposes: 1) a novel closed-loop simulator supporting both imitation and reinforcement learning, 2) a causal benchmark derived from the Waymo Open Dataset to rigorously assess the impact of the copycat problem, and 3) a novel framework integrating imitation learning and reinforcement learning to overcome the limitations of purely imitative approaches.","The code for this work will be released soon."],"url":"http://arxiv.org/abs/2504.14709v1"}
{"created":"2025-04-20 18:51:08","title":"Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives","abstract":"This study explores BERTopic's potential for modeling open-ended Belgian Dutch daily narratives, contrasting its performance with Latent Dirichlet Allocation (LDA) and KMeans. Although LDA scores well on certain automated metrics, human evaluations reveal semantically irrelevant co-occurrences, highlighting the limitations of purely statistic-based methods. In contrast, BERTopic's reliance on contextual embeddings yields culturally resonant themes, underscoring the importance of hybrid evaluation frameworks that account for morphologically rich languages. KMeans performed less coherently than prior research suggested, pointing to the unique challenges posed by personal narratives. Our findings emphasize the need for robust generalization in NLP models, especially in underrepresented linguistic contexts.","sentences":["This study explores BERTopic's potential for modeling open-ended Belgian Dutch daily narratives, contrasting its performance with Latent Dirichlet Allocation (LDA) and KMeans.","Although LDA scores well on certain automated metrics, human evaluations reveal semantically irrelevant co-occurrences, highlighting the limitations of purely statistic-based methods.","In contrast, BERTopic's reliance on contextual embeddings yields culturally resonant themes, underscoring the importance of hybrid evaluation frameworks that account for morphologically rich languages.","KMeans performed less coherently than prior research suggested, pointing to the unique challenges posed by personal narratives.","Our findings emphasize the need for robust generalization in NLP models, especially in underrepresented linguistic contexts."],"url":"http://arxiv.org/abs/2504.14707v1"}
{"created":"2025-04-20 18:37:51","title":"Can We Ignore Labels In Out of Distribution Detection?","abstract":"Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems. One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety. Due to the cost of labeled data, recent works have investigated the feasibility of self-supervised learning (SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In this work, we identify a set of conditions for a theoretical guarantee of failure in unlabeled OOD detection algorithms from an information-theoretic perspective. These conditions are present in all OOD tasks dealing with real-world data: I) we provide theoretical proof of unlabeled OOD detection failure when there exists zero mutual information between the learning objective and the in-distribution labels, a.k.a. 'label blindness', II) we define a new OOD task - Adjacent OOD detection - that tests for label blindness and accounts for a previously ignored safety gap in all OOD detection benchmarks, and III) we perform experiments demonstrating that existing unlabeled OOD methods fail under conditions suggested by our label blindness theory and analyze the implications for future research in unlabeled OOD methods.","sentences":["Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems.","One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety.","Due to the cost of labeled data, recent works have investigated the feasibility of self-supervised learning (SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection.","In this work, we identify a set of conditions for a theoretical guarantee of failure in unlabeled OOD detection algorithms from an information-theoretic perspective.","These conditions are present in all OOD tasks dealing with real-world data: I) we provide theoretical proof of unlabeled OOD detection failure when there exists zero mutual information between the learning objective and the in-distribution labels, a.k.a. 'label blindness', II) we define a new OOD task - Adjacent OOD detection - that tests for label blindness and accounts for a previously ignored safety gap in all OOD detection benchmarks, and III) we perform experiments demonstrating that existing unlabeled OOD methods fail under conditions suggested by our label blindness theory and analyze the implications for future research in unlabeled OOD methods."],"url":"http://arxiv.org/abs/2504.14704v1"}
{"created":"2025-04-20 18:28:13","title":"IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays","abstract":"Spine surgery is a high-risk intervention demanding precise execution, often supported by image-based navigation systems. Recently, supervised learning approaches have gained attention for reconstructing 3D spinal anatomy from sparse fluoroscopic data, significantly reducing reliance on radiation-intensive 3D imaging systems. However, these methods typically require large amounts of annotated training data and may struggle to generalize across varying patient anatomies or imaging conditions. Instance-learning approaches like Gaussian splatting could offer an alternative by avoiding extensive annotation requirements. While Gaussian splatting has shown promise for novel view synthesis, its application to sparse, arbitrarily posed real intraoperative X-rays has remained largely unexplored. This work addresses this limitation by extending the $R^2$-Gaussian splatting framework to reconstruct anatomically consistent 3D volumes under these challenging conditions. We introduce an anatomy-guided radiographic standardization step using style transfer, improving visual consistency across views, and enhancing reconstruction quality. Notably, our framework requires no pretraining, making it inherently adaptable to new patients and anatomies. We evaluated our approach using an ex-vivo dataset. Expert surgical evaluation confirmed the clinical utility of the 3D reconstructions for navigation, especially when using 20 to 30 views, and highlighted the standardization's benefit for anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM) confirmed performance trade-offs compared to idealized settings, but also validated the improvement gained from standardization over raw inputs. This work demonstrates the feasibility of instance-based volumetric reconstruction from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for surgical navigation.","sentences":["Spine surgery is a high-risk intervention demanding precise execution, often supported by image-based navigation systems.","Recently, supervised learning approaches have gained attention for reconstructing 3D spinal anatomy from sparse fluoroscopic data, significantly reducing reliance on radiation-intensive 3D imaging systems.","However, these methods typically require large amounts of annotated training data and may struggle to generalize across varying patient anatomies or imaging conditions.","Instance-learning approaches like Gaussian splatting could offer an alternative by avoiding extensive annotation requirements.","While Gaussian splatting has shown promise for novel view synthesis, its application to sparse, arbitrarily posed real intraoperative X-rays has remained largely unexplored.","This work addresses this limitation by extending the $R^2$-Gaussian splatting framework to reconstruct anatomically consistent 3D volumes under these challenging conditions.","We introduce an anatomy-guided radiographic standardization step using style transfer, improving visual consistency across views, and enhancing reconstruction quality.","Notably, our framework requires no pretraining, making it inherently adaptable to new patients and anatomies.","We evaluated our approach using an ex-vivo dataset.","Expert surgical evaluation confirmed the clinical utility of the 3D reconstructions for navigation, especially when using 20 to 30 views, and highlighted the standardization's benefit for anatomical clarity.","Benchmarking via quantitative 2D metrics (PSNR/SSIM) confirmed performance trade-offs compared to idealized settings, but also validated the improvement gained from standardization over raw inputs.","This work demonstrates the feasibility of instance-based volumetric reconstruction from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for surgical navigation."],"url":"http://arxiv.org/abs/2504.14699v1"}
{"created":"2025-04-20 18:20:11","title":"Reveal-or-Obscure: A Differentially Private Sampling Algorithm for Discrete Distributions","abstract":"We introduce a differentially private (DP) algorithm called reveal-or-obscure (ROO) to generate a single representative sample from a dataset of $n$ observations drawn i.i.d. from an unknown discrete distribution $P$. Unlike methods that add explicit noise to the estimated empirical distribution, ROO achieves $\\epsilon$-differential privacy by randomly choosing whether to \"reveal\" or \"obscure\" the empirical distribution. While ROO is structurally identical to Algorithm 1 proposed by Cheu and Nayak (arXiv:2412.10512), we prove a strictly better bound on the sampling complexity than that established in Theorem 12 of (arXiv:2412.10512). To further improve the privacy-utility trade-off, we propose a novel generalized sampling algorithm called Data-Specific ROO (DS-ROO), where the probability of obscuring the empirical distribution of the dataset is chosen adaptively. We prove that DS-ROO satisfies $\\epsilon$-DP, and provide empirical evidence that DS-ROO can achieve better utility under the same privacy budget of vanilla ROO.","sentences":["We introduce a differentially private (DP) algorithm called reveal-or-obscure (ROO) to generate a single representative sample from a dataset of $n$ observations drawn i.i.d.","from an unknown discrete distribution $P$.","Unlike methods that add explicit noise to the estimated empirical distribution, ROO achieves $\\epsilon$-differential privacy by randomly choosing whether to \"reveal\" or \"obscure\" the empirical distribution.","While ROO is structurally identical to Algorithm 1 proposed by Cheu and Nayak (arXiv:2412.10512), we prove a strictly better bound on the sampling complexity than that established in Theorem 12 of (arXiv:2412.10512).","To further improve the privacy-utility trade-off, we propose a novel generalized sampling algorithm called Data-Specific ROO (DS-ROO), where the probability of obscuring the empirical distribution of the dataset is chosen adaptively.","We prove that DS-ROO satisfies $\\epsilon$-DP, and provide empirical evidence that DS-ROO can achieve better utility under the same privacy budget of vanilla ROO."],"url":"http://arxiv.org/abs/2504.14696v1"}
{"created":"2025-04-20 18:06:55","title":"Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data","abstract":"Federated learning (FL) enables multiple clients to collaboratively train a global model while keeping local data decentralized. Data heterogeneity (non-IID) across clients has imposed significant challenges to FL, which makes local models re-optimize towards their own local optima and forget the global knowledge, resulting in performance degradation and convergence slowdown. Many existing works have attempted to address the non-IID issue by adding an extra global-model-based regularizing item to the local training but without an adaption scheme, which is not efficient enough to achieve high performance with deep learning models. In this paper, we propose a Selective Self-Distillation method for Federated learning (FedSSD), which imposes adaptive constraints on the local updates by self-distilling the global model's knowledge and selectively weighting it by evaluating the credibility at both the class and sample level. The convergence guarantee of FedSSD is theoretically analyzed and extensive experiments are conducted on three public benchmark datasets, which demonstrates that FedSSD achieves better generalization and robustness in fewer communication rounds, compared with other state-of-the-art FL methods.","sentences":["Federated learning (FL) enables multiple clients to collaboratively train a global model while keeping local data decentralized.","Data heterogeneity (non-IID) across clients has imposed significant challenges to FL, which makes local models re-optimize towards their own local optima and forget the global knowledge, resulting in performance degradation and convergence slowdown.","Many existing works have attempted to address the non-IID issue by adding an extra global-model-based regularizing item to the local training but without an adaption scheme, which is not efficient enough to achieve high performance with deep learning models.","In this paper, we propose a Selective Self-Distillation method for Federated learning (FedSSD), which imposes adaptive constraints on the local updates by self-distilling the global model's knowledge and selectively weighting it by evaluating the credibility at both the class and sample level.","The convergence guarantee of FedSSD is theoretically analyzed and extensive experiments are conducted on three public benchmark datasets, which demonstrates that FedSSD achieves better generalization and robustness in fewer communication rounds, compared with other state-of-the-art FL methods."],"url":"http://arxiv.org/abs/2504.14694v1"}
{"created":"2025-04-20 17:53:56","title":"OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding","abstract":"The practical deployment of medical vision-language models (Med-VLMs) necessitates seamless integration of textual data with diverse visual modalities, including 2D/3D images and videos, yet existing models typically employ separate encoders for different modalities. To address this limitation, we present OmniV-Med, a unified framework for multimodal medical understanding. Our technical contributions are threefold: First, we construct OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K instructional samples spanning 14 medical image modalities and 11 clinical tasks. Second, we devise a rotary position-adaptive encoder that processes multi-resolution 2D/3D images and videos within a unified architecture, diverging from conventional modality-specific encoders. Third, we introduce a medical-aware token pruning mechanism that exploits spatial-temporal redundancy in volumetric data (e.g., consecutive CT slices) and medical videos, effectively reducing 60\\% of visual tokens without performance degradation. Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art performance on 7 benchmarks spanning 2D/3D medical imaging and video understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains comparable performance while requiring only 8 RTX3090 GPUs for training and supporting efficient long-video inference. Data, code and model will be released.","sentences":["The practical deployment of medical vision-language models (Med-VLMs) necessitates seamless integration of textual data with diverse visual modalities, including 2D/3D images and videos, yet existing models typically employ separate encoders for different modalities.","To address this limitation, we present OmniV-Med, a unified framework for multimodal medical understanding.","Our technical contributions are threefold:","First, we construct OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K instructional samples spanning 14 medical image modalities and 11 clinical tasks.","Second, we devise a rotary position-adaptive encoder that processes multi-resolution 2D/3D images and videos within a unified architecture, diverging from conventional modality-specific encoders.","Third, we introduce a medical-aware token pruning mechanism that exploits spatial-temporal redundancy in volumetric data (e.g., consecutive CT slices) and medical videos, effectively reducing 60\\% of visual tokens without performance degradation.","Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art performance on 7 benchmarks spanning 2D/3D medical imaging and video understanding tasks.","Notably, our lightweight variant (OmniV-Med-1.5B) attains comparable performance while requiring only 8 RTX3090 GPUs for training and supporting efficient long-video inference.","Data, code and model will be released."],"url":"http://arxiv.org/abs/2504.14692v1"}
{"created":"2025-04-20 17:36:52","title":"Uncovering Issues in the Radio Access Network by Looking at the Neighbors","abstract":"Mobile network operators (MNOs) manage Radio Access Networks (RANs) with massive amounts of cells over multiple radio generations (2G-5G). To handle such complexity, operations teams rely on monitoring systems, including anomaly detection tools that identify unexpected behaviors. In this paper, we present c-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph Neural Networks (GNNs). Our solution captures spatio-temporal variations by analyzing the behavior of individual cells in relation to their local neighborhoods, enabling the detection of anomalies that are independent of external mobility factors. This, in turn, allows focusing on anomalies associated with network issues (e.g., misconfigurations, equipment failures). We evaluate c-ANEMON using real-world data from a large European metropolitan area (7,890 cells; 3 months). First, we show that the GNN model within our solution generalizes effectively to cells from previously unseen areas, suggesting the possibility of using a single model across extensive deployment regions. Then, we analyze the anomalies detected by c-ANEMON through manual inspection and define several categories of long-lasting anomalies (6+ hours). Notably, 45.95% of these anomalies fall into a category that is more likely to require intervention by operations teams.","sentences":["Mobile network operators (MNOs) manage Radio Access Networks (RANs) with massive amounts of cells over multiple radio generations (2G-5G).","To handle such complexity, operations teams rely on monitoring systems, including anomaly detection tools that identify unexpected behaviors.","In this paper, we present c-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph Neural Networks (GNNs).","Our solution captures spatio-temporal variations by analyzing the behavior of individual cells in relation to their local neighborhoods, enabling the detection of anomalies that are independent of external mobility factors.","This, in turn, allows focusing on anomalies associated with network issues (e.g., misconfigurations, equipment failures).","We evaluate c-ANEMON using real-world data from a large European metropolitan area (7,890 cells; 3 months).","First, we show that the GNN model within our solution generalizes effectively to cells from previously unseen areas, suggesting the possibility of using a single model across extensive deployment regions.","Then, we analyze the anomalies detected by c-ANEMON through manual inspection and define several categories of long-lasting anomalies (6+ hours).","Notably, 45.95% of these anomalies fall into a category that is more likely to require intervention by operations teams."],"url":"http://arxiv.org/abs/2504.14686v1"}
{"created":"2025-04-20 17:19:03","title":"Polynomial-Time Constant-Approximation for Fair Sum-of-Radii Clustering","abstract":"In a seminal work, Chierichetti et al. introduced the $(t,k)$-fair clustering problem: Given a set of red points and a set of blue points in a metric space, a clustering is called fair if the number of red points in each cluster is at most $t$ times and at least $1/t$ times the number of blue points in that cluster. The goal is to compute a fair clustering with at most $k$ clusters that optimizes certain objective function. Considering this problem, they designed a polynomial-time $O(1)$- and $O(t)$-approximation for the $k$-center and the $k$-median objective, respectively. Recently, Carta et al. studied this problem with the sum-of-radii objective and obtained a $(6+\\epsilon)$-approximation with running time $O((k\\log_{1+\\epsilon}(k/\\epsilon))^kn^{O(1)})$, i.e., fixed-parameter tractable in $k$. Here $n$ is the input size. In this work, we design the first polynomial-time $O(1)$-approximation for $(t,k)$-fair clustering with the sum-of-radii objective, improving the result of Carta et al. Our result places sum-of-radii in the same group of objectives as $k$-center, that admit polynomial-time $O(1)$-approximations. This result also implies a polynomial-time $O(1)$-approximation for the Euclidean version of the problem, for which an $f(k)\\cdot n^{O(1)}$-time $(1+\\epsilon)$-approximation was known due to Drexler et al.. Here $f$ is an exponential function of $k$. We are also able to extend our result to any arbitrary $\\ell\\ge 2$ number of colors when $t=1$. This matches known results for the $k$-center and $k$-median objectives in this case. The significant disparity of sum-of-radii compared to $k$-center and $k$-median presents several complex challenges, all of which we successfully overcome in our work. Our main contribution is a novel cluster-merging-based analysis technique for sum-of-radii that helps us achieve the constant-approximation bounds.","sentences":["In a seminal work, Chierichetti et al. introduced the $(t,k)$-fair clustering problem: Given a set of red points and a set of blue points in a metric space, a clustering is called fair if the number of red points in each cluster is at most $t$ times and at least $1/t$ times the number of blue points in that cluster.","The goal is to compute a fair clustering with at most $k$ clusters that optimizes certain objective function.","Considering this problem, they designed a polynomial-time $O(1)$- and $O(t)$-approximation for the $k$-center and the $k$-median objective, respectively.","Recently, Carta et al. studied this problem with the sum-of-radii objective and obtained a $(6+\\epsilon)$-approximation with running time $O((k\\log_{1+\\epsilon}(k/\\epsilon))^kn^{O(1)})$, i.e., fixed-parameter tractable in $k$. Here $n$ is the input size.","In this work, we design the first polynomial-time $O(1)$-approximation for $(t,k)$-fair clustering with the sum-of-radii objective, improving the result of Carta et al.","Our result places sum-of-radii in the same group of objectives as $k$-center, that admit polynomial-time $O(1)$-approximations.","This result also implies a polynomial-time $O(1)$-approximation for the Euclidean version of the problem, for which an $f(k)\\cdot n^{O(1)}$-time $(1+\\epsilon)$-approximation was known due to Drexler et al..","Here $f$ is an exponential function of $k$. We are also able to extend our result to any arbitrary $\\ell\\ge 2","$ number of colors when $t=1$.","This matches known results for the $k$-center and $k$-median objectives in this case.","The significant disparity of sum-of-radii compared to $k$-center and $k$-median presents several complex challenges, all of which we successfully overcome in our work.","Our main contribution is a novel cluster-merging-based analysis technique for sum-of-radii that helps us achieve the constant-approximation bounds."],"url":"http://arxiv.org/abs/2504.14683v1"}
{"created":"2025-04-20 16:20:30","title":"Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data","abstract":"The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting. To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods. Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions. Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework's succuss.","sentences":["The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting.","To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM.","TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods.","Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions.","Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework's succuss."],"url":"http://arxiv.org/abs/2504.14669v1"}
{"created":"2025-04-20 16:14:07","title":"DMPCN: Dynamic Modulated Predictive Coding Network with Hybrid Feedback Representations","abstract":"Traditional predictive coding networks, inspired by theories of brain function, consistently achieve promising results across various domains, extending their influence into the field of computer vision. However, the performance of the predictive coding networks is limited by their error feedback mechanism, which traditionally employs either local or global recurrent updates, leading to suboptimal performance in processing both local and broader details simultaneously. In addition, traditional predictive coding networks face difficulties in dynamically adjusting to the complexity and context of varying input data, which is crucial for achieving high levels of performance in diverse scenarios. Furthermore, there is a gap in the development and application of specific loss functions that could more effectively guide the model towards optimal performance. To deal with these issues, this paper introduces a hybrid prediction error feedback mechanism with dynamic modulation for deep predictive coding networks by effectively combining global contexts and local details while adjusting feedback based on input complexity. Additionally, we present a loss function tailored to this framework to improve accuracy by focusing on precise prediction error minimization. Experimental results demonstrate the superiority of our model over other approaches, showcasing faster convergence and higher predictive accuracy in CIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets.","sentences":["Traditional predictive coding networks, inspired by theories of brain function, consistently achieve promising results across various domains, extending their influence into the field of computer vision.","However, the performance of the predictive coding networks is limited by their error feedback mechanism, which traditionally employs either local or global recurrent updates, leading to suboptimal performance in processing both local and broader details simultaneously.","In addition, traditional predictive coding networks face difficulties in dynamically adjusting to the complexity and context of varying input data, which is crucial for achieving high levels of performance in diverse scenarios.","Furthermore, there is a gap in the development and application of specific loss functions that could more effectively guide the model towards optimal performance.","To deal with these issues, this paper introduces a hybrid prediction error feedback mechanism with dynamic modulation for deep predictive coding networks by effectively combining global contexts and local details while adjusting feedback based on input complexity.","Additionally, we present a loss function tailored to this framework to improve accuracy by focusing on precise prediction error minimization.","Experimental results demonstrate the superiority of our model over other approaches, showcasing faster convergence and higher predictive accuracy in CIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets."],"url":"http://arxiv.org/abs/2504.14665v1"}
{"created":"2025-04-20 16:00:38","title":"Frequency-domain Learning with Kernel Prior for Blind Image Deblurring","abstract":"While achieving excellent results on various datasets, many deep learning methods for image deblurring suffer from limited generalization capabilities with out-of-domain data. This limitation is likely caused by their dependence on certain domain-specific datasets. To address this challenge, we argue that it is necessary to introduce the kernel prior into deep learning methods, as the kernel prior remains independent of the image context. For effective fusion of kernel prior information, we adopt a rational implementation method inspired by traditional deblurring algorithms that perform deconvolution in the frequency domain. We propose a module called Frequency Integration Module (FIM) for fusing the kernel prior and combine it with a frequency-based deblurring Transfomer network. Experimental results demonstrate that our method outperforms state-of-the-art methods on multiple blind image deblurring tasks, showcasing robust generalization abilities. Source code will be available soon.","sentences":["While achieving excellent results on various datasets, many deep learning methods for image deblurring suffer from limited generalization capabilities with out-of-domain data.","This limitation is likely caused by their dependence on certain domain-specific datasets.","To address this challenge, we argue that it is necessary to introduce the kernel prior into deep learning methods, as the kernel prior remains independent of the image context.","For effective fusion of kernel prior information, we adopt a rational implementation method inspired by traditional deblurring algorithms that perform deconvolution in the frequency domain.","We propose a module called Frequency Integration Module (FIM) for fusing the kernel prior and combine it with a frequency-based deblurring Transfomer network.","Experimental results demonstrate that our method outperforms state-of-the-art methods on multiple blind image deblurring tasks, showcasing robust generalization abilities.","Source code will be available soon."],"url":"http://arxiv.org/abs/2504.14664v1"}
{"created":"2025-04-20 16:00:37","title":"The Developer Experience of LGBTQIA+ People in Agile Teams: a Multivocal Literature Review","abstract":"Research on underrepresented populations is essential for fostering greater diversity within the software industry. Team diversity is important for reasons that go beyond ethics. Diversity contributes to greater innovation and productivity, helping decrease turnover rates and reduce team conflicts. Within this context, LGBTQIA+ software engineering professionals face unique challenges, e.g., self-isolation and invisibility feeling. Developer Experience (DX) encompasses cognitive, emotional, and motivational considerations, supporting the idea that improving how DX can enhance team performance, strengthen collaboration, and lead to more successful software projects. This study aimed to examine traditional and grey literature data through a Multivocal Literature Review focused on the DX of LGBTQIA+ professionals in agile teams. Our findings reveal that issues such as invisibility, prejudice, and discrimination adversely affect their experiences, compounded by the predominance of heterosexual males in the field. Conversely, professionals who feel welcomed by their teams and organizations, especially in processes tailored to their needs, report more positive team dynamics and engagement.","sentences":["Research on underrepresented populations is essential for fostering greater diversity within the software industry.","Team diversity is important for reasons that go beyond ethics.","Diversity contributes to greater innovation and productivity, helping decrease turnover rates and reduce team conflicts.","Within this context, LGBTQIA+ software engineering professionals face unique challenges, e.g., self-isolation and invisibility feeling.","Developer Experience (DX) encompasses cognitive, emotional, and motivational considerations, supporting the idea that improving how DX can enhance team performance, strengthen collaboration, and lead to more successful software projects.","This study aimed to examine traditional and grey literature data through a Multivocal Literature Review focused on the DX of LGBTQIA+ professionals in agile teams.","Our findings reveal that issues such as invisibility, prejudice, and discrimination adversely affect their experiences, compounded by the predominance of heterosexual males in the field.","Conversely, professionals who feel welcomed by their teams and organizations, especially in processes tailored to their needs, report more positive team dynamics and engagement."],"url":"http://arxiv.org/abs/2504.14663v1"}
{"created":"2025-04-20 15:37:05","title":"A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs","abstract":"Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to create privacy preserving and harmonized structured data, supporting numerous applications in healthcare. Key benefits of synthetic data include precise control over the data schema, improved fairness and representation of patient populations, and the ability to share datasets without concerns about compromising real individuals privacy. Consequently, the AI community has increasingly turned to Large Language Models (LLMs) to generate synthetic data across various domains. However, a significant challenge in healthcare is ensuring that synthetic health records reliably generalize across different hospitals, a long standing issue in the field. In this work, we evaluate the current state of commercial LLMs for generating synthetic data and investigate multiple aspects of the generation process to identify areas where these models excel and where they fall short. Our main finding from this work is that while LLMs can reliably generate synthetic health records for smaller subsets of features, they struggle to preserve realistic distributions and correlations as the dimensionality of the data increases, ultimately limiting their ability to generalize across diverse hospital settings.","sentences":["Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to create privacy preserving and harmonized structured data, supporting numerous applications in healthcare.","Key benefits of synthetic data include precise control over the data schema, improved fairness and representation of patient populations, and the ability to share datasets without concerns about compromising real individuals privacy.","Consequently, the AI community has increasingly turned to Large Language Models (LLMs) to generate synthetic data across various domains.","However, a significant challenge in healthcare is ensuring that synthetic health records reliably generalize across different hospitals, a long standing issue in the field.","In this work, we evaluate the current state of commercial LLMs for generating synthetic data and investigate multiple aspects of the generation process to identify areas where these models excel and where they fall short.","Our main finding from this work is that while LLMs can reliably generate synthetic health records for smaller subsets of features, they struggle to preserve realistic distributions and correlations as the dimensionality of the data increases, ultimately limiting their ability to generalize across diverse hospital settings."],"url":"http://arxiv.org/abs/2504.14657v1"}
{"created":"2025-04-20 15:25:59","title":"BLACKOUT: Data-Oblivious Computation with Blinded Capabilities","abstract":"Lack of memory-safety and exposure to side channels are two prominent, persistent challenges for the secure implementation of software. Memory-safe programming languages promise to significantly reduce the prevalence of memory-safety bugs, but make it more difficult to implement side-channel-resistant code. We aim to address both memory-safety and side-channel resistance by augmenting memory-safe hardware with the ability for data-oblivious programming. We describe an extension to the CHERI capability architecture to provide blinded capabilities that allow data-oblivious computation to be carried out by userspace tasks. We also present BLACKOUT, our realization of blinded capabilities on a FPGA softcore based on the speculative out-of-order CHERI-Toooba processor and extend the CHERI-enabled Clang/LLVM compiler and the CheriBSD operating system with support for blinded capabilities. BLACKOUT makes writing side-channel-resistant code easier by making non-data-oblivious operations via blinded capabilities explicitly fault. Through rigorous evaluation we show that BLACKOUT ensures memory operated on through blinded capabilities is securely allocated, used, and reclaimed and demonstrate that, in benchmarks comparable to those used by previous work, BLACKOUT imposes only a small performance degradation (1.5% geometric mean) compared to the baseline CHERI-Toooba processor.","sentences":["Lack of memory-safety and exposure to side channels are two prominent, persistent challenges for the secure implementation of software.","Memory-safe programming languages promise to significantly reduce the prevalence of memory-safety bugs, but make it more difficult to implement side-channel-resistant code.","We aim to address both memory-safety and side-channel resistance by augmenting memory-safe hardware with the ability for data-oblivious programming.","We describe an extension to the CHERI capability architecture to provide blinded capabilities that allow data-oblivious computation to be carried out by userspace tasks.","We also present BLACKOUT, our realization of blinded capabilities on a FPGA softcore based on the speculative out-of-order CHERI-Toooba processor and extend the CHERI-enabled Clang/LLVM compiler and the CheriBSD operating system with support for blinded capabilities.","BLACKOUT makes writing side-channel-resistant code easier by making non-data-oblivious operations via blinded capabilities explicitly fault.","Through rigorous evaluation we show that BLACKOUT ensures memory operated on through blinded capabilities is securely allocated, used, and reclaimed and demonstrate that, in benchmarks comparable to those used by previous work, BLACKOUT imposes only a small performance degradation (1.5% geometric mean) compared to the baseline CHERI-Toooba processor."],"url":"http://arxiv.org/abs/2504.14654v1"}
{"created":"2025-04-20 15:25:58","title":"Wireless Large AI Model: Shaping the AI-Native Future of 6G and Beyond","abstract":"The emergence of sixth-generation and beyond communication systems is expected to fundamentally transform digital experiences through introducing unparalleled levels of intelligence, efficiency, and connectivity. A promising technology poised to enable this revolutionary vision is the wireless large AI model (WLAM), characterized by its exceptional capabilities in data processing, inference, and decision-making. In light of these remarkable capabilities, this paper provides a comprehensive survey of WLAM, elucidating its fundamental principles, diverse applications, critical challenges, and future research opportunities. We begin by introducing the background of WLAM and analyzing the key synergies with wireless networks, emphasizing the mutual benefits. Subsequently, we explore the foundational characteristics of WLAM, delving into their unique relevance in wireless environments. Then, the role of WLAM in optimizing wireless communication systems across various use cases and the reciprocal benefits are systematically investigated. Furthermore, we discuss the integration of WLAM with emerging technologies, highlighting their potential to enable transformative capabilities and breakthroughs in wireless communication. Finally, we thoroughly examine the high-level challenges hindering the practical implementation of WLAM and discuss pivotal future research directions.","sentences":["The emergence of sixth-generation and beyond communication systems is expected to fundamentally transform digital experiences through introducing unparalleled levels of intelligence, efficiency, and connectivity.","A promising technology poised to enable this revolutionary vision is the wireless large AI model (WLAM), characterized by its exceptional capabilities in data processing, inference, and decision-making.","In light of these remarkable capabilities, this paper provides a comprehensive survey of WLAM, elucidating its fundamental principles, diverse applications, critical challenges, and future research opportunities.","We begin by introducing the background of WLAM and analyzing the key synergies with wireless networks, emphasizing the mutual benefits.","Subsequently, we explore the foundational characteristics of WLAM, delving into their unique relevance in wireless environments.","Then, the role of WLAM in optimizing wireless communication systems across various use cases and the reciprocal benefits are systematically investigated.","Furthermore, we discuss the integration of WLAM with emerging technologies, highlighting their potential to enable transformative capabilities and breakthroughs in wireless communication.","Finally, we thoroughly examine the high-level challenges hindering the practical implementation of WLAM and discuss pivotal future research directions."],"url":"http://arxiv.org/abs/2504.14653v1"}
{"created":"2025-04-20 14:29:39","title":"AlphaZero-Edu: Making AlphaZero Accessible to Everyone","abstract":"Recent years have witnessed significant progress in reinforcement learning, especially with Zero-like paradigms, which have greatly boosted the generalization and reasoning abilities of large-scale language models. Nevertheless, existing frameworks are often plagued by high implementation complexity and poor reproducibility. To tackle these challenges, we present AlphaZero-Edu, a lightweight, education-focused implementation built upon the mathematical framework of AlphaZero. It boasts a modular architecture that disentangles key components, enabling transparent visualization of the algorithmic processes. Additionally, it is optimized for resource-efficient training on a single NVIDIA RTX 3090 GPU and features highly parallelized self-play data generation, achieving a 3.2-fold speedup with 8 processes. In Gomoku matches, the framework has demonstrated exceptional performance, achieving a consistently high win rate against human opponents. AlphaZero-Edu has been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu, providing an accessible and practical benchmark for both academic research and industrial applications.","sentences":["Recent years have witnessed significant progress in reinforcement learning, especially with Zero-like paradigms, which have greatly boosted the generalization and reasoning abilities of large-scale language models.","Nevertheless, existing frameworks are often plagued by high implementation complexity and poor reproducibility.","To tackle these challenges, we present AlphaZero-Edu, a lightweight, education-focused implementation built upon the mathematical framework of AlphaZero.","It boasts a modular architecture that disentangles key components, enabling transparent visualization of the algorithmic processes.","Additionally, it is optimized for resource-efficient training on a single NVIDIA RTX 3090 GPU and features highly parallelized self-play data generation, achieving a 3.2-fold speedup with 8 processes.","In Gomoku matches, the framework has demonstrated exceptional performance, achieving a consistently high win rate against human opponents.","AlphaZero-Edu has been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu, providing an accessible and practical benchmark for both academic research and industrial applications."],"url":"http://arxiv.org/abs/2504.14636v1"}
{"created":"2025-04-20 14:24:54","title":"Latent Representations for Visual Proprioception in Inexpensive Robots","abstract":"Robotic manipulation requires explicit or implicit knowledge of the robot's joint positions. Precise proprioception is standard in high-quality industrial robots but is often unavailable in inexpensive robots operating in unstructured environments. In this paper, we ask: to what extent can a fast, single-pass regression architecture perform visual proprioception from a single external camera image, available even in the simplest manipulation settings? We explore several latent representations, including CNNs, VAEs, ViTs, and bags of uncalibrated fiducial markers, using fine-tuning techniques adapted to the limited data available. We evaluate the achievable accuracy through experiments on an inexpensive 6-DoF robot.","sentences":["Robotic manipulation requires explicit or implicit knowledge of the robot's joint positions.","Precise proprioception is standard in high-quality industrial robots but is often unavailable in inexpensive robots operating in unstructured environments.","In this paper, we ask: to what extent can a fast, single-pass regression architecture perform visual proprioception from a single external camera image, available even in the simplest manipulation settings?","We explore several latent representations, including CNNs, VAEs, ViTs, and bags of uncalibrated fiducial markers, using fine-tuning techniques adapted to the limited data available.","We evaluate the achievable accuracy through experiments on an inexpensive 6-DoF robot."],"url":"http://arxiv.org/abs/2504.14634v1"}
{"created":"2025-04-20 14:10:02","title":"GENE-FL: Gene-Driven Parameter-Efficient Dynamic Federated Learning","abstract":"Real-world \\underline{F}ederated \\underline{L}earning systems often encounter \\underline{D}ynamic clients with \\underline{A}gnostic and highly heterogeneous data distributions (DAFL), which pose challenges for efficient communication and model initialization. To address these challenges, we draw inspiration from the recently proposed Learngene paradigm, which compresses the large-scale model into lightweight, cross-task meta-information fragments. Learngene effectively encapsulates and communicates core knowledge, making it particularly well-suited for DAFL, where dynamic client participation requires communication efficiency and rapid adaptation to new data distributions. Based on this insight, we propose a Gene-driven parameter-efficient dynamic Federated Learning (GENE-FL) framework. First, local models perform quadratic constraints based on parameters with high Fisher values in the global model, as these parameters are considered to encapsulate generalizable knowledge. Second, we apply the strategy of parameter sensitivity analysis in local model parameters to condense the \\textit{learnGene} for interaction. Finally, the server aggregates these small-scale trained \\textit{learnGene}s into a robust \\textit{learnGene} with cross-task generalization capability, facilitating the rapid initialization of dynamic agnostic client models. Extensive experimental results demonstrate that GENE-FL reduces \\textbf{4 $\\times$} communication costs compared to FEDAVG and effectively initializes agnostic client models with only about \\textbf{9.04} MB.","sentences":["Real-world \\underline{F}ederated \\underline{L}earning systems often encounter \\underline{D}ynamic clients with \\underline{A}gnostic and highly heterogeneous data distributions (DAFL), which pose challenges for efficient communication and model initialization.","To address these challenges, we draw inspiration from the recently proposed Learngene paradigm, which compresses the large-scale model into lightweight, cross-task meta-information fragments.","Learngene effectively encapsulates and communicates core knowledge, making it particularly well-suited for DAFL, where dynamic client participation requires communication efficiency and rapid adaptation to new data distributions.","Based on this insight, we propose a Gene-driven parameter-efficient dynamic Federated Learning (GENE-FL) framework.","First, local models perform quadratic constraints based on parameters with high Fisher values in the global model, as these parameters are considered to encapsulate generalizable knowledge.","Second, we apply the strategy of parameter sensitivity analysis in local model parameters to condense the \\textit{learnGene} for interaction.","Finally, the server aggregates these small-scale trained \\textit{learnGene}s into a robust \\textit{learnGene} with cross-task generalization capability, facilitating the rapid initialization of dynamic agnostic client models.","Extensive experimental results demonstrate that GENE-FL reduces \\textbf{4","$\\times$} communication costs compared to FEDAVG and effectively initializes agnostic client models with only about \\textbf{9.04} MB."],"url":"http://arxiv.org/abs/2504.14628v1"}
{"created":"2025-04-20 14:05:17","title":"Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence","abstract":"Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\\%--1075\\% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\\% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.","sentences":["Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\\%--1075\\% higher than human designs.","We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency).","To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players.","Experiments show CircuitMind enables 55.6\\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics.","Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\\% of human experts without requiring specialized training.","These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs.","Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind."],"url":"http://arxiv.org/abs/2504.14625v1"}
{"created":"2025-04-20 13:58:35","title":"Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts","abstract":"Wireless signal-based human sensing technologies, such as WiFi, millimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID), enable the detection and interpretation of human presence, posture, and activities, thereby providing critical support for applications in public security, healthcare, and smart environments. These technologies exhibit notable advantages due to their non-contact operation and environmental adaptability; however, existing systems often fail to leverage the textual information inherent in datasets. To address this, we propose an innovative text-enhanced wireless sensing framework, WiTalk, that seamlessly integrates semantic knowledge through three hierarchical prompt strategies-label-only, brief description, and detailed action description-without requiring architectural modifications or incurring additional data costs. We rigorously validate this framework across three public benchmark datasets: XRF55 for human action recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action localization (TAL). Experimental results demonstrate significant performance improvements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%, 2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD improves by 4.98%; and on XRFV2, the mean average precision gains across various methods range from 4.02% to 13.68%. Our codes have been included in https://github.com/yangzhenkui/WiTalk.","sentences":["Wireless signal-based human sensing technologies, such as WiFi, millimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID), enable the detection and interpretation of human presence, posture, and activities, thereby providing critical support for applications in public security, healthcare, and smart environments.","These technologies exhibit notable advantages due to their non-contact operation and environmental adaptability; however, existing systems often fail to leverage the textual information inherent in datasets.","To address this, we propose an innovative text-enhanced wireless sensing framework, WiTalk, that seamlessly integrates semantic knowledge through three hierarchical prompt strategies-label-only, brief description, and detailed action description-without requiring architectural modifications or incurring additional data costs.","We rigorously validate this framework across three public benchmark datasets: XRF55 for human action recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action localization (TAL).","Experimental results demonstrate significant performance improvements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%, 2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD improves by 4.98%; and on XRFV2, the mean average precision gains across various methods range from 4.02% to 13.68%.","Our codes have been included in https://github.com/yangzhenkui/WiTalk."],"url":"http://arxiv.org/abs/2504.14621v1"}
{"created":"2025-04-20 13:54:28","title":"Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations","abstract":"This is the first in a series of papers exploring the rapidly expanding new opportunities arising from recent progress in language technologies for individual translators and language service providers with modest resources. The advent of advanced neural machine translation systems, large language models, and their integration into workflows via computer-assisted translation tools and translation management systems have reshaped the translation landscape. These advancements enable not only translation but also quality evaluation, error spotting, glossary generation, and adaptation to domain-specific needs, creating new technical opportunities for freelancers. In this series, we aim to empower translators with actionable methods to harness these advancements. Our approach emphasizes Translation Analytics, a suite of evaluation techniques traditionally reserved for large-scale industry applications but now becoming increasingly available for smaller-scale users. This first paper introduces a practical framework for adapting automatic evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers' needs. We illustrate the potential of these metrics using a trilingual corpus derived from a real-world project in the medical domain and provide statistical analysis correlating human evaluations with automatic scores. Our findings emphasize the importance of proactive engagement with emerging technologies to not only adapt but thrive in the evolving professional environment.","sentences":["This is the first in a series of papers exploring the rapidly expanding new opportunities arising from recent progress in language technologies for individual translators and language service providers with modest resources.","The advent of advanced neural machine translation systems, large language models, and their integration into workflows via computer-assisted translation tools and translation management systems have reshaped the translation landscape.","These advancements enable not only translation but also quality evaluation, error spotting, glossary generation, and adaptation to domain-specific needs, creating new technical opportunities for freelancers.","In this series, we aim to empower translators with actionable methods to harness these advancements.","Our approach emphasizes Translation Analytics, a suite of evaluation techniques traditionally reserved for large-scale industry applications but now becoming increasingly available for smaller-scale users.","This first paper introduces a practical framework for adapting automatic evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers' needs.","We illustrate the potential of these metrics using a trilingual corpus derived from a real-world project in the medical domain and provide statistical analysis correlating human evaluations with automatic scores.","Our findings emphasize the importance of proactive engagement with emerging technologies to not only adapt but thrive in the evolving professional environment."],"url":"http://arxiv.org/abs/2504.14619v1"}
{"created":"2025-04-20 13:46:26","title":"Semantic HARQ for Intelligent Transportation Systems: Joint Source-Channel Coding-Powered Reliable Retransmissions","abstract":"The surge of data traffic in Intelligent Transportation Systems (ITS) places a significant challenge on limited wireless resources. Semantic communication, which transmits essential semantics of the raw data, offers a promising solution by reducing redundancy and improving spectrum efficiency. However, high vehicle mobility, dynamic channel conditions, and dense vehicular networks severely impact transmission reliability in ITS. To address these limitations, we integrate Hybrid Automatic Repeat reQuest (HARQ) with Joint Source-Channel Coding (JSCC) to provide reliable semantic communications for ITS. To counteract the adverse effects of time-varying fading channels and noise, we propose a generative signal reconstructor module supported by a local knowledge base, which employs a discriminator for channel error detection and a conditional generative network for error correction. We propose three innovative semantic HARQ (sem-HARQ) schemes, Type I sem-HARQ (sem-HARQ-I), sem-HARQ with weighted combining (sem-HARQ-WC), and sem-HARQ with synonymous combining (sem-HARQ-SC) to enable reliable JSCC-based semantic communications. At the transmitter, both sem-HARQ-I and sem-HARQ-WC retransmit the same semantic signals, while sem-HARQ-SC introduces redundant semantics across different HARQ rounds through synonymous mapping. At the receiver, sem-HARQ-I performs semantic decoding based solely on the currently received signal. In contrast, sem-HARQ-WC enhances reliability by fusing the current received semantic signal with prior erroneous signals at the feature or decision level, thereby exploiting semantic information from failed HARQ rounds. Similarly, sem-HARQ-SC employs feature-level combining, leveraging incremental semantic redundancy to merge semantic features from retransmissions.","sentences":["The surge of data traffic in Intelligent Transportation Systems (ITS) places a significant challenge on limited wireless resources.","Semantic communication, which transmits essential semantics of the raw data, offers a promising solution by reducing redundancy and improving spectrum efficiency.","However, high vehicle mobility, dynamic channel conditions, and dense vehicular networks severely impact transmission reliability in ITS.","To address these limitations, we integrate Hybrid Automatic Repeat reQuest (HARQ) with Joint Source-Channel Coding (JSCC) to provide reliable semantic communications for ITS.","To counteract the adverse effects of time-varying fading channels and noise, we propose a generative signal reconstructor module supported by a local knowledge base, which employs a discriminator for channel error detection and a conditional generative network for error correction.","We propose three innovative semantic HARQ (sem-HARQ) schemes, Type I sem-HARQ (sem-HARQ-I), sem-HARQ with weighted combining (sem-HARQ-WC), and sem-HARQ with synonymous combining (sem-HARQ-SC) to enable reliable JSCC-based semantic communications.","At the transmitter, both sem-HARQ-I and sem-HARQ-WC retransmit the same semantic signals, while sem-HARQ-SC introduces redundant semantics across different HARQ rounds through synonymous mapping.","At the receiver, sem-HARQ-I performs semantic decoding based solely on the currently received signal.","In contrast, sem-HARQ-WC enhances reliability by fusing the current received semantic signal with prior erroneous signals at the feature or decision level, thereby exploiting semantic information from failed HARQ rounds.","Similarly, sem-HARQ-SC employs feature-level combining, leveraging incremental semantic redundancy to merge semantic features from retransmissions."],"url":"http://arxiv.org/abs/2504.14615v1"}
{"created":"2025-04-20 13:31:49","title":"No Imputation of Missing Values In Tabular Data Classification Using Incremental Learning","abstract":"Tabular data sets with varying missing values are prepared for machine learning using an arbitrary imputation strategy. Synthetic values generated by imputation models often concern data stakeholders about computational complexity, data quality, and data-driven outcomes. This paper eliminates these concerns by proposing no imputation incremental learning (NIIL) of tabular data with varying missing value rates and types. The proposed method incrementally learns partitions of overlapping feature sets while using attention masks to exclude missing values from attention scoring. The average classification performance rank order across 15 diverse tabular data sets highlights the superiority of NIIL over 11 state-of-the-art learning methods with or without missing value imputations. Further experiments substantiate the robustness of NIIL against varying missing value types and rates compared to methods that involve the imputation of missing values. Our empirical analysis reveals that a feature partition size of half of the original feature space is, computation-wise and accuracy-wise, the best choice for the proposed incremental learning. The proposed method is one of the first deep learning solutions that can effectively learn tabular data without requiring the imputation of missing values.","sentences":["Tabular data sets with varying missing values are prepared for machine learning using an arbitrary imputation strategy.","Synthetic values generated by imputation models often concern data stakeholders about computational complexity, data quality, and data-driven outcomes.","This paper eliminates these concerns by proposing no imputation incremental learning (NIIL) of tabular data with varying missing value rates and types.","The proposed method incrementally learns partitions of overlapping feature sets while using attention masks to exclude missing values from attention scoring.","The average classification performance rank order across 15 diverse tabular data sets highlights the superiority of NIIL over 11 state-of-the-art learning methods with or without missing value imputations.","Further experiments substantiate the robustness of NIIL against varying missing value types and rates compared to methods that involve the imputation of missing values.","Our empirical analysis reveals that a feature partition size of half of the original feature space is, computation-wise and accuracy-wise, the best choice for the proposed incremental learning.","The proposed method is one of the first deep learning solutions that can effectively learn tabular data without requiring the imputation of missing values."],"url":"http://arxiv.org/abs/2504.14610v1"}
