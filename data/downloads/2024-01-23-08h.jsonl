{"created":"2024-01-22 18:59:29","title":"Exploring Simple Open-Vocabulary Semantic Segmentation","abstract":"Open-vocabulary semantic segmentation models aim to accurately assign a semantic label to each pixel in an image from a set of arbitrary open-vocabulary texts. In order to learn such pixel-level alignment, current approaches typically rely on a combination of (i) image-level VL model (e.g. CLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this paper, we introduce S-Seg, a novel model that can achieve surprisingly strong performance without depending on any of the above elements. S-Seg leverages pseudo-mask and language to train a MaskFormer, and can be easily trained from publicly available image-text datasets. Contrary to prior works, our model directly trains for pixel-level features and language alignment. Once trained, S-Seg generalizes well to multiple testing datasets without requiring fine-tuning. In addition, S-Seg has the extra benefits of scalability with data and consistently improvement when augmented with self-training. We believe that our simple yet effective approach will serve as a solid baseline for future research.","sentences":["Open-vocabulary semantic segmentation models aim to accurately assign a semantic label to each pixel in an image from a set of arbitrary open-vocabulary texts.","In order to learn such pixel-level alignment, current approaches typically rely on a combination of (i) image-level VL model (e.g. CLIP), (ii) ground truth masks, and (iii) custom grouping encoders.","In this paper, we introduce S-Seg, a novel model that can achieve surprisingly strong performance without depending on any of the above elements.","S-Seg leverages pseudo-mask and language to train a MaskFormer, and can be easily trained from publicly available image-text datasets.","Contrary to prior works, our model directly trains for pixel-level features and language alignment.","Once trained, S-Seg generalizes well to multiple testing datasets without requiring fine-tuning.","In addition, S-Seg has the extra benefits of scalability with data and consistently improvement when augmented with self-training.","We believe that our simple yet effective approach will serve as a solid baseline for future research."],"url":"http://arxiv.org/abs/2401.12217v1"}
{"created":"2024-01-22 18:59:07","title":"Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models","abstract":"Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at https://github.com/RL4M/MED-PEFT.","sentences":["Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks.","However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored.","As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models.","Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets.","Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters.","Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14.","We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks.","Code and models are available at https://github.com/RL4M/MED-PEFT."],"url":"http://arxiv.org/abs/2401.12215v1"}
{"created":"2024-01-22 18:51:07","title":"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation","abstract":"Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \\emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \\emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \\url{https://stanford-aimi.github.io/chexagent.html}.","sentences":["Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice.","Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes.","However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation.","In this work, we address these challenges by first introducing \\emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets.","We then present \\emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs.","To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities.","Finally, we introduce \\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks.","Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks.","Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities.","Our project is at \\url{https://stanford-aimi.github.io/chexagent.html}."],"url":"http://arxiv.org/abs/2401.12208v1"}
{"created":"2024-01-22 18:35:02","title":"Programmable EM Sensor Array for Golden-Model Free Run-time Trojan Detection and Localization","abstract":"Side-channel analysis has been proven effective at detecting hardware Trojans in integrated circuits (ICs). However, most detection techniques rely on large external probes and antennas for data collection and require a long measurement time to detect Trojans. Such limitations make these techniques impractical for run-time deployment and ineffective in detecting small Trojans with subtle side-channel signatures. To overcome these challenges, we propose a Programmable Sensor Array (PSA) for run-time hardware Trojan detection, localization, and identification. PSA is a tampering-resilient integrated on-chip magnetic field sensor array that can be re-programmed to change the sensors' shape, size, and location. Using PSA, EM side-channel measurement results collected from sensors at different locations on an IC can be analyzed to localize and identify the Trojan. The PSA has better performance than conventional external magnetic probes and state-of-the-art on-chip single-coil magnetic field sensors. We fabricated an AES-128 test chip with four AES Hardware Trojans. They were successfully detected, located, and identified with the proposed on-chip PSA within 10 milliseconds using our proposed cross-domain analysis.","sentences":["Side-channel analysis has been proven effective at detecting hardware Trojans in integrated circuits (ICs).","However, most detection techniques rely on large external probes and antennas for data collection and require a long measurement time to detect Trojans.","Such limitations make these techniques impractical for run-time deployment and ineffective in detecting small Trojans with subtle side-channel signatures.","To overcome these challenges, we propose a Programmable Sensor Array (PSA) for run-time hardware Trojan detection, localization, and identification.","PSA is a tampering-resilient integrated on-chip magnetic field sensor array that can be re-programmed to change the sensors' shape, size, and location.","Using PSA, EM side-channel measurement results collected from sensors at different locations on an IC can be analyzed to localize and identify the Trojan.","The PSA has better performance than conventional external magnetic probes and state-of-the-art on-chip single-coil magnetic field sensors.","We fabricated an AES-128 test chip with four AES Hardware Trojans.","They were successfully detected, located, and identified with the proposed on-chip PSA within 10 milliseconds using our proposed cross-domain analysis."],"url":"http://arxiv.org/abs/2401.12193v1"}
{"created":"2024-01-22 18:34:42","title":"Text Embedding Inversion Attacks on Multilingual Language Models","abstract":"Representing textual information as real-numbered embeddings has become the norm in NLP. Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model. This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them. However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors. %As many international and multilingual companies leverage EaaS, there is an urgent need for research into multilingual LLM security. To this end, this work investigates LLM security from the perspective of multilingual embedding inversion. Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario. Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts. This stems from the reduced data requirements for achieving comparable inversion performance in settings where the underlying language is not known a-priori. To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security.","sentences":["Representing textual information as real-numbered embeddings has become the norm in NLP.","Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model.","This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them.","However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors.","%As many international and multilingual companies leverage EaaS, there is an urgent need for research into multilingual LLM security.","To this end, this work investigates LLM security from the perspective of multilingual embedding inversion.","Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario.","Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts.","This stems from the reduced data requirements for achieving comparable inversion performance in settings where the underlying language is not known a-priori.","To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security."],"url":"http://arxiv.org/abs/2401.12192v1"}
{"created":"2024-01-22 18:01:01","title":"SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities","abstract":"Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/","sentences":["Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics.","While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences.","We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data.","To this end, we present a system to facilitate this approach.","We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images.","We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture.","Our work features the first internet-scale 3D spatial reasoning dataset in metric space.","By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA.","Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability.","Project website: https://spatial-vlm.github.io/"],"url":"http://arxiv.org/abs/2401.12168v1"}
{"created":"2024-01-22 17:56:07","title":"Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE","abstract":"Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate. Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical. In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed. Initially, over all pixels, an image data matrix is created in high dimensional feature space. Then, t-SNE projects the high dimensional data onto 3D embedding. By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set. The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results.","sentences":["Image segmentation is a clustering task whereby each pixel is assigned a cluster label.","Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available.","This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate.","Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical.","In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed.","Initially, over all pixels, an image data matrix is created in high dimensional feature space.","Then, t-SNE projects the high dimensional data onto 3D embedding.","By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set.","The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm.","The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results."],"url":"http://arxiv.org/abs/2401.12164v1"}
{"created":"2024-01-22 17:36:23","title":"Personalized Over-the-Air Federated Learning with Personalized Reconfigurable Intelligent Surfaces","abstract":"Over-the-air federated learning (OTA-FL) provides bandwidth-efficient learning by leveraging the inherent superposition property of wireless channels. Personalized federated learning balances performance for users with diverse datasets, addressing real-life data heterogeneity. We propose the first personalized OTA-FL scheme through multi-task learning, assisted by personal reconfigurable intelligent surfaces (RIS) for each user. We take a cross-layer approach that optimizes communication and computation resources for global and personalized tasks in time-varying channels with imperfect channel state information, using multi-task learning for non-i.i.d data. Our PROAR-PFed algorithm adaptively designs power, local iterations, and RIS configurations. We present convergence analysis for non-convex objectives and demonstrate that PROAR-PFed outperforms state-of-the-art on the Fashion-MNIST dataset.","sentences":["Over-the-air federated learning (OTA-FL) provides bandwidth-efficient learning by leveraging the inherent superposition property of wireless channels.","Personalized federated learning balances performance for users with diverse datasets, addressing real-life data heterogeneity.","We propose the first personalized OTA-FL scheme through multi-task learning, assisted by personal reconfigurable intelligent surfaces (RIS) for each user.","We take a cross-layer approach that optimizes communication and computation resources for global and personalized tasks in time-varying channels with imperfect channel state information, using multi-task learning for non-i.i.d data.","Our PROAR-PFed algorithm adaptively designs power, local iterations, and RIS configurations.","We present convergence analysis for non-convex objectives and demonstrate that PROAR-PFed outperforms state-of-the-art on the Fashion-MNIST dataset."],"url":"http://arxiv.org/abs/2401.12149v1"}
{"created":"2024-01-22 17:15:02","title":"VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games","abstract":"Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.","sentences":["Understanding and recognizing emotions are important and challenging issues in the metaverse era.","Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications.","In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players.","We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively.","We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets.","The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope.","We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments.","Moreover, we discussed the implications of this work for communities and applications.","The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD."],"url":"http://arxiv.org/abs/2401.12133v1"}
{"created":"2024-01-22 17:14:47","title":"Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis Using Sequential Multisequence MRI","abstract":"Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures. Our hypothesis is that quantum models will provide competitive performance. Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS. These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT). Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss. Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20. Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631). Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001). Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time. Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging.","sentences":["Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS).","In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures.","Our hypothesis is that quantum models will provide competitive performance.","Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS.","These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT).","Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss.","Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20.","Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean.","Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915).","VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631).","Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001).","Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time.","Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging."],"url":"http://arxiv.org/abs/2401.12132v1"}
{"created":"2024-01-22 16:51:01","title":"Extracting Formulae in Many-Valued Logic from Deep Neural Networks","abstract":"We propose a new perspective on deep ReLU networks, namely as circuit counterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV) generalization of Boolean logic. An algorithm for extracting formulae in MV logic from deep ReLU networks is presented. As the algorithm applies to networks with general, in particular also real-valued, weights, it can be used to extract logical formulae from deep ReLU networks trained on data.","sentences":["We propose a new perspective on deep ReLU networks, namely as circuit counterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV) generalization of Boolean logic.","An algorithm for extracting formulae in MV logic from deep ReLU networks is presented.","As the algorithm applies to networks with general, in particular also real-valued, weights, it can be used to extract logical formulae from deep ReLU networks trained on data."],"url":"http://arxiv.org/abs/2401.12113v1"}
{"created":"2024-01-22 16:45:15","title":"On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using Streaming Data","abstract":"In parcel delivery, the \"last mile\" from the parcel hub to the customer is costly, especially for time-sensitive delivery tasks that have to be completed within hours after arrival. Recently, crowdshipping has attracted increased attention as a new alternative to traditional delivery modes. In crowdshipping, private citizens (\"the crowd\") perform short detours in their daily lives to contribute to parcel delivery in exchange for small incentives. However, achieving desirable crowd behavior is challenging as the crowd is highly dynamic and consists of autonomous, self-interested individuals. Leveraging crowdshipping for time-sensitive deliveries remains an open challenge. In this paper, we present an agent-based approach to on-time parcel delivery with crowds. Our system performs data stream processing on the couriers' smartphone sensor data to predict delivery delays. Whenever a delay is predicted, the system attempts to forge an agreement for transferring the parcel from the current deliverer to a more promising courier nearby. Our experiments show that through accurate delay predictions and purposeful task transfers many delays can be prevented that would occur without our approach.","sentences":["In parcel delivery, the \"last mile\" from the parcel hub to the customer is costly, especially for time-sensitive delivery tasks that have to be completed within hours after arrival.","Recently, crowdshipping has attracted increased attention as a new alternative to traditional delivery modes.","In crowdshipping, private citizens (\"the crowd\") perform short detours in their daily lives to contribute to parcel delivery in exchange for small incentives.","However, achieving desirable crowd behavior is challenging as the crowd is highly dynamic and consists of autonomous, self-interested individuals.","Leveraging crowdshipping for time-sensitive deliveries remains an open challenge.","In this paper, we present an agent-based approach to on-time parcel delivery with crowds.","Our system performs data stream processing on the couriers' smartphone sensor data to predict delivery delays.","Whenever a delay is predicted, the system attempts to forge an agreement for transferring the parcel from the current deliverer to a more promising courier nearby.","Our experiments show that through accurate delay predictions and purposeful task transfers many delays can be prevented that would occur without our approach."],"url":"http://arxiv.org/abs/2401.12108v1"}
{"created":"2024-01-22 16:38:33","title":"LearnedWMP: Workload Memory Prediction Using Distribution of Query Templates","abstract":"In a modern DBMS, working memory is frequently the limiting factor when processing in-memory analytic query operations such as joins, sorting, and aggregation. Existing resource estimation approaches for a DBMS estimate the resource consumption of a query by computing an estimate of each individual database operator in the query execution plan. Such an approach is slow and error-prone as it relies upon simplifying assumptions, such as uniformity and independence of the underlying data. Additionally, the existing approach focuses on individual queries separately and does not factor in other queries in the workload that may be executed concurrently. In this research, we are interested in query performance optimization under concurrent execution of a batch of queries (a workload). Specifically, we focus on predicting the memory demand for a workload rather than providing separate estimates for each query within it. We introduce the problem of workload memory prediction and formalize it as a distribution regression problem. We propose Learned Workload Memory Prediction (LearnedWMP) to improve and simplify estimating the working memory demands of workloads. Through a comprehensive experimental evaluation, we show that LearnedWMP reduces the memory estimation error of the state-of-the-practice method by up to 47.6%. Compared to an alternative single-query model, during training and inferencing, the LearnedWMP model and its variants were 3x to 10x faster. Moreover, LearnedWMP-based models were at least 50% smaller in most cases. Overall, the results demonstrate the advantages of the LearnedWMP approach and its potential for a broader impact on query performance optimization.","sentences":["In a modern DBMS, working memory is frequently the limiting factor when processing in-memory analytic query operations such as joins, sorting, and aggregation.","Existing resource estimation approaches for a DBMS estimate the resource consumption of a query by computing an estimate of each individual database operator in the query execution plan.","Such an approach is slow and error-prone as it relies upon simplifying assumptions, such as uniformity and independence of the underlying data.","Additionally, the existing approach focuses on individual queries separately and does not factor in other queries in the workload that may be executed concurrently.","In this research, we are interested in query performance optimization under concurrent execution of a batch of queries (a workload).","Specifically, we focus on predicting the memory demand for a workload rather than providing separate estimates for each query within it.","We introduce the problem of workload memory prediction and formalize it as a distribution regression problem.","We propose Learned Workload Memory Prediction (LearnedWMP) to improve and simplify estimating the working memory demands of workloads.","Through a comprehensive experimental evaluation, we show that LearnedWMP reduces the memory estimation error of the state-of-the-practice method by up to 47.6%.","Compared to an alternative single-query model, during training and inferencing, the LearnedWMP model and its variants were 3x to 10x faster.","Moreover, LearnedWMP-based models were at least 50% smaller in most cases.","Overall, the results demonstrate the advantages of the LearnedWMP approach and its potential for a broader impact on query performance optimization."],"url":"http://arxiv.org/abs/2401.12103v1"}
{"created":"2024-01-22 16:25:27","title":"Revisiting Demonstration Selection Strategies in In-Context Learning","abstract":"Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and it is still unclear why this happens or what factors will influence its choice. In this work, we first revisit the factors contributing to this variance from both data and model aspects, and find that the choice of demonstration is both data- and model-dependent. We further proposed a data- and model-dependent demonstration selection method, \\textbf{TopK + ConE}, based on the assumption that \\textit{the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, resulting in a simple and effective recipe for ICL. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code will be released.","sentences":["Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model.","However, the performance of ICL varies significantly with the choice of demonstrations, and it is still unclear why this happens or what factors will influence its choice.","In this work, we first revisit the factors contributing to this variance from both data and model aspects, and find that the choice of demonstration is both data- and model-dependent.","We further proposed a data- and model-dependent demonstration selection method, \\textbf{TopK + ConE}, based on the assumption that \\textit{the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, resulting in a simple and effective recipe for ICL.","Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales.","Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods.","Code will be released."],"url":"http://arxiv.org/abs/2401.12087v1"}
{"created":"2024-01-22 16:24:43","title":"West-of-N: Synthetic Preference Generation for Improved Reward Modeling","abstract":"The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model. In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs. Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training. This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query. Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data. This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges.","sentences":["The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model.","In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs.","Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training.","This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query.","Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data.","This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges."],"url":"http://arxiv.org/abs/2401.12086v1"}
{"created":"2024-01-22 16:20:14","title":"Temporal Blind Spots in Large Language Models","abstract":"Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks. These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance. However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations. Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents. In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding. We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets. Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information. In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates. Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks. The code is available\\footnote{https://github.com/jwallat/temporalblindspots}.","sentences":["Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks.","These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance.","However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations.","Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents.","In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding.","We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets.","Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information.","In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates.","Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks.","The code is available\\footnote{https://github.com/jwallat/temporalblindspots}."],"url":"http://arxiv.org/abs/2401.12078v1"}
{"created":"2024-01-22 16:14:27","title":"NLP-based Relation Extraction Methods in RE","abstract":"Mobile app repositories have been largely used in scientific research as large-scale, highly adaptive crowdsourced information systems. These software platforms can potentially nourish multiple software and requirements engineering tasks based on user reviews and other natural language documents, including feedback analysis, recommender systems and topic modelling. Consequently, researchers often endeavour to overcome domain-specific challenges, including integration of heterogeneous data sources, large-scale data collection and adaptation of a publicly available data set for a given research scenario. In this paper, we present MApp-KG, a combination of software resources and data artefacts in the field of mobile app repositories to support extended knowledge generation tasks. Our contribution aims to provide a framework for automatically constructing a knowledge graph modelling a domain-specific catalogue of mobile apps. Complementarily, we distribute MApp-KG in a public triplestore and as a static data snapshot, which may be promptly employed for future research and reproduction of our findings.","sentences":["Mobile app repositories have been largely used in scientific research as large-scale, highly adaptive crowdsourced information systems.","These software platforms can potentially nourish multiple software and requirements engineering tasks based on user reviews and other natural language documents, including feedback analysis, recommender systems and topic modelling.","Consequently, researchers often endeavour to overcome domain-specific challenges, including integration of heterogeneous data sources, large-scale data collection and adaptation of a publicly available data set for a given research scenario.","In this paper, we present MApp-KG, a combination of software resources and data artefacts in the field of mobile app repositories to support extended knowledge generation tasks.","Our contribution aims to provide a framework for automatically constructing a knowledge graph modelling a domain-specific catalogue of mobile apps.","Complementarily, we distribute MApp-KG in a public triplestore and as a static data snapshot, which may be promptly employed for future research and reproduction of our findings."],"url":"http://arxiv.org/abs/2401.12075v1"}
{"created":"2024-01-22 16:13:45","title":"Cross-lingual Transfer Learning for Javanese Dependency Parsing","abstract":"While structure learning achieves remarkable performance in high-resource languages, the situation differs for under-represented languages due to the scarcity of annotated data. This study focuses on assessing the efficacy of transfer learning in enhancing dependency parsing for Javanese, a language spoken by 80 million individuals but characterized by limited representation in natural language processing. We utilized the Universal Dependencies dataset consisting of dependency treebanks from more than 100 languages, including Javanese. We propose two learning strategies to train the model: transfer learning (TL) and hierarchical transfer learning (HTL). While TL only uses a source language to pre-train the model, the HTL method uses a source language and an intermediate language in the learning process. The results show that our best model uses the HTL method, which improves performance with an increase of 10% for both UAS and LAS evaluations compared to the baseline model.","sentences":["While structure learning achieves remarkable performance in high-resource languages, the situation differs for under-represented languages due to the scarcity of annotated data.","This study focuses on assessing the efficacy of transfer learning in enhancing dependency parsing for Javanese, a language spoken by 80 million individuals but characterized by limited representation in natural language processing.","We utilized the Universal Dependencies dataset consisting of dependency treebanks from more than 100 languages, including Javanese.","We propose two learning strategies to train the model: transfer learning (TL) and hierarchical transfer learning (HTL).","While TL only uses a source language to pre-train the model, the HTL method uses a source language and an intermediate language in the learning process.","The results show that our best model uses the HTL method, which improves performance with an increase of 10% for both UAS and LAS evaluations compared to the baseline model."],"url":"http://arxiv.org/abs/2401.12072v1"}
{"created":"2024-01-22 16:11:11","title":"An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators","abstract":"Memory bandwidth is known to be a performance bottleneck for FPGA accelerators, especially when they deal with large multi-dimensional data-sets. A large body of work focuses on reducing of off-chip transfers, but few authors try to improve the efficiency of transfers. This paper addresses the later issue by proposing (i) a compiler-based approach to accelerator's data layout to maximize contiguous access to off-chip memory, and (ii) data packing and runtime compression techniques that take advantage of this layout to further improve memory performance. We show that our approach can decrease the I/O cycles up to $7\\times$ compared to un-optimized memory accesses.","sentences":["Memory bandwidth is known to be a performance bottleneck for FPGA accelerators, especially when they deal with large multi-dimensional data-sets.","A large body of work focuses on reducing of off-chip transfers, but few authors try to improve the efficiency of transfers.","This paper addresses the later issue by proposing (i) a compiler-based approach to accelerator's data layout to maximize contiguous access to off-chip memory, and (ii) data packing and runtime compression techniques that take advantage of this layout to further improve memory performance.","We show that our approach can decrease the I/O cycles up to $7\\times$ compared to un-optimized memory accesses."],"url":"http://arxiv.org/abs/2401.12071v1"}
{"created":"2024-01-22 16:09:47","title":"Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text","abstract":"Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.","sentences":["Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors.","However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text.","Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs.","The method, called Binoculars, achieves state-of-the-art accuracy without any training data.","It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications.","We comprehensively evaluate Binoculars on a number of text sources and in varied situations.","Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data."],"url":"http://arxiv.org/abs/2401.12070v1"}
{"created":"2024-01-22 16:08:41","title":"Beyond TreeSHAP: Efficient Computation of Any-Order Shapley Interactions for Tree Ensembles","abstract":"While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models. As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions. The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models. Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order. In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models. TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP. We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets.","sentences":["While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models.","As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions.","The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models.","Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order.","In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models.","TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP.","We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets."],"url":"http://arxiv.org/abs/2401.12069v1"}
{"created":"2024-01-22 15:53:52","title":"SEDAC: A CVAE-Based Data Augmentation Method for Security Bug Report Identification","abstract":"Bug tracking systems store many bug reports, some of which are related to security. Identifying those security bug reports (SBRs) may help us predict some security-related bugs and solve security issues promptly so that the project can avoid threats and attacks. However, in the real world, the ratio of security bug reports is severely low; thus, directly training a prediction model with raw data may result in inaccurate results. Faced with the massive challenge of data imbalance, many researchers in the past have attempted to use text filtering or clustering methods to minimize the proportion of non-security bug reports (NSBRs) or apply oversampling methods to synthesize SBRs to make the dataset as balanced as possible. Nevertheless, there are still two challenges to those methods: 1) They ignore long-distance contextual information. 2) They fail to generate an utterly balanced dataset. To tackle these two challenges, we propose SEDAC, a new SBR identification method that generates similar bug report vectors to solve data imbalance problems and accurately detect security bug reports. Unlike previous studies, it first converts bug reports into individual bug report vectors with distilBERT, which are based on word2vec. Then, it trains a generative model through conditional variational auto-encoder (CVAE) to generate similar vectors with security labels, which makes the number of SBRs equal to NSBRs'. Finally, balanced data are used to train a security bug report classifier. To evaluate the effectiveness of our framework, we conduct it on 45,940 bug reports from Chromium and four Apache projects. The experimental results show that SEDAC outperforms all the baselines in g-measure with improvements of around 14.24%-50.10%.","sentences":["Bug tracking systems store many bug reports, some of which are related to security.","Identifying those security bug reports (SBRs) may help us predict some security-related bugs and solve security issues promptly so that the project can avoid threats and attacks.","However, in the real world, the ratio of security bug reports is severely low; thus, directly training a prediction model with raw data may result in inaccurate results.","Faced with the massive challenge of data imbalance, many researchers in the past have attempted to use text filtering or clustering methods to minimize the proportion of non-security bug reports (NSBRs) or apply oversampling methods to synthesize SBRs to make the dataset as balanced as possible.","Nevertheless, there are still two challenges to those methods: 1) They ignore long-distance contextual information.","2) They fail to generate an utterly balanced dataset.","To tackle these two challenges, we propose SEDAC, a new SBR identification method that generates similar bug report vectors to solve data imbalance problems and accurately detect security bug reports.","Unlike previous studies, it first converts bug reports into individual bug report vectors with distilBERT, which are based on word2vec.","Then, it trains a generative model through conditional variational auto-encoder (CVAE) to generate similar vectors with security labels, which makes the number of SBRs equal to NSBRs'.","Finally, balanced data are used to train a security bug report classifier.","To evaluate the effectiveness of our framework, we conduct it on 45,940 bug reports from Chromium and four Apache projects.","The experimental results show that SEDAC outperforms all the baselines in g-measure with improvements of around 14.24%-50.10%."],"url":"http://arxiv.org/abs/2401.12060v1"}
{"created":"2024-01-22 15:42:21","title":"CloSe: A 3D Clothing Segmentation Dataset and Model","abstract":"3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization. To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work. The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data. We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels. Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data. Dataset, model, and tool can be found at https://virtualhumans.mpi-inf.mpg.de/close3dv24/.","sentences":["3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries.","Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization.","To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes.","Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds.","CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work.","The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data.","We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing.","We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels.","Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data.","Dataset, model, and tool can be found at https://virtualhumans.mpi-inf.mpg.de/close3dv24/."],"url":"http://arxiv.org/abs/2401.12051v1"}
{"created":"2024-01-22 15:17:54","title":"MINT: A wrapper to make multi-modal and multi-image AI models interactive","abstract":"During the diagnostic process, doctors incorporate multimodal information including imaging and the medical history - and similarly medical AI development has increasingly become multimodal. In this paper we tackle a more subtle challenge: doctors take a targeted medical history to obtain only the most pertinent pieces of information; how do we enable AI to do the same? We develop a wrapper method named MINT (Make your model INTeractive) that automatically determines what pieces of information are most valuable at each step, and ask for only the most useful information. We demonstrate the efficacy of MINT wrapping a skin disease prediction model, where multiple images and a set of optional answers to $25$ standard metadata questions (i.e., structured medical history) are used by a multi-modal deep network to provide a differential diagnosis. We show that MINT can identify whether metadata inputs are needed and if so, which question to ask next. We also demonstrate that when collecting multiple images, MINT can identify if an additional image would be beneficial, and if so, which type of image to capture. We showed that MINT reduces the number of metadata and image inputs needed by 82% and 36.2% respectively, while maintaining predictive performance. Using real-world AI dermatology system data, we show that needing fewer inputs can retain users that may otherwise fail to complete the system submission and drop off without a diagnosis. Qualitative examples show MINT can closely mimic the step-by-step decision making process of a clinical workflow and how this is different for straight forward cases versus more difficult, ambiguous cases. Finally we demonstrate how MINT is robust to different underlying multi-model classifiers and can be easily adapted to user requirements without significant model re-training.","sentences":["During the diagnostic process, doctors incorporate multimodal information including imaging and the medical history - and similarly medical AI development has increasingly become multimodal.","In this paper we tackle a more subtle challenge: doctors take a targeted medical history to obtain only the most pertinent pieces of information; how do we enable AI to do the same?","We develop a wrapper method named MINT (Make your model INTeractive) that automatically determines what pieces of information are most valuable at each step, and ask for only the most useful information.","We demonstrate the efficacy of MINT wrapping a skin disease prediction model, where multiple images and a set of optional answers to $25$ standard metadata questions (i.e., structured medical history) are used by a multi-modal deep network to provide a differential diagnosis.","We show that MINT can identify whether metadata inputs are needed and if so, which question to ask next.","We also demonstrate that when collecting multiple images, MINT can identify if an additional image would be beneficial, and if so, which type of image to capture.","We showed that MINT reduces the number of metadata and image inputs needed by 82% and 36.2% respectively, while maintaining predictive performance.","Using real-world AI dermatology system data, we show that needing fewer inputs can retain users that may otherwise fail to complete the system submission and drop off without a diagnosis.","Qualitative examples show MINT can closely mimic the step-by-step decision making process of a clinical workflow and how this is different for straight forward cases versus more difficult, ambiguous cases.","Finally we demonstrate how MINT is robust to different underlying multi-model classifiers and can be easily adapted to user requirements without significant model re-training."],"url":"http://arxiv.org/abs/2401.12032v1"}
{"created":"2024-01-22 15:11:57","title":"Multimodal Visual-Tactile Representation Learning through Self-Supervised Contrastive Pre-Training","abstract":"The rapidly evolving field of robotics necessitates methods that can facilitate the fusion of multiple modalities. Specifically, when it comes to interacting with tangible objects, effectively combining visual and tactile sensory data is key to understanding and navigating the complex dynamics of the physical world, enabling a more nuanced and adaptable response to changing environments. Nevertheless, much of the earlier work in merging these two sensory modalities has relied on supervised methods utilizing datasets labeled by humans.This paper introduces MViTac, a novel methodology that leverages contrastive learning to integrate vision and touch sensations in a self-supervised fashion. By availing both sensory inputs, MViTac leverages intra and inter-modality losses for learning representations, resulting in enhanced material property classification and more adept grasping prediction. Through a series of experiments, we showcase the effectiveness of our method and its superiority over existing state-of-the-art self-supervised and supervised techniques. In evaluating our methodology, we focus on two distinct tasks: material classification and grasping success prediction. Our results indicate that MViTac facilitates the development of improved modality encoders, yielding more robust representations as evidenced by linear probing assessments.","sentences":["The rapidly evolving field of robotics necessitates methods that can facilitate the fusion of multiple modalities.","Specifically, when it comes to interacting with tangible objects, effectively combining visual and tactile sensory data is key to understanding and navigating the complex dynamics of the physical world, enabling a more nuanced and adaptable response to changing environments.","Nevertheless, much of the earlier work in merging these two sensory modalities has relied on supervised methods utilizing datasets labeled by humans.","This paper introduces MViTac, a novel methodology that leverages contrastive learning to integrate vision and touch sensations in a self-supervised fashion.","By availing both sensory inputs, MViTac leverages intra and inter-modality losses for learning representations, resulting in enhanced material property classification and more adept grasping prediction.","Through a series of experiments, we showcase the effectiveness of our method and its superiority over existing state-of-the-art self-supervised and supervised techniques.","In evaluating our methodology, we focus on two distinct tasks: material classification and grasping success prediction.","Our results indicate that MViTac facilitates the development of improved modality encoders, yielding more robust representations as evidenced by linear probing assessments."],"url":"http://arxiv.org/abs/2401.12024v1"}
{"created":"2024-01-22 15:04:46","title":"PairwiseHist: Fast, Accurate and Space-Efficient Approximate Query Processing with Data Compression","abstract":"Exponential growth in data collection is creating significant challenges for data storage and analytics latency.Approximate Query Processing (AQP) has long been touted as a solution for accelerating analytics on large datasets, however, there is still room for improvement across all key performance criteria. In this paper, we propose a novel histogram-based data synopsis called PairwiseHist that uses recursive hypothesis testing to ensure accurate histograms and can be built on top of data compressed using Generalized Deduplication (GD). We thus show that GD data compression can contribute to AQP. Compared to state-of-the-art AQP approaches, PairwiseHist achieves better performance across all key metrics, including 2.6$ \\times $ higher accuracy, 3.5$ \\times $ lower latency, 24$ \\times $ smaller synopses and 1.5--4$ \\times $ faster construction time.","sentences":["Exponential growth in data collection is creating significant challenges for data storage and analytics latency.","Approximate Query Processing (AQP) has long been touted as a solution for accelerating analytics on large datasets, however, there is still room for improvement across all key performance criteria.","In this paper, we propose a novel histogram-based data synopsis called PairwiseHist that uses recursive hypothesis testing to ensure accurate histograms and can be built on top of data compressed using Generalized Deduplication (GD).","We thus show that GD data compression can contribute to AQP.","Compared to state-of-the-art AQP approaches, PairwiseHist achieves better performance across all key metrics, including 2.6$ \\times $ higher accuracy, 3.5$ \\times $ lower latency, 24$ \\times $ smaller synopses and 1.5--4$ \\times $ faster construction time."],"url":"http://arxiv.org/abs/2401.12018v1"}
{"created":"2024-01-22 15:00:32","title":"Robustness to distribution shifts of compressed networks for edge devices","abstract":"It is necessary to develop efficient DNNs deployed on edge devices with limited computation resources. However, the compressed networks often execute new tasks in the target domain, which is different from the source domain where the original network is trained. It is important to investigate the robustness of compressed networks in two types of data distribution shifts: domain shifts and adversarial perturbations. In this study, we discover that compressed models are less robust to distribution shifts than their original networks. Interestingly, larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size as the smaller networks. Furthermore, compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks. Finally, post-training quantization is a reliable method for achieving significant robustness to distribution shifts, and it outperforms both pruned and distilled models in terms of robustness.","sentences":["It is necessary to develop efficient DNNs deployed on edge devices with limited computation resources.","However, the compressed networks often execute new tasks in the target domain, which is different from the source domain where the original network is trained.","It is important to investigate the robustness of compressed networks in two types of data distribution shifts: domain shifts and adversarial perturbations.","In this study, we discover that compressed models are less robust to distribution shifts than their original networks.","Interestingly, larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size as the smaller networks.","Furthermore, compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks.","Finally, post-training quantization is a reliable method for achieving significant robustness to distribution shifts, and it outperforms both pruned and distilled models in terms of robustness."],"url":"http://arxiv.org/abs/2401.12014v1"}
{"created":"2024-01-22 14:59:11","title":"TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients","abstract":"Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are \"lazy\" and train their models solely for few epochs for next global aggregation. TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings. We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution. Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC.","sentences":["Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years.","In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data.","Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity.","The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical.","In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are \"lazy\" and train their models solely for few epochs for next global aggregation.","TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings.","We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution.","Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC."],"url":"http://arxiv.org/abs/2401.12012v1"}
{"created":"2024-01-22 14:58:54","title":"Architecting Data-Intensive Applications : From Data Architecture Design to Its Quality Assurance","abstract":"Context - The exponential growth of data is becoming a significant concern. Managing this data has become incredibly challenging, especially when dealing with various sources in different formats and speeds. Moreover, Ensuring data quality has become increasingly crucial for effective decision-making and operational processes. Data Architecture is crucial in describing, collecting, storing, processing, and analyzing data to meet business needs. Providing an abstract view of data-intensive applications is essential to ensure that the data is transformed into valuable information. We must take these challenges seriously to ensure we can effectively manage and use the data to our advantage. Objective - To establish an architecture framework that enables a comprehensive description of the data architecture and effectively streamlines data quality monitoring. Method - The architecture framework utilizes Model Driven Engineering (MDE) techniques. Its backing of data-intensive architecture descriptions empowers with an automated generation for data quality checks. Result - The Framework offers a comprehensive solution for data-intensive applications to model their architecture efficiently and monitor the quality of their data. It automates the entire process and ensures precision and consistency in data. With DAT, architects and analysts gain access to a powerful tool that simplifies their workflow and empowers them to make informed decisions based on reliable data insights. Conclusion - We have evaluated the DAT on more than five cases within various industry domains, demonstrating its exceptional adaptability and effectiveness.","sentences":["Context - The exponential growth of data is becoming a significant concern.","Managing this data has become incredibly challenging, especially when dealing with various sources in different formats and speeds.","Moreover, Ensuring data quality has become increasingly crucial for effective decision-making and operational processes.","Data Architecture is crucial in describing, collecting, storing, processing, and analyzing data to meet business needs.","Providing an abstract view of data-intensive applications is essential to ensure that the data is transformed into valuable information.","We must take these challenges seriously to ensure we can effectively manage and use the data to our advantage.","Objective - To establish an architecture framework that enables a comprehensive description of the data architecture and effectively streamlines data quality monitoring.","Method -","The architecture framework utilizes Model Driven Engineering (MDE) techniques.","Its backing of data-intensive architecture descriptions empowers with an automated generation for data quality checks.","Result -","The Framework offers a comprehensive solution for data-intensive applications to model their architecture efficiently and monitor the quality of their data.","It automates the entire process and ensures precision and consistency in data.","With DAT, architects and analysts gain access to a powerful tool that simplifies their workflow and empowers them to make informed decisions based on reliable data insights.","Conclusion - We have evaluated the DAT on more than five cases within various industry domains, demonstrating its exceptional adaptability and effectiveness."],"url":"http://arxiv.org/abs/2401.12011v1"}
{"created":"2024-01-22 14:55:01","title":"Tensor-view Topological Graph Neural Network","abstract":"Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation learning modules that disentangle feature tensor aggregation and transformation and learn to preserve multi-modal structure with less computation. Theoretically, we derive high probability bounds on both the out-of-sample and in-sample mean squared approximation errors for our proposed Tensor Transformation Layer (TTL). Real data experiments show that the proposed TTG-NN outperforms 20 state-of-the-art methods on various graph benchmarks.","sentences":["Graph classification is an important learning task for graph-structured data.","Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems.","Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation.","To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations.","This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels.","Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation learning modules that disentangle feature tensor aggregation and transformation and learn to preserve multi-modal structure with less computation.","Theoretically, we derive high probability bounds on both the out-of-sample and in-sample mean squared approximation errors for our proposed Tensor Transformation Layer (TTL).","Real data experiments show that the proposed TTG-NN outperforms 20 state-of-the-art methods on various graph benchmarks."],"url":"http://arxiv.org/abs/2401.12007v1"}
{"created":"2024-01-22 14:52:34","title":"HgbNet: predicting hemoglobin level/anemia degree from EHR data","abstract":"Anemia is a prevalent medical condition that typically requires invasive blood tests for diagnosis and monitoring. Electronic health records (EHRs) have emerged as valuable data sources for numerous medical studies. EHR-based hemoglobin level/anemia degree prediction is non-invasive and rapid but still faces some challenges due to the fact that EHR data is typically an irregular multivariate time series containing a significant number of missing values and irregular time intervals. To address these issues, we introduce HgbNet, a machine learning-based prediction model that emulates clinicians' decision-making processes for hemoglobin level/anemia degree prediction. The model incorporates a NanDense layer with a missing indicator to handle missing values and employs attention mechanisms to account for both local irregularity and global irregularity. We evaluate the proposed method using two real-world datasets across two use cases. In our first use case, we predict hemoglobin level/anemia degree at moment T+1 by utilizing records from moments prior to T+1. In our second use case, we integrate all historical records with additional selected test results at moment T+1 to predict hemoglobin level/anemia degree at the same moment, T+1. HgbNet outperforms the best baseline results across all datasets and use cases. These findings demonstrate the feasibility of estimating hemoglobin levels and anemia degree from EHR data, positioning HgbNet as an effective non-invasive anemia diagnosis solution that could potentially enhance the quality of life for millions of affected individuals worldwide. To our knowledge, HgbNet is the first machine learning model leveraging EHR data for hemoglobin level/anemia degree prediction.","sentences":["Anemia is a prevalent medical condition that typically requires invasive blood tests for diagnosis and monitoring.","Electronic health records (EHRs) have emerged as valuable data sources for numerous medical studies.","EHR-based hemoglobin level/anemia degree prediction is non-invasive and rapid but still faces some challenges due to the fact that EHR data is typically an irregular multivariate time series containing a significant number of missing values and irregular time intervals.","To address these issues, we introduce HgbNet, a machine learning-based prediction model that emulates clinicians' decision-making processes for hemoglobin level/anemia degree prediction.","The model incorporates a NanDense layer with a missing indicator to handle missing values and employs attention mechanisms to account for both local irregularity and global irregularity.","We evaluate the proposed method using two real-world datasets across two use cases.","In our first use case, we predict hemoglobin level/anemia degree at moment T+1","by utilizing records from moments prior to T+1.","In our second use case, we integrate all historical records with additional selected test results at moment T+1 to predict hemoglobin level/anemia degree at the same moment, T+1.","HgbNet outperforms the best baseline results across all datasets and use cases.","These findings demonstrate the feasibility of estimating hemoglobin levels and anemia degree from EHR data, positioning HgbNet as an effective non-invasive anemia diagnosis solution that could potentially enhance the quality of life for millions of affected individuals worldwide.","To our knowledge, HgbNet is the first machine learning model leveraging EHR data for hemoglobin level/anemia degree prediction."],"url":"http://arxiv.org/abs/2401.12002v1"}
{"created":"2024-01-22 14:51:01","title":"Integrating Statistical Significance and Discriminative Power in Pattern Discovery","abstract":"Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains. Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power. Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality. We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria. To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\\delta$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL). Results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search. Although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, N-way (N>3), transactional, and sequential data structures.   Availability: The code is freely available at https://github.com/JupitersMight/MOF_Triclustering under the MIT license.","sentences":["Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains.","Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power.","Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality.","We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria.","To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\\delta$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL).","Results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search.","Although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, N-way (N>3), transactional, and sequential data structures.   ","Availability: The code is freely available at https://github.com/JupitersMight/MOF_Triclustering under the MIT license."],"url":"http://arxiv.org/abs/2401.12000v1"}
{"created":"2024-01-22 14:44:07","title":"Tight Bounds on the Message Complexity of Distributed Tree Verification","abstract":"We consider the message complexity of verifying whether a given subgraph of the communication network forms a tree with specific properties both in the KT-$\\rho$ (nodes know their $\\rho$-hop neighborhood, including node IDs) and the KT-$0$ (nodes do not have this knowledge) models. We develop a rather general framework that helps in establishing tight lower bounds for various tree verification problems. We also consider two different verification requirements: namely that every node detects in the case the input is incorrect, as well as the requirement that at least one node detects. The results are stronger than previous ones in the sense that we assume that each node knows the number $n$ of nodes in the graph (in some cases) or an $\\alpha$ approximation of $n$ (in other cases). For spanning tree verification, we show that the message complexity inherently depends on the quality of the given approximation of $n$: We show a tight lower bound of $\\Omega(n^2)$ for the case $\\alpha \\ge \\sqrt{2}$ and a much better upper bound (i.e., $O(n \\log n)$) when nodes are given a tighter approximation. On the other hand, our framework also yields an $\\Omega(n^2)$ lower bound on the message complexity of verifying a minimum spanning tree (MST), which reveals a polynomial separation between ST verification and MST verification. This result holds for randomized algorithms with perfect knowledge of the network size, and even when just one node detects illegal inputs, thus improving over the work of Kor, Korman, and Peleg (2013). For verifying a $d$-approximate BFS tree, we show that the same lower bound holds even if nodes know $n$ exactly, however, the lower bound is sensitive to $d$, which is the stretch parameter.","sentences":["We consider the message complexity of verifying whether a given subgraph of the communication network forms a tree with specific properties both in the KT-$\\rho$ (nodes know their $\\rho$-hop neighborhood, including node IDs) and the KT-$0$ (nodes do not have this knowledge) models.","We develop a rather general framework that helps in establishing tight lower bounds for various tree verification problems.","We also consider two different verification requirements: namely that every node detects in the case the input is incorrect, as well as the requirement that at least one node detects.","The results are stronger than previous ones in the sense that we assume that each node knows the number $n$ of nodes in the graph (in some cases) or an $\\alpha$ approximation of $n$ (in other cases).","For spanning tree verification, we show that the message complexity inherently depends on the quality of the given approximation of $n$: We show a tight lower bound of $\\Omega(n^2)$ for the case $\\alpha \\ge \\sqrt{2}$ and a much better upper bound (i.e., $O(n \\log n)$) when nodes are given a tighter approximation.","On the other hand, our framework also yields an $\\Omega(n^2)$ lower bound on the message complexity of verifying a minimum spanning tree (MST), which reveals a polynomial separation between ST verification and MST verification.","This result holds for randomized algorithms with perfect knowledge of the network size, and even when just one node detects illegal inputs, thus improving over the work of Kor, Korman, and Peleg (2013).","For verifying a $d$-approximate BFS tree, we show that the same lower bound holds even if nodes know $n$ exactly, however, the lower bound is sensitive to $d$, which is the stretch parameter."],"url":"http://arxiv.org/abs/2401.11991v1"}
{"created":"2024-01-22 14:26:02","title":"Cross-Validation Conformal Risk Control","abstract":"Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. The original CRC requires the available data set to be split between training and validation data sets. This can be problematic when data availability is limited, resulting in inefficient set predictors. In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC. The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor. Furthermore, numerical experiments show that CV-CRC can reduce the average set size with respect to CRC when the available data are limited.","sentences":["Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees.","Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate.","The original CRC requires the available data set to be split between training and validation data sets.","This can be problematic when data availability is limited, resulting in inefficient set predictors.","In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC.","The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions.","CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor.","Furthermore, numerical experiments show that CV-CRC can reduce the average set size with respect to CRC when the available data are limited."],"url":"http://arxiv.org/abs/2401.11974v1"}
{"created":"2024-01-22 14:24:03","title":"Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing","abstract":"The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning. Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges, offering a roadmap for future directions.","sentences":["The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP).","While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks.","Meanwhile, the symbolic methods excel in representing knowledge-rich data.","However, they struggle to adapt dynamic data and generalize the knowledge.","Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths.","Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks.","In this paper, we present an overview of hybrid approaches used for NLP.","Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning.","Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges, offering a roadmap for future directions."],"url":"http://arxiv.org/abs/2401.11972v1"}
{"created":"2024-01-22 14:17:03","title":"Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research","abstract":"Automated fact-checking has drawn considerable attention over the past few decades due to the increase in the diffusion of misinformation on online platforms. This is often carried out as a sequence of tasks comprising (i) the detection of sentences circulating in online platforms which constitute claims needing verification, followed by (ii) the verification process of those claims. This survey focuses on the former, by discussing existing efforts towards detecting claims needing fact-checking, with a particular focus on multilingual data and methods. This is a challenging and fertile direction where existing methods are yet far from matching human performance due to the profoundly challenging nature of the issue. Especially, the dissemination of information across multiple social platforms, articulated in multiple languages and modalities demands more generalized solutions for combating misinformation. Focusing on multilingual misinformation, we present a comprehensive survey of existing multilingual claim detection research. We present state-of-the-art multilingual claim detection research categorized into three key factors of the problem, verifiability, priority, and similarity. Further, we present a detailed overview of the existing multilingual datasets along with the challenges and suggest possible future advancements.","sentences":["Automated fact-checking has drawn considerable attention over the past few decades due to the increase in the diffusion of misinformation on online platforms.","This is often carried out as a sequence of tasks comprising (i) the detection of sentences circulating in online platforms which constitute claims needing verification, followed by (ii) the verification process of those claims.","This survey focuses on the former, by discussing existing efforts towards detecting claims needing fact-checking, with a particular focus on multilingual data and methods.","This is a challenging and fertile direction where existing methods are yet far from matching human performance due to the profoundly challenging nature of the issue.","Especially, the dissemination of information across multiple social platforms, articulated in multiple languages and modalities demands more generalized solutions for combating misinformation.","Focusing on multilingual misinformation, we present a comprehensive survey of existing multilingual claim detection research.","We present state-of-the-art multilingual claim detection research categorized into three key factors of the problem, verifiability, priority, and similarity.","Further, we present a detailed overview of the existing multilingual datasets along with the challenges and suggest possible future advancements."],"url":"http://arxiv.org/abs/2401.11969v1"}
{"created":"2024-01-22 14:16:37","title":"Effective Intrusion Detection in Heterogeneous Internet-of-Things Networks via Ensemble Knowledge Distillation-based Federated Learning","abstract":"With the rapid development of low-cost consumer electronics and cloud computing, Internet-of-Things (IoT) devices are widely adopted for supporting next-generation distributed systems such as smart cities and industrial control systems. IoT devices are often susceptible to cyber attacks due to their open deployment environment and limited computing capabilities for stringent security controls. Hence, Intrusion Detection Systems (IDS) have emerged as one of the effective ways of securing IoT networks by monitoring and detecting abnormal activities. However, existing IDS approaches rely on centralized servers to generate behaviour profiles and detect anomalies, causing high response time and large operational costs due to communication overhead. Besides, sharing of behaviour data in an open and distributed IoT network environment may violate on-device privacy requirements. Additionally, various IoT devices tend to capture heterogeneous data, which complicates the training of behaviour models. In this paper, we introduce Federated Learning (FL) to collaboratively train a decentralized shared model of IDS, without exposing training data to others. Furthermore, we propose an effective method called Federated Learning Ensemble Knowledge Distillation (FLEKD) to mitigate the heterogeneity problems across various clients. FLEKD enables a more flexible aggregation method than conventional model fusion techniques. Experiment results on the public dataset CICIDS2019 demonstrate that the proposed approach outperforms local training and traditional FL in terms of both speed and performance and significantly improves the system's ability to detect unknown attacks. Finally, we evaluate our proposed framework's performance in three potential real-world scenarios and show FLEKD has a clear advantage in experimental results.","sentences":["With the rapid development of low-cost consumer electronics and cloud computing, Internet-of-Things (IoT) devices are widely adopted for supporting next-generation distributed systems such as smart cities and industrial control systems.","IoT devices are often susceptible to cyber attacks due to their open deployment environment and limited computing capabilities for stringent security controls.","Hence, Intrusion Detection Systems (IDS) have emerged as one of the effective ways of securing IoT networks by monitoring and detecting abnormal activities.","However, existing IDS approaches rely on centralized servers to generate behaviour profiles and detect anomalies, causing high response time and large operational costs due to communication overhead.","Besides, sharing of behaviour data in an open and distributed IoT network environment may violate on-device privacy requirements.","Additionally, various IoT devices tend to capture heterogeneous data, which complicates the training of behaviour models.","In this paper, we introduce Federated Learning (FL) to collaboratively train a decentralized shared model of IDS, without exposing training data to others.","Furthermore, we propose an effective method called Federated Learning Ensemble Knowledge Distillation (FLEKD) to mitigate the heterogeneity problems across various clients.","FLEKD enables a more flexible aggregation method than conventional model fusion techniques.","Experiment results on the public dataset CICIDS2019 demonstrate that the proposed approach outperforms local training and traditional FL in terms of both speed and performance and significantly improves the system's ability to detect unknown attacks.","Finally, we evaluate our proposed framework's performance in three potential real-world scenarios and show FLEKD has a clear advantage in experimental results."],"url":"http://arxiv.org/abs/2401.11968v1"}
{"created":"2024-01-22 14:02:56","title":"Observation-Guided Meteorological Field Downscaling at Station Scale: A Benchmark and a New Method","abstract":"Downscaling (DS) of meteorological variables involves obtaining high-resolution states from low-resolution meteorological fields and is an important task in weather forecasting. Previous methods based on deep learning treat downscaling as a super-resolution task in computer vision and utilize high-resolution gridded meteorological fields as supervision to improve resolution at specific grid scales. However, this approach has struggled to align with the continuous distribution characteristics of meteorological fields, leading to an inherent systematic bias between the downscaled results and the actual observations at meteorological stations. In this paper, we extend meteorological downscaling to arbitrary scattered station scales, establish a brand new benchmark and dataset, and retrieve meteorological states at any given station location from a coarse-resolution meteorological field. Inspired by data assimilation techniques, we integrate observational data into the downscaling process, providing multi-scale observational priors. Building on this foundation, we propose a new downscaling model based on hypernetwork architecture, namely HyperDS, which efficiently integrates different observational information into the model training, achieving continuous scale modeling of the meteorological field. Through extensive experiments, our proposed method outperforms other specially designed baseline models on multiple surface variables. Notably, the mean squared error (MSE) for wind speed and surface pressure improved by 67% and 19.5% compared to other methods. We will release the dataset and code subsequently.","sentences":["Downscaling (DS) of meteorological variables involves obtaining high-resolution states from low-resolution meteorological fields and is an important task in weather forecasting.","Previous methods based on deep learning treat downscaling as a super-resolution task in computer vision and utilize high-resolution gridded meteorological fields as supervision to improve resolution at specific grid scales.","However, this approach has struggled to align with the continuous distribution characteristics of meteorological fields, leading to an inherent systematic bias between the downscaled results and the actual observations at meteorological stations.","In this paper, we extend meteorological downscaling to arbitrary scattered station scales, establish a brand new benchmark and dataset, and retrieve meteorological states at any given station location from a coarse-resolution meteorological field.","Inspired by data assimilation techniques, we integrate observational data into the downscaling process, providing multi-scale observational priors.","Building on this foundation, we propose a new downscaling model based on hypernetwork architecture, namely HyperDS, which efficiently integrates different observational information into the model training, achieving continuous scale modeling of the meteorological field.","Through extensive experiments, our proposed method outperforms other specially designed baseline models on multiple surface variables.","Notably, the mean squared error (MSE) for wind speed and surface pressure improved by 67% and 19.5% compared to other methods.","We will release the dataset and code subsequently."],"url":"http://arxiv.org/abs/2401.11960v1"}
{"created":"2024-01-22 13:54:26","title":"RUMBoost: Gradient Boosted Random Utility Models","abstract":"This paper introduces the RUMBoost model, a novel discrete choice modelling approach that combines the interpretability and behavioural robustness of Random Utility Models (RUMs) with the generalisation and predictive ability of deep learning methods. We obtain the full functional form of non-linear utility specifications by replacing each linear parameter in the utility functions of a RUM with an ensemble of gradient boosted regression trees. This enables piece-wise constant utility values to be imputed for all alternatives directly from the data for any possible combination of input variables. We introduce additional constraints on the ensembles to ensure three crucial features of the utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii) monotonicity of marginal utilities, and (iii) an intrinsically interpretable functional form, where the exact response of the model is known throughout the entire input space. Furthermore, we introduce an optimisation-based smoothing technique that replaces the piece-wise constant utility values of alternative attributes with monotonic piece-wise cubic splines to identify non-linear parameters with defined gradient. We demonstrate the potential of the RUMBoost model compared to various ML and Random Utility benchmark models for revealed preference mode choice data from London. The results highlight the great predictive performance and the direct interpretability of our proposed approach. Furthermore, the smoothed attribute utility functions allow for the calculation of various behavioural indicators and marginal utilities. Finally, we demonstrate the flexibility of our methodology by showing how the RUMBoost model can be extended to complex model specifications, including attribute interactions, correlation within alternative error terms and heterogeneity within the population.","sentences":["This paper introduces the RUMBoost model, a novel discrete choice modelling approach that combines the interpretability and behavioural robustness of Random Utility Models (RUMs) with the generalisation and predictive ability of deep learning methods.","We obtain the full functional form of non-linear utility specifications by replacing each linear parameter in the utility functions of a RUM with an ensemble of gradient boosted regression trees.","This enables piece-wise constant utility values to be imputed for all alternatives directly from the data for any possible combination of input variables.","We introduce additional constraints on the ensembles to ensure three crucial features of the utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii) monotonicity of marginal utilities, and (iii) an intrinsically interpretable functional form, where the exact response of the model is known throughout the entire input space.","Furthermore, we introduce an optimisation-based smoothing technique that replaces the piece-wise constant utility values of alternative attributes with monotonic piece-wise cubic splines to identify non-linear parameters with defined gradient.","We demonstrate the potential of the RUMBoost model compared to various ML and Random Utility benchmark models for revealed preference mode choice data from London.","The results highlight the great predictive performance and the direct interpretability of our proposed approach.","Furthermore, the smoothed attribute utility functions allow for the calculation of various behavioural indicators and marginal utilities.","Finally, we demonstrate the flexibility of our methodology by showing how the RUMBoost model can be extended to complex model specifications, including attribute interactions, correlation within alternative error terms and heterogeneity within the population."],"url":"http://arxiv.org/abs/2401.11954v1"}
{"created":"2024-01-22 13:35:27","title":"A Dynamic YOLO-Based Sequence-Matching Model for Efficient Coverless Image Steganography","abstract":"Many existing coverless steganography methods establish a mapping relationship between cover images and hidden data. There exists an issue that the number of images stored in the database grows exponentially as the steganographic capacity rises. The need for a high steganographic capacity makes it challenging to build an image database. To improve the image library utilization and anti-attack capability of the steganography system, we present an efficient coverless scheme based on dynamically matched substrings. YOLO is employed for selecting optimal objects, and a mapping dictionary is established between these objects and scrambling factors. With the aid of this dictionary, each image is effectively assigned to a specific scrambling factor, which is used to scramble the receiver's sequence key. To achieve sufficient steganography capability based on a limited image library, all substrings of the scrambled sequences hold the potential to hide data. After completing the secret information matching, the ideal number of stego images will be obtained from the database. According to experimental results, this technology outperforms most previous works on data load, transmission security, and hiding capacity. Under typical geometric attacks, it can recover 79.85\\% of secret information on average. Furthermore, only approximately 200 random images are needed to meet a capacity of 19 bits per image.","sentences":["Many existing coverless steganography methods establish a mapping relationship between cover images and hidden data.","There exists an issue that the number of images stored in the database grows exponentially as the steganographic capacity rises.","The need for a high steganographic capacity makes it challenging to build an image database.","To improve the image library utilization and anti-attack capability of the steganography system, we present an efficient coverless scheme based on dynamically matched substrings.","YOLO is employed for selecting optimal objects, and a mapping dictionary is established between these objects and scrambling factors.","With the aid of this dictionary, each image is effectively assigned to a specific scrambling factor, which is used to scramble the receiver's sequence key.","To achieve sufficient steganography capability based on a limited image library, all substrings of the scrambled sequences hold the potential to hide data.","After completing the secret information matching, the ideal number of stego images will be obtained from the database.","According to experimental results, this technology outperforms most previous works on data load, transmission security, and hiding capacity.","Under typical geometric attacks, it can recover 79.85\\% of secret information on average.","Furthermore, only approximately 200 random images are needed to meet a capacity of 19 bits per image."],"url":"http://arxiv.org/abs/2401.11946v1"}
{"created":"2024-01-22 13:23:38","title":"Accelerating Causal Algorithms for Industrial-scale Data: A Distributed Computing Approach with Ray Framework","abstract":"The increasing need for causal analysis in large-scale industrial datasets necessitates the development of efficient and scalable causal algorithms for real-world applications. This paper addresses the challenge of scaling causal algorithms in the context of conducting causal analysis on extensive datasets commonly encountered in industrial settings. Our proposed solution involves enhancing the scalability of causal algorithm libraries, such as EconML, by leveraging the parallelism capabilities offered by the distributed computing framework Ray. We explore the potential of parallelizing key iterative steps within causal algorithms to significantly reduce overall runtime, supported by a case study that examines the impact on estimation times and costs. Through this approach, we aim to provide a more effective solution for implementing causal analysis in large-scale industrial applications.","sentences":["The increasing need for causal analysis in large-scale industrial datasets necessitates the development of efficient and scalable causal algorithms for real-world applications.","This paper addresses the challenge of scaling causal algorithms in the context of conducting causal analysis on extensive datasets commonly encountered in industrial settings.","Our proposed solution involves enhancing the scalability of causal algorithm libraries, such as EconML, by leveraging the parallelism capabilities offered by the distributed computing framework Ray.","We explore the potential of parallelizing key iterative steps within causal algorithms to significantly reduce overall runtime, supported by a case study that examines the impact on estimation times and costs.","Through this approach, we aim to provide a more effective solution for implementing causal analysis in large-scale industrial applications."],"url":"http://arxiv.org/abs/2401.11932v1"}
{"created":"2024-01-22 13:15:40","title":"The Bigger the Better? Rethinking the Effective Model Scale in Long-term Time Series Forecasting","abstract":"Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches. While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity. These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms. This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity. Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data. Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with hierarchical decomposition. This novel architecture not only inverts the prevailing trend toward model expansion but also accomplishes precise forecasting with drastically fewer computations and parameters. Remarkably, HDformer outperforms existing state-of-the-art LTSF models, while requiring over 99\\% fewer parameters. Through this work, we advocate a paradigm shift in LTSF, emphasizing the importance to tailor the model to the inherent dynamics of time series data-a timely reminder that in the realm of LTSF, bigger is not invariably better.","sentences":["Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches.","While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity.","These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms.","This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity.","Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data.","Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with hierarchical decomposition.","This novel architecture not only inverts the prevailing trend toward model expansion but also accomplishes precise forecasting with drastically fewer computations and parameters.","Remarkably, HDformer outperforms existing state-of-the-art LTSF models, while requiring over 99\\% fewer parameters.","Through this work, we advocate a paradigm shift in LTSF, emphasizing the importance to tailor the model to the inherent dynamics of time series data-a timely reminder that in the realm of LTSF, bigger is not invariably better."],"url":"http://arxiv.org/abs/2401.11929v1"}
{"created":"2024-01-22 13:01:49","title":"Secure Multi-hop Telemetry Broadcasts for UAV Swarm Communication","abstract":"Unmanned Aerial Vehicles (UAVs) are evolving as adaptable platforms for a wide range of applications such as precise inspections, emergency response, and remote sensing. Autonomous UAV swarms require efficient and stable communication during deployment for a successful mission execution. For instance, the periodic exchange of telemetry data between all swarm members provides the foundation for formation flight and collision avoidance. However, due to the mobility of the vehicles and instability of wireless transmissions, maintaining a secure and reliable all-to-all communication remains challenging. This paper investigates encrypted and authenticated multi-hop broadcast communication based on the transmission of custom IEEE 802.11 Wi-Fi data frames.","sentences":["Unmanned Aerial Vehicles (UAVs) are evolving as adaptable platforms for a wide range of applications such as precise inspections, emergency response, and remote sensing.","Autonomous UAV swarms require efficient and stable communication during deployment for a successful mission execution.","For instance, the periodic exchange of telemetry data between all swarm members provides the foundation for formation flight and collision avoidance.","However, due to the mobility of the vehicles and instability of wireless transmissions, maintaining a secure and reliable all-to-all communication remains challenging.","This paper investigates encrypted and authenticated multi-hop broadcast communication based on the transmission of custom IEEE 802.11 Wi-Fi data frames."],"url":"http://arxiv.org/abs/2401.11915v1"}
{"created":"2024-01-22 12:39:36","title":"AI, insurance, discrimination and unfair differentiation. An overview and research agenda","abstract":"Insurers increasingly use AI. We distinguish two situations in which insurers use AI: (i) data-intensive underwriting, and (ii) behaviour-based insurance. (i) First, insurers can use AI for data analysis to assess risks: data-intensive underwriting. Underwriting is, in short, calculating risks and amending the insurance premium accordingly. (ii) Second, insurers can use AI to monitor the behaviour of consumers in real-time: behaviour-based insurance. For example, some car insurers give a discount if a consumer agrees to being tracked by the insurer and drives safely. While the two trends bring many advantages, they may also have discriminatory effects. This paper focuses on the following question. Which discrimination-related effects may occur if insurers use data-intensive underwriting and behaviour-based insurance? We focus on two types of discrimination-related effects: discrimination and other unfair differentiation. (i) Discrimination harms certain groups who are protected by non-discrimination law, for instance people with certain ethnicities. (ii) Unfair differentiation does not harm groups that are protected by non-discrimination law, but it does seem unfair. We introduce four factors to consider when assessing the fairness of insurance practices. The paper builds on literature from various disciplines including law, philosophy, and computer science.","sentences":["Insurers increasingly use AI.","We distinguish two situations in which insurers use AI: (i) data-intensive underwriting, and (ii) behaviour-based insurance.","(i) First, insurers can use AI for data analysis to assess risks: data-intensive underwriting.","Underwriting is, in short, calculating risks and amending the insurance premium accordingly.","(ii) Second, insurers can use AI to monitor the behaviour of consumers in real-time: behaviour-based insurance.","For example, some car insurers give a discount if a consumer agrees to being tracked by the insurer and drives safely.","While the two trends bring many advantages, they may also have discriminatory effects.","This paper focuses on the following question.","Which discrimination-related effects may occur if insurers use data-intensive underwriting and behaviour-based insurance?","We focus on two types of discrimination-related effects: discrimination and other unfair differentiation.","(i) Discrimination harms certain groups who are protected by non-discrimination law, for instance people with certain ethnicities.","(ii) Unfair differentiation does not harm groups that are protected by non-discrimination law, but it does seem unfair.","We introduce four factors to consider when assessing the fairness of insurance practices.","The paper builds on literature from various disciplines including law, philosophy, and computer science."],"url":"http://arxiv.org/abs/2401.11892v1"}
{"created":"2024-01-22 12:28:50","title":"Multimodal Deep Learning of Word-of-Mouth Text and Demographics to Predict Customer Rating: Handling Consumer Heterogeneity in Marketing","abstract":"In the marketing field, understanding consumer heterogeneity, which is the internal or psychological difference among consumers that cannot be captured by behavioral logs, has long been a critical challenge. However, a number of consumers today usually post their evaluation on the specific product on the online platform, which can be the valuable source of such unobservable differences among consumers. Several previous studies have shown the validity of the analysis on text modality, but on the other hand, such analyses may not necessarily demonstrate sufficient predictive accuracy for text alone, as they may not include information readily available from cross-sectional data, such as consumer profile data. In addition, recent advances in machine learning techniques, such as large-scale language models (LLMs) and multimodal learning have made it possible to deal with the various kind of dataset simultaneously, including textual data and the traditional cross-sectional data, and the joint representations can be effectively obtained from multiple modalities. Therefore, this study constructs a product evaluation model that takes into account consumer heterogeneity by multimodal learning of online product reviews and consumer profile information. We also compare multiple models using different modalities or hyper-parameters to demonstrate the robustness of multimodal learning in marketing analysis.","sentences":["In the marketing field, understanding consumer heterogeneity, which is the internal or psychological difference among consumers that cannot be captured by behavioral logs, has long been a critical challenge.","However, a number of consumers today usually post their evaluation on the specific product on the online platform, which can be the valuable source of such unobservable differences among consumers.","Several previous studies have shown the validity of the analysis on text modality, but on the other hand, such analyses may not necessarily demonstrate sufficient predictive accuracy for text alone, as they may not include information readily available from cross-sectional data, such as consumer profile data.","In addition, recent advances in machine learning techniques, such as large-scale language models (LLMs) and multimodal learning have made it possible to deal with the various kind of dataset simultaneously, including textual data and the traditional cross-sectional data, and the joint representations can be effectively obtained from multiple modalities.","Therefore, this study constructs a product evaluation model that takes into account consumer heterogeneity by multimodal learning of online product reviews and consumer profile information.","We also compare multiple models using different modalities or hyper-parameters to demonstrate the robustness of multimodal learning in marketing analysis."],"url":"http://arxiv.org/abs/2401.11888v1"}
{"created":"2024-01-22 12:11:55","title":"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety","abstract":"Multi-agent systems, augmented with Large Language Models (LLMs), demonstrate significant capabilities for collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. From the perspective of agent psychology, we discover that the dark psychological states of agents can lead to severe safety issues. To address these issues, we propose a comprehensive framework grounded in agent psychology. In our framework, we focus on three aspects: identifying how dark personality traits in agents might lead to risky behaviors, designing defense strategies to mitigate these risks, and evaluating the safety of multi-agent systems from both psychological and behavioral perspectives. Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents' propensity for self-reflection when engaging in dangerous behavior, and the correlation between agents' psychological assessments and their dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We will make our data and code publicly accessible at https:/github.com/AI4Good24/PsySafe.","sentences":["Multi-agent systems, augmented with Large Language Models (LLMs), demonstrate significant capabilities for collective intelligence.","However, the potential misuse of this intelligence for malicious purposes presents significant risks.","To date, comprehensive research on the safety issues associated with multi-agent systems remains limited.","From the perspective of agent psychology, we discover that the dark psychological states of agents can lead to severe safety issues.","To address these issues, we propose a comprehensive framework grounded in agent psychology.","In our framework, we focus on three aspects: identifying how dark personality traits in agents might lead to risky behaviors, designing defense strategies to mitigate these risks, and evaluating the safety of multi-agent systems from both psychological and behavioral perspectives.","Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents' propensity for self-reflection when engaging in dangerous behavior, and the correlation between agents' psychological assessments and their dangerous behaviors.","We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems.","We will make our data and code publicly accessible at https:/github.com/AI4Good24/PsySafe."],"url":"http://arxiv.org/abs/2401.11880v1"}
{"created":"2024-01-22 11:29:44","title":"A Review of Physics-Informed Machine Learning Methods with Applications to Condition Monitoring and Anomaly Detection","abstract":"This study presents a comprehensive overview of PIML techniques in the context of condition monitoring. The central concept driving PIML is the incorporation of known physical laws and constraints into machine learning algorithms, enabling them to learn from available data while remaining consistent with physical principles. Through fusing domain knowledge with data-driven learning, PIML methods offer enhanced accuracy and interpretability in comparison to purely data-driven approaches. In this comprehensive survey, detailed examinations are performed with regard to the methodology by which known physical principles are integrated within machine learning frameworks, as well as their suitability for specific tasks within condition monitoring. Incorporation of physical knowledge into the ML model may be realized in a variety of methods, with each having its unique advantages and drawbacks. The distinct advantages and limitations of each methodology for the integration of physics within data-driven models are detailed, considering factors such as computational efficiency, model interpretability, and generalizability to different systems in condition monitoring and fault detection. Several case studies and works of literature utilizing this emerging concept are presented to demonstrate the efficacy of PIML in condition monitoring applications. From the literature reviewed, the versatility and potential of PIML in condition monitoring may be demonstrated. Novel PIML methods offer an innovative solution for addressing the complexities of condition monitoring and associated challenges. This comprehensive survey helps form the foundation for future work in the field. As the technology continues to advance, PIML is expected to play a crucial role in enhancing maintenance strategies, system reliability, and overall operational efficiency in engineering systems.","sentences":["This study presents a comprehensive overview of PIML techniques in the context of condition monitoring.","The central concept driving PIML is the incorporation of known physical laws and constraints into machine learning algorithms, enabling them to learn from available data while remaining consistent with physical principles.","Through fusing domain knowledge with data-driven learning, PIML methods offer enhanced accuracy and interpretability in comparison to purely data-driven approaches.","In this comprehensive survey, detailed examinations are performed with regard to the methodology by which known physical principles are integrated within machine learning frameworks, as well as their suitability for specific tasks within condition monitoring.","Incorporation of physical knowledge into the ML model may be realized in a variety of methods, with each having its unique advantages and drawbacks.","The distinct advantages and limitations of each methodology for the integration of physics within data-driven models are detailed, considering factors such as computational efficiency, model interpretability, and generalizability to different systems in condition monitoring and fault detection.","Several case studies and works of literature utilizing this emerging concept are presented to demonstrate the efficacy of PIML in condition monitoring applications.","From the literature reviewed, the versatility and potential of PIML in condition monitoring may be demonstrated.","Novel PIML methods offer an innovative solution for addressing the complexities of condition monitoring and associated challenges.","This comprehensive survey helps form the foundation for future work in the field.","As the technology continues to advance, PIML is expected to play a crucial role in enhancing maintenance strategies, system reliability, and overall operational efficiency in engineering systems."],"url":"http://arxiv.org/abs/2401.11860v1"}
{"created":"2024-01-22 11:18:19","title":"Optimization in Sanger Sequencing","abstract":"The main objective of this paper is to solve the optimization problem that is associated with the classification of DNA samples in PCR plates for Sanger sequencing. To achieve this goal, we design an integer linear programming model. Given that the real instances involve the classification of thousands of samples and the linear model can only be solved for small instances, the paper includes a heuristic to cope with bigger problems. The heuristic algorithm is based on the simulated annealing technique. This algorithm obtains satisfactory solutions to the problem in a short amount of time. It has been tested with real data and yields improved results compared to some commercial software typically used in (clinical) laboratories. Moreover, the algorithm has already been implemented in the laboratory and is being successfully used.","sentences":["The main objective of this paper is to solve the optimization problem that is associated with the classification of DNA samples in PCR plates for Sanger sequencing.","To achieve this goal, we design an integer linear programming model.","Given that the real instances involve the classification of thousands of samples and the linear model can only be solved for small instances, the paper includes a heuristic to cope with bigger problems.","The heuristic algorithm is based on the simulated annealing technique.","This algorithm obtains satisfactory solutions to the problem in a short amount of time.","It has been tested with real data and yields improved results compared to some commercial software typically used in (clinical) laboratories.","Moreover, the algorithm has already been implemented in the laboratory and is being successfully used."],"url":"http://arxiv.org/abs/2401.11854v1"}
{"created":"2024-01-22 11:15:07","title":"The Right Model for the Job: An Evaluation of Legal Multi-Label Classification Baselines","abstract":"Multi-Label Classification (MLC) is a common task in the legal domain, where more than one label may be assigned to a legal document. A wide range of methods can be applied, ranging from traditional ML approaches to the latest Transformer-based architectures. In this work, we perform an evaluation of different MLC methods using two public legal datasets, POSTURE50K and EURLEX57K. By varying the amount of training data and the number of labels, we explore the comparative advantage offered by different approaches in relation to the dataset properties. Our findings highlight DistilRoBERTa and LegalBERT as performing consistently well in legal MLC with reasonable computational demands. T5 also demonstrates comparable performance while offering advantages as a generative model in the presence of changing label sets. Finally, we show that the CrossEncoder exhibits potential for notable macro-F1 score improvements, albeit with increased computational costs.","sentences":["Multi-Label Classification (MLC) is a common task in the legal domain, where more than one label may be assigned to a legal document.","A wide range of methods can be applied, ranging from traditional ML approaches to the latest Transformer-based architectures.","In this work, we perform an evaluation of different MLC methods using two public legal datasets, POSTURE50K and EURLEX57K. By varying the amount of training data and the number of labels, we explore the comparative advantage offered by different approaches in relation to the dataset properties.","Our findings highlight DistilRoBERTa and LegalBERT as performing consistently well in legal MLC with reasonable computational demands.","T5 also demonstrates comparable performance while offering advantages as a generative model in the presence of changing label sets.","Finally, we show that the CrossEncoder exhibits potential for notable macro-F1 score improvements, albeit with increased computational costs."],"url":"http://arxiv.org/abs/2401.11852v1"}
{"created":"2024-01-22 11:04:55","title":"SignVTCL: Multi-Modal Continuous Sign Language Recognition Enhanced by Visual-Textual Contrastive Learning","abstract":"Sign language recognition (SLR) plays a vital role in facilitating communication for the hearing-impaired community. SLR is a weakly supervised task where entire videos are annotated with glosses, making it challenging to identify the corresponding gloss within a video segment. Recent studies indicate that the main bottleneck in SLR is the insufficient training caused by the limited availability of large-scale datasets. To address this challenge, we present SignVTCL, a multi-modal continuous sign language recognition framework enhanced by visual-textual contrastive learning, which leverages the full potential of multi-modal data and the generalization ability of language model. SignVTCL integrates multi-modal data (video, keypoints, and optical flow) simultaneously to train a unified visual backbone, thereby yielding more robust visual representations. Furthermore, SignVTCL contains a visual-textual alignment approach incorporating gloss-level and sentence-level alignment to ensure precise correspondence between visual features and glosses at the level of individual glosses and sentence. Experimental results conducted on three datasets, Phoenix-2014, Phoenix-2014T, and CSL-Daily, demonstrate that SignVTCL achieves state-of-the-art results compared with previous methods.","sentences":["Sign language recognition (SLR) plays a vital role in facilitating communication for the hearing-impaired community.","SLR is a weakly supervised task where entire videos are annotated with glosses, making it challenging to identify the corresponding gloss within a video segment.","Recent studies indicate that the main bottleneck in SLR is the insufficient training caused by the limited availability of large-scale datasets.","To address this challenge, we present SignVTCL, a multi-modal continuous sign language recognition framework enhanced by visual-textual contrastive learning, which leverages the full potential of multi-modal data and the generalization ability of language model.","SignVTCL integrates multi-modal data (video, keypoints, and optical flow) simultaneously to train a unified visual backbone, thereby yielding more robust visual representations.","Furthermore, SignVTCL contains a visual-textual alignment approach incorporating gloss-level and sentence-level alignment to ensure precise correspondence between visual features and glosses at the level of individual glosses and sentence.","Experimental results conducted on three datasets, Phoenix-2014, Phoenix-2014T, and CSL-Daily, demonstrate that SignVTCL achieves state-of-the-art results compared with previous methods."],"url":"http://arxiv.org/abs/2401.11847v1"}
{"created":"2024-01-22 11:01:52","title":"Adaptive Fusion of Multi-view Remote Sensing data for Optimal Sub-field Crop Yield Prediction","abstract":"Accurate crop yield prediction is of utmost importance for informed decision-making in agriculture, aiding farmers, and industry stakeholders. However, this task is complex and depends on multiple factors, such as environmental conditions, soil properties, and management practices. Combining heterogeneous data views poses a fusion challenge, like identifying the view-specific contribution to the predictive task. We present a novel multi-view learning approach to predict crop yield for different crops (soybean, wheat, rapeseed) and regions (Argentina, Uruguay, and Germany). Our multi-view input data includes multi-spectral optical images from Sentinel-2 satellites and weather data as dynamic features during the crop growing season, complemented by static features like soil properties and topographic information. To effectively fuse the data, we introduce a Multi-view Gated Fusion (MVGF) model, comprising dedicated view-encoders and a Gated Unit (GU) module. The view-encoders handle the heterogeneity of data sources with varying temporal resolutions by learning a view-specific representation. These representations are adaptively fused via a weighted sum. The fusion weights are computed for each sample by the GU using a concatenation of the view-representations. The MVGF model is trained at sub-field level with 10 m resolution pixels. Our evaluations show that the MVGF outperforms conventional models on the same task, achieving the best results by incorporating all the data sources, unlike the usual fusion results in the literature. For Argentina, the MVGF model achieves an R2 value of 0.68 at sub-field yield prediction, while at field level evaluation (comparing field averages), it reaches around 0.80 across different countries. The GU module learned different weights based on the country and crop-type, aligning with the variable significance of each data source to the prediction task.","sentences":["Accurate crop yield prediction is of utmost importance for informed decision-making in agriculture, aiding farmers, and industry stakeholders.","However, this task is complex and depends on multiple factors, such as environmental conditions, soil properties, and management practices.","Combining heterogeneous data views poses a fusion challenge, like identifying the view-specific contribution to the predictive task.","We present a novel multi-view learning approach to predict crop yield for different crops (soybean, wheat, rapeseed) and regions (Argentina, Uruguay, and Germany).","Our multi-view input data includes multi-spectral optical images from Sentinel-2 satellites and weather data as dynamic features during the crop growing season, complemented by static features like soil properties and topographic information.","To effectively fuse the data, we introduce a Multi-view Gated Fusion (MVGF) model, comprising dedicated view-encoders and a Gated Unit (GU) module.","The view-encoders handle the heterogeneity of data sources with varying temporal resolutions by learning a view-specific representation.","These representations are adaptively fused via a weighted sum.","The fusion weights are computed for each sample by the GU using a concatenation of the view-representations.","The MVGF model is trained at sub-field level with 10 m resolution pixels.","Our evaluations show that the MVGF outperforms conventional models on the same task, achieving the best results by incorporating all the data sources, unlike the usual fusion results in the literature.","For Argentina, the MVGF model achieves an R2 value of 0.68 at sub-field yield prediction, while at field level evaluation (comparing field averages), it reaches around 0.80 across different countries.","The GU module learned different weights based on the country and crop-type, aligning with the variable significance of each data source to the prediction task."],"url":"http://arxiv.org/abs/2401.11844v1"}
{"created":"2024-01-22 10:57:11","title":"Learning to Approximate Adaptive Kernel Convolution on Graphs","abstract":"Various Graph Neural Networks (GNNs) have been successful in analyzing data in non-Euclidean spaces, however, they have limitations such as oversmoothing, i.e., information becomes excessively averaged as the number of hidden layers increases. The issue stems from the intrinsic formulation of conventional graph convolution where the nodal features are aggregated from a direct neighborhood per layer across the entire nodes in the graph. As setting different number of hidden layers per node is infeasible, recent works leverage a diffusion kernel to redefine the graph structure and incorporate information from farther nodes. Unfortunately, such approaches suffer from heavy diagonalization of a graph Laplacian or learning a large transform matrix. In this regards, we propose a diffusion learning framework, where the range of feature aggregation is controlled by the scale of a diffusion kernel. For efficient computation, we derive closed-form derivatives of approximations of the graph convolution with respect to the scale, so that node-wise range can be adaptively learned. With a downstream classifier, the entire framework is made trainable in an end-to-end manner. Our model is tested on various standard datasets for node-wise classification for the state-of-the-art performance, and it is also validated on a real-world brain network data for graph classifications to demonstrate its practicality for Alzheimer classification.","sentences":["Various Graph Neural Networks (GNNs) have been successful in analyzing data in non-Euclidean spaces, however, they have limitations such as oversmoothing, i.e., information becomes excessively averaged as the number of hidden layers increases.","The issue stems from the intrinsic formulation of conventional graph convolution where the nodal features are aggregated from a direct neighborhood per layer across the entire nodes in the graph.","As setting different number of hidden layers per node is infeasible, recent works leverage a diffusion kernel to redefine the graph structure and incorporate information from farther nodes.","Unfortunately, such approaches suffer from heavy diagonalization of a graph Laplacian or learning a large transform matrix.","In this regards, we propose a diffusion learning framework, where the range of feature aggregation is controlled by the scale of a diffusion kernel.","For efficient computation, we derive closed-form derivatives of approximations of the graph convolution with respect to the scale, so that node-wise range can be adaptively learned.","With a downstream classifier, the entire framework is made trainable in an end-to-end manner.","Our model is tested on various standard datasets for node-wise classification for the state-of-the-art performance, and it is also validated on a real-world brain network data for graph classifications to demonstrate its practicality for Alzheimer classification."],"url":"http://arxiv.org/abs/2401.11840v1"}
{"created":"2024-01-22 10:52:22","title":"Privacy-Preserving Data Fusion for Traffic State Estimation: A Vertical Federated Learning Approach","abstract":"This paper proposes a privacy-preserving data fusion method for traffic state estimation (TSE). Unlike existing works that assume all data sources to be accessible by a single trusted party, we explicitly address data privacy concerns that arise in the collaboration and data sharing between multiple data owners, such as municipal authorities (MAs) and mobility providers (MPs). To this end, we propose a novel vertical federated learning (FL) approach, FedTSE, that enables multiple data owners to collaboratively train and apply a TSE model without having to exchange their private data. To enhance the applicability of the proposed FedTSE in common TSE scenarios with limited availability of ground-truth data, we further propose a privacy-preserving physics-informed FL approach, i.e., FedTSE-PI, that integrates traffic models into FL. Real-world data validation shows that the proposed methods can protect privacy while yielding similar accuracy to the oracle method without privacy considerations.","sentences":["This paper proposes a privacy-preserving data fusion method for traffic state estimation (TSE).","Unlike existing works that assume all data sources to be accessible by a single trusted party, we explicitly address data privacy concerns that arise in the collaboration and data sharing between multiple data owners, such as municipal authorities (MAs) and mobility providers (MPs).","To this end, we propose a novel vertical federated learning (FL) approach, FedTSE, that enables multiple data owners to collaboratively train and apply a TSE model without having to exchange their private data.","To enhance the applicability of the proposed FedTSE in common TSE scenarios with limited availability of ground-truth data, we further propose a privacy-preserving physics-informed FL approach, i.e., FedTSE-PI, that integrates traffic models into FL.","Real-world data validation shows that the proposed methods can protect privacy while yielding similar accuracy to the oracle method without privacy considerations."],"url":"http://arxiv.org/abs/2401.11836v1"}
{"created":"2024-01-22 10:26:52","title":"MInD: Improving Multimodal Sentiment Analysis via Multimodal Information Disentanglement","abstract":"Learning effective joint representations has been a central task in multimodal sentiment analysis. Previous methods focus on leveraging the correlations between different modalities and enhancing performance through sophisticated fusion techniques. However, challenges still exist due to the inherent heterogeneity of distinct modalities, which may lead to distributional gap, impeding the full exploitation of inter-modal information and resulting in redundancy and impurity in the information extracted from features. To address this problem, we introduce the Multimodal Information Disentanglement (MInD) approach. MInD decomposes the multimodal inputs into a modality-invariant component, a modality-specific component, and a remnant noise component for each modality through a shared encoder and multiple private encoders. The shared encoder aims to explore the shared information and commonality across modalities, while the private encoders are deployed to capture the distinctive information and characteristic features. These representations thus furnish a comprehensive perspective of the multimodal data, facilitating the fusion process instrumental for subsequent prediction tasks. Furthermore, MInD improves the learned representations by explicitly modeling the task-irrelevant noise in an adversarial manner. Experimental evaluations conducted on benchmark datasets, including CMU-MOSI, CMU-MOSEI, and UR-Funny, demonstrate MInD's superior performance over existing state-of-the-art methods in both multimodal emotion recognition and multimodal humor detection tasks.","sentences":["Learning effective joint representations has been a central task in multimodal sentiment analysis.","Previous methods focus on leveraging the correlations between different modalities and enhancing performance through sophisticated fusion techniques.","However, challenges still exist due to the inherent heterogeneity of distinct modalities, which may lead to distributional gap, impeding the full exploitation of inter-modal information and resulting in redundancy and impurity in the information extracted from features.","To address this problem, we introduce the Multimodal Information Disentanglement (MInD) approach.","MInD decomposes the multimodal inputs into a modality-invariant component, a modality-specific component, and a remnant noise component for each modality through a shared encoder and multiple private encoders.","The shared encoder aims to explore the shared information and commonality across modalities, while the private encoders are deployed to capture the distinctive information and characteristic features.","These representations thus furnish a comprehensive perspective of the multimodal data, facilitating the fusion process instrumental for subsequent prediction tasks.","Furthermore, MInD improves the learned representations by explicitly modeling the task-irrelevant noise in an adversarial manner.","Experimental evaluations conducted on benchmark datasets, including CMU-MOSI, CMU-MOSEI, and UR-Funny, demonstrate MInD's superior performance over existing state-of-the-art methods in both multimodal emotion recognition and multimodal humor detection tasks."],"url":"http://arxiv.org/abs/2401.11818v1"}
{"created":"2024-01-22 10:22:06","title":"Cyclic viscoelastic-viscoplastic behavior of epoxy nanocomposites under hygrothermal conditions: A phase-field fracture model","abstract":"In this study, a finite deformation phase-field formulation is developed to investigate the effect of hygrothermal conditions on the viscoelastic-viscoplastic fracture behavior of epoxy nanocomposites under cyclic loading. The formulation incorporates a definition of the Helmholtz free energy, which considers the effect of nanoparticles, moisture content, and temperature. The free energy is additively decomposed into a deviatoric equilibrium, a deviatoric non-equilibrium, and a volumetric contribution, with distinct definitions for tension and compression. The proposed derivation offers a realistic modeling of damage and viscoplasticity mechanisms in the nanocomposites by coupling the phase-field damage model with a modified crack driving force and a viscoelastic-viscoplastic model. Numerical simulations are conducted to study the cyclic force-displacement response of both dry and saturated boehmite nanoparticle (BNP)/epoxy samples, considering BNP contents and temperature. Comparing numerical results with experimental data shows good agreement at various BNP contents. In addition, the predictive capability of the phase-field model is evaluated through simulations of single-edge notched nanocomposite plates subjected to monolithic tensile and shear loading.","sentences":["In this study, a finite deformation phase-field formulation is developed to investigate the effect of hygrothermal conditions on the viscoelastic-viscoplastic fracture behavior of epoxy nanocomposites under cyclic loading.","The formulation incorporates a definition of the Helmholtz free energy, which considers the effect of nanoparticles, moisture content, and temperature.","The free energy is additively decomposed into a deviatoric equilibrium, a deviatoric non-equilibrium, and a volumetric contribution, with distinct definitions for tension and compression.","The proposed derivation offers a realistic modeling of damage and viscoplasticity mechanisms in the nanocomposites by coupling the phase-field damage model with a modified crack driving force and a viscoelastic-viscoplastic model.","Numerical simulations are conducted to study the cyclic force-displacement response of both dry and saturated boehmite nanoparticle (BNP)/epoxy samples, considering BNP contents and temperature.","Comparing numerical results with experimental data shows good agreement at various BNP contents.","In addition, the predictive capability of the phase-field model is evaluated through simulations of single-edge notched nanocomposite plates subjected to monolithic tensile and shear loading."],"url":"http://arxiv.org/abs/2401.11813v1"}
{"created":"2024-01-22 10:14:45","title":"Generalization and Informativeness of Conformal Prediction","abstract":"The safe integration of machine learning modules in decision-making processes hinges on their ability to quantify uncertainty. A popular technique to achieve this goal is conformal prediction (CP), which transforms an arbitrary base predictor into a set predictor with coverage guarantees. While CP certifies the predicted set to contain the target quantity with a user-defined tolerance, it does not provide control over the average size of the predicted sets, i.e., over the informativeness of the prediction. In this work, a theoretical connection is established between the generalization properties of the base predictor and the informativeness of the resulting CP prediction sets. To this end, an upper bound is derived on the expected size of the CP set predictor that builds on generalization error bounds for the base predictor. The derived upper bound provides insights into the dependence of the average size of the CP set predictor on the amount of calibration data, the target reliability, and the generalization performance of the base predictor. The theoretical insights are validated using simple numerical regression and classification tasks.","sentences":["The safe integration of machine learning modules in decision-making processes hinges on their ability to quantify uncertainty.","A popular technique to achieve this goal is conformal prediction (CP), which transforms an arbitrary base predictor into a set predictor with coverage guarantees.","While CP certifies the predicted set to contain the target quantity with a user-defined tolerance, it does not provide control over the average size of the predicted sets, i.e., over the informativeness of the prediction.","In this work, a theoretical connection is established between the generalization properties of the base predictor and the informativeness of the resulting CP prediction sets.","To this end, an upper bound is derived on the expected size of the CP set predictor that builds on generalization error bounds for the base predictor.","The derived upper bound provides insights into the dependence of the average size of the CP set predictor on the amount of calibration data, the target reliability, and the generalization performance of the base predictor.","The theoretical insights are validated using simple numerical regression and classification tasks."],"url":"http://arxiv.org/abs/2401.11810v1"}
{"created":"2024-01-22 09:54:49","title":"Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction","abstract":"Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge arises in determining the student network architecture rather than considering it inadvertently. To address this challenge, we propose an algorithm that utilizes the cost function to calculate pruning scores, addressing small network architecture search issues, and jointly fine-tunes the network resulting from each pruning stage using KD. Ultimately, we evaluate our proposed ideas on two real-world datasets, PeMSD7 and PeMSD8. The results indicate that our method can maintain the student's accuracy close to that of the teacher, even with the retention of only $3\\%$ of network parameters.","sentences":["Efficient real-time traffic prediction is crucial for reducing transportation time.","To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs.","Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data.","Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction.","In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher.","We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher.","However, a challenge arises in determining the student network architecture rather than considering it inadvertently.","To address this challenge, we propose an algorithm that utilizes the cost function to calculate pruning scores, addressing small network architecture search issues, and jointly fine-tunes the network resulting from each pruning stage using KD.","Ultimately, we evaluate our proposed ideas on two real-world datasets, PeMSD7 and PeMSD8.","The results indicate that our method can maintain the student's accuracy close to that of the teacher, even with the retention of only $3\\%$ of network parameters."],"url":"http://arxiv.org/abs/2401.11798v1"}
{"created":"2024-01-22 09:53:20","title":"Local Agnostic Video Explanations: a Study on the Applicability of Removal-Based Explanations to Video","abstract":"Explainable artificial intelligence techniques are becoming increasingly important with the rise of deep learning applications in various domains. These techniques aim to provide a better understanding of complex \"black box\" models and enhance user trust while maintaining high learning performance. While many studies have focused on explaining deep learning models in computer vision for image input, video explanations remain relatively unexplored due to the temporal dimension's complexity. In this paper, we present a unified framework for local agnostic explanations in the video domain. Our contributions include: (1) Extending a fine-grained explanation framework tailored for computer vision data, (2) Adapting six existing explanation techniques to work on video data by incorporating temporal information and enabling local explanations, and (3) Conducting an evaluation and comparison of the adapted explanation methods using different models and datasets. We discuss the possibilities and choices involved in the removal-based explanation process for visual data. The adaptation of six explanation methods for video is explained, with comparisons to existing approaches. We evaluate the performance of the methods using automated metrics and user-based evaluation, showing that 3D RISE, 3D LIME, and 3D Kernel SHAP outperform other methods. By decomposing the explanation process into manageable steps, we facilitate the study of each choice's impact and allow for further refinement of explanation methods to suit specific datasets and models.","sentences":["Explainable artificial intelligence techniques are becoming increasingly important with the rise of deep learning applications in various domains.","These techniques aim to provide a better understanding of complex \"black box\" models and enhance user trust while maintaining high learning performance.","While many studies have focused on explaining deep learning models in computer vision for image input, video explanations remain relatively unexplored due to the temporal dimension's complexity.","In this paper, we present a unified framework for local agnostic explanations in the video domain.","Our contributions include: (1) Extending a fine-grained explanation framework tailored for computer vision data, (2) Adapting six existing explanation techniques to work on video data by incorporating temporal information and enabling local explanations, and (3) Conducting an evaluation and comparison of the adapted explanation methods using different models and datasets.","We discuss the possibilities and choices involved in the removal-based explanation process for visual data.","The adaptation of six explanation methods for video is explained, with comparisons to existing approaches.","We evaluate the performance of the methods using automated metrics and user-based evaluation, showing that 3D RISE, 3D LIME, and 3D Kernel SHAP outperform other methods.","By decomposing the explanation process into manageable steps, we facilitate the study of each choice's impact and allow for further refinement of explanation methods to suit specific datasets and models."],"url":"http://arxiv.org/abs/2401.11796v1"}
{"created":"2024-01-22 09:52:37","title":"Spherical Density-Equalizing Map for Genus-0 Closed Surfaces","abstract":"Density-equalizing maps are a class of mapping methods in which the shape deformation is driven by prescribed density information. In recent years, they have been widely used for data visualization on planar domains and planar parameterization of open surfaces. However, the theory and computation of density-equalizing maps for closed surfaces are much less explored. In this work, we develop a novel method for computing spherical density-equalizing maps for genus-0 closed surfaces. Specifically, we first compute a conformal parameterization of the given genus-0 closed surface onto the unit sphere. Then, we perform density equalization on the spherical domain based on the given density information to achieve a spherical density-equalizing map. The bijectivity of the mapping is guaranteed using quasi-conformal theory. We further propose a method for incorporating the harmonic energy and landmark constraints into our formulation to achieve landmark-aligned spherical density-equalizing maps balancing different distortion measures. Using the proposed methods, a large variety of spherical parameterizations can be achieved. Applications to surface registration, remeshing, and data visualization are presented to demonstrate the effectiveness of our methods.","sentences":["Density-equalizing maps are a class of mapping methods in which the shape deformation is driven by prescribed density information.","In recent years, they have been widely used for data visualization on planar domains and planar parameterization of open surfaces.","However, the theory and computation of density-equalizing maps for closed surfaces are much less explored.","In this work, we develop a novel method for computing spherical density-equalizing maps for genus-0 closed surfaces.","Specifically, we first compute a conformal parameterization of the given genus-0 closed surface onto the unit sphere.","Then, we perform density equalization on the spherical domain based on the given density information to achieve a spherical density-equalizing map.","The bijectivity of the mapping is guaranteed using quasi-conformal theory.","We further propose a method for incorporating the harmonic energy and landmark constraints into our formulation to achieve landmark-aligned spherical density-equalizing maps balancing different distortion measures.","Using the proposed methods, a large variety of spherical parameterizations can be achieved.","Applications to surface registration, remeshing, and data visualization are presented to demonstrate the effectiveness of our methods."],"url":"http://arxiv.org/abs/2401.11795v1"}
{"created":"2024-01-22 09:41:05","title":"SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation","abstract":"Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using training image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may only capture discriminative image regions of target object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Class-associated Semantic Refinement to learn the prompts that adequately describe and suppress the image backgrounds associated with each target object category. In this way, our proposed framework is able to perform better semantic matching between object regions and the associated text labels, resulting in desired pseudo masks for training the segmentation model. The proposed SemPLeS framework achieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS COCO, and demonstrated interpretability with the semantic visualization of our learned prompts. The codes will be released.","sentences":["Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using training image data with only image-level supervision.","Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps.","However, the produced heatmaps may only capture discriminative image regions of target object categories or the associated co-occurring backgrounds.","To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP space to enhance the semantic alignment between the segmented regions and the target object categories.","More specifically, we propose Contrastive Prompt Learning and Class-associated Semantic Refinement to learn the prompts that adequately describe and suppress the image backgrounds associated with each target object category.","In this way, our proposed framework is able to perform better semantic matching between object regions and the associated text labels, resulting in desired pseudo masks for training the segmentation model.","The proposed SemPLeS framework achieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS COCO, and demonstrated interpretability with the semantic visualization of our learned prompts.","The codes will be released."],"url":"http://arxiv.org/abs/2401.11791v1"}
{"created":"2024-01-22 09:40:52","title":"Deep Learning for Computer Vision based Activity Recognition and Fall Detection of the Elderly: a Systematic Review","abstract":"As the percentage of elderly people in developed countries increases worldwide, the healthcare of this collective is a worrying matter, especially if it includes the preservation of their autonomy. In this direction, many studies are being published on Ambient Assisted Living (AAL) systems, which help to reduce the preoccupations raised by the independent living of the elderly. In this study, a systematic review of the literature is presented on fall detection and Human Activity Recognition (HAR) for the elderly, as the two main tasks to solve to guarantee the safety of elderly people living alone. To address the current tendency to perform these two tasks, the review focuses on the use of Deep Learning (DL) based approaches on computer vision data. In addition, different collections of data like DL models, datasets or hardware (e.g. depth or thermal cameras) are gathered from the reviewed studies and provided for reference in future studies. Strengths and weaknesses of existing approaches are also discussed and, based on them, our recommendations for future works are provided.","sentences":["As the percentage of elderly people in developed countries increases worldwide, the healthcare of this collective is a worrying matter, especially if it includes the preservation of their autonomy.","In this direction, many studies are being published on Ambient Assisted Living (AAL) systems, which help to reduce the preoccupations raised by the independent living of the elderly.","In this study, a systematic review of the literature is presented on fall detection and Human Activity Recognition (HAR) for the elderly, as the two main tasks to solve to guarantee the safety of elderly people living alone.","To address the current tendency to perform these two tasks, the review focuses on the use of Deep Learning (DL) based approaches on computer vision data.","In addition, different collections of data like DL models, datasets or hardware (e.g. depth or thermal cameras) are gathered from the reviewed studies and provided for reference in future studies.","Strengths and weaknesses of existing approaches are also discussed and, based on them, our recommendations for future works are provided."],"url":"http://arxiv.org/abs/2401.11790v1"}
{"created":"2024-01-22 09:29:42","title":"Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective","abstract":"Estimating 3D full-body pose from sparse sensor data is a pivotal technique employed for the reconstruction of realistic human motions in Augmented Reality and Virtual Reality. However, translating sparse sensor signals into comprehensive human motion remains a challenge since the sparsely distributed sensors in common VR systems fail to capture the motion of full human body. In this paper, we use well-designed Body Pose Graph (BPG) to represent the human body and translate the challenge into a prediction problem of graph missing nodes. Then, we propose a novel full-body motion reconstruction framework based on BPG. To establish BPG, nodes are initially endowed with features extracted from sparse sensor signals. Features from identifiable joint nodes across diverse sensors are amalgamated and processed from both temporal and spatial perspectives. Temporal dynamics are captured using the Temporal Pyramid Structure, while spatial relations in joint movements inform the spatial attributes. The resultant features serve as the foundational elements of the BPG nodes. To further refine the BPG, node features are updated through a graph neural network that incorporates edge reflecting varying joint relations. Our method's effectiveness is evidenced by the attained state-of-the-art performance, particularly in lower body motion, outperforming other baseline methods. Additionally, an ablation study validates the efficacy of each module in our proposed framework.","sentences":["Estimating 3D full-body pose from sparse sensor data is a pivotal technique employed for the reconstruction of realistic human motions in Augmented Reality and Virtual Reality.","However, translating sparse sensor signals into comprehensive human motion remains a challenge since the sparsely distributed sensors in common VR systems fail to capture the motion of full human body.","In this paper, we use well-designed Body Pose Graph (BPG) to represent the human body and translate the challenge into a prediction problem of graph missing nodes.","Then, we propose a novel full-body motion reconstruction framework based on BPG.","To establish BPG, nodes are initially endowed with features extracted from sparse sensor signals.","Features from identifiable joint nodes across diverse sensors are amalgamated and processed from both temporal and spatial perspectives.","Temporal dynamics are captured using the Temporal Pyramid Structure, while spatial relations in joint movements inform the spatial attributes.","The resultant features serve as the foundational elements of the BPG nodes.","To further refine the BPG, node features are updated through a graph neural network that incorporates edge reflecting varying joint relations.","Our method's effectiveness is evidenced by the attained state-of-the-art performance, particularly in lower body motion, outperforming other baseline methods.","Additionally, an ablation study validates the efficacy of each module in our proposed framework."],"url":"http://arxiv.org/abs/2401.11783v1"}
{"created":"2024-01-22 08:47:20","title":"Data-oriented Coordinated Uplink Transmission for Massive IoT System","abstract":"Recently, the paradigm of massive ultra-reliable low-latency IoT communications (URLLC-IoT) has gained growing interest. Reliable delay-critical uplink transmission in IoT is a challenging task since low-complex devices typically do not support multiple antennas or demanding signal processing tasks. However, in many IoT services the data volumes are small and deployments may include massive number of devices. We consider on a clustered uplink transmission with two cooperation approaches: First, we focus on scenario where location-based channel knowledge map (CKM) is applied to enable cooperation. Second, we consider a scenario where scarce channel side-information is applied in transmission. In both scenarios we also model and analyse the impact of erroneous information. In the performance evaluation we apply the recently introduced data-oriented approach that has gathered significant attention in the context of short-packet transmissions. Specifically, it introduces a transient performance metric for small data transmissions, where the amount of data and available bandwidth play crucial roles. Results show that cooperation between clustered IoT devices may provide notable benefits in terms of increased range. It is noticed that the performance is heavily depending on the strength of the static channel component in the CKM based cooperation. The channel side-information based cooperation is robust against changes in the radio environment but sensitive to possible errors in the channel side-information. Even with large IoT device clusters, side-information errors may set a limit for the use of services assuming high-reliability and low-latency. Analytic results are verified against simulations, showing only minor differences at low probability levels.","sentences":["Recently, the paradigm of massive ultra-reliable low-latency IoT communications (URLLC-IoT) has gained growing interest.","Reliable delay-critical uplink transmission in IoT is a challenging task since low-complex devices typically do not support multiple antennas or demanding signal processing tasks.","However, in many IoT services the data volumes are small and deployments may include massive number of devices.","We consider on a clustered uplink transmission with two cooperation approaches: First, we focus on scenario where location-based channel knowledge map (CKM) is applied to enable cooperation.","Second, we consider a scenario where scarce channel side-information is applied in transmission.","In both scenarios we also model and analyse the impact of erroneous information.","In the performance evaluation we apply the recently introduced data-oriented approach that has gathered significant attention in the context of short-packet transmissions.","Specifically, it introduces a transient performance metric for small data transmissions, where the amount of data and available bandwidth play crucial roles.","Results show that cooperation between clustered IoT devices may provide notable benefits in terms of increased range.","It is noticed that the performance is heavily depending on the strength of the static channel component in the CKM based cooperation.","The channel side-information based cooperation is robust against changes in the radio environment but sensitive to possible errors in the channel side-information.","Even with large IoT device clusters, side-information errors may set a limit for the use of services assuming high-reliability and low-latency.","Analytic results are verified against simulations, showing only minor differences at low probability levels."],"url":"http://arxiv.org/abs/2401.11761v1"}
{"created":"2024-01-22 08:45:29","title":"Towards Effective and General Graph Unlearning via Mutual Evolution","abstract":"With the rapid advancement of AI applications, the growing needs for data privacy and model robustness have highlighted the importance of machine unlearning, especially in thriving graph-based scenarios. However, most existing graph unlearning strategies primarily rely on well-designed architectures or manual process, rendering them less user-friendly and posing challenges in terms of deployment efficiency. Furthermore, striking a balance between unlearning performance and framework generalization is also a pivotal concern. To address the above issues, we propose \\underline{\\textbf{M}}utual \\underline{\\textbf{E}}volution \\underline{\\textbf{G}}raph \\underline{\\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm that simultaneously evolves the predictive and unlearning capacities of graph unlearning. By incorporating aforementioned two components, MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements. Extensive experiments on 9 graph benchmark datasets demonstrate the superior performance of MEGU in addressing unlearning requirements at the feature, node, and edge levels. Specifically, MEGU achieves average performance improvements of 2.7\\%, 2.5\\%, and 3.2\\% across these three levels of unlearning tasks when compared to state-of-the-art baselines. Furthermore, MEGU exhibits satisfactory training efficiency, reducing time and space overhead by an average of 159.8x and 9.6x, respectively, in comparison to retraining GNN from scratch.","sentences":["With the rapid advancement of AI applications, the growing needs for data privacy and model robustness have highlighted the importance of machine unlearning, especially in thriving graph-based scenarios.","However, most existing graph unlearning strategies primarily rely on well-designed architectures or manual process, rendering them less user-friendly and posing challenges in terms of deployment efficiency.","Furthermore, striking a balance between unlearning performance and framework generalization is also a pivotal concern.","To address the above issues, we propose \\underline{\\textbf{M}}utual \\underline{\\textbf{E}}volution \\underline{\\textbf{G}}raph \\underline{\\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm that simultaneously evolves the predictive and unlearning capacities of graph unlearning.","By incorporating aforementioned two components, MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements.","Extensive experiments on 9 graph benchmark datasets demonstrate the superior performance of MEGU in addressing unlearning requirements at the feature, node, and edge levels.","Specifically, MEGU achieves average performance improvements of 2.7\\%, 2.5\\%, and 3.2\\% across these three levels of unlearning tasks when compared to state-of-the-art baselines.","Furthermore, MEGU exhibits satisfactory training efficiency, reducing time and space overhead by an average of 159.8x and 9.6x, respectively, in comparison to retraining GNN from scratch."],"url":"http://arxiv.org/abs/2401.11760v1"}
{"created":"2024-01-22 08:40:32","title":"Integrated Sensing, Communication, and Computing: An Information-oriented Resource Transaction Mechanism","abstract":"Information acquisition from target perception represents the key enabling technology of the Internet of Automatic Vehicles (IoAV), which is essential for the decision-making and control operation of connected automatic vehicles (CAVs). Exploring target information involves multiple operations on data, e.g., wireless sensing (for data acquisition), communication (for data transmission), and computing (for data analysis), which all rely on the consumption of time-space-frequency-computing (TSFC) multi-domain resources. Due to the coupled resource sharing of sensing, communication, and computing procedures, the resource management of information-oriented IoAV is commonly formulated as a non-convex NP-hard problem. In this article, further combining the integrated sensing and communication (ISAC) and computing, we introduce the integrated sensing, communication, and computing (ISCC), wherein the TSFC resources are decoupled from the specific processes and shared universally among sensing, communication, and computing processes. Furthermore, the information-oriented resource trading platform (IRTP) is established, which transforms the problem of ISCC resource management into a resource-information substitution model. Finally, we embed the employment topology structure in IoAV into neural network architecture, taking advantage of the graph neural network (GNN) and multi-worker reinforcement learning, and propose the dynamic resource management strategy based on the asynchronous advantage GNN (A2GNN) algorithm, which can achieve the convergence both of information gain maximization and resource consumption minimization, realizing efficient information-oriented resource management.","sentences":["Information acquisition from target perception represents the key enabling technology of the Internet of Automatic Vehicles (IoAV), which is essential for the decision-making and control operation of connected automatic vehicles (CAVs).","Exploring target information involves multiple operations on data, e.g., wireless sensing (for data acquisition), communication (for data transmission), and computing (for data analysis), which all rely on the consumption of time-space-frequency-computing (TSFC) multi-domain resources.","Due to the coupled resource sharing of sensing, communication, and computing procedures, the resource management of information-oriented IoAV is commonly formulated as a non-convex NP-hard problem.","In this article, further combining the integrated sensing and communication (ISAC) and computing, we introduce the integrated sensing, communication, and computing (ISCC), wherein the TSFC resources are decoupled from the specific processes and shared universally among sensing, communication, and computing processes.","Furthermore, the information-oriented resource trading platform (IRTP) is established, which transforms the problem of ISCC resource management into a resource-information substitution model.","Finally, we embed the employment topology structure in IoAV into neural network architecture, taking advantage of the graph neural network (GNN) and multi-worker reinforcement learning, and propose the dynamic resource management strategy based on the asynchronous advantage GNN (A2GNN) algorithm, which can achieve the convergence both of information gain maximization and resource consumption minimization, realizing efficient information-oriented resource management."],"url":"http://arxiv.org/abs/2401.11759v1"}
{"created":"2024-01-22 08:23:31","title":"AdaFGL: A New Paradigm for Federated Node Classification with Topology Heterogeneity","abstract":"Recently, Federated Graph Learning (FGL) has attracted significant attention as a distributed framework based on graph neural networks, primarily due to its capability to break data silos. Existing FGL studies employ community split on the homophilous global graph by default to simulate federated semi-supervised node classification settings. Such a strategy assumes the consistency of topology between the multi-client subgraphs and the global graph, where connected nodes are highly likely to possess similar feature distributions and the same label. However, in real-world implementations, the varying perspectives of local data engineering result in various subgraph topologies, posing unique heterogeneity challenges in FGL. Unlike the well-known label Non-independent identical distribution (Non-iid) problems in federated learning, FGL heterogeneity essentially reveals the topological divergence among multiple clients, namely homophily or heterophily. To simulate and handle this unique challenge, we introduce the concept of structure Non-iid split and then present a new paradigm called \\underline{Ada}ptive \\underline{F}ederated \\underline{G}raph \\underline{L}earning (AdaFGL), a decoupled two-step personalized approach. To begin with, AdaFGL employs standard multi-client federated collaborative training to acquire the federated knowledge extractor by aggregating uploaded models in the final round at the server. Then, each client conducts personalized training based on the local subgraph and the federated knowledge extractor. Extensive experiments on the 12 graph benchmark datasets validate the superior performance of AdaFGL over state-of-the-art baselines. Specifically, in terms of test accuracy, our proposed AdaFGL outperforms baselines by significant margins of 3.24\\% and 5.57\\% on community split and structure Non-iid split, respectively.","sentences":["Recently, Federated Graph Learning (FGL) has attracted significant attention as a distributed framework based on graph neural networks, primarily due to its capability to break data silos.","Existing FGL studies employ community split on the homophilous global graph by default to simulate federated semi-supervised node classification settings.","Such a strategy assumes the consistency of topology between the multi-client subgraphs and the global graph, where connected nodes are highly likely to possess similar feature distributions and the same label.","However, in real-world implementations, the varying perspectives of local data engineering result in various subgraph topologies, posing unique heterogeneity challenges in FGL.","Unlike the well-known label Non-independent identical distribution (Non-iid) problems in federated learning, FGL heterogeneity essentially reveals the topological divergence among multiple clients, namely homophily or heterophily.","To simulate and handle this unique challenge, we introduce the concept of structure Non-iid split and then present a new paradigm called \\underline{Ada}ptive \\underline{F}ederated \\underline{G}raph \\underline{L}earning (AdaFGL), a decoupled two-step personalized approach.","To begin with, AdaFGL employs standard multi-client federated collaborative training to acquire the federated knowledge extractor by aggregating uploaded models in the final round at the server.","Then, each client conducts personalized training based on the local subgraph and the federated knowledge extractor.","Extensive experiments on the 12 graph benchmark datasets validate the superior performance of AdaFGL over state-of-the-art baselines.","Specifically, in terms of test accuracy, our proposed AdaFGL outperforms baselines by significant margins of 3.24\\% and 5.57\\% on community split and structure Non-iid split, respectively."],"url":"http://arxiv.org/abs/2401.11750v1"}
{"created":"2024-01-22 08:20:47","title":"GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?","abstract":"Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients. However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL. In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images. The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss. Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8\\% data of ImageNet, while GAN-based methods necessitate over 70\\%. Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods. Our approach significantly alleviates the auxiliary data requirement on both amount and distribution in gradient inversion attacks, hence posing more substantial threat to real-world FL.","sentences":["Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients.","However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL.","In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model.","GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images.","The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss.","Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8\\% data of ImageNet, while GAN-based methods necessitate over 70\\%.","Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods.","Our approach significantly alleviates the auxiliary data requirement on both amount and distribution in gradient inversion attacks, hence posing more substantial threat to real-world FL."],"url":"http://arxiv.org/abs/2401.11748v1"}
{"created":"2024-01-22 08:00:49","title":"Knowledge Navigation: Inferring the Interlocking Map of Knowledge from Research Trajectories","abstract":"\"If I have seen further, it is by standing on the shoulders of giants,\" Isaac Newton's renowned statement hints that new knowledge builds upon existing foundations, which means there exists an interdependent relationship between knowledge, which, yet uncovered, is implied in the historical development of scientific systems for hundreds of years. By leveraging natural language processing techniques, this study introduces an innovative embedding scheme designed to infer the \"knowledge interlocking map.\" This map, derived from the research trajectories of millions of scholars, reveals the intricate connections among knowledge. We validate that the inferred map effectively delineates disciplinary boundaries and captures the intricate relationships between diverse concepts. The utility of the interlocking map is showcased through multiple applications. Firstly, we demonstrated the multi-step analogy inferences within the knowledge space and the functional connectivity between concepts in different disciplines. Secondly, we trace the evolution of knowledge across domains, observing trends such as shifts from \"Theoretical\" to \"Applied\" or \"Chemistry\" to \"Biomedical\" along predefined functional directions. Lastly, by analyzing the high-dimensional knowledge network structure, we found that knowledge connects each other with shorter global pathways, and the interdisciplinary knowledge plays a critical role in accessibility of the global knowledge network. Our framework offers a novel approach to mining knowledge inheritance pathways in extensive scientific literature, which is of great significance for understanding scientific development patterns, tailoring scientific learning trajectories, and accelerating scientific progress.","sentences":["\"If I have seen further, it is by standing on the shoulders of giants,\" Isaac Newton's renowned statement hints that new knowledge builds upon existing foundations, which means there exists an interdependent relationship between knowledge, which, yet uncovered, is implied in the historical development of scientific systems for hundreds of years.","By leveraging natural language processing techniques, this study introduces an innovative embedding scheme designed to infer the \"knowledge interlocking map.\"","This map, derived from the research trajectories of millions of scholars, reveals the intricate connections among knowledge.","We validate that the inferred map effectively delineates disciplinary boundaries and captures the intricate relationships between diverse concepts.","The utility of the interlocking map is showcased through multiple applications.","Firstly, we demonstrated the multi-step analogy inferences within the knowledge space and the functional connectivity between concepts in different disciplines.","Secondly, we trace the evolution of knowledge across domains, observing trends such as shifts from \"Theoretical\" to \"Applied\" or \"Chemistry\" to \"Biomedical\" along predefined functional directions.","Lastly, by analyzing the high-dimensional knowledge network structure, we found that knowledge connects each other with shorter global pathways, and the interdisciplinary knowledge plays a critical role in accessibility of the global knowledge network.","Our framework offers a novel approach to mining knowledge inheritance pathways in extensive scientific literature, which is of great significance for understanding scientific development patterns, tailoring scientific learning trajectories, and accelerating scientific progress."],"url":"http://arxiv.org/abs/2401.11742v1"}
{"created":"2024-01-22 07:29:22","title":"Sphractal: Estimating the Fractal Dimension of Surfaces Computed from Precise Atomic Coordinates via Box-Counting Algorithm","abstract":"The fractal dimension of a surface allows its degree of roughness to be characterised quantitatively. However, limited effort has been attempted to compute the fractal dimension of surfaces computed from precisely known atomic coordinates from computational biomolecular and nanomaterial studies. This work proposes methods to estimate the fractal dimension of the surface of any three-dimensional object composed of spheres, by representing it as either a voxelised point cloud or a mathematically exact surface, and computing its box-counting dimension. Sphractal is published as a Python package that provides these functionalities, and its utility is demonstrated on a set of simulated palladium nanoparticle data.","sentences":["The fractal dimension of a surface allows its degree of roughness to be characterised quantitatively.","However, limited effort has been attempted to compute the fractal dimension of surfaces computed from precisely known atomic coordinates from computational biomolecular and nanomaterial studies.","This work proposes methods to estimate the fractal dimension of the surface of any three-dimensional object composed of spheres, by representing it as either a voxelised point cloud or a mathematically exact surface, and computing its box-counting dimension.","Sphractal is published as a Python package that provides these functionalities, and its utility is demonstrated on a set of simulated palladium nanoparticle data."],"url":"http://arxiv.org/abs/2401.11737v1"}
{"created":"2024-01-22 07:24:15","title":"Attention on Personalized Clinical Decision Support System: Federated Learning Approach","abstract":"Health management has become a primary problem as new kinds of diseases and complex symptoms are introduced to a rapidly growing modern society. Building a better and smarter healthcare infrastructure is one of the ultimate goals of a smart city. To the best of our knowledge, neural network models are already employed to assist healthcare professionals in achieving this goal. Typically, training a neural network requires a rich amount of data but heterogeneous and vulnerable properties of clinical data introduce a challenge for the traditional centralized network. Moreover, adding new inputs to a medical database requires re-training an existing model from scratch. To tackle these challenges, we proposed a deep learning-based clinical decision support system trained and managed under a federated learning paradigm. We focused on a novel strategy to guarantee the safety of patient privacy and overcome the risk of cyberattacks while enabling large-scale clinical data mining. As a result, we can leverage rich clinical data for training each local neural network without the need for exchanging the confidential data of patients. Moreover, we implemented the proposed scheme as a sequence-to-sequence model architecture integrating the attention mechanism. Thus, our objective is to provide a personalized clinical decision support system with evolvable characteristics that can deliver accurate solutions and assist healthcare professionals in medical diagnosing.","sentences":["Health management has become a primary problem as new kinds of diseases and complex symptoms are introduced to a rapidly growing modern society.","Building a better and smarter healthcare infrastructure is one of the ultimate goals of a smart city.","To the best of our knowledge, neural network models are already employed to assist healthcare professionals in achieving this goal.","Typically, training a neural network requires a rich amount of data but heterogeneous and vulnerable properties of clinical data introduce a challenge for the traditional centralized network.","Moreover, adding new inputs to a medical database requires re-training an existing model from scratch.","To tackle these challenges, we proposed a deep learning-based clinical decision support system trained and managed under a federated learning paradigm.","We focused on a novel strategy to guarantee the safety of patient privacy and overcome the risk of cyberattacks while enabling large-scale clinical data mining.","As a result, we can leverage rich clinical data for training each local neural network without the need for exchanging the confidential data of patients.","Moreover, we implemented the proposed scheme as a sequence-to-sequence model architecture integrating the attention mechanism.","Thus, our objective is to provide a personalized clinical decision support system with evolvable characteristics that can deliver accurate solutions and assist healthcare professionals in medical diagnosing."],"url":"http://arxiv.org/abs/2401.11736v1"}
{"created":"2024-01-22 07:07:06","title":"Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models","abstract":"Symbols (or more broadly, non-natural language textual representations) such as numerical sequences, molecular formulas, and table delimiters widely exist, playing important roles in various tasks such as abstract reasoning, chemical property prediction, and table question answering. Despite the impressive natural language comprehension capabilities of large language models (LLMs), their reasoning abilities for symbols remain inadequate, which could attributed to the difference between symbol representations and general natural languages. We propose symbol-to-language (S2L), a tuning-free method that enables large language models to solve symbol-related problems with information expressed in natural language. Specifically, S2L first converts the symbols involved to language-based representations, which can be implemented by prompting LLMs or leveraging external tools, then these language-based representations are integrated into the original problem via direct substitution or concatenation, serving as useful input information for LLMs. We evaluate the S2L method using both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment analysis in social media. Experimental results show that S2L consistently leads to superior performance. For example, by employing S2L for GPT-4, there can be average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and Dyck language, respectively. Codes and data are available at https://github.com/THUNLP-MT/symbol2language.","sentences":["Symbols (or more broadly, non-natural language textual representations) such as numerical sequences, molecular formulas, and table delimiters widely exist, playing important roles in various tasks such as abstract reasoning, chemical property prediction, and table question answering.","Despite the impressive natural language comprehension capabilities of large language models (LLMs), their reasoning abilities for symbols remain inadequate, which could attributed to the difference between symbol representations and general natural languages.","We propose symbol-to-language (S2L), a tuning-free method that enables large language models to solve symbol-related problems with information expressed in natural language.","Specifically, S2L first converts the symbols involved to language-based representations, which can be implemented by prompting LLMs or leveraging external tools, then these language-based representations are integrated into the original problem via direct substitution or concatenation, serving as useful input information for LLMs.","We evaluate the S2L method using both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment analysis in social media.","Experimental results show that S2L consistently leads to superior performance.","For example, by employing S2L for GPT-4, there can be average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and Dyck language, respectively.","Codes and data are available at https://github.com/THUNLP-MT/symbol2language."],"url":"http://arxiv.org/abs/2401.11725v1"}
{"created":"2024-01-22 06:56:52","title":"Augmenting Prototype Network with TransMix for Few-shot Hyperspectral Image Classification","abstract":"Few-shot hyperspectral image classification aims to identify the classes of each pixel in the images by only marking few of these pixels. And in order to obtain the spatial-spectral joint features of each pixel, the fixed-size patches centering around each pixel are often used for classification. However, observing the classification results of existing methods, we found that boundary patches corresponding to the pixels which are located at the boundary of the objects in the hyperspectral images, are hard to classify. These boundary patchs are mixed with multi-class spectral information. Inspired by this, we propose to augment the prototype network with TransMix for few-shot hyperspectrial image classification(APNT). While taking the prototype network as the backbone, it adopts the transformer as feature extractor to learn the pixel-to-pixel relation and pay different attentions to different pixels. At the same time, instead of directly using the patches which are cut from the hyperspectral images for training, it randomly mixs up two patches to imitate the boundary patches and uses the synthetic patches to train the model, with the aim to enlarge the number of hard training samples and enhance their diversity. And by following the data agumentation technique TransMix, the attention returned by the transformer is also used to mix up the labels of two patches to generate better labels for synthetic patches. Compared with existing methods, the proposed method has demonstrated sate of the art performance and better robustness for few-shot hyperspectral image classification in our experiments.","sentences":["Few-shot hyperspectral image classification aims to identify the classes of each pixel in the images by only marking few of these pixels.","And in order to obtain the spatial-spectral joint features of each pixel, the fixed-size patches centering around each pixel are often used for classification.","However, observing the classification results of existing methods, we found that boundary patches corresponding to the pixels which are located at the boundary of the objects in the hyperspectral images, are hard to classify.","These boundary patchs are mixed with multi-class spectral information.","Inspired by this, we propose to augment the prototype network with TransMix for few-shot hyperspectrial image classification(APNT).","While taking the prototype network as the backbone, it adopts the transformer as feature extractor to learn the pixel-to-pixel relation and pay different attentions to different pixels.","At the same time, instead of directly using the patches which are cut from the hyperspectral images for training, it randomly mixs up two patches to imitate the boundary patches and uses the synthetic patches to train the model, with the aim to enlarge the number of hard training samples and enhance their diversity.","And by following the data agumentation technique TransMix, the attention returned by the transformer is also used to mix up the labels of two patches to generate better labels for synthetic patches.","Compared with existing methods, the proposed method has demonstrated sate of the art performance and better robustness for few-shot hyperspectral image classification in our experiments."],"url":"http://arxiv.org/abs/2401.11724v1"}
{"created":"2024-01-22 06:52:35","title":"Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them","abstract":"The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as adversary models, attack targets, and key security attributes (confidentiality, availability, and integrity). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this paper seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.","sentences":["The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025.","These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains.","Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem.","For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities.","This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks.","Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as adversary models, attack targets, and key security attributes (confidentiality, availability, and integrity).","We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts.","Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy.","We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape.","Through our comprehensive review and analysis, this paper seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT."],"url":"http://arxiv.org/abs/2401.11723v1"}
{"created":"2024-01-22 06:47:00","title":"Graph Condensation: A Survey","abstract":"The burgeoning volume of graph data poses significant challenges in storage, transmission, and particularly the training of graph neural networks (GNNs). To address these challenges, graph condensation (GC) has emerged as an innovative solution. GC focuses on synthesizing a compact yet highly representative graph, on which GNNs can achieve performance comparable to trained on the large original graph. The notable efficacy of GC and its broad prospects have garnered significant attention and spurred extensive research. This survey paper provides an up-to-date and systematic overview of GC, organizing existing research into four categories aligned with critical GC evaluation criteria: effectiveness, generalization, fairness, and efficiency. To facilitate an in-depth and comprehensive understanding of GC, we examine various methods under each category and thoroughly discuss two essential components within GC: optimization strategies and condensed graph generation. Additionally, we introduce the applications of GC in a variety of fields, and highlight the present challenges and novel insights in GC, promoting advancements in future research.","sentences":["The burgeoning volume of graph data poses significant challenges in storage, transmission, and particularly the training of graph neural networks (GNNs).","To address these challenges, graph condensation (GC) has emerged as an innovative solution.","GC focuses on synthesizing a compact yet highly representative graph, on which GNNs can achieve performance comparable to trained on the large original graph.","The notable efficacy of GC and its broad prospects have garnered significant attention and spurred extensive research.","This survey paper provides an up-to-date and systematic overview of GC, organizing existing research into four categories aligned with critical GC evaluation criteria: effectiveness, generalization, fairness, and efficiency.","To facilitate an in-depth and comprehensive understanding of GC, we examine various methods under each category and thoroughly discuss two essential components within GC: optimization strategies and condensed graph generation.","Additionally, we introduce the applications of GC in a variety of fields, and highlight the present challenges and novel insights in GC, promoting advancements in future research."],"url":"http://arxiv.org/abs/2401.11720v1"}
{"created":"2024-01-22 06:43:13","title":"SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation","abstract":"Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost. Existing methods mainly rely on Class Activation Mapping (CAM) to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-/tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at https://github.com/Barrett-python/SFC.","sentences":["Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost.","Existing methods mainly rely on Class Activation Mapping (CAM) to obtain pseudo-labels for training semantic segmentation models.","In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes.","This degrades pseudo-label quality and further influences final semantic segmentation performance.","To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation.","Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training.","The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-/tail-class classifier weights.","Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances.","The project is available at https://github.com/Barrett-python/SFC."],"url":"http://arxiv.org/abs/2401.11719v1"}
{"created":"2024-01-22 05:44:43","title":"Admission Prediction in Undergraduate Applications: an Interpretable Deep Learning Approach","abstract":"This article addresses the challenge of validating the admission committee's decisions for undergraduate admissions. In recent years, the traditional review process has struggled to handle the overwhelmingly large amount of applicants' data. Moreover, this traditional assessment often leads to human bias, which might result in discrimination among applicants. Although classical machine learning-based approaches exist that aim to verify the quantitative assessment made by the application reviewers, these methods lack scalability and suffer from performance issues when a large volume of data is in place. In this context, we propose deep learning-based classifiers, namely Feed-Forward and Input Convex neural networks, which overcome the challenges faced by the existing methods. Furthermore, we give additional insights into our model by incorporating an interpretability module, namely LIME. Our training and test datasets comprise applicants' data with a wide range of variables and information. Our models achieve higher accuracy compared to the best-performing traditional machine learning-based approach by a considerable margin of 3.03\\%. Additionally, we show the sensitivity of different features and their relative impacts on the overall admission decision using the LIME technique.","sentences":["This article addresses the challenge of validating the admission committee's decisions for undergraduate admissions.","In recent years, the traditional review process has struggled to handle the overwhelmingly large amount of applicants' data.","Moreover, this traditional assessment often leads to human bias, which might result in discrimination among applicants.","Although classical machine learning-based approaches exist that aim to verify the quantitative assessment made by the application reviewers, these methods lack scalability and suffer from performance issues when a large volume of data is in place.","In this context, we propose deep learning-based classifiers, namely Feed-Forward and Input Convex neural networks, which overcome the challenges faced by the existing methods.","Furthermore, we give additional insights into our model by incorporating an interpretability module, namely LIME.","Our training and test datasets comprise applicants' data with a wide range of variables and information.","Our models achieve higher accuracy compared to the best-performing traditional machine learning-based approach by a considerable margin of 3.03\\%.","Additionally, we show the sensitivity of different features and their relative impacts on the overall admission decision using the LIME technique."],"url":"http://arxiv.org/abs/2401.11698v1"}
{"created":"2024-01-22 05:26:18","title":"Parametric Matrix Models","abstract":"We present a general class of machine learning algorithms called parametric matrix models. Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations. The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations. Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed. While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows for parameter extrapolation and interpretability.","sentences":["We present a general class of machine learning algorithms called parametric matrix models.","Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations.","The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations.","Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed.","While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems.","After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems.","For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows for parameter extrapolation and interpretability."],"url":"http://arxiv.org/abs/2401.11694v1"}
{"created":"2024-01-22 04:54:42","title":"TIM: An Efficient Temporal Interaction Module for Spiking Transformer","abstract":"Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets. The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers. These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets. Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs. To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures. TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while significantly boosting their temporal information handling capabilities. Through rigorous experimentation, TIM has demonstrated its effectiveness in exploiting temporal information, leading to state-of-the-art performance across various neuromorphic datasets.","sentences":["Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets.","The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers.","These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets.","Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs.","To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures.","TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while significantly boosting their temporal information handling capabilities.","Through rigorous experimentation, TIM has demonstrated its effectiveness in exploiting temporal information, leading to state-of-the-art performance across various neuromorphic datasets."],"url":"http://arxiv.org/abs/2401.11687v1"}
{"created":"2024-01-22 03:07:24","title":"An Improved Grey Wolf Optimization Algorithm for Heart Disease Prediction","abstract":"This paper presents a unique solution to challenges in medical image processing by incorporating an adaptive curve grey wolf optimization (ACGWO) algorithm into neural network backpropagation. Neural networks show potential in medical data but suffer from issues like overfitting and lack of interpretability due to imbalanced and scarce data. Traditional Gray Wolf Optimization (GWO) also has its drawbacks, such as a lack of population diversity and premature convergence. This paper addresses these problems by introducing an adaptive algorithm, enhancing the standard GWO with a sigmoid function. This algorithm was extensively compared to four leading algorithms using six well-known test functions, outperforming them effectively. Moreover, by utilizing the ACGWO, we increase the robustness and generalization of the neural network, resulting in more interpretable predictions. Applied to the publicly accessible Cleveland Heart Disease dataset, our technique surpasses ten other methods, achieving 86.8% accuracy, indicating its potential for efficient heart disease prediction in the clinical setting.","sentences":["This paper presents a unique solution to challenges in medical image processing by incorporating an adaptive curve grey wolf optimization (ACGWO) algorithm into neural network backpropagation.","Neural networks show potential in medical data but suffer from issues like overfitting and lack of interpretability due to imbalanced and scarce data.","Traditional Gray Wolf Optimization (GWO) also has its drawbacks, such as a lack of population diversity and premature convergence.","This paper addresses these problems by introducing an adaptive algorithm, enhancing the standard GWO with a sigmoid function.","This algorithm was extensively compared to four leading algorithms using six well-known test functions, outperforming them effectively.","Moreover, by utilizing the ACGWO, we increase the robustness and generalization of the neural network, resulting in more interpretable predictions.","Applied to the publicly accessible Cleveland Heart Disease dataset, our technique surpasses ten other methods, achieving 86.8% accuracy, indicating its potential for efficient heart disease prediction in the clinical setting."],"url":"http://arxiv.org/abs/2401.11669v1"}
{"created":"2024-01-22 02:42:36","title":"\"I Got Flagged for Supposed Bullying, Even Though It Was in Response to Someone Harassing Me About My Disability.\": A Study of Blind TikTokers' Content Moderation Experiences","abstract":"The Human-Computer Interaction (HCI) community has consistently focused on the experiences of users moderated by social media platforms. Recently, scholars have noticed that moderation practices could perpetuate biases, resulting in the marginalization of user groups undergoing moderation. However, most studies have primarily addressed marginalization related to issues such as racism or sexism, with little attention given to the experiences of people with disabilities. In this paper, we present a study on the moderation experiences of blind users on TikTok, also known as \"BlindToker,\" to address this gap. We conducted semi-structured interviews with 20 BlindTokers and used thematic analysis to analyze the data. Two main themes emerged: BlindTokers' situated content moderation experiences and their reactions to content moderation. We reported on the lack of accessibility on TikTok's platform, contributing to the moderation and marginalization of BlindTokers. Additionally, we discovered instances of harassment from trolls that prompted BlindTokers to respond with harsh language, triggering further moderation. We discussed these findings in the context of the literature on moderation, marginalization, and transformative justice, seeking solutions to address such issues.","sentences":["The Human-Computer Interaction (HCI) community has consistently focused on the experiences of users moderated by social media platforms.","Recently, scholars have noticed that moderation practices could perpetuate biases, resulting in the marginalization of user groups undergoing moderation.","However, most studies have primarily addressed marginalization related to issues such as racism or sexism, with little attention given to the experiences of people with disabilities.","In this paper, we present a study on the moderation experiences of blind users on TikTok, also known as \"BlindToker,\" to address this gap.","We conducted semi-structured interviews with 20 BlindTokers and used thematic analysis to analyze the data.","Two main themes emerged: BlindTokers' situated content moderation experiences and their reactions to content moderation.","We reported on the lack of accessibility on TikTok's platform, contributing to the moderation and marginalization of BlindTokers.","Additionally, we discovered instances of harassment from trolls that prompted BlindTokers to respond with harsh language, triggering further moderation.","We discussed these findings in the context of the literature on moderation, marginalization, and transformative justice, seeking solutions to address such issues."],"url":"http://arxiv.org/abs/2401.11663v1"}
{"created":"2024-01-22 02:33:38","title":"Differentiable Tree Search in Latent State Space","abstract":"In decision-making problems with limited training data, policy functions approximated using deep neural networks often exhibit suboptimal performance. An alternative approach involves learning a world model from the limited data and determining actions through online search. However, the performance is adversely affected by compounding errors arising from inaccuracies in the learnt world model. While methods like TreeQN have attempted to address these inaccuracies by incorporating algorithmic structural biases into their architectures, the biases they introduce are often weak and insufficient for complex decision-making tasks. In this work, we introduce Differentiable Tree Search (DTS), a novel neural network architecture that significantly strengthens the inductive bias by embedding the algorithmic structure of a best-first online search algorithm. DTS employs a learnt world model to conduct a fully differentiable online search in latent state space. The world model is jointly optimised with the search algorithm, enabling the learning of a robust world model and mitigating the effect of model inaccuracies. We address potential Q-function discontinuities arising from naive incorporation of best-first search by adopting a stochastic tree expansion policy, formulating search tree expansion as a decision-making task, and introducing an effective variance reduction technique for the gradient computation. We evaluate DTS in an offline-RL setting with a limited training data scenario on Procgen games and grid navigation task, and demonstrate that DTS outperforms popular model-free and model-based baselines.","sentences":["In decision-making problems with limited training data, policy functions approximated using deep neural networks often exhibit suboptimal performance.","An alternative approach involves learning a world model from the limited data and determining actions through online search.","However, the performance is adversely affected by compounding errors arising from inaccuracies in the learnt world model.","While methods like TreeQN have attempted to address these inaccuracies by incorporating algorithmic structural biases into their architectures, the biases they introduce are often weak and insufficient for complex decision-making tasks.","In this work, we introduce Differentiable Tree Search (DTS), a novel neural network architecture that significantly strengthens the inductive bias by embedding the algorithmic structure of a best-first online search algorithm.","DTS employs a learnt world model to conduct a fully differentiable online search in latent state space.","The world model is jointly optimised with the search algorithm, enabling the learning of a robust world model and mitigating the effect of model inaccuracies.","We address potential Q-function discontinuities arising from naive incorporation of best-first search by adopting a stochastic tree expansion policy, formulating search tree expansion as a decision-making task, and introducing an effective variance reduction technique for the gradient computation.","We evaluate DTS in an offline-RL setting with a limited training data scenario on Procgen games and grid navigation task, and demonstrate that DTS outperforms popular model-free and model-based baselines."],"url":"http://arxiv.org/abs/2401.11660v1"}
{"created":"2024-01-22 02:17:36","title":"OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated learning","abstract":"Federated learning (FL) has emerged as a promising approach to collaboratively train machine learning models across multiple edge devices while preserving privacy. The success of FL hinges on the efficiency of participating models and their ability to handle the unique challenges of distributed learning. While several variants of Vision Transformer (ViT) have shown great potential as alternatives to modern convolutional neural networks (CNNs) for centralized training, the unprecedented size and higher computational demands hinder their deployment on resource-constrained edge devices, challenging their widespread application in FL. Since client devices in FL typically have limited computing resources and communication bandwidth, models intended for such devices must strike a balance between model size, computational efficiency, and the ability to adapt to the diverse and non-IID data distributions encountered in FL. To address these challenges, we propose OnDev-LCT: Lightweight Convolutional Transformers for On-Device vision tasks with limited training data and resources. Our models incorporate image-specific inductive biases through the LCT tokenizer by leveraging efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features, while the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global representations of images. Extensive experiments on benchmark image datasets indicate that our models outperform existing lightweight vision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data heterogeneity and communication bottlenecks.","sentences":["Federated learning (FL) has emerged as a promising approach to collaboratively train machine learning models across multiple edge devices while preserving privacy.","The success of FL hinges on the efficiency of participating models and their ability to handle the unique challenges of distributed learning.","While several variants of Vision Transformer (ViT) have shown great potential as alternatives to modern convolutional neural networks (CNNs) for centralized training, the unprecedented size and higher computational demands hinder their deployment on resource-constrained edge devices, challenging their widespread application in FL.","Since client devices in FL typically have limited computing resources and communication bandwidth, models intended for such devices must strike a balance between model size, computational efficiency, and the ability to adapt to the diverse and non-IID data distributions encountered in FL.","To address these challenges, we propose OnDev-LCT: Lightweight Convolutional Transformers for On-Device vision tasks with limited training data and resources.","Our models incorporate image-specific inductive biases through the LCT tokenizer by leveraging efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features, while the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global representations of images.","Extensive experiments on benchmark image datasets indicate that our models outperform existing lightweight vision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data heterogeneity and communication bottlenecks."],"url":"http://arxiv.org/abs/2401.11652v1"}
{"created":"2024-01-22 01:58:32","title":"Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation","abstract":"Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.","sentences":["Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients.","Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance.","To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation.","First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation.","We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data.","A series of experiments on MIMIC-III data demonstrates effectiveness of our approach."],"url":"http://arxiv.org/abs/2401.11648v1"}
{"created":"2024-01-22 01:57:31","title":"LW-FedSSL: Resource-efficient Layer-wise Federated Self-supervised Learning","abstract":"Many recent studies integrate federated learning (FL) with self-supervised learning (SSL) to take advantage of raw training data distributed across edge devices. However, edge devices often struggle with high computation and communication costs imposed by SSL and FL algorithms. To tackle this hindrance, we propose LW-FedSSL, a layer-wise federated self-supervised learning approach that allows edge devices to incrementally train one layer of the model at a time. LW-FedSSL comprises server-side calibration and representation alignment mechanisms to maintain comparable performance with end-to-end FedSSL while significantly lowering clients' resource requirements. The server-side calibration mechanism takes advantage of the resource-rich server in an FL environment to assist in global model training. Meanwhile, the representation alignment mechanism encourages closeness between representations of FL local models and those of the global model. Our experiments show that LW-FedSSL has a $3.3 \\times$ lower memory requirement and a $3.2 \\times$ cheaper communication cost than its end-to-end counterpart. We also explore a progressive training strategy called Prog-FedSSL that outperforms end-to-end training with a similar memory requirement and a $1.8 \\times$ cheaper communication cost.","sentences":["Many recent studies integrate federated learning (FL) with self-supervised learning (SSL) to take advantage of raw training data distributed across edge devices.","However, edge devices often struggle with high computation and communication costs imposed by SSL and FL algorithms.","To tackle this hindrance, we propose LW-FedSSL, a layer-wise federated self-supervised learning approach that allows edge devices to incrementally train one layer of the model at a time.","LW-FedSSL comprises server-side calibration and representation alignment mechanisms to maintain comparable performance with end-to-end FedSSL while significantly lowering clients' resource requirements.","The server-side calibration mechanism takes advantage of the resource-rich server in an FL environment to assist in global model training.","Meanwhile, the representation alignment mechanism encourages closeness between representations of FL local models and those of the global model.","Our experiments show that LW-FedSSL has a $3.3 \\times$ lower memory requirement and a $3.2 \\times$ cheaper communication cost than its end-to-end counterpart.","We also explore a progressive training strategy called Prog-FedSSL that outperforms end-to-end training with a similar memory requirement and a $1.8 \\times$ cheaper communication cost."],"url":"http://arxiv.org/abs/2401.11647v1"}
{"created":"2024-01-22 01:06:17","title":"Revolutionizing Finance with LLMs: An Overview of Applications and Insights","abstract":"In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields. Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively. In the financial domain, the deployment of LLMs is gaining momentum. These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction. In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks. Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions. Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks. This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry.","sentences":["In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields.","Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively.","In the financial domain, the deployment of LLMs is gaining momentum.","These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice.","Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction.","In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks.","Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions.","Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks.","This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry."],"url":"http://arxiv.org/abs/2401.11641v1"}
{"created":"2024-01-22 00:00:30","title":"Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss","abstract":"The fusion of vision and language has brought about a transformative shift in computer vision through the emergence of Vision-Language Models (VLMs). However, the resource-intensive nature of existing VLMs poses a significant challenge. We need an accessible method for developing the next generation of VLMs. To address this issue, we propose Zoom-shot, a novel method for transferring the zero-shot capabilities of CLIP to any pre-trained vision encoder. We do this by exploiting the multimodal information (i.e. text and image) present in the CLIP latent space through the use of specifically designed multimodal loss functions. These loss functions are (1) cycle-consistency loss and (2) our novel prompt-guided knowledge distillation loss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's zero-shot classification, to capture the interactions between text and image features. With our multimodal losses, we train a $\\textbf{linear mapping}$ between the CLIP latent space and the latent space of a pre-trained vision encoder, for only a $\\textbf{single epoch}$. Furthermore, Zoom-shot is entirely unsupervised and is trained using $\\textbf{unpaired}$ data. We test the zero-shot capabilities of a range of vision encoders augmented as new VLMs, on coarse and fine-grained classification datasets, outperforming the previous state-of-the-art in this problem domain. In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; and our state-of-the-art results can be obtained by reducing training from 20% to 1% of the ImageNet training data with 20 epochs. All code and models are available on GitHub.","sentences":["The fusion of vision and language has brought about a transformative shift in computer vision through the emergence of Vision-Language Models (VLMs).","However, the resource-intensive nature of existing VLMs poses a significant challenge.","We need an accessible method for developing the next generation of VLMs.","To address this issue, we propose Zoom-shot, a novel method for transferring the zero-shot capabilities of CLIP to any pre-trained vision encoder.","We do this by exploiting the multimodal information (i.e. text and image) present in the CLIP latent space through the use of specifically designed multimodal loss functions.","These loss functions are (1) cycle-consistency loss and (2) our novel prompt-guided knowledge distillation loss (PG-KD).","PG-KD combines the concept of knowledge distillation with CLIP's zero-shot classification, to capture the interactions between text and image features.","With our multimodal losses, we train a $\\textbf{linear mapping}$ between the CLIP latent space and the latent space of a pre-trained vision encoder, for only a $\\textbf{single epoch}$.","Furthermore, Zoom-shot is entirely unsupervised and is trained using $\\textbf{unpaired}$ data.","We test the zero-shot capabilities of a range of vision encoders augmented as new VLMs, on coarse and fine-grained classification datasets, outperforming the previous state-of-the-art in this problem domain.","In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; and our state-of-the-art results can be obtained by reducing training from 20% to 1% of the ImageNet training data with 20 epochs.","All code and models are available on GitHub."],"url":"http://arxiv.org/abs/2401.11633v1"}
{"created":"2024-01-21 23:56:57","title":"What Are We Optimizing For? A Human-centric Evaluation Of Deep Learning-based Recommender Systems","abstract":"Deep learning-based (DL) models in recommender systems (RecSys) have gained significant recognition for their remarkable accuracy in predicting user preferences. However, their performance often lacks a comprehensive evaluation from a human-centric perspective, which encompasses various dimensions beyond simple interest matching. In this work, we have developed a robust human-centric evaluation framework that incorporates seven diverse metrics to assess the quality of recommendations generated by five recent open-sourced DL models. Our evaluation datasets consist of both offline benchmark data and personalized online recommendation feedback collected from 445 real users. We find that (1) different DL models have different pros and cons in the multi-dimensional metrics that we test with; (2) users generally want a combination of accuracy with at least one another human values in the recommendation; (3) the degree of combination of different values needs to be carefully experimented to user preferred level.","sentences":["Deep learning-based (DL) models in recommender systems (RecSys) have gained significant recognition for their remarkable accuracy in predicting user preferences.","However, their performance often lacks a comprehensive evaluation from a human-centric perspective, which encompasses various dimensions beyond simple interest matching.","In this work, we have developed a robust human-centric evaluation framework that incorporates seven diverse metrics to assess the quality of recommendations generated by five recent open-sourced DL models.","Our evaluation datasets consist of both offline benchmark data and personalized online recommendation feedback collected from 445 real users.","We find that (1) different DL models have different pros and cons in the multi-dimensional metrics that we test with; (2) users generally want a combination of accuracy with at least one another human values in the recommendation; (3) the degree of combination of different values needs to be carefully experimented to user preferred level."],"url":"http://arxiv.org/abs/2401.11632v1"}
{"created":"2024-01-21 23:54:05","title":"Text-to-Image Cross-Modal Generation: A Systematic Review","abstract":"We review research on generating visual data from text from the angle of \"cross-modal generation.\" This point of view allows us to draw parallels between various methods geared towards working on input text and producing visual output, without limiting the analysis to narrow sub-areas. It also results in the identification of common templates in the field, which are then compared and contrasted both within pools of similar methods and across lines of research. We provide a breakdown of text-to-image generation into various flavors of image-from-text methods, video-from-text methods, image editing, self-supervised and graph-based approaches. In this discussion, we focus on research papers published at 8 leading machine learning conferences in the years 2016-2022, also incorporating a number of relevant papers not matching the outlined search criteria. The conducted review suggests a significant increase in the number of papers published in the area and highlights research gaps and potential lines of investigation. To our knowledge, this is the first review to systematically look at text-to-image generation from the perspective of \"cross-modal generation.\"","sentences":["We review research on generating visual data from text from the angle of \"cross-modal generation.\"","This point of view allows us to draw parallels between various methods geared towards working on input text and producing visual output, without limiting the analysis to narrow sub-areas.","It also results in the identification of common templates in the field, which are then compared and contrasted both within pools of similar methods and across lines of research.","We provide a breakdown of text-to-image generation into various flavors of image-from-text methods, video-from-text methods, image editing, self-supervised and graph-based approaches.","In this discussion, we focus on research papers published at 8 leading machine learning conferences in the years 2016-2022, also incorporating a number of relevant papers not matching the outlined search criteria.","The conducted review suggests a significant increase in the number of papers published in the area and highlights research gaps and potential lines of investigation.","To our knowledge, this is the first review to systematically look at text-to-image generation from the perspective of \"cross-modal generation.\""],"url":"http://arxiv.org/abs/2401.11631v1"}
{"created":"2024-01-21 22:50:44","title":"A Survey on African Computer Vision Datasets, Topics and Researchers","abstract":"Computer vision encompasses a range of tasks such as object detection, semantic segmentation, and 3D reconstruction. Despite its relevance to African communities, research in this field within Africa represents only 0.06% of top-tier publications over the past decade. This study undertakes a thorough analysis of 63,000 Scopus-indexed computer vision publications from Africa, spanning from 2012 to 2022. The aim is to provide a survey of African computer vision topics, datasets and researchers. A key aspect of our study is the identification and categorization of African Computer Vision datasets using large language models that automatically parse abstracts of these publications. We also provide a compilation of unofficial African Computer Vision datasets distributed through challenges or data hosting platforms, and provide a full taxonomy of dataset categories. Our survey also pinpoints computer vision topics trends specific to different African regions, indicating their unique focus areas. Additionally, we carried out an extensive survey to capture the views of African researchers on the current state of computer vision research in the continent and the structural barriers they believe need urgent attention. In conclusion, this study catalogs and categorizes Computer Vision datasets and topics contributed or initiated by African institutions and identifies barriers to publishing in top-tier Computer Vision venues. This survey underscores the importance of encouraging African researchers and institutions in advancing computer vision research in the continent. It also stresses on the need for research topics to be more aligned with the needs of African communities.","sentences":["Computer vision encompasses a range of tasks such as object detection, semantic segmentation, and 3D reconstruction.","Despite its relevance to African communities, research in this field within Africa represents only 0.06% of top-tier publications over the past decade.","This study undertakes a thorough analysis of 63,000 Scopus-indexed computer vision publications from Africa, spanning from 2012 to 2022.","The aim is to provide a survey of African computer vision topics, datasets and researchers.","A key aspect of our study is the identification and categorization of African Computer Vision datasets using large language models that automatically parse abstracts of these publications.","We also provide a compilation of unofficial African Computer Vision datasets distributed through challenges or data hosting platforms, and provide a full taxonomy of dataset categories.","Our survey also pinpoints computer vision topics trends specific to different African regions, indicating their unique focus areas.","Additionally, we carried out an extensive survey to capture the views of African researchers on the current state of computer vision research in the continent and the structural barriers they believe need urgent attention.","In conclusion, this study catalogs and categorizes Computer Vision datasets and topics contributed or initiated by African institutions and identifies barriers to publishing in top-tier Computer Vision venues.","This survey underscores the importance of encouraging African researchers and institutions in advancing computer vision research in the continent.","It also stresses on the need for research topics to be more aligned with the needs of African communities."],"url":"http://arxiv.org/abs/2401.11617v1"}
{"created":"2024-01-21 22:18:29","title":"Continuous Field Reconstruction from Sparse Observations with Implicit Neural Networks","abstract":"Reliably reconstructing physical fields from sparse sensor data is a challenge that frequently arises in many scientific domains. In practice, the process generating the data often is not understood to sufficient accuracy. Therefore, there is a growing interest in using the deep neural network route to address the problem. This work presents a novel approach that learns a continuous representation of the physical field using implicit neural representations (INRs). Specifically, after factorizing spatiotemporal variability into spatial and temporal components using the separation of variables technique, the method learns relevant basis functions from sparsely sampled irregular data points to develop a continuous representation of the data. In experimental evaluations, the proposed model outperforms recent INR methods, offering superior reconstruction quality on simulation data from a state-of-the-art climate model and a second dataset that comprises ultra-high resolution satellite-based sea surface temperature fields.","sentences":["Reliably reconstructing physical fields from sparse sensor data is a challenge that frequently arises in many scientific domains.","In practice, the process generating the data often is not understood to sufficient accuracy.","Therefore, there is a growing interest in using the deep neural network route to address the problem.","This work presents a novel approach that learns a continuous representation of the physical field using implicit neural representations (INRs).","Specifically, after factorizing spatiotemporal variability into spatial and temporal components using the separation of variables technique, the method learns relevant basis functions from sparsely sampled irregular data points to develop a continuous representation of the data.","In experimental evaluations, the proposed model outperforms recent INR methods, offering superior reconstruction quality on simulation data from a state-of-the-art climate model and a second dataset that comprises ultra-high resolution satellite-based sea surface temperature fields."],"url":"http://arxiv.org/abs/2401.11611v1"}
{"created":"2024-01-21 22:11:29","title":"Graph Edits for Counterfactual Explanations: A Unified GNN Approach","abstract":"Counterfactuals have been established as a popular explainability technique which leverages a set of minimal edits to alter the prediction of a classifier. When considering conceptual counterfactuals, the edits requested should correspond to salient concepts present in the input data. At the same time, conceptual distances are defined by knowledge graphs, ensuring the optimality of conceptual edits. In this work, we extend previous endeavors on conceptual counterfactuals by introducing \\textit{graph edits as counterfactual explanations}: should we represent input data as graphs, which is the shortest graph edit path that results in an alternative classification label as provided by a black-box classifier?","sentences":["Counterfactuals have been established as a popular explainability technique which leverages a set of minimal edits to alter the prediction of a classifier.","When considering conceptual counterfactuals, the edits requested should correspond to salient concepts present in the input data.","At the same time, conceptual distances are defined by knowledge graphs, ensuring the optimality of conceptual edits.","In this work, we extend previous endeavors on conceptual counterfactuals by introducing \\textit{graph edits as counterfactual explanations}: should we represent input data as graphs, which is the shortest graph edit path that results in an alternative classification label as provided by a black-box classifier?"],"url":"http://arxiv.org/abs/2401.11609v1"}
{"created":"2024-01-21 21:05:32","title":"Reducing Usefulness of Stolen Credentials in SSO Contexts","abstract":"Approximately 61% of cyber attacks involve adversaries in possession of valid credentials. Attackers acquire credentials through various means, including phishing, dark web data drops, password reuse, etc. Multi-factor authentication (MFA) helps to thwart attacks that use valid credentials, but attackers still commonly breach systems by tricking users into accepting MFA step up requests through techniques, such as ``MFA Bombing'', where multiple requests are sent to a user until they accept one. Currently, there are several solutions to this problem, each with varying levels of security and increasing invasiveness on user devices. This paper proposes a token-based enrollment architecture that is less invasive to user devices than mobile device management, but still offers strong protection against use of stolen credentials and MFA attacks.","sentences":["Approximately 61% of cyber attacks involve adversaries in possession of valid credentials.","Attackers acquire credentials through various means, including phishing, dark web data drops, password reuse, etc.","Multi-factor authentication (MFA) helps to thwart attacks that use valid credentials, but attackers still commonly breach systems by tricking users into accepting MFA step up requests through techniques, such as ``MFA Bombing'', where multiple requests are sent to a user until they accept one.","Currently, there are several solutions to this problem, each with varying levels of security and increasing invasiveness on user devices.","This paper proposes a token-based enrollment architecture that is less invasive to user devices than mobile device management, but still offers strong protection against use of stolen credentials and MFA attacks."],"url":"http://arxiv.org/abs/2401.11599v1"}
{"created":"2024-01-21 21:04:05","title":"TetraLoss: Improving the Robustness of Face Recognition against Morphing Attacks","abstract":"Face recognition systems are widely deployed in high-security applications such as for biometric verification at border controls. Despite their high accuracy on pristine data, it is well-known that digital manipulations, such as face morphing, pose a security threat to face recognition systems. Malicious actors can exploit the facilities offered by the identity document issuance process to obtain identity documents containing morphed images. Thus, subjects who contributed to the creation of the morphed image can with high probability use the identity document to bypass automated face recognition systems. In recent years, no-reference (i.e., single image) and differential morphing attack detectors have been proposed to tackle this risk. These systems are typically evaluated in isolation from the face recognition system that they have to operate jointly with and do not consider the face recognition process. Contrary to most existing works, we present a novel method for adapting deep learning-based face recognition systems to be more robust against face morphing attacks. To this end, we introduce TetraLoss, a novel loss function that learns to separate morphed face images from its contributing subjects in the embedding space while still preserving high biometric verification performance. In a comprehensive evaluation, we show that the proposed method can significantly enhance the original system while also significantly outperforming other tested baseline methods.","sentences":["Face recognition systems are widely deployed in high-security applications such as for biometric verification at border controls.","Despite their high accuracy on pristine data, it is well-known that digital manipulations, such as face morphing, pose a security threat to face recognition systems.","Malicious actors can exploit the facilities offered by the identity document issuance process to obtain identity documents containing morphed images.","Thus, subjects who contributed to the creation of the morphed image can with high probability use the identity document to bypass automated face recognition systems.","In recent years, no-reference (i.e., single image) and differential morphing attack detectors have been proposed to tackle this risk.","These systems are typically evaluated in isolation from the face recognition system that they have to operate jointly with and do not consider the face recognition process.","Contrary to most existing works, we present a novel method for adapting deep learning-based face recognition systems to be more robust against face morphing attacks.","To this end, we introduce TetraLoss, a novel loss function that learns to separate morphed face images from its contributing subjects in the embedding space while still preserving high biometric verification performance.","In a comprehensive evaluation, we show that the proposed method can significantly enhance the original system while also significantly outperforming other tested baseline methods."],"url":"http://arxiv.org/abs/2401.11598v1"}
{"created":"2024-01-21 20:46:21","title":"Differential Privacy in Hierarchical Federated Learning: A Formal Analysis and Evaluation","abstract":"While federated learning (FL) eliminates the transmission of raw data over a network, it is still vulnerable to privacy breaches from the communicated model parameters. In this work, we formalize Differentially Private Hierarchical Federated Learning (DP-HFL), a DP-enhanced FL methodology that seeks to improve the privacy-utility tradeoff inherent in FL. Building upon recent proposals for Hierarchical Differential Privacy (HDP), one of the key concepts of DP-HFL is adapting DP noise injection at different layers of an established FL hierarchy -- edge devices, edge servers, and cloud servers -- according to the trust models within particular subnetworks. We conduct a comprehensive analysis of the convergence behavior of DP-HFL, revealing conditions on parameter tuning under which the model training process converges sublinearly to a stationarity gap, with this gap depending on the network hierarchy, trust model, and target privacy level. Subsequent numerical evaluations demonstrate that DP-HFL obtains substantial improvements in convergence speed over baselines for different privacy budgets, and validate the impact of network configuration on training.","sentences":["While federated learning (FL) eliminates the transmission of raw data over a network, it is still vulnerable to privacy breaches from the communicated model parameters.","In this work, we formalize Differentially Private Hierarchical Federated Learning (DP-HFL), a DP-enhanced FL methodology that seeks to improve the privacy-utility tradeoff inherent in FL.","Building upon recent proposals for Hierarchical Differential Privacy (HDP), one of the key concepts of DP-HFL is adapting DP noise injection at different layers of an established FL hierarchy -- edge devices, edge servers, and cloud servers -- according to the trust models within particular subnetworks.","We conduct a comprehensive analysis of the convergence behavior of DP-HFL, revealing conditions on parameter tuning under which the model training process converges sublinearly to a stationarity gap, with this gap depending on the network hierarchy, trust model, and target privacy level.","Subsequent numerical evaluations demonstrate that DP-HFL obtains substantial improvements in convergence speed over baselines for different privacy budgets, and validate the impact of network configuration on training."],"url":"http://arxiv.org/abs/2401.11592v1"}
{"created":"2024-01-21 18:43:55","title":"Distributed Multi-Task Learning for Stochastic Bandits with Context Distribution and Stage-wise Constraints","abstract":"We present the problem of conservative distributed multi-task learning in stochastic linear contextual bandits with heterogeneous agents. This extends conservative linear bandits to a distributed setting where M agents tackle different but related tasks while adhering to stage-wise performance constraints. The exact context is unknown, and only a context distribution is available to the agents as in many practical applications that involve a prediction mechanism to infer context, such as stock market prediction and weather forecast. We propose a distributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm constructs a pruned action set during each round to ensure the constraints are met. Additionally, it includes synchronized sharing of estimates among agents via a central server using well-structured synchronization steps. We prove the regret and communication bounds on the algorithm. We extend the problem to a setting where the agents are unaware of the baseline reward. For this setting, we provide a modified algorithm, DiSC-UCB2, and we show that the modified algorithm achieves the same regret and communication bounds. We empirically validated the performance of our algorithm on synthetic data and real-world Movielens-100K data.","sentences":["We present the problem of conservative distributed multi-task learning in stochastic linear contextual bandits with heterogeneous agents.","This extends conservative linear bandits to a distributed setting where M agents tackle different but related tasks while adhering to stage-wise performance constraints.","The exact context is unknown, and only a context distribution is available to the agents as in many practical applications that involve a prediction mechanism to infer context, such as stock market prediction and weather forecast.","We propose a distributed upper confidence bound (UCB) algorithm, DiSC-UCB.","Our algorithm constructs a pruned action set during each round to ensure the constraints are met.","Additionally, it includes synchronized sharing of estimates among agents via a central server using well-structured synchronization steps.","We prove the regret and communication bounds on the algorithm.","We extend the problem to a setting where the agents are unaware of the baseline reward.","For this setting, we provide a modified algorithm, DiSC-UCB2, and we show that the modified algorithm achieves the same regret and communication bounds.","We empirically validated the performance of our algorithm on synthetic data and real-world Movielens-100K data."],"url":"http://arxiv.org/abs/2401.11563v1"}
{"created":"2024-01-21 16:46:04","title":"Multi-View Neural 3D Reconstruction of Micro-/Nanostructures with Atomic Force Microscopy","abstract":"Atomic Force Microscopy (AFM) is a widely employed tool for micro-/nanoscale topographic imaging. However, conventional AFM scanning struggles to reconstruct complex 3D micro-/nanostructures precisely due to limitations such as incomplete sample topography capturing and tip-sample convolution artifacts. Here, we propose a multi-view neural-network-based framework with AFM (MVN-AFM), which accurately reconstructs surface models of intricate micro-/nanostructures. Unlike previous works, MVN-AFM does not depend on any specially shaped probes or costly modifications to the AFM system. To achieve this, MVN-AFM uniquely employs an iterative method to align multi-view data and eliminate AFM artifacts simultaneously. Furthermore, we pioneer the application of neural implicit surface reconstruction in nanotechnology and achieve markedly improved results. Extensive experiments show that MVN-AFM effectively eliminates artifacts present in raw AFM images and reconstructs various micro-/nanostructures including complex geometrical microstructures printed via Two-photon Lithography and nanoparticles such as PMMA nanospheres and ZIF-67 nanocrystals. This work presents a cost-effective tool for micro-/nanoscale 3D analysis.","sentences":["Atomic Force Microscopy (AFM) is a widely employed tool for micro-/nanoscale topographic imaging.","However, conventional AFM scanning struggles to reconstruct complex 3D micro-/nanostructures precisely due to limitations such as incomplete sample topography capturing and tip-sample convolution artifacts.","Here, we propose a multi-view neural-network-based framework with AFM (MVN-AFM), which accurately reconstructs surface models of intricate micro-/nanostructures.","Unlike previous works, MVN-AFM does not depend on any specially shaped probes or costly modifications to the AFM system.","To achieve this, MVN-AFM uniquely employs an iterative method to align multi-view data and eliminate AFM artifacts simultaneously.","Furthermore, we pioneer the application of neural implicit surface reconstruction in nanotechnology and achieve markedly improved results.","Extensive experiments show that MVN-AFM effectively eliminates artifacts present in raw AFM images and reconstructs various micro-/nanostructures including complex geometrical microstructures printed via Two-photon Lithography and nanoparticles such as PMMA nanospheres and ZIF-67 nanocrystals.","This work presents a cost-effective tool for micro-/nanoscale 3D analysis."],"url":"http://arxiv.org/abs/2401.11541v1"}
{"created":"2024-01-21 15:57:04","title":"Tempo: Confidentiality Preservation in Cloud-Based Neural Network Training","abstract":"Cloud deep learning platforms provide cost-effective deep neural network (DNN) training for customers who lack computation resources. However, cloud systems are often untrustworthy and vulnerable to attackers, leading to growing concerns about model privacy. Recently, researchers have sought to protect data privacy in deep learning by leveraging CPU trusted execution environments (TEEs), which minimize the use of cryptography, but existing works failed to simultaneously utilize the computational resources of GPUs to assist in training and prevent model leakage. This paper presents Tempo, the first cloud-based deep learning system that cooperates with TEE and distributed GPUs for efficient DNN training with model confidentiality preserved. To tackle the challenge of preserving privacy while offloading linear algebraic operations from TEE to GPUs for efficient batch computation, we introduce a customized permutation-based obfuscation algorithm to blind both inputs and model parameters. An optimization mechanism that reduces encryption operations is proposed for faster weight updates during backpropagation to speed up training. We implement Tempo and evaluate it with both training and inference for two prevalent DNNs. Empirical results indicate that Tempo outperforms baselines and offers sufficient privacy protection.","sentences":["Cloud deep learning platforms provide cost-effective deep neural network (DNN) training for customers who lack computation resources.","However, cloud systems are often untrustworthy and vulnerable to attackers, leading to growing concerns about model privacy.","Recently, researchers have sought to protect data privacy in deep learning by leveraging CPU trusted execution environments (TEEs), which minimize the use of cryptography, but existing works failed to simultaneously utilize the computational resources of GPUs to assist in training and prevent model leakage.","This paper presents Tempo, the first cloud-based deep learning system that cooperates with TEE and distributed GPUs for efficient DNN training with model confidentiality preserved.","To tackle the challenge of preserving privacy while offloading linear algebraic operations from TEE to GPUs for efficient batch computation, we introduce a customized permutation-based obfuscation algorithm to blind both inputs and model parameters.","An optimization mechanism that reduces encryption operations is proposed for faster weight updates during backpropagation to speed up training.","We implement Tempo and evaluate it with both training and inference for two prevalent DNNs.","Empirical results indicate that Tempo outperforms baselines and offers sufficient privacy protection."],"url":"http://arxiv.org/abs/2401.11531v1"}
{"created":"2024-01-21 15:23:54","title":"Is it a Real CD Mismatch in Interdomain Routing?","abstract":"In inter-domain routing, a packet is not always forwarded along the Autonomous System (AS) level path determined by the BGP routing protocol. This is often called control-plane and data-plane (CD) mismatch, which allows for flexible traffic control, but also leads to operation and security issues. We systematically analyze this phenomenon with path pairs collected from 128 pairs of vantage points over more than 5 years, and use multiple IP-to-AS mapping methods to compare CD paths. What is interesting is that, working at such a large scale in turn helps us design a novel method to fairly evaluate the accuracy of various existing mapping methods, and further develop a new mapping method, i.e., LearnToCorrect, that can correct more than 70\\% mapping errors of the state-of-the-art one. Then we devise to identify real mismatches with LearnToCorrect, and estimate that the real-mismatch ratio in the wild is typically less than 6\\%. At last, we use our proposed methods to detect routing security issues, which are previously difficult to accurately find out.","sentences":["In inter-domain routing, a packet is not always forwarded along the Autonomous System (AS) level path determined by the BGP routing protocol.","This is often called control-plane and data-plane (CD) mismatch, which allows for flexible traffic control, but also leads to operation and security issues.","We systematically analyze this phenomenon with path pairs collected from 128 pairs of vantage points over more than 5 years, and use multiple IP-to-AS mapping methods to compare CD paths.","What is interesting is that, working at such a large scale in turn helps us design a novel method to fairly evaluate the accuracy of various existing mapping methods, and further develop a new mapping method, i.e., LearnToCorrect, that can correct more than 70\\% mapping errors of the state-of-the-art one.","Then we devise to identify real mismatches with LearnToCorrect, and estimate that the real-mismatch ratio in the wild is typically less than 6\\%.","At last, we use our proposed methods to detect routing security issues, which are previously difficult to accurately find out."],"url":"http://arxiv.org/abs/2401.11520v1"}
{"created":"2024-01-21 15:22:15","title":"CaBuAr: California Burned Areas dataset for delineation","abstract":"Forest wildfires represent one of the catastrophic events that, over the last decades, caused huge environmental and humanitarian damages. In addition to a significant amount of carbon dioxide emission, they are a source of risk to society in both short-term (e.g., temporary city evacuation due to fire) and long-term (e.g., higher risks of landslides) cases. Consequently, the availability of tools to support local authorities in automatically identifying burned areas plays an important role in the continuous monitoring requirement to alleviate the aftereffects of such catastrophic events. The great availability of satellite acquisitions coupled with computer vision techniques represents an important step in developing such tools. This paper introduces a novel open dataset that tackles the burned area delineation problem, a binary segmentation problem applied to satellite imagery. The presented resource consists of pre- and post-fire Sentinel-2 L2A acquisitions of California forest fires that took place starting in 2015. Raster annotations were generated from the data released by California's Department of Forestry and Fire Protection. Moreover, in conjunction with the dataset, we release three different baselines based on spectral indexes analyses, SegFormer, and U-Net models.","sentences":["Forest wildfires represent one of the catastrophic events that, over the last decades, caused huge environmental and humanitarian damages.","In addition to a significant amount of carbon dioxide emission, they are a source of risk to society in both short-term (e.g., temporary city evacuation due to fire) and long-term (e.g., higher risks of landslides) cases.","Consequently, the availability of tools to support local authorities in automatically identifying burned areas plays an important role in the continuous monitoring requirement to alleviate the aftereffects of such catastrophic events.","The great availability of satellite acquisitions coupled with computer vision techniques represents an important step in developing such tools.","This paper introduces a novel open dataset that tackles the burned area delineation problem, a binary segmentation problem applied to satellite imagery.","The presented resource consists of pre- and post-fire Sentinel-2 L2A acquisitions of California forest fires that took place starting in 2015.","Raster annotations were generated from the data released by California's Department of Forestry and Fire Protection.","Moreover, in conjunction with the dataset, we release three different baselines based on spectral indexes analyses, SegFormer, and U-Net models."],"url":"http://arxiv.org/abs/2401.11519v1"}
{"created":"2024-01-21 14:51:09","title":"Information-Theoretic State Variable Selection for Reinforcement Learning","abstract":"Identifying the most suitable variables to represent the state is a fundamental challenge in Reinforcement Learning (RL). These variables must efficiently capture the information necessary for making optimal decisions. In order to address this problem, in this paper, we introduce the Transfer Entropy Redundancy Criterion (TERC), an information-theoretic criterion, which determines if there is \\textit{entropy transferred} from state variables to actions during training. We define an algorithm based on TERC that provably excludes variables from the state that have no effect on the final performance of the agent, resulting in more sample efficient learning. Experimental results show that this speed-up is present across three different algorithm classes (represented by tabular Q-learning, Actor-Critic, and Proximal Policy Optimization (PPO)) in a variety of environments. Furthermore, to highlight the differences between the proposed methodology and the current state-of-the-art feature selection approaches, we present a series of controlled experiments on synthetic data, before generalizing to real-world decision-making tasks. We also introduce a representation of the problem that compactly captures the transfer of information from state variables to actions as Bayesian networks.","sentences":["Identifying the most suitable variables to represent the state is a fundamental challenge in Reinforcement Learning (RL).","These variables must efficiently capture the information necessary for making optimal decisions.","In order to address this problem, in this paper, we introduce the Transfer Entropy Redundancy Criterion (TERC), an information-theoretic criterion, which determines if there is \\textit{entropy transferred} from state variables to actions during training.","We define an algorithm based on TERC that provably excludes variables from the state that have no effect on the final performance of the agent, resulting in more sample efficient learning.","Experimental results show that this speed-up is present across three different algorithm classes (represented by tabular Q-learning, Actor-Critic, and Proximal Policy Optimization (PPO)) in a variety of environments.","Furthermore, to highlight the differences between the proposed methodology and the current state-of-the-art feature selection approaches, we present a series of controlled experiments on synthetic data, before generalizing to real-world decision-making tasks.","We also introduce a representation of the problem that compactly captures the transfer of information from state variables to actions as Bayesian networks."],"url":"http://arxiv.org/abs/2401.11512v1"}
{"created":"2024-01-21 14:35:54","title":"Simple Domain Adaptation for Sparse Retrievers","abstract":"In Information Retrieval, and more generally in Natural Language Processing, adapting models to specific domains is conducted through fine-tuning. Despite the successes achieved by this method and its versatility, the need for human-curated and labeled data makes it impractical to transfer to new tasks, domains, and/or languages when training data doesn't exist. Using the model without training (zero-shot) is another option that however suffers an effectiveness cost, especially in the case of first-stage retrievers. Numerous research directions have emerged to tackle these issues, most of them in the context of adapting to a task or a language. However, the literature is scarcer for domain (or topic) adaptation. In this paper, we address this issue of cross-topic discrepancy for a sparse first-stage retriever by transposing a method initially designed for language adaptation. By leveraging pre-training on the target data to learn domain-specific knowledge, this technique alleviates the need for annotated data and expands the scope of domain adaptation. Despite their relatively good generalization ability, we show that even sparse retrievers can benefit from our simple domain adaptation method.","sentences":["In Information Retrieval, and more generally in Natural Language Processing, adapting models to specific domains is conducted through fine-tuning.","Despite the successes achieved by this method and its versatility, the need for human-curated and labeled data makes it impractical to transfer to new tasks, domains, and/or languages when training data doesn't exist.","Using the model without training (zero-shot) is another option that however suffers an effectiveness cost, especially in the case of first-stage retrievers.","Numerous research directions have emerged to tackle these issues, most of them in the context of adapting to a task or a language.","However, the literature is scarcer for domain (or topic) adaptation.","In this paper, we address this issue of cross-topic discrepancy for a sparse first-stage retriever by transposing a method initially designed for language adaptation.","By leveraging pre-training on the target data to learn domain-specific knowledge, this technique alleviates the need for annotated data and expands the scope of domain adaptation.","Despite their relatively good generalization ability, we show that even sparse retrievers can benefit from our simple domain adaptation method."],"url":"http://arxiv.org/abs/2401.11509v1"}
{"created":"2024-01-21 14:30:20","title":"CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray Report Labeling","abstract":"Free-text radiology reports present a rich data source for various medical tasks, but effectively labeling these texts remains challenging. Traditional rule-based labeling methods fall short of capturing the nuances of diverse free-text patterns. Moreover, models using expert-annotated data are limited by data scarcity and pre-defined classes, impacting their performance, flexibility and scalability. To address these issues, our study offers three main contributions: 1) We demonstrate the potential of GPT as an adept labeler using carefully designed prompts. 2) Utilizing only the data labeled by GPT, we trained a BERT-based labeler, CheX-GPT, which operates faster and more efficiently than its GPT counterpart. 3) To benchmark labeler performance, we introduced a publicly available expert-annotated test set, MIMIC-500, comprising 500 cases from the MIMIC validation set. Our findings demonstrate that CheX-GPT not only excels in labeling accuracy over existing models, but also showcases superior efficiency, flexibility, and scalability, supported by our introduction of the MIMIC-500 dataset for robust benchmarking. Code and models are available at https://github.com/kakaobrain/CheXGPT.","sentences":["Free-text radiology reports present a rich data source for various medical tasks, but effectively labeling these texts remains challenging.","Traditional rule-based labeling methods fall short of capturing the nuances of diverse free-text patterns.","Moreover, models using expert-annotated data are limited by data scarcity and pre-defined classes, impacting their performance, flexibility and scalability.","To address these issues, our study offers three main contributions: 1) We demonstrate the potential of GPT as an adept labeler using carefully designed prompts.","2) Utilizing only the data labeled by GPT, we trained a BERT-based labeler, CheX-GPT, which operates faster and more efficiently than its GPT counterpart.","3) To benchmark labeler performance, we introduced a publicly available expert-annotated test set, MIMIC-500, comprising 500 cases from the MIMIC validation set.","Our findings demonstrate that CheX-GPT not only excels in labeling accuracy over existing models, but also showcases superior efficiency, flexibility, and scalability, supported by our introduction of the MIMIC-500 dataset for robust benchmarking.","Code and models are available at https://github.com/kakaobrain/CheXGPT."],"url":"http://arxiv.org/abs/2401.11505v1"}
{"created":"2024-01-21 14:09:49","title":"Self-Supervised Bird's Eye View Motion Prediction with Cross-Modality Signals","abstract":"Learning the dense bird's eye view (BEV) motion flow in a self-supervised manner is an emerging research for robotics and autonomous driving. Current self-supervised methods mainly rely on point correspondences between point clouds, which may introduce the problems of fake flow and inconsistency, hindering the model's ability to learn accurate and realistic motion. In this paper, we introduce a novel cross-modality self-supervised training framework that effectively addresses these issues by leveraging multi-modality data to obtain supervision signals. We design three innovative supervision signals to preserve the inherent properties of scene motion, including the masked Chamfer distance loss, the piecewise rigidity loss, and the temporal consistency loss. Through extensive experiments, we demonstrate that our proposed self-supervised framework outperforms all previous self-supervision methods for the motion prediction task.","sentences":["Learning the dense bird's eye view (BEV) motion flow in a self-supervised manner is an emerging research for robotics and autonomous driving.","Current self-supervised methods mainly rely on point correspondences between point clouds, which may introduce the problems of fake flow and inconsistency, hindering the model's ability to learn accurate and realistic motion.","In this paper, we introduce a novel cross-modality self-supervised training framework that effectively addresses these issues by leveraging multi-modality data to obtain supervision signals.","We design three innovative supervision signals to preserve the inherent properties of scene motion, including the masked Chamfer distance loss, the piecewise rigidity loss, and the temporal consistency loss.","Through extensive experiments, we demonstrate that our proposed self-supervised framework outperforms all previous self-supervision methods for the motion prediction task."],"url":"http://arxiv.org/abs/2401.11499v1"}
{"created":"2024-01-21 13:30:02","title":"MapChange: Enhancing Semantic Change Detection with Temporal-Invariant Historical Maps Based on Deep Triplet Network","abstract":"Semantic Change Detection (SCD) is recognized as both a crucial and challenging task in the field of image analysis. Traditional methods for SCD have predominantly relied on the comparison of image pairs. However, this approach is significantly hindered by substantial imaging differences, which arise due to variations in shooting times, atmospheric conditions, and angles. Such discrepancies lead to two primary issues: the under-detection of minor yet significant changes, and the generation of false alarms due to temporal variances. These factors often result in unchanged objects appearing markedly different in multi-temporal images. In response to these challenges, the MapChange framework has been developed. This framework introduces a novel paradigm that synergizes temporal-invariant historical map data with contemporary high-resolution images. By employing this combination, the temporal variance inherent in conventional image pair comparisons is effectively mitigated. The efficacy of the MapChange framework has been empirically validated through comprehensive testing on two public datasets. These tests have demonstrated the framework's marked superiority over existing state-of-the-art SCD methods.","sentences":["Semantic Change Detection (SCD) is recognized as both a crucial and challenging task in the field of image analysis.","Traditional methods for SCD have predominantly relied on the comparison of image pairs.","However, this approach is significantly hindered by substantial imaging differences, which arise due to variations in shooting times, atmospheric conditions, and angles.","Such discrepancies lead to two primary issues: the under-detection of minor yet significant changes, and the generation of false alarms due to temporal variances.","These factors often result in unchanged objects appearing markedly different in multi-temporal images.","In response to these challenges, the MapChange framework has been developed.","This framework introduces a novel paradigm that synergizes temporal-invariant historical map data with contemporary high-resolution images.","By employing this combination, the temporal variance inherent in conventional image pair comparisons is effectively mitigated.","The efficacy of the MapChange framework has been empirically validated through comprehensive testing on two public datasets.","These tests have demonstrated the framework's marked superiority over existing state-of-the-art SCD methods."],"url":"http://arxiv.org/abs/2401.11489v1"}
{"created":"2024-01-21 13:18:20","title":"Towards Better Inclusivity: A Diverse Tweet Corpus of English Varieties","abstract":"The prevalence of social media presents a growing opportunity to collect and analyse examples of English varieties. Whilst usage of these varieties was - and, in many cases, still is - used only in spoken contexts or hard-to-access private messages, social media sites like Twitter provide a platform for users to communicate informally in a scrapeable format. Notably, Indian English (Hinglish), Singaporean English (Singlish), and African-American English (AAE) can be commonly found online. These varieties pose a challenge to existing natural language processing (NLP) tools as they often differ orthographically and syntactically from standard English for which the majority of these tools are built. NLP models trained on standard English texts produced biased outcomes for users of underrepresented varieties. Some research has aimed to overcome the inherent biases caused by unrepresentative data through techniques like data augmentation or adjusting training models.   We aim to address the issue of bias at its root - the data itself. We curate a dataset of tweets from countries with high proportions of underserved English variety speakers, and propose an annotation framework of six categorical classifications along a pseudo-spectrum that measures the degree of standard English and that thereby indirectly aims to surface the manifestations of English varieties in these tweets. Following best annotation practices, our growing corpus features 170,800 tweets taken from 7 countries, labeled by annotators who are from those countries and can communicate in regionally-dominant varieties of English. Our corpus highlights the accuracy discrepancies in pre-trained language identifiers between western English and non-western (i.e., less standard) English varieties. We hope to contribute to the growing literature identifying and reducing the implicit demographic discrepancies in NLP.","sentences":["The prevalence of social media presents a growing opportunity to collect and analyse examples of English varieties.","Whilst usage of these varieties was - and, in many cases, still is - used only in spoken contexts or hard-to-access private messages, social media sites like Twitter provide a platform for users to communicate informally in a scrapeable format.","Notably, Indian English (Hinglish), Singaporean English (Singlish), and African-American English (AAE) can be commonly found online.","These varieties pose a challenge to existing natural language processing (NLP) tools as they often differ orthographically and syntactically from standard English for which the majority of these tools are built.","NLP models trained on standard English texts produced biased outcomes for users of underrepresented varieties.","Some research has aimed to overcome the inherent biases caused by unrepresentative data through techniques like data augmentation or adjusting training models.   ","We aim to address the issue of bias at its root - the data itself.","We curate a dataset of tweets from countries with high proportions of underserved English variety speakers, and propose an annotation framework of six categorical classifications along a pseudo-spectrum that measures the degree of standard English and that thereby indirectly aims to surface the manifestations of English varieties in these tweets.","Following best annotation practices, our growing corpus features 170,800 tweets taken from 7 countries, labeled by annotators who are from those countries and can communicate in regionally-dominant varieties of English.","Our corpus highlights the accuracy discrepancies in pre-trained language identifiers between western English and non-western (i.e., less standard) English varieties.","We hope to contribute to the growing literature identifying and reducing the implicit demographic discrepancies in NLP."],"url":"http://arxiv.org/abs/2401.11487v1"}
