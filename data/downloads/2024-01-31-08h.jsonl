{"created":"2024-01-30 18:58:43","title":"Weaver: Foundation Models for Creative Writing","abstract":"This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.","sentences":["This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation.","Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models.","We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation.","The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost.","Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them.","Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes.","Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage).","We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance.","Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs."],"url":"http://arxiv.org/abs/2401.17268v1"}
{"created":"2024-01-30 18:49:44","title":"You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation","abstract":"In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step. We propose a novel scale distillation approach to train our SR model. Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher. We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training. This process is repeated iteratively until we reach the target scale factor of the final model. The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve. We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference. Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it. We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step.","sentences":["In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step.","We propose a novel scale distillation approach to train our SR model.","Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher.","We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training.","This process is repeated iteratively until we reach the target scale factor of the final model.","The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve.","We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference.","Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it.","We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step."],"url":"http://arxiv.org/abs/2401.17258v1"}
{"created":"2024-01-30 18:37:45","title":"LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation","abstract":"Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from mixed data sources. Additionally, LLaMP substantially reduces the hallucinated volumetric strain in a diamond cubic silicon structure from 66.3% to 0. The proposed framework offers an intuitive and nearly hallucination-free approach to exploring materials informatics and establishes a pathway for knowledge distillation and fine-tuning other language models. We envision the framework as a valuable component for scientific hypotheses and a foundation for future autonomous laboratories where multiple LLM agents communicate and cooperate with robotics to drive material synthesis and chemical reactions without hard-coded human logic and intervention.","sentences":["Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial.","However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data.","Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP).","Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis.","We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from mixed data sources.","Additionally, LLaMP substantially reduces the hallucinated volumetric strain in a diamond cubic silicon structure from 66.3% to 0.","The proposed framework offers an intuitive and nearly hallucination-free approach to exploring materials informatics and establishes a pathway for knowledge distillation and fine-tuning other language models.","We envision the framework as a valuable component for scientific hypotheses and a foundation for future autonomous laboratories where multiple LLM agents communicate and cooperate with robotics to drive material synthesis and chemical reactions without hard-coded human logic and intervention."],"url":"http://arxiv.org/abs/2401.17244v1"}
{"created":"2024-01-30 18:24:50","title":"Explicit Good Codes Approaching Distance 1 in Ulam Metric","abstract":"The Ulam distance of two permutations on $[n]$ is $n$ minus the length of their longest common subsequence. In this paper, we show that for every $\\varepsilon>0$, there exists some $\\alpha>0$, and an infinite set $\\Gamma\\subseteq \\mathbb{N}$, such that for all $n\\in\\Gamma$, there is an explicit set $C_n$ of $(n!)^{\\alpha}$ many permutations on $[n]$, such that every pair of permutations in $C_n$ has pairwise Ulam distance at least $(1-\\varepsilon)\\cdot n$. Moreover, we can compute the $i^{\\text{th}}$ permutation in $C_n$ in poly$(n)$ time and can also decode in poly$(n)$ time, a permutation $\\pi$ on $[n]$ to its closest permutation $\\pi^*$ in $C_n$, if the Ulam distance of $\\pi$ and $\\pi^*$ is less than $ \\frac{(1-\\varepsilon)\\cdot n}{4} $.   Previously, it was implicitly known by combining works of Goldreich and Wigderson [Israel Journal of Mathematics'23] and Farnoud, Skachek, and Milenkovic [IEEE Transactions on Information Theory'13] in a black-box manner, that it is possible to explicitly construct $(n!)^{\\Omega(1)}$ many permutations on $[n]$, such that every pair of them have pairwise Ulam distance at least $\\frac{n}{6}\\cdot (1-\\varepsilon)$, for any $\\varepsilon>0$, and the bound on the distance can be improved to $\\frac{n}{4}\\cdot (1-\\varepsilon)$ if the construction of Goldreich and Wigderson is directly analyzed in the Ulam metric.","sentences":["The Ulam distance of two permutations on $[n]$ is $n$ minus the length of their longest common subsequence.","In this paper, we show that for every $\\varepsilon>0$, there exists some $\\alpha>0$, and an infinite set $\\Gamma\\subseteq \\mathbb{N}$, such that for all $n\\in\\Gamma$, there is an explicit set $C_n$ of $(n!)^{\\alpha}$ many permutations on $[n]$, such that every pair of permutations in $C_n$ has pairwise Ulam distance at least $(1-\\varepsilon)\\cdot n$. Moreover, we can compute the $i^{\\text{th}}$ permutation in $C_n$ in poly$(n)$ time and can also decode in poly$(n)$ time, a permutation $\\pi$ on $[n]$ to its closest permutation $\\pi^*$ in $C_n$, if the Ulam distance of $\\pi$ and $\\pi^*$ is less than $ \\frac{(1-\\varepsilon)\\cdot n}{4} $.   ","Previously, it was implicitly known by combining works of Goldreich and Wigderson","[Israel Journal of Mathematics'23] and Farnoud, Skachek, and Milenkovic [IEEE Transactions on Information Theory'13] in a black-box manner, that it is possible to explicitly construct $(n!)^{\\Omega(1)}$ many permutations on $[n]$, such that every pair of them have pairwise Ulam distance at least $\\frac{n}{6}\\cdot (1-\\varepsilon)$, for any $\\varepsilon>0$, and the bound on the distance can be improved to $\\frac{n}{4}\\cdot (1-\\varepsilon)$ if the construction of Goldreich and Wigderson is directly analyzed in the Ulam metric."],"url":"http://arxiv.org/abs/2401.17235v1"}
{"created":"2024-01-30 18:18:41","title":"ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment","abstract":"Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models. Addressing this gap, we present, for the first time, \"Re(presentational)Al(ignment)net\", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visual representational patterns across object categories and different neural data modalities. Furthermore, we discover that alignment with human brain representations improves the model's adversarial robustness. Our findings suggest that ReAlnet sets a new precedent in the field, bridging the gap between artificial and human vision, and paving the way for more brain-like artificial intelligence systems.","sentences":["Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains.","Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models.","Addressing this gap, we present, for the first time, \"Re(presentational)Al(ignment)net\", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations.","Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visual representational patterns across object categories and different neural data modalities.","Furthermore, we discover that alignment with human brain representations improves the model's adversarial robustness.","Our findings suggest that ReAlnet sets a new precedent in the field, bridging the gap between artificial and human vision, and paving the way for more brain-like artificial intelligence systems."],"url":"http://arxiv.org/abs/2401.17231v1"}
{"created":"2024-01-30 17:49:53","title":"Self-Supervised Representation Learning for Nerve Fiber Distribution Patterns in 3D-PLI","abstract":"A comprehensive understanding of the organizational principles in the human brain requires, among other factors, well-quantifiable descriptors of nerve fiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a microscopic imaging technique that enables insights into the fine-grained organization of myelinated nerve fibers with high resolution. Descriptors characterizing the fiber architecture observed in 3D-PLI would enable downstream analysis tasks such as multimodal correlation studies, clustering, and mapping. However, best practices for observer-independent characterization of fiber architecture in 3D-PLI are not yet available. To this end, we propose the application of a fully data-driven approach to characterize nerve fiber architecture in 3D-PLI images using self-supervised representation learning. We introduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the spatial neighborhood of texture examples across histological brain sections of a 3D reconstructed volume to sample positive pairs for contrastive learning. We combine this sampling strategy with specifically designed image augmentations to gain robustness to typical variations in 3D-PLI parameter maps. The approach is demonstrated for the 3D reconstructed occipital lobe of a vervet monkey brain. We show that extracted features are highly sensitive to different configurations of nerve fibers, yet robust to variations between consecutive brain sections arising from histological processing. We demonstrate their practical applicability for retrieving clusters of homogeneous fiber architecture and performing data mining for interactively selected templates of specific components of fiber architecture such as U-fibers.","sentences":["A comprehensive understanding of the organizational principles in the human brain requires, among other factors, well-quantifiable descriptors of nerve fiber architecture.","Three-dimensional polarized light imaging (3D-PLI) is a microscopic imaging technique that enables insights into the fine-grained organization of myelinated nerve fibers with high resolution.","Descriptors characterizing the fiber architecture observed in 3D-PLI would enable downstream analysis tasks such as multimodal correlation studies, clustering, and mapping.","However, best practices for observer-independent characterization of fiber architecture in 3D-PLI are not yet available.","To this end, we propose the application of a fully data-driven approach to characterize nerve fiber architecture in 3D-PLI images using self-supervised representation learning.","We introduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the spatial neighborhood of texture examples across histological brain sections of a 3D reconstructed volume to sample positive pairs for contrastive learning.","We combine this sampling strategy with specifically designed image augmentations to gain robustness to typical variations in 3D-PLI parameter maps.","The approach is demonstrated for the 3D reconstructed occipital lobe of a vervet monkey brain.","We show that extracted features are highly sensitive to different configurations of nerve fibers, yet robust to variations between consecutive brain sections arising from histological processing.","We demonstrate their practical applicability for retrieving clusters of homogeneous fiber architecture and performing data mining for interactively selected templates of specific components of fiber architecture such as U-fibers."],"url":"http://arxiv.org/abs/2401.17207v1"}
{"created":"2024-01-30 17:38:48","title":"CPR++: Object Localization via Single Coarse Point Supervision","abstract":"Point-based object localization (POL), which pursues high-performance object sensing under low-cost data annotation, has attracted increased attention. However, the point annotation mode inevitably introduces semantic variance due to the inconsistency of annotated points. Existing POL heavily rely on strict annotation rules, which are difficult to define and apply, to handle the problem. In this study, we propose coarse point refinement (CPR), which to our best knowledge is the first attempt to alleviate semantic variance from an algorithmic perspective. CPR reduces the semantic variance by selecting a semantic centre point in a neighbourhood region to replace the initial annotated point. Furthermore, We design a sampling region estimation module to dynamically compute a sampling region for each object and use a cascaded structure to achieve end-to-end optimization. We further integrate a variance regularization into the structure to concentrate the predicted scores, yielding CPR++. We observe that CPR++ can obtain scale information and further reduce the semantic variance in a global region, thus guaranteeing high-performance object localization. Extensive experiments on four challenging datasets validate the effectiveness of both CPR and CPR++. We hope our work can inspire more research on designing algorithms rather than annotation rules to address the semantic variance problem in POL. The dataset and code will be public at github.com/ucas-vg/PointTinyBenchmark.","sentences":["Point-based object localization (POL), which pursues high-performance object sensing under low-cost data annotation, has attracted increased attention.","However, the point annotation mode inevitably introduces semantic variance due to the inconsistency of annotated points.","Existing POL heavily rely on strict annotation rules, which are difficult to define and apply, to handle the problem.","In this study, we propose coarse point refinement (CPR), which to our best knowledge is the first attempt to alleviate semantic variance from an algorithmic perspective.","CPR reduces the semantic variance by selecting a semantic centre point in a neighbourhood region to replace the initial annotated point.","Furthermore, We design a sampling region estimation module to dynamically compute a sampling region for each object and use a cascaded structure to achieve end-to-end optimization.","We further integrate a variance regularization into the structure to concentrate the predicted scores, yielding CPR++.","We observe that CPR++ can obtain scale information and further reduce the semantic variance in a global region, thus guaranteeing high-performance object localization.","Extensive experiments on four challenging datasets validate the effectiveness of both CPR and CPR++.","We hope our work can inspire more research on designing algorithms rather than annotation rules to address the semantic variance problem in POL.","The dataset and code will be public at github.com/ucas-vg/PointTinyBenchmark."],"url":"http://arxiv.org/abs/2401.17203v1"}
{"created":"2024-01-30 17:31:19","title":"Data-efficient Fine-tuning for LLM-based Recommendation","abstract":"Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.   To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method based on two scores, i.e., influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of sample removal on the overall performance. To achieve low costs of the data pruning process, we use a small-sized surrogate model to replace LLMs to obtain the influence score. Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. Empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, the proposed method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%.","sentences":["Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation.","However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application.","To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data.","We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning.","While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.   ","To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process.","To pursue the two objectives, we propose a novel data pruning method based on two scores, i.e., influence score and effort score, to efficiently identify the influential samples.","Particularly, the influence score is introduced to accurately estimate the influence of sample removal on the overall performance.","To achieve low costs of the data pruning process, we use a small-sized surrogate model to replace LLMs to obtain the influence score.","Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs.","Empirical results on three real-world datasets validate the effectiveness of our proposed method.","In particular, the proposed method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%."],"url":"http://arxiv.org/abs/2401.17197v1"}
{"created":"2024-01-30 17:30:44","title":"Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers","abstract":"In text classification, creating an adversarial example means subtly perturbing a few words in a sentence without changing its meaning, causing it to be misclassified by a classifier. A concerning observation is that a significant portion of adversarial examples generated by existing methods change only one word. This single-word perturbation vulnerability represents a significant weakness in classifiers, which malicious users can exploit to efficiently create a multitude of adversarial examples. This paper studies this problem and makes the following key contributions: (1) We introduce a novel metric \\r{ho} to quantitatively assess a classifier's robustness against single-word perturbation. (2) We present the SP-Attack, designed to exploit the single-word perturbation vulnerability, achieving a higher attack success rate, better preserving sentence meaning, while reducing computation costs compared to state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims to improve \\r{ho} by applying data augmentation in learning. Experimental results on 4 datasets and BERT and distilBERT classifiers show that SP-Defense improves \\r{ho} by 14.6% and 13.9% and decreases the attack success rate of SP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the attack success rate of existing attack methods that involve multiple-word perturbations.","sentences":["In text classification, creating an adversarial example means subtly perturbing a few words in a sentence without changing its meaning, causing it to be misclassified by a classifier.","A concerning observation is that a significant portion of adversarial examples generated by existing methods change only one word.","This single-word perturbation vulnerability represents a significant weakness in classifiers, which malicious users can exploit to efficiently create a multitude of adversarial examples.","This paper studies this problem and makes the following key contributions: (1) We introduce a novel metric \\r{ho} to quantitatively assess a classifier's robustness against single-word perturbation.","(2) We present the SP-Attack, designed to exploit the single-word perturbation vulnerability, achieving a higher attack success rate, better preserving sentence meaning, while reducing computation costs compared to state-of-the-art adversarial methods.","(3) We propose SP-Defense, which aims to improve \\r{ho} by applying data augmentation in learning.","Experimental results on 4 datasets and BERT and distilBERT classifiers show that SP-Defense improves \\r{ho} by 14.6% and 13.9% and decreases the attack success rate of SP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the attack success rate of existing attack methods that involve multiple-word perturbations."],"url":"http://arxiv.org/abs/2401.17196v1"}
{"created":"2024-01-30 17:14:05","title":"Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning","abstract":"While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to learn the alignment between images and multilingual texts. To alleviate CF raised by covariate shift and lexical overlap, we further propose a novel approach that ensures the identical distribution of all token embeddings during initialization and regularizes token embedding learning during training. We construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600 datasets and then evaluate multilingual image-text retrieval performance. Extensive experiments verify the effectiveness of CLL-CLIP and show that our approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on XM3600, and improve various state-of-the-art methods consistently. Our code and data are available at \\url{https://github.com/yangbang18/CLFM}.","sentences":["While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities.","To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability.","In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF).","We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment.","Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences.","It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to learn the alignment between images and multilingual texts.","To alleviate CF raised by covariate shift and lexical overlap, we further propose a novel approach that ensures the identical distribution of all token embeddings during initialization and regularizes token embedding learning during training.","We construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600 datasets and then evaluate multilingual image-text retrieval performance.","Extensive experiments verify the effectiveness of CLL-CLIP and show that our approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on XM3600, and improve various state-of-the-art methods consistently.","Our code and data are available at \\url{https://github.com/yangbang18/CLFM}."],"url":"http://arxiv.org/abs/2401.17186v1"}
{"created":"2024-01-30 17:13:29","title":"Multi-Camera Asynchronous Ball Localization and Trajectory Prediction with Factor Graphs and Human Poses","abstract":"The rapid and precise localization and prediction of a ball are critical for developing agile robots in ball sports, particularly in sports like tennis characterized by high-speed ball movements and powerful spins. The Magnus effect induced by spin adds complexity to trajectory prediction during flight and bounce dynamics upon contact with the ground. In this study, we introduce an innovative approach that combines a multi-camera system with factor graphs for real-time and asynchronous 3D tennis ball localization. Additionally, we estimate hidden states like velocity and spin for trajectory prediction. Furthermore, to enhance spin inference early in the ball's flight, where limited observations are available, we integrate human pose data using a temporal convolutional network (TCN) to compute spin priors within the factor graph. This refinement provides more accurate spin priors at the beginning of the factor graph, leading to improved early-stage hidden state inference for prediction. Our result shows the trained TCN can predict the spin priors with RMSE of 5.27 Hz. Integrating TCN into the factor graph reduces the prediction error of landing positions by over 63.6% compared to a baseline method that utilized an adaptive extended Kalman filter.","sentences":["The rapid and precise localization and prediction of a ball are critical for developing agile robots in ball sports, particularly in sports like tennis characterized by high-speed ball movements and powerful spins.","The Magnus effect induced by spin adds complexity to trajectory prediction during flight and bounce dynamics upon contact with the ground.","In this study, we introduce an innovative approach that combines a multi-camera system with factor graphs for real-time and asynchronous 3D tennis ball localization.","Additionally, we estimate hidden states like velocity and spin for trajectory prediction.","Furthermore, to enhance spin inference early in the ball's flight, where limited observations are available, we integrate human pose data using a temporal convolutional network (TCN) to compute spin priors within the factor graph.","This refinement provides more accurate spin priors at the beginning of the factor graph, leading to improved early-stage hidden state inference for prediction.","Our result shows the trained TCN can predict the spin priors with RMSE of 5.27 Hz.","Integrating TCN into the factor graph reduces the prediction error of landing positions by over 63.6% compared to a baseline method that utilized an adaptive extended Kalman filter."],"url":"http://arxiv.org/abs/2401.17185v1"}
{"created":"2024-01-30 17:04:47","title":"Zero-Shot Reinforcement Learning via Function Encoders","abstract":"Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encoder task representation.","sentences":["Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge.","The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks.","To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions.","By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation.","Thus, the agent is able to achieve transfer between related tasks at run time with no additional training.","We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encoder task representation."],"url":"http://arxiv.org/abs/2401.17173v1"}
{"created":"2024-01-30 16:56:54","title":"Conditional and Modal Reasoning in Large Language Models","abstract":"The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.","sentences":["The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science.","In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones.","We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king').","These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning.","Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans.","Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals.","Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals."],"url":"http://arxiv.org/abs/2401.17169v1"}
{"created":"2024-01-30 16:56:32","title":"Stale Profile Matching","abstract":"Profile-guided optimizations rely on profile data for directing compilers to generate optimized code. To achieve the maximum performance boost, profile data needs to be collected on the same version of the binary that is being optimized. In practice however, there is typically a gap between the profile collection and the release, which makes a portion of the profile invalid for optimizations. This phenomenon is known as profile staleness, and it is a serious practical problem for data-center workloads both for compilers and binary optimizers.   In this paper we thoroughly study the staleness problem and propose the first practical solution for utilizing profiles collected on binaries built from several revisions behind the release. Our algorithm is developed and implemented in a mainstream open-source post-link optimizer, BOLT. An extensive evaluation on a variety of standalone benchmarks and production services indicates that the new method recovers up to $0.8$ of the maximum BOLT benefit, even when most of the input profile data is stale and would have been discarded by the optimizer otherwise.","sentences":["Profile-guided optimizations rely on profile data for directing compilers to generate optimized code.","To achieve the maximum performance boost, profile data needs to be collected on the same version of the binary that is being optimized.","In practice however, there is typically a gap between the profile collection and the release, which makes a portion of the profile invalid for optimizations.","This phenomenon is known as profile staleness, and it is a serious practical problem for data-center workloads both for compilers and binary optimizers.   ","In this paper we thoroughly study the staleness problem and propose the first practical solution for utilizing profiles collected on binaries built from several revisions behind the release.","Our algorithm is developed and implemented in a mainstream open-source post-link optimizer, BOLT.","An extensive evaluation on a variety of standalone benchmarks and production services indicates that the new method recovers up to $0.8$ of the maximum BOLT benefit, even when most of the input profile data is stale and would have been discarded by the optimizer otherwise."],"url":"http://arxiv.org/abs/2401.17168v1"}
{"created":"2024-01-30 16:27:44","title":"Dependency-Aware Online Caching","abstract":"We consider a variant of the online caching problem where the items exhibit dependencies among each other: an item can reside in the cache only if all its dependent items are also in the cache. The dependency relations can form any directed acyclic graph. These requirements arise e.g., in systems such as CacheFlow (SOSR 2016) that cache forwarding rules for packet classification in IP-based communication networks.   First, we present an optimal randomized online caching algorithm which accounts for dependencies among the items. Our randomized algorithm is $O( \\log k)$-competitive, where $k$ is the size of the cache, meaning that our algorithm never incurs the cost of $O(\\log k)$ times higher than even an optimal algorithm that knows the future input sequence.   Second, we consider the bypassing model, where requests can be served at a fixed price without fetching the item and its dependencies into the cache -- a variant of caching with dependencies introduced by Bienkowski et al. at SPAA 2017. For this setting, we give an $O( \\sqrt{k \\cdot \\log k})$-competitive algorithm, which significantly improves the best known competitiveness. We conduct a small case study, to find out that our algorithm incurs on average 2x lower cost.","sentences":["We consider a variant of the online caching problem where the items exhibit dependencies among each other: an item can reside in the cache only if all its dependent items are also in the cache.","The dependency relations can form any directed acyclic graph.","These requirements arise e.g., in systems such as CacheFlow (SOSR 2016) that cache forwarding rules for packet classification in IP-based communication networks.   ","First, we present an optimal randomized online caching algorithm which accounts for dependencies among the items.","Our randomized algorithm is $O( \\log k)$-competitive, where $k$ is the size of the cache, meaning that our algorithm never incurs the cost of $O(\\log k)$ times higher than even an optimal algorithm that knows the future input sequence.   ","Second, we consider the bypassing model, where requests can be served at a fixed price without fetching the item and its dependencies into the cache -- a variant of caching with dependencies introduced by Bienkowski et al.","at SPAA 2017.","For this setting, we give an $O( \\sqrt{k \\cdot \\log k})$-competitive algorithm, which significantly improves the best known competitiveness.","We conduct a small case study, to find out that our algorithm incurs on average 2x lower cost."],"url":"http://arxiv.org/abs/2401.17146v1"}
{"created":"2024-01-30 16:19:55","title":"Large Language Model Evaluation via Matrix Entropy","abstract":"Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains. Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs.   In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs. It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability. Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings. For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law. For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing alignment quality and we find that modern large multi-modal models exhibit great alignment performance.","sentences":["Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains.","Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs.   ","In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs.","It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability.","Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings.","For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law.","For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing alignment quality and we find that modern large multi-modal models exhibit great alignment performance."],"url":"http://arxiv.org/abs/2401.17139v1"}
{"created":"2024-01-30 16:15:55","title":"Systematically Assessing the Security Risks of AI/ML-enabled Connected Healthcare Systems","abstract":"The adoption of machine-learning-enabled systems in the healthcare domain is on the rise. While the use of ML in healthcare has several benefits, it also expands the threat surface of medical systems. We show that the use of ML in medical systems, particularly connected systems that involve interfacing the ML engine with multiple peripheral devices, has security risks that might cause life-threatening damage to a patient's health in case of adversarial interventions. These new risks arise due to security vulnerabilities in the peripheral devices and communication channels. We present a case study where we demonstrate an attack on an ML-enabled blood glucose monitoring system by introducing adversarial data points during inference. We show that an adversary can achieve this by exploiting a known vulnerability in the Bluetooth communication channel connecting the glucose meter with the ML-enabled app. We further show that state-of-the-art risk assessment techniques are not adequate for identifying and assessing these new risks. Our study highlights the need for novel risk analysis methods for analyzing the security of AI-enabled connected health devices.","sentences":["The adoption of machine-learning-enabled systems in the healthcare domain is on the rise.","While the use of ML in healthcare has several benefits, it also expands the threat surface of medical systems.","We show that the use of ML in medical systems, particularly connected systems that involve interfacing the ML engine with multiple peripheral devices, has security risks that might cause life-threatening damage to a patient's health in case of adversarial interventions.","These new risks arise due to security vulnerabilities in the peripheral devices and communication channels.","We present a case study where we demonstrate an attack on an ML-enabled blood glucose monitoring system by introducing adversarial data points during inference.","We show that an adversary can achieve this by exploiting a known vulnerability in the Bluetooth communication channel connecting the glucose meter with the ML-enabled app.","We further show that state-of-the-art risk assessment techniques are not adequate for identifying and assessing these new risks.","Our study highlights the need for novel risk analysis methods for analyzing the security of AI-enabled connected health devices."],"url":"http://arxiv.org/abs/2401.17136v1"}
{"created":"2024-01-30 16:00:14","title":"Personalized Differential Privacy for Ridge Regression","abstract":"The increased application of machine learning (ML) in sensitive domains requires protecting the training data through privacy frameworks, such as differential privacy (DP). DP requires to specify a uniform privacy level $\\varepsilon$ that expresses the maximum privacy loss that each data point in the entire dataset is willing to tolerate. Yet, in practice, different data points often have different privacy requirements. Having to set one uniform privacy level is usually too restrictive, often forcing a learner to guarantee the stringent privacy requirement, at a large cost to accuracy. To overcome this limitation, we introduce our novel Personalized-DP Output Perturbation method (PDP-OP) that enables to train Ridge regression models with individual per data point privacy levels. We provide rigorous privacy proofs for our PDP-OP as well as accuracy guarantees for the resulting model. This work is the first to provide such theoretical accuracy guarantees when it comes to personalized DP in machine learning, whereas previous work only provided empirical evaluations. We empirically evaluate PDP-OP on synthetic and real datasets and with diverse privacy distributions. We show that by enabling each data point to specify their own privacy requirement, we can significantly improve the privacy-accuracy trade-offs in DP. We also show that PDP-OP outperforms the personalized privacy techniques of Jorgensen et al. (2015).","sentences":["The increased application of machine learning (ML) in sensitive domains requires protecting the training data through privacy frameworks, such as differential privacy (DP).","DP requires to specify a uniform privacy level $\\varepsilon$ that expresses the maximum privacy loss that each data point in the entire dataset is willing to tolerate.","Yet, in practice, different data points often have different privacy requirements.","Having to set one uniform privacy level is usually too restrictive, often forcing a learner to guarantee the stringent privacy requirement, at a large cost to accuracy.","To overcome this limitation, we introduce our novel Personalized-DP Output Perturbation method (PDP-OP) that enables to train Ridge regression models with individual per data point privacy levels.","We provide rigorous privacy proofs for our PDP-OP as well as accuracy guarantees for the resulting model.","This work is the first to provide such theoretical accuracy guarantees when it comes to personalized DP in machine learning, whereas previous work only provided empirical evaluations.","We empirically evaluate PDP-OP on synthetic and real datasets and with diverse privacy distributions.","We show that by enabling each data point to specify their own privacy requirement, we can significantly improve the privacy-accuracy trade-offs in DP.","We also show that PDP-OP outperforms the personalized privacy techniques of Jorgensen et al. (2015)."],"url":"http://arxiv.org/abs/2401.17127v1"}
{"created":"2024-01-30 15:57:53","title":"Characterising resource management performance in Kubernetes","abstract":"A key challenge for supporting elastic behaviour in cloud systems is to achieve a good performance in automated (de-)provisioning and scheduling of computing resources. One of the key aspects that can be significant is the overheads associated with deploying, terminating and maintaining resources. Therefore, due to their lower start up and termination overhead, containers are rapidly replacing Virtual Machines (VMs) in many cloud deployments, as the computation instance of choice. In this paper, we analyse the performance of Kubernetes achieved through a Petri net-based performance model. Kubernetes is a container management system for a distributed cluster environment. Our model can be characterised using data from a Kubernetes deployment, and can be exploited for supporting capacity planning and designing Kubernetes-based elastic applications.","sentences":["A key challenge for supporting elastic behaviour in cloud systems is to achieve a good performance in automated (de-)provisioning and scheduling of computing resources.","One of the key aspects that can be significant is the overheads associated with deploying, terminating and maintaining resources.","Therefore, due to their lower start up and termination overhead, containers are rapidly replacing Virtual Machines (VMs) in many cloud deployments, as the computation instance of choice.","In this paper, we analyse the performance of Kubernetes achieved through a Petri net-based performance model.","Kubernetes is a container management system for a distributed cluster environment.","Our model can be characterised using data from a Kubernetes deployment, and can be exploited for supporting capacity planning and designing Kubernetes-based elastic applications."],"url":"http://arxiv.org/abs/2401.17125v1"}
{"created":"2024-01-30 15:54:25","title":"Physical Priors Augmented Event-Based 3D Reconstruction","abstract":"3D neural implicit representations play a significant component in many robotic applications. However, reconstructing neural radiance fields (NeRF) from realistic event data remains a challenge due to the sparsities and the lack of information when only event streams are available. In this paper, we utilize motion, geometry, and density priors behind event data to impose strong physical constraints to augment NeRF training. The proposed novel pipeline can directly benefit from those priors to reconstruct 3D scenes without additional inputs. Moreover, we present a novel density-guided patch-based sampling strategy for robust and efficient learning, which not only accelerates training procedures but also conduces to expressions of local geometries. More importantly, we establish the first large dataset for event-based 3D reconstruction, which contains 101 objects with various materials and geometries, along with the groundtruth of images and depth maps for all camera viewpoints, which significantly facilitates other research in the related fields. The code and dataset will be publicly available at https://github.com/Mercerai/PAEv3d.","sentences":["3D neural implicit representations play a significant component in many robotic applications.","However, reconstructing neural radiance fields (NeRF) from realistic event data remains a challenge due to the sparsities and the lack of information when only event streams are available.","In this paper, we utilize motion, geometry, and density priors behind event data to impose strong physical constraints to augment NeRF training.","The proposed novel pipeline can directly benefit from those priors to reconstruct 3D scenes without additional inputs.","Moreover, we present a novel density-guided patch-based sampling strategy for robust and efficient learning, which not only accelerates training procedures but also conduces to expressions of local geometries.","More importantly, we establish the first large dataset for event-based 3D reconstruction, which contains 101 objects with various materials and geometries, along with the groundtruth of images and depth maps for all camera viewpoints, which significantly facilitates other research in the related fields.","The code and dataset will be publicly available at https://github.com/Mercerai/PAEv3d."],"url":"http://arxiv.org/abs/2401.17121v1"}
{"created":"2024-01-30 15:53:07","title":"Explainable data-driven modeling via mixture of experts: towards effective blending of grey and black-box models","abstract":"Traditional models grounded in first principles often struggle with accuracy as the system's complexity increases. Conversely, machine learning approaches, while powerful, face challenges in interpretability and in handling physical constraints. Efforts to combine these models often often stumble upon difficulties in finding a balance between accuracy and complexity. To address these issues, we propose a comprehensive framework based on a \"mixture of experts\" rationale. This approach enables the data-based fusion of diverse local models, leveraging the full potential of first-principle-based priors. Our solution allows independent training of experts, drawing on techniques from both machine learning and system identification, and it supports both collaborative and competitive learning paradigms. To enhance interpretability, we penalize abrupt variations in the expert's combination. Experimental results validate the effectiveness of our approach in producing an interpretable combination of models closely resembling the target phenomena.","sentences":["Traditional models grounded in first principles often struggle with accuracy as the system's complexity increases.","Conversely, machine learning approaches, while powerful, face challenges in interpretability and in handling physical constraints.","Efforts to combine these models often often stumble upon difficulties in finding a balance between accuracy and complexity.","To address these issues, we propose a comprehensive framework based on a \"mixture of experts\" rationale.","This approach enables the data-based fusion of diverse local models, leveraging the full potential of first-principle-based priors.","Our solution allows independent training of experts, drawing on techniques from both machine learning and system identification, and it supports both collaborative and competitive learning paradigms.","To enhance interpretability, we penalize abrupt variations in the expert's combination.","Experimental results validate the effectiveness of our approach in producing an interpretable combination of models closely resembling the target phenomena."],"url":"http://arxiv.org/abs/2401.17118v1"}
{"created":"2024-01-30 15:32:56","title":"The Influence of Presentation and Performance on User Satisfaction","abstract":"The effectiveness of an IR system is gauged not just by its ability to retrieve relevant results but also by how it presents these results to users; an engaging presentation often correlates with increased user satisfaction. While existing research has delved into the link between user satisfaction, IR performance metrics, and presentation, these aspects have typically been investigated in isolation. Our research aims to bridge this gap by examining the relationship between query performance, presentation and user satisfaction. For our analysis, we conducted a between-subjects experiment comparing the effectiveness of various result card layouts for an ad-hoc news search interface. Drawing data from the TREC WaPo 2018 collection, we centered our study on four specific topics. Within each of these topics, we assessed six distinct queries with varying nDCG values. Our study involved 164 participants who were exposed to one of five distinct layouts containing result cards, such as \"title'', \"title+image'', or \"title+image+summary''. Our findings indicate that while nDCG is a strong predictor of user satisfaction at the query level, there exists no linear relationship between the performance of the query, presentation of results and user satisfaction. However, when considering the total gain on the initial result page, we observed that presentation does play a significant role in user satisfaction (at the query level) for certain layouts with result cards such as, title+image or title+image+summary. Our results also suggest that the layout differences have complex and multifaceted impacts on satisfaction. We demonstrate the capacity to equalize user satisfaction levels between queries of varying performance by changing how results are presented. This emphasizes the necessity to harmonize both performance and presentation in IR systems, considering users' diverse preferences.","sentences":["The effectiveness of an IR system is gauged not just by its ability to retrieve relevant results but also by how it presents these results to users; an engaging presentation often correlates with increased user satisfaction.","While existing research has delved into the link between user satisfaction, IR performance metrics, and presentation, these aspects have typically been investigated in isolation.","Our research aims to bridge this gap by examining the relationship between query performance, presentation and user satisfaction.","For our analysis, we conducted a between-subjects experiment comparing the effectiveness of various result card layouts for an ad-hoc news search interface.","Drawing data from the TREC WaPo 2018 collection, we centered our study on four specific topics.","Within each of these topics, we assessed six distinct queries with varying nDCG values.","Our study involved 164 participants who were exposed to one of five distinct layouts containing result cards, such as \"title'', \"title+image'', or \"title+image+summary''.","Our findings indicate that while nDCG is a strong predictor of user satisfaction at the query level, there exists no linear relationship between the performance of the query, presentation of results and user satisfaction.","However, when considering the total gain on the initial result page, we observed that presentation does play a significant role in user satisfaction (at the query level) for certain layouts with result cards such as, title+image or title+image+summary.","Our results also suggest that the layout differences have complex and multifaceted impacts on satisfaction.","We demonstrate the capacity to equalize user satisfaction levels between queries of varying performance by changing how results are presented.","This emphasizes the necessity to harmonize both performance and presentation in IR systems, considering users' diverse preferences."],"url":"http://arxiv.org/abs/2401.17100v1"}
{"created":"2024-01-30 15:30:03","title":"MT-Ranker: Reference-free machine translation evaluation by inter-system ranking","abstract":"Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem -- producing an absolute translation-quality score. This approach has two limitations: i) the scores lack interpretability, and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent. In practice, we often care about whether a new MT system is better or worse than some competitors. In addition, reference-free MT evaluation is increasingly practical and necessary. Unfortunately, these two practical considerations have yet to be jointly explored. In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem. Given the source sentence and a pair of translations, our system predicts which translation is better. In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior correlation with human judgments by merely using indirect supervision from natural language inference and weak supervision from our synthetic data. In the context of reference-free evaluation, MT-Ranker, trained without any human annotations, achieves state-of-the-art results on the WMT Shared Metrics Task benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark, ACES, which contains fine-grained evaluation criteria such as addition, omission, and mistranslation errors, MT-Ranker marks state-of-the-art against reference-free as well as reference-based baselines.","sentences":["Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem -- producing an absolute translation-quality score.","This approach has two limitations: i) the scores lack interpretability, and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent.","In practice, we often care about whether a new MT system is better or worse than some competitors.","In addition, reference-free MT evaluation is increasingly practical and necessary.","Unfortunately, these two practical considerations have yet to be jointly explored.","In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem.","Given the source sentence and a pair of translations, our system predicts which translation is better.","In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior correlation with human judgments by merely using indirect supervision from natural language inference and weak supervision from our synthetic data.","In the context of reference-free evaluation, MT-Ranker, trained without any human annotations, achieves state-of-the-art results on the WMT Shared Metrics Task benchmarks DARR20, MQM20, and MQM21.","On a more challenging benchmark, ACES, which contains fine-grained evaluation criteria such as addition, omission, and mistranslation errors, MT-Ranker marks state-of-the-art against reference-free as well as reference-based baselines."],"url":"http://arxiv.org/abs/2401.17099v1"}
{"created":"2024-01-30 15:29:32","title":"CharNet: Generalized Approach for High-Complexity Character Classification","abstract":"Handwritten character recognition (HCR) is a challenging problem for machine learning researchers. Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias. With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem. The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features. With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges. Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results. Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity. This paper proposes a straightforward, generalizable, and highly effective approach (CharNet) for detailed character image classification and compares its performance to that of existing approaches.","sentences":["Handwritten character recognition (HCR) is a challenging problem for machine learning researchers.","Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias.","With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem.","The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features.","With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges.","Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results.","Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity.","This paper proposes a straightforward, generalizable, and highly effective approach (CharNet) for detailed character image classification and compares its performance to that of existing approaches."],"url":"http://arxiv.org/abs/2401.17098v1"}
{"created":"2024-01-30 15:21:50","title":"Traffic estimation in unobserved network locations using data-driven macroscopic models","abstract":"This paper leverages macroscopic models and multi-source spatiotemporal data collected from automatic traffic counters and probe vehicles to accurately estimate traffic flow and travel time in links where these measurements are unavailable. This problem is critical in transportation planning applications where the sensor coverage is low and the planned interventions have network-wide impacts. The proposed model, named the Macroscopic Traffic Estimator (MaTE), can perform network-wide estimations of traffic flow and travel time only using the set of observed measurements of these quantities. Because MaTE is grounded in macroscopic flow theory, all parameters and variables are interpretable. The estimated traffic flow satisfies fundamental flow conservation constraints and exhibits an increasing monotonic relationship with the estimated travel time. Using logit-based stochastic traffic assignment as the principle for routing flow behavior makes the model fully differentiable with respect to the model parameters. This property facilitates the application of computational graphs to learn parameters from vast amounts of spatiotemporal data. We also integrate neural networks and polynomial kernel functions to capture link flow interactions and enrich the mapping of traffic flows into travel times. MaTE also adds a destination choice model and a trip generation model that uses historical data on the number of trips generated by location. Experiments on synthetic data show that the model can accurately estimate travel time and traffic flow in out-of-sample links. Results obtained using real-world multi-source data from a large-scale transportation network suggest that MaTE outperforms data-driven benchmarks, especially in travel time estimation. The estimated parameters of MaTE are also informative about the hourly change in travel demand and supply characteristics of the transportation network.","sentences":["This paper leverages macroscopic models and multi-source spatiotemporal data collected from automatic traffic counters and probe vehicles to accurately estimate traffic flow and travel time in links where these measurements are unavailable.","This problem is critical in transportation planning applications where the sensor coverage is low and the planned interventions have network-wide impacts.","The proposed model, named the Macroscopic Traffic Estimator (MaTE), can perform network-wide estimations of traffic flow and travel time only using the set of observed measurements of these quantities.","Because MaTE is grounded in macroscopic flow theory, all parameters and variables are interpretable.","The estimated traffic flow satisfies fundamental flow conservation constraints and exhibits an increasing monotonic relationship with the estimated travel time.","Using logit-based stochastic traffic assignment as the principle for routing flow behavior makes the model fully differentiable with respect to the model parameters.","This property facilitates the application of computational graphs to learn parameters from vast amounts of spatiotemporal data.","We also integrate neural networks and polynomial kernel functions to capture link flow interactions and enrich the mapping of traffic flows into travel times.","MaTE also adds a destination choice model and a trip generation model that uses historical data on the number of trips generated by location.","Experiments on synthetic data show that the model can accurately estimate travel time and traffic flow in out-of-sample links.","Results obtained using real-world multi-source data from a large-scale transportation network suggest that MaTE outperforms data-driven benchmarks, especially in travel time estimation.","The estimated parameters of MaTE are also informative about the hourly change in travel demand and supply characteristics of the transportation network."],"url":"http://arxiv.org/abs/2401.17095v1"}
{"created":"2024-01-30 15:09:37","title":"Active Generation Network of Human Skeleton for Action Recognition","abstract":"Data generation is a data augmentation technique for enhancing the generalization ability for skeleton-based human action recognition. Most existing data generation methods face challenges to ensure the temporal consistency of the dynamic information for action. In addition, the data generated by these methods lack diversity when only a few training samples are available. To solve those problems, We propose a novel active generative network (AGN), which can adaptively learn various action categories by motion style transfer to generate new actions when the data for a particular action is only a single sample or few samples. The AGN consists of an action generation network and an uncertainty metric network. The former, with ST-GCN as the Backbone, can implicitly learn the morphological features of the target action while preserving the category features of the source action. The latter guides generating actions. Specifically, an action recognition model generates prediction vectors for each action, which is then scored using an uncertainty metric. Finally, UMN provides the uncertainty sampling basis for the generated actions.","sentences":["Data generation is a data augmentation technique for enhancing the generalization ability for skeleton-based human action recognition.","Most existing data generation methods face challenges to ensure the temporal consistency of the dynamic information for action.","In addition, the data generated by these methods lack diversity when only a few training samples are available.","To solve those problems, We propose a novel active generative network (AGN), which can adaptively learn various action categories by motion style transfer to generate new actions when the data for a particular action is only a single sample or few samples.","The AGN consists of an action generation network and an uncertainty metric network.","The former, with ST-GCN as the Backbone, can implicitly learn the morphological features of the target action while preserving the category features of the source action.","The latter guides generating actions.","Specifically, an action recognition model generates prediction vectors for each action, which is then scored using an uncertainty metric.","Finally, UMN provides the uncertainty sampling basis for the generated actions."],"url":"http://arxiv.org/abs/2401.17086v1"}
{"created":"2024-01-30 14:56:59","title":"Non-central panorama indoor dataset","abstract":"Omnidirectional images are one of the main sources of information for learning based scene understanding algorithms. However, annotated datasets of omnidirectional images cannot keep the pace of these learning based algorithms development. Among the different panoramas and in contrast to standard central ones, non-central panoramas provide geometrical information in the distortion of the image from which we can retrieve 3D information of the environment [2]. However, due to the lack of commercial non-central devices, up until now there was no dataset of these kinds of panoramas. In this data paper, we present the first dataset of non-central panoramas for indoor scene understanding. The dataset is composed by {\\bf 2574} RGB non-central panoramas taken in around 650 different rooms. Each panorama has associated a depth map and annotations to obtain the layout of the room from the image as a structural edge map, list of corners in the image, the 3D corners of the room and the camera pose. The images are taken from photorealistic virtual environments and pixel-wise automatically annotated.","sentences":["Omnidirectional images are one of the main sources of information for learning based scene understanding algorithms.","However, annotated datasets of omnidirectional images cannot keep the pace of these learning based algorithms development.","Among the different panoramas and in contrast to standard central ones, non-central panoramas provide geometrical information in the distortion of the image from which we can retrieve 3D information of the environment [2].","However, due to the lack of commercial non-central devices, up until now there was no dataset of these kinds of panoramas.","In this data paper, we present the first dataset of non-central panoramas for indoor scene understanding.","The dataset is composed by {\\bf 2574} RGB non-central panoramas taken in around 650 different rooms.","Each panorama has associated a depth map and annotations to obtain the layout of the room from the image as a structural edge map, list of corners in the image, the 3D corners of the room and the camera pose.","The images are taken from photorealistic virtual environments and pixel-wise automatically annotated."],"url":"http://arxiv.org/abs/2401.17075v1"}
{"created":"2024-01-30 14:52:40","title":"Ultra-low power sensor devices for monitoring physical activity and respiratory frequency in farmed fish","abstract":"Integration of technological solutions aims to improve accuracy, precision and repeatability in farming operations, and biosensor devices are increasingly used for understanding basic biology during livestock production. The aim of this study was to design and validate a miniaturized tri-axial accelerometer for non-invasive monitoring of farmed fish with re-programmable schedule protocols.The device was attached to the operculum of gilthead sea bream and European sea bass juveniles for monitoring their physical activity by measurements of movement accelerations in x and y-axes, while records of operculum beats served as a measurement of respiratory frequency. Data post-processing of exercised fish in swimming test chambers revealed an exponential increase of fish accelerations with the increase of fish speed from 1 body-length to 4 body-lengths per second, while a close relationship between oxygen consumption and opercular frequency was consistently found.The usefulness of low computational load for data pre-processing with on-board algorithms was verified from low to submaximal exercise, increasing this procedure the autonomy of the system up to 6 h of data recording with different programmable schedules. Visual observations regarding tissue damage, feeding behavior and circulating levels of stress markers did not reveal at short term a negative impact of device tagging. Reduced plasma levels of triglycerides revealed a transient inhibition of feed intake in small fish, but this disturbance was not detected in larger fish. All this considered together is the proof of concept that miniaturized devices are suitable for non-invasive and reliable metabolic phenotyping of farmed fish to improve their overall performance and welfare. Further work is underway for improving the attachment procedure and the full device packaging.","sentences":["Integration of technological solutions aims to improve accuracy, precision and repeatability in farming operations, and biosensor devices are increasingly used for understanding basic biology during livestock production.","The aim of this study was to design and validate a miniaturized tri-axial accelerometer for non-invasive monitoring of farmed fish with re-programmable schedule protocols.","The device was attached to the operculum of gilthead sea bream and European sea bass juveniles for monitoring their physical activity by measurements of movement accelerations in x and y-axes, while records of operculum beats served as a measurement of respiratory frequency.","Data post-processing of exercised fish in swimming test chambers revealed an exponential increase of fish accelerations with the increase of fish speed from 1 body-length to 4 body-lengths per second, while a close relationship between oxygen consumption and opercular frequency was consistently found.","The usefulness of low computational load for data pre-processing with on-board algorithms was verified from low to submaximal exercise, increasing this procedure the autonomy of the system up to 6 h of data recording with different programmable schedules.","Visual observations regarding tissue damage, feeding behavior and circulating levels of stress markers did not reveal at short term a negative impact of device tagging.","Reduced plasma levels of triglycerides revealed a transient inhibition of feed intake in small fish, but this disturbance was not detected in larger fish.","All this considered together is the proof of concept that miniaturized devices are suitable for non-invasive and reliable metabolic phenotyping of farmed fish to improve their overall performance and welfare.","Further work is underway for improving the attachment procedure and the full device packaging."],"url":"http://arxiv.org/abs/2401.17070v1"}
{"created":"2024-01-30 14:42:35","title":"Efficient Gesture Recognition on Spiking Convolutional Networks Through Sensor Fusion of Event-Based and Depth Data","abstract":"As intelligent systems become increasingly important in our daily lives, new ways of interaction are needed. Classical user interfaces pose issues for the physically impaired and are partially not practical or convenient. Gesture recognition is an alternative, but often not reactive enough when conventional cameras are used. This work proposes a Spiking Convolutional Neural Network, processing event- and depth data for gesture recognition. The network is simulated using the open-source neuromorphic computing framework LAVA for offline training and evaluation on an embedded system. For the evaluation three open source data sets are used. Since these do not represent the applied bi-modality, a new data set with synchronized event- and depth data was recorded. The results show the viability of temporal encoding on depth information and modality fusion, even on differently encoded data, to be beneficial to network performance and generalization capabilities.","sentences":["As intelligent systems become increasingly important in our daily lives, new ways of interaction are needed.","Classical user interfaces pose issues for the physically impaired and are partially not practical or convenient.","Gesture recognition is an alternative, but often not reactive enough when conventional cameras are used.","This work proposes a Spiking Convolutional Neural Network, processing event-","and depth data for gesture recognition.","The network is simulated using the open-source neuromorphic computing framework LAVA for offline training and evaluation on an embedded system.","For the evaluation three open source data sets are used.","Since these do not represent the applied bi-modality, a new data set with synchronized event- and depth data was recorded.","The results show the viability of temporal encoding on depth information and modality fusion, even on differently encoded data, to be beneficial to network performance and generalization capabilities."],"url":"http://arxiv.org/abs/2401.17064v1"}
{"created":"2024-01-30 14:41:28","title":"Outline of an Independent Systematic Blackbox Test for ML-based Systems","abstract":"This article proposes a test procedure that can be used to test ML models and ML-based systems independently of the actual training process. In this way, the typical quality statements such as accuracy and precision of these models and system can be verified independently, taking into account their black box character and the immanent stochastic properties of ML models and their training data. The article presents first results from a set of test experiments and suggest extensions to existing test methods reflecting the stochastic nature of ML models and ML-based systems.","sentences":["This article proposes a test procedure that can be used to test ML models and ML-based systems independently of the actual training process.","In this way, the typical quality statements such as accuracy and precision of these models and system can be verified independently, taking into account their black box character and the immanent stochastic properties of ML models and their training data.","The article presents first results from a set of test experiments and suggest extensions to existing test methods reflecting the stochastic nature of ML models and ML-based systems."],"url":"http://arxiv.org/abs/2401.17062v1"}
{"created":"2024-01-30 14:40:03","title":"Learning Approximation Sets for Exploratory Queries","abstract":"In data exploration, executing complex non-aggregate queries over large databases can be time-consuming. Our paper introduces a novel approach to address this challenge, focusing on finding an optimized subset of data, referred to as the approximation set, for query execution. The goal is to maximize query result quality while minimizing execution time. We formalize this problem as Approximate Non-Aggregates Query Processing (ANAQP) and establish its NP-completeness. To tackle this, we propose an approximate solution using advanced Reinforcement Learning architecture, termed ASQP-RL. This approach overcomes challenges related to the large action space and the need for generalization beyond a known query workload. Experimental results on two benchmarks demonstrate the superior performance of ASQP-RL, outperforming baselines by 30% in accuracy and achieving efficiency gains of 10-35X. Our research sheds light on the potential of reinforcement learning techniques for advancing data management tasks. Experimental results on two benchmarks show that ASQP-RL significantly outperforms the baselines both in terms of accuracy (30% better) and efficiency (10-35X). This research provides valuable insights into the potential of RL techniques for future advancements in data management tasks.","sentences":["In data exploration, executing complex non-aggregate queries over large databases can be time-consuming.","Our paper introduces a novel approach to address this challenge, focusing on finding an optimized subset of data, referred to as the approximation set, for query execution.","The goal is to maximize query result quality while minimizing execution time.","We formalize this problem as Approximate Non-Aggregates Query Processing (ANAQP) and establish its NP-completeness.","To tackle this, we propose an approximate solution using advanced Reinforcement Learning architecture, termed ASQP-RL.","This approach overcomes challenges related to the large action space and the need for generalization beyond a known query workload.","Experimental results on two benchmarks demonstrate the superior performance of ASQP-RL, outperforming baselines by 30% in accuracy and achieving efficiency gains of 10-35X. Our research sheds light on the potential of reinforcement learning techniques for advancing data management tasks.","Experimental results on two benchmarks show that ASQP-RL significantly outperforms the baselines both in terms of accuracy (30% better) and efficiency (10-35X).","This research provides valuable insights into the potential of RL techniques for future advancements in data management tasks."],"url":"http://arxiv.org/abs/2401.17059v1"}
{"created":"2024-01-30 14:33:18","title":"Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again","abstract":"Deep learning for tabular data has garnered increasing attention in recent years, yet employing deep models for structured data remains challenging. While these models excel with unstructured data, their efficacy with structured data has been limited. Recent research has introduced retrieval-augmented models to address this gap, demonstrating promising results in supervised tasks such as classification and regression. In this work, we investigate using retrieval-augmented models for anomaly detection on tabular data. We propose a reconstruction-based approach in which a transformer model learns to reconstruct masked features of \\textit{normal} samples. We test the effectiveness of KNN-based and attention-based modules to select relevant samples to help in the reconstruction process of the target sample. Our experiments on a benchmark of 31 tabular datasets reveal that augmenting this reconstruction-based anomaly detection (AD) method with non-parametric relationships via retrieval modules may significantly boost performance.","sentences":["Deep learning for tabular data has garnered increasing attention in recent years, yet employing deep models for structured data remains challenging.","While these models excel with unstructured data, their efficacy with structured data has been limited.","Recent research has introduced retrieval-augmented models to address this gap, demonstrating promising results in supervised tasks such as classification and regression.","In this work, we investigate using retrieval-augmented models for anomaly detection on tabular data.","We propose a reconstruction-based approach in which a transformer model learns to reconstruct masked features of \\textit{normal} samples.","We test the effectiveness of KNN-based and attention-based modules to select relevant samples to help in the reconstruction process of the target sample.","Our experiments on a benchmark of 31 tabular datasets reveal that augmenting this reconstruction-based anomaly detection (AD) method with non-parametric relationships via retrieval modules may significantly boost performance."],"url":"http://arxiv.org/abs/2401.17052v1"}
{"created":"2024-01-30 14:18:31","title":"Taking Action Towards Graceful Interaction: The Effects of Performing Actions on Modelling Policies for Instruction Clarification Requests","abstract":"Clarification requests are a mechanism to help solve communication problems, e.g. due to ambiguity or underspecification, in instruction-following interactions. Despite their importance, even skilful models struggle with producing or interpreting such repair acts. In this work, we test three hypotheses concerning the effects of action taking as an auxiliary task in modelling iCR policies. Contrary to initial expectations, we conclude that its contribution to learning an iCR policy is limited, but some information can still be extracted from prediction uncertainty. We present further evidence that even well-motivated, Transformer-based models fail to learn good policies for when to ask Instruction CRs (iCRs), while the task of determining what to ask about can be more successfully modelled. Considering the implications of these findings, we further discuss the shortcomings of the data-driven paradigm for learning meta-communication acts.","sentences":["Clarification requests are a mechanism to help solve communication problems, e.g. due to ambiguity or underspecification, in instruction-following interactions.","Despite their importance, even skilful models struggle with producing or interpreting such repair acts.","In this work, we test three hypotheses concerning the effects of action taking as an auxiliary task in modelling iCR policies.","Contrary to initial expectations, we conclude that its contribution to learning an iCR policy is limited, but some information can still be extracted from prediction uncertainty.","We present further evidence that even well-motivated, Transformer-based models fail to learn good policies for when to ask Instruction CRs (iCRs), while the task of determining what to ask about can be more successfully modelled.","Considering the implications of these findings, we further discuss the shortcomings of the data-driven paradigm for learning meta-communication acts."],"url":"http://arxiv.org/abs/2401.17039v1"}
{"created":"2024-01-30 14:16:24","title":"Towards Assessing the Synthetic-to-Measured Adversarial Vulnerability of SAR ATR","abstract":"Recently, there has been increasing concern about the vulnerability of deep neural network (DNN)-based synthetic aperture radar (SAR) automatic target recognition (ATR) to adversarial attacks, where a DNN could be easily deceived by clean input with imperceptible but aggressive perturbations. This paper studies the synthetic-to-measured (S2M) transfer setting, where an attacker generates adversarial perturbation based solely on synthetic data and transfers it against victim models trained with measured data. Compared with the current measured-to-measured (M2M) transfer setting, our approach does not need direct access to the victim model or the measured SAR data. We also propose the transferability estimation attack (TEA) to uncover the adversarial risks in this more challenging and practical scenario. The TEA makes full use of the limited similarity between the synthetic and measured data pairs for blind estimation and optimization of S2M transferability, leading to feasible surrogate model enhancement without mastering the victim model and data. Comprehensive evaluations based on the publicly available synthetic and measured paired labeled experiment (SAMPLE) dataset demonstrate that the TEA outperforms state-of-the-art methods and can significantly enhance various attack algorithms in computer vision and remote sensing applications. Codes and data are available at https://github.com/scenarri/S2M-TEA.","sentences":["Recently, there has been increasing concern about the vulnerability of deep neural network (DNN)-based synthetic aperture radar (SAR) automatic target recognition (ATR) to adversarial attacks, where a DNN could be easily deceived by clean input with imperceptible but aggressive perturbations.","This paper studies the synthetic-to-measured (S2M) transfer setting, where an attacker generates adversarial perturbation based solely on synthetic data and transfers it against victim models trained with measured data.","Compared with the current measured-to-measured (M2M) transfer setting, our approach does not need direct access to the victim model or the measured SAR data.","We also propose the transferability estimation attack (TEA) to uncover the adversarial risks in this more challenging and practical scenario.","The TEA makes full use of the limited similarity between the synthetic and measured data pairs for blind estimation and optimization of S2M transferability, leading to feasible surrogate model enhancement without mastering the victim model and data.","Comprehensive evaluations based on the publicly available synthetic and measured paired labeled experiment (SAMPLE) dataset demonstrate that the TEA outperforms state-of-the-art methods and can significantly enhance various attack algorithms in computer vision and remote sensing applications.","Codes and data are available at https://github.com/scenarri/S2M-TEA."],"url":"http://arxiv.org/abs/2401.17038v1"}
{"created":"2024-01-30 14:16:06","title":"Bayesian Optimization with Noise-Free Observations: Improved Regret Bounds via Random Exploration","abstract":"This paper studies Bayesian optimization with noise-free observations. We introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate. Our algorithms retain the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arXiv:2002.05096, hence solving a COLT open problem. Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization strategies in several examples.","sentences":["This paper studies Bayesian optimization with noise-free observations.","We introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate.","Our algorithms retain the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arXiv:2002.05096, hence solving a COLT open problem.","Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization strategies in several examples."],"url":"http://arxiv.org/abs/2401.17037v1"}
{"created":"2024-01-30 14:16:02","title":"Intrinsic Data Constraints and Upper Bounds in Binary Classification Performance","abstract":"The structure of data organization is widely recognized as having a substantial influence on the efficacy of machine learning algorithms, particularly in binary classification tasks. Our research provides a theoretical framework suggesting that the maximum potential of binary classifiers on a given dataset is primarily constrained by the inherent qualities of the data. Through both theoretical reasoning and empirical examination, we employed standard objective functions, evaluative metrics, and binary classifiers to arrive at two principal conclusions. Firstly, we show that the theoretical upper bound of binary classification performance on actual datasets can be theoretically attained. This upper boundary represents a calculable equilibrium between the learning loss and the metric of evaluation. Secondly, we have computed the precise upper bounds for three commonly used evaluation metrics, uncovering a fundamental uniformity with our overarching thesis: the upper bound is intricately linked to the dataset's characteristics, independent of the classifier in use. Additionally, our subsequent analysis uncovers a detailed relationship between the upper limit of performance and the level of class overlap within the binary classification data. This relationship is instrumental for pinpointing the most effective feature subsets for use in feature engineering.","sentences":["The structure of data organization is widely recognized as having a substantial influence on the efficacy of machine learning algorithms, particularly in binary classification tasks.","Our research provides a theoretical framework suggesting that the maximum potential of binary classifiers on a given dataset is primarily constrained by the inherent qualities of the data.","Through both theoretical reasoning and empirical examination, we employed standard objective functions, evaluative metrics, and binary classifiers to arrive at two principal conclusions.","Firstly, we show that the theoretical upper bound of binary classification performance on actual datasets can be theoretically attained.","This upper boundary represents a calculable equilibrium between the learning loss and the metric of evaluation.","Secondly, we have computed the precise upper bounds for three commonly used evaluation metrics, uncovering a fundamental uniformity with our overarching thesis: the upper bound is intricately linked to the dataset's characteristics, independent of the classifier in use.","Additionally, our subsequent analysis uncovers a detailed relationship between the upper limit of performance and the level of class overlap within the binary classification data.","This relationship is instrumental for pinpointing the most effective feature subsets for use in feature engineering."],"url":"http://arxiv.org/abs/2401.17036v1"}
{"created":"2024-01-30 14:12:39","title":"Robust Kernel Sparse Subspace Clustering","abstract":"Kernel methods are applied to many problems in pattern recognition, including subspace clustering (SC). That way, nonlinear problems in the input data space become linear in mapped high-dimensional feature space. Thereby, computationally tractable nonlinear algorithms are enabled through implicit mapping by the virtue of kernel trick. However, kernelization of linear algorithms is possible only if square of the Froebenious norm of the error term is used in related optimization problem. That, however, implies normal distribution of the error. That is not appropriate for non-Gaussian errors such as gross sparse corruptions that are modeled by -norm. Herein, to the best of our knowledge, we propose for the first time robust kernel sparse SC (RKSSC) algorithm for data with gross sparse corruptions. The concept, in principle, can be applied to other SC algorithms to achieve robustness to the presence of such type of corruption. We validated proposed approach on two well-known datasets with linear robust SSC algorithm as a baseline model. According to Wilcoxon test, clustering performance obtained by the RKSSC algorithm is statistically significantly better than corresponding performance obtained by the robust SSC algorithm. MATLAB code of proposed RKSSC algorithm is posted on https://github.com/ikopriva/RKSSC.","sentences":["Kernel methods are applied to many problems in pattern recognition, including subspace clustering (SC).","That way, nonlinear problems in the input data space become linear in mapped high-dimensional feature space.","Thereby, computationally tractable nonlinear algorithms are enabled through implicit mapping by the virtue of kernel trick.","However, kernelization of linear algorithms is possible only if square of the Froebenious norm of the error term is used in related optimization problem.","That, however, implies normal distribution of the error.","That is not appropriate for non-Gaussian errors such as gross sparse corruptions that are modeled by -norm.","Herein, to the best of our knowledge, we propose for the first time robust kernel sparse SC (RKSSC) algorithm for data with gross sparse corruptions.","The concept, in principle, can be applied to other SC algorithms to achieve robustness to the presence of such type of corruption.","We validated proposed approach on two well-known datasets with linear robust SSC algorithm as a baseline model.","According to Wilcoxon test, clustering performance obtained by the RKSSC algorithm is statistically significantly better than corresponding performance obtained by the robust SSC algorithm.","MATLAB code of proposed RKSSC algorithm is posted on https://github.com/ikopriva/RKSSC."],"url":"http://arxiv.org/abs/2401.17035v1"}
{"created":"2024-01-30 14:09:41","title":"Multilayer Graph Approach to Deep Subspace Clustering","abstract":"Deep subspace clustering (DSC) networks based on self-expressive model learn representation matrix, often implemented in terms of fully connected network, in the embedded space. After the learning is finished, representation matrix is used by spectral clustering module to assign labels to clusters. However, such approach ignores complementary information that exist in other layers of the encoder (including the input data themselves). Herein, we apply selected linear subspace clustering algorithm to learn representation matrices from representations learned by all layers of encoder network including the input data. Afterward, we learn a multilayer graph that in a multi-view like manner integrates information from graph Laplacians of all used layers. That improves further performance of selected DSC network. Furthermore, we also provide formulation of our approach to cluster out-of-sample/test data points. We validate proposed approach on four well-known datasets with two DSC networks as baseline models. In almost all the cases, proposed approach achieved statistically significant improvement in three performance metrics. MATLAB code of proposed algorithm is posted on https://github.com/lovro-sinda/MLG-DSC.","sentences":["Deep subspace clustering (DSC) networks based on self-expressive model learn representation matrix, often implemented in terms of fully connected network, in the embedded space.","After the learning is finished, representation matrix is used by spectral clustering module to assign labels to clusters.","However, such approach ignores complementary information that exist in other layers of the encoder (including the input data themselves).","Herein, we apply selected linear subspace clustering algorithm to learn representation matrices from representations learned by all layers of encoder network including the input data.","Afterward, we learn a multilayer graph that in a multi-view like manner integrates information from graph Laplacians of all used layers.","That improves further performance of selected DSC network.","Furthermore, we also provide formulation of our approach to cluster out-of-sample/test data points.","We validate proposed approach on four well-known datasets with two DSC networks as baseline models.","In almost all the cases, proposed approach achieved statistically significant improvement in three performance metrics.","MATLAB code of proposed algorithm is posted on https://github.com/lovro-sinda/MLG-DSC."],"url":"http://arxiv.org/abs/2401.17033v1"}
{"created":"2024-01-30 14:09:35","title":"M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation","abstract":"One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator and we show that it significantly enhances the learning efficiency in different manipulation tasks. This is evidenced by faster convergence rates and higher cumulative rewards per episode, compared to standard RL algorithms without our representation learning approach.","sentences":["One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities.","Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms.","However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives.","To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL).","Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms.","Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm.","We evaluate M2CURL on the Tactile Gym 2 simulator and we show that it significantly enhances the learning efficiency in different manipulation tasks.","This is evidenced by faster convergence rates and higher cumulative rewards per episode, compared to standard RL algorithms without our representation learning approach."],"url":"http://arxiv.org/abs/2401.17032v1"}
{"created":"2024-01-30 14:02:49","title":"Heterogeneous treatment effect estimation with subpopulation identification for personalized medicine in opioid use disorder","abstract":"Deep learning models have demonstrated promising results in estimating treatment effects (TEE). However, most of them overlook the variations in treatment outcomes among subgroups with distinct characteristics. This limitation hinders their ability to provide accurate estimations and treatment recommendations for specific subgroups. In this study, we introduce a novel neural network-based framework, named SubgroupTE, which incorporates subgroup identification and treatment effect estimation. SubgroupTE identifies diverse subgroups and simultaneously estimates treatment effects for each subgroup, improving the treatment effect estimation by considering the heterogeneity of treatment responses. Comparative experiments on synthetic data show that SubgroupTE outperforms existing models in treatment effect estimation. Furthermore, experiments on a real-world dataset related to opioid use disorder (OUD) demonstrate the potential of our approach to enhance personalized treatment recommendations for OUD patients.","sentences":["Deep learning models have demonstrated promising results in estimating treatment effects (TEE).","However, most of them overlook the variations in treatment outcomes among subgroups with distinct characteristics.","This limitation hinders their ability to provide accurate estimations and treatment recommendations for specific subgroups.","In this study, we introduce a novel neural network-based framework, named SubgroupTE, which incorporates subgroup identification and treatment effect estimation.","SubgroupTE","identifies diverse subgroups and simultaneously estimates treatment effects for each subgroup, improving the treatment effect estimation by considering the heterogeneity of treatment responses.","Comparative experiments on synthetic data show that SubgroupTE outperforms existing models in treatment effect estimation.","Furthermore, experiments on a real-world dataset related to opioid use disorder (OUD) demonstrate the potential of our approach to enhance personalized treatment recommendations for OUD patients."],"url":"http://arxiv.org/abs/2401.17027v1"}
{"created":"2024-01-30 13:52:40","title":"GPU-Accelerated Batch-Dynamic Subgraph Matching","abstract":"Subgraph matching has garnered increasing attention for its diverse real-world applications. Given the dynamic nature of real-world graphs, addressing evolving scenarios without incurring prohibitive overheads has been a focus of research. However, existing approaches for dynamic subgraph matching often proceed serially, retrieving incremental matches for each updated edge individually. This approach falls short when handling batch data updates, leading to a decrease in system throughput. Leveraging the parallel processing power of GPUs, which can execute a massive number of cores simultaneously, has been widely recognized for performance acceleration in various domains. Surprisingly, systematic exploration of subgraph matching in the context of batch-dynamic graphs, particularly on a GPU platform, remains untouched. In this paper, we bridge this gap by introducing an efficient framework, GAMMA (GPU-Accelerated Batch-Dynamic Subgraph Matching). Our approach features a DFS-based warp-centric batch-dynamic subgraph matching algorithm. To ensure load balance in the DFS-based search, we propose warp-level work stealing via shared memory. Additionally, we introduce coalesced search to reduce redundant computations. Comprehensive experiments demonstrate the superior performance of GAMMA. Compared to state-of-the-art algorithms, GAMMA showcases a performance improvement up to hundreds of times.","sentences":["Subgraph matching has garnered increasing attention for its diverse real-world applications.","Given the dynamic nature of real-world graphs, addressing evolving scenarios without incurring prohibitive overheads has been a focus of research.","However, existing approaches for dynamic subgraph matching often proceed serially, retrieving incremental matches for each updated edge individually.","This approach falls short when handling batch data updates, leading to a decrease in system throughput.","Leveraging the parallel processing power of GPUs, which can execute a massive number of cores simultaneously, has been widely recognized for performance acceleration in various domains.","Surprisingly, systematic exploration of subgraph matching in the context of batch-dynamic graphs, particularly on a GPU platform, remains untouched.","In this paper, we bridge this gap by introducing an efficient framework, GAMMA (GPU-Accelerated Batch-Dynamic Subgraph Matching).","Our approach features a DFS-based warp-centric batch-dynamic subgraph matching algorithm.","To ensure load balance in the DFS-based search, we propose warp-level work stealing via shared memory.","Additionally, we introduce coalesced search to reduce redundant computations.","Comprehensive experiments demonstrate the superior performance of GAMMA.","Compared to state-of-the-art algorithms, GAMMA showcases a performance improvement up to hundreds of times."],"url":"http://arxiv.org/abs/2401.17018v1"}
{"created":"2024-01-30 13:47:15","title":"Age of Actuated Information and Age of Actuation in a Data-Caching Energy Harvesting Actuator","abstract":"In this paper, we introduce two metrics, namely, age of actuation (AoA) and age of actuated information (AoAI), within a discrete-time system model that integrates data caching and energy harvesting (EH). AoA evaluates the timeliness of actions irrespective of the age of the information, while AoAI considers the freshness of the utilized data packet. We use Markov Chain analysis to model the system's evolution. Furthermore, we employ three-dimensional Markov Chain analysis to characterize the stationary distributions for AoA and AoAI and calculate their average values. Our findings from the analysis, validated by simulations, show that while AoAI consistently decreases with increased data and energy packet arrival rates, AoA presents a more complex behavior, with potential increases under conditions of limited data or energy resources. These metrics go towards the semantics of information and goal-oriented communications since they consider the timeliness of utilizing the information to perform an action.","sentences":["In this paper, we introduce two metrics, namely, age of actuation (AoA) and age of actuated information (AoAI), within a discrete-time system model that integrates data caching and energy harvesting (EH).","AoA evaluates the timeliness of actions irrespective of the age of the information, while AoAI considers the freshness of the utilized data packet.","We use Markov Chain analysis to model the system's evolution.","Furthermore, we employ three-dimensional Markov Chain analysis to characterize the stationary distributions for AoA and AoAI and calculate their average values.","Our findings from the analysis, validated by simulations, show that while AoAI consistently decreases with increased data and energy packet arrival rates, AoA presents a more complex behavior, with potential increases under conditions of limited data or energy resources.","These metrics go towards the semantics of information and goal-oriented communications since they consider the timeliness of utilizing the information to perform an action."],"url":"http://arxiv.org/abs/2401.17011v1"}
{"created":"2024-01-30 13:40:36","title":"SAL-PIM: A Subarray-level Processing-in-Memory Architecture with LUT-based Linear Interpolation for Transformer-based Text Generation","abstract":"Text generation is a compelling sub-field of natural language processing, aiming to generate human-readable text from input words. In particular, the decoder-only generative models, such as generative pre-trained transformer (GPT), are widely used for text generation, with two major computational stages: summarization and generation. Unlike the summarization stage, which can process the input tokens in parallel, the generation stage is difficult to accelerate due to its sequential generation of output tokens through iteration. Moreover, each iteration requires reading a whole model with little data reuse opportunity. Therefore, the workload of transformer-based text generation is severely memory-bound, making the external memory bandwidth system bottleneck. In this paper, we proposed a subarray-level processing-in-memory architecture named SAL-PIM, HBM-based PIM architecture for the end-to-end acceleration of transformer-based text generation. The SAL-PIM architecture includes three architectural features. First, the SAL-PIM architecture utilizes higher internal bandwidth by integrating multiple subarray-level arithmetic logic units with optimized data mapping schemes. Second, the SAL-PIM architecture adopts LUT-based linear interpolation to perform complex non-linear functions in PIM. Third, the SAL-PIM architecture accelerates end-to-end inference on PIM in text generation. Furthermore, to validate the SAL-PIM architecture, we built cycle-accurate simulator and implemented the SAL-PIM's logic units in 28-nm CMOS technology. As a result, when the input size is from 32 to 128 and the output size is from 1 to 256, SAL-PIM achieves a maximum of 4.72 times speedup and an average of 1.83 times speedup for the text generation based on the GPT-2 medium model compared to the server-level GPU.","sentences":["Text generation is a compelling sub-field of natural language processing, aiming to generate human-readable text from input words.","In particular, the decoder-only generative models, such as generative pre-trained transformer (GPT), are widely used for text generation, with two major computational stages: summarization and generation.","Unlike the summarization stage, which can process the input tokens in parallel, the generation stage is difficult to accelerate due to its sequential generation of output tokens through iteration.","Moreover, each iteration requires reading a whole model with little data reuse opportunity.","Therefore, the workload of transformer-based text generation is severely memory-bound, making the external memory bandwidth system bottleneck.","In this paper, we proposed a subarray-level processing-in-memory architecture named SAL-PIM, HBM-based PIM architecture for the end-to-end acceleration of transformer-based text generation.","The SAL-PIM architecture includes three architectural features.","First, the SAL-PIM architecture utilizes higher internal bandwidth by integrating multiple subarray-level arithmetic logic units with optimized data mapping schemes.","Second, the SAL-PIM architecture adopts LUT-based linear interpolation to perform complex non-linear functions in PIM.","Third, the SAL-PIM architecture accelerates end-to-end inference on PIM in text generation.","Furthermore, to validate the SAL-PIM architecture, we built cycle-accurate simulator and implemented the SAL-PIM's logic units in 28-nm CMOS technology.","As a result, when the input size is from 32 to 128 and the output size is from 1 to 256, SAL-PIM achieves a maximum of 4.72 times speedup and an average of 1.83 times speedup for the text generation based on the GPT-2 medium model compared to the server-level GPU."],"url":"http://arxiv.org/abs/2401.17005v1"}
{"created":"2024-01-30 13:32:22","title":"The Sherali-Adams and Weisfeiler-Leman hierarchies in (Promise Valued) Constraint Satisfaction Problems","abstract":"In this paper we study the interactions between so-called fractional relaxations of the integer programs (IPs) which encode homomorphism and isomorphism of relational structures. We give a combinatorial characterization of a certain natural linear programming (LP) relaxation of homomorphism in terms of fractional isomorphism. As a result, we show that the families of constraint satisfaction problems (CSPs) that are solvable by such linear program are precisely those that are closed under an equivalence relation which we call Weisfeiler-Leman invariance. We also generalize this result to the much broader framework of Promise Valued Constraint Satisfaction Problems, which brings together two well-studied extensions of the CSP framework. Finally, we consider the hierarchies of increasingly tighter relaxations of the homomorphism and isomorphism IPs obtained by applying the Sherali-Adams and Weisfeiler-Leman methods respectively. We extend our combinatorial characterization of the basic LP to higher levels of the Sherali-Adams hierarchy, and we generalize a well-known logical characterization of the Weisfeiler-Leman test from graphs to relational structures.","sentences":["In this paper we study the interactions between so-called fractional relaxations of the integer programs (IPs) which encode homomorphism and isomorphism of relational structures.","We give a combinatorial characterization of a certain natural linear programming (LP) relaxation of homomorphism in terms of fractional isomorphism.","As a result, we show that the families of constraint satisfaction problems (CSPs) that are solvable by such linear program are precisely those that are closed under an equivalence relation which we call Weisfeiler-Leman invariance.","We also generalize this result to the much broader framework of Promise Valued Constraint Satisfaction Problems, which brings together two well-studied extensions of the CSP framework.","Finally, we consider the hierarchies of increasingly tighter relaxations of the homomorphism and isomorphism IPs obtained by applying the Sherali-Adams and Weisfeiler-Leman methods respectively.","We extend our combinatorial characterization of the basic LP to higher levels of the Sherali-Adams hierarchy, and we generalize a well-known logical characterization of the Weisfeiler-Leman test from graphs to relational structures."],"url":"http://arxiv.org/abs/2401.16998v1"}
{"created":"2024-01-30 13:24:44","title":"Randomized Key Encapsulation/Consolidation","abstract":"This article bridges the gap between two topics used in sharing an encryption key: (i) Key Consolidation, i.e., extracting two identical strings of bits from two information sources with similarities (common randomness). (ii) Quantum-safe Key Encapsulation by incorporating randomness in Public/Private Key pairs. In the context of Key Consolidation, the proposed scheme adds to the complexity Eve faces in extracting useful data from leaked information. In this context, it is applied to the method proposed in [1] for establishing common randomness from round-trip travel times in a packet data network. The proposed method allows adapting the secrecy level to the amount of similarity in common randomness. It can even encapsulate a Quantum-safe encryption key in the extreme case that no common randomness is available. In the latter case, it is shown that the proposed scheme offers improvements with respect to the McEliece cryptosystem which currently forms the foundation for Quantum safe key encapsulation.   [1] A. K. Khandani, \"Looping for Encryption Key Generation Over the Internet: A New Frontier in Physical Layer Security,\" 2023 Biennial Symposium on Communications (BSC), Montreal, QC, Canada, 2023, pp. 59-64","sentences":["This article bridges the gap between two topics used in sharing an encryption key: (i) Key Consolidation, i.e., extracting two identical strings of bits from two information sources with similarities (common randomness).","(ii) Quantum-safe Key Encapsulation by incorporating randomness in Public/Private Key pairs.","In the context of Key Consolidation, the proposed scheme adds to the complexity Eve faces in extracting useful data from leaked information.","In this context, it is applied to the method proposed in [1] for establishing common randomness from round-trip travel times in a packet data network.","The proposed method allows adapting the secrecy level to the amount of similarity in common randomness.","It can even encapsulate a Quantum-safe encryption key in the extreme case that no common randomness is available.","In the latter case, it is shown that the proposed scheme offers improvements with respect to the McEliece cryptosystem which currently forms the foundation for Quantum safe key encapsulation.   ","[1] A. K. Khandani, \"Looping for Encryption Key Generation Over the Internet: A New Frontier in Physical Layer Security,\" 2023 Biennial Symposium on Communications (BSC), Montreal, QC, Canada, 2023, pp.","59-64"],"url":"http://arxiv.org/abs/2401.16993v1"}
{"created":"2024-01-30 13:10:33","title":"ActDroid: An active learning framework for Android malware detection","abstract":"The growing popularity of Android requires malware detection systems that can keep up with the pace of new software being released. According to a recent study, a new piece of malware appears online every 12 seconds. To address this, we treat Android malware detection as a streaming data problem and explore the use of active online learning as a means of mitigating the problem of labelling applications in a timely and cost-effective manner. Our resulting framework achieves accuracies of up to 96\\%, requires as little of 24\\% of the training data to be labelled, and compensates for concept drift that occurs between the release and labelling of an application. We also consider the broader practicalities of online learning within Android malware detection, and systematically explore the trade-offs between using different static, dynamic and hybrid feature sets to classify malware.","sentences":["The growing popularity of Android requires malware detection systems that can keep up with the pace of new software being released.","According to a recent study, a new piece of malware appears online every 12 seconds.","To address this, we treat Android malware detection as a streaming data problem and explore the use of active online learning as a means of mitigating the problem of labelling applications in a timely and cost-effective manner.","Our resulting framework achieves accuracies of up to 96\\%, requires as little of 24\\% of the training data to be labelled, and compensates for concept drift that occurs between the release and labelling of an application.","We also consider the broader practicalities of online learning within Android malware detection, and systematically explore the trade-offs between using different static, dynamic and hybrid feature sets to classify malware."],"url":"http://arxiv.org/abs/2401.16982v1"}
{"created":"2024-01-30 13:04:20","title":"Re3val: Reinforced and Reranked Generative Retrieval","abstract":"Generative retrieval models encode pointers to information in a corpus as an index within the model's parameters. These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks. However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can't be tuned for the downstream readers as decoding the page title is a non-differentiable operation. This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data. Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding. Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts from the KILT database using the rerank page titles. Upon grounding the top five reranked contexts, Re3val demonstrates the Top 1 KILT scores compared to all other generative retrieval models across five KILT datasets.","sentences":["Generative retrieval models encode pointers to information in a corpus as an index within the model's parameters.","These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks.","However, we identify two limitations: the generative retrieval does not account for contextual information.","Secondly, the retrieval can't be tuned for the downstream readers as decoding the page title is a non-differentiable operation.","This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data.","Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding.","Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets.","Subsequently, we extract and rerank contexts from the KILT database using the rerank page titles.","Upon grounding the top five reranked contexts, Re3val demonstrates the Top 1 KILT scores compared to all other generative retrieval models across five KILT datasets."],"url":"http://arxiv.org/abs/2401.16979v1"}
{"created":"2024-01-30 12:57:52","title":"CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning","abstract":"Causal discovery is the challenging task of inferring causal structure from data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive observations alone are not enough to distinguish correlation from causation, there has been a recent push to incorporate interventions into machine learning research. Reinforcement learning provides a convenient framework for such an active approach to learning. This paper presents CORE, a deep reinforcement learning-based approach for causal discovery and intervention planning. CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions. Our results demonstrate that CORE generalizes to unseen graphs and efficiently uncovers causal structures. Furthermore, CORE scales to larger graphs with up to 10 variables and outperforms existing approaches in structure estimation accuracy and sample efficiency. All relevant code and supplementary material can be found at https://github.com/sa-and/CORE","sentences":["Causal discovery is the challenging task of inferring causal structure from data.","Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive observations alone are not enough to distinguish correlation from causation, there has been a recent push to incorporate interventions into machine learning research.","Reinforcement learning provides a convenient framework for such an active approach to learning.","This paper presents CORE, a deep reinforcement learning-based approach for causal discovery and intervention planning.","CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions.","Our results demonstrate that CORE generalizes to unseen graphs and efficiently uncovers causal structures.","Furthermore, CORE scales to larger graphs with up to 10 variables and outperforms existing approaches in structure estimation accuracy and sample efficiency.","All relevant code and supplementary material can be found at https://github.com/sa-and/CORE"],"url":"http://arxiv.org/abs/2401.16974v1"}
{"created":"2024-01-30 12:54:54","title":"Autonomy Loops for Monitoring, Operational Data Analytics, Feedback, and Response in HPC Operations","abstract":"Many High Performance Computing (HPC) facilities have developed and deployed frameworks in support of continuous monitoring and operational data analytics (MODA) to help improve efficiency and throughput. Because of the complexity and scale of systems and workflows and the need for low-latency response to address dynamic circumstances, automated feedback and response have the potential to be more effective than current human-in-the-loop approaches which are laborious and error prone. Progress has been limited, however, by factors such as the lack of infrastructure and feedback hooks, and successful deployment is often site- and case-specific. In this position paper we report on the outcomes and plans from a recent Dagstuhl Seminar, seeking to carve a path for community progress in the development of autonomous feedback loops for MODA, based on the established formalism of similar (MAPE-K) loops in autonomous computing and self-adaptive systems. By defining and developing such loops for significant cases experienced across HPC sites, we seek to extract commonalities and develop conventions that will facilitate interoperability and interchangeability with system hardware, software, and applications across different sites, and will motivate vendors and others to provide telemetry interfaces and feedback hooks to enable community development and pervasive deployment of MODA autonomy loops.","sentences":["Many High Performance Computing (HPC) facilities have developed and deployed frameworks in support of continuous monitoring and operational data analytics (MODA) to help improve efficiency and throughput.","Because of the complexity and scale of systems and workflows and the need for low-latency response to address dynamic circumstances, automated feedback and response have the potential to be more effective than current human-in-the-loop approaches which are laborious and error prone.","Progress has been limited, however, by factors such as the lack of infrastructure and feedback hooks, and successful deployment is often site- and case-specific.","In this position paper we report on the outcomes and plans from a recent Dagstuhl Seminar, seeking to carve a path for community progress in the development of autonomous feedback loops for MODA, based on the established formalism of similar (MAPE-K) loops in autonomous computing and self-adaptive systems.","By defining and developing such loops for significant cases experienced across HPC sites, we seek to extract commonalities and develop conventions that will facilitate interoperability and interchangeability with system hardware, software, and applications across different sites, and will motivate vendors and others to provide telemetry interfaces and feedback hooks to enable community development and pervasive deployment of MODA autonomy loops."],"url":"http://arxiv.org/abs/2401.16971v1"}
{"created":"2024-01-30 12:20:34","title":"An ARGoS plug-in for the Crazyflie drone","abstract":"We present a new plug-in for the ARGoS swarm robotic simulator to implement the Crazyflie drone, including its controllers, sensors, and some expansion decks. We have based our development on the former Spiri drone, upgrading the position controller, adding a new speed controller, LED ring, onboard camera, and battery discharge model. We have compared this new plug-in in terms of accuracy and efficiency with data obtained from real Crazyflie drones. All our experiments showed that the proposed plug-in worked well, presenting high levels of accuracy. We believe that this is an important contribution to robot simulations which will extend ARGoS capabilities through the use of our proposed, open-source plug-in.","sentences":["We present a new plug-in for the ARGoS swarm robotic simulator to implement the Crazyflie drone, including its controllers, sensors, and some expansion decks.","We have based our development on the former Spiri drone, upgrading the position controller, adding a new speed controller, LED ring, onboard camera, and battery discharge model.","We have compared this new plug-in in terms of accuracy and efficiency with data obtained from real Crazyflie drones.","All our experiments showed that the proposed plug-in worked well, presenting high levels of accuracy.","We believe that this is an important contribution to robot simulations which will extend ARGoS capabilities through the use of our proposed, open-source plug-in."],"url":"http://arxiv.org/abs/2401.16948v1"}
{"created":"2024-01-30 12:03:40","title":"Multi-modal Representation Learning for Cross-modal Prediction of Continuous Weather Patterns from Discrete Low-Dimensional Data","abstract":"World is looking for clean and renewable energy sources that do not pollute the environment, in an attempt to reduce greenhouse gas emissions that contribute to global warming. Wind energy has significant potential to not only reduce greenhouse emission, but also meet the ever increasing demand for energy. To enable the effective utilization of wind energy, addressing the following three challenges in wind data analysis is crucial. Firstly, improving data resolution in various climate conditions to ensure an ample supply of information for assessing potential energy resources. Secondly, implementing dimensionality reduction techniques for data collected from sensors/simulations to efficiently manage and store large datasets. Thirdly, extrapolating wind data from one spatial specification to another, particularly in cases where data acquisition may be impractical or costly. We propose a deep learning based approach to achieve multi-modal continuous resolution wind data prediction from discontinuous wind data, along with data dimensionality reduction.","sentences":["World is looking for clean and renewable energy sources that do not pollute the environment, in an attempt to reduce greenhouse gas emissions that contribute to global warming.","Wind energy has significant potential to not only reduce greenhouse emission, but also meet the ever increasing demand for energy.","To enable the effective utilization of wind energy, addressing the following three challenges in wind data analysis is crucial.","Firstly, improving data resolution in various climate conditions to ensure an ample supply of information for assessing potential energy resources.","Secondly, implementing dimensionality reduction techniques for data collected from sensors/simulations to efficiently manage and store large datasets.","Thirdly, extrapolating wind data from one spatial specification to another, particularly in cases where data acquisition may be impractical or costly.","We propose a deep learning based approach to achieve multi-modal continuous resolution wind data prediction from discontinuous wind data, along with data dimensionality reduction."],"url":"http://arxiv.org/abs/2401.16936v1"}
{"created":"2024-01-30 11:29:11","title":"Interactive Byzantine-Resilient Gradient Coding for General Data Assignments","abstract":"We tackle the problem of Byzantine errors in distributed gradient descent within the Byzantine-resilient gradient coding framework. Our proposed solution can recover the exact full gradient in the presence of $s$ malicious workers with a data replication factor of only $s+1$. It generalizes previous solutions to any data assignment scheme that has a regular replication over all data samples. The scheme detects malicious workers through additional interactive communication and a small number of local computations at the main node, leveraging group-wise comparisons between workers with a provably optimal grouping strategy. The scheme requires at most $s$ interactive rounds that incur a total communication cost logarithmic in the number of data samples.","sentences":["We tackle the problem of Byzantine errors in distributed gradient descent within the Byzantine-resilient gradient coding framework.","Our proposed solution can recover the exact full gradient in the presence of $s$ malicious workers with a data replication factor of only $s+1$. It generalizes previous solutions to any data assignment scheme that has a regular replication over all data samples.","The scheme detects malicious workers through additional interactive communication and a small number of local computations at the main node, leveraging group-wise comparisons between workers with a provably optimal grouping strategy.","The scheme requires at most $s$ interactive rounds that incur a total communication cost logarithmic in the number of data samples."],"url":"http://arxiv.org/abs/2401.16915v1"}
{"created":"2024-01-30 11:04:36","title":"Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese as Multilingual Code-Switching","abstract":"Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model's pre-training data. Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language, thereby enhancing cross-lingual transfer capabilities. However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded. In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script. We present a novel dataset annotated with word-level etymology. We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese language. We contrast indiscriminate transliteration or translation to mixing processing pipelines that only transliterate words of Arabic origin, thereby resulting in text with a mixture of scripts. We fine-tune the processed data on four downstream tasks and show that conditional transliteration based on word etymology yields the best results, surpassing fine-tuning with raw Maltese or Maltese processed with non-selective pipelines.","sentences":["Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model's pre-training data.","Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language, thereby enhancing cross-lingual transfer capabilities.","However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded.","In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script.","We present a novel dataset annotated with word-level etymology.","We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese language.","We contrast indiscriminate transliteration or translation to mixing processing pipelines that only transliterate words of Arabic origin, thereby resulting in text with a mixture of scripts.","We fine-tune the processed data on four downstream tasks and show that conditional transliteration based on word etymology yields the best results, surpassing fine-tuning with raw Maltese or Maltese processed with non-selective pipelines."],"url":"http://arxiv.org/abs/2401.16895v1"}
{"created":"2024-01-30 10:31:15","title":"Enhancing EEG Signal-Based Emotion Recognition with Synthetic Data: Diffusion Modeel Approach","abstract":"Emotions are crucial in human life, influencing perceptions, relationships, behaviour, and choices. Emotion recognition using Electroencephalography (EEG) in the Brain-Computer Interface (BCI) domain presents significant challenges, particularly the need for extensive datasets. This study aims to generate synthetic EEG samples that are similar to real samples but are distinct by augmenting noise to a conditional denoising diffusion probabilistic model, thus addressing the prevalent issue of data scarcity in EEG research. The proposed method is tested on the DEAP dataset, showcasing a 1.94% improvement in classification performance when using synthetic data. This is higher compared to the traditional GAN-based and DDPM-based approaches. The proposed diffusion-based approach for EEG data generation appears promising in refining the accuracy of emotion recognition systems and marks a notable contribution to EEG-based emotion recognition. Our research further evaluates the effectiveness of state-of-the-art classifiers on EEG data, employing both real and synthetic data with varying noise levels.","sentences":["Emotions are crucial in human life, influencing perceptions, relationships, behaviour, and choices.","Emotion recognition using Electroencephalography (EEG) in the Brain-Computer Interface (BCI) domain presents significant challenges, particularly the need for extensive datasets.","This study aims to generate synthetic EEG samples that are similar to real samples but are distinct by augmenting noise to a conditional denoising diffusion probabilistic model, thus addressing the prevalent issue of data scarcity in EEG research.","The proposed method is tested on the DEAP dataset, showcasing a 1.94% improvement in classification performance when using synthetic data.","This is higher compared to the traditional GAN-based and DDPM-based approaches.","The proposed diffusion-based approach for EEG data generation appears promising in refining the accuracy of emotion recognition systems and marks a notable contribution to EEG-based emotion recognition.","Our research further evaluates the effectiveness of state-of-the-art classifiers on EEG data, employing both real and synthetic data with varying noise levels."],"url":"http://arxiv.org/abs/2401.16878v1"}
{"created":"2024-01-30 10:24:05","title":"A Scalable RISC-V Vector Processor Enabling Efficient Multi-Precision DNN Inference","abstract":"RISC-V processors encounter substantial challenges in deploying multi-precision deep neural networks (DNNs) due to their restricted precision support, constrained throughput, and suboptimal dataflow design. To tackle these challenges, a scalable RISC-V vector (RVV) processor, namely SPEED, is proposed to enable efficient multi-precision DNN inference by innovations from customized instructions, hardware architecture, and dataflow mapping. Firstly, dedicated customized RISC-V instructions are proposed based on RVV extensions, providing SPEED with fine-grained control over processing precision ranging from 4 to 16 bits. Secondly, a parameterized multi-precision systolic array unit is incorporated within the scalable module to enhance parallel processing capability and data reuse opportunities. Finally, a mixed multi-precision dataflow strategy, compatible with different convolution kernels and data precision, is proposed to effectively improve data utilization and computational efficiency. We perform synthesis of SPEED in TSMC 28nm technology. The experimental results demonstrate that SPEED achieves a peak throughput of 287.41 GOPS and an energy efficiency of 1335.79 GOPS/W at 4-bit precision condition, respectively. Moreover, when compared to the pioneer open-source vector processor Ara, SPEED provides an area efficiency improvement of 2.04$\\times$ and 1.63$\\times$ under 16-bit and 8-bit precision conditions, respectively, which shows SPEED's significant potential for efficient multi-precision DNN inference.","sentences":["RISC-V processors encounter substantial challenges in deploying multi-precision deep neural networks (DNNs) due to their restricted precision support, constrained throughput, and suboptimal dataflow design.","To tackle these challenges, a scalable RISC-V vector (RVV) processor, namely SPEED, is proposed to enable efficient multi-precision DNN inference by innovations from customized instructions, hardware architecture, and dataflow mapping.","Firstly, dedicated customized RISC-V instructions are proposed based on RVV extensions, providing SPEED with fine-grained control over processing precision ranging from 4 to 16 bits.","Secondly, a parameterized multi-precision systolic array unit is incorporated within the scalable module to enhance parallel processing capability and data reuse opportunities.","Finally, a mixed multi-precision dataflow strategy, compatible with different convolution kernels and data precision, is proposed to effectively improve data utilization and computational efficiency.","We perform synthesis of SPEED in TSMC 28nm technology.","The experimental results demonstrate that SPEED achieves a peak throughput of 287.41 GOPS and an energy efficiency of 1335.79 GOPS/W at 4-bit precision condition, respectively.","Moreover, when compared to the pioneer open-source vector processor Ara, SPEED provides an area efficiency improvement of 2.04$\\times$ and 1.63$\\times$ under 16-bit and 8-bit precision conditions, respectively, which shows SPEED's significant potential for efficient multi-precision DNN inference."],"url":"http://arxiv.org/abs/2401.16872v1"}
{"created":"2024-01-30 10:15:35","title":"New Centralized MSR Codes With Small Sub-packetization","abstract":"Centralized repair refers to repairing $h\\geq 2$ node failures using $d$ helper nodes in a centralized way, where the repair bandwidth is counted by the total amount of data downloaded from the helper nodes. A centralized MSR code is an MDS array code with $(h,d)$-optimal repair for some $h$ and $d$. In this paper, we present several classes of centralized MSR codes with small sub-packetization. At first, we construct an alternative MSR code with $(1,d_i)$-optimal repair for multiple repair degrees $d_i$ simultaneously. Based on the code structure, we are able to construct a centralized MSR code with $(h_i,d_i)$-optimal repair property for all possible $(h_i,d_i)$ with $h_i\\mid (d_i-k)$ simultaneously. The sub-packetization is no more than ${\\rm lcm}(1,2,\\ldots,n-k)(n-k)^n$, which is much smaller than a previous work given by Ye and Barg ($({\\rm lcm}(1,2,\\ldots,n-k))^n$). Moreover, for general parameters $2\\leq h\\leq n-k$ and $k\\leq d\\leq n-h$, we further give a centralized MSR code enabling $(h,d)$-optimal repair with sub-packetization smaller than all previous works.","sentences":["Centralized repair refers to repairing $h\\geq 2$ node failures using $d$ helper nodes in a centralized way, where the repair bandwidth is counted by the total amount of data downloaded from the helper nodes.","A centralized MSR code is an MDS array code with $(h,d)$-optimal repair for some $h$ and $d$. In this paper, we present several classes of centralized MSR codes with small sub-packetization.","At first, we construct an alternative MSR code with $(1,d_i)$-optimal repair for multiple repair degrees $d_i$ simultaneously.","Based on the code structure, we are able to construct a centralized MSR code with $(h_i,d_i)$-optimal repair property for all possible $(h_i,d_i)$ with $h_i\\mid (d_i-k)$ simultaneously.","The sub-packetization is no more than ${\\rm lcm}(1,2,\\ldots,n-k)(n-k)^n$, which is much smaller than a previous work given by Ye and Barg ($({\\rm lcm}(1,2,\\ldots,n-k))^n$).","Moreover, for general parameters $2\\leq h\\leq n-k$ and $k\\leq d\\leq n-h$, we further give a centralized MSR code enabling $(h,d)$-optimal repair with sub-packetization smaller than all previous works."],"url":"http://arxiv.org/abs/2401.16866v1"}
{"created":"2024-01-30 10:05:03","title":"State Value Generation with Prompt Learning and Self-Training for Low-Resource Dialogue State Tracking","abstract":"Recently, low-resource dialogue state tracking (DST) has received increasing attention. First obtaining state values then based on values to generate slot types has made great progress in this task. However, obtaining state values is still an under-studied problem. Existing extraction-based approaches cannot capture values that require the understanding of context and are not generalizable either. To address these issues, we propose a novel State VAlue Generation based framework (SVAG), decomposing DST into state value generation and domain slot generation. Specifically, we propose to generate state values and use self-training to further improve state value generation. Moreover, we design an estimator aiming at detecting incomplete generation and incorrect generation for pseudo-labeled data selection during self-training. Experimental results on the MultiWOZ 2.1 dataset show that our method which has only less than 1 billion parameters achieves state-of-the-art performance under the data ratio settings of 5%, 10%, and 25% when limited to models under 100 billion parameters. Compared to models with more than 100 billion parameters, SVAG still reaches competitive results.","sentences":["Recently, low-resource dialogue state tracking (DST) has received increasing attention.","First obtaining state values then based on values to generate slot types has made great progress in this task.","However, obtaining state values is still an under-studied problem.","Existing extraction-based approaches cannot capture values that require the understanding of context and are not generalizable either.","To address these issues, we propose a novel State VAlue Generation based framework (SVAG), decomposing DST into state value generation and domain slot generation.","Specifically, we propose to generate state values and use self-training to further improve state value generation.","Moreover, we design an estimator aiming at detecting incomplete generation and incorrect generation for pseudo-labeled data selection during self-training.","Experimental results on the MultiWOZ 2.1 dataset show that our method which has only less than 1 billion parameters achieves state-of-the-art performance under the data ratio settings of 5%, 10%, and 25% when limited to models under 100 billion parameters.","Compared to models with more than 100 billion parameters, SVAG still reaches competitive results."],"url":"http://arxiv.org/abs/2401.16862v1"}
{"created":"2024-01-30 09:55:14","title":"Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess","abstract":"This paper presents a new approach that integrates deep learning with computational chess, using both the Mixture of Experts (MoE) method and Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized models, each designed to respond to specific changes in the game's input data. This results in a framework with sparsely activated models, which provides significant computational benefits. Our framework combines the MoE method with MCTS, in order to align it with the strategic phases of chess, thus departing from the conventional ``one-for-all'' model. Instead, we utilize distinct game phase definitions to effectively distribute computational tasks across multiple expert neural networks. Our empirical research shows a substantial improvement in playing strength, surpassing the traditional single-model framework. This validates the efficacy of our integrated approach and highlights the potential of incorporating expert knowledge and strategic principles into neural network design. The fusion of MoE and MCTS offers a promising avenue for advancing machine learning architectures.","sentences":["This paper presents a new approach that integrates deep learning with computational chess, using both the Mixture of Experts (MoE) method and Monte-Carlo Tree Search (MCTS).","Our methodology employs a suite of specialized models, each designed to respond to specific changes in the game's input data.","This results in a framework with sparsely activated models, which provides significant computational benefits.","Our framework combines the MoE method with MCTS, in order to align it with the strategic phases of chess, thus departing from the conventional ``one-for-all'' model.","Instead, we utilize distinct game phase definitions to effectively distribute computational tasks across multiple expert neural networks.","Our empirical research shows a substantial improvement in playing strength, surpassing the traditional single-model framework.","This validates the efficacy of our integrated approach and highlights the potential of incorporating expert knowledge and strategic principles into neural network design.","The fusion of MoE and MCTS offers a promising avenue for advancing machine learning architectures."],"url":"http://arxiv.org/abs/2401.16852v1"}
{"created":"2024-01-30 09:39:04","title":"Reading yesterday's news. Layout recognition by segmentation of historical newspaper pages","abstract":"Newspapers are important sources for historians interested in past societies' cultural values, social structures, and their changes. Since the 19th century, newspapers have been widely available and spread regionally. Today, historical newspapers are digitized but unavailable in a separate metadata-enhanced form. Machine-readable metadata, however, is a prerequisite for a mass statistical analysis of this source. This paper focuses on parsing the complex layout of historic newspaper pages, which today's machines do not understand well. We argue for using neural networks, which require detailed annotated data in large numbers. Our Bonn newspaper dataset consists of 486 pages of the \\textit{K\\\"olnische Zeitung} from the years 1866 and 1924. We propose solving the newspaper-understanding problem by training a U-Net on our new dataset, which delivers satisfactory performance.","sentences":["Newspapers are important sources for historians interested in past societies' cultural values, social structures, and their changes.","Since the 19th century, newspapers have been widely available and spread regionally.","Today, historical newspapers are digitized but unavailable in a separate metadata-enhanced form.","Machine-readable metadata, however, is a prerequisite for a mass statistical analysis of this source.","This paper focuses on parsing the complex layout of historic newspaper pages, which today's machines do not understand well.","We argue for using neural networks, which require detailed annotated data in large numbers.","Our Bonn newspaper dataset consists of 486 pages of the \\textit{K\\\"olnische Zeitung} from the years 1866 and 1924.","We propose solving the newspaper-understanding problem by training a U-Net on our new dataset, which delivers satisfactory performance."],"url":"http://arxiv.org/abs/2401.16845v1"}
{"created":"2024-01-30 09:34:15","title":"Evaluating ML-Based Anomaly Detection Across Datasets of Varied Integrity: A Case Study","abstract":"Cybersecurity remains a critical challenge in the digital age, with network traffic flow anomaly detection being a key pivotal instrument in the fight against cyber threats. In this study, we address the prevalent issue of data integrity in network traffic datasets, which are instrumental in developing machine learning (ML) models for anomaly detection. We introduce two refined versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed using NFStream to ensure methodologically sound flow expiration and labeling. Our research contrasts the performance of the Random Forest (RF) algorithm across the original CICIDS-2017, its refined counterparts WTMC-2021 and CRiSIS-2022, and our NFStream-generated datasets, in both binary and multi-class classification contexts. We observe that the RF model exhibits exceptional robustness, achieving consistent high-performance metrics irrespective of the underlying dataset quality, which prompts a critical discussion on the actual impact of data integrity on ML efficacy. Our study underscores the importance of continual refinement and methodological rigor in dataset generation for network security research. As the landscape of network threats evolves, so must the tools and techniques used to detect and analyze them.","sentences":["Cybersecurity remains a critical challenge in the digital age, with network traffic flow anomaly detection being a key pivotal instrument in the fight against cyber threats.","In this study, we address the prevalent issue of data integrity in network traffic datasets, which are instrumental in developing machine learning (ML) models for anomaly detection.","We introduce two refined versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed using NFStream to ensure methodologically sound flow expiration and labeling.","Our research contrasts the performance of the Random Forest (RF) algorithm across the original CICIDS-2017, its refined counterparts WTMC-2021 and CRiSIS-2022, and our NFStream-generated datasets, in both binary and multi-class classification contexts.","We observe that the RF model exhibits exceptional robustness, achieving consistent high-performance metrics irrespective of the underlying dataset quality, which prompts a critical discussion on the actual impact of data integrity on ML efficacy.","Our study underscores the importance of continual refinement and methodological rigor in dataset generation for network security research.","As the landscape of network threats evolves, so must the tools and techniques used to detect and analyze them."],"url":"http://arxiv.org/abs/2401.16843v1"}
{"created":"2024-01-30 09:27:13","title":"jaxsnn: Event-driven Gradient Estimation for Analog Neuromorphic Hardware","abstract":"Traditional neuromorphic hardware architectures rely on event-driven computation, where the asynchronous transmission of events, such as spikes, triggers local computations within synapses and neurons. While machine learning frameworks are commonly used for gradient-based training, their emphasis on dense data structures poses challenges for processing asynchronous data such as spike trains. This problem is particularly pronounced for typical tensor data structures. In this context, we present a novel library (jaxsnn) built on top of JAX, that departs from conventional machine learning frameworks by providing flexibility in the data structures used and the handling of time, while maintaining Autograd functionality and composability. Our library facilitates the simulation of spiking neural networks and gradient estimation, with a focus on compatibility with time-continuous neuromorphic backends, such as the BrainScaleS-2 system, during the forward pass. This approach opens avenues for more efficient and flexible training of spiking neural networks, bridging the gap between traditional neuromorphic architectures and contemporary machine learning frameworks.","sentences":["Traditional neuromorphic hardware architectures rely on event-driven computation, where the asynchronous transmission of events, such as spikes, triggers local computations within synapses and neurons.","While machine learning frameworks are commonly used for gradient-based training, their emphasis on dense data structures poses challenges for processing asynchronous data such as spike trains.","This problem is particularly pronounced for typical tensor data structures.","In this context, we present a novel library (jaxsnn) built on top of JAX, that departs from conventional machine learning frameworks by providing flexibility in the data structures used and the handling of time, while maintaining Autograd functionality and composability.","Our library facilitates the simulation of spiking neural networks and gradient estimation, with a focus on compatibility with time-continuous neuromorphic backends, such as the BrainScaleS-2 system, during the forward pass.","This approach opens avenues for more efficient and flexible training of spiking neural networks, bridging the gap between traditional neuromorphic architectures and contemporary machine learning frameworks."],"url":"http://arxiv.org/abs/2401.16841v1"}
{"created":"2024-01-30 09:22:37","title":"Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition","abstract":"Nonnegative Matrix Factorization (NMF) is an important unsupervised learning method to extract meaningful features from data. To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability. This advancement offers a more efficient core representation for the original data. However, in the real world, the data is more natural to be represented as a multi-dimensional array, such as images or videos. The NMF's application to high-dimensional data involves vectorization, which risks losing essential multi-dimensional correlations. To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product. This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable Nonnegative Tensor Factorization (NTF). In this work, we provide an alternating index selection method to select the coseparable core. Furthermore, we validate the t-CUR sampling theory and integrate it with the tensor Discrete Empirical Interpolation Method (t-DEIM) to introduce an alternative, randomized index selection process. These methods have been tested on both synthetic and facial analysis datasets. The results demonstrate the efficiency of coseparable NTF when compared to coseparable NMF.","sentences":["Nonnegative Matrix Factorization (NMF) is an important unsupervised learning method to extract meaningful features from data.","To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability.","This advancement offers a more efficient core representation for the original data.","However, in the real world, the data is more natural to be represented as a multi-dimensional array, such as images or videos.","The NMF's application to high-dimensional data involves vectorization, which risks losing essential multi-dimensional correlations.","To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product.","This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable Nonnegative Tensor Factorization (NTF).","In this work, we provide an alternating index selection method to select the coseparable core.","Furthermore, we validate the t-CUR sampling theory and integrate it with the tensor Discrete Empirical Interpolation Method (t-DEIM) to introduce an alternative, randomized index selection process.","These methods have been tested on both synthetic and facial analysis datasets.","The results demonstrate the efficiency of coseparable NTF when compared to coseparable NMF."],"url":"http://arxiv.org/abs/2401.16836v1"}
{"created":"2024-01-30 09:19:50","title":"Analysis of Knowledge Tracing performance on synthesised student data","abstract":"Knowledge Tracing (KT) aims to predict the future performance of students by tracking the development of their knowledge states. Despite all the recent progress made in this field, the application of KT models in education systems is still restricted from the data perspectives: 1) limited access to real life data due to data protection concerns, 2) lack of diversity in public datasets, 3) noises in benchmark datasets such as duplicate records. To resolve these problems, we simulated student data with three statistical strategies based on public datasets and tested their performance on two KT baselines. While we observe only minor performance improvement with additional synthetic data, our work shows that using only synthetic data for training can lead to similar performance as real data.","sentences":["Knowledge Tracing (KT) aims to predict the future performance of students by tracking the development of their knowledge states.","Despite all the recent progress made in this field, the application of KT models in education systems is still restricted from the data perspectives: 1) limited access to real life data due to data protection concerns, 2) lack of diversity in public datasets, 3) noises in benchmark datasets such as duplicate records.","To resolve these problems, we simulated student data with three statistical strategies based on public datasets and tested their performance on two KT baselines.","While we observe only minor performance improvement with additional synthetic data, our work shows that using only synthetic data for training can lead to similar performance as real data."],"url":"http://arxiv.org/abs/2401.16832v1"}
{"created":"2024-01-30 08:11:36","title":"Encoding Temporal Statistical-space Priors via Augmented Representation","abstract":"Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are available throughout the writing for a clear and rigorous understanding.","sentences":["Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains.","Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners.","In response, we leverage a simple representation augmentation technique to overcome these challenges.","Our augmented representation acts as a statistical-space prior encoded at each time step.","In response, we name our method Statistical-space Augmented Representation (SSAR).","The underlying high-dimensional data-generating process inspires our representation augmentation.","We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms.","Our approach significantly beats all five up-to-date baselines.","Moreover, the highly modular nature of our approach can easily be applied to various settings.","Lastly, fully-fledged theoretical perspectives are available throughout the writing for a clear and rigorous understanding."],"url":"http://arxiv.org/abs/2401.16808v1"}
{"created":"2024-01-30 07:50:32","title":"PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset","abstract":"This article motivates, describes, and presents the PBSCSR dataset for studying composer style recognition of piano sheet music. Our overarching goal was to create a dataset for studying composer style recognition that is \"as accessible as MNIST and as challenging as ImageNet.\" To achieve this goal, we sample fixed-length bootleg score fragments from piano sheet music images on IMSLP. The dataset itself contains 40,000 62x64 bootleg score images for a 9-way classification task, 100,000 62x64 bootleg score images for a 100-way classification task, and 29,310 unlabeled variable-length bootleg score images for pretraining. The labeled data is presented in a form that mirrors MNIST images, in order to make it extremely easy to visualize, manipulate, and train models in an efficient manner. Additionally, we include relevant metadata to allow access to the underlying raw sheet music images and other related data on IMSLP. We describe several research tasks that could be studied with the dataset, including variations of composer style recognition in a few-shot or zero-shot setting. For tasks that have previously proposed models, we release code and baseline results for future works to compare against. We also discuss open research questions that the PBSCSR data is especially well suited to facilitate research on and areas of fruitful exploration in future work.","sentences":["This article motivates, describes, and presents the PBSCSR dataset for studying composer style recognition of piano sheet music.","Our overarching goal was to create a dataset for studying composer style recognition that is \"as accessible as MNIST and as challenging as ImageNet.\"","To achieve this goal, we sample fixed-length bootleg score fragments from piano sheet music images on IMSLP.","The dataset itself contains 40,000 62x64 bootleg score images for a 9-way classification task, 100,000 62x64 bootleg score images for a 100-way classification task, and 29,310 unlabeled variable-length bootleg score images for pretraining.","The labeled data is presented in a form that mirrors MNIST images, in order to make it extremely easy to visualize, manipulate, and train models in an efficient manner.","Additionally, we include relevant metadata to allow access to the underlying raw sheet music images and other related data on IMSLP.","We describe several research tasks that could be studied with the dataset, including variations of composer style recognition in a few-shot or zero-shot setting.","For tasks that have previously proposed models, we release code and baseline results for future works to compare against.","We also discuss open research questions that the PBSCSR data is especially well suited to facilitate research on and areas of fruitful exploration in future work."],"url":"http://arxiv.org/abs/2401.16803v1"}
{"created":"2024-01-30 07:31:51","title":"Online Algorithm for Node Feature Forecasting in Temporal Graphs","abstract":"In this paper, we propose an online algorithm \"mspace\" for forecasting node features in temporal graphs, which adeptly captures spatial cross-correlation among different nodes as well as the temporal autocorrelation within a node. The algorithm can be used for both probabilistic and deterministic multi-step forecasting, making it applicable for estimation and generation tasks. Comparative evaluations against various baselines, including graph neural network (GNN) based models and classical Kalman filters, demonstrate that mspace performs at par with the state-of-the-art and even surpasses them on some datasets. Importantly, mspace demonstrates consistent robustness across datasets with varying training sizes, a notable advantage over GNN-based methods requiring abundant training samples to learn the spatiotemporal trends in the data effectively. Therefore, employing mspace is advantageous in scenarios where the training sample availability is limited. Additionally, we establish theoretical bounds on multi-step forecasting error of mspace and show that it scales as $O(q)$ for $q$-step forecast.","sentences":["In this paper, we propose an online algorithm \"mspace\" for forecasting node features in temporal graphs, which adeptly captures spatial cross-correlation among different nodes as well as the temporal autocorrelation within a node.","The algorithm can be used for both probabilistic and deterministic multi-step forecasting, making it applicable for estimation and generation tasks.","Comparative evaluations against various baselines, including graph neural network (GNN) based models and classical Kalman filters, demonstrate that mspace performs at par with the state-of-the-art and even surpasses them on some datasets.","Importantly, mspace demonstrates consistent robustness across datasets with varying training sizes, a notable advantage over GNN-based methods requiring abundant training samples to learn the spatiotemporal trends in the data effectively.","Therefore, employing mspace is advantageous in scenarios where the training sample availability is limited.","Additionally, we establish theoretical bounds on multi-step forecasting error of mspace and show that it scales as $O(q)$ for $q$-step forecast."],"url":"http://arxiv.org/abs/2401.16800v1"}
{"created":"2024-01-30 07:19:36","title":"Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of Traditional EHR Data Imputation in Downstream Clinical Prediction","abstract":"Analyzing the health status of patients based on Electronic Health Records (EHR) is a fundamental research problem in medical informatics. The presence of extensive missing values in EHR makes it challenging for deep neural networks to directly model the patient's health status based on EHR. Existing deep learning training protocols require the use of statistical information or imputation models to reconstruct missing values; however, the protocols inject non-realistic data into downstream EHR analysis models, significantly limiting model performance. This paper introduces Learnable Prompt as Pseudo Imputation (PAI) as a new training protocol. PAI no longer introduces any imputed data but constructs a learnable prompt to model the implicit preferences of the downstream model for missing values, resulting in a significant performance improvement for all EHR analysis models. Additionally, our experiments show that PAI exhibits higher robustness in situations of data insufficiency and high missing rates. More importantly, in a real-world application involving cross-institutional data with zero-shot evaluation, PAI demonstrates stronger model generalization capabilities for non-overlapping features.","sentences":["Analyzing the health status of patients based on Electronic Health Records (EHR) is a fundamental research problem in medical informatics.","The presence of extensive missing values in EHR makes it challenging for deep neural networks to directly model the patient's health status based on EHR.","Existing deep learning training protocols require the use of statistical information or imputation models to reconstruct missing values; however, the protocols inject non-realistic data into downstream EHR analysis models, significantly limiting model performance.","This paper introduces Learnable Prompt as Pseudo Imputation (PAI) as a new training protocol.","PAI no longer introduces any imputed data but constructs a learnable prompt to model the implicit preferences of the downstream model for missing values, resulting in a significant performance improvement for all EHR analysis models.","Additionally, our experiments show that PAI exhibits higher robustness in situations of data insufficiency and high missing rates.","More importantly, in a real-world application involving cross-institutional data with zero-shot evaluation, PAI demonstrates stronger model generalization capabilities for non-overlapping features."],"url":"http://arxiv.org/abs/2401.16796v1"}
{"created":"2024-01-30 07:16:09","title":"Performance Insights-based AI-driven Football Transfer Fee Prediction","abstract":"We developed an artificial intelligence approach to predict the transfer fee of a football player. This model can help clubs make better decisions about which players to buy and sell, which can lead to improved performance and increased club budgets. Having collected data on player performance, transfer fees, and other factors that might affect a player's value, we then used this data to train a machine learning model that can accurately predict a player's impact on the game. We further passed the obtained results as one of the features to the predictor of transfer fees. The model can help clubs identify players who are undervalued and who could be sold for a profit. It can also help clubs avoid overpaying for players. We believe that our model can be a valuable tool for football clubs. It can help them make better decisions about player recruitment and transfers.","sentences":["We developed an artificial intelligence approach to predict the transfer fee of a football player.","This model can help clubs make better decisions about which players to buy and sell, which can lead to improved performance and increased club budgets.","Having collected data on player performance, transfer fees, and other factors that might affect a player's value, we then used this data to train a machine learning model that can accurately predict a player's impact on the game.","We further passed the obtained results as one of the features to the predictor of transfer fees.","The model can help clubs identify players who are undervalued and who could be sold for a profit.","It can also help clubs avoid overpaying for players.","We believe that our model can be a valuable tool for football clubs.","It can help them make better decisions about player recruitment and transfers."],"url":"http://arxiv.org/abs/2401.16795v1"}
{"created":"2024-01-30 07:11:03","title":"WideSA: A High Array Utilization Mapping Scheme for Uniform Recurrences on the Versal ACAP Architecture","abstract":"The Versal Adaptive Compute Acceleration Platform (ACAP) is a new architecture that combines AI Engines (AIEs) with reconfigurable fabric. This architecture offers significant acceleration potential for uniform recurrences in various domains, such as deep learning, high-performance computation, and signal processing. However, efficiently mapping these computations onto the Versal ACAP architecture while achieving high utilization of AIEs poses a challenge.   To address this issue, we propose a mapping scheme called \\fname, which aims to accelerate uniform recurrences on the Versal ACAP architecture by leveraging the features of both the hardware and the computations. Considering the array architecture of AIEs, our approach utilizes space-time transformations based on the polyhedral model to generate legally optimized systolic array mappings. Concurrently, we have developed a routing-aware PLIO assignment algorithm tailored for communication on the AIE array, and the algorithm aims at successful compilation while maximizing array utilization. Furthermore, we introduce an automatic mapping framework. This framework is designed to generate the corresponding executable code for uniform recurrences, which encompasses the AIE kernel program, programmable logic bitstreams, and the host program. The experimental results validate the effectiveness of our mapping scheme. Specifically, when applying our scheme to matrix multiplication computations on the VCK5000 board, we achieve a throughput of 4.15TOPS on float data type, which is 1.11$\\times$ higher compared to the state-of-the-art accelerator on the Versal ACAP architecture.","sentences":["The Versal Adaptive Compute Acceleration Platform (ACAP) is a new architecture that combines AI Engines (AIEs) with reconfigurable fabric.","This architecture offers significant acceleration potential for uniform recurrences in various domains, such as deep learning, high-performance computation, and signal processing.","However, efficiently mapping these computations onto the Versal ACAP architecture while achieving high utilization of AIEs poses a challenge.   ","To address this issue, we propose a mapping scheme called \\fname, which aims to accelerate uniform recurrences on the Versal ACAP architecture by leveraging the features of both the hardware and the computations.","Considering the array architecture of AIEs, our approach utilizes space-time transformations based on the polyhedral model to generate legally optimized systolic array mappings.","Concurrently, we have developed a routing-aware PLIO assignment algorithm tailored for communication on the AIE array, and the algorithm aims at successful compilation while maximizing array utilization.","Furthermore, we introduce an automatic mapping framework.","This framework is designed to generate the corresponding executable code for uniform recurrences, which encompasses the AIE kernel program, programmable logic bitstreams, and the host program.","The experimental results validate the effectiveness of our mapping scheme.","Specifically, when applying our scheme to matrix multiplication computations on the VCK5000 board, we achieve a throughput of 4.15TOPS on float data type, which is 1.11$\\times$ higher compared to the state-of-the-art accelerator on the Versal ACAP architecture."],"url":"http://arxiv.org/abs/2401.16792v1"}
{"created":"2024-01-30 07:09:48","title":"Accelerated Cloud for Artificial Intelligence (ACAI)","abstract":"Training an effective Machine learning (ML) model is an iterative process that requires effort in multiple dimensions. Vertically, a single pipeline typically includes an initial ETL (Extract, Transform, Load) of raw datasets, a model training stage, and an evaluation stage where the practitioners obtain statistics of the model performance. Horizontally, many such pipelines may be required to find the best model within a search space of model configurations. Many practitioners resort to maintaining logs manually and writing simple glue code to automate the workflow. However, carrying out this process on the cloud is not a trivial task in terms of resource provisioning, data management, and bookkeeping of job histories to make sure the results are reproducible. We propose an end-to-end cloud-based machine learning platform, Accelerated Cloud for AI (ACAI), to help improve the productivity of ML practitioners. ACAI achieves this goal by enabling cloud-based storage of indexed, labeled, and searchable data, as well as automatic resource provisioning, job scheduling, and experiment tracking. Specifically, ACAI provides practitioners (1) a data lake for storing versioned datasets and their corresponding metadata, and (2) an execution engine for executing ML jobs on the cloud with automatic resource provisioning (auto-provision), logging and provenance tracking. To evaluate ACAI, we test the efficacy of our auto-provisioner on the MNIST handwritten digit classification task, and we study the usability of our system using experiments and interviews. We show that our auto-provisioner produces a 1.7x speed-up and 39% cost reduction, and our system reduces experiment time for ML scientists by 20% on typical ML use cases.","sentences":["Training an effective Machine learning (ML) model is an iterative process that requires effort in multiple dimensions.","Vertically, a single pipeline typically includes an initial ETL (Extract, Transform, Load) of raw datasets, a model training stage, and an evaluation stage where the practitioners obtain statistics of the model performance.","Horizontally, many such pipelines may be required to find the best model within a search space of model configurations.","Many practitioners resort to maintaining logs manually and writing simple glue code to automate the workflow.","However, carrying out this process on the cloud is not a trivial task in terms of resource provisioning, data management, and bookkeeping of job histories to make sure the results are reproducible.","We propose an end-to-end cloud-based machine learning platform, Accelerated Cloud for AI (ACAI), to help improve the productivity of ML practitioners.","ACAI achieves this goal by enabling cloud-based storage of indexed, labeled, and searchable data, as well as automatic resource provisioning, job scheduling, and experiment tracking.","Specifically, ACAI provides practitioners (1) a data lake for storing versioned datasets and their corresponding metadata, and (2) an execution engine for executing ML jobs on the cloud with automatic resource provisioning (auto-provision), logging and provenance tracking.","To evaluate ACAI, we test the efficacy of our auto-provisioner on the MNIST handwritten digit classification task, and we study the usability of our system using experiments and interviews.","We show that our auto-provisioner produces a 1.7x speed-up and 39% cost reduction, and our system reduces experiment time for ML scientists by 20% on typical ML use cases."],"url":"http://arxiv.org/abs/2401.16791v1"}
{"created":"2024-01-30 06:51:24","title":"Graph Fairness Learning under Distribution Shifts","abstract":"Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. Motivated by our theoretical analysis, we propose our framework FatraGNN. Specifically, to guarantee fairness performance on unknown testing graphs, we propose a graph generator to produce numerous graphs with significant bias and under different distributions. Then we minimize the representation distances for each certain group between the training graph and generated graphs. This empowers our model to achieve high classification and fairness performance even on generated graphs with significant bias, thereby effectively handling unknown testing graphs. Experiments on real-world and semi-synthetic datasets demonstrate the effectiveness of our model in terms of both accuracy and fairness.","sentences":["Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data.","However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race.","Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph.","Will graph fairness performance decrease under distribution shifts?","How does distribution shifts affect graph fairness learning?","All these open questions are largely unexplored from a theoretical perspective.","To answer these questions, we first theoretically identify the factors that determine bias on a graph.","Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph.","Motivated by our theoretical analysis, we propose our framework FatraGNN.","Specifically, to guarantee fairness performance on unknown testing graphs, we propose a graph generator to produce numerous graphs with significant bias and under different distributions.","Then we minimize the representation distances for each certain group between the training graph and generated graphs.","This empowers our model to achieve high classification and fairness performance even on generated graphs with significant bias, thereby effectively handling unknown testing graphs.","Experiments on real-world and semi-synthetic datasets demonstrate the effectiveness of our model in terms of both accuracy and fairness."],"url":"http://arxiv.org/abs/2401.16784v1"}
{"created":"2024-01-30 06:35:52","title":"Addressing Distribution Shift in Time Series Forecasting with Instance Normalization Flows","abstract":"Due to non-stationarity of time series, the distribution shift problem largely hinders the performance of time series forecasting. Existing solutions either fail for the shifts beyond simple statistics or the limited compatibility with forecasting models. In this paper, we propose a general decoupled formulation for time series forecasting, with no reliance on fixed statistics and no restriction on forecasting architectures. Then, we make such a formulation formalized into a bi-level optimization problem, to enable the joint learning of the transformation (outer loop) and forecasting (inner loop). Moreover, the special requirements of expressiveness and bi-direction for the transformation motivate us to propose instance normalization flows (IN-Flow), a novel invertible network for time series transformation. Extensive experiments demonstrate our method consistently outperforms state-of-the-art baselines on both synthetic and real-world data.","sentences":["Due to non-stationarity of time series, the distribution shift problem largely hinders the performance of time series forecasting.","Existing solutions either fail for the shifts beyond simple statistics or the limited compatibility with forecasting models.","In this paper, we propose a general decoupled formulation for time series forecasting, with no reliance on fixed statistics and no restriction on forecasting architectures.","Then, we make such a formulation formalized into a bi-level optimization problem, to enable the joint learning of the transformation (outer loop) and forecasting (inner loop).","Moreover, the special requirements of expressiveness and bi-direction for the transformation motivate us to propose instance normalization flows (IN-Flow), a novel invertible network for time series transformation.","Extensive experiments demonstrate our method consistently outperforms state-of-the-art baselines on both synthetic and real-world data."],"url":"http://arxiv.org/abs/2401.16777v1"}
{"created":"2024-01-30 06:22:19","title":"Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator","abstract":"Imitation learning is often used in addition to reinforcement learning in environments where reward design is difficult or where the reward is sparse, but it is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data. Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift. The methods based on reinforcement learning, such as inverse reinforcement learning and Generative Adversarial imitation learning (GAIL), can learn from only a few expert data. However, they often need to interact with the environment. Soft Q imitation learning (SQIL) addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards. In order to make this algorithm more robust to distribution shift, we propose more efficient and robust algorithm by adding to this method a reward function based on adversarial inverse reinforcement learning that rewards the agent for performing actions in status similar to the demo. We call this algorithm Discriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo environments.","sentences":["Imitation learning is often used in addition to reinforcement learning in environments where reward design is difficult or where the reward is sparse, but it is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data.","Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift.","The methods based on reinforcement learning, such as inverse reinforcement learning and Generative Adversarial imitation learning (GAIL), can learn from only a few expert data.","However, they often need to interact with the environment.","Soft Q imitation learning (SQIL) addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards.","In order to make this algorithm more robust to distribution shift, we propose more efficient and robust algorithm by adding to this method a reward function based on adversarial inverse reinforcement learning that rewards the agent for performing actions in status similar to the demo.","We call this algorithm Discriminator Soft Q Imitation Learning (DSQIL).","We evaluated it on MuJoCo environments."],"url":"http://arxiv.org/abs/2401.16772v1"}
{"created":"2024-01-30 06:06:57","title":"Detection and Recovery Against Deep Neural Network Fault Injection Attacks Based on Contrastive Learning","abstract":"Deep Neural Network (DNN) models when implemented on executing devices as the inference engines are susceptible to Fault Injection Attacks (FIAs) that manipulate model parameters to disrupt inference execution with disastrous performance. This work introduces Contrastive Learning (CL) of visual representations i.e., a self-supervised learning approach into the deep learning training and inference pipeline to implement DNN inference engines with self-resilience under FIAs. Our proposed CL based FIA Detection and Recovery (CFDR) framework features (i) real-time detection with only a single batch of testing data and (ii) fast recovery effective even with only a small amount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on multiple types of FIAs, our CFDR shows promising detection and recovery effectiveness.","sentences":["Deep Neural Network (DNN) models when implemented on executing devices as the inference engines are susceptible to Fault Injection Attacks (FIAs) that manipulate model parameters to disrupt inference execution with disastrous performance.","This work introduces Contrastive Learning (CL) of visual representations i.e., a self-supervised learning approach into the deep learning training and inference pipeline to implement DNN inference engines with self-resilience under FIAs.","Our proposed CL based FIA Detection and Recovery (CFDR) framework features (i) real-time detection with only a single batch of testing data and (ii) fast recovery effective even with only a small amount of unlabeled testing data.","Evaluated with the CIFAR-10 dataset on multiple types of FIAs, our CFDR shows promising detection and recovery effectiveness."],"url":"http://arxiv.org/abs/2401.16766v1"}
{"created":"2024-01-30 05:41:59","title":"Sandi: A System for Accountability and Applications in Direct Communication","abstract":"We construct a system, Sandi, to bring trust in online communication between parties that share little or no context. Sandi is based on a unique ``somewhat monotone'' privacy-preserving reputation system, with strong privacy and security properties. Registered senders request cryptographic tags from Sandi, which they attach to their messages. Message receivers do not need registered accounts, but they can use a sender's score to decide how much the sender should be trusted. If a receiver finds the message inappropriate, they can use the tag to report the sender to Sandi, thus decreasing the sender's score. The design of Sandi ensures compatibility with any communication system that allows for small binary data transmission.   Sandi aims to benefit both senders and receivers. Senders benefit, as receivers are more likely to react to their messages with reputation scores attached. Receivers benefit, as they can make better choices in who to interact with based on indisputable evidence from prior receivers.   Sandi does not require senders or receivers to maintain long-term secret keys. We provide a score integrity guarantee for the senders, a full communication privacy guarantee for the senders and receivers, a report privacy guarantee to protect reporting receivers, and an unlinkability guarantee to protect senders.   Finally, we provide a game-theoretic analysis for the sender. We prove that, for any score function satisfying a list of properties, Sandi drives rational senders towards a strategy, which reduces the amount of inappropriate messages.","sentences":["We construct a system, Sandi, to bring trust in online communication between parties that share little or no context.","Sandi is based on a unique ``somewhat monotone'' privacy-preserving reputation system, with strong privacy and security properties.","Registered senders request cryptographic tags from Sandi, which they attach to their messages.","Message receivers do not need registered accounts, but they can use a sender's score to decide how much the sender should be trusted.","If a receiver finds the message inappropriate, they can use the tag to report the sender to Sandi, thus decreasing the sender's score.","The design of Sandi ensures compatibility with any communication system that allows for small binary data transmission.   ","Sandi aims to benefit both senders and receivers.","Senders benefit, as receivers are more likely to react to their messages with reputation scores attached.","Receivers benefit, as they can make better choices in who to interact with based on indisputable evidence from prior receivers.   ","Sandi does not require senders or receivers to maintain long-term secret keys.","We provide a score integrity guarantee for the senders, a full communication privacy guarantee for the senders and receivers, a report privacy guarantee to protect reporting receivers, and an unlinkability guarantee to protect senders.   ","Finally, we provide a game-theoretic analysis for the sender.","We prove that, for any score function satisfying a list of properties, Sandi drives rational senders towards a strategy, which reduces the amount of inappropriate messages."],"url":"http://arxiv.org/abs/2401.16759v1"}
{"created":"2024-01-30 04:56:55","title":"Detecting Racist Text in Bengali: An Ensemble Deep Learning Framework","abstract":"Racism is an alarming phenomenon in our country as well as all over the world. Every day we have come across some racist comments in our daily life and virtual life. Though we can eradicate this racism from virtual life (such as Social Media). In this paper, we have tried to detect those racist comments with NLP and deep learning techniques. We have built a novel dataset in the Bengali Language. Further, we annotated the dataset and conducted data label validation. After extensive utilization of deep learning methodologies, we have successfully achieved text detection with an impressive accuracy rate of 87.94\\% using the Ensemble approach. We have applied RNN and LSTM models using BERT Embeddings. However, the MCNN-LSTM model performed highest among all those models. Lastly, the Ensemble approach has been followed to combine all the model results to increase overall performance.","sentences":["Racism is an alarming phenomenon in our country as well as all over the world.","Every day we have come across some racist comments in our daily life and virtual life.","Though we can eradicate this racism from virtual life (such as Social Media).","In this paper, we have tried to detect those racist comments with NLP and deep learning techniques.","We have built a novel dataset in the Bengali Language.","Further, we annotated the dataset and conducted data label validation.","After extensive utilization of deep learning methodologies, we have successfully achieved text detection with an impressive accuracy rate of 87.94\\% using the Ensemble approach.","We have applied RNN and LSTM models using BERT Embeddings.","However, the MCNN-LSTM model performed highest among all those models.","Lastly, the Ensemble approach has been followed to combine all the model results to increase overall performance."],"url":"http://arxiv.org/abs/2401.16748v1"}
{"created":"2024-01-30 04:50:28","title":"MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models","abstract":"Large language models (LLMs) are increasingly relied upon for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks predominantly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or by creating new examples with GPT-4 to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 11 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance. MT-Eval is released publicly to encourage future research towards more robust conversational models.","sentences":["Large language models (LLMs) are increasingly relied upon for complex multi-turn conversations across diverse real-world applications.","However, existing benchmarks predominantly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions.","To address this gap, we introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities.","By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up.","We construct multi-turn queries for each category either by augmenting existing datasets or by creating new examples with GPT-4 to avoid data leakage.","To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance.","Our evaluation of 11 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks.","We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities.","Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance.","MT-Eval is released publicly to encourage future research towards more robust conversational models."],"url":"http://arxiv.org/abs/2401.16745v1"}
{"created":"2024-01-30 04:29:48","title":"Engineering A Large Language Model From Scratch","abstract":"The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical theory, the system achieves state-of-the-art results on natural language tasks whilst remaining interpretable and robust.","sentences":["The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency.","Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration.","The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs.","Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings.","Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines.","Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals.","By unifying modern deep learning techniques with software design principles and mathematical theory, the system achieves state-of-the-art results on natural language tasks whilst remaining interpretable and robust."],"url":"http://arxiv.org/abs/2401.16736v1"}
{"created":"2024-01-30 04:11:24","title":"Flash: A Hybrid Private Inference Protocol for Deep CNNs with High Accuracy and Low Latency on CPU","abstract":"This paper presents Flash, an optimized private inference (PI) hybrid protocol utilizing both homomorphic encryption (HE) and secure two-party computation (2PC), which can reduce the end-to-end PI latency for deep CNN models less than 1 minute with CPU. To this end, first, Flash proposes a low-latency convolution algorithm built upon a fast slot rotation operation and a novel data encoding scheme, which results in 4-94x performance gain over the state-of-the-art. Second, to minimize the communication cost introduced by the standard nonlinear activation function ReLU, Flash replaces the entire ReLUs with the polynomial $x^2+x$ and trains deep CNN models with the new activation function. The trained models improve the inference accuracy for CIFAR-10/100 and TinyImageNet by 16% on average (up to 40% for ResNet-32) compared to prior art. Last, Flash proposes an efficient 2PC-based $x^2+x$ evaluation protocol that does not require any offline communication and that reduces the total communication cost to process the activation layer by 84-196x over the state-of-the-art. As a result, the end-to-end PI latency of Flash implemented on CPU is 0.02 minute for CIFAR-100 and 0.57 minute for TinyImageNet classification, while the total data communication is 0.07GB for CIFAR-100 and 0.22GB for TinyImageNet. Flash improves the state-of-the-art PI by 16-45x in latency and 84-196x in communication cost. Moreover, even for ImageNet, Flash can deliver the latency less than 1 minute on CPU with the total communication less than 1GB.","sentences":["This paper presents Flash, an optimized private inference (PI) hybrid protocol utilizing both homomorphic encryption (HE) and secure two-party computation (2PC), which can reduce the end-to-end PI latency for deep CNN models less than 1 minute with CPU.","To this end, first, Flash proposes a low-latency convolution algorithm built upon a fast slot rotation operation and a novel data encoding scheme, which results in 4-94x performance gain over the state-of-the-art.","Second, to minimize the communication cost introduced by the standard nonlinear activation function ReLU, Flash replaces the entire ReLUs with the polynomial $x^2+x$ and trains deep CNN models with the new activation function.","The trained models improve the inference accuracy for CIFAR-10/100 and TinyImageNet by 16% on average (up to 40% for ResNet-32) compared to prior art.","Last, Flash proposes an efficient 2PC-based $x^2+x$ evaluation protocol that does not require any offline communication and that reduces the total communication cost to process the activation layer by 84-196x over the state-of-the-art.","As a result, the end-to-end PI latency of Flash implemented on CPU is 0.02 minute for CIFAR-100 and 0.57 minute for TinyImageNet classification, while the total data communication is 0.07GB for CIFAR-100 and 0.22GB for TinyImageNet.","Flash improves the state-of-the-art PI by 16-45x in latency and 84-196x in communication cost.","Moreover, even for ImageNet, Flash can deliver the latency less than 1 minute on CPU with the total communication less than 1GB."],"url":"http://arxiv.org/abs/2401.16732v1"}
{"created":"2024-01-30 04:06:25","title":"Towards Generating Informative Textual Description for Neurons in Language Models","abstract":"Recent developments in transformer-based language models have allowed them to capture a wide variety of world knowledge that can be adapted to downstream tasks with limited resources. However, what pieces of information are understood in these models is unclear, and neuron-level contributions in identifying them are largely unknown. Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model. In this paper, we take BERT as an example and we try to remove these constraints and propose a novel and scalable framework that ties textual descriptions to neurons. We leverage the potential of generative language models to discover human-interpretable descriptors present in a dataset and use an unsupervised approach to explain neurons with these descriptors. Through various qualitative and quantitative analyses, we demonstrate the effectiveness of this framework in generating useful data-specific descriptors with little human involvement in identifying the neurons that encode these descriptors. In particular, our experiment shows that the proposed approach achieves 75% precision@2, and 50% recall@2","sentences":["Recent developments in transformer-based language models have allowed them to capture a wide variety of world knowledge that can be adapted to downstream tasks with limited resources.","However, what pieces of information are understood in these models is unclear, and neuron-level contributions in identifying them are largely unknown.","Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model.","In this paper, we take BERT as an example and we try to remove these constraints and propose a novel and scalable framework that ties textual descriptions to neurons.","We leverage the potential of generative language models to discover human-interpretable descriptors present in a dataset and use an unsupervised approach to explain neurons with these descriptors.","Through various qualitative and quantitative analyses, we demonstrate the effectiveness of this framework in generating useful data-specific descriptors with little human involvement in identifying the neurons that encode these descriptors.","In particular, our experiment shows that the proposed approach achieves 75% precision@2, and 50% recall@2"],"url":"http://arxiv.org/abs/2401.16731v1"}
{"created":"2024-01-30 04:01:18","title":"Widely Linear Matched Filter: A Lynchpin towards the Interpretability of Complex-valued CNNs","abstract":"A recent study on the interpretability of real-valued convolutional neural networks (CNNs) \\cite{Stankovic_Mandic_2023CNN} has revealed a direct and physically meaningful link with the task of finding features in data through matched filters. However, applying this paradigm to illuminate the interpretability of complex-valued CNNs meets a formidable obstacle: the extension of matched filtering to a general class of noncircular complex-valued data, referred to here as the widely linear matched filter (WLMF), has been only implicit in the literature. To this end, to establish the interpretability of the operation of complex-valued CNNs, we introduce a general WLMF paradigm, provide its solution and undertake analysis of its performance. For rigor, our WLMF solution is derived without imposing any assumption on the probability density of noise. The theoretical advantages of the WLMF over its standard strictly linear counterpart (SLMF) are provided in terms of their output signal-to-noise-ratios (SNRs), with WLMF consistently exhibiting enhanced SNR. Moreover, the lower bound on the SNR gain of WLMF is derived, together with condition to attain this bound. This serves to revisit the convolution-activation-pooling chain in complex-valued CNNs through the lens of matched filtering, which reveals the potential of WLMFs to provide physical interpretability and enhance explainability of general complex-valued CNNs. Simulations demonstrate the agreement between the theoretical and numerical results.","sentences":["A recent study on the interpretability of real-valued convolutional neural networks (CNNs) \\cite{Stankovic_Mandic_2023CNN} has revealed a direct and physically meaningful link with the task of finding features in data through matched filters.","However, applying this paradigm to illuminate the interpretability of complex-valued CNNs meets a formidable obstacle: the extension of matched filtering to a general class of noncircular complex-valued data, referred to here as the widely linear matched filter (WLMF), has been only implicit in the literature.","To this end, to establish the interpretability of the operation of complex-valued CNNs, we introduce a general WLMF paradigm, provide its solution and undertake analysis of its performance.","For rigor, our WLMF solution is derived without imposing any assumption on the probability density of noise.","The theoretical advantages of the WLMF over its standard strictly linear counterpart (SLMF) are provided in terms of their output signal-to-noise-ratios (SNRs), with WLMF consistently exhibiting enhanced SNR.","Moreover, the lower bound on the SNR gain of WLMF is derived, together with condition to attain this bound.","This serves to revisit the convolution-activation-pooling chain in complex-valued CNNs through the lens of matched filtering, which reveals the potential of WLMFs to provide physical interpretability and enhance explainability of general complex-valued CNNs.","Simulations demonstrate the agreement between the theoretical and numerical results."],"url":"http://arxiv.org/abs/2401.16729v1"}
{"created":"2024-01-30 03:17:02","title":"LF Tracy: A Unified Single-Pipeline Approach for Salient Object Detection in Light Field Cameras","abstract":"Leveraging the rich information extracted from light field (LF) cameras is instrumental for dense prediction tasks. However, adapting light field data to enhance Salient Object Detection (SOD) still follows the traditional RGB methods and remains under-explored in the community. Previous approaches predominantly employ a custom two-stream design to discover the implicit angular feature within light field cameras, leading to significant information isolation between different LF representations. In this study, we propose an efficient paradigm (LF Tracy) to address this limitation. We eschew the conventional specialized fusion and decoder architecture for a dual-stream backbone in favor of a unified, single-pipeline approach. This comprises firstly a simple yet effective data augmentation strategy called MixLD to bridge the connection of spatial, depth, and implicit angular information under different LF representations. A highly efficient information aggregation (IA) module is then introduced to boost asymmetric feature-wise information fusion. Owing to this innovative approach, our model surpasses the existing state-of-the-art methods, particularly demonstrating a 23% improvement over previous results on the latest large-scale PKU dataset. By utilizing only 28.9M parameters, the model achieves a 10% increase in accuracy with 3M additional parameters compared to its backbone using RGB images and an 86% rise to its backbone using LF images. The source code will be made publicly available at https://github.com/FeiBryantkit/LF-Tracy.","sentences":["Leveraging the rich information extracted from light field (LF) cameras is instrumental for dense prediction tasks.","However, adapting light field data to enhance Salient Object Detection (SOD) still follows the traditional RGB methods and remains under-explored in the community.","Previous approaches predominantly employ a custom two-stream design to discover the implicit angular feature within light field cameras, leading to significant information isolation between different LF representations.","In this study, we propose an efficient paradigm (LF Tracy) to address this limitation.","We eschew the conventional specialized fusion and decoder architecture for a dual-stream backbone in favor of a unified, single-pipeline approach.","This comprises firstly a simple yet effective data augmentation strategy called MixLD to bridge the connection of spatial, depth, and implicit angular information under different LF representations.","A highly efficient information aggregation (IA) module is then introduced to boost asymmetric feature-wise information fusion.","Owing to this innovative approach, our model surpasses the existing state-of-the-art methods, particularly demonstrating a 23% improvement over previous results on the latest large-scale PKU dataset.","By utilizing only 28.9M parameters, the model achieves a 10% increase in accuracy with 3M additional parameters compared to its backbone using RGB images and an 86% rise to its backbone using LF images.","The source code will be made publicly available at https://github.com/FeiBryantkit/LF-Tracy."],"url":"http://arxiv.org/abs/2401.16712v1"}
{"created":"2024-01-30 03:14:52","title":"Dynamic Human Digital Twin Deployment at the Edge for Task Execution: A Two-Timescale Accuracy-Aware Online Optimization","abstract":"Human digital twin (HDT) is an emerging paradigm that bridges physical twins (PTs) with powerful virtual twins (VTs) for assisting complex task executions in human-centric services. In this paper, we study a two-timescale online optimization for building HDT under an end-edge-cloud collaborative framework. As a unique feature of HDT, we consider that PTs' corresponding VTs are deployed on edge servers, consisting of not only generic models placed by downloading experiential knowledge from the cloud but also customized models updated by collecting personalized data from end devices. To maximize task execution accuracy with stringent energy and delay constraints, and by taking into account HDT's inherent mobility and status variation uncertainties, we jointly and dynamically optimize VTs' construction and PTs' task offloading, along with communication and computation resource allocations. Observing that decision variables are asynchronous with different triggers, we propose a novel two-timescale accuracy-aware online optimization approach (TACO). Specifically, TACO utilizes an improved Lyapunov method to decompose the problem into multiple instant ones, and then leverages piecewise McCormick envelopes and block coordinate descent based algorithms, addressing two timescales alternately. Theoretical analyses and simulations show that the proposed approach can reach asymptotic optimum within a polynomial-time complexity, and demonstrate its superiority over counterparts.","sentences":["Human digital twin (HDT) is an emerging paradigm that bridges physical twins (PTs) with powerful virtual twins (VTs) for assisting complex task executions in human-centric services.","In this paper, we study a two-timescale online optimization for building HDT under an end-edge-cloud collaborative framework.","As a unique feature of HDT, we consider that PTs' corresponding VTs are deployed on edge servers, consisting of not only generic models placed by downloading experiential knowledge from the cloud but also customized models updated by collecting personalized data from end devices.","To maximize task execution accuracy with stringent energy and delay constraints, and by taking into account HDT's inherent mobility and status variation uncertainties, we jointly and dynamically optimize VTs' construction and PTs' task offloading, along with communication and computation resource allocations.","Observing that decision variables are asynchronous with different triggers, we propose a novel two-timescale accuracy-aware online optimization approach (TACO).","Specifically, TACO utilizes an improved Lyapunov method to decompose the problem into multiple instant ones, and then leverages piecewise McCormick envelopes and block coordinate descent based algorithms, addressing two timescales alternately.","Theoretical analyses and simulations show that the proposed approach can reach asymptotic optimum within a polynomial-time complexity, and demonstrate its superiority over counterparts."],"url":"http://arxiv.org/abs/2401.16710v1"}
{"created":"2024-01-30 03:00:25","title":"Towards Precise 3D Human Pose Estimation with Multi-Perspective Spatial-Temporal Relational Transformers","abstract":"3D human pose estimation captures the human joint points in three-dimensional space while keeping the depth information and physical structure. That is essential for applications that require precise pose information, such as human-computer interaction, scene understanding, and rehabilitation training. Due to the challenges in data collection, mainstream datasets of 3D human pose estimation are primarily composed of multi-view video data collected in laboratory environments, which contains rich spatial-temporal correlation information besides the image frame content. Given the remarkable self-attention mechanism of transformers, capable of capturing the spatial-temporal correlation from multi-view video datasets, we propose a multi-stage framework for 3D sequence-to-sequence (seq2seq) human pose detection. Firstly, the spatial module represents the human pose feature by intra-image content, while the frame-image relation module extracts temporal relationships and 3D spatial positional relationship features between the multi-perspective images. Secondly, the self-attention mechanism is adopted to eliminate the interference from non-human body parts and reduce computing resources. Our method is evaluated on Human3.6M, a popular 3D human pose detection dataset. Experimental results demonstrate that our approach achieves state-of-the-art performance on this dataset.","sentences":["3D human pose estimation captures the human joint points in three-dimensional space while keeping the depth information and physical structure.","That is essential for applications that require precise pose information, such as human-computer interaction, scene understanding, and rehabilitation training.","Due to the challenges in data collection, mainstream datasets of 3D human pose estimation are primarily composed of multi-view video data collected in laboratory environments, which contains rich spatial-temporal correlation information besides the image frame content.","Given the remarkable self-attention mechanism of transformers, capable of capturing the spatial-temporal correlation from multi-view video datasets, we propose a multi-stage framework for 3D sequence-to-sequence (seq2seq) human pose detection.","Firstly, the spatial module represents the human pose feature by intra-image content, while the frame-image relation module extracts temporal relationships and 3D spatial positional relationship features between the multi-perspective images.","Secondly, the self-attention mechanism is adopted to eliminate the interference from non-human body parts and reduce computing resources.","Our method is evaluated on Human3.6M, a popular 3D human pose detection dataset.","Experimental results demonstrate that our approach achieves state-of-the-art performance on this dataset."],"url":"http://arxiv.org/abs/2401.16700v1"}
{"created":"2024-01-30 02:58:45","title":"Towards Unified Interactive Visual Grounding in The Wild","abstract":"Interactive visual grounding in Human-Robot Interaction (HRI) is challenging yet practical due to the inevitable ambiguity in natural languages. It requires robots to disambiguate the user input by active information gathering. Previous approaches often rely on predefined templates to ask disambiguation questions, resulting in performance reduction in realistic interactive scenarios. In this paper, we propose TiO, an end-to-end system for interactive visual grounding in human-robot interaction. Benefiting from a unified formulation of visual dialogue and grounding, our method can be trained on a joint of extensive public data, and show superior generality to diversified and challenging open-world scenarios. In the experiments, we validate TiO on GuessWhat?! and InViG benchmarks, setting new state-of-the-art performance by a clear margin. Moreover, we conduct HRI experiments on the carefully selected 150 challenging scenes as well as real-robot platforms. Results show that our method demonstrates superior generality to diversified visual and language inputs with a high success rate. Codes and demos are available at https://github.com/jxu124/TiO.","sentences":["Interactive visual grounding in Human-Robot Interaction (HRI) is challenging yet practical due to the inevitable ambiguity in natural languages.","It requires robots to disambiguate the user input by active information gathering.","Previous approaches often rely on predefined templates to ask disambiguation questions, resulting in performance reduction in realistic interactive scenarios.","In this paper, we propose TiO, an end-to-end system for interactive visual grounding in human-robot interaction.","Benefiting from a unified formulation of visual dialogue and grounding, our method can be trained on a joint of extensive public data, and show superior generality to diversified and challenging open-world scenarios.","In the experiments, we validate TiO on GuessWhat?!","and InViG benchmarks, setting new state-of-the-art performance by a clear margin.","Moreover, we conduct HRI experiments on the carefully selected 150 challenging scenes as well as real-robot platforms.","Results show that our method demonstrates superior generality to diversified visual and language inputs with a high success rate.","Codes and demos are available at https://github.com/jxu124/TiO."],"url":"http://arxiv.org/abs/2401.16699v1"}
{"created":"2024-01-30 02:30:22","title":"A Detailed Historical and Statistical Analysis of the Influence of Hardware Artifacts on SPEC Integer Benchmark Performance","abstract":"The Standard Performance Evaluation Corporation (SPEC) CPU benchmark has been widely used as a measure of computing performance for decades. The SPEC is an industry-standardized, CPU-intensive benchmark suite and the collective data provide a proxy for the history of worldwide CPU and system performance. Past efforts have not provided or enabled answers to questions such as, how has the SPEC benchmark suite evolved empirically over time and what micro-architecture artifacts have had the most influence on performance? -- have any micro-benchmarks within the suite had undue influence on the results and comparisons among the codes? -- can the answers to these questions provide insights to the future of computer system performance? To answer these questions, we detail our historical and statistical analysis of specific hardware artifacts (clock frequencies, core counts, etc.) on the performance of the SPEC benchmarks since 1995. We discuss in detail several methods to normalize across benchmark evolutions. We perform both isolated and collective sensitivity analyses for various hardware artifacts and we identify one benchmark (libquantum) that had somewhat undue influence on performance outcomes. We also present the use of SPEC data to predict future performance.","sentences":["The Standard Performance Evaluation Corporation (SPEC) CPU benchmark has been widely used as a measure of computing performance for decades.","The SPEC is an industry-standardized, CPU-intensive benchmark suite and the collective data provide a proxy for the history of worldwide CPU and system performance.","Past efforts have not provided or enabled answers to questions such as, how has the SPEC benchmark suite evolved empirically over time and what micro-architecture artifacts have had the most influence on performance?","-- have any micro-benchmarks within the suite had undue influence on the results and comparisons among the codes?","-- can the answers to these questions provide insights to the future of computer system performance?","To answer these questions, we detail our historical and statistical analysis of specific hardware artifacts (clock frequencies, core counts, etc.)","on the performance of the SPEC benchmarks since 1995.","We discuss in detail several methods to normalize across benchmark evolutions.","We perform both isolated and collective sensitivity analyses for various hardware artifacts and we identify one benchmark (libquantum) that had somewhat undue influence on performance outcomes.","We also present the use of SPEC data to predict future performance."],"url":"http://arxiv.org/abs/2401.16690v1"}
{"created":"2024-01-30 02:18:30","title":"Revisiting Gradient Pruning: A Dual Realization for Defending against Gradient Attacks","abstract":"Collaborative learning (CL) is a distributed learning framework that aims to protect user privacy by allowing users to jointly train a model by sharing their gradient updates only. However, gradient inversion attacks (GIAs), which recover users' training data from shared gradients, impose severe privacy threats to CL. Existing defense methods adopt different techniques, e.g., differential privacy, cryptography, and perturbation defenses, to defend against the GIAs. Nevertheless, all current defense methods suffer from a poor trade-off between privacy, utility, and efficiency. To mitigate the weaknesses of existing solutions, we propose a novel defense method, Dual Gradient Pruning (DGP), based on gradient pruning, which can improve communication efficiency while preserving the utility and privacy of CL. Specifically, DGP slightly changes gradient pruning with a stronger privacy guarantee. And DGP can also significantly improve communication efficiency with a theoretical analysis of its convergence and generalization. Our extensive experiments show that DGP can effectively defend against the most powerful GIAs and reduce the communication cost without sacrificing the model's utility.","sentences":["Collaborative learning (CL) is a distributed learning framework that aims to protect user privacy by allowing users to jointly train a model by sharing their gradient updates only.","However, gradient inversion attacks (GIAs), which recover users' training data from shared gradients, impose severe privacy threats to CL.","Existing defense methods adopt different techniques, e.g., differential privacy, cryptography, and perturbation defenses, to defend against the GIAs.","Nevertheless, all current defense methods suffer from a poor trade-off between privacy, utility, and efficiency.","To mitigate the weaknesses of existing solutions, we propose a novel defense method, Dual Gradient Pruning (DGP), based on gradient pruning, which can improve communication efficiency while preserving the utility and privacy of CL.","Specifically, DGP slightly changes gradient pruning with a stronger privacy guarantee.","And DGP can also significantly improve communication efficiency with a theoretical analysis of its convergence and generalization.","Our extensive experiments show that DGP can effectively defend against the most powerful GIAs and reduce the communication cost without sacrificing the model's utility."],"url":"http://arxiv.org/abs/2401.16687v1"}
{"created":"2024-01-30 01:55:34","title":"T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives","abstract":"Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy.   To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently overlap serialized communication while minimizing resource contention with compute. T3 transparently fuses producer operations with the subsequent communication via a simple configuration of the producer's output address space and requires minor software changes. At the hardware level, T3 adds a lightweight track and trigger mechanism to orchestrate the producer's compute, and communication. It further uses compute-enhanced memories for communication's attendant compute. As a result, T3 reduces resource contention, and efficiently overlaps serialized communication with computation. For important Transformer models like T-NLG, T3 speeds up communication-heavy sublayers by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models scale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM and MT-NLG.","sentences":["Large Language Models increasingly rely on distributed techniques for their training and inference.","These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases.","While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution.","One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner.","However, this fine-grained interleaving of communication and computation in software can be difficult.","Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy.   ","To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently overlap serialized communication while minimizing resource contention with compute.","T3 transparently fuses producer operations with the subsequent communication via a simple configuration of the producer's output address space and requires minor software changes.","At the hardware level, T3 adds a lightweight track and trigger mechanism to orchestrate the producer's compute, and communication.","It further uses compute-enhanced memories for communication's attendant compute.","As a result, T3 reduces resource contention, and efficiently overlaps serialized communication with computation.","For important Transformer models like T-NLG, T3 speeds up communication-heavy sublayers by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%).","Furthermore, T3's benefits persist as models scale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM and MT-NLG."],"url":"http://arxiv.org/abs/2401.16677v1"}
{"created":"2024-01-30 01:45:03","title":"AutoIE: An Automated Framework for Information Extraction from Scientific Literature","abstract":"In the rapidly evolving field of scientific research, efficiently extracting key information from the burgeoning volume of scientific papers remains a formidable challenge. This paper introduces an innovative framework designed to automate the extraction of vital data from scientific PDF documents, enabling researchers to discern future research trajectories more readily. AutoIE uniquely integrates four novel components: (1) A multi-semantic feature fusion-based approach for PDF document layout analysis; (2) Advanced functional block recognition in scientific texts; (3) A synergistic technique for extracting and correlating information on molecular sieve synthesis; (4) An online learning paradigm tailored for molecular sieve literature. Our SBERT model achieves high Marco F1 scores of 87.19 and 89.65 on CoNLL04 and ADE datasets. In addition, a practical application of AutoIE in the petrochemical molecular sieve synthesis domain demonstrates its efficacy, evidenced by an impressive 78\\% accuracy rate. This research paves the way for enhanced data management and interpretation in molecular sieve synthesis. It is a valuable asset for seasoned experts and newcomers in this specialized field.","sentences":["In the rapidly evolving field of scientific research, efficiently extracting key information from the burgeoning volume of scientific papers remains a formidable challenge.","This paper introduces an innovative framework designed to automate the extraction of vital data from scientific PDF documents, enabling researchers to discern future research trajectories more readily.","AutoIE uniquely integrates four novel components: (1) A multi-semantic feature fusion-based approach for PDF document layout analysis; (2) Advanced functional block recognition in scientific texts; (3) A synergistic technique for extracting and correlating information on molecular sieve synthesis; (4) An online learning paradigm tailored for molecular sieve literature.","Our SBERT model achieves high Marco F1 scores of 87.19 and 89.65 on CoNLL04 and ADE datasets.","In addition, a practical application of AutoIE in the petrochemical molecular sieve synthesis domain demonstrates its efficacy, evidenced by an impressive 78\\% accuracy rate.","This research paves the way for enhanced data management and interpretation in molecular sieve synthesis.","It is a valuable asset for seasoned experts and newcomers in this specialized field."],"url":"http://arxiv.org/abs/2401.16672v1"}
{"created":"2024-01-30 01:28:48","title":"Fast Dual-Regularized Autoencoder for Sparse Biological Data","abstract":"Relationship inference from sparse data is an important task with applications ranging from product recommendation to drug discovery. A recently proposed linear model for sparse matrix completion has demonstrated surprising advantage in speed and accuracy over more sophisticated recommender systems algorithms. Here we extend the linear model to develop a shallow autoencoder for the dual neighborhood-regularized matrix completion problem. We demonstrate the speed and accuracy advantage of our approach over the existing state-of-the-art in predicting drug-target interactions and drug-disease associations.","sentences":["Relationship inference from sparse data is an important task with applications ranging from product recommendation to drug discovery.","A recently proposed linear model for sparse matrix completion has demonstrated surprising advantage in speed and accuracy over more sophisticated recommender systems algorithms.","Here we extend the linear model to develop a shallow autoencoder for the dual neighborhood-regularized matrix completion problem.","We demonstrate the speed and accuracy advantage of our approach over the existing state-of-the-art in predicting drug-target interactions and drug-disease associations."],"url":"http://arxiv.org/abs/2401.16664v1"}
{"created":"2024-01-30 01:24:43","title":"Generalization of LiNGAM that allows confounding","abstract":"LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding. Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding. As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types. In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact. This method efficiently achieves a globally optimal variable order through the shortest path problem formulation. LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations. Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, both in the presence and absence of confounding.","sentences":["LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding.","Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding.","As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types.","In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact.","This method efficiently achieves a globally optimal variable order through the shortest path problem formulation.","LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations.","Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, both in the presence and absence of confounding."],"url":"http://arxiv.org/abs/2401.16661v1"}
{"created":"2024-01-30 01:22:18","title":"OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer","abstract":"Recent studies have advocated for fully open foundation models to promote transparency and open science. As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data and open-source toolkits. With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders. In this work, we aim to improve the performance and efficiency of OWSM without extra training data. We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available. It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed. We publicly release the data preparation scripts, pre-trained models and training logs.","sentences":["Recent studies have advocated for fully open foundation models to promote transparency and open science.","As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data and open-source toolkits.","With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders.","In this work, we aim to improve the performance and efficiency of OWSM without extra training data.","We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available.","It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed.","We publicly release the data preparation scripts, pre-trained models and training logs."],"url":"http://arxiv.org/abs/2401.16658v1"}
