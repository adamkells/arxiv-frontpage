{"created":"2024-01-02 18:53:13","title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models","abstract":"Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.","sentences":["Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs).","In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data.","We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model.","At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself.","More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data.","Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT.","Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution.","Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench.","Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data.","This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents."],"url":"http://arxiv.org/abs/2401.01335v1"}
{"created":"2024-01-02 18:40:03","title":"TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview","abstract":"Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works. The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them. iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action. These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions about the information space include: finding options, comparing options, identifying the pros and cons of options, etc. Given the different personas and their information need (expressed through the sequence of questions), diverse conversation trajectories will arise -- because the answers to these similar queries will be very different. In this paper, we report on the first year of TREC iKAT, describing the task, topics, data collection, and evaluation framework. We further review the submissions and summarize the findings.","sentences":["Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works.","The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT).","However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context.","The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them.","iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action.","These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions about the information space include: finding options, comparing options, identifying the pros and cons of options, etc.","Given the different personas and their information need (expressed through the sequence of questions), diverse conversation trajectories will arise -- because the answers to these similar queries will be very different.","In this paper, we report on the first year of TREC iKAT, describing the task, topics, data collection, and evaluation framework.","We further review the submissions and summarize the findings."],"url":"http://arxiv.org/abs/2401.01330v1"}
{"created":"2024-01-02 17:58:43","title":"Classifying Words with 3-sort Automata","abstract":"Grammatical inference consists in learning a language or a grammar from data. In this paper, we consider a number of models for inferring a non-deterministic finite automaton (NFA) with 3 sorts of states, that must accept some words, and reject some other words from a given sample. We then propose a transformation from this 3-sort NFA into weighted-frequency and probabilistic NFA, and we apply the latter to a classification task. The experimental evaluation of our approach shows that the probabilistic NFAs can be successfully applied for classification tasks on both real-life and superficial benchmark data sets.","sentences":["Grammatical inference consists in learning a language or a grammar from data.","In this paper, we consider a number of models for inferring a non-deterministic finite automaton (NFA) with 3 sorts of states, that must accept some words, and reject some other words from a given sample.","We then propose a transformation from this 3-sort NFA into weighted-frequency and probabilistic NFA, and we apply the latter to a classification task.","The experimental evaluation of our approach shows that the probabilistic NFAs can be successfully applied for classification tasks on both real-life and superficial benchmark data sets."],"url":"http://arxiv.org/abs/2401.01314v1"}
{"created":"2024-01-02 17:56:30","title":"A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models","abstract":"As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.","sentences":["As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded.","This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives.","The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations.","Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training.","While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input.","This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc.","This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs.","Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023).","Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types.","This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs.","Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs."],"url":"http://arxiv.org/abs/2401.01313v2"}
{"created":"2024-01-02 17:30:46","title":"Experimental Validation of Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles","abstract":"In this paper, we validate the performance of the a sensor fusion-based Global Navigation Satellite System (GNSS) spoofing attack detection framework for Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSS receiver, along with Inertial Measurement Unit (IMU) is used. The detection framework incorporates two strategies: The first strategy involves comparing the predicted location shift, which is the distance traveled between two consecutive timestamps, with the inertial sensor-based location shift. For this purpose, data from low-cost in-vehicle inertial sensors such as the accelerometer and gyroscope sensor are fused and fed into a long short-term memory (LSTM) neural network. The second strategy employs a Random-Forest supervised machine learning model to detect and classify turns, distinguishing between left and right turns using the output from the steering angle sensor. In experiments, two types of spoofing attack models: turn-by-turn and wrong turn are simulated. These spoofing attacks are modeled as SQL injection attacks, where, upon successful implementation, the navigation system perceives injected spoofed location information as legitimate while being unable to detect legitimate GNSS signals. Importantly, the IMU data remains uncompromised throughout the spoofing attack. To test the effectiveness of the detection framework, experiments are conducted in Tuscaloosa, AL, mimicking urban road structures. The results demonstrate the framework's ability to detect various sophisticated GNSS spoofing attacks, even including slow position drifting attacks. Overall, the experimental results showcase the robustness and efficacy of the sensor fusion-based spoofing attack detection approach in safeguarding AVs against GNSS spoofing threats.","sentences":["In this paper, we validate the performance of the a sensor fusion-based Global Navigation Satellite System (GNSS) spoofing attack detection framework for Autonomous Vehicles (AVs).","To collect data, a vehicle equipped with a GNSS receiver, along with Inertial Measurement Unit (IMU) is used.","The detection framework incorporates two strategies: The first strategy involves comparing the predicted location shift, which is the distance traveled between two consecutive timestamps, with the inertial sensor-based location shift.","For this purpose, data from low-cost in-vehicle inertial sensors such as the accelerometer and gyroscope sensor are fused and fed into a long short-term memory (LSTM) neural network.","The second strategy employs a Random-Forest supervised machine learning model to detect and classify turns, distinguishing between left and right turns using the output from the steering angle sensor.","In experiments, two types of spoofing attack models: turn-by-turn and wrong turn are simulated.","These spoofing attacks are modeled as SQL injection attacks, where, upon successful implementation, the navigation system perceives injected spoofed location information as legitimate while being unable to detect legitimate GNSS signals.","Importantly, the IMU data remains uncompromised throughout the spoofing attack.","To test the effectiveness of the detection framework, experiments are conducted in Tuscaloosa, AL, mimicking urban road structures.","The results demonstrate the framework's ability to detect various sophisticated GNSS spoofing attacks, even including slow position drifting attacks.","Overall, the experimental results showcase the robustness and efficacy of the sensor fusion-based spoofing attack detection approach in safeguarding AVs against GNSS spoofing threats."],"url":"http://arxiv.org/abs/2401.01304v1"}
{"created":"2024-01-02 17:30:18","title":"On the uniqueness and computation of commuting extensions","abstract":"A tuple (Z_1,...,Z_p) of matrices of size r is said to be a commuting extension of a tuple (A_1,...,A_p) of matrices of size n <r if the Z_i pairwise commute and each A_i sits in the upper left corner of a block decomposition of Z_i. This notion was discovered and rediscovered in several contexts including algebraic complexity theory (in Strassen's work on tensor rank), in numerical analysis for the construction of cubature formulas and in quantum mechanics for the study of computational methods and the study of the so-called \"quantum Zeno dynamics.\" Commuting extensions have also attracted the attention of the linear algebra community. In this paper we present 3 types of results:   (i) Theorems on the uniqueness of commuting extensions for three matrices or more.   (ii) Algorithms for the computation of commuting extensions of minimal size. These algorithms work under the same assumptions as our uniqueness theorems. They are applicable up to r=4n/3, and are apparently the first provably efficient algorithms for this problem applicable beyond r=n+1.   (iii) A genericity theorem showing that our algorithms and uniqueness theorems can be applied to a wide range of input matrices.","sentences":["A tuple (Z_1,...,Z_p) of matrices of size r is said to be a commuting extension of a tuple (A_1,...,A_p) of matrices of size n <r if the Z_i pairwise commute and each A_i sits in the upper left corner of a block decomposition of Z_i.","This notion was discovered and rediscovered in several contexts including algebraic complexity theory (in Strassen's work on tensor rank), in numerical analysis for the construction of cubature formulas and in quantum mechanics for the study of computational methods and the study of the so-called \"quantum Zeno dynamics.\"","Commuting extensions have also attracted the attention of the linear algebra community.","In this paper we present 3 types of results:   (i) Theorems on the uniqueness of commuting extensions for three matrices or more.   ","(ii) Algorithms for the computation of commuting extensions of minimal size.","These algorithms work under the same assumptions as our uniqueness theorems.","They are applicable up to r=4n/3, and are apparently the first provably efficient algorithms for this problem applicable beyond r=n+1.   ","(iii) A genericity theorem showing that our algorithms and uniqueness theorems can be applied to a wide range of input matrices."],"url":"http://arxiv.org/abs/2401.01302v1"}
{"created":"2024-01-02 16:56:13","title":"Physics-informed Generalizable Wireless Channel Modeling with Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges","abstract":"Channel modeling is fundamental in advancing wireless systems and has thus attracted considerable research focus. Recent trends have seen a growing reliance on data-driven techniques to facilitate the modeling process and yield accurate channel predictions. In this work, we first provide a concise overview of data-driven channel modeling methods, highlighting their limitations. Subsequently, we introduce the concept and advantages of physics-informed neural network (PINN)-based modeling and a summary of recent contributions in this area. Our findings demonstrate that PINN-based approaches in channel modeling exhibit promising attributes such as generalizability, interpretability, and robustness. We offer a comprehensive architecture for PINN methodology, designed to inform and inspire future model development. A case-study of our recent work on precise indoor channel prediction with semantic segmentation and deep learning is presented. The study concludes by addressing the challenges faced and suggesting potential research directions in this field.","sentences":["Channel modeling is fundamental in advancing wireless systems and has thus attracted considerable research focus.","Recent trends have seen a growing reliance on data-driven techniques to facilitate the modeling process and yield accurate channel predictions.","In this work, we first provide a concise overview of data-driven channel modeling methods, highlighting their limitations.","Subsequently, we introduce the concept and advantages of physics-informed neural network (PINN)-based modeling and a summary of recent contributions in this area.","Our findings demonstrate that PINN-based approaches in channel modeling exhibit promising attributes such as generalizability, interpretability, and robustness.","We offer a comprehensive architecture for PINN methodology, designed to inform and inspire future model development.","A case-study of our recent work on precise indoor channel prediction with semantic segmentation and deep learning is presented.","The study concludes by addressing the challenges faced and suggesting potential research directions in this field."],"url":"http://arxiv.org/abs/2401.01288v1"}
{"created":"2024-01-02 16:52:50","title":"Socially Responsible Computing in an Introductory Course","abstract":"Given the potential for technology to inflict harm and injustice on society, it is imperative that we cultivate a sense of social responsibility among our students as they progress through the Computer Science (CS) curriculum. Our students need to be able to examine the social complexities in which technology development and use are situated. Also, aligning students' personal goals and their ability to achieve them in their field of study is important for promoting motivation and a sense of belonging. Promoting communal goals while learning computing can help broaden participation, particularly among groups who have been historically marginalized in computing. Keeping these considerations in mind, we piloted an introductory Java programming course in which activities engaging students in ethical and socially responsible considerations were integrated across modules. Rather than adding social on top of the technical content, our curricular approach seeks to weave them together. The data from the class suggests that the students found the inclusion of the social context in the technical assignments to be more motivating and expressed greater agency in realizing social change. We share our approach to designing this new introductory socially responsible computing course and the students' reflections. We also highlight seven considerations for educators seeking to incorporate socially responsible computing.","sentences":["Given the potential for technology to inflict harm and injustice on society, it is imperative that we cultivate a sense of social responsibility among our students as they progress through the Computer Science (CS) curriculum.","Our students need to be able to examine the social complexities in which technology development and use are situated.","Also, aligning students' personal goals and their ability to achieve them in their field of study is important for promoting motivation and a sense of belonging.","Promoting communal goals while learning computing can help broaden participation, particularly among groups who have been historically marginalized in computing.","Keeping these considerations in mind, we piloted an introductory Java programming course in which activities engaging students in ethical and socially responsible considerations were integrated across modules.","Rather than adding social on top of the technical content, our curricular approach seeks to weave them together.","The data from the class suggests that the students found the inclusion of the social context in the technical assignments to be more motivating and expressed greater agency in realizing social change.","We share our approach to designing this new introductory socially responsible computing course and the students' reflections.","We also highlight seven considerations for educators seeking to incorporate socially responsible computing."],"url":"http://arxiv.org/abs/2401.01285v1"}
{"created":"2024-01-02 16:37:42","title":"GEqO: ML-Accelerated Semantic Equivalence Detection","abstract":"Large scale analytics engines have become a core dependency for modern data-driven enterprises to derive business insights and drive actions. These engines support a large number of analytic jobs processing huge volumes of data on a daily basis, and workloads are often inundated with overlapping computations across multiple jobs. Reusing common computation is crucial for efficient cluster resource utilization and reducing job execution time. Detecting common computation is the first and key step for reducing this computational redundancy. However, detecting equivalence on large-scale analytics engines requires efficient and scalable solutions that are fully automated. In addition, to maximize computation reuse, equivalence needs to be detected at the semantic level instead of just the syntactic level (i.e., the ability to detect semantic equivalence of seemingly different-looking queries). Unfortunately, existing solutions fall short of satisfying these requirements.   In this paper, we take a major step towards filling this gap by proposing GEqO, a portable and lightweight machine-learning-based framework for efficiently identifying semantically equivalent computations at scale. GEqO introduces two machine-learning-based filters that quickly prune out nonequivalent subexpressions and employs a semi-supervised learning feedback loop to iteratively improve its model with an intelligent sampling mechanism. Further, with its novel database-agnostic featurization method, GEqO can transfer the learning from one workload and database to another. Our extensive empirical evaluation shows that, on TPC-DS-like queries, GEqO yields significant performance gains-up to 200x faster than automated verifiers-and finds up to 2x more equivalences than optimizer and signature-based equivalence detection approaches.","sentences":["Large scale analytics engines have become a core dependency for modern data-driven enterprises to derive business insights and drive actions.","These engines support a large number of analytic jobs processing huge volumes of data on a daily basis, and workloads are often inundated with overlapping computations across multiple jobs.","Reusing common computation is crucial for efficient cluster resource utilization and reducing job execution time.","Detecting common computation is the first and key step for reducing this computational redundancy.","However, detecting equivalence on large-scale analytics engines requires efficient and scalable solutions that are fully automated.","In addition, to maximize computation reuse, equivalence needs to be detected at the semantic level instead of just the syntactic level (i.e., the ability to detect semantic equivalence of seemingly different-looking queries).","Unfortunately, existing solutions fall short of satisfying these requirements.   ","In this paper, we take a major step towards filling this gap by proposing GEqO, a portable and lightweight machine-learning-based framework for efficiently identifying semantically equivalent computations at scale.","GEqO introduces two machine-learning-based filters that quickly prune out nonequivalent subexpressions and employs a semi-supervised learning feedback loop to iteratively improve its model with an intelligent sampling mechanism.","Further, with its novel database-agnostic featurization method, GEqO can transfer the learning from one workload and database to another.","Our extensive empirical evaluation shows that, on TPC-DS-like queries, GEqO yields significant performance gains-up to 200x faster than automated verifiers-and finds up to 2x more equivalences than optimizer and signature-based equivalence detection approaches."],"url":"http://arxiv.org/abs/2401.01280v1"}
{"created":"2024-01-02 16:20:40","title":"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation","abstract":"Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.","sentences":["Recently, the advent of large language models (LLMs) has revolutionized generative agents.","Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users.","However, the absence of a comprehensive benchmark impedes progress in this field.","To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset.","The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts.","It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike.","CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions.","Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.","Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval."],"url":"http://arxiv.org/abs/2401.01275v1"}
{"created":"2024-01-02 16:14:30","title":"LLbezpeky: Leveraging Large Language Models for Vulnerability Detection","abstract":"Despite the continued research and progress in building secure systems, Android applications continue to be ridden with vulnerabilities, necessitating effective detection methods. Current strategies involving static and dynamic analysis tools come with limitations like overwhelming number of false positives and limited scope of analysis which make either difficult to adopt. Over the past years, machine learning based approaches have been extensively explored for vulnerability detection, but its real-world applicability is constrained by data requirements and feature engineering challenges. Large Language Models (LLMs), with their vast parameters, have shown tremendous potential in understanding semnatics in human as well as programming languages. We dive into the efficacy of LLMs for detecting vulnerabilities in the context of Android security. We focus on building an AI-driven workflow to assist developers in identifying and rectifying vulnerabilities. Our experiments show that LLMs outperform our expectations in finding issues within applications correctly flagging insecure apps in 91.67% of cases in the Ghera benchmark. We use inferences from our experiments towards building a robust and actionable vulnerability detection system and demonstrate its effectiveness. Our experiments also shed light on how different various simple configurations can affect the True Positive (TP) and False Positive (FP) rates.","sentences":["Despite the continued research and progress in building secure systems, Android applications continue to be ridden with vulnerabilities, necessitating effective detection methods.","Current strategies involving static and dynamic analysis tools come with limitations like overwhelming number of false positives and limited scope of analysis which make either difficult to adopt.","Over the past years, machine learning based approaches have been extensively explored for vulnerability detection, but its real-world applicability is constrained by data requirements and feature engineering challenges.","Large Language Models (LLMs), with their vast parameters, have shown tremendous potential in understanding semnatics in human as well as programming languages.","We dive into the efficacy of LLMs for detecting vulnerabilities in the context of Android security.","We focus on building an AI-driven workflow to assist developers in identifying and rectifying vulnerabilities.","Our experiments show that LLMs outperform our expectations in finding issues within applications correctly flagging insecure apps in 91.67% of cases in the Ghera benchmark.","We use inferences from our experiments towards building a robust and actionable vulnerability detection system and demonstrate its effectiveness.","Our experiments also shed light on how different various simple configurations can affect the True Positive (TP) and False Positive (FP) rates."],"url":"http://arxiv.org/abs/2401.01269v1"}
{"created":"2024-01-02 16:14:02","title":"$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy","abstract":"In deep learning, classification tasks are formalized as optimization problems solved via the minimization of the cross-entropy. However, recent advancements in the design of objective functions allow the $f$-divergence measure to generalize the formulation of the optimization problem for classification. With this goal in mind, we adopt a Bayesian perspective and formulate the classification task as a maximum a posteriori probability problem. We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences. In addition, driven by the challenge of improving the state-of-the-art approach, we propose a bottom-up method that leads us to the formulation of a new objective function (and posterior probability estimator) corresponding to a novel $f$-divergence referred to as shifted log (SL). First, we theoretically prove the convergence property of the posterior probability estimators. Then, we numerically test the set of proposed objective functions in three application scenarios: toy examples, image data sets, and signal detection/decoding problems. The analyzed tasks demonstrate the effectiveness of the proposed estimators and that the SL divergence achieves the highest classification accuracy in almost all the scenarios.","sentences":["In deep learning, classification tasks are formalized as optimization problems solved via the minimization of the cross-entropy.","However, recent advancements in the design of objective functions allow the $f$-divergence measure to generalize the formulation of the optimization problem for classification.","With this goal in mind, we adopt a Bayesian perspective and formulate the classification task as a maximum a posteriori probability problem.","We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences.","In addition, driven by the challenge of improving the state-of-the-art approach, we propose a bottom-up method that leads us to the formulation of a new objective function (and posterior probability estimator) corresponding to a novel $f$-divergence referred to as shifted log (SL).","First, we theoretically prove the convergence property of the posterior probability estimators.","Then, we numerically test the set of proposed objective functions in three application scenarios: toy examples, image data sets, and signal detection/decoding problems.","The analyzed tasks demonstrate the effectiveness of the proposed estimators and that the SL divergence achieves the highest classification accuracy in almost all the scenarios."],"url":"http://arxiv.org/abs/2401.01268v1"}
{"created":"2024-01-02 15:58:17","title":"Profiling Programming Language Learning","abstract":"This paper documents a year-long experiment to \"profile\" the process of learning a programming language: gathering data to understand what makes a language hard to learn, and using that data to improve the learning process. We added interactive quizzes to The Rust Programming Language, the official textbook for learning Rust. Over 13 months, 62,526 readers answered questions 1,140,202 times. First, we analyze the trajectories of readers. We find that many readers drop-out of the book early when faced with difficult language concepts like Rust's ownership types. Second, we use classical test theory and item response theory to analyze the characteristics of quiz questions. We find that better questions are more conceptual in nature, such as asking why a program does not compile vs. whether a program compiles. Third, we performed 12 interventions into the book to help readers with difficult questions. We find that on average, interventions improved quiz scores on the targeted questions by +20%. Fourth, we show that our technique can likely generalize to languages with smaller user bases by simulating our statistical inferences on small N. These results demonstrate that quizzes are a simple and useful technique for understanding language learning at all scales.","sentences":["This paper documents a year-long experiment to \"profile\" the process of learning a programming language: gathering data to understand what makes a language hard to learn, and using that data to improve the learning process.","We added interactive quizzes to The Rust Programming Language, the official textbook for learning Rust.","Over 13 months, 62,526 readers answered questions 1,140,202 times.","First, we analyze the trajectories of readers.","We find that many readers drop-out of the book early when faced with difficult language concepts like Rust's ownership types.","Second, we use classical test theory and item response theory to analyze the characteristics of quiz questions.","We find that better questions are more conceptual in nature, such as asking why a program does not compile vs. whether a program compiles.","Third, we performed 12 interventions into the book to help readers with difficult questions.","We find that on average, interventions improved quiz scores on the targeted questions by +20%.","Fourth, we show that our technique can likely generalize to languages with smaller user bases by simulating our statistical inferences on small N.","These results demonstrate that quizzes are a simple and useful technique for understanding language learning at all scales."],"url":"http://arxiv.org/abs/2401.01257v1"}
{"created":"2024-01-02 15:40:35","title":"Deplatforming Norm-Violating Influencers on Social Media Reduces Overall Online Attention Toward Them","abstract":"From politicians to podcast hosts, online platforms have systematically banned (``deplatformed'') influential users for breaking platform guidelines. Previous inquiries on the effectiveness of this intervention are inconclusive because 1) they consider only few deplatforming events; 2) they consider only overt engagement traces (e.g., likes and posts) but not passive engagement (e.g., views); 3) they do not consider all the potential places users impacted by the deplatforming event might migrate to. We address these limitations in a longitudinal, quasi-experimental study of 165 deplatforming events targeted at 101 influencers. We collect deplatforming events from Reddit posts and then manually curate the data, ensuring the correctness of a large dataset of deplatforming events. Then, we link these events to Google Trends and Wikipedia page views, platform-agnostic measures of online attention that capture the general public's interest in specific influencers. Through a difference-in-differences approach, we find that deplatforming reduces online attention toward influencers. After 12 months, we estimate that online attention toward deplatformed influencers is reduced by -63% (95% CI [-75%,-46%]) on Google and by -43% (95% CI [-57%,-24%]) on Wikipedia. Further, as we study over a hundred deplatforming events, we can analyze in which cases deplatforming is more or less impactful, revealing nuances about the intervention. Notably, we find that both permanent and temporary deplatforming reduce online attention toward influencers; Overall, this work contributes to the ongoing effort to map the effectiveness of content moderation interventions, driving platform governance away from speculation.","sentences":["From politicians to podcast hosts, online platforms have systematically banned (``deplatformed'') influential users for breaking platform guidelines.","Previous inquiries on the effectiveness of this intervention are inconclusive because 1) they consider only few deplatforming events; 2) they consider only overt engagement traces (e.g., likes and posts) but not passive engagement (e.g., views); 3) they do not consider all the potential places users impacted by the deplatforming event might migrate to.","We address these limitations in a longitudinal, quasi-experimental study of 165 deplatforming events targeted at 101 influencers.","We collect deplatforming events from Reddit posts and then manually curate the data, ensuring the correctness of a large dataset of deplatforming events.","Then, we link these events to Google Trends and Wikipedia page views, platform-agnostic measures of online attention that capture the general public's interest in specific influencers.","Through a difference-in-differences approach, we find that deplatforming reduces online attention toward influencers.","After 12 months, we estimate that online attention toward deplatformed influencers is reduced by -63% (95%","CI","[-75%,-46%]) on Google and by -43% (95% CI","[-57%,-24%]) on Wikipedia.","Further, as we study over a hundred deplatforming events, we can analyze in which cases deplatforming is more or less impactful, revealing nuances about the intervention.","Notably, we find that both permanent and temporary deplatforming reduce online attention toward influencers; Overall, this work contributes to the ongoing effort to map the effectiveness of content moderation interventions, driving platform governance away from speculation."],"url":"http://arxiv.org/abs/2401.01253v1"}
{"created":"2024-01-02 15:18:23","title":"Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning","abstract":"Broadband infrastructure owners do not always know how their customers are connected in the local networks, which are structured as rooted trees. A recent study is able to infer the topology of a local network using discrete time series data from the leaves of the tree (customers). In this study we propose a contrastive approach for learning a binary event encoder from continuous time series data. As a preliminary result, we show that our approach has some potential in learning a valuable encoder.","sentences":["Broadband infrastructure owners do not always know how their customers are connected in the local networks, which are structured as rooted trees.","A recent study is able to infer the topology of a local network using discrete time series data from the leaves of the tree (customers).","In this study we propose a contrastive approach for learning a binary event encoder from continuous time series data.","As a preliminary result, we show that our approach has some potential in learning a valuable encoder."],"url":"http://arxiv.org/abs/2401.01242v1"}
{"created":"2024-01-02 14:58:26","title":"Motif-aware Riemannian Graph Neural Network with Generative-Contrastive Learning","abstract":"Graphs are typical non-Euclidean data of complex structures. In recent years, Riemannian graph representation learning has emerged as an exciting alternative to Euclidean ones. However, Riemannian methods are still in an early stage: most of them present a single curvature (radius) regardless of structural complexity, suffer from numerical instability due to the exponential/logarithmic map, and lack the ability to capture motif regularity. In light of the issues above, we propose the problem of \\emph{Motif-aware Riemannian Graph Representation Learning}, seeking a numerically stable encoder to capture motif regularity in a diverse-curvature manifold without labels. To this end, we present a novel Motif-aware Riemannian model with Generative-Contrastive learning (MotifRGC), which conducts a minmax game in Riemannian manifold in a self-supervised manner. First, we propose a new type of Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold by a product layer with the diversified factor, and replace the exponential/logarithmic map by a stable kernel layer. Second, we introduce a motif-aware Riemannian generative-contrastive learning to capture motif regularity in the constructed manifold and learn motif-aware node representation without external labels. Empirical results show the superiority of MofitRGC.","sentences":["Graphs are typical non-Euclidean data of complex structures.","In recent years, Riemannian graph representation learning has emerged as an exciting alternative to Euclidean ones.","However, Riemannian methods are still in an early stage: most of them present a single curvature (radius) regardless of structural complexity, suffer from numerical instability due to the exponential/logarithmic map, and lack the ability to capture motif regularity.","In light of the issues above, we propose the problem of \\emph{Motif-aware Riemannian Graph Representation Learning}, seeking a numerically stable encoder to capture motif regularity in a diverse-curvature manifold without labels.","To this end, we present a novel Motif-aware Riemannian model with Generative-Contrastive learning (MotifRGC), which conducts a minmax game in Riemannian manifold in a self-supervised manner.","First, we propose a new type of Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold by a product layer with the diversified factor, and replace the exponential/logarithmic map by a stable kernel layer.","Second, we introduce a motif-aware Riemannian generative-contrastive learning to capture motif regularity in the constructed manifold and learn motif-aware node representation without external labels.","Empirical results show the superiority of MofitRGC."],"url":"http://arxiv.org/abs/2401.01232v1"}
{"created":"2024-01-02 14:36:28","title":"IdentiFace : A VGG Based Multimodal Facial Biometric System","abstract":"The development of facial biometric systems has contributed greatly to the development of the computer vision field. Nowadays, there's always a need to develop a multimodal system that combines multiple biometric traits in an efficient, meaningful way. In this paper, we introduce \"IdentiFace\" which is a multimodal facial biometric system that combines the core of facial recognition with some of the most important soft biometric traits such as gender, face shape, and emotion. We also focused on developing the system using only VGG-16 inspired architecture with minor changes across different subsystems. This unification allows for simpler integration across modalities. It makes it easier to interpret the learned features between the tasks which gives a good indication about the decision-making process across the facial modalities and potential connection. For the recognition problem, we acquired a 99.2% test accuracy for five classes with high intra-class variations using data collected from the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the public dataset[2] in the gender recognition problem. We were also able to achieve a testing accuracy of 88.03% in the face-shape problem using the celebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy of 66.13% in the emotion task which is considered a very acceptable accuracy compared to related work on the FER2013 dataset[4].","sentences":["The development of facial biometric systems has contributed greatly to the development of the computer vision field.","Nowadays, there's always a need to develop a multimodal system that combines multiple biometric traits in an efficient, meaningful way.","In this paper, we introduce \"IdentiFace\" which is a multimodal facial biometric system that combines the core of facial recognition with some of the most important soft biometric traits such as gender, face shape, and emotion.","We also focused on developing the system using only VGG-16 inspired architecture with minor changes across different subsystems.","This unification allows for simpler integration across modalities.","It makes it easier to interpret the learned features between the tasks which gives a good indication about the decision-making process across the facial modalities and potential connection.","For the recognition problem, we acquired a 99.2% test accuracy for five classes with high intra-class variations using data collected from the FERET database[1].","We achieved 99.4% on our dataset and 95.15% on the public dataset[2] in the gender recognition problem.","We were also able to achieve a testing accuracy of 88.03% in the face-shape problem using the celebrity face-shape dataset[3].","Finally, we achieved a decent testing accuracy of 66.13% in the emotion task which is considered a very acceptable accuracy compared to related work on the FER2013 dataset[4]."],"url":"http://arxiv.org/abs/2401.01227v1"}
{"created":"2024-01-02 14:18:11","title":"Distribution Matching for Multi-Task Learning of Classification Tasks: a Large-Scale Study on Faces & Beyond","abstract":"Multi-Task Learning (MTL) is a framework, where multiple related tasks are learned jointly and benefit from a shared representation space, or parameter transfer. To provide sufficient learning support, modern MTL uses annotated data with full, or sufficiently large overlap across tasks, i.e., each input sample is annotated for all, or most of the tasks. However, collecting such annotations is prohibitive in many real applications, and cannot benefit from datasets available for individual tasks. In this work, we challenge this setup and show that MTL can be successful with classification tasks with little, or non-overlapping annotations, or when there is big discrepancy in the size of labeled data per task. We explore task-relatedness for co-annotation and co-training, and propose a novel approach, where knowledge exchange is enabled between the tasks via distribution matching. To demonstrate the general applicability of our method, we conducted diverse case studies in the domains of affective computing, face recognition, species recognition, and shopping item classification using nine datasets. Our large-scale study of affective tasks for basic expression recognition and facial action unit detection illustrates that our approach is network agnostic and brings large performance improvements compared to the state-of-the-art in both tasks and across all studied databases. In all case studies, we show that co-training via task-relatedness is advantageous and prevents negative transfer (which occurs when MT model's performance is worse than that of at least one single-task model).","sentences":["Multi-Task Learning (MTL) is a framework, where multiple related tasks are learned jointly and benefit from a shared representation space, or parameter transfer.","To provide sufficient learning support, modern MTL uses annotated data with full, or sufficiently large overlap across tasks, i.e., each input sample is annotated for all, or most of the tasks.","However, collecting such annotations is prohibitive in many real applications, and cannot benefit from datasets available for individual tasks.","In this work, we challenge this setup and show that MTL can be successful with classification tasks with little, or non-overlapping annotations, or when there is big discrepancy in the size of labeled data per task.","We explore task-relatedness for co-annotation and co-training, and propose a novel approach, where knowledge exchange is enabled between the tasks via distribution matching.","To demonstrate the general applicability of our method, we conducted diverse case studies in the domains of affective computing, face recognition, species recognition, and shopping item classification using nine datasets.","Our large-scale study of affective tasks for basic expression recognition and facial action unit detection illustrates that our approach is network agnostic and brings large performance improvements compared to the state-of-the-art in both tasks and across all studied databases.","In all case studies, we show that co-training via task-relatedness is advantageous and prevents negative transfer (which occurs when MT model's performance is worse than that of at least one single-task model)."],"url":"http://arxiv.org/abs/2401.01219v2"}
{"created":"2024-01-02 14:04:42","title":"YOLO algorithm with hybrid attention feature pyramid network for solder joint defect detection","abstract":"Traditional manual detection for solder joint defect is no longer applied during industrial production due to low efficiency, inconsistent evaluation, high cost and lack of real-time data. A new approach has been proposed to address the issues of low accuracy, high false detection rates and computational cost of solder joint defect detection in surface mount technology of industrial scenarios. The proposed solution is a hybrid attention mechanism designed specifically for the solder joint defect detection algorithm to improve quality control in the manufacturing process by increasing the accuracy while reducing the computational cost. The hybrid attention mechanism comprises a proposed enhanced multi-head self-attention and coordinate attention mechanisms increase the ability of attention networks to perceive contextual information and enhances the utilization range of network features. The coordinate attention mechanism enhances the connection between different channels and reduces location information loss. The hybrid attention mechanism enhances the capability of the network to perceive long-distance position information and learn local features. The improved algorithm model has good detection ability for solder joint defect detection, with mAP reaching 91.5%, 4.3% higher than the You Only Look Once version 5 algorithm and better than other comparative algorithms. Compared to other versions, mean Average Precision, Precision, Recall, and Frame per Seconds indicators have also improved. The improvement of detection accuracy can be achieved while meeting real-time detection requirements.","sentences":["Traditional manual detection for solder joint defect is no longer applied during industrial production due to low efficiency, inconsistent evaluation, high cost and lack of real-time data.","A new approach has been proposed to address the issues of low accuracy, high false detection rates and computational cost of solder joint defect detection in surface mount technology of industrial scenarios.","The proposed solution is a hybrid attention mechanism designed specifically for the solder joint defect detection algorithm to improve quality control in the manufacturing process by increasing the accuracy while reducing the computational cost.","The hybrid attention mechanism comprises a proposed enhanced multi-head self-attention and coordinate attention mechanisms increase the ability of attention networks to perceive contextual information and enhances the utilization range of network features.","The coordinate attention mechanism enhances the connection between different channels and reduces location information loss.","The hybrid attention mechanism enhances the capability of the network to perceive long-distance position information and learn local features.","The improved algorithm model has good detection ability for solder joint defect detection, with mAP reaching 91.5%, 4.3% higher than the You Only Look Once version 5 algorithm and better than other comparative algorithms.","Compared to other versions, mean Average Precision, Precision, Recall, and Frame per Seconds indicators have also improved.","The improvement of detection accuracy can be achieved while meeting real-time detection requirements."],"url":"http://arxiv.org/abs/2401.01214v1"}
{"created":"2024-01-02 13:13:28","title":"PPBFL: A Privacy Protected Blockchain-based Federated Learning Model","abstract":"With the rapid development of machine learning and growing concerns about data privacy, federated learning has become an increasingly prominent focus. However, challenges such as attacks on model parameters and the lack of incentive mechanisms hinder the effectiveness of federated learning. Therefore, we propose a Privacy Protected Blockchain-based Federated Learning Model (PPBFL) to enhance the security of federated learning and promote the active participation of nodes in model training. Blockchain ensures that model parameters stored in the InterPlanetary File System (IPFS) remain unaltered. A novel adaptive differential privacy addition algorithm is simultaneously applied to local and global models, preserving the privacy of local models and preventing a decrease in the security of the global model due to the presence of numerous local models in federated learning. Additionally, we introduce a new mix transactions mechanism to better protect the identity privacy of local training clients. Security analysis and experimental results demonstrate that PPBFL outperforms baseline methods in both model performance and security.","sentences":["With the rapid development of machine learning and growing concerns about data privacy, federated learning has become an increasingly prominent focus.","However, challenges such as attacks on model parameters and the lack of incentive mechanisms hinder the effectiveness of federated learning.","Therefore, we propose a Privacy Protected Blockchain-based Federated Learning Model (PPBFL) to enhance the security of federated learning and promote the active participation of nodes in model training.","Blockchain ensures that model parameters stored in the InterPlanetary File System (IPFS) remain unaltered.","A novel adaptive differential privacy addition algorithm is simultaneously applied to local and global models, preserving the privacy of local models and preventing a decrease in the security of the global model due to the presence of numerous local models in federated learning.","Additionally, we introduce a new mix transactions mechanism to better protect the identity privacy of local training clients.","Security analysis and experimental results demonstrate that PPBFL outperforms baseline methods in both model performance and security."],"url":"http://arxiv.org/abs/2401.01204v1"}
{"created":"2024-01-02 13:03:39","title":"Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms","abstract":"Skin lesions are classified in benign or malignant. Among the malignant, melanoma is a very aggressive cancer and the major cause of deaths. So, early diagnosis of skin cancer is very desired. In the last few years, there is a growing interest in computer aided diagnostic (CAD) using most image and clinical data of the lesion. These sources of information present limitations due to their inability to provide information of the molecular structure of the lesion. NIR spectroscopy may provide an alternative source of information to automated CAD of skin lesions. The most commonly used techniques and classification algorithms used in spectroscopy are Principal Component Analysis (PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support Vector Machines (SVM). Nonetheless, there is a growing interest in applying the modern techniques of machine and deep learning (MDL) to spectroscopy. One of the main limitations to apply MDL to spectroscopy is the lack of public datasets. Since there is no public dataset of NIR spectral data to skin lesions, as far as we know, an effort has been made and a new dataset named NIR-SC-UFES, has been collected, annotated and analyzed generating the gold-standard for classification of NIR spectral data to skin cancer. Next, the machine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional neural network (1D-CNN) were investigated to classify cancer and non-cancer skin lesions. Experimental results indicate the best performance obtained by LightGBM with pre-processing using standard normal variate (SNV), feature extraction providing values of 0.839 for balanced accuracy, 0.851 for recall, 0.852 for precision, and 0.850 for F-score. The obtained results indicate the first steps in CAD of skin lesions aiming the automated triage of patients with skin lesions in vivo using NIR spectral data.","sentences":["Skin lesions are classified in benign or malignant.","Among the malignant, melanoma is a very aggressive cancer and the major cause of deaths.","So, early diagnosis of skin cancer is very desired.","In the last few years, there is a growing interest in computer aided diagnostic (CAD) using most image and clinical data of the lesion.","These sources of information present limitations due to their inability to provide information of the molecular structure of the lesion.","NIR spectroscopy may provide an alternative source of information to automated CAD of skin lesions.","The most commonly used techniques and classification algorithms used in spectroscopy are Principal Component Analysis (PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support Vector Machines (SVM).","Nonetheless, there is a growing interest in applying the modern techniques of machine and deep learning (MDL) to spectroscopy.","One of the main limitations to apply MDL to spectroscopy is the lack of public datasets.","Since there is no public dataset of NIR spectral data to skin lesions, as far as we know, an effort has been made and a new dataset named NIR-SC-UFES, has been collected, annotated and analyzed generating the gold-standard for classification of NIR spectral data to skin cancer.","Next, the machine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional neural network (1D-CNN) were investigated to classify cancer and non-cancer skin lesions.","Experimental results indicate the best performance obtained by LightGBM with pre-processing using standard normal variate (SNV), feature extraction providing values of 0.839 for balanced accuracy, 0.851 for recall, 0.852 for precision, and 0.850 for F-score.","The obtained results indicate the first steps in CAD of skin lesions aiming the automated triage of patients with skin lesions in vivo using NIR spectral data."],"url":"http://arxiv.org/abs/2401.01200v1"}
{"created":"2024-01-02 12:41:17","title":"Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised Pretrained Transformers for Single- and Multi-Objective Continuous Optimization Problems","abstract":"In many recent works, the potential of Exploratory Landscape Analysis (ELA) features to numerically characterize, in particular, single-objective continuous optimization problems has been demonstrated. These numerical features provide the input for all kinds of machine learning tasks on continuous optimization problems, ranging, i.a., from High-level Property Prediction to Automated Algorithm Selection and Automated Algorithm Configuration. Without ELA features, analyzing and understanding the characteristics of single-objective continuous optimization problems would be impossible.   Yet, despite their undisputed usefulness, ELA features suffer from several drawbacks. These include, in particular, (1.) a strong correlation between multiple features, as well as (2.) its very limited applicability to multi-objective continuous optimization problems. As a remedy, recent works proposed deep learning-based approaches as alternatives to ELA. In these works, e.g., point-cloud transformers were used to characterize an optimization problem's fitness landscape. However, these approaches require a large amount of labeled training data.   Within this work, we propose a hybrid approach, Deep-ELA, which combines (the benefits of) deep learning and ELA features. Specifically, we pre-trained four transformers on millions of randomly generated optimization problems to learn deep representations of the landscapes of continuous single- and multi-objective optimization problems. Our proposed framework can either be used out-of-the-box for analyzing single- and multi-objective continuous optimization problems, or subsequently fine-tuned to various tasks focussing on algorithm behavior and problem understanding.","sentences":["In many recent works, the potential of Exploratory Landscape Analysis (ELA) features to numerically characterize, in particular, single-objective continuous optimization problems has been demonstrated.","These numerical features provide the input for all kinds of machine learning tasks on continuous optimization problems, ranging, i.a., from High-level Property Prediction to Automated Algorithm Selection and Automated Algorithm Configuration.","Without ELA features, analyzing and understanding the characteristics of single-objective continuous optimization problems would be impossible.   ","Yet, despite their undisputed usefulness, ELA features suffer from several drawbacks.","These include, in particular, (1.)","a strong correlation between multiple features, as well as (2.)","its very limited applicability to multi-objective continuous optimization problems.","As a remedy, recent works proposed deep learning-based approaches as alternatives to ELA.","In these works, e.g., point-cloud transformers were used to characterize an optimization problem's fitness landscape.","However, these approaches require a large amount of labeled training data.   ","Within this work, we propose a hybrid approach, Deep-ELA, which combines (the benefits of) deep learning and ELA features.","Specifically, we pre-trained four transformers on millions of randomly generated optimization problems to learn deep representations of the landscapes of continuous single- and multi-objective optimization problems.","Our proposed framework can either be used out-of-the-box for analyzing single- and multi-objective continuous optimization problems, or subsequently fine-tuned to various tasks focussing on algorithm behavior and problem understanding."],"url":"http://arxiv.org/abs/2401.01192v1"}
{"created":"2024-01-02 12:23:49","title":"Unifying Structured Data as Graph for Data-to-Text Pre-Training","abstract":"Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performances. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different data-to-text generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account. Extensive experiments on six benchmark datasets show the effectiveness of our model. Our source codes are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.","sentences":["Data-to-text (D2T) generation aims to transform structured data into natural language text.","Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performances.","However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph).","In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different data-to-text generation tasks as graph-to-text generation.","To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer.","Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph.","In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account.","Extensive experiments on six benchmark datasets show the effectiveness of our model.","Our source codes are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t."],"url":"http://arxiv.org/abs/2401.01183v1"}
{"created":"2024-01-02 12:14:41","title":"Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training","abstract":"Modern healthcare often utilises radiographic images alongside textual reports for diagnostics, encouraging the use of Vision-Language Self-Supervised Learning (VL-SSL) with large pre-trained models to learn versatile medical vision representations. However, most existing VL-SSL frameworks are trained end-to-end, which is computation-heavy and can lose vital prior information embedded in pre-trained encoders. To address both issues, we introduce the backbone-agnostic Adaptor framework, which preserves medical knowledge in pre-trained image and text encoders by keeping them frozen, and employs a lightweight Adaptor module for cross-modal learning. Experiments on medical image classification and segmentation tasks across three datasets reveal that our framework delivers competitive performance while cutting trainable parameters by over 90% compared to current pre-training approaches. Notably, when fine-tuned with just 1% of data, Adaptor outperforms several Transformer-based methods trained on full datasets in medical image segmentation.","sentences":["Modern healthcare often utilises radiographic images alongside textual reports for diagnostics, encouraging the use of Vision-Language Self-Supervised Learning (VL-SSL) with large pre-trained models to learn versatile medical vision representations.","However, most existing VL-SSL frameworks are trained end-to-end, which is computation-heavy and can lose vital prior information embedded in pre-trained encoders.","To address both issues, we introduce the backbone-agnostic Adaptor framework, which preserves medical knowledge in pre-trained image and text encoders by keeping them frozen, and employs a lightweight Adaptor module for cross-modal learning.","Experiments on medical image classification and segmentation tasks across three datasets reveal that our framework delivers competitive performance while cutting trainable parameters by over 90% compared to current pre-training approaches.","Notably, when fine-tuned with just 1% of data, Adaptor outperforms several Transformer-based methods trained on full datasets in medical image segmentation."],"url":"http://arxiv.org/abs/2401.01179v1"}
{"created":"2024-01-02 12:06:31","title":"En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data","abstract":"We present En3D, an enhanced generative scheme for sculpting high-quality 3D human avatars. Unlike previous works that rely on scarce 3D datasets or limited 2D collections with imbalanced viewing angles and imprecise pose priors, our approach aims to develop a zero-shot 3D generative scheme capable of producing visually realistic, geometrically accurate and content-wise diverse 3D humans without relying on pre-existing 3D or 2D assets. To address this challenge, we introduce a meticulously crafted workflow that implements accurate physical modeling to learn the enhanced 3D generative model from synthetic 2D data. During inference, we integrate optimization modules to bridge the gap between realistic appearances and coarse 3D shapes. Specifically, En3D comprises three modules: a 3D generator that accurately models generalizable 3D humans with realistic appearance from synthesized balanced, diverse, and structured human images; a geometry sculptor that enhances shape quality using multi-view normal constraints for intricate human anatomy; and a texturing module that disentangles explicit texture maps with fidelity and editability, leveraging semantical UV partitioning and a differentiable rasterizer. Experimental results show that our approach significantly outperforms prior works in terms of image quality, geometry accuracy and content diversity. We also showcase the applicability of our generated avatars for animation and editing, as well as the scalability of our approach for content-style free adaptation.","sentences":["We present En3D, an enhanced generative scheme for sculpting high-quality 3D human avatars.","Unlike previous works that rely on scarce 3D datasets or limited 2D collections with imbalanced viewing angles and imprecise pose priors, our approach aims to develop a zero-shot 3D generative scheme capable of producing visually realistic, geometrically accurate and content-wise diverse 3D humans without relying on pre-existing 3D or 2D assets.","To address this challenge, we introduce a meticulously crafted workflow that implements accurate physical modeling to learn the enhanced 3D generative model from synthetic 2D data.","During inference, we integrate optimization modules to bridge the gap between realistic appearances and coarse 3D shapes.","Specifically, En3D comprises three modules: a 3D generator that accurately models generalizable 3D humans with realistic appearance from synthesized balanced, diverse, and structured human images; a geometry sculptor that enhances shape quality using multi-view normal constraints for intricate human anatomy; and a texturing module that disentangles explicit texture maps with fidelity and editability, leveraging semantical UV partitioning and a differentiable rasterizer.","Experimental results show that our approach significantly outperforms prior works in terms of image quality, geometry accuracy and content diversity.","We also showcase the applicability of our generated avatars for animation and editing, as well as the scalability of our approach for content-style free adaptation."],"url":"http://arxiv.org/abs/2401.01173v1"}
{"created":"2024-01-02 11:47:58","title":"Reinforcement Learning for SAR View Angle Inversion with Differentiable SAR Renderer","abstract":"The electromagnetic inverse problem has long been a research hotspot. This study aims to reverse radar view angles in synthetic aperture radar (SAR) images given a target model. Nonetheless, the scarcity of SAR data, combined with the intricate background interference and imaging mechanisms, limit the applications of existing learning-based approaches. To address these challenges, we propose an interactive deep reinforcement learning (DRL) framework, where an electromagnetic simulator named differentiable SAR render (DSR) is embedded to facilitate the interaction between the agent and the environment, simulating a human-like process of angle prediction. Specifically, DSR generates SAR images at arbitrary view angles in real-time. And the differences in sequential and semantic aspects between the view angle-corresponding images are leveraged to construct the state space in DRL, which effectively suppress the complex background interference, enhance the sensitivity to temporal variations, and improve the capability to capture fine-grained information. Additionally, in order to maintain the stability and convergence of our method, a series of reward mechanisms, such as memory difference, smoothing and boundary penalty, are utilized to form the final reward function. Extensive experiments performed on both simulated and real datasets demonstrate the effectiveness and robustness of our proposed method. When utilized in the cross-domain area, the proposed method greatly mitigates inconsistency between simulated and real domains, outperforming reference methods significantly.","sentences":["The electromagnetic inverse problem has long been a research hotspot.","This study aims to reverse radar view angles in synthetic aperture radar (SAR) images given a target model.","Nonetheless, the scarcity of SAR data, combined with the intricate background interference and imaging mechanisms, limit the applications of existing learning-based approaches.","To address these challenges, we propose an interactive deep reinforcement learning (DRL) framework, where an electromagnetic simulator named differentiable SAR render (DSR) is embedded to facilitate the interaction between the agent and the environment, simulating a human-like process of angle prediction.","Specifically, DSR generates SAR images at arbitrary view angles in real-time.","And the differences in sequential and semantic aspects between the view angle-corresponding images are leveraged to construct the state space in DRL, which effectively suppress the complex background interference, enhance the sensitivity to temporal variations, and improve the capability to capture fine-grained information.","Additionally, in order to maintain the stability and convergence of our method, a series of reward mechanisms, such as memory difference, smoothing and boundary penalty, are utilized to form the final reward function.","Extensive experiments performed on both simulated and real datasets demonstrate the effectiveness and robustness of our proposed method.","When utilized in the cross-domain area, the proposed method greatly mitigates inconsistency between simulated and real domains, outperforming reference methods significantly."],"url":"http://arxiv.org/abs/2401.01165v1"}
{"created":"2024-01-02 11:46:44","title":"Distilling Local Texture Features for Colorectal Tissue Classification in Low Data Regimes","abstract":"Multi-class colorectal tissue classification is a challenging problem that is typically addressed in a setting, where it is assumed that ample amounts of training data is available. However, manual annotation of fine-grained colorectal tissue samples of multiple classes, especially the rare ones like stromal tumor and anal cancer is laborious and expensive. To address this, we propose a knowledge distillation-based approach, named KD-CTCNet, that effectively captures local texture information from few tissue samples, through a distillation loss, to improve the standard CNN features. The resulting enriched feature representation achieves improved classification performance specifically in low data regimes. Extensive experiments on two public datasets of colorectal tissues reveal the merits of the proposed contributions, with a consistent gain achieved over different approaches across low data settings. The code and models are publicly available on GitHub.","sentences":["Multi-class colorectal tissue classification is a challenging problem that is typically addressed in a setting, where it is assumed that ample amounts of training data is available.","However, manual annotation of fine-grained colorectal tissue samples of multiple classes, especially the rare ones like stromal tumor and anal cancer is laborious and expensive.","To address this, we propose a knowledge distillation-based approach, named KD-CTCNet, that effectively captures local texture information from few tissue samples, through a distillation loss, to improve the standard CNN features.","The resulting enriched feature representation achieves improved classification performance specifically in low data regimes.","Extensive experiments on two public datasets of colorectal tissues reveal the merits of the proposed contributions, with a consistent gain achieved over different approaches across low data settings.","The code and models are publicly available on GitHub."],"url":"http://arxiv.org/abs/2401.01164v1"}
{"created":"2024-01-02 11:13:01","title":"Deep Learning-Based Detection for Marker Codes over Insertion and Deletion Channels","abstract":"Marker code is an effective coding scheme to protect data from insertions and deletions. It has potential applications in future storage systems, such as DNA storage and racetrack memory. When decoding marker codes, perfect channel state information (CSI), i.e., insertion and deletion probabilities, are required to detect insertion and deletion errors. Sometimes, the perfect CSI is not easy to obtain or the accurate channel model is unknown. Therefore, it is deserved to develop detecting algorithms for marker code without the knowledge of perfect CSI. In this paper, we propose two CSI-agnostic detecting algorithms for marker code based on deep learning. The first one is a model-driven deep learning method, which deep unfolds the original iterative detecting algorithm of marker code. In this method, CSI become weights in neural networks and these weights can be learned from training data. The second one is a data-driven method which is an end-to-end system based on the deep bidirectional gated recurrent unit network. Simulation results show that error performances of the proposed methods are significantly better than that of the original detection algorithm with CSI uncertainty. Furthermore, the proposed data-driven method exhibits better error performances than other methods for unknown channel models.","sentences":["Marker code is an effective coding scheme to protect data from insertions and deletions.","It has potential applications in future storage systems, such as DNA storage and racetrack memory.","When decoding marker codes, perfect channel state information (CSI), i.e., insertion and deletion probabilities, are required to detect insertion and deletion errors.","Sometimes, the perfect CSI is not easy to obtain or the accurate channel model is unknown.","Therefore, it is deserved to develop detecting algorithms for marker code without the knowledge of perfect CSI.","In this paper, we propose two CSI-agnostic detecting algorithms for marker code based on deep learning.","The first one is a model-driven deep learning method, which deep unfolds the original iterative detecting algorithm of marker code.","In this method, CSI become weights in neural networks and these weights can be learned from training data.","The second one is a data-driven method which is an end-to-end system based on the deep bidirectional gated recurrent unit network.","Simulation results show that error performances of the proposed methods are significantly better than that of the original detection algorithm with CSI uncertainty.","Furthermore, the proposed data-driven method exhibits better error performances than other methods for unknown channel models."],"url":"http://arxiv.org/abs/2401.01155v1"}
{"created":"2024-01-02 11:08:39","title":"Applying Bayesian Data Analysis for Causal Inference about Requirements Quality: A Replicated Experiment","abstract":"Context: It is commonly accepted that the quality of requirements specifications impacts subsequent software engineering activities. However, we still lack empirical evidence to support organizations in deciding whether their requirements are good enough or impede subsequent activities. Objective: We aim to contribute empirical evidence to the effect that requirements quality defects have on a software engineering activity that depends on this requirement. Method: We replicate a controlled experiment in which 25 participants from industry and university generate domain models from four natural language requirements containing different quality defects. We evaluate the resulting models using both frequentist and Bayesian data analysis. Results: Contrary to our expectations, our results show that the use of passive voice only has a minor impact on the resulting domain models. The use of ambiguous pronouns, however, shows a strong effect on various properties of the resulting domain models. Most notably, ambiguous pronouns lead to incorrect associations in domain models. Conclusion: Despite being equally advised against by literature and frequentist methods, the Bayesian data analysis shows that the two investigated quality defects have vastly different impacts on software engineering activities and, hence, deserve different levels of attention. Our employed method can be further utilized by researchers to improve reliable, detailed empirical evidence on requirements quality.","sentences":["Context: It is commonly accepted that the quality of requirements specifications impacts subsequent software engineering activities.","However, we still lack empirical evidence to support organizations in deciding whether their requirements are good enough or impede subsequent activities.","Objective: We aim to contribute empirical evidence to the effect that requirements quality defects have on a software engineering activity that depends on this requirement.","Method: We replicate a controlled experiment in which 25 participants from industry and university generate domain models from four natural language requirements containing different quality defects.","We evaluate the resulting models using both frequentist and Bayesian data analysis.","Results: Contrary to our expectations, our results show that the use of passive voice only has a minor impact on the resulting domain models.","The use of ambiguous pronouns, however, shows a strong effect on various properties of the resulting domain models.","Most notably, ambiguous pronouns lead to incorrect associations in domain models.","Conclusion: Despite being equally advised against by literature and frequentist methods, the Bayesian data analysis shows that the two investigated quality defects have vastly different impacts on software engineering activities and, hence, deserve different levels of attention.","Our employed method can be further utilized by researchers to improve reliable, detailed empirical evidence on requirements quality."],"url":"http://arxiv.org/abs/2401.01154v1"}
{"created":"2024-01-02 11:04:51","title":"The social graph based on real data","abstract":"In this paper, we propose a model enabling the creation of a social graph corresponding to real society. The procedure uses data describing the real social relations in the community, like marital status or number of kids. Results show the power-law behavior of the distribution of links and, typical for small worlds, the independence of the clustering coefficient on the size of the graph.","sentences":["In this paper, we propose a model enabling the creation of a social graph corresponding to real society.","The procedure uses data describing the real social relations in the community, like marital status or number of kids.","Results show the power-law behavior of the distribution of links and, typical for small worlds, the independence of the clustering coefficient on the size of the graph."],"url":"http://arxiv.org/abs/2401.01152v1"}
{"created":"2024-01-02 11:01:25","title":"CXL and the Return of Scale-Up Database Engines","abstract":"The growing trend towards specialization has led to a proliferation of accelerators and alternative processing devices. When embedded in conventional computer architectures, the PCIe link connecting the CPU to these devices becomes a bottleneck. Several proposals for alternative designs have been put forward, with these efforts having now converged into the Compute Express Link (CXL) specification. CXL is an interconnect protocol on top of PCIe with a more modern and powerful interface. While still on version 1.0 in terms of commercial availability, the potential of CXL to radically change the underlying architecture has already attracted considerable attention. This attention has been focused mainly on the possibility of using CXL to build a shared memory system among the machines in a rack. We argue, however, that such benefits are just the beginning of more significant changes that will have a major impact on database engines and data processing systems. In a nutshell, while the cloud favored scale-out approaches, CXL brings back scale-up architectures. In the paper we describe how CXL enables such architectures, and the research challenges associated with the emerging scale-up, heterogeneous hardware platforms.","sentences":["The growing trend towards specialization has led to a proliferation of accelerators and alternative processing devices.","When embedded in conventional computer architectures, the PCIe link connecting the CPU to these devices becomes a bottleneck.","Several proposals for alternative designs have been put forward, with these efforts having now converged into the Compute Express Link (CXL) specification.","CXL is an interconnect protocol on top of PCIe with a more modern and powerful interface.","While still on version 1.0 in terms of commercial availability, the potential of CXL to radically change the underlying architecture has already attracted considerable attention.","This attention has been focused mainly on the possibility of using CXL to build a shared memory system among the machines in a rack.","We argue, however, that such benefits are just the beginning of more significant changes that will have a major impact on database engines and data processing systems.","In a nutshell, while the cloud favored scale-out approaches, CXL brings back scale-up architectures.","In the paper we describe how CXL enables such architectures, and the research challenges associated with the emerging scale-up, heterogeneous hardware platforms."],"url":"http://arxiv.org/abs/2401.01150v1"}
{"created":"2024-01-02 10:56:24","title":"Privacy Preserving Personal Assistant with On-Device Diarization and Spoken Dialogue System for Home and Beyond","abstract":"In the age of personal voice assistants, the question of privacy arises. These digital companions often lack memory of past interactions, while relying heavily on the internet for speech processing, raising privacy concerns. Modern smartphones now enable on-device speech processing, making cloud-based solutions unnecessary. Personal assistants for the elderly should excel at memory recall, especially in medical examinations. The e-ViTA project developed a versatile conversational application with local processing and speaker recognition. This paper highlights the importance of speaker diarization enriched with sensor data fusion for contextualized conversation preservation. The use cases applied to the e-VITA project have shown that truly personalized dialogue is pivotal for individual voice assistants. Secure local processing and sensor data fusion ensure virtual companions meet individual user needs without compromising privacy or data security.","sentences":["In the age of personal voice assistants, the question of privacy arises.","These digital companions often lack memory of past interactions, while relying heavily on the internet for speech processing, raising privacy concerns.","Modern smartphones now enable on-device speech processing, making cloud-based solutions unnecessary.","Personal assistants for the elderly should excel at memory recall, especially in medical examinations.","The e-ViTA project developed a versatile conversational application with local processing and speaker recognition.","This paper highlights the importance of speaker diarization enriched with sensor data fusion for contextualized conversation preservation.","The use cases applied to the e-VITA project have shown that truly personalized dialogue is pivotal for individual voice assistants.","Secure local processing and sensor data fusion ensure virtual companions meet individual user needs without compromising privacy or data security."],"url":"http://arxiv.org/abs/2401.01146v1"}
{"created":"2024-01-02 10:42:42","title":"Spiker+: a framework for the generation of efficient Spiking Neural Networks FPGA accelerators for inference at the edge","abstract":"Including Artificial Neural Networks in embedded systems at the edge allows applications to exploit Artificial Intelligence capabilities directly within devices operating at the network periphery. This paper introduces Spiker+, a comprehensive framework for generating efficient, low-power, and low-area customized Spiking Neural Networks (SNN) accelerators on FPGA for inference at the edge. Spiker+ presents a configurable multi-layer hardware SNN, a library of highly efficient neuron architectures, and a design framework, enabling the development of complex neural network accelerators with few lines of Python code. Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking Heidelberg Digits (SHD). On the MNIST, it demonstrates competitive performance compared to state-of-the-art SNN accelerators. It outperforms them in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMs (BRAMs), which makes it fit in very small FPGA, and power consumption, draining only 180mW for a complete inference on an input image. The latency is comparable to the ones observed in the state-of-the-art, with 780us/img. To the authors' knowledge, Spiker+ is the first SNN accelerator tested on the SHD. In this case, the accelerator requires 18,268 logic cells and 51 BRAM, with an overall power consumption of 430mW and a latency of 54 us for a complete inference on input data. This underscores the significance of Spiker+ in the hardware-accelerated SNN landscape, making it an excellent solution to deploy configurable and tunable SNN architectures in resource and power-constrained edge applications.","sentences":["Including Artificial Neural Networks in embedded systems at the edge allows applications to exploit Artificial Intelligence capabilities directly within devices operating at the network periphery.","This paper introduces Spiker+, a comprehensive framework for generating efficient, low-power, and low-area customized Spiking Neural Networks (SNN) accelerators on FPGA for inference at the edge.","Spiker+ presents a configurable multi-layer hardware SNN, a library of highly efficient neuron architectures, and a design framework, enabling the development of complex neural network accelerators with few lines of Python code.","Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking Heidelberg Digits (SHD).","On the MNIST, it demonstrates competitive performance compared to state-of-the-art SNN accelerators.","It outperforms them in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMs (BRAMs), which makes it fit in very small FPGA, and power consumption, draining only 180mW for a complete inference on an input image.","The latency is comparable to the ones observed in the state-of-the-art, with 780us/img.","To the authors' knowledge, Spiker+ is the first SNN accelerator tested on the SHD.","In this case, the accelerator requires 18,268 logic cells and 51 BRAM, with an overall power consumption of 430mW and a latency of 54 us for a complete inference on input data.","This underscores the significance of Spiker+ in the hardware-accelerated SNN landscape, making it an excellent solution to deploy configurable and tunable SNN architectures in resource and power-constrained edge applications."],"url":"http://arxiv.org/abs/2401.01141v1"}
{"created":"2024-01-02 10:10:29","title":"Joint Generative Modeling of Scene Graphs and Images via Diffusion Models","abstract":"In this paper, we present a novel generative task: joint scene graph - image generation. While previous works have explored image generation conditioned on scene graphs or layouts, our task is distinctive and important as it involves generating scene graphs themselves unconditionally from noise, enabling efficient and interpretable control for image generation. Our task is challenging, requiring the generation of plausible scene graphs with heterogeneous attributes for nodes (objects) and edges (relations among objects), including continuous object bounding boxes and discrete object and relation categories. We introduce a novel diffusion model, DiffuseSG, that jointly models the adjacency matrix along with heterogeneous node and edge attributes. We explore various types of encodings for the categorical data, relaxing it into a continuous space. With a graph transformer being the denoiser, DiffuseSG successively denoises the scene graph representation in a continuous space and discretizes the final representation to generate the clean scene graph. Additionally, we introduce an IoU regularization to enhance the empirical performance. Our model significantly outperforms existing methods in scene graph generation on the Visual Genome and COCO-Stuff datasets, both on standard and newly introduced metrics that better capture the problem complexity. Moreover, we demonstrate the additional benefits of our model in two downstream applications: 1) excelling in a series of scene graph completion tasks, and 2) improving scene graph detection models by using extra training samples generated from DiffuseSG.","sentences":["In this paper, we present a novel generative task: joint scene graph - image generation.","While previous works have explored image generation conditioned on scene graphs or layouts, our task is distinctive and important as it involves generating scene graphs themselves unconditionally from noise, enabling efficient and interpretable control for image generation.","Our task is challenging, requiring the generation of plausible scene graphs with heterogeneous attributes for nodes (objects) and edges (relations among objects), including continuous object bounding boxes and discrete object and relation categories.","We introduce a novel diffusion model, DiffuseSG, that jointly models the adjacency matrix along with heterogeneous node and edge attributes.","We explore various types of encodings for the categorical data, relaxing it into a continuous space.","With a graph transformer being the denoiser, DiffuseSG successively denoises the scene graph representation in a continuous space and discretizes the final representation to generate the clean scene graph.","Additionally, we introduce an IoU regularization to enhance the empirical performance.","Our model significantly outperforms existing methods in scene graph generation on the Visual Genome and COCO-Stuff datasets, both on standard and newly introduced metrics that better capture the problem complexity.","Moreover, we demonstrate the additional benefits of our model in two downstream applications: 1) excelling in a series of scene graph completion tasks, and 2) improving scene graph detection models by using extra training samples generated from DiffuseSG."],"url":"http://arxiv.org/abs/2401.01130v1"}
{"created":"2024-01-02 09:31:14","title":"Utilizing Autoregressive Networks for Full Lifecycle Data Generation of Rolling Bearings for RUL Prediction","abstract":"The prediction of rolling bearing lifespan is of significant importance in industrial production. However, the scarcity of high-quality, full lifecycle data has been a major constraint in achieving precise predictions. To address this challenge, this paper introduces the CVGAN model, a novel framework capable of generating one-dimensional vibration signals in both horizontal and vertical directions, conditioned on historical vibration data and remaining useful life. In addition, we propose an autoregressive generation method that can iteratively utilize previously generated vibration information to guide the generation of current signals. The effectiveness of the CVGAN model is validated through experiments conducted on the PHM 2012 dataset. Our findings demonstrate that the CVGAN model, in terms of both MMD and FID metrics, outperforms many advanced methods in both autoregressive and non-autoregressive generation modes. Notably, training using the full lifecycle data generated by the CVGAN model significantly improves the performance of the predictive model. This result highlights the effectiveness of the data generated by CVGans in enhancing the predictive power of these models.","sentences":["The prediction of rolling bearing lifespan is of significant importance in industrial production.","However, the scarcity of high-quality, full lifecycle data has been a major constraint in achieving precise predictions.","To address this challenge, this paper introduces the CVGAN model, a novel framework capable of generating one-dimensional vibration signals in both horizontal and vertical directions, conditioned on historical vibration data and remaining useful life.","In addition, we propose an autoregressive generation method that can iteratively utilize previously generated vibration information to guide the generation of current signals.","The effectiveness of the CVGAN model is validated through experiments conducted on the PHM 2012 dataset.","Our findings demonstrate that the CVGAN model, in terms of both MMD and FID metrics, outperforms many advanced methods in both autoregressive and non-autoregressive generation modes.","Notably, training using the full lifecycle data generated by the CVGAN model significantly improves the performance of the predictive model.","This result highlights the effectiveness of the data generated by CVGans in enhancing the predictive power of these models."],"url":"http://arxiv.org/abs/2401.01119v1"}
{"created":"2024-01-02 08:52:53","title":"Constructing Approximate Single-Source Distance Sensitivity Oracles in Nearly Linear Time","abstract":"For an input graph $G=(V, E)$ and a source vertex $s \\in V$, the \\emph{$\\alpha$-approximate vertex fault-tolerant distance sensitivity oracle} (\\emph{$\\alpha$-VSDO}) answers an $\\alpha$-approximate distance from $s$ to $t$ in $G-x$ for any query $(x, t)$. It is a data structure version of the so-called single-source replacement path problem (SSRP). In this paper, we present a new \\emph{nearly linear time} algorithm of constructing the $(1 + \\epsilon)$-VSDO for any weighted directed graph of $n$ vertices and $m$ edges with integer weights in range $[1, W]$, and any positive constant $\\epsilon \\in (0, 1]$. More precisely, the presented oracle attains $\\tilde{O}(m / \\epsilon + n /\\epsilon^2)$ construction time, $\\tilde{O}(n/ \\epsilon)$ size, and $\\tilde{O}(1/\\epsilon)$ query time for any polynomially-bounded $W$. To the best of our knowledge, this is the first non-trivial result for SSRP/VSDO beating the trivial $\\tilde{O}(mn)$ computation time for directed graphs with polynomially-bounded edge weights. Such a result has been unknown so far even for the setting of $(1 + \\epsilon)$-approximation. It also implies that the known barrier of $\\Omega(m\\sqrt{n})$ time for the exact SSRP by Chechik and Magen~[ICALP2020] does not apply to the case of approximation.","sentences":["For an input graph $G=(V, E)$ and a source vertex $s \\in V$, the \\emph{$\\alpha$-approximate vertex fault-tolerant distance sensitivity oracle} (\\emph{$\\alpha$-VSDO}) answers an $\\alpha$-approximate distance from $s$ to $t$ in $G-x$ for any query $(x, t)$. It is a data structure version of the so-called single-source replacement path problem (SSRP).","In this paper, we present a new \\emph{nearly linear time} algorithm of constructing the $(1 + \\epsilon)$-VSDO for any weighted directed graph of $n$ vertices and $m$ edges with integer weights in range $[1, W]$, and any positive constant $\\epsilon \\in (0, 1]$. More precisely, the presented oracle attains $\\tilde{O}(m / \\epsilon + n /\\epsilon^2)$ construction time, $\\tilde{O}(n/ \\epsilon)$ size, and $\\tilde{O}(1/\\epsilon)$ query time for any polynomially-bounded $W$. To the best of our knowledge, this is the first non-trivial result for SSRP/VSDO beating the trivial $\\tilde{O}(mn)$ computation time for directed graphs with polynomially-bounded edge weights.","Such a result has been unknown so far even for the setting of $(1 + \\epsilon)$-approximation.","It also implies that the known barrier of $\\Omega(m\\sqrt{n})$ time for the exact SSRP by Chechik and Magen~[ICALP2020] does not apply to the case of approximation."],"url":"http://arxiv.org/abs/2401.01103v1"}
{"created":"2024-01-02 08:45:41","title":"Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing","abstract":"Face recognition systems have raised concerns due to their vulnerability to different presentation attacks, and system security has become an increasingly critical concern. Although many face anti-spoofing (FAS) methods perform well in intra-dataset scenarios, their generalization remains a challenge. To address this issue, some methods adopt domain adversarial training (DAT) to extract domain-invariant features. However, the competition between the encoder and the domain discriminator can cause the network to be difficult to train and converge. In this paper, we propose a domain adversarial attack (DAA) method to mitigate the training instability problem by adding perturbations to the input images, which makes them indistinguishable across domains and enables domain alignment. Moreover, since models trained on limited data and types of attacks cannot generalize well to unknown attacks, we propose a dual perceptual and generative knowledge distillation framework for face anti-spoofing that utilizes pre-trained face-related models containing rich face priors. Specifically, we adopt two different face-related models as teachers to transfer knowledge to the target student model. The pre-trained teacher models are not from the task of face anti-spoofing but from perceptual and generative tasks, respectively, which implicitly augment the data. By combining both DAA and dual-teacher knowledge distillation, we develop a dual teacher knowledge distillation with domain alignment framework (DTDA) for face anti-spoofing. The advantage of our proposed method has been verified through extensive ablation studies and comparison with state-of-the-art methods on public datasets across multiple protocols.","sentences":["Face recognition systems have raised concerns due to their vulnerability to different presentation attacks, and system security has become an increasingly critical concern.","Although many face anti-spoofing (FAS) methods perform well in intra-dataset scenarios, their generalization remains a challenge.","To address this issue, some methods adopt domain adversarial training (DAT) to extract domain-invariant features.","However, the competition between the encoder and the domain discriminator can cause the network to be difficult to train and converge.","In this paper, we propose a domain adversarial attack (DAA) method to mitigate the training instability problem by adding perturbations to the input images, which makes them indistinguishable across domains and enables domain alignment.","Moreover, since models trained on limited data and types of attacks cannot generalize well to unknown attacks, we propose a dual perceptual and generative knowledge distillation framework for face anti-spoofing that utilizes pre-trained face-related models containing rich face priors.","Specifically, we adopt two different face-related models as teachers to transfer knowledge to the target student model.","The pre-trained teacher models are not from the task of face anti-spoofing but from perceptual and generative tasks, respectively, which implicitly augment the data.","By combining both DAA and dual-teacher knowledge distillation, we develop a dual teacher knowledge distillation with domain alignment framework (DTDA) for face anti-spoofing.","The advantage of our proposed method has been verified through extensive ablation studies and comparison with state-of-the-art methods on public datasets across multiple protocols."],"url":"http://arxiv.org/abs/2401.01102v1"}
{"created":"2024-01-02 08:43:06","title":"Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding","abstract":"As a pivotal approach in machine learning and data science, manifold learning aims to uncover the intrinsic low-dimensional structure within complex nonlinear manifolds in high-dimensional space. By exploiting the manifold hypothesis, various techniques for nonlinear dimension reduction have been developed to facilitate visualization, classification, clustering, and gaining key insights. Although existing manifold learning methods have achieved remarkable successes, they still suffer from extensive distortions incurred in the global structure, which hinders the understanding of underlying patterns. Scalability issues also limit their applicability for handling large-scale data. Here, we propose a scalable manifold learning (scML) method that can manipulate large-scale and high-dimensional data in an efficient manner. It starts by seeking a set of landmarks to construct the low-dimensional skeleton of the entire data and then incorporates the non-landmarks into the landmark space based on the constrained locally linear embedding (CLLE). We empirically validated the effectiveness of scML on synthetic datasets and real-world benchmarks of different types, and applied it to analyze the single-cell transcriptomics and detect anomalies in electrocardiogram (ECG) signals. scML scales well with increasing data sizes and exhibits promising performance in preserving the global structure. The experiments demonstrate notable robustness in embedding quality as the sample rate decreases.","sentences":["As a pivotal approach in machine learning and data science, manifold learning aims to uncover the intrinsic low-dimensional structure within complex nonlinear manifolds in high-dimensional space.","By exploiting the manifold hypothesis, various techniques for nonlinear dimension reduction have been developed to facilitate visualization, classification, clustering, and gaining key insights.","Although existing manifold learning methods have achieved remarkable successes, they still suffer from extensive distortions incurred in the global structure, which hinders the understanding of underlying patterns.","Scalability issues also limit their applicability for handling large-scale data.","Here, we propose a scalable manifold learning (scML) method that can manipulate large-scale and high-dimensional data in an efficient manner.","It starts by seeking a set of landmarks to construct the low-dimensional skeleton of the entire data and then incorporates the non-landmarks into the landmark space based on the constrained locally linear embedding (CLLE).","We empirically validated the effectiveness of scML on synthetic datasets and real-world benchmarks of different types, and applied it to analyze the single-cell transcriptomics and detect anomalies in electrocardiogram (ECG) signals.","scML scales well with increasing data sizes and exhibits promising performance in preserving the global structure.","The experiments demonstrate notable robustness in embedding quality as the sample rate decreases."],"url":"http://arxiv.org/abs/2401.01100v1"}
