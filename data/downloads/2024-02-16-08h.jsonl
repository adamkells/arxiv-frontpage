{"created":"2024-02-15 18:59:43","title":"Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling","abstract":"Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.","sentences":["Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics.","These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements).","While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors.","These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift.","For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment.","In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction.","HiSS stacks structured state-space models on top of each other to create a temporal hierarchy.","Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE.","Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques.","Code, datasets and videos can be found on https://hiss-csp.github.io."],"url":"http://arxiv.org/abs/2402.10211v1"}
{"created":"2024-02-15 18:59:18","title":"Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation","abstract":"Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (\"winner\" and \"loser\" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.","sentences":["Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs).","While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data.","Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (\"winner\" and \"loser\" images) for each text prompt.","In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process.","Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment.","Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration.","By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data."],"url":"http://arxiv.org/abs/2402.10210v1"}
{"created":"2024-02-15 18:56:46","title":"Bridging Associative Memory and Probabilistic Modeling","abstract":"Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence. The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \\textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the evidence lower bound. Third, using tools from associative memory, we analytically and numerically characterize the memory capacity of Gaussian kernel density estimators, a widespread tool in probababilistic modeling. Fourth, we study a widespread implementation choice in transformers -- normalization followed by self attention -- to show it performs clustering on the hypersphere. Altogether, this work urges further exchange of useful ideas between these two continents of artificial intelligence.","sentences":["Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence.","The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions.","Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions.","We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \\textit{in-context learning of energy functions}.","Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the evidence lower bound.","Third, using tools from associative memory, we analytically and numerically characterize the memory capacity of Gaussian kernel density estimators, a widespread tool in probababilistic modeling.","Fourth, we study a widespread implementation choice in transformers -- normalization followed by self attention -- to show it performs clustering on the hypersphere.","Altogether, this work urges further exchange of useful ideas between these two continents of artificial intelligence."],"url":"http://arxiv.org/abs/2402.10202v1"}
{"created":"2024-02-15 18:48:21","title":"FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients","abstract":"Federated learning (FL) is a distributed learning paradigm that facilitates collaborative training of a shared global model across devices while keeping data localized. The deployment of FL in numerous real-world applications faces delays, primarily due to the prevalent reliance on supervised tasks. Generating detailed labels at edge devices, if feasible, is demanding, given resource constraints and the imperative for continuous data updates. In addressing these challenges, solutions such as federated semi-supervised learning (FSSL), which relies on unlabeled clients' data and a limited amount of labeled data on the server, become pivotal. In this paper, we propose FedAnchor, an innovative FSSL method that introduces a unique double-head structure, called anchor head, paired with the classification head trained exclusively on labeled anchor data on the server. The anchor head is empowered with a newly designed label contrastive loss based on the cosine similarity metric. Our approach mitigates the confirmation bias and overfitting issues associated with pseudo-labeling techniques based on high-confidence model prediction samples. Extensive experiments on CIFAR10/100 and SVHN datasets demonstrate that our method outperforms the state-of-the-art method by a significant margin in terms of convergence rate and model accuracy.","sentences":["Federated learning (FL) is a distributed learning paradigm that facilitates collaborative training of a shared global model across devices while keeping data localized.","The deployment of FL in numerous real-world applications faces delays, primarily due to the prevalent reliance on supervised tasks.","Generating detailed labels at edge devices, if feasible, is demanding, given resource constraints and the imperative for continuous data updates.","In addressing these challenges, solutions such as federated semi-supervised learning (FSSL), which relies on unlabeled clients' data and a limited amount of labeled data on the server, become pivotal.","In this paper, we propose FedAnchor, an innovative FSSL method that introduces a unique double-head structure, called anchor head, paired with the classification head trained exclusively on labeled anchor data on the server.","The anchor head is empowered with a newly designed label contrastive loss based on the cosine similarity metric.","Our approach mitigates the confirmation bias and overfitting issues associated with pseudo-labeling techniques based on high-confidence model prediction samples.","Extensive experiments on CIFAR10/100 and SVHN datasets demonstrate that our method outperforms the state-of-the-art method by a significant margin in terms of convergence rate and model accuracy."],"url":"http://arxiv.org/abs/2402.10191v1"}
{"created":"2024-02-15 18:46:24","title":"Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models","abstract":"In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: \\url{https://github.com/lingchen0331/UQ_ICL}.","sentences":["In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt.","However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed.","Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning.","In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty).","We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties.","The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion.","Extensive experiments are conducted to demonstrate the effectiveness of the decomposition.","The code and data are available at: \\url{https://github.com/lingchen0331/UQ_ICL}."],"url":"http://arxiv.org/abs/2402.10189v1"}
{"created":"2024-02-15 18:41:35","title":"Self-consistent Validation for Machine Learning Electronic Structure","abstract":"Machine learning has emerged as a significant approach to efficiently tackle electronic structure problems. Despite its potential, there is less guarantee for the model to generalize to unseen data that hinders its application in real-world scenarios. To address this issue, a technique has been proposed to estimate the accuracy of the predictions. This method integrates machine learning with self-consistent field methods to achieve both low validation cost and interpret-ability. This, in turn, enables exploration of the model's ability with active learning and instills confidence in its integration into real-world studies.","sentences":["Machine learning has emerged as a significant approach to efficiently tackle electronic structure problems.","Despite its potential, there is less guarantee for the model to generalize to unseen data that hinders its application in real-world scenarios.","To address this issue, a technique has been proposed to estimate the accuracy of the predictions.","This method integrates machine learning with self-consistent field methods to achieve both low validation cost and interpret-ability.","This, in turn, enables exploration of the model's ability with active learning and instills confidence in its integration into real-world studies."],"url":"http://arxiv.org/abs/2402.10186v1"}
{"created":"2024-02-15 18:39:24","title":"Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective","abstract":"There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of our analysis is the superiority of the tree-based information structure in reward modeling, compared to chain-based baselines adopted by conventional RLHF methods. We derive that under highly complex contexts with limited data, the tree-based reward model (RM) induces up to $\\Theta(\\log n/\\log\\log n)$ times less variance than chain-based RM where $n$ is the dataset size. To validate our theoretical contribution, we demonstrate that on three different NLP tasks, the tree-based RM achieves 65% win rate on average against chain-based baselines. Looking forward, we hope our framework can serve as a step towards understanding goal misgeneralization.","sentences":["There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance.","Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling.","Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions.","Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior.","Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF.","To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space.","A key insight of our analysis is the superiority of the tree-based information structure in reward modeling, compared to chain-based baselines adopted by conventional RLHF methods.","We derive that under highly complex contexts with limited data, the tree-based reward model (RM) induces up to $\\Theta(\\log n/\\log\\log n)$ times less variance than chain-based RM where $n$ is the dataset size.","To validate our theoretical contribution, we demonstrate that on three different NLP tasks, the tree-based RM achieves 65% win rate on average against chain-based baselines.","Looking forward, we hope our framework can serve as a step towards understanding goal misgeneralization."],"url":"http://arxiv.org/abs/2402.10184v1"}
{"created":"2024-02-15 18:26:11","title":"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset","abstract":"Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.","sentences":["Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills.","Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses.","A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs.","Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs.","The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model.","Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models.","We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license."],"url":"http://arxiv.org/abs/2402.10176v1"}
{"created":"2024-02-15 18:19:18","title":"OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models","abstract":"Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20\\%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30\\%$.","sentences":["Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare.","However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques.","This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions.","OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations.","OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts.","Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20\\%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30\\%$."],"url":"http://arxiv.org/abs/2402.10172v1"}
{"created":"2024-02-15 18:19:16","title":"Data Engineering for Scaling Language Models to 128K Context","abstract":"We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \\textit{quantity} and \\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\textit{domain balance} and \\textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.","sentences":["We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering.","We hypothesize that long context modeling, in particular \\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture.","We investigate the \\textit{quantity} and \\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\textit{domain balance} and \\textit{length upsampling}.","Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important.","We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K."],"url":"http://arxiv.org/abs/2402.10171v1"}
{"created":"2024-02-15 18:11:02","title":"DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning","abstract":"A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music data base that are closely related to the presented query sequence.","sentences":["A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike.","Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections.","In this work, we propose a deep learning based approach to Raga recognition.","Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN).","We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole.","Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task.","Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music data base that are closely related to the presented query sequence."],"url":"http://arxiv.org/abs/2402.10168v1"}
{"created":"2024-02-15 18:08:58","title":"Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks","abstract":"Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage. In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory, study its properties, and its real world implications in AI. The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference. Instead, the model stores data as waves that is updated by the wave's boundary conditions. We rigorously examine the model's capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems. The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem. To understand the model's real-world applicability, we explore two cases: linear boundary condition and non-linear, self-attention-driven boundary condition. The experiments reveal that the linear scenario is effectively learned by Recurrent Neural Networks (RNNs) through backpropagation when modeling history-dependent dynamical systems. Conversely, the non-linear scenario parallels the autoregressive loop of an attention-only transformer. Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures.","sentences":["Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage.","In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory, study its properties, and its real world implications in AI.","The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference.","Instead, the model stores data as waves that is updated by the wave's boundary conditions.","We rigorously examine the model's capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems.","The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem.","To understand the model's real-world applicability, we explore two cases: linear boundary condition and non-linear, self-attention-driven boundary condition.","The experiments reveal that the linear scenario is effectively learned by Recurrent Neural Networks (RNNs) through backpropagation when modeling history-dependent dynamical systems.","Conversely, the non-linear scenario parallels the autoregressive loop of an attention-only transformer.","Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures."],"url":"http://arxiv.org/abs/2402.10163v1"}
{"created":"2024-02-15 18:02:47","title":"InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization","abstract":"Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability. However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms. We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures. By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability. We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications. Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties. We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation.","sentences":["Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making.","As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability.","However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms.","We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures.","By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability.","We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications.","Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties.","We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation."],"url":"http://arxiv.org/abs/2402.10158v1"}
{"created":"2024-02-15 17:49:50","title":"A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets","abstract":"Federated Learning is a machine learning approach that enables the training of a deep learning model among several participants with sensitive data that wish to share their own knowledge without compromising the privacy of their data. In this research, the authors employ a secured Federated Learning method with an additional layer of privacy and proposes a method for addressing the non-IID challenge. Moreover, differential privacy is compared with chaotic-based encryption as layer of privacy. The experimental approach assesses the performance of the federated deep learning model with differential privacy using both IID and non-IID data. In each experiment, the Federated Learning process improves the average performance metrics of the deep neural network, even in the case of non-IID data.","sentences":["Federated Learning is a machine learning approach that enables the training of a deep learning model among several participants with sensitive data that wish to share their own knowledge without compromising the privacy of their data.","In this research, the authors employ a secured Federated Learning method with an additional layer of privacy and proposes a method for addressing the non-IID challenge.","Moreover, differential privacy is compared with chaotic-based encryption as layer of privacy.","The experimental approach assesses the performance of the federated deep learning model with differential privacy using both IID and non-IID data.","In each experiment, the Federated Learning process improves the average performance metrics of the deep neural network, even in the case of non-IID data."],"url":"http://arxiv.org/abs/2402.10145v1"}
{"created":"2024-02-15 17:40:02","title":"TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles","abstract":"In light of recent advances in large language models~(LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users' expression mirroring. We benchmark TOAD on two response generation tasks and the results show that modeling more verbose or responses without user expression mirroring is more challenging.","sentences":["In light of recent advances in large language models~(LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios.","However, the creation of high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be slow and costly.","To address these challenges, we introduce Task-Oriented Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline.","The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options.","Two aspects of system response styles are considered, verbosity level and users' expression mirroring.","We benchmark TOAD on two response generation tasks and the results show that modeling more verbose or responses without user expression mirroring is more challenging."],"url":"http://arxiv.org/abs/2402.10137v1"}
{"created":"2024-02-15 17:38:32","title":"Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data","abstract":"The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on federated learning, an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. In the initial proposal of federated learning the architecture was centralised and the aggregation was done with federated averaging, meaning that a central server will orchestrate the federation using the most straightforward averaging strategy. This research is focused on testing different federated strategies in a peer-to-peer environment. The authors propose various aggregation strategies for federated learning, including weighted averaging aggregation, using different factors and strategies based on participant contribution. The strategies are tested with varying data sizes to identify the most robust ones. This research tests the strategies with several biomedical datasets and the results of the experiments show that the accuracy-based weighted average outperforms the classical federated averaging method.","sentences":["The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on federated learning, an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data.","In the initial proposal of federated learning the architecture was centralised and the aggregation was done with federated averaging, meaning that a central server will orchestrate the federation using the most straightforward averaging strategy.","This research is focused on testing different federated strategies in a peer-to-peer environment.","The authors propose various aggregation strategies for federated learning, including weighted averaging aggregation, using different factors and strategies based on participant contribution.","The strategies are tested with varying data sizes to identify the most robust ones.","This research tests the strategies with several biomedical datasets and the results of the experiments show that the accuracy-based weighted average outperforms the classical federated averaging method."],"url":"http://arxiv.org/abs/2402.10135v1"}
{"created":"2024-02-15 17:37:25","title":"Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem","abstract":"Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models. Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using large language models to propose levels based on the gameplay data continuously collected from individual players. We compared the levels generated using our approach with levels generated with more traditional procedural generation techniques. Our easily reproducible method has proven viable in a production setting and outperformed levels generated by traditional methods in the probability that a player will not quit the game mid-level.","sentences":["Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs.","In newer approaches, procedural content generation utilizes machine learning.","However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive.","The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models.","Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it.","Therefore, this paper presents a novel approach to achieving personalization by using large language models to propose levels based on the gameplay data continuously collected from individual players.","We compared the levels generated using our approach with levels generated with more traditional procedural generation techniques.","Our easily reproducible method has proven viable in a production setting and outperformed levels generated by traditional methods in the probability that a player will not quit the game mid-level."],"url":"http://arxiv.org/abs/2402.10133v1"}
{"created":"2024-02-15 17:20:55","title":"Mitigating subjectivity and bias in AI development indices: A robust approach to redefining country rankings","abstract":"Countries worldwide have been implementing different actions national strategies for Artificial Intelligence (AI) to shape policy priorities and guide their development concerning AI. Several AI indices have emerged to assess countries' progress in AI development, aiding decision-making on investments and policy choices. Typically, these indices combine multiple indicators using linear additive methods such as weighted sums, although they are limited in their ability to account for interactions among indicators. Another limitation concerns the use of deterministic weights, which can be perceived as subjective and vulnerable to debate and scrutiny, especially by nations that feel disadvantaged. Aiming at mitigating these problems, we conduct a methodological analysis to derive AI indices based on multiple criteria decision analysis. Initially, we assess correlations between different AI dimensions and employ the Choquet integral to model them. Thus, we apply the Stochastic Multicriteria Acceptability Analysis (SMAA) to conduct a sensitivity analysis using both weighted sum and Choquet integral in order to evaluate the stability of the indices with regard the weights. Finally, we introduce a novel ranking methodology based on SMAA, which considers several sets of weights to derive the ranking of countries. As a result, instead of using predefined weights, in the proposed approach, the ranking is achieved based on the probabilities of countries in occupying a specific position. In the computational analysis, we utilize the data employed in The Global AI Index proposed by Tortoise. Results reveal correlations in the data, and our approach effectively mitigates bias. In the sensitivity analysis, we scrutinize changes in the ranking resulting from weight adjustments. We demonstrate that our proposal rankings closely align with those derived from weight variations, proving to be more robust.","sentences":["Countries worldwide have been implementing different actions national strategies for Artificial Intelligence (AI) to shape policy priorities and guide their development concerning AI.","Several AI indices have emerged to assess countries' progress in AI development, aiding decision-making on investments and policy choices.","Typically, these indices combine multiple indicators using linear additive methods such as weighted sums, although they are limited in their ability to account for interactions among indicators.","Another limitation concerns the use of deterministic weights, which can be perceived as subjective and vulnerable to debate and scrutiny, especially by nations that feel disadvantaged.","Aiming at mitigating these problems, we conduct a methodological analysis to derive AI indices based on multiple criteria decision analysis.","Initially, we assess correlations between different AI dimensions and employ the Choquet integral to model them.","Thus, we apply the Stochastic Multicriteria Acceptability Analysis (SMAA) to conduct a sensitivity analysis using both weighted sum and Choquet integral in order to evaluate the stability of the indices with regard the weights.","Finally, we introduce a novel ranking methodology based on SMAA, which considers several sets of weights to derive the ranking of countries.","As a result, instead of using predefined weights, in the proposed approach, the ranking is achieved based on the probabilities of countries in occupying a specific position.","In the computational analysis, we utilize the data employed in The Global AI Index proposed by Tortoise.","Results reveal correlations in the data, and our approach effectively mitigates bias.","In the sensitivity analysis, we scrutinize changes in the ranking resulting from weight adjustments.","We demonstrate that our proposal rankings closely align with those derived from weight variations, proving to be more robust."],"url":"http://arxiv.org/abs/2402.10122v1"}
{"created":"2024-02-15 17:06:21","title":"Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning","abstract":"Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs. Our codes, models, and data will be released at https://github.com/tianyi-lab/Reflection_Tuning.","sentences":["Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality.","Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned.","This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data.","This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance.","Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data.","We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs.","Our codes, models, and data will be released at https://github.com/tianyi-lab/Reflection_Tuning."],"url":"http://arxiv.org/abs/2402.10110v1"}
{"created":"2024-02-15 17:05:48","title":"Towards Reducing Diagnostic Errors with Interpretable Risk Prediction","abstract":"Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual \"true\" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.","sentences":["Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs).","In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors.","In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential.","To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual \"true\" diagnoses.","We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made.","We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model.","We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses."],"url":"http://arxiv.org/abs/2402.10109v1"}
{"created":"2024-02-15 16:56:25","title":"A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research","abstract":"Distributed Artificial Intelligence is attracting interest day by day. In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way. The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation. This method is applied to a cancer detection problem, proving that the performance of the model is improved by the Federated Learning process, and obtaining similar results to the ones that can be found in the literature.","sentences":["Distributed Artificial Intelligence is attracting interest day by day.","In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way.","The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation.","This method is applied to a cancer detection problem, proving that the performance of the model is improved by the Federated Learning process, and obtaining similar results to the ones that can be found in the literature."],"url":"http://arxiv.org/abs/2402.10102v1"}
{"created":"2024-02-15 16:51:38","title":"Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling","abstract":"Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while considering both the data and system heterogeneity. Experimental results under practical wireless network settings with real-world prototype demonstrate that the proposed independent sampling scheme substantially outperforms the current best sampling schemes under various training models and datasets.","sentences":["Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency.","While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks.","In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation.","We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme.","Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while considering both the data and system heterogeneity.","Experimental results under practical wireless network settings with real-world prototype demonstrate that the proposed independent sampling scheme substantially outperforms the current best sampling schemes under various training models and datasets."],"url":"http://arxiv.org/abs/2402.10097v1"}
{"created":"2024-02-15 16:49:42","title":"Classification Diffusion Models","abstract":"A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\\textit{classify}$ between data samples and samples from some reference distribution. These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images. A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\\textit{denoise}$ data samples. These approaches achieve state-of-the-art results in image, video, and audio generation. In this work, we present $\\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods. Our approach is based on the observation that an MSE-optimal denoiser for white Gaussian noise can be expressed in terms of the gradient of a cross-entropy-optimal classifier for predicting the noise level. As we illustrate, CDM achieves better denoising results compared to DDM, and leads to at least comparable FID in image generation. CDM is also capable of highly efficient one-step exact likelihood estimation, achieving state-of-the-art results among methods that use a single step. Code is available on the project's webpage in https://shaharYadin.github.io/CDM/ .","sentences":["A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\\textit{classify}$ between data samples and samples from some reference distribution.","These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images.","A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\\textit{denoise}$ data samples.","These approaches achieve state-of-the-art results in image, video, and audio generation.","In this work, we present $\\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods.","Our approach is based on the observation that an MSE-optimal denoiser for white Gaussian noise can be expressed in terms of the gradient of a cross-entropy-optimal classifier for predicting the noise level.","As we illustrate, CDM achieves better denoising results compared to DDM, and leads to at least comparable FID in image generation.","CDM is also capable of highly efficient one-step exact likelihood estimation, achieving state-of-the-art results among methods that use a single step.","Code is available on the project's webpage in https://shaharYadin.github.io/CDM/ ."],"url":"http://arxiv.org/abs/2402.10095v1"}
{"created":"2024-02-15 16:46:16","title":"MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations","abstract":"We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger models that were trained on up to 2000x more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page: https://ml-jku.github.io/MIM-Refiner","sentences":["We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models.","The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers.","Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers.","In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   ","The refinement process is short but effective.","Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features.","Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger models that were trained on up to 2000x more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page: https://ml-jku.github.io/MIM-Refiner"],"url":"http://arxiv.org/abs/2402.10093v1"}
{"created":"2024-02-15 16:42:04","title":"FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning","abstract":"Federated Learning (FL) represents a promising approach to typical privacy concerns associated with centralized Machine Learning (ML) deployments. Despite its well-known advantages, FL is vulnerable to security attacks such as Byzantine behaviors and poisoning attacks, which can significantly degrade model performance and hinder convergence. The effectiveness of existing approaches to mitigate complex attacks, such as median, trimmed mean, or Krum aggregation functions, has been only partially demonstrated in the case of specific attacks. Our study introduces a novel robust aggregation mechanism utilizing the Fourier Transform (FT), which is able to effectively handling sophisticated attacks without prior knowledge of the number of attackers. Employing this data technique, weights generated by FL clients are projected into the frequency domain to ascertain their density function, selecting the one exhibiting the highest frequency. Consequently, malicious clients' weights are excluded. Our proposed approach was tested against various model poisoning attacks, demonstrating superior performance over state-of-the-art aggregation methods.","sentences":["Federated Learning (FL) represents a promising approach to typical privacy concerns associated with centralized Machine Learning (ML) deployments.","Despite its well-known advantages, FL is vulnerable to security attacks such as Byzantine behaviors and poisoning attacks, which can significantly degrade model performance and hinder convergence.","The effectiveness of existing approaches to mitigate complex attacks, such as median, trimmed mean, or Krum aggregation functions, has been only partially demonstrated in the case of specific attacks.","Our study introduces a novel robust aggregation mechanism utilizing the Fourier Transform (FT), which is able to effectively handling sophisticated attacks without prior knowledge of the number of attackers.","Employing this data technique, weights generated by FL clients are projected into the frequency domain to ascertain their density function, selecting the one exhibiting the highest frequency.","Consequently, malicious clients' weights are excluded.","Our proposed approach was tested against various model poisoning attacks, demonstrating superior performance over state-of-the-art aggregation methods."],"url":"http://arxiv.org/abs/2402.10082v1"}
{"created":"2024-02-15 16:37:14","title":"GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via Reinforcement Learning","abstract":"Graph neural networks (GNNs) have recently demonstrated significant success. Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a low cost. However, most existing methods for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios. This further adversely affects the classification performance. To tackle this issue, in this paper, we propose a novel reinforced class-balanced active learning framework for GNNs, namely, GraphCBAL. It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes. GraphCBAL designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance. We further upgrade GraphCBAL to GraphCBAL++ by introducing a punishment mechanism to obtain a more class-balanced labeled set. Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed approaches, achieving superior performance over state-of-the-art baselines. In particular, our methods can strike the balance between classification results and class balance.","sentences":["Graph neural networks (GNNs) have recently demonstrated significant success.","Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a low cost.","However, most existing methods for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios.","This further adversely affects the classification performance.","To tackle this issue, in this paper, we propose a novel reinforced class-balanced active learning framework for GNNs, namely, GraphCBAL.","It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes.","GraphCBAL designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance.","We further upgrade GraphCBAL to GraphCBAL++ by introducing a punishment mechanism to obtain a more class-balanced labeled set.","Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed approaches, achieving superior performance over state-of-the-art baselines.","In particular, our methods can strike the balance between classification results and class balance."],"url":"http://arxiv.org/abs/2402.10074v1"}
{"created":"2024-02-15 16:31:54","title":"NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction","abstract":"Drawing inspiration from the primate brain's intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based Transformer architecture. The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis. In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively. Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated. In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices are provided one at a time. The NYCTALE framework then computes an evidence vector associated with contribution of each new CT image. A decision is made once the total accumulated evidence surpasses a specific threshold. Preliminary experimental analyses conducted using a challenging in-house dataset comprising 114 subjects. The results are noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even with approximately 60% less training data on this demanding and small dataset.","sentences":["Drawing inspiration from the primate brain's intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based Transformer architecture.","The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis.","In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night.","The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively.","Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated.","In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices are provided one at a time.","The NYCTALE framework then computes an evidence vector associated with contribution of each new CT image.","A decision is made once the total accumulated evidence surpasses a specific threshold.","Preliminary experimental analyses conducted using a challenging in-house dataset comprising 114 subjects.","The results are noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even with approximately 60% less training data on this demanding and small dataset."],"url":"http://arxiv.org/abs/2402.10066v1"}
{"created":"2024-02-15 16:30:55","title":"How Much Does Each Datapoint Leak Your Privacy? Quantifying the Per-datum Membership Leakage","abstract":"We study the per-datum Membership Inference Attacks (MIAs), where an attacker aims to infer whether a fixed target datum has been included in the input dataset of an algorithm and thus, violates privacy. First, we define the membership leakage of a datum as the advantage of the optimal adversary targeting to identify it. Then, we quantify the per-datum membership leakage for the empirical mean, and show that it depends on the Mahalanobis distance between the target datum and the data-generating distribution. We further assess the effect of two privacy defences, i.e. adding Gaussian noise and sub-sampling. We quantify exactly how both of them decrease the per-datum membership leakage. Our analysis builds on a novel proof technique that combines an Edgeworth expansion of the likelihood ratio test and a Lindeberg-Feller central limit theorem. Our analysis connects the existing likelihood ratio and scalar product attacks, and also justifies different canary selection strategies used in the privacy auditing literature. Finally, our experiments demonstrate the impacts of the leakage score, the sub-sampling ratio and the noise scale on the per-datum membership leakage as indicated by the theory.","sentences":["We study the per-datum Membership Inference Attacks (MIAs), where an attacker aims to infer whether a fixed target datum has been included in the input dataset of an algorithm and thus, violates privacy.","First, we define the membership leakage of a datum as the advantage of the optimal adversary targeting to identify it.","Then, we quantify the per-datum membership leakage for the empirical mean, and show that it depends on the Mahalanobis distance between the target datum and the data-generating distribution.","We further assess the effect of two privacy defences, i.e. adding Gaussian noise and sub-sampling.","We quantify exactly how both of them decrease the per-datum membership leakage.","Our analysis builds on a novel proof technique that combines an Edgeworth expansion of the likelihood ratio test and a Lindeberg-Feller central limit theorem.","Our analysis connects the existing likelihood ratio and scalar product attacks, and also justifies different canary selection strategies used in the privacy auditing literature.","Finally, our experiments demonstrate the impacts of the leakage score, the sub-sampling ratio and the noise scale on the per-datum membership leakage as indicated by the theory."],"url":"http://arxiv.org/abs/2402.10065v1"}
{"created":"2024-02-15 16:30:45","title":"Balancing the Causal Effects in Class-Incremental Learning","abstract":"Class-Incremental Learning (CIL) is a practical and challenging problem for achieving general artificial intelligence. Recently, Pre-Trained Models (PTMs) have led to breakthroughs in both visual and natural language processing tasks. Despite recent studies showing PTMs' potential ability to learn sequentially, a plethora of work indicates the necessity of alleviating the catastrophic forgetting of PTMs. Through a pilot study and a causal analysis of CIL, we reveal that the crux lies in the imbalanced causal effects between new and old data. Specifically, the new data encourage models to adapt to new classes while hindering the adaptation of old classes. Similarly, the old data encourages models to adapt to old classes while hindering the adaptation of new classes. In other words, the adaptation process between new and old classes conflicts from the causal perspective. To alleviate this problem, we propose Balancing the Causal Effects (BaCE) in CIL. Concretely, BaCE proposes two objectives for building causal paths from both new and old data to the prediction of new and classes, respectively. In this way, the model is encouraged to adapt to all classes with causal effects from both new and old data and thus alleviates the causal imbalance problem. We conduct extensive experiments on continual image classification, continual text classification, and continual named entity recognition. Empirical results show that BaCE outperforms a series of CIL methods on different tasks and settings.","sentences":["Class-Incremental Learning (CIL) is a practical and challenging problem for achieving general artificial intelligence.","Recently, Pre-Trained Models (PTMs) have led to breakthroughs in both visual and natural language processing tasks.","Despite recent studies showing PTMs' potential ability to learn sequentially, a plethora of work indicates the necessity of alleviating the catastrophic forgetting of PTMs.","Through a pilot study and a causal analysis of CIL, we reveal that the crux lies in the imbalanced causal effects between new and old data.","Specifically, the new data encourage models to adapt to new classes while hindering the adaptation of old classes.","Similarly, the old data encourages models to adapt to old classes while hindering the adaptation of new classes.","In other words, the adaptation process between new and old classes conflicts from the causal perspective.","To alleviate this problem, we propose Balancing the Causal Effects (BaCE) in CIL.","Concretely, BaCE proposes two objectives for building causal paths from both new and old data to the prediction of new and classes, respectively.","In this way, the model is encouraged to adapt to all classes with causal effects from both new and old data and thus alleviates the causal imbalance problem.","We conduct extensive experiments on continual image classification, continual text classification, and continual named entity recognition.","Empirical results show that BaCE outperforms a series of CIL methods on different tasks and settings."],"url":"http://arxiv.org/abs/2402.10063v1"}
{"created":"2024-02-15 16:29:46","title":"X-maps: Direct Depth Lookup for Event-based Structured Light Systems","abstract":"We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras. These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach. Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search. Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process. Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy. Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events. This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial. We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results. Additional results and code are available on our project page: https://fraunhoferhhi.github.io/X-maps/","sentences":["We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras.","These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach.","Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search.","Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process.","Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy.","Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events.","This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial.","We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results.","Additional results and code are available on our project page: https://fraunhoferhhi.github.io/X-maps/"],"url":"http://arxiv.org/abs/2402.10061v1"}
{"created":"2024-02-15 16:23:24","title":"Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates","abstract":"Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society. Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms. We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics. Specifically, we show how the popular Friedkin--Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data. We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others. We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a $(1+\\varepsilon)$-approximate solution in time $\\tilde{O}(m\\sqrt{n} \\lg(1/\\varepsilon))$, where $m$ is the number of edges in the graph and $n$ is the number of vertices. We also present an algorithm that provably computes an $\\varepsilon$-approximation of our model in near-linear time. We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network. Finally, we release an anonymized graph dataset with ground-truth opinions and more than 27\\,000 nodes (the previously largest publicly available dataset contains less than 550 nodes).","sentences":["Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society.","Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms.","We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics.","Specifically, we show how the popular Friedkin--Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data.","We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others.","We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a $(1+\\varepsilon)$-approximate solution in time $\\tilde{O}(m\\sqrt{n} \\lg(1/\\varepsilon))$, where $m$ is the number of edges in the graph and $n$ is the number of vertices.","We also present an algorithm that provably computes an $\\varepsilon$-approximation of our model in near-linear time.","We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network.","Finally, we release an anonymized graph dataset with ground-truth opinions and more than 27\\,000 nodes (the previously largest publicly available dataset contains less than 550 nodes)."],"url":"http://arxiv.org/abs/2402.10053v1"}
{"created":"2024-02-15 16:21:14","title":"Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination","abstract":"While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning. Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also with parameter-efficient fine-tuning, offering a novel pathway to addressing the challenges with private and sensitive data in LLM applications.","sentences":["While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data.","This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities?","In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning.","Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios.","As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks.","Our results demonstrate the usefulness of this approach across different models and sizes, and also with parameter-efficient fine-tuning, offering a novel pathway to addressing the challenges with private and sensitive data in LLM applications."],"url":"http://arxiv.org/abs/2402.10052v1"}
{"created":"2024-02-15 16:07:35","title":"On the Domain Generalizability of RF Fingerprints Through Multifractal Dimension Representation","abstract":"RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a possible method for enabling secure device identification and authentication. Traditional approaches are commonly susceptible to the domain adaptation problem where a model trained on data collected under one domain performs badly when tested on data collected under a different domain. Some examples of a domain change include varying the location or environment of the device and varying the time or day of the data collection. In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are domain generalizable. We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ (in-phase and quadrature) signals, and we evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices. Our experimental results show that the proposed VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using IQ data samples.","sentences":["RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a possible method for enabling secure device identification and authentication.","Traditional approaches are commonly susceptible to the domain adaptation problem where a model trained on data collected under one domain performs badly when tested on data collected under a different domain.","Some examples of a domain change include varying the location or environment of the device and varying the time or day of the data collection.","In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are domain generalizable.","We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ (in-phase and quadrature) signals, and we evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices.","Our experimental results show that the proposed VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using IQ data samples."],"url":"http://arxiv.org/abs/2402.10044v1"}
{"created":"2024-02-15 15:58:42","title":"Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity","abstract":"Purpose: We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely.   Methods: We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data. We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client. We evaluated two federated learning methods, FedAvg and FedProx for these settings.   Results: Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool. However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings.   Conclusion: Despite the effectiveness of federated learning in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms. Notably, FedProx appears to be more robust to the increased heterogeneity.","sentences":["Purpose: We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely.   ","Methods: We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data.","We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client.","We evaluated two federated learning methods, FedAvg and FedProx for these settings.   ","Results: Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool.","However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings.   ","Conclusion: Despite the effectiveness of federated learning in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms.","Notably, FedProx appears to be more robust to the increased heterogeneity."],"url":"http://arxiv.org/abs/2402.10035v1"}
{"created":"2024-02-15 15:53:46","title":"Systematic Literature Review of EM-SCA Attacks on Encryption","abstract":"Cryptography is vital for data security, but cryptographic algorithms can still be vulnerable to side-channel attacks (SCAs), physical assaults exploiting power consumption and EM radiation. SCAs pose a significant threat to cryptographic integrity, compromising device keys. While literature on SCAs focuses on real-world devices, the rise of sophisticated devices necessitates fresh approaches. Electromagnetic side-channel analysis (EM-SCA) gathers information by monitoring EM radiation, capable of retrieving encryption keys and detecting malicious activity. This study evaluates EM-SCA's impact on encryption across scenarios and explores its role in digital forensics and law enforcement. Addressing encryption susceptibility to EM-SCA can empower forensic investigators in overcoming encryption challenges, maintaining their crucial role in law enforcement. Additionally, the paper defines EM-SCA's current state in attacking encryption, highlighting vulnerable and resistant encryption algorithms and devices, and promising EM-SCA approaches. This study offers a comprehensive analysis of EM-SCA in law enforcement and digital forensics, suggesting avenues for further research.","sentences":["Cryptography is vital for data security, but cryptographic algorithms can still be vulnerable to side-channel attacks (SCAs), physical assaults exploiting power consumption and EM radiation.","SCAs pose a significant threat to cryptographic integrity, compromising device keys.","While literature on SCAs focuses on real-world devices, the rise of sophisticated devices necessitates fresh approaches.","Electromagnetic side-channel analysis (EM-SCA) gathers information by monitoring EM radiation, capable of retrieving encryption keys and detecting malicious activity.","This study evaluates EM-SCA's impact on encryption across scenarios and explores its role in digital forensics and law enforcement.","Addressing encryption susceptibility to EM-SCA can empower forensic investigators in overcoming encryption challenges, maintaining their crucial role in law enforcement.","Additionally, the paper defines EM-SCA's current state in attacking encryption, highlighting vulnerable and resistant encryption algorithms and devices, and promising EM-SCA approaches.","This study offers a comprehensive analysis of EM-SCA in law enforcement and digital forensics, suggesting avenues for further research."],"url":"http://arxiv.org/abs/2402.10030v1"}
{"created":"2024-02-15 15:39:46","title":"SAWEC: Sensing-Assisted Wireless Edge Computing","abstract":"Emerging mobile virtual reality (VR) systems will require to continuously perform complex computer vision tasks on ultra-high-resolution video frames through the execution of deep neural networks (DNNs)-based algorithms. Since state-of-the-art DNNs require computational power that is excessive for mobile devices, techniques based on wireless edge computing (WEC) have been recently proposed. However, existing WEC methods require the transmission and processing of a high amount of video data which may ultimately saturate the wireless link. In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing (SAWEC) paradigm to address this issue. SAWEC leverages knowledge about the physical environment to reduce the end-to-end latency and overall computational burden by transmitting to the edge server only the relevant data for the delivery of the service. Our intuition is that the transmission of the portion of the video frames where there are no changes with respect to previous frames can be avoided. Specifically, we leverage wireless sensing techniques to estimate the location of objects in the environment and obtain insights about the environment dynamics. Hence, only the part of the frames where any environmental change is detected is transmitted and processed. We evaluated SAWEC by using a 10K 360$^{\\circ}$ camera with a Wi-Fi 6 sensing system operating at 160 MHz and performing localization and tracking. We perform experiments in an anechoic chamber and a hall room with two human subjects in six different setups. Experimental results show that SAWEC reduces the channel occupation, and end-to-end latency by 93.81%, and 96.19% respectively while improving the instance segmentation performance by 46.98% with respect to state-of-the-art WEC approaches. For reproducibility purposes, we pledge to share our whole dataset and code repository.","sentences":["Emerging mobile virtual reality (VR) systems will require to continuously perform complex computer vision tasks on ultra-high-resolution video frames through the execution of deep neural networks (DNNs)-based algorithms.","Since state-of-the-art DNNs require computational power that is excessive for mobile devices, techniques based on wireless edge computing (WEC) have been recently proposed.","However, existing WEC methods require the transmission and processing of a high amount of video data which may ultimately saturate the wireless link.","In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing (SAWEC) paradigm to address this issue.","SAWEC leverages knowledge about the physical environment to reduce the end-to-end latency and overall computational burden by transmitting to the edge server only the relevant data for the delivery of the service.","Our intuition is that the transmission of the portion of the video frames where there are no changes with respect to previous frames can be avoided.","Specifically, we leverage wireless sensing techniques to estimate the location of objects in the environment and obtain insights about the environment dynamics.","Hence, only the part of the frames where any environmental change is detected is transmitted and processed.","We evaluated SAWEC by using a 10K 360$^{\\circ}$ camera with a Wi-Fi 6 sensing system operating at 160 MHz and performing localization and tracking.","We perform experiments in an anechoic chamber and a hall room with two human subjects in six different setups.","Experimental results show that SAWEC reduces the channel occupation, and end-to-end latency by 93.81%, and 96.19% respectively while improving the instance segmentation performance by 46.98% with respect to state-of-the-art WEC approaches.","For reproducibility purposes, we pledge to share our whole dataset and code repository."],"url":"http://arxiv.org/abs/2402.10021v1"}
{"created":"2024-02-15 15:28:10","title":"A Piecewise Approach for the Analysis of Exact Algorithms","abstract":"To analyze the worst-case running time of branching algorithms, the majority of work in exponential time algorithms focuses on designing complicated branching rules over developing better analysis methods for simple algorithms. In the mid-$2000$s, Fomin et al. [2005] introduced measure & conquer, an advanced general analysis method, sparking widespread adoption for obtaining tighter worst-case running time upper bounds for many fundamental NP-complete problems. Yet, much potential in this direction remains untapped, as most subsequent work applied it without further advancement. Motivated by this, we present piecewise analysis, a new general method that analyzes the running time of branching algorithms. Our approach is to define a similarity ratio that divides instances into groups and then analyze the running time within each group separately. The similarity ratio is a scale between two parameters of an instance I. Instead of relying on a single measure and a single analysis for the whole instance space, our method allows to take advantage of different intrinsic properties of instances with different similarity ratios. To showcase its potential, we reanalyze two $17$-year-old algorithms from Fomin et al. [2007] that solve $4$-Coloring and #$3$-Coloring respectively. The original analysis in their paper gave running times of $O(1.7272^n)$ and $O(1.6262^n)$ respectively for these algorithms, our analysis improves these running times to $O(1.7215^n)$ and $O(1.6232^n)$. Among the two improvements, our new running time $O(1.7215^n)$ is the first improvement in the best known running time for the 4-Coloring problem since 2007.","sentences":["To analyze the worst-case running time of branching algorithms, the majority of work in exponential time algorithms focuses on designing complicated branching rules over developing better analysis methods for simple algorithms.","In the mid-$2000$s, Fomin et al.","[2005] introduced measure & conquer, an advanced general analysis method, sparking widespread adoption for obtaining tighter worst-case running time upper bounds for many fundamental NP-complete problems.","Yet, much potential in this direction remains untapped, as most subsequent work applied it without further advancement.","Motivated by this, we present piecewise analysis, a new general method that analyzes the running time of branching algorithms.","Our approach is to define a similarity ratio that divides instances into groups and then analyze the running time within each group separately.","The similarity ratio is a scale between two parameters of an instance I. Instead of relying on a single measure and a single analysis for the whole instance space, our method allows to take advantage of different intrinsic properties of instances with different similarity ratios.","To showcase its potential, we reanalyze two $17$-year-old algorithms from Fomin et al.","[2007] that solve $4$-Coloring and #$3$-Coloring respectively.","The original analysis in their paper gave running times of $O(1.7272^n)$ and $O(1.6262^n)$ respectively for these algorithms, our analysis improves these running times to $O(1.7215^n)$ and $O(1.6232^n)$. Among the two improvements, our new running time $O(1.7215^n)$ is the first improvement in the best known running time for the 4-Coloring problem since 2007."],"url":"http://arxiv.org/abs/2402.10015v1"}
{"created":"2024-02-15 15:06:33","title":"Privacy Attacks in Decentralized Learning","abstract":"Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some of the factors that affect the performance of the attack, namely the graph topology, the number of attackers, and their position in the graph.","sentences":["Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph.","The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others.","In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood.","Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD.","We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large.","We empirically investigate some of the factors that affect the performance of the attack, namely the graph topology, the number of attackers, and their position in the graph."],"url":"http://arxiv.org/abs/2402.10001v1"}
{"created":"2024-02-15 14:56:44","title":"Scalability limitations of Kademlia DHTs when enabling Data Availability Sampling in Ethereum","abstract":"Scalability in blockchain remains a significant challenge, especially when prioritizing decentralization and security. The Ethereum community has proposed comprehensive data-sharding techniques to overcome storage, computational, and network processing limitations. In this context, the propagation and availability of large blocks become the subject of research to achieve scalable data-sharding. This paper provides insights after exploring the usage of a Kademlia-based DHT to enable Data Availability Sampling (DAS) in Ethereum. It presents a DAS-DHT simulator to study this problem and validates the results of the simulator with experiments in a real DHT network, IPFS. Our results help us understand what parts of DAS can be achieved based on existing Kademlia DHT solutions and which ones cannot. We discuss the limitations of DHT solutions and discuss other alternatives.","sentences":["Scalability in blockchain remains a significant challenge, especially when prioritizing decentralization and security.","The Ethereum community has proposed comprehensive data-sharding techniques to overcome storage, computational, and network processing limitations.","In this context, the propagation and availability of large blocks become the subject of research to achieve scalable data-sharding.","This paper provides insights after exploring the usage of a Kademlia-based DHT to enable Data Availability Sampling (DAS) in Ethereum.","It presents a DAS-DHT simulator to study this problem and validates the results of the simulator with experiments in a real DHT network, IPFS.","Our results help us understand what parts of DAS can be achieved based on existing Kademlia DHT solutions and which ones cannot.","We discuss the limitations of DHT solutions and discuss other alternatives."],"url":"http://arxiv.org/abs/2402.09993v1"}
{"created":"2024-02-15 14:46:03","title":"Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition","abstract":"The face expression is the first thing we pay attention to when we want to understand a person's state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85\\% for the InceptionResNetV2 model.","sentences":["The face expression is the first thing we pay attention to when we want to understand a person's state of mind.","Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field.","In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task.","We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type.","Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures.","To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases.","The combination of these techniques allows to reach average accuracy values of the order of 85\\% for the InceptionResNetV2 model."],"url":"http://arxiv.org/abs/2402.09982v1"}
{"created":"2024-02-15 14:34:23","title":"Interference Mitigation for Network-Level ISAC: An Optimization Perspective","abstract":"Future wireless networks are envisioned to simultaneously provide high data-rate communication and ubiquitous environment-aware services for numerous users. One promising approach to meet this demand is to employ network-level integrated sensing and communications (ISAC) by jointly designing the signal processing and resource allocation over the entire network. However, to unleash the full potential of network-level ISAC, some critical challenges must be tackled. Among them, interference management is one of the most significant ones. In this article, we build up a bridge between interference mitigation techniques and the corresponding optimization methods, which facilitates efficient interference mitigation in network-level ISAC systems. In particular, we first identify several types of interference in network-level ISAC systems, including self-interference, mutual interference, crosstalk, clutter, and multiuser interference. Then, we present several promising techniques that can be utilized to suppress specific types of interference. For each type of interference, we discuss the corresponding problem formulation and identify the associated optimization methods. Moreover, to illustrate the effectiveness of the proposed interference mitigation techniques, two concrete network-level ISAC systems, namely coordinated cellular network-based and distributed antenna-based ISAC systems, are investigated from interference management perspective. Experiment results indicate that it is beneficial to collaboratively employ different interference mitigation techniques and leverage the network structure to achieve the full potential of network-level ISAC. Finally, we highlight several promising future research directions for the design of ISAC systems.","sentences":["Future wireless networks are envisioned to simultaneously provide high data-rate communication and ubiquitous environment-aware services for numerous users.","One promising approach to meet this demand is to employ network-level integrated sensing and communications (ISAC) by jointly designing the signal processing and resource allocation over the entire network.","However, to unleash the full potential of network-level ISAC, some critical challenges must be tackled.","Among them, interference management is one of the most significant ones.","In this article, we build up a bridge between interference mitigation techniques and the corresponding optimization methods, which facilitates efficient interference mitigation in network-level ISAC systems.","In particular, we first identify several types of interference in network-level ISAC systems, including self-interference, mutual interference, crosstalk, clutter, and multiuser interference.","Then, we present several promising techniques that can be utilized to suppress specific types of interference.","For each type of interference, we discuss the corresponding problem formulation and identify the associated optimization methods.","Moreover, to illustrate the effectiveness of the proposed interference mitigation techniques, two concrete network-level ISAC systems, namely coordinated cellular network-based and distributed antenna-based ISAC systems, are investigated from interference management perspective.","Experiment results indicate that it is beneficial to collaboratively employ different interference mitigation techniques and leverage the network structure to achieve the full potential of network-level ISAC.","Finally, we highlight several promising future research directions for the design of ISAC systems."],"url":"http://arxiv.org/abs/2402.09974v1"}
{"created":"2024-02-15 14:29:21","title":"TSTEM: A Cognitive Platform for Collecting Cyber Threat Intelligence in the Wild","abstract":"The extraction of cyber threat intelligence (CTI) from open sources is a rapidly expanding defensive strategy that enhances the resilience of both Information Technology (IT) and Operational Technology (OT) environments against large-scale cyber-attacks. While previous research has focused on improving individual components of the extraction process, the community lacks open-source platforms for deploying streaming CTI data pipelines in the wild. To address this gap, the study describes the implementation of an efficient and well-performing platform capable of processing compute-intensive data pipelines based on the cloud computing paradigm for real-time detection, collecting, and sharing CTI from different online sources. We developed a prototype platform (TSTEM), a containerized microservice architecture that uses Tweepy, Scrapy, Terraform, ELK, Kafka, and MLOps to autonomously search, extract, and index IOCs in the wild. Moreover, the provisioning, monitoring, and management of the TSTEM platform are achieved through infrastructure as a code (IaC). Custom focus crawlers collect web content, which is then processed by a first-level classifier to identify potential indicators of compromise (IOCs). If deemed relevant, the content advances to a second level of extraction for further examination. Throughout this process, state-of-the-art NLP models are utilized for classification and entity extraction, enhancing the overall IOC extraction methodology. Our experimental results indicate that these models exhibit high accuracy (exceeding 98%) in the classification and extraction tasks, achieving this performance within a time frame of less than a minute. The effectiveness of our system can be attributed to a finely-tuned IOC extraction method that operates at multiple stages, ensuring precise identification of relevant information with low false positives.","sentences":["The extraction of cyber threat intelligence (CTI) from open sources is a rapidly expanding defensive strategy that enhances the resilience of both Information Technology (IT) and Operational Technology (OT) environments against large-scale cyber-attacks.","While previous research has focused on improving individual components of the extraction process, the community lacks open-source platforms for deploying streaming CTI data pipelines in the wild.","To address this gap, the study describes the implementation of an efficient and well-performing platform capable of processing compute-intensive data pipelines based on the cloud computing paradigm for real-time detection, collecting, and sharing CTI from different online sources.","We developed a prototype platform (TSTEM), a containerized microservice architecture that uses Tweepy, Scrapy, Terraform, ELK, Kafka, and MLOps to autonomously search, extract, and index IOCs in the wild.","Moreover, the provisioning, monitoring, and management of the TSTEM platform are achieved through infrastructure as a code (IaC).","Custom focus crawlers collect web content, which is then processed by a first-level classifier to identify potential indicators of compromise (IOCs).","If deemed relevant, the content advances to a second level of extraction for further examination.","Throughout this process, state-of-the-art NLP models are utilized for classification and entity extraction, enhancing the overall IOC extraction methodology.","Our experimental results indicate that these models exhibit high accuracy (exceeding 98%) in the classification and extraction tasks, achieving this performance within a time frame of less than a minute.","The effectiveness of our system can be attributed to a finely-tuned IOC extraction method that operates at multiple stages, ensuring precise identification of relevant information with low false positives."],"url":"http://arxiv.org/abs/2402.09973v1"}
{"created":"2024-02-15 14:28:01","title":"Parameterized Vertex Integrity Revisited","abstract":"Vertex integrity is a graph parameter that measures the connectivity of a graph. Informally, its meaning is that a graph has small vertex integrity if it has a small separator whose removal disconnects the graph into connected components which are themselves also small. Graphs with low vertex integrity are extremely structured; this renders many hard problems tractable and has recently attracted interest in this notion from the parameterized complexity community. In this paper we revisit the NP-complete problem of computing the vertex integrity of a given graph from the point of view of structural parameterizations. We present a number of new results, which also answer some recently posed open questions from the literature. Specifically: We show that unweighted vertex integrity is W[1]-hard parameterized by treedepth; we show that the problem remains W[1]-hard if we parameterize by feedback edge set size (via a reduction from a Bin Packing variant which may be of independent interest); and complementing this we show that the problem is FPT by max-leaf number. Furthermore, for weighted vertex integrity, we show that the problem admits a single-exponential FPT algorithm parameterized by vertex cover or by modular width, the latter result improving upon a previous algorithm which required weights to be polynomially bounded.","sentences":["Vertex integrity is a graph parameter that measures the connectivity of a graph.","Informally, its meaning is that a graph has small vertex integrity if it has a small separator whose removal disconnects the graph into connected components which are themselves also small.","Graphs with low vertex integrity are extremely structured; this renders many hard problems tractable and has recently attracted interest in this notion from the parameterized complexity community.","In this paper we revisit the NP-complete problem of computing the vertex integrity of a given graph from the point of view of structural parameterizations.","We present a number of new results, which also answer some recently posed open questions from the literature.","Specifically: We show that unweighted vertex integrity is W[1]-hard parameterized by treedepth; we show that the problem remains W[1]-hard if we parameterize by feedback edge set size (via a reduction from a Bin Packing variant which may be of independent interest); and complementing this we show that the problem is FPT by max-leaf number.","Furthermore, for weighted vertex integrity, we show that the problem admits a single-exponential FPT algorithm parameterized by vertex cover or by modular width, the latter result improving upon a previous algorithm which required weights to be polynomially bounded."],"url":"http://arxiv.org/abs/2402.09971v1"}
{"created":"2024-02-15 14:16:59","title":"ViGEO: an Assessment of Vision GNNs in Earth Observation","abstract":"Satellite missions and Earth Observation (EO) systems represent fundamental assets for environmental monitoring and the timely identification of catastrophic events, long-term monitoring of both natural resources and human-made assets, such as vegetation, water bodies, forests as well as buildings. Different EO missions enables the collection of information on several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2. Thus, given the recent advances of machine learning, computer vision and the availability of labeled data, researchers demonstrated the feasibility and the precision of land-use monitoring systems and remote sensing image classification through the use of deep neural networks. Such systems may help domain experts and governments in constant environmental monitoring, enabling timely intervention in case of catastrophic events (e.g., forest wildfire in a remote area). Despite the recent advances in the field of computer vision, many works limit their analysis on Convolutional Neural Networks (CNNs) and, more recently, to vision transformers (ViTs). Given the recent successes of Graph Neural Networks (GNNs) on non-graph data, such as time-series and images, we investigate the performances of a recent Vision GNN architecture (ViG) applied to the task of land cover classification. The experimental results show that ViG achieves state-of-the-art performances in multiclass and multilabel classification contexts, surpassing both ViT and ResNet on large-scale benchmarks.","sentences":["Satellite missions and Earth Observation (EO) systems represent fundamental assets for environmental monitoring and the timely identification of catastrophic events, long-term monitoring of both natural resources and human-made assets, such as vegetation, water bodies, forests as well as buildings.","Different EO missions enables the collection of information on several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2.","Thus, given the recent advances of machine learning, computer vision and the availability of labeled data, researchers demonstrated the feasibility and the precision of land-use monitoring systems and remote sensing image classification through the use of deep neural networks.","Such systems may help domain experts and governments in constant environmental monitoring, enabling timely intervention in case of catastrophic events (e.g., forest wildfire in a remote area).","Despite the recent advances in the field of computer vision, many works limit their analysis on Convolutional Neural Networks (CNNs) and, more recently, to vision transformers (ViTs).","Given the recent successes of Graph Neural Networks (GNNs) on non-graph data, such as time-series and images, we investigate the performances of a recent Vision GNN architecture (ViG) applied to the task of land cover classification.","The experimental results show that ViG achieves state-of-the-art performances in multiclass and multilabel classification contexts, surpassing both ViT and ResNet on large-scale benchmarks."],"url":"http://arxiv.org/abs/2402.09962v1"}
{"created":"2024-02-15 14:09:28","title":"LLM-based Federated Recommendation","abstract":"Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   To address these challenges, we introduce a Privacy-Preserving LLM-based Recommendation (PPLR) framework. The PPLR framework employs two primary strategies. First, it implements a dynamic balance strategy, which involves the design of dynamic parameter aggregation and adjustment of learning speed for different clients during the training phase, to ensure relatively balanced performance across all clients. Second, PPLR adopts a flexible storage strategy, selectively retaining certain sensitive layers of the language model on the client side while offloading non-sensitive layers to the server. This approach aims to preserve user privacy while efficiently saving computational and storage resources. Experimental results demonstrate that PPLR not only achieves a balanced performance among clients but also enhances overall system performance in a manner that is both computationally and storage-efficient, while effectively protecting user privacy.","sentences":["Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods.","However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information.","The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues.","To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach.","Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   ","To address these challenges, we introduce a Privacy-Preserving LLM-based Recommendation (PPLR) framework.","The PPLR framework employs two primary strategies.","First, it implements a dynamic balance strategy, which involves the design of dynamic parameter aggregation and adjustment of learning speed for different clients during the training phase, to ensure relatively balanced performance across all clients.","Second, PPLR adopts a flexible storage strategy, selectively retaining certain sensitive layers of the language model on the client side while offloading non-sensitive layers to the server.","This approach aims to preserve user privacy while efficiently saving computational and storage resources.","Experimental results demonstrate that PPLR not only achieves a balanced performance among clients but also enhances overall system performance in a manner that is both computationally and storage-efficient, while effectively protecting user privacy."],"url":"http://arxiv.org/abs/2402.09959v1"}
{"created":"2024-02-15 14:08:08","title":"On Designing Features for Condition Monitoring of Rotating Machines","abstract":"Various methods for designing input features have been proposed for fault recognition in rotating machines using one-dimensional raw sensor data. The available methods are complex, rely on empirical approaches, and may differ depending on the condition monitoring data used. Therefore, this article proposes a novel algorithm to design input features that unifies the feature extraction process for different time-series sensor data. This new insight for designing/extracting input features is obtained through the lens of histogram theory. The proposed algorithm extracts discriminative input features, which are suitable for a simple classifier to deep neural network-based classifiers. The designed input features are given as input to the classifier with end-to-end training in a single framework for machine conditions recognition. The proposed scheme has been validated through three real-time datasets: a) acoustic dataset, b) CWRU vibration dataset, and c) IMS vibration dataset. The real-time results and comparative study show the effectiveness of the proposed scheme for the prediction of the machine's health states.","sentences":["Various methods for designing input features have been proposed for fault recognition in rotating machines using one-dimensional raw sensor data.","The available methods are complex, rely on empirical approaches, and may differ depending on the condition monitoring data used.","Therefore, this article proposes a novel algorithm to design input features that unifies the feature extraction process for different time-series sensor data.","This new insight for designing/extracting input features is obtained through the lens of histogram theory.","The proposed algorithm extracts discriminative input features, which are suitable for a simple classifier to deep neural network-based classifiers.","The designed input features are given as input to the classifier with end-to-end training in a single framework for machine conditions recognition.","The proposed scheme has been validated through three real-time datasets: a) acoustic dataset, b) CWRU vibration dataset, and c) IMS vibration dataset.","The real-time results and comparative study show the effectiveness of the proposed scheme for the prediction of the machine's health states."],"url":"http://arxiv.org/abs/2402.09957v1"}
{"created":"2024-02-15 13:52:23","title":"Multi-Word Tokenization for Sequence Compression","abstract":"Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.","sentences":["Large Language Models have proven highly successful at modelling a variety of tasks.","However, this comes at a steep computational cost that hinders wider industrial uptake.","In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens.","MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance.","Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation."],"url":"http://arxiv.org/abs/2402.09949v1"}
{"created":"2024-02-15 13:41:23","title":"FedLion: Faster Adaptive Federated Optimization with Fewer Communication","abstract":"In Federated Learning (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training. To address this challenge, we introduce FedLion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into the FL framework. Through comprehensive evaluations on two widely adopted FL benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs. Last but not least, this work also includes a novel theoretical analysis, showcasing that FedLion attains faster convergence rate than established FL algorithms like FedAvg.","sentences":["In Federated Learning (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training.","To address this challenge, we introduce FedLion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al.","2o23)",", into the FL framework.","Through comprehensive evaluations on two widely adopted FL benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA.","Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs.","Last but not least, this work also includes a novel theoretical analysis, showcasing that FedLion attains faster convergence rate than established FL algorithms like FedAvg."],"url":"http://arxiv.org/abs/2402.09941v1"}
{"created":"2024-02-15 13:39:55","title":"Generative AI in the Construction Industry: A State-of-the-art Analysis","abstract":"The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents. The results show that retrieval augmented generation (RAG) improves the baseline LLM by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of generative AI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.","sentences":["The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance.","Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges.","However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry.","This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents.","The results show that retrieval augmented generation (RAG) improves the baseline LLM by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility.","This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of generative AI techniques to enhance productivity, quality, safety, and sustainability across the construction industry."],"url":"http://arxiv.org/abs/2402.09939v1"}
{"created":"2024-02-15 13:11:07","title":"Characterizing Role Models in Software Practitioners' Career: An Interview Study","abstract":"A role model is a person who serves as an example for others to follow, especially in terms of values, behavior, achievements, and personal characteristics. In this paper, authors study how role models influence software practitioners careers, an aspect not studied in the literature before. By means of this study, authors aim to understand if there are any salient role model archetypes and what characteristics are valued by participants in their role models. To do so, authors use a thematic coding approach to analyze the data collected from interviewing ten Latin American software practitioners. Findings reveal that role models were perceived as sources of knowledge, yet the majority of participants, regardless of their career stage, displayed a stronger interest in the human side and the moral values that their role models embodied. This study also shows that any practitioner can be viewed as a role model.","sentences":["A role model is a person who serves as an example for others to follow, especially in terms of values, behavior, achievements, and personal characteristics.","In this paper, authors study how role models influence software practitioners careers, an aspect not studied in the literature before.","By means of this study, authors aim to understand if there are any salient role model archetypes and what characteristics are valued by participants in their role models.","To do so, authors use a thematic coding approach to analyze the data collected from interviewing ten Latin American software practitioners.","Findings reveal that role models were perceived as sources of knowledge, yet the majority of participants, regardless of their career stage, displayed a stronger interest in the human side and the moral values that their role models embodied.","This study also shows that any practitioner can be viewed as a role model."],"url":"http://arxiv.org/abs/2402.09925v1"}
{"created":"2024-02-15 12:53:25","title":"Road Graph Generator: Mapping roads at construction sites from GPS data","abstract":"We present a method for road inference from GPS trajectories to map construction sites. This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads. Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a graph, which subsequently can be used for planning and task-allocation. We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway.","sentences":["We present a method for road inference from GPS trajectories to map construction sites.","This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads.","Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a graph, which subsequently can be used for planning and task-allocation.","We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway."],"url":"http://arxiv.org/abs/2402.09919v1"}
{"created":"2024-02-15 12:39:57","title":"BUSTER: a \"BUSiness Transaction Entity Recognition\" dataset","abstract":"Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists of 3779 manually annotated documents on financial transactions. We establish several baselines exploiting both general-purpose and domain-specific language models. The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER.","sentences":["Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging.","One of the reasons resides in the displacement between popular benchmarks and actual data.","Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health.","To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset.","The dataset consists of 3779 manually annotated documents on financial transactions.","We establish several baselines exploiting both general-purpose and domain-specific language models.","The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER."],"url":"http://arxiv.org/abs/2402.09916v1"}
{"created":"2024-02-15 12:17:15","title":"DE-COP: Detecting Copyrighted Content in Language Models Training Data","abstract":"How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $\\approx$ 4% accuracy. Our code and datasets are available at https://github.com/avduarte333/DE-COP_Method","sentences":["How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed?","We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text.","We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training.","DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases.","We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases.","Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available.","Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $\\approx$ 4% accuracy.","Our code and datasets are available at https://github.com/avduarte333/DE-COP_Method"],"url":"http://arxiv.org/abs/2402.09910v1"}
{"created":"2024-02-15 12:12:19","title":"Generative Representational Instruction Tuning","abstract":"All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.","sentences":["All text-based language problems can be reduced to either generation or embedding.","Current models only perform well at one or the other.","We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions.","Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks.","By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models.","Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss.","Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models.","Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm."],"url":"http://arxiv.org/abs/2402.09906v1"}
{"created":"2024-02-15 11:45:34","title":"COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web Application for Classifying COVID-19 Discussions","abstract":"The COVID-19 pandemic has had adverse effects on both physical and mental health. During this pandemic, numerous studies have focused on gaining insights into health-related perspectives from social media. In this study, our primary objective is to develop a machine learning-based web application for automatically classifying COVID-19-related discussions on social media. To achieve this, we label COVID-19-related Twitter data, provide benchmark classification results, and develop a web application. We collected data using the Twitter API and labeled a total of 6,667 tweets into five different classes: health risks, prevention, symptoms, transmission, and treatment. We extracted features using various feature extraction methods and applied them to seven different traditional machine learning algorithms, including Decision Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest Neighbour, Logistic Regression, and Linear SVC. Additionally, we used four deep learning algorithms: LSTM, CNN, RNN, and BERT, for classification. Overall, we achieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning. The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing other traditional machine learning approaches. Our study not only contributes to the field of health-related data analysis but also provides a valuable resource in the form of a web-based tool for efficient data classification, which can aid in addressing public health challenges and increasing awareness during pandemics. We made the dataset and application publicly available, which can be downloaded from this link https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website.","sentences":["The COVID-19 pandemic has had adverse effects on both physical and mental health.","During this pandemic, numerous studies have focused on gaining insights into health-related perspectives from social media.","In this study, our primary objective is to develop a machine learning-based web application for automatically classifying COVID-19-related discussions on social media.","To achieve this, we label COVID-19-related Twitter data, provide benchmark classification results, and develop a web application.","We collected data using the Twitter API and labeled a total of 6,667 tweets into five different classes: health risks, prevention, symptoms, transmission, and treatment.","We extracted features using various feature extraction methods and applied them to seven different traditional machine learning algorithms, including Decision Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest Neighbour, Logistic Regression, and Linear SVC.","Additionally, we used four deep learning algorithms: LSTM, CNN, RNN, and BERT, for classification.","Overall, we achieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning.","The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing other traditional machine learning approaches.","Our study not only contributes to the field of health-related data analysis but also provides a valuable resource in the form of a web-based tool for efficient data classification, which can aid in addressing public health challenges and increasing awareness during pandemics.","We made the dataset and application publicly available, which can be downloaded from this link https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website."],"url":"http://arxiv.org/abs/2402.09897v1"}
{"created":"2024-02-15 11:08:23","title":"Explaining Kernel Clustering via Decision Trees","abstract":"Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpretable model.","sentences":["Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods.","Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees.","However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data.","In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means.","We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpretable model."],"url":"http://arxiv.org/abs/2402.09881v1"}
{"created":"2024-02-15 10:58:22","title":"Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks","abstract":"Adversarial attacks represent a substantial challenge in Natural Language Processing (NLP). This study undertakes a systematic exploration of this challenge in two distinct phases: vulnerability evaluation and resilience enhancement of Transformer-based models under adversarial attacks.   In the evaluation phase, we assess the susceptibility of three Transformer configurations, encoder-decoder, encoder-only, and decoder-only setups, to adversarial attacks of escalating complexity across datasets containing offensive language and misinformation. Encoder-only models manifest a 14% and 21% performance drop in offensive language detection and misinformation detection tasks, respectively. Decoder-only models register a 16% decrease in both tasks, while encoder-decoder models exhibit a maximum performance drop of 14% and 26% in the respective tasks.   The resilience-enhancement phase employs adversarial training, integrating pre-camouflaged and dynamically altered data. This approach effectively reduces the performance drop in encoder-only models to an average of 5% in offensive language detection and 2% in misinformation detection tasks. Decoder-only models, occasionally exceeding original performance, limit the performance drop to 7% and 2% in the respective tasks. Although not surpassing the original performance, Encoder-decoder models can reduce the drop to an average of 6% and 2% respectively.   Results suggest a trade-off between performance and robustness, with some models maintaining similar performance while gaining robustness. Our study and adversarial training techniques have been incorporated into an open-source tool for generating camouflaged datasets. However, methodology effectiveness depends on the specific camouflage technique and data encountered, emphasizing the need for continued exploration.","sentences":["Adversarial attacks represent a substantial challenge in Natural Language Processing (NLP).","This study undertakes a systematic exploration of this challenge in two distinct phases: vulnerability evaluation and resilience enhancement of Transformer-based models under adversarial attacks.   ","In the evaluation phase, we assess the susceptibility of three Transformer configurations, encoder-decoder, encoder-only, and decoder-only setups, to adversarial attacks of escalating complexity across datasets containing offensive language and misinformation.","Encoder-only models manifest a 14% and 21% performance drop in offensive language detection and misinformation detection tasks, respectively.","Decoder-only models register a 16% decrease in both tasks, while encoder-decoder models exhibit a maximum performance drop of 14% and 26% in the respective tasks.   ","The resilience-enhancement phase employs adversarial training, integrating pre-camouflaged and dynamically altered data.","This approach effectively reduces the performance drop in encoder-only models to an average of 5% in offensive language detection and 2% in misinformation detection tasks.","Decoder-only models, occasionally exceeding original performance, limit the performance drop to 7% and 2% in the respective tasks.","Although not surpassing the original performance, Encoder-decoder models can reduce the drop to an average of 6% and 2% respectively.   ","Results suggest a trade-off between performance and robustness, with some models maintaining similar performance while gaining robustness.","Our study and adversarial training techniques have been incorporated into an open-source tool for generating camouflaged datasets.","However, methodology effectiveness depends on the specific camouflage technique and data encountered, emphasizing the need for continued exploration."],"url":"http://arxiv.org/abs/2402.09874v1"}
{"created":"2024-02-15 10:55:01","title":"MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music","abstract":"The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset with multi-dimensional, high-precision music annotations, the Caichong Music Dataset (CaiMD), and carefully selected 1,000 high-quality entries to serve as the test set for MuChin. Based on MuChin, we analyzed the discrepancies between professionals and amateurs in terms of music description, and empirically demonstrated the effectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed MuChin to evaluate existing music understanding models on their ability to provide colloquial descriptions of music. All data related to the benchmark and the code for scoring have been open-sourced.","sentences":["The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music.","However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks.","To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music.","We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics.","Utilizing this method, we built a dataset with multi-dimensional, high-precision music annotations, the Caichong Music Dataset (CaiMD), and carefully selected 1,000 high-quality entries to serve as the test set for MuChin.","Based on MuChin, we analyzed the discrepancies between professionals and amateurs in terms of music description, and empirically demonstrated the effectiveness of annotated data for fine-tuning LLMs.","Ultimately, we employed MuChin to evaluate existing music understanding models on their ability to provide colloquial descriptions of music.","All data related to the benchmark and the code for scoring have been open-sourced."],"url":"http://arxiv.org/abs/2402.09871v1"}
{"created":"2024-02-15 10:47:44","title":"Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking","abstract":"Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.","sentences":["Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation.","However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns.","To address these limitations, we propose two innovative data-driven filtering methods.","Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy.","Moreover, it dispenses with most domain-specific design choices characteristic of the KF.","The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise.","Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods.","Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to.","We also conduct noise robustness analysis of our filters with convincing positive results.","We further propose a new cost function for associating observations with tracks.","Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets."],"url":"http://arxiv.org/abs/2402.09865v1"}
{"created":"2024-02-15 10:28:41","title":"From Exploration to End of Life: Unpacking Sustainability in Physicalization Practices","abstract":"Data physicalizations have gained prominence across domains, but their environmental impact has been largely overlooked. This work addresses this gap by investigating the interplay between sustainability and physicalization practices. We conducted interviews with experts from diverse backgrounds, followed by a survey to gather insights into how they approach physicalization projects and reflect on sustainability. Our thematic analysis revealed sustainability considerations throughout the entire physicalization life cycle -- a framework that encompasses various stages in a physicalization's existence. Notably, we found no single agreed-upon definition for sustainable physicalizations, highlighting the complexity of integrating sustainability into physicalization practices. We outline sustainability challenges and strategies based on participants' experiences and propose the Sustainable Physicalization Practices (SuPPra) Matrix, providing a structured approach for designers to reflect on and enhance the environmental impact of their future physicalizations.","sentences":["Data physicalizations have gained prominence across domains, but their environmental impact has been largely overlooked.","This work addresses this gap by investigating the interplay between sustainability and physicalization practices.","We conducted interviews with experts from diverse backgrounds, followed by a survey to gather insights into how they approach physicalization projects and reflect on sustainability.","Our thematic analysis revealed sustainability considerations throughout the entire physicalization life cycle -- a framework that encompasses various stages in a physicalization's existence.","Notably, we found no single agreed-upon definition for sustainable physicalizations, highlighting the complexity of integrating sustainability into physicalization practices.","We outline sustainability challenges and strategies based on participants' experiences and propose the Sustainable Physicalization Practices (SuPPra) Matrix, providing a structured approach for designers to reflect on and enhance the environmental impact of their future physicalizations."],"url":"http://arxiv.org/abs/2402.09860v1"}
{"created":"2024-02-15 10:01:55","title":"Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent","abstract":"The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see https://huggingface.co/jat-project/jat), including a pioneering general-purpose dataset.","sentences":["The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research.","The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model.","In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types.","The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights.","The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see https://huggingface.co/jat-project/jat), including a pioneering general-purpose dataset."],"url":"http://arxiv.org/abs/2402.09844v1"}
{"created":"2024-02-15 10:00:49","title":"LAPDoc: Layout-Aware Prompting for Documents","abstract":"Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experiments we investigate the effects on the commercial ChatGPT model and the open-source LLM Solar. We demonstrate that using our approach both LLMs show improved performance on various standard document benchmarks. In addition, we study the impact of noisy OCR and layout errors, as well as the limitations of LLMs when it comes to utilizing document layout. Our results indicate that layout enrichment can improve the performance of purely text-based LLMs for document understanding by up to 15% compared to just using plain document text. In conclusion, this approach should be considered for the best model choice between text-based LLM or multi-modal document transformers.","sentences":["Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks.","Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout.","This involves a separate fine-tuning step for which additional training data is required.","At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks.","In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment.","We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information.","In our experiments we investigate the effects on the commercial ChatGPT model and the open-source LLM Solar.","We demonstrate that using our approach both LLMs show improved performance on various standard document benchmarks.","In addition, we study the impact of noisy OCR and layout errors, as well as the limitations of LLMs when it comes to utilizing document layout.","Our results indicate that layout enrichment can improve the performance of purely text-based LLMs for document understanding by up to 15% compared to just using plain document text.","In conclusion, this approach should be considered for the best model choice between text-based LLM or multi-modal document transformers."],"url":"http://arxiv.org/abs/2402.09841v1"}
{"created":"2024-02-15 09:58:23","title":"Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models","abstract":"Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem. In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour. Specifically, we design a context-aware chain-of-thoughts prompting technique to align LLMs with context-aware mobility behaviour by few-shot in-context learning. Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit the synergistic effect between LLM reasoning and mechanistic gravity model. It leverages the step-by-step LLM reasoning to recursively generate a temporal template of activity intentions, which are then mapped to physical locations with a mechanistic gravity model. Experiments on two real-world datasets show MobiGeaR achieves state-of-the-art performance across all metrics, and substantially reduces the size of training samples at the same time. Besides, MobiGeaR also significantly improves the semantic-awareness of mobility generation by improving the intention accuracy by 62.23% and the generated mobility data is proven effective in boosting the performance of downstream applications. The implementation of our approach is available in the paper.","sentences":["Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control.","However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models.","Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions.","They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness.","Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem.","In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour.","Specifically, we design a context-aware chain-of-thoughts prompting technique to align LLMs with context-aware mobility behaviour by few-shot in-context learning.","Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit the synergistic effect between LLM reasoning and mechanistic gravity model.","It leverages the step-by-step LLM reasoning to recursively generate a temporal template of activity intentions, which are then mapped to physical locations with a mechanistic gravity model.","Experiments on two real-world datasets show MobiGeaR achieves state-of-the-art performance across all metrics, and substantially reduces the size of training samples at the same time.","Besides, MobiGeaR also significantly improves the semantic-awareness of mobility generation by improving the intention accuracy by 62.23% and the generated mobility data is proven effective in boosting the performance of downstream applications.","The implementation of our approach is available in the paper."],"url":"http://arxiv.org/abs/2402.09836v1"}
{"created":"2024-02-15 09:55:51","title":"Parameterized Algorithms for Steiner Forest in Bounded Width Graphs","abstract":"In this paper we reassess the parameterized complexity and approximability of the well-studied Steiner Forest problem in several graph classes of bounded width. The problem takes an edge-weighted graph and pairs of vertices as input, and the aim is to find a minimum cost subgraph in which each given vertex pair lies in the same connected component. It is known that this problem is APX-hard in general, and NP-hard on graphs of treewidth 3, treedepth 4, and feedback vertex set size 2. However, Bateni, Hajiaghayi and Marx [JACM, 2011] gave an approximation scheme with a runtime of $n^{O(\\frac{k^2}{\\varepsilon})}$ on graphs of treewidth $k$. Our main result is a much faster efficient parameterized approximation scheme (EPAS) with a runtime of $2^{O(\\frac{k^2}{\\varepsilon} \\log \\frac{k^2}{\\varepsilon})} \\cdot n^{O(1)}$. If $k$ instead is the vertex cover number of the input graph, we show how to compute the optimum solution in $2^{O(k \\log k)} \\cdot n^{O(1)}$ time, and we also prove that this runtime dependence on $k$ is asymptotically best possible, under ETH. Furthermore, if $k$ is the size of a feedback edge set, then we obtain a faster $2^{O(k)} \\cdot n^{O(1)}$ time algorithm, which again cannot be improved under ETH.","sentences":["In this paper we reassess the parameterized complexity and approximability of the well-studied Steiner Forest problem in several graph classes of bounded width.","The problem takes an edge-weighted graph and pairs of vertices as input, and the aim is to find a minimum cost subgraph in which each given vertex pair lies in the same connected component.","It is known that this problem is APX-hard in general, and NP-hard on graphs of treewidth 3, treedepth 4, and feedback vertex set size 2.","However, Bateni, Hajiaghayi and Marx","[JACM, 2011] gave an approximation scheme with a runtime of $n^{O(\\frac{k^2}{\\varepsilon})}$ on graphs of treewidth $k$.","Our main result is a much faster efficient parameterized approximation scheme (EPAS) with a runtime of $2^{O(\\frac{k^2}{\\varepsilon} \\log \\frac{k^2}{\\varepsilon})}","\\cdot n^{O(1)}$.","If $k$ instead is the vertex cover number of the input graph, we show how to compute the optimum solution in $2^{O(k \\log k)} \\cdot n^{O(1)}$ time, and we also prove that this runtime dependence on $k$ is asymptotically best possible, under ETH.","Furthermore, if $k$ is the size of a feedback edge set, then we obtain a faster $2^{O(k)} \\cdot n^{O(1)}$ time algorithm, which again cannot be improved under ETH."],"url":"http://arxiv.org/abs/2402.09835v1"}
{"created":"2024-02-15 09:55:39","title":"All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining","abstract":"Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.","sentences":["Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP).","One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'.","This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions.","Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'.","However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer.","This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources.","In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning.","Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks.","Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach.","By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model."],"url":"http://arxiv.org/abs/2402.09834v1"}
{"created":"2024-02-15 09:48:20","title":"Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data","abstract":"Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions. This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods. GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection. The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets. And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real. The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative Adversarial network (GANs) algorithm to enhance the security of the transaction process.The study demonstrates the potential of GANs in enhancing transaction security through deep learning techniques.","sentences":["Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions.","This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods.","GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection.","The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets.","And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real.","The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative Adversarial network (GANs) algorithm to enhance the security of the transaction process.","The study demonstrates the potential of GANs in enhancing transaction security through deep learning techniques."],"url":"http://arxiv.org/abs/2402.09830v1"}
{"created":"2024-02-15 09:45:54","title":"Validation of homogenized finite element models of human metastatic vertebrae using digital volume correlation","abstract":"The incidence of vertebral fragility fracture is increased by the presence of preexisting pathologies such as metastatic disease. Computational tools could support the fracture prediction and consequently the decision of the best medical treatment. Anyway, validation is required to use these tools in clinical practice. To address this necessity, in this study subject-specific homogenized finite element models of single vertebrae were generated from micro CT images for both healthy and metastatic vertebrae and validated against experimental data. More in detail, spine segments were tested under compression and imaged with micro CT. The displacements field could be extracted for each vertebra singularly using the digital volume correlation full-field technique. Homogenized finite element models of each vertebra could hence be built from the micro CT images, applying boundary conditions consistent with the experimental displacements at the endplates. Numerical and experimental displacements and strains fields were eventually compared. In addition, the outcomes of a micro CT based homogenized model were compared to the ones of a clinical-CT based model. Good agreement between experimental and computational displacement fields, both for healthy and metastatic vertebrae, was found. Comparison between micro CT based and clinical-CT based outcomes showed strong correlations. Furthermore, models were able to qualitatively identify the regions which experimentally showed the highest strain concentration. In conclusion, the combination of experimental full-field technique and the in-silico modelling allowed the development of a promising pipeline for validation of fracture risk predictors, although further improvements in both fields are needed to better analyse quantitatively the post-yield behaviour of the vertebra.","sentences":["The incidence of vertebral fragility fracture is increased by the presence of preexisting pathologies such as metastatic disease.","Computational tools could support the fracture prediction and consequently the decision of the best medical treatment.","Anyway, validation is required to use these tools in clinical practice.","To address this necessity, in this study subject-specific homogenized finite element models of single vertebrae were generated from micro CT images for both healthy and metastatic vertebrae and validated against experimental data.","More in detail, spine segments were tested under compression and imaged with micro CT.","The displacements field could be extracted for each vertebra singularly using the digital volume correlation full-field technique.","Homogenized finite element models of each vertebra could hence be built from the micro CT images, applying boundary conditions consistent with the experimental displacements at the endplates.","Numerical and experimental displacements and strains fields were eventually compared.","In addition, the outcomes of a micro CT based homogenized model were compared to the ones of a clinical-CT based model.","Good agreement between experimental and computational displacement fields, both for healthy and metastatic vertebrae, was found.","Comparison between micro CT based and clinical-CT based outcomes showed strong correlations.","Furthermore, models were able to qualitatively identify the regions which experimentally showed the highest strain concentration.","In conclusion, the combination of experimental full-field technique and the in-silico modelling allowed the development of a promising pipeline for validation of fracture risk predictors, although further improvements in both fields are needed to better analyse quantitatively the post-yield behaviour of the vertebra."],"url":"http://arxiv.org/abs/2402.09828v1"}
{"created":"2024-02-15 09:35:57","title":"Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection","abstract":"In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the original model to decline. With this in mind, this paper uses deep learning for advanced threat detection to improve cybersecurity resilienc e in the financial industry. Many network security researchers have shifted their focus to exceptio n-based intrusion detection techniques. The detection technology mainly uses statistical machine learning methods - collecting normal program and network behavior data, extracting multidimensional features, and training decision machine learning models on this basis (commonly used include naive Bayes, decision trees, support vector machines, random forests, etc.). In the detection phase, program code or network behavior that deviates from the normal value beyond the tolerance is considered malicious code or network attack behavior.","sentences":["In the age of the Internet, people's lives are increasingly dependent on today's network technology.","However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges.","Maintaining network security and protecting the legitimate interests of users is at the heart of network construction.","Threat detection is an important part of a complete and effective defense system.","In the field of network information security, the technical update of network attack and network protection is spiraling.","How to effectively detect unknown threats is one of the concerns of network protection.","Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the original model to decline.","With this in mind, this paper uses deep learning for advanced threat detection to improve cybersecurity resilienc e in the financial industry.","Many network security researchers have shifted their focus to exceptio n-based intrusion detection techniques.","The detection technology mainly uses statistical machine learning methods - collecting normal program and network behavior data, extracting multidimensional features, and training decision machine learning models on this basis (commonly used include naive Bayes, decision trees, support vector machines, random forests, etc.).","In the detection phase, program code or network behavior that deviates from the normal value beyond the tolerance is considered malicious code or network attack behavior."],"url":"http://arxiv.org/abs/2402.09820v1"}
{"created":"2024-02-15 09:18:18","title":"TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming","abstract":"Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON","sentences":["Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection.","However, their performance relies heavily on the availability and quality of training data.","There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts.","This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages.","Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text.","Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available.","Manual annotation of such data requires a lot of time, effort, and expertise.","In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework.","One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches.","TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages.","We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data.","Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text.","Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON"],"url":"http://arxiv.org/abs/2402.09811v1"}
{"created":"2024-02-15 09:15:09","title":"Effective and Scalable Math Support: Evidence on the Impact of an AI- Tutor on Math Achievement in Ghana","abstract":"This study evaluates the impact of Rori, an AI powered conversational math tutor accessible via WhatsApp, on the math performance of approximately 1,000 students in grades 3-9 across 11 schools in Ghana. Each school was assigned to a treatment group or control group; the students in the control group continued their regular math instruction, while students in the treatment group engaged with Rori, for two 30-minute sessions per week over 8 months in addition to regular math instruction. We find that the math growth scores were substantially higher for the treatment group with an effect size of 0.37, and that the results were statistically significant (p < 0.001). The fact that Rori works with basic mobile devices on low-bandwidth data networks gives the intervention strong potential to support personalized learning on other low-and-middle-income countries (LMICs), where laptop ownership and high-speed internet - prerequisite for many video-centered learning platforms - remain extremely limited. While the results should be interpreted judiciously, as they only report on year 1 of the intervention, and future research is necessary to better understand which conditions are necessary for successful implementation, they do suggest that chat-based tutoring solutions leveraging artificial intelligence could offer a costeffective approach to enhancing learning outcomes for millions of students globally.","sentences":["This study evaluates the impact of Rori, an AI powered conversational math tutor accessible via WhatsApp, on the math performance of approximately 1,000 students in grades 3-9 across 11 schools in Ghana.","Each school was assigned to a treatment group or control group; the students in the control group continued their regular math instruction, while students in the treatment group engaged with Rori, for two 30-minute sessions per week over 8 months in addition to regular math instruction.","We find that the math growth scores were substantially higher for the treatment group with an effect size of 0.37, and that the results were statistically significant (p < 0.001).","The fact that Rori works with basic mobile devices on low-bandwidth data networks gives the intervention strong potential to support personalized learning on other low-and-middle-income countries (LMICs), where laptop ownership and high-speed internet - prerequisite for many video-centered learning platforms - remain extremely limited.","While the results should be interpreted judiciously, as they only report on year 1 of the intervention, and future research is necessary to better understand which conditions are necessary for successful implementation, they do suggest that chat-based tutoring solutions leveraging artificial intelligence could offer a costeffective approach to enhancing learning outcomes for millions of students globally."],"url":"http://arxiv.org/abs/2402.09809v1"}
{"created":"2024-02-15 09:06:25","title":"Enabling Edge processing on LoRaWAN architecture","abstract":"LoRaWAN is a wireless technology that enables high-density deployments of IoT devices. Designed for Low Power Wide Area Networks (LPWAN), LoRaWAN employs large cells to service a potentially extremely high number of devices. The technology enforces a centralized architecture, directing all data generated by the devices to a single network server for data processing. End-to-end encryption is used to guarantee the confidentiality and security of data. In this demo, we present \\edgelora, a system architecture designed to incorporate edge processing in LoRaWAN without compromising security and confidentiality of data. \\edgelora maintains backward compatibility and addresses scalability issues arising from handling large amounts of data sourced from a diverse range of devices. The demo provides evidence on the advantages in terms of reduced latency, lower network bandwidth requirements, higher scalability, and improved security and privacy resulting from the application of the Edge processing paradigm to LoRaWAN.","sentences":["LoRaWAN is a wireless technology that enables high-density deployments of IoT devices.","Designed for Low Power Wide Area Networks (LPWAN), LoRaWAN employs large cells to service a potentially extremely high number of devices.","The technology enforces a centralized architecture, directing all data generated by the devices to a single network server for data processing.","End-to-end encryption is used to guarantee the confidentiality and security of data.","In this demo, we present \\edgelora, a system architecture designed to incorporate edge processing in LoRaWAN without compromising security and confidentiality of data.","\\edgelora maintains backward compatibility and addresses scalability issues arising from handling large amounts of data sourced from a diverse range of devices.","The demo provides evidence on the advantages in terms of reduced latency, lower network bandwidth requirements, higher scalability, and improved security and privacy resulting from the application of the Edge processing paradigm to LoRaWAN."],"url":"http://arxiv.org/abs/2402.09805v1"}
{"created":"2024-02-15 08:58:03","title":"EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models","abstract":"Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.","sentences":["Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination.","To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text.","However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms.","To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data.","Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead.","Our code and datasets will be publicly available."],"url":"http://arxiv.org/abs/2402.09801v1"}
{"created":"2024-02-15 08:52:31","title":"A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings","abstract":"In this work, we propose a novel cross-talk rejection framework for a multi-channel multi-talker setup for a live multiparty interactive show. Our far-field audio setup is required to be hands-free during live interaction and comprises four adjacent talkers with directional microphones in the same space. Such setups often introduce heavy cross-talk between channels, resulting in reduced automatic speech recognition (ASR) and natural language understanding (NLU) performance. To address this problem, we propose voice activity detection (VAD) model for all talkers using multichannel information, which is then used to filter audio for downstream tasks. We adopt a synthetic training data generation approach through playback and re-recording for such scenarios, simulating challenging speech overlap conditions. We train our models on this synthetic data and demonstrate that our approach outperforms single-channel VAD models and energy-based multi-channel VAD algorithm in various acoustic environments. In addition to VAD results, we also present multiparty ASR evaluation results to highlight the impact of using our VAD model for filtering audio in downstream tasks by significantly reducing the insertion error.","sentences":["In this work, we propose a novel cross-talk rejection framework for a multi-channel multi-talker setup for a live multiparty interactive show.","Our far-field audio setup is required to be hands-free during live interaction and comprises four adjacent talkers with directional microphones in the same space.","Such setups often introduce heavy cross-talk between channels, resulting in reduced automatic speech recognition (ASR) and natural language understanding (NLU) performance.","To address this problem, we propose voice activity detection (VAD) model for all talkers using multichannel information, which is then used to filter audio for downstream tasks.","We adopt a synthetic training data generation approach through playback and re-recording for such scenarios, simulating challenging speech overlap conditions.","We train our models on this synthetic data and demonstrate that our approach outperforms single-channel VAD models and energy-based multi-channel VAD algorithm in various acoustic environments.","In addition to VAD results, we also present multiparty ASR evaluation results to highlight the impact of using our VAD model for filtering audio in downstream tasks by significantly reducing the insertion error."],"url":"http://arxiv.org/abs/2402.09797v1"}
{"created":"2024-02-15 08:50:36","title":"An advanced data fabric architecture leveraging homomorphic encryption and federated learning","abstract":"Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems. In a Federated learning architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning. This paper introduces a secure approach for medical image analysis using federated learning and partially homomorphic encryption within a distributed data fabric architecture. With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features. The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data. The study demonstrates the method's effectiveness through a case study on pituitary tumor classification, achieving a significant level of accuracy. However, the primary focus of the study is on the development and evaluation of federated learning and partially homomorphic encryption as tools for secure medical image analysis. The results highlight the potential of these techniques to be applied to other privacy-sensitive domains and contribute to the growing body of research on secure and privacy-preserving machine learning.","sentences":["Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems.","In a Federated learning architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning.","This paper introduces a secure approach for medical image analysis using federated learning and partially homomorphic encryption within a distributed data fabric architecture.","With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features.","The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data.","The study demonstrates the method's effectiveness through a case study on pituitary tumor classification, achieving a significant level of accuracy.","However, the primary focus of the study is on the development and evaluation of federated learning and partially homomorphic encryption as tools for secure medical image analysis.","The results highlight the potential of these techniques to be applied to other privacy-sensitive domains and contribute to the growing body of research on secure and privacy-preserving machine learning."],"url":"http://arxiv.org/abs/2402.09795v1"}
{"created":"2024-02-15 08:43:17","title":"Multi-vertebral CT-based FE models implementing linear isotropic population-based material properties for the intervertebral discs cannot accurately predict strains","abstract":"Vertebral fractures prediction in clinics lacks of accuracy. The most used scores have limitations in distinguishing between subjects at risk or not. Finite element (FE) models generated from computed tomography (CT) of these patients may improve the predictive capability. Many models have already been proposed but the most of them considered the single vertebral body, excluding from the analysis the role of the inter-vertebral discs in the distribution of the load through the spine. Multi-vertebral models instead allow to examine more complex boundary condition. However, CT scans do not provide subject-specif information about the material properties of the disc. Consequently, the goal of the study was to validate a multi-vertebral FE model with subject specific modelling of the vertebral bone and population-based properties assigned to the disc, idealizing them with a linear isotropic material. Boundary condition were assigned in order to reproduce an experimental test performed on the same specimen and recorded using digital image correlation technique (DIC). FE and DIC strains on the vertebral surfaces are compared point-wise. Young's modulus values in the range 25-30 MPa allowed to achieve a comparable order of magnitude between experimental and computational data. However, the two distribution remained strongly different. To conclude, subject-specific material properties need to be assigned also to the discs as well as to the vertebrae to achieve acceptable accuracy in the assessment of the fracture risk.","sentences":["Vertebral fractures prediction in clinics lacks of accuracy.","The most used scores have limitations in distinguishing between subjects at risk or not.","Finite element (FE) models generated from computed tomography (CT) of these patients may improve the predictive capability.","Many models have already been proposed but the most of them considered the single vertebral body, excluding from the analysis the role of the inter-vertebral discs in the distribution of the load through the spine.","Multi-vertebral models instead allow to examine more complex boundary condition.","However, CT scans do not provide subject-specif information about the material properties of the disc.","Consequently, the goal of the study was to validate a multi-vertebral FE model with subject specific modelling of the vertebral bone and population-based properties assigned to the disc, idealizing them with a linear isotropic material.","Boundary condition were assigned in order to reproduce an experimental test performed on the same specimen and recorded using digital image correlation technique (DIC).","FE and DIC strains on the vertebral surfaces are compared point-wise.","Young's modulus values in the range 25-30 MPa allowed to achieve a comparable order of magnitude between experimental and computational data.","However, the two distribution remained strongly different.","To conclude, subject-specific material properties need to be assigned also to the discs as well as to the vertebrae to achieve acceptable accuracy in the assessment of the fracture risk."],"url":"http://arxiv.org/abs/2402.09790v1"}
{"created":"2024-02-15 08:21:50","title":"MC-DBN: A Deep Belief Network-Based Model for Modality Completion","abstract":"Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate monitoring domains. Comprehensive experiments showcase the model's capacity to bridge the semantic divide present in multi-modal data, subsequently enhancing its performance. The source code is available at: https://github.com/logan-0623/DBN-generate","sentences":["Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring.","Utilizing diverse data sources can substantially improve prediction accuracy.","Nonetheless, additional data may not always align with the original dataset.","Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information.","Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN).","This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data.","It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model.","We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate monitoring domains.","Comprehensive experiments showcase the model's capacity to bridge the semantic divide present in multi-modal data, subsequently enhancing its performance.","The source code is available at: https://github.com/logan-0623/DBN-generate"],"url":"http://arxiv.org/abs/2402.09782v1"}
{"created":"2024-02-15 08:10:09","title":"A Comprehensive Review on Computer Vision Analysis of Aerial Data","abstract":"With the emergence of new technologies in the field of airborne platforms and imaging sensors, aerial data analysis is becoming very popular, capitalizing on its advantages over land data. This paper presents a comprehensive review of the computer vision tasks within the domain of aerial data analysis. While addressing fundamental aspects such as object detection and tracking, the primary focus is on pivotal tasks like change detection, object segmentation, and scene-level analysis. The paper provides the comparison of various hyper parameters employed across diverse architectures and tasks. A substantial section is dedicated to an in-depth discussion on libraries, their categorization, and their relevance to different domain expertise. The paper encompasses aerial datasets, the architectural nuances adopted, and the evaluation metrics associated with all the tasks in aerial data analysis. Applications of computer vision tasks in aerial data across different domains are explored, with case studies providing further insights. The paper thoroughly examines the challenges inherent in aerial data analysis, offering practical solutions. Additionally, unresolved issues of significance are identified, paving the way for future research directions in the field of aerial data analysis.","sentences":["With the emergence of new technologies in the field of airborne platforms and imaging sensors, aerial data analysis is becoming very popular, capitalizing on its advantages over land data.","This paper presents a comprehensive review of the computer vision tasks within the domain of aerial data analysis.","While addressing fundamental aspects such as object detection and tracking, the primary focus is on pivotal tasks like change detection, object segmentation, and scene-level analysis.","The paper provides the comparison of various hyper parameters employed across diverse architectures and tasks.","A substantial section is dedicated to an in-depth discussion on libraries, their categorization, and their relevance to different domain expertise.","The paper encompasses aerial datasets, the architectural nuances adopted, and the evaluation metrics associated with all the tasks in aerial data analysis.","Applications of computer vision tasks in aerial data across different domains are explored, with case studies providing further insights.","The paper thoroughly examines the challenges inherent in aerial data analysis, offering practical solutions.","Additionally, unresolved issues of significance are identified, paving the way for future research directions in the field of aerial data analysis."],"url":"http://arxiv.org/abs/2402.09781v1"}
{"created":"2024-02-15 08:03:12","title":"NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models","abstract":"The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach allows us to leverage numerous teachers with varying capacities to progressively guide the pruned model, enhancing overall performance. Extensive experiments across various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity.","sentences":["The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware.","Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization.","In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models.","Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students.","However, it becomes challenging in the context of LLMs due to memory constraints.","To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune).","NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles.","This approach allows us to leverage numerous teachers with varying capacities to progressively guide the pruned model, enhancing overall performance.","Extensive experiments across various tasks demonstrate the effectiveness of NutePrune.","In LLaMA-7B zero-shot experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity."],"url":"http://arxiv.org/abs/2402.09773v1"}
{"created":"2024-02-15 07:41:53","title":"Reeb Complements for Exploring Inclusions Between Isosurfaces From Two Scalar Fields","abstract":"This article proposes to integrate two Reeb graphs with the information of their isosurfaces' inclusion relation. As computing power evolves, there arise numerical data that have small-scale physics inside larger ones -- for example, small clouds in a simulation can be contained inside an atmospheric layer, which is further contained in an enormous hurricane. Extracting such inclusions between isosurfaces is a challenge for isosurfacing: the user would have to explore the vast combinations of isosurfaces $(f_1^{-1}(l_1), f_2^{-1}(l_2))$ from scalar fields $f_i: M \\to \\mathbb{R}$, $i = 1, 2$, where $M$ is a domain manifold and $f_i$ are physical quantities, to find inclusion of one isosurface within another. For this, we propose the \\textit{Reeb complement}, a topological space that integrates two Reeb graphs with the inclusion relation. The Reeb complement has a natural partition that classifies equivalent containment of isosurfaces. This is a handy characteristic to let the Reeb complement serve as an overview of the inclusion relationship in the data. We also propose level-of-detail control of the inclusions through simplification of the Reeb complement.","sentences":["This article proposes to integrate two Reeb graphs with the information of their isosurfaces' inclusion relation.","As computing power evolves, there arise numerical data that have small-scale physics inside larger ones -- for example, small clouds in a simulation can be contained inside an atmospheric layer, which is further contained in an enormous hurricane.","Extracting such inclusions between isosurfaces is a challenge for isosurfacing: the user would have to explore the vast combinations of isosurfaces $(f_1^{-1}(l_1), f_2^{-1}(l_2))$ from scalar fields $f_i: M \\to \\mathbb{R}$, $i = 1, 2$, where $M$ is a domain manifold and $f_i$ are physical quantities, to find inclusion of one isosurface within another.","For this, we propose the \\textit{Reeb complement}, a topological space that integrates two Reeb graphs with the inclusion relation.","The Reeb complement has a natural partition that classifies equivalent containment of isosurfaces.","This is a handy characteristic to let the Reeb complement serve as an overview of the inclusion relationship in the data.","We also propose level-of-detail control of the inclusions through simplification of the Reeb complement."],"url":"http://arxiv.org/abs/2402.09768v1"}
{"created":"2024-02-15 07:00:06","title":"Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming","abstract":"Recently, the potential of large language models (LLMs) has been widely used in assisting programming. However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration. Our work probes the reflection type of artists in the creation process with such collaboration. We compare two common collaboration approaches: invoking the entire program and multiple subtasks. Our findings exhibit artists' different stimulated reflections in two different methods. Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews. In this sense, our work reveals the artistic potential of LLM in creative coding. Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design suggestions for future work of AI-assisted creative tasks.","sentences":["Recently, the potential of large language models (LLMs) has been widely used in assisting programming.","However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration.","Our work probes the reflection type of artists in the creation process with such collaboration.","We compare two common collaboration approaches: invoking the entire program and multiple subtasks.","Our findings exhibit artists' different stimulated reflections in two different methods.","Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews.","In this sense, our work reveals the artistic potential of LLM in creative coding.","Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design suggestions for future work of AI-assisted creative tasks."],"url":"http://arxiv.org/abs/2402.09750v1"}
{"created":"2024-02-15 06:36:07","title":"QuRating: Selecting High-Quality Data for Training Language Models","abstract":"Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity, as selecting only the highest-rated documents leads to poor results. When we sample using quality ratings as logits over documents, our models achieve lower perplexity and stronger in-context learning performance than baselines. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.","sentences":["Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics.","We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive.","In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value.","We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly.","We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria.","In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data.","We find that it is important to balance quality and diversity, as selecting only the highest-rated documents leads to poor results.","When we sample using quality ratings as logits over documents, our models achieve lower perplexity and stronger in-context learning performance than baselines.","Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset.","We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."],"url":"http://arxiv.org/abs/2402.09739v1"}
{"created":"2024-02-15 06:26:10","title":"Federated Analytics-Empowered Frequent Pattern Mining for Decentralized Web 3.0 Applications","abstract":"The emerging Web 3.0 paradigm aims to decentralize existing web services, enabling desirable properties such as transparency, incentives, and privacy preservation. However, current Web 3.0 applications supported by blockchain infrastructure still cannot support complex data analytics tasks in a scalable and privacy-preserving way. This paper introduces the emerging federated analytics (FA) paradigm into the realm of Web 3.0 services, enabling data to stay local while still contributing to complex web analytics tasks in a privacy-preserving way. We propose FedWeb, a tailored FA design for important frequent pattern mining tasks in Web 3.0. FedWeb remarkably reduces the number of required participating data owners to support privacy-preserving Web 3.0 data analytics based on a novel distributed differential privacy technique. The correctness of mining results is guaranteed by a theoretically rigid candidate filtering scheme based on Hoeffding's inequality and Chebychev's inequality. Two response budget saving solutions are proposed to further reduce participating data owners. Experiments on three representative Web 3.0 scenarios show that FedWeb can improve data utility by ~25.3% and reduce the participating data owners by ~98.4%.","sentences":["The emerging Web 3.0 paradigm aims to decentralize existing web services, enabling desirable properties such as transparency, incentives, and privacy preservation.","However, current Web 3.0 applications supported by blockchain infrastructure still cannot support complex data analytics tasks in a scalable and privacy-preserving way.","This paper introduces the emerging federated analytics (FA) paradigm into the realm of Web 3.0 services, enabling data to stay local while still contributing to complex web analytics tasks in a privacy-preserving way.","We propose FedWeb, a tailored FA design for important frequent pattern mining tasks in Web 3.0.","FedWeb remarkably reduces the number of required participating data owners to support privacy-preserving Web 3.0 data analytics based on a novel distributed differential privacy technique.","The correctness of mining results is guaranteed by a theoretically rigid candidate filtering scheme based on Hoeffding's inequality and Chebychev's inequality.","Two response budget saving solutions are proposed to further reduce participating data owners.","Experiments on three representative Web 3.0 scenarios show that FedWeb can improve data utility by ~25.3% and reduce the participating data owners by ~98.4%."],"url":"http://arxiv.org/abs/2402.09736v1"}
{"created":"2024-02-15 05:56:35","title":"Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System","abstract":"This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system. We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system's latency, user attention levels, and preferred resolutions. Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a reinforcement learning problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers. To learn the generalized policy, we propose a framework that employs federated learning (FL) and prompt-based sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT. Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training. The design of prompts integrating user-environment cues and user-preferred allocation improves the model's adaptability to various user environments during online execution.","sentences":["This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system.","We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system's latency, user attention levels, and preferred resolutions.","Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a reinforcement learning problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers.","To learn the generalized policy, we propose a framework that employs federated learning (FL) and prompt-based sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT.","Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training.","The design of prompts integrating user-environment cues and user-preferred allocation improves the model's adaptability to various user environments during online execution."],"url":"http://arxiv.org/abs/2402.09729v1"}
{"created":"2024-02-15 05:35:04","title":"Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization","abstract":"Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores compared to the base models, respectively, and our CMLMC-EECR achieves the best performance with a comparable translation quality with the Transformer. The experiments results demonstrate the effectiveness of our method.","sentences":["Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens.","However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases.","In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR).","We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions.","We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference.","The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores compared to the base models, respectively, and our CMLMC-EECR achieves the best performance with a comparable translation quality with the Transformer.","The experiments results demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.09725v1"}
{"created":"2024-02-15 05:19:53","title":"DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service","abstract":"Federated learning (FL) has emerged as a prevalent distributed machine learning scheme that enables collaborative model training without aggregating raw data. Cloud service providers further embrace Federated Learning as a Service (FLaaS), allowing data analysts to execute their FL training pipelines over differentially-protected data. Due to the intrinsic properties of differential privacy, the enforced privacy level on data blocks can be viewed as a privacy budget that requires careful scheduling to cater to diverse training pipelines. Existing privacy budget scheduling studies prioritize either efficiency or fairness individually. In this paper, we propose DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes both efficiency and fairness. We first develop a comprehensive utility function incorporating data analyst-level dominant shares and FL-specific performance metrics. A sequential allocation mechanism is then designed using the Lagrange multiplier method and effective greedy heuristics. We theoretically prove that DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and Weak Strategy Proofness. We also theoretically prove the existence of a fairness-efficiency tradeoff in privacy budgeting. Extensive experiments demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an average efficiency improvement of $1.44\\times \\sim 3.49 \\times$, and an average fairness improvement of $1.37\\times \\sim 24.32 \\times$.","sentences":["Federated learning (FL) has emerged as a prevalent distributed machine learning scheme that enables collaborative model training without aggregating raw data.","Cloud service providers further embrace Federated Learning as a Service (FLaaS), allowing data analysts to execute their FL training pipelines over differentially-protected data.","Due to the intrinsic properties of differential privacy, the enforced privacy level on data blocks can be viewed as a privacy budget that requires careful scheduling to cater to diverse training pipelines.","Existing privacy budget scheduling studies prioritize either efficiency or fairness individually.","In this paper, we propose DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes both efficiency and fairness.","We first develop a comprehensive utility function incorporating data analyst-level dominant shares and FL-specific performance metrics.","A sequential allocation mechanism is then designed using the Lagrange multiplier method and effective greedy heuristics.","We theoretically prove that DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and Weak Strategy Proofness.","We also theoretically prove the existence of a fairness-efficiency tradeoff in privacy budgeting.","Extensive experiments demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an average efficiency improvement of $1.44\\times \\sim 3.49 \\times$, and an average fairness improvement of $1.37\\times \\sim 24.32 \\times$."],"url":"http://arxiv.org/abs/2402.09715v1"}
{"created":"2024-02-15 05:07:54","title":"Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement","abstract":"Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model. This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs. We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding.","sentences":["Disentangled representation learning strives to extract the intrinsic factors within observed data.","Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs.","In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations.","We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion.","Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs.","We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model.","This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs.","We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding."],"url":"http://arxiv.org/abs/2402.09712v1"}
{"created":"2024-02-15 05:06:53","title":"Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks","abstract":"Deep learning offers a promising solution to improve spectrum access techniques by utilizing data-driven approaches to manage and share limited spectrum resources for emerging applications. For several of these applications, the sensitive wireless data (such as spectrograms) are stored in a shared database or multistakeholder cloud environment and are therefore prone to privacy leaks. This paper aims to address such privacy concerns by examining the representative case study of shared database scenarios in 5G Open Radio Access Network (O-RAN) networks where we have a shared database within the near-real-time (near-RT) RAN intelligent controller. We focus on securing the data that can be used by machine learning (ML) models for spectrum sharing and interference mitigation applications without compromising the model and network performances. The underlying idea is to leverage a (i) Shuffling-based learnable encryption technique to encrypt the data, following which, (ii) employ a custom Vision transformer (ViT) as the trained ML model that is capable of performing accurate inferences on such encrypted data. The paper offers a thorough analysis and comparisons with analogous convolutional neural networks (CNN) as well as deeper architectures (such as ResNet-50) as baselines. Our experiments showcase that the proposed approach significantly outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the percent accuracy and F1-Score respectively when operated on encrypted data. Though deeper ResNet-50 architecture is obtained as a slightly more accurate model, with an increase of 4.4%, the proposed approach boasts a reduction of parameters by 99.32%, and thus, offers a much-improved prediction time by nearly 60%.","sentences":["Deep learning offers a promising solution to improve spectrum access techniques by utilizing data-driven approaches to manage and share limited spectrum resources for emerging applications.","For several of these applications, the sensitive wireless data (such as spectrograms) are stored in a shared database or multistakeholder cloud environment and are therefore prone to privacy leaks.","This paper aims to address such privacy concerns by examining the representative case study of shared database scenarios in 5G Open Radio Access Network (O-RAN) networks where we have a shared database within the near-real-time (near-RT) RAN intelligent controller.","We focus on securing the data that can be used by machine learning (ML) models for spectrum sharing and interference mitigation applications without compromising the model and network performances.","The underlying idea is to leverage a (i) Shuffling-based learnable encryption technique to encrypt the data, following which, (ii) employ a custom Vision transformer (ViT) as the trained ML model that is capable of performing accurate inferences on such encrypted data.","The paper offers a thorough analysis and comparisons with analogous convolutional neural networks (CNN) as well as deeper architectures (such as ResNet-50) as baselines.","Our experiments showcase that the proposed approach significantly outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the percent accuracy and F1-Score respectively when operated on encrypted data.","Though deeper ResNet-50 architecture is obtained as a slightly more accurate model, with an increase of 4.4%, the proposed approach boasts a reduction of parameters by 99.32%, and thus, offers a much-improved prediction time by nearly 60%."],"url":"http://arxiv.org/abs/2402.09710v1"}
{"created":"2024-02-15 04:58:35","title":"On the adversarial robustness of Locality-Sensitive Hashing in Hamming space","abstract":"Locality-sensitive hashing~[Indyk,Motwani'98] is a classical data structure for approximate nearest neighbor search. It allows, after a close to linear time preprocessing of the input dataset, to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size. The resulting data structure is randomized and succeeds with high probability for every fixed query.   In many modern applications of nearest neighbor search the queries are chosen adaptively. In this paper, we study the robustness of the locality-sensitive hashing to adaptive queries in Hamming space. We present a simple adversary that can, under mild assumptions on the initial point set, provably find a query to the approximate near neighbor search data structure that the data structure fails on. Crucially, our adaptive algorithm finds the hard query exponentially faster than random sampling.","sentences":["Locality-sensitive hashing~[Indyk,Motwani'98] is a classical data structure for approximate nearest neighbor search.","It allows, after a close to linear time preprocessing of the input dataset, to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size.","The resulting data structure is randomized and succeeds with high probability for every fixed query.   ","In many modern applications of nearest neighbor search the queries are chosen adaptively.","In this paper, we study the robustness of the locality-sensitive hashing to adaptive queries in Hamming space.","We present a simple adversary that can, under mild assumptions on the initial point set, provably find a query to the approximate near neighbor search data structure that the data structure fails on.","Crucially, our adaptive algorithm finds the hard query exponentially faster than random sampling."],"url":"http://arxiv.org/abs/2402.09707v1"}
{"created":"2024-02-15 04:08:49","title":"Reward Poisoning Attack Against Offline Reinforcement Learning","abstract":"We study the problem of reward poisoning attacks against general offline reinforcement learning with deep neural networks for function approximation. We consider a black-box threat model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation. We propose an attack strategy called `policy contrast attack'. The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing. To the best of our knowledge, we propose the first black-box reward poisoning attack in the general offline RL setting. We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different kinds of learning datasets.","sentences":["We study the problem of reward poisoning attacks against general offline reinforcement learning with deep neural networks for function approximation.","We consider a black-box threat model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation.","We propose an attack strategy called `policy contrast attack'.","The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing.","To the best of our knowledge, we propose the first black-box reward poisoning attack in the general offline RL setting.","We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different kinds of learning datasets."],"url":"http://arxiv.org/abs/2402.09695v1"}
{"created":"2024-02-15 03:45:44","title":"Robust Learning-Augmented Dictionaries","abstract":"We present the first learning-augmented data structure for implementing dictionaries with optimal consistency and robustness. Our data structure, named RobustSL, is a skip list augmented by predictions of access frequencies of elements in a data sequence. With proper predictions, RobustSL has optimal consistency (achieves static optimality). At the same time, it maintains a logarithmic running time for each operation, ensuring optimal robustness, even if predictions are generated adversarially. Therefore, RobustSL has all the advantages of the recent learning-augmented data structures of Lin, Luo, and Woodruff (ICML 2022) and Cao et al. (arXiv 2023), while providing robustness guarantees that are absent in the previous work. Numerical experiments show that RobustSL outperforms alternative data structures using both synthetic and real datasets.","sentences":["We present the first learning-augmented data structure for implementing dictionaries with optimal consistency and robustness.","Our data structure, named RobustSL, is a skip list augmented by predictions of access frequencies of elements in a data sequence.","With proper predictions, RobustSL has optimal consistency (achieves static optimality).","At the same time, it maintains a logarithmic running time for each operation, ensuring optimal robustness, even if predictions are generated adversarially.","Therefore, RobustSL has all the advantages of the recent learning-augmented data structures of Lin, Luo, and Woodruff (ICML 2022) and Cao et al. (arXiv 2023), while providing robustness guarantees that are absent in the previous work.","Numerical experiments show that RobustSL outperforms alternative data structures using both synthetic and real datasets."],"url":"http://arxiv.org/abs/2402.09687v1"}
{"created":"2024-02-15 03:43:00","title":"Pheno-Robot: An Auto-Digital Modelling System for In-Situ Phenotyping in the Field","abstract":"Accurate reconstruction of plant models for phenotyping analysis is critical for optimising sustainable agricultural practices in precision agriculture. Traditional laboratory-based phenotyping, while valuable, falls short of understanding how plants grow under uncontrolled conditions. Robotic technologies offer a promising avenue for large-scale, direct phenotyping in real-world environments. This study explores the deployment of emerging robotics and digital technology in plant phenotyping to improve performance and efficiency. Three critical functional modules: environmental understanding, robotic motion planning, and in-situ phenotyping, are introduced to automate the entire process. Experimental results demonstrate the effectiveness of the system in agricultural environments. The pheno-robot system autonomously collects high-quality data by navigating around plants. In addition, the in-situ modelling model reconstructs high-quality plant models from the data collected by the robot. The developed robotic system shows high efficiency and robustness, demonstrating its potential to advance plant science in real-world agricultural environments.","sentences":["Accurate reconstruction of plant models for phenotyping analysis is critical for optimising sustainable agricultural practices in precision agriculture.","Traditional laboratory-based phenotyping, while valuable, falls short of understanding how plants grow under uncontrolled conditions.","Robotic technologies offer a promising avenue for large-scale, direct phenotyping in real-world environments.","This study explores the deployment of emerging robotics and digital technology in plant phenotyping to improve performance and efficiency.","Three critical functional modules: environmental understanding, robotic motion planning, and in-situ phenotyping, are introduced to automate the entire process.","Experimental results demonstrate the effectiveness of the system in agricultural environments.","The pheno-robot system autonomously collects high-quality data by navigating around plants.","In addition, the in-situ modelling model reconstructs high-quality plant models from the data collected by the robot.","The developed robotic system shows high efficiency and robustness, demonstrating its potential to advance plant science in real-world agricultural environments."],"url":"http://arxiv.org/abs/2402.09685v1"}
{"created":"2024-02-15 03:09:54","title":"Prompt-based Personalized Federated Learning for Medical Visual Question Answering","abstract":"We present a novel prompt-based personalized federated learning (pFL) method to address data heterogeneity and privacy concerns in traditional medical visual question answering (VQA) methods. Specifically, we regard medical datasets from different organs as clients and use pFL to train personalized transformer-based VQA models for each client. To address the high computational complexity of client-to-client communication in previous pFL methods, we propose a succinct information sharing system by introducing prompts that are small learnable parameters. In addition, the proposed method introduces a reliability parameter to prevent the negative effects of low performance and irrelevant clients. Finally, extensive evaluations on various heterogeneous medical datasets attest to the effectiveness of our proposed method.","sentences":["We present a novel prompt-based personalized federated learning (pFL) method to address data heterogeneity and privacy concerns in traditional medical visual question answering (VQA) methods.","Specifically, we regard medical datasets from different organs as clients and use pFL to train personalized transformer-based VQA models for each client.","To address the high computational complexity of client-to-client communication in previous pFL methods, we propose a succinct information sharing system by introducing prompts that are small learnable parameters.","In addition, the proposed method introduces a reliability parameter to prevent the negative effects of low performance and irrelevant clients.","Finally, extensive evaluations on various heterogeneous medical datasets attest to the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2402.09677v1"}
{"created":"2024-02-15 03:05:45","title":"HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network","abstract":"In data science, hypergraphs are natural models for data exhibiting multi-way relations, whereas graphs only capture pairwise. Nonetheless, many proposed hypergraph neural networks effectively reduce hypergraphs to undirected graphs via symmetrized matrix representations, potentially losing important information. We propose an alternative approach to hypergraph neural networks in which the hypergraph is represented as a non-reversible Markov chain. We use this Markov chain to construct a complex Hermitian Laplacian matrix - the magnetic Laplacian - which serves as the input to our proposed hypergraph neural network. We study HyperMagNet for the task of node classification, and demonstrate its effectiveness over graph-reduction based hypergraph neural networks.","sentences":["In data science, hypergraphs are natural models for data exhibiting multi-way relations, whereas graphs only capture pairwise.","Nonetheless, many proposed hypergraph neural networks effectively reduce hypergraphs to undirected graphs via symmetrized matrix representations, potentially losing important information.","We propose an alternative approach to hypergraph neural networks in which the hypergraph is represented as a non-reversible Markov chain.","We use this Markov chain to construct a complex Hermitian Laplacian matrix - the magnetic Laplacian - which serves as the input to our proposed hypergraph neural network.","We study HyperMagNet for the task of node classification, and demonstrate its effectiveness over graph-reduction based hypergraph neural networks."],"url":"http://arxiv.org/abs/2402.09676v1"}
{"created":"2024-02-15 02:27:57","title":"How to Train Data-Efficient LLMs","abstract":"The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster.","sentences":["The training of large language models (LLMs) is expensive.","In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption.","We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space.","Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example.","To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample.","In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories.","Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster."],"url":"http://arxiv.org/abs/2402.09668v1"}
{"created":"2024-02-15 02:27:51","title":"Unbalanced Random Matching Markets with Partial Preferences","abstract":"Properties of stable matchings in the popular random-matching-market model have been studied for over 50 years. In a random matching market, each agent has complete preferences drawn uniformly and independently at random. Wilson (1972), Knuth (1976) and Pittel (1989) proved that in balanced random matching markets, the proposers are matched to their $\\ln n$th choice on average. In this paper, we consider markets where agents have partial (truncated) preferences, that is, the proposers only rank their top $d$ partners. Despite the long history of the problem, the following fundamental question remained unanswered: \\emph{what is the smallest value of $d$ that results in a perfect stable matching with high probability?} In this paper, we answer this question exactly -- we prove that a degree of $\\ln^2 n$ is necessary and sufficient. That is, we show that if $d < (1-\\epsilon) \\ln^2 n$ then no stable matching is perfect and if $d > (1+ \\epsilon) \\ln^2 n$, then every stable matching is perfect with high probability. This settles a recent conjecture by Kanoria, Min and Qian (2021).   We generalize this threshold for unbalanced markets: we consider a matching market with $n$ agents on the shorter side and $n(\\alpha+1)$ agents on the longer side. We show that for markets with $\\alpha =o(1)$, the sharp threshold characterizing the existence of perfect stable matching occurs when $d$ is $\\ln n \\cdot \\ln \\left(\\frac{1 + \\alpha}{\\alpha + (1/n(\\alpha+1))} \\right)$.   Finally, we extend the line of work studying the effect of imbalance on the expected rank of the proposers (termed the ``stark effect of competition''). We establish the regime in unbalanced markets that forces this stark effect to take shape in markets with partial preferences.","sentences":["Properties of stable matchings in the popular random-matching-market model have been studied for over 50 years.","In a random matching market, each agent has complete preferences drawn uniformly and independently at random.","Wilson (1972), Knuth (1976) and Pittel (1989) proved that in balanced random matching markets, the proposers are matched to their $\\ln n$th choice on average.","In this paper, we consider markets where agents have partial (truncated) preferences, that is, the proposers only rank their top $d$ partners.","Despite the long history of the problem, the following fundamental question remained unanswered: \\emph{what is the smallest value of $d$ that results in a perfect stable matching with high probability?}","In this paper, we answer this question exactly -- we prove that a degree of $\\ln^2 n$ is necessary and sufficient.","That is, we show that if $d < (1-\\epsilon)","\\ln^2 n$ then no stable matching is perfect and if $d > (1+ \\epsilon)","\\ln^2 n$, then every stable matching is perfect with high probability.","This settles a recent conjecture by Kanoria, Min and Qian (2021).   ","We generalize this threshold for unbalanced markets: we consider a matching market with $n$ agents on the shorter side and $n(\\alpha+1)$ agents on the longer side.","We show that for markets with $\\alpha =o(1)$, the sharp threshold characterizing the existence of perfect stable matching occurs when $d$ is $\\ln n \\cdot \\ln \\left(\\frac{1 + \\alpha}{\\alpha + (1/n(\\alpha+1))} \\right)$.   Finally, we extend the line of work studying the effect of imbalance on the expected rank of the proposers (termed the ``stark effect of competition'').","We establish the regime in unbalanced markets that forces this stark effect to take shape in markets with partial preferences."],"url":"http://arxiv.org/abs/2402.09667v1"}
{"created":"2024-02-15 02:27:23","title":"EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph Completion","abstract":"Commonsense knowledge graph completion is a new challenge for commonsense knowledge graph construction and application. In contrast to factual knowledge graphs such as Freebase and YAGO, commonsense knowledge graphs (CSKGs; e.g., ConceptNet) utilize free-form text to represent named entities, short phrases, and events as their nodes. Such a loose structure results in large and sparse CSKGs, which makes the semantic understanding of these nodes more critical for learning rich commonsense knowledge graph embedding. While current methods leverage semantic similarities to increase the graph density, the semantic plausibility of the nodes and their relations are under-explored. Previous works adopt conceptual abstraction to improve the consistency of modeling (event) plausibility, but they are not scalable enough and still suffer from data sparsity. In this paper, we propose to adopt textual entailment to find implicit entailment relations between CSKG nodes, to effectively densify the subgraph connecting nodes within the same conceptual class, which indicates a similar level of plausibility. Each node in CSKG finds its top entailed nodes using a finetuned transformer over natural language inference (NLI) tasks, which sufficiently capture textual entailment signals. The entailment relation between these nodes are further utilized to: 1) build new connections between source triplets and entailed nodes to densify the sparse CSKGs; 2) enrich the generalization ability of node representations by comparing the node embeddings with a contrastive loss. Experiments on two standard CSKGs demonstrate that our proposed framework EntailE can improve the performance of CSKG completion tasks under both transductive and inductive settings.","sentences":["Commonsense knowledge graph completion is a new challenge for commonsense knowledge graph construction and application.","In contrast to factual knowledge graphs such as Freebase and YAGO, commonsense knowledge graphs (CSKGs; e.g., ConceptNet) utilize free-form text to represent named entities, short phrases, and events as their nodes.","Such a loose structure results in large and sparse CSKGs, which makes the semantic understanding of these nodes more critical for learning rich commonsense knowledge graph embedding.","While current methods leverage semantic similarities to increase the graph density, the semantic plausibility of the nodes and their relations are under-explored.","Previous works adopt conceptual abstraction to improve the consistency of modeling (event) plausibility, but they are not scalable enough and still suffer from data sparsity.","In this paper, we propose to adopt textual entailment to find implicit entailment relations between CSKG nodes, to effectively densify the subgraph connecting nodes within the same conceptual class, which indicates a similar level of plausibility.","Each node in CSKG finds its top entailed nodes using a finetuned transformer over natural language inference (NLI) tasks, which sufficiently capture textual entailment signals.","The entailment relation between these nodes are further utilized to: 1) build new connections between source triplets and entailed nodes to densify the sparse CSKGs; 2) enrich the generalization ability of node representations by comparing the node embeddings with a contrastive loss.","Experiments on two standard CSKGs demonstrate that our proposed framework EntailE can improve the performance of CSKG completion tasks under both transductive and inductive settings."],"url":"http://arxiv.org/abs/2402.09666v1"}
{"created":"2024-02-15 02:24:46","title":"CodeMind: A Framework to Challenge Large Language Models for Code Reasoning","abstract":"Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning.","sentences":["Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage.","As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs.","CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR).","The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize.","The third one evaluates the extent to which LLMs implement the specified expected behavior.","Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize.","However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls.","Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning."],"url":"http://arxiv.org/abs/2402.09664v1"}
{"created":"2024-02-15 02:06:06","title":"User Modeling and User Profiling: A Comprehensive Survey","abstract":"The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data structures. We also address the critical need for privacy-preserving techniques and the push towards explainability and fairness in user modeling approaches. By examining the definitions of core terminology, we aim to clarify ambiguities and foster a clearer understanding of the field by proposing two novel encyclopedic definitions of the main terms. Furthermore, we explore the application of user modeling in various domains, such as fake news detection, cybersecurity, and personalized education. This survey serves as a comprehensive resource for researchers and practitioners, offering insights into the evolution of user modeling and profiling and guiding the development of more personalized, ethical, and effective AI systems.","sentences":["The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences.","These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems.","This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research.","We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends.","Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data structures.","We also address the critical need for privacy-preserving techniques and the push towards explainability and fairness in user modeling approaches.","By examining the definitions of core terminology, we aim to clarify ambiguities and foster a clearer understanding of the field by proposing two novel encyclopedic definitions of the main terms.","Furthermore, we explore the application of user modeling in various domains, such as fake news detection, cybersecurity, and personalized education.","This survey serves as a comprehensive resource for researchers and practitioners, offering insights into the evolution of user modeling and profiling and guiding the development of more personalized, ethical, and effective AI systems."],"url":"http://arxiv.org/abs/2402.09660v1"}
{"created":"2024-02-15 01:25:19","title":"Foul prediction with estimated poses from soccer broadcast video","abstract":"Recent advances in computer vision have made significant progress in tracking and pose estimation of sports players. However, there have been fewer studies on behavior prediction with pose estimation in sports, in particular, the prediction of soccer fouls is challenging because of the smaller image size of each player and of difficulty in the usage of e.g., the ball and pose information. In our research, we introduce an innovative deep learning approach for anticipating soccer fouls. This method integrates video data, bounding box positions, image details, and pose information by curating a novel soccer foul dataset. Our model utilizes a combination of convolutional and recurrent neural networks (CNNs and RNNs) to effectively merge information from these four modalities. The experimental results show that our full model outperformed the ablated models, and all of the RNN modules, bounding box position and image, and estimated pose were useful for the foul prediction. Our findings have important implications for a deeper understanding of foul play in soccer and provide a valuable reference for future research and practice in this area.","sentences":["Recent advances in computer vision have made significant progress in tracking and pose estimation of sports players.","However, there have been fewer studies on behavior prediction with pose estimation in sports, in particular, the prediction of soccer fouls is challenging because of the smaller image size of each player and of difficulty in the usage of e.g., the ball and pose information.","In our research, we introduce an innovative deep learning approach for anticipating soccer fouls.","This method integrates video data, bounding box positions, image details, and pose information by curating a novel soccer foul dataset.","Our model utilizes a combination of convolutional and recurrent neural networks (CNNs and RNNs) to effectively merge information from these four modalities.","The experimental results show that our full model outperformed the ablated models, and all of the RNN modules, bounding box position and image, and estimated pose were useful for the foul prediction.","Our findings have important implications for a deeper understanding of foul play in soccer and provide a valuable reference for future research and practice in this area."],"url":"http://arxiv.org/abs/2402.09650v1"}
{"created":"2024-02-15 00:14:41","title":"Smart Information Exchange for Unsupervised Federated Learning via Reinforcement Learning","abstract":"One of the main challenges of decentralized machine learning paradigms such as Federated Learning (FL) is the presence of local non-i.i.d. datasets. Device-to-device transfers (D2D) between distributed devices has been shown to be an effective tool for dealing with this problem and robust to stragglers. In an unsupervised case, however, it is not obvious how data exchanges should take place due to the absence of labels. In this paper, we propose an approach to create an optimal graph for data transfer using Reinforcement Learning. The goal is to form links that will provide the most benefit considering the environment's constraints and improve convergence speed in an unsupervised FL environment. Numerical analysis shows the advantages in terms of convergence speed and straggler resilience of the proposed method to different available FL schemes and benchmark datasets.","sentences":["One of the main challenges of decentralized machine learning paradigms such as Federated Learning (FL) is the presence of local non-i.i.d. datasets.","Device-to-device transfers (D2D) between distributed devices has been shown to be an effective tool for dealing with this problem and robust to stragglers.","In an unsupervised case, however, it is not obvious how data exchanges should take place due to the absence of labels.","In this paper, we propose an approach to create an optimal graph for data transfer using Reinforcement Learning.","The goal is to form links that will provide the most benefit considering the environment's constraints and improve convergence speed in an unsupervised FL environment.","Numerical analysis shows the advantages in terms of convergence speed and straggler resilience of the proposed method to different available FL schemes and benchmark datasets."],"url":"http://arxiv.org/abs/2402.09629v1"}
{"created":"2024-02-14 23:40:36","title":"Schnorr Approval-Based Secure and Privacy-Preserving IoV Data Aggregation","abstract":"Secure and privacy-preserving data aggregation in the Internet of Vehicles (IoV) continues to be a focal point of interest in both the industry and academia. Aiming at tackling the challenges and solving the remaining limitations of existing works, this paper introduces a novel Schnorr approval-based IoV data aggregation framework based on a two-layered architecture. In this framework, a server can aggregate the IoV data from clusters without inferring the raw data, real identity and trajectories of vehicles. Notably, we avoid incorporating the widely-accepted techniques such as homomorphic encryption and digital pseudonym to avoid introducing high computation cost to vehicles. We propose a novel concept, data approval, based on the Schnorr signature scheme. With the approval, the fake data injection attack carried out by a cluster head can be defended against. The separation of liability is achieved as well. The evaluation shows that the framework is secure and lightweight for vehicles in terms of the computation and communication costs.","sentences":["Secure and privacy-preserving data aggregation in the Internet of Vehicles (IoV) continues to be a focal point of interest in both the industry and academia.","Aiming at tackling the challenges and solving the remaining limitations of existing works, this paper introduces a novel Schnorr approval-based IoV data aggregation framework based on a two-layered architecture.","In this framework, a server can aggregate the IoV data from clusters without inferring the raw data, real identity and trajectories of vehicles.","Notably, we avoid incorporating the widely-accepted techniques such as homomorphic encryption and digital pseudonym to avoid introducing high computation cost to vehicles.","We propose a novel concept, data approval, based on the Schnorr signature scheme.","With the approval, the fake data injection attack carried out by a cluster head can be defended against.","The separation of liability is achieved as well.","The evaluation shows that the framework is secure and lightweight for vehicles in terms of the computation and communication costs."],"url":"http://arxiv.org/abs/2402.09621v1"}
{"created":"2024-02-14 23:12:09","title":"LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations","abstract":"The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus on how to utilize existing LLMs for mining and understanding relationships in graph data, applying these techniques to recommendation tasks. We propose an innovative framework that combines the strong contextual representation capabilities of LLMs with the relationship extraction and analysis functions of GNNs for mining relationships in graph data. Specifically, we design a new prompt construction framework that integrates relational information of graph data into natural language expressions, aiding LLMs in more intuitively grasping the connectivity information within graph data. Additionally, we introduce graph relationship understanding and analysis functions into LLMs to enhance their focus on connectivity information in graph data. Our evaluation on real-world datasets demonstrates the framework's ability to understand connectivity information in graph data.","sentences":["The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains.","However, the potential of these models in mining relationships from graph data remains under-explored.","Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining.","Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks.","A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships.","This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis.","We focus on how to utilize existing LLMs for mining and understanding relationships in graph data, applying these techniques to recommendation tasks.","We propose an innovative framework that combines the strong contextual representation capabilities of LLMs with the relationship extraction and analysis functions of GNNs for mining relationships in graph data.","Specifically, we design a new prompt construction framework that integrates relational information of graph data into natural language expressions, aiding LLMs in more intuitively grasping the connectivity information within graph data.","Additionally, we introduce graph relationship understanding and analysis functions into LLMs to enhance their focus on connectivity information in graph data.","Our evaluation on real-world datasets demonstrates the framework's ability to understand connectivity information in graph data."],"url":"http://arxiv.org/abs/2402.09617v1"}
{"created":"2024-02-14 23:09:15","title":"API Pack: A Massive Multilingual Dataset for API Call Generation","abstract":"We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.","sentences":["We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities.","Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding.","Fine-tuning CodeLlama-13B","on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls.","Scaling to 100k examples improves generalization to new APIs not seen during training.","In addition, cross-lingual API call generation is achieved without needing extensive data per language.","The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url."],"url":"http://arxiv.org/abs/2402.09615v1"}
{"created":"2024-02-14 23:01:03","title":"Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP","abstract":"Foundations models are presented as generalists that often perform well over a myriad of tasks. Fine-tuning these models, even on limited data, provides an additional boost in task-specific performance but often at the cost of their wider generalization, an effect termed catastrophic forgetting. In this paper, we analyze the relation between task difficulty in the CLIP model and the performance of several simple parameter-efficient fine-tuning methods through the lens of domain generalization and catastrophic forgetting. We provide evidence that the silhouette score of the zero-shot image and text embeddings is a better measure of task difficulty than the average cosine similarity of correct image/label embeddings, and discuss observable relationships between task difficulty, fine-tuning method, domain generalization, and catastrophic forgetting. Additionally, the averaged results across tasks and performance measures demonstrate that a simplified method that trains only a subset of attention weights, which we call A-CLIP, yields a balance between domain generalization and catastrophic forgetting.","sentences":["Foundations models are presented as generalists that often perform well over a myriad of tasks.","Fine-tuning these models, even on limited data, provides an additional boost in task-specific performance but often at the cost of their wider generalization, an effect termed catastrophic forgetting.","In this paper, we analyze the relation between task difficulty in the CLIP model and the performance of several simple parameter-efficient fine-tuning methods through the lens of domain generalization and catastrophic forgetting.","We provide evidence that the silhouette score of the zero-shot image and text embeddings is a better measure of task difficulty than the average cosine similarity of correct image/label embeddings, and discuss observable relationships between task difficulty, fine-tuning method, domain generalization, and catastrophic forgetting.","Additionally, the averaged results across tasks and performance measures demonstrate that a simplified method that trains only a subset of attention weights, which we call A-CLIP, yields a balance between domain generalization and catastrophic forgetting."],"url":"http://arxiv.org/abs/2402.09613v1"}
{"created":"2024-02-14 22:57:03","title":"Towards Privacy-Aware Sign Language Translation at Scale","abstract":"A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT.","sentences":["A major impediment to the advancement of sign language translation (SLT) is data scarcity.","Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions.","Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for.","In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues.","We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset.","SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4.","Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT."],"url":"http://arxiv.org/abs/2402.09611v1"}
{"created":"2024-02-14 22:26:07","title":"Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation","abstract":"Test-time adaptation (TTA) refers to adapting a trained model to a new domain during testing. Existing TTA techniques rely on having multiple test images from the same domain, yet this may be impractical in real-world applications such as medical imaging, where data acquisition is expensive and imaging conditions vary frequently. Here, we approach such a task, of adapting a medical image segmentation model with only a single unlabeled test image. Most TTA approaches, which directly minimize the entropy of predictions, fail to improve performance significantly in this setting, in which we also observe the choice of batch normalization (BN) layer statistics to be a highly important yet unstable factor due to only having a single test domain example. To overcome this, we propose to instead \\textit{integrate} over predictions made with various estimates of target domain statistics between the training and test statistics, weighted based on their entropy statistics.","sentences":["Test-time adaptation (TTA) refers to adapting a trained model to a new domain during testing.","Existing TTA techniques rely on having multiple test images from the same domain, yet this may be impractical in real-world applications such as medical imaging, where data acquisition is expensive and imaging conditions vary frequently.","Here, we approach such a task, of adapting a medical image segmentation model with only a single unlabeled test image.","Most TTA approaches, which directly minimize the entropy of predictions, fail to improve performance significantly in this setting, in which we also observe the choice of batch normalization (BN) layer statistics to be a highly important yet unstable factor due to only having a single test domain example.","To overcome this, we propose to instead \\textit{integrate} over predictions made with various estimates of target domain statistics between the training and test statistics, weighted based on their entropy statistics."],"url":"http://arxiv.org/abs/2402.09604v1"}
{"created":"2024-02-14 22:15:37","title":"Low-Rank Graph Contrastive Learning for Node Classification","abstract":"Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification. However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies. In this work, we propose a novel and robust GNN encoder, Low-Rank Graph Contrastive Learning (LR-GCL). Our method performs transductive node classification in two steps. First, a low-rank GCL encoder named LR-GCL is trained by prototypical contrastive learning with low-rank regularization. Next, using the features produced by LR-GCL, a linear transductive classification algorithm is used to classify the unlabeled nodes in the graph. Our LR-GCL is inspired by the low frequency property of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. To the best of our knowledge, our theoretical result is among the first to theoretically demonstrate the advantage of low-rank learning in graph contrastive learning supported by strong empirical performance. Extensive experiments on public benchmarks demonstrate the superior performance of LR-GCL and the robustness of the learned node representations. The code of LR-GCL is available at \\url{https://anonymous.4open.science/r/Low-Rank_Graph_Contrastive_Learning-64A6/}.","sentences":["Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification.","However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies.","In this work, we propose a novel and robust GNN encoder, Low-Rank Graph Contrastive Learning (LR-GCL).","Our method performs transductive node classification in two steps.","First, a low-rank GCL encoder named LR-GCL is trained by prototypical contrastive learning with low-rank regularization.","Next, using the features produced by LR-GCL, a linear transductive classification algorithm is used to classify the unlabeled nodes in the graph.","Our LR-GCL is inspired by the low frequency property of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning.","To the best of our knowledge, our theoretical result is among the first to theoretically demonstrate the advantage of low-rank learning in graph contrastive learning supported by strong empirical performance.","Extensive experiments on public benchmarks demonstrate the superior performance of LR-GCL and the robustness of the learned node representations.","The code of LR-GCL is available at \\url{https://anonymous.4open.science/r/Low-Rank_Graph_Contrastive_Learning-64A6/}."],"url":"http://arxiv.org/abs/2402.09600v1"}
{"created":"2024-02-14 22:00:57","title":"Pulmonologists-Level lung cancer detection based on standard blood test results and smoking status using an explainable machine learning approach","abstract":"Lung cancer (LC) remains the primary cause of cancer-related mortality, largely due to late-stage diagnoses. Effective strategies for early detection are therefore of paramount importance. In recent years, machine learning (ML) has demonstrated considerable potential in healthcare by facilitating the detection of various diseases. In this retrospective development and validation study, we developed an ML model based on dynamic ensemble selection (DES) for LC detection. The model leverages standard blood sample analysis and smoking history data from a large population at risk in Denmark. The study includes all patients examined on suspicion of LC in the Region of Southern Denmark from 2009 to 2018. We validated and compared the predictions by the DES model with diagnoses provided by five pulmonologists. Among the 38,944 patients, 9,940 had complete data of which 2,505 (25\\%) had LC. The DES model achieved an area under the roc curve of 0.77$\\pm$0.01, sensitivity of 76.2\\%$\\pm$2.4\\%, specificity of 63.8\\%$\\pm$2.3\\%, positive predictive value of 41.6\\%$\\pm$1.2\\%, and F\\textsubscript{1}-score of 53.8\\%$\\pm$1.1\\%. The DES model outperformed all five pulmonologists, achieving a sensitivity 9\\% higher than their average. The model identified smoking status, age, total calcium levels, neutrophil count, and lactate dehydrogenase as the most important factors for the detection of LC. The results highlight the successful application of the ML approach in detecting LC, surpassing pulmonologists' performance. Incorporating clinical and laboratory data in future risk assessment models can improve decision-making and facilitate timely referrals.","sentences":["Lung cancer (LC) remains the primary cause of cancer-related mortality, largely due to late-stage diagnoses.","Effective strategies for early detection are therefore of paramount importance.","In recent years, machine learning (ML) has demonstrated considerable potential in healthcare by facilitating the detection of various diseases.","In this retrospective development and validation study, we developed an ML model based on dynamic ensemble selection (DES) for LC detection.","The model leverages standard blood sample analysis and smoking history data from a large population at risk in Denmark.","The study includes all patients examined on suspicion of LC in the Region of Southern Denmark from 2009 to 2018.","We validated and compared the predictions by the DES model with diagnoses provided by five pulmonologists.","Among the 38,944 patients, 9,940 had complete data of which 2,505 (25\\%) had LC.","The DES model achieved an area under the roc curve of 0.77$\\pm$0.01, sensitivity of 76.2\\%$\\pm$2.4\\%, specificity of 63.8\\%$\\pm$2.3\\%, positive predictive value of 41.6\\%$\\pm$1.2\\%, and F\\textsubscript{1}-score of 53.8\\%$\\pm$1.1\\%.","The DES model outperformed all five pulmonologists, achieving a sensitivity 9\\% higher than their average.","The model identified smoking status, age, total calcium levels, neutrophil count, and lactate dehydrogenase as the most important factors for the detection of LC.","The results highlight the successful application of the ML approach in detecting LC, surpassing pulmonologists' performance.","Incorporating clinical and laboratory data in future risk assessment models can improve decision-making and facilitate timely referrals."],"url":"http://arxiv.org/abs/2402.09596v1"}
{"created":"2024-02-14 21:37:59","title":"A Web-Based Tool for Automatic Data Collection, Curation, and Visualization of Complex Healthcare Survey Studies including Social Network Analysis","abstract":"There is a great concern nowadays regarding alcohol consumption and drug abuse, especially in young people. Analyzing the social environment where these adolescents are immersed, as well as a series of measures determining the alcohol abuse risk or personal situation and perception using a number of questionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain insight into the current situation of a given individual regarding his/her consumption behavior. But this analysis, in order to be achieved, requires the use of tools that can ease the process of questionnaire creation, data gathering, curation and representation, and later analysis and visualization to the user. This research presents the design and construction of a web-based platform able to facilitate each of the mentioned processes by integrating the different phases into an intuitive system with a graphical user interface that hides the complexity underlying each of the questionnaires and techniques used and presenting the results in a flexible and visual way, avoiding any manual handling of data during the process. Advantages of this approach are shown and compared to the previous situation where some of the tasks were accomplished by time consuming and error prone manipulations of data.","sentences":["There is a great concern nowadays regarding alcohol consumption and drug abuse, especially in young people.","Analyzing the social environment where these adolescents are immersed, as well as a series of measures determining the alcohol abuse risk or personal situation and perception using a number of questionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain insight into the current situation of a given individual regarding his/her consumption behavior.","But this analysis, in order to be achieved, requires the use of tools that can ease the process of questionnaire creation, data gathering, curation and representation, and later analysis and visualization to the user.","This research presents the design and construction of a web-based platform able to facilitate each of the mentioned processes by integrating the different phases into an intuitive system with a graphical user interface that hides the complexity underlying each of the questionnaires and techniques used and presenting the results in a flexible and visual way, avoiding any manual handling of data during the process.","Advantages of this approach are shown and compared to the previous situation where some of the tasks were accomplished by time consuming and error prone manipulations of data."],"url":"http://arxiv.org/abs/2402.09592v1"}
{"created":"2024-02-14 21:31:41","title":"DeepATLAS: One-Shot Localization for Biomedical Data","abstract":"This paper introduces the DeepATLAS foundational model for localization tasks in the domain of high-dimensional biomedical data. Upon convergence of the proposed self-supervised objective, a pretrained model maps an input to an anatomically-consistent embedding from which any point or set of points (e.g., boxes or segmentations) may be identified in a one-shot or few-shot approach. As a representative benchmark, a DeepATLAS model pretrained on a comprehensive cohort of 51,000+ unlabeled 3D computed tomography exams yields high one-shot segmentation performance on over 50 anatomic structures across four different external test sets, either matching or exceeding the performance of a standard supervised learning model. Further improvements in accuracy can be achieved by adding a small amount of labeled data using either a semisupervised or more conventional fine-tuning strategy.","sentences":["This paper introduces the DeepATLAS foundational model for localization tasks in the domain of high-dimensional biomedical data.","Upon convergence of the proposed self-supervised objective, a pretrained model maps an input to an anatomically-consistent embedding from which any point or set of points (e.g., boxes or segmentations) may be identified in a one-shot or few-shot approach.","As a representative benchmark, a DeepATLAS model pretrained on a comprehensive cohort of 51,000+ unlabeled 3D computed tomography exams yields high one-shot segmentation performance on over 50 anatomic structures across four different external test sets, either matching or exceeding the performance of a standard supervised learning model.","Further improvements in accuracy can be achieved by adding a small amount of labeled data using either a semisupervised or more conventional fine-tuning strategy."],"url":"http://arxiv.org/abs/2402.09587v1"}
{"created":"2024-02-14 21:29:28","title":"WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using Weight Regularization","abstract":"A common phenomena confining the representation quality in Self-Supervised Learning (SSL) is dimensional collapse (also known as rank degeneration), where the learned representations are mapped to a low dimensional subspace of the representation space. The State-of-the-Art SSL methods have shown to suffer from dimensional collapse and fall behind maintaining full rank. Recent approaches to prevent this problem have proposed using contrastive losses, regularization techniques, or architectural tricks. We propose WERank, a new regularizer on the weight parameters of the network to prevent rank degeneration at different layers of the network. We provide empirical evidence and mathematical justification to demonstrate the effectiveness of the proposed regularization method in preventing dimensional collapse. We verify the impact of WERank on graph SSL where dimensional collapse is more pronounced due to the lack of proper data augmentation. We empirically demonstrate that WERank is effective in helping BYOL to achieve higher rank during SSL pre-training and consequently downstream accuracy during evaluation probing. Ablation studies and experimental analysis shed lights on the underlying factors behind the performance gains of the proposed approach.","sentences":["A common phenomena confining the representation quality in Self-Supervised Learning (SSL) is dimensional collapse (also known as rank degeneration), where the learned representations are mapped to a low dimensional subspace of the representation space.","The State-of-the-Art SSL methods have shown to suffer from dimensional collapse and fall behind maintaining full rank.","Recent approaches to prevent this problem have proposed using contrastive losses, regularization techniques, or architectural tricks.","We propose WERank, a new regularizer on the weight parameters of the network to prevent rank degeneration at different layers of the network.","We provide empirical evidence and mathematical justification to demonstrate the effectiveness of the proposed regularization method in preventing dimensional collapse.","We verify the impact of WERank on graph SSL where dimensional collapse is more pronounced due to the lack of proper data augmentation.","We empirically demonstrate that WERank is effective in helping BYOL to achieve higher rank during SSL pre-training and consequently downstream accuracy during evaluation probing.","Ablation studies and experimental analysis shed lights on the underlying factors behind the performance gains of the proposed approach."],"url":"http://arxiv.org/abs/2402.09586v1"}
{"created":"2024-02-14 21:25:06","title":"Domain Adaptation for Contrastive Audio-Language Models","abstract":"Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time. The zero-shot performance of ALM improves by using suitable text prompts for each domain. The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance. Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training. Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations. Our method learns a domain vector by enforcing consistency across augmented views of the testing audio. We extensively evaluate our approach on 12 downstream tasks across domains. With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement. After adaptation, the model still retains the generalization property of ALMs.","sentences":["Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time.","The zero-shot performance of ALM improves by using suitable text prompts for each domain.","The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance.","Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training.","Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations.","Our method learns a domain vector by enforcing consistency across augmented views of the testing audio.","We extensively evaluate our approach on 12 downstream tasks across domains.","With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement.","After adaptation, the model still retains the generalization property of ALMs."],"url":"http://arxiv.org/abs/2402.09585v1"}
{"created":"2024-02-14 20:40:15","title":"Irreducible Markov Chains on spaces of graphs with fixed degree-color sequences","abstract":"We study a colored generalization of the famous simple-switch Markov chain for sampling the set of graphs with a fixed degree sequence. Here we consider the space of graphs with colored vertices, in which we fix the degree sequence and another statistic arising from the vertex coloring, and prove that the set can be connected with simple color-preserving switches or moves. These moves form a basis for defining an irreducible Markov chain necessary for testing statistical model fit to block-partitioned network data. Our methods further generalize well-known algebraic results from the 1990s: namely, that the corresponding moves can be used to construct a regular triangulation for a generalization of the second hypersimplex. On the other hand, in contrast to the monochromatic case, we show that for simple graphs, the 1-norm of the moves necessary to connect the space increases with the number of colors.","sentences":["We study a colored generalization of the famous simple-switch Markov chain for sampling the set of graphs with a fixed degree sequence.","Here we consider the space of graphs with colored vertices, in which we fix the degree sequence and another statistic arising from the vertex coloring, and prove that the set can be connected with simple color-preserving switches or moves.","These moves form a basis for defining an irreducible Markov chain necessary for testing statistical model fit to block-partitioned network data.","Our methods further generalize well-known algebraic results from the 1990s: namely, that the corresponding moves can be used to construct a regular triangulation for a generalization of the second hypersimplex.","On the other hand, in contrast to the monochromatic case, we show that for simple graphs, the 1-norm of the moves necessary to connect the space increases with the number of colors."],"url":"http://arxiv.org/abs/2402.09568v1"}
{"created":"2024-02-14 20:33:11","title":"Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph","abstract":"Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification. Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes. Followingthis, we propose a novel Graph-Skeleton1 model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures. Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph.","sentences":["Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot.","Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design.","Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications.","One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes.","In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally.","To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification.","Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes.","Followingthis, we propose a novel Graph-Skeleton1 model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures.","Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method.","In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph."],"url":"http://arxiv.org/abs/2402.09565v1"}
{"created":"2024-02-14 20:26:52","title":"ABIDES-Economist: Agent-Based Simulation of Economic Systems with Learning Agents","abstract":"We introduce a multi-agent simulator for economic systems comprised of heterogeneous Households, heterogeneous Firms, Central Bank and Government agents, that could be subjected to exogenous, stochastic shocks. The interaction between agents defines the production and consumption of goods in the economy alongside the flow of money. Each agent can be designed to act according to fixed, rule-based strategies or learn their strategies using interactions with others in the simulator. We ground our simulator by choosing agent heterogeneity parameters based on economic literature, while designing their action spaces in accordance with real data in the United States. Our simulator facilitates the use of reinforcement learning strategies for the agents via an OpenAI Gym style environment definition for the economic system. We demonstrate the utility of our simulator by simulating and analyzing two hypothetical (yet interesting) economic scenarios. The first scenario investigates the impact of heterogeneous household skills on their learned preferences to work at different firms. The second scenario examines the impact of a positive production shock to one of two firms on its pricing strategy in comparison to the second firm. We aspire that our platform sets a stage for subsequent research at the intersection of artificial intelligence and economics.","sentences":["We introduce a multi-agent simulator for economic systems comprised of heterogeneous Households, heterogeneous Firms, Central Bank and Government agents, that could be subjected to exogenous, stochastic shocks.","The interaction between agents defines the production and consumption of goods in the economy alongside the flow of money.","Each agent can be designed to act according to fixed, rule-based strategies or learn their strategies using interactions with others in the simulator.","We ground our simulator by choosing agent heterogeneity parameters based on economic literature, while designing their action spaces in accordance with real data in the United States.","Our simulator facilitates the use of reinforcement learning strategies for the agents via an OpenAI Gym style environment definition for the economic system.","We demonstrate the utility of our simulator by simulating and analyzing two hypothetical (yet interesting) economic scenarios.","The first scenario investigates the impact of heterogeneous household skills on their learned preferences to work at different firms.","The second scenario examines the impact of a positive production shock to one of two firms on its pricing strategy in comparison to the second firm.","We aspire that our platform sets a stage for subsequent research at the intersection of artificial intelligence and economics."],"url":"http://arxiv.org/abs/2402.09563v1"}
{"created":"2024-02-14 20:24:33","title":"Patch-based adaptive temporal filter and residual evaluation","abstract":"In coherent imaging systems, speckle is a signal-dependent noise that visually strongly degrades images' appearance. A huge amount of SAR data has been acquired from different sensors with different wavelengths, resolutions, incidences and polarizations. We extend the nonlocal filtering strategy to the temporal domain and propose a patch-based adaptive temporal filter (PATF) to take advantage of well-registered multi-temporal SAR images. A patch-based generalised likelihood ratio test is processed to suppress the changed object effects on the multitemporal denoising results. Then, the similarities are transformed into corresponding weights with an exponential function. The denoised value is calculated with a temporal weighted average. Spatial adaptive denoising methods can improve the patch-based weighted temporal average image when the time series is limited. The spatial adaptive denoising step is optional when the time series is large enough. Without reference image, we propose using a patch-based auto-covariance residual evaluation method to examine the ratio image between the noisy and denoised images and look for possible remaining structural contents. It can process automatically and does not rely on a supervised selection of homogeneous regions. It also provides a global score for the whole image. Numerous results demonstrate the effectiveness of the proposed time series denoising method and the usefulness of the residual evaluation method.","sentences":["In coherent imaging systems, speckle is a signal-dependent noise that visually strongly degrades images' appearance.","A huge amount of SAR data has been acquired from different sensors with different wavelengths, resolutions, incidences and polarizations.","We extend the nonlocal filtering strategy to the temporal domain and propose a patch-based adaptive temporal filter (PATF) to take advantage of well-registered multi-temporal SAR images.","A patch-based generalised likelihood ratio test is processed to suppress the changed object effects on the multitemporal denoising results.","Then, the similarities are transformed into corresponding weights with an exponential function.","The denoised value is calculated with a temporal weighted average.","Spatial adaptive denoising methods can improve the patch-based weighted temporal average image when the time series is limited.","The spatial adaptive denoising step is optional when the time series is large enough.","Without reference image, we propose using a patch-based auto-covariance residual evaluation method to examine the ratio image between the noisy and denoised images and look for possible remaining structural contents.","It can process automatically and does not rely on a supervised selection of homogeneous regions.","It also provides a global score for the whole image.","Numerous results demonstrate the effectiveness of the proposed time series denoising method and the usefulness of the residual evaluation method."],"url":"http://arxiv.org/abs/2402.09561v1"}
{"created":"2024-02-14 20:19:24","title":"Bidirectional Generative Pre-training for Improving Time Series Representation Learning","abstract":"Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-series sequences, even more so after fine-tuning on the task.","sentences":["Learning time-series representations for discriminative tasks has been a long-standing challenge.","Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction.","We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers.","This pre-training task preserves original distribution and data shapes of the time-series.","Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities.","Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs.","By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-series sequences, even more so after fine-tuning on the task."],"url":"http://arxiv.org/abs/2402.09558v1"}
{"created":"2024-02-14 20:17:04","title":"Enhancing Source Code Representations for Deep Learning with Static Analysis","abstract":"Deep learning techniques applied to program analysis tasks such as code classification, summarization, and bug detection have seen widespread interest. Traditional approaches, however, treat programming source code as natural language text, which may neglect significant structural or semantic details. Additionally, most current methods of representing source code focus solely on the code, without considering beneficial additional context. This paper explores the integration of static analysis and additional context such as bug reports and design patterns into source code representations for deep learning models. We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and augment it with additional context information obtained from bug reports and design patterns, creating an enriched source code representation that significantly enhances the performance of common software engineering tasks such as code classification and code clone detection. Utilizing existing open-source code data, our approach improves the representation and processing of source code, thereby improving task performance.","sentences":["Deep learning techniques applied to program analysis tasks such as code classification, summarization, and bug detection have seen widespread interest.","Traditional approaches, however, treat programming source code as natural language text, which may neglect significant structural or semantic details.","Additionally, most current methods of representing source code focus solely on the code, without considering beneficial additional context.","This paper explores the integration of static analysis and additional context such as bug reports and design patterns into source code representations for deep learning models.","We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and augment it with additional context information obtained from bug reports and design patterns, creating an enriched source code representation that significantly enhances the performance of common software engineering tasks such as code classification and code clone detection.","Utilizing existing open-source code data, our approach improves the representation and processing of source code, thereby improving task performance."],"url":"http://arxiv.org/abs/2402.09557v1"}
{"created":"2024-02-14 20:10:30","title":"Statistical and Machine Learning Models for Predicting Fire and Other Emergency Events","abstract":"Emergency events in a city cause considerable economic loss to individuals, their families, and the community. Accurate and timely prediction of events can help the emergency fire and rescue services in preparing for and mitigating the consequences of emergency events. In this paper, we present a systematic development of predictive models for various types of emergency events in the City of Edmonton, Canada. We present methods for (i) data collection and dataset development; (ii) descriptive analysis of each event type and its characteristics at different spatiotemporal levels; (iii) feature analysis and selection based on correlation coefficient analysis and feature importance analysis; and (iv) development of prediction models for the likelihood of occurrence of each event type at different temporal and spatial resolutions. We analyze the association of event types with socioeconomic and demographic data at the neighborhood level, identify a set of predictors for each event type, and develop predictive models with negative binomial regression. We conduct evaluations at neighborhood and fire station service area levels. Our results show that the models perform well for most of the event types with acceptable prediction errors for weekly and monthly periods. The evaluation shows that the prediction accuracy is consistent at the level of the fire station, so the predictions can be used in management by fire rescue service departments for planning resource allocation for these time periods. We also examine the impact of the COVID-19 pandemic on the occurrence of events and on the accuracy of event predictor models. Our findings show that COVID-19 had a significant impact on the performance of the event prediction models.","sentences":["Emergency events in a city cause considerable economic loss to individuals, their families, and the community.","Accurate and timely prediction of events can help the emergency fire and rescue services in preparing for and mitigating the consequences of emergency events.","In this paper, we present a systematic development of predictive models for various types of emergency events in the City of Edmonton, Canada.","We present methods for (i) data collection and dataset development; (ii) descriptive analysis of each event type and its characteristics at different spatiotemporal levels; (iii) feature analysis and selection based on correlation coefficient analysis and feature importance analysis; and (iv) development of prediction models for the likelihood of occurrence of each event type at different temporal and spatial resolutions.","We analyze the association of event types with socioeconomic and demographic data at the neighborhood level, identify a set of predictors for each event type, and develop predictive models with negative binomial regression.","We conduct evaluations at neighborhood and fire station service area levels.","Our results show that the models perform well for most of the event types with acceptable prediction errors for weekly and monthly periods.","The evaluation shows that the prediction accuracy is consistent at the level of the fire station, so the predictions can be used in management by fire rescue service departments for planning resource allocation for these time periods.","We also examine the impact of the COVID-19 pandemic on the occurrence of events and on the accuracy of event predictor models.","Our findings show that COVID-19 had a significant impact on the performance of the event prediction models."],"url":"http://arxiv.org/abs/2402.09553v1"}
{"created":"2024-02-14 19:34:28","title":"Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning","abstract":"In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning methods across multiple problem settings, regardless of the amount of available replay memory.","sentences":["In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream.","Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data.","Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy.","Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting.","We minimize these instabilities through a simple modification of the optimization geometry.","Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data.","We demonstrate that LPR consistently improves replay-based online continual learning methods across multiple problem settings, regardless of the amount of available replay memory."],"url":"http://arxiv.org/abs/2402.09542v1"}
{"created":"2024-02-14 19:31:45","title":"Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?","abstract":"For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model. The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets. In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set. Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed empirically that DP with large $\\epsilon$ can successfully defend against state-of-the-art MIAs. Existing DP theory cannot explain these empirical findings: e.g., the theoretical privacy guarantees of $\\epsilon \\geq 7$ are essentially vacuous. In this paper, we aim to close this gap between theory and practice and understand why a large DP parameter can prevent practical MIAs. To tackle this problem, we propose a new privacy notion called practical membership privacy (PMP). PMP models a practical attacker's uncertainty about the contents of the private data. The PMP parameter has a natural interpretation in terms of the success rate of a practical MIA on a given data set. We quantitatively analyze the PMP parameter of two fundamental DP mechanisms: the exponential mechanism and Gaussian mechanism. Our analysis reveals that a large DP parameter often translates into a much smaller PMP parameter, which guarantees strong privacy against practical MIAs. Using our findings, we offer principled guidance for practitioners in choosing the DP parameter.","sentences":["For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model.","The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets.","In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set.","Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed empirically that DP with large $\\epsilon$ can successfully defend against state-of-the-art MIAs.","Existing DP theory cannot explain these empirical findings: e.g., the theoretical privacy guarantees of $\\epsilon \\geq 7$ are essentially vacuous.","In this paper, we aim to close this gap between theory and practice and understand why a large DP parameter can prevent practical MIAs.","To tackle this problem, we propose a new privacy notion called practical membership privacy (PMP).","PMP models a practical attacker's uncertainty about the contents of the private data.","The PMP parameter has a natural interpretation in terms of the success rate of a practical MIA on a given data set.","We quantitatively analyze the PMP parameter of two fundamental DP mechanisms: the exponential mechanism and Gaussian mechanism.","Our analysis reveals that a large DP parameter often translates into a much smaller PMP parameter, which guarantees strong privacy against practical MIAs.","Using our findings, we offer principled guidance for practitioners in choosing the DP parameter."],"url":"http://arxiv.org/abs/2402.09540v1"}
{"created":"2024-02-14 19:29:04","title":"Learning From Lessons Learned: Preliminary Findings From a Study of Learning From Failure","abstract":"Due to various sources of uncertainty, emergent behavior, and ongoing changes, the reliability of many socio-technical systems depends on an iterative and collaborative process in which organizations (1) analyze and learn from system failures, and then (2) co-evolve both the technical and human parts of their systems based on what they learn. Many organizations have defined processes for learning from failure, often involving postmortem analyses conducted after any system failures that are judged to be sufficiently severe. Despite established processes and tool support, our preliminary research, and professional experience, suggest that it is not straightforward to take what was learned from a failure and successfully improve the reliability of the socio-technical system. To better understand this collaborative process and the associated challenges, we are conducting a study of how teams learn from failure. We are gathering incident reports from multiple organizations and conducting interviews with engineers and managers with relevant experience. Our analytic interest is in what is learned by teams as they reflect on failures, the learning processes involved, and how they use what is learned. Our data collection and analysis are not yet complete, but we have so far analyzed 13 incident reports and seven interviews. In this short paper we (1) present our preliminary findings, and (2) outline our broader research plans.","sentences":["Due to various sources of uncertainty, emergent behavior, and ongoing changes, the reliability of many socio-technical systems depends on an iterative and collaborative process in which organizations (1) analyze and learn from system failures, and then (2) co-evolve both the technical and human parts of their systems based on what they learn.","Many organizations have defined processes for learning from failure, often involving postmortem analyses conducted after any system failures that are judged to be sufficiently severe.","Despite established processes and tool support, our preliminary research, and professional experience, suggest that it is not straightforward to take what was learned from a failure and successfully improve the reliability of the socio-technical system.","To better understand this collaborative process and the associated challenges, we are conducting a study of how teams learn from failure.","We are gathering incident reports from multiple organizations and conducting interviews with engineers and managers with relevant experience.","Our analytic interest is in what is learned by teams as they reflect on failures, the learning processes involved, and how they use what is learned.","Our data collection and analysis are not yet complete, but we have so far analyzed 13 incident reports and seven interviews.","In this short paper we (1) present our preliminary findings, and (2) outline our broader research plans."],"url":"http://arxiv.org/abs/2402.09538v1"}
{"created":"2024-02-14 19:10:40","title":"Reducing Texture Bias of Deep Neural Networks via Edge Enhancing Diffusion","abstract":"Convolutional neural networks (CNNs) for image processing tend to focus on localized texture patterns, commonly referred to as texture bias. While most of the previous works in the literature focus on the task of image classification, we go beyond this and study the texture bias of CNNs in semantic segmentation. In this work, we propose to train CNNs on pre-processed images with less texture to reduce the texture bias. Therein, the challenge is to suppress image texture while preserving shape information. To this end, we utilize edge enhancing diffusion (EED), an anisotropic image diffusion method initially introduced for image compression, to create texture reduced duplicates of existing datasets. Extensive numerical studies are performed with both CNNs and vision transformer models trained on original data and EED-processed data from the Cityscapes dataset and the CARLA driving simulator. We observe strong texture-dependence of CNNs and moderate texture-dependence of transformers. Training CNNs on EED-processed images enables the models to become completely ignorant with respect to texture, demonstrating resilience with respect to texture re-introduction to any degree. Additionally we analyze the performance reduction in depth on a level of connected components in the semantic segmentation and study the influence of EED pre-processing on domain generalization as well as adversarial robustness.","sentences":["Convolutional neural networks (CNNs) for image processing tend to focus on localized texture patterns, commonly referred to as texture bias.","While most of the previous works in the literature focus on the task of image classification, we go beyond this and study the texture bias of CNNs in semantic segmentation.","In this work, we propose to train CNNs on pre-processed images with less texture to reduce the texture bias.","Therein, the challenge is to suppress image texture while preserving shape information.","To this end, we utilize edge enhancing diffusion (EED), an anisotropic image diffusion method initially introduced for image compression, to create texture reduced duplicates of existing datasets.","Extensive numerical studies are performed with both CNNs and vision transformer models trained on original data and EED-processed data from the Cityscapes dataset and the CARLA driving simulator.","We observe strong texture-dependence of CNNs and moderate texture-dependence of transformers.","Training CNNs on EED-processed images enables the models to become completely ignorant with respect to texture, demonstrating resilience with respect to texture re-introduction to any degree.","Additionally we analyze the performance reduction in depth on a level of connected components in the semantic segmentation and study the influence of EED pre-processing on domain generalization as well as adversarial robustness."],"url":"http://arxiv.org/abs/2402.09530v1"}
{"created":"2024-02-14 19:08:30","title":"Jasper: Scalable and Fair Multicast for Financial Exchanges in the Cloud","abstract":"Financial exchanges have recently shown an interest in migrating to the public cloud for scalability, elasticity, and cost savings. However, financial exchanges often have strict network requirements that can be difficult to meet on the cloud. Notably, market participants (MPs) trade based on market data about different activities in the market. Exchanges often use switch multicast to disseminate market data to MPs. However, if one MP receives market data earlier than another, that MP would have an unfair advantage. To prevent this, financial exchanges often equalize exchange-to-MP cable lengths to provide near-simultaneous reception of market data at MPs.   As a cloud tenant, however, building a fair multicast service is challenging because of the lack of switch support for multicast, high latency variance, and the lack of native mechanisms for simultaneous data delivery in the cloud. Jasper introduces a solution that creates an overlay multicast tree within a cloud region that minimizes latency and latency variations through hedging, leverages recent advancements in clock synchronization to achieve simultaneous delivery, and addresses various sources of latency through an optimized DPDK/eBPF implementation -- while scaling to 1000+ emulated receivers. Jasper outperforms a prior system CloudEx and a commercial multicast solution provided by Amazon Web Services.","sentences":["Financial exchanges have recently shown an interest in migrating to the public cloud for scalability, elasticity, and cost savings.","However, financial exchanges often have strict network requirements that can be difficult to meet on the cloud.","Notably, market participants (MPs) trade based on market data about different activities in the market.","Exchanges often use switch multicast to disseminate market data to MPs.","However, if one MP receives market data earlier than another, that MP would have an unfair advantage.","To prevent this, financial exchanges often equalize exchange-to-MP cable lengths to provide near-simultaneous reception of market data at MPs.   ","As a cloud tenant, however, building a fair multicast service is challenging because of the lack of switch support for multicast, high latency variance, and the lack of native mechanisms for simultaneous data delivery in the cloud.","Jasper introduces a solution that creates an overlay multicast tree within a cloud region that minimizes latency and latency variations through hedging, leverages recent advancements in clock synchronization to achieve simultaneous delivery, and addresses various sources of latency through an optimized DPDK/eBPF implementation -- while scaling to 1000+ emulated receivers.","Jasper outperforms a prior system CloudEx and a commercial multicast solution provided by Amazon Web Services."],"url":"http://arxiv.org/abs/2402.09527v1"}
