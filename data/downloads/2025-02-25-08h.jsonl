{"created":"2025-02-24 18:59:55","title":"Towards Hierarchical Rectified Flow","abstract":"We formulate a hierarchical rectified flow to model data distributions. It hierarchically couples multiple ordinary differential equations (ODEs) and defines a time-differentiable stochastic process that generates a data distribution from a known source distribution. Each ODE resembles the ODE that is solved in a classic rectified flow, but differs in its domain, i.e., location, velocity, acceleration, etc. Unlike the classic rectified flow formulation, which formulates a single ODE in the location domain and only captures the expected velocity field (sufficient to capture a multi-modal data distribution), the hierarchical rectified flow formulation models the multi-modal random velocity field, acceleration field, etc., in their entirety. This more faithful modeling of the random velocity field enables integration paths to intersect when the underlying ODE is solved during data generation. Intersecting paths in turn lead to integration trajectories that are more straight than those obtained in the classic rectified flow formulation, where integration paths cannot intersect. This leads to modeling of data distributions with fewer neural function evaluations. We empirically verify this on synthetic 1D and 2D data as well as MNIST, CIFAR-10, and ImageNet-32 data. Code is available at: https://riccizz.github.io/HRF/.","sentences":["We formulate a hierarchical rectified flow to model data distributions.","It hierarchically couples multiple ordinary differential equations (ODEs) and defines a time-differentiable stochastic process that generates a data distribution from a known source distribution.","Each ODE resembles the ODE that is solved in a classic rectified flow, but differs in its domain, i.e., location, velocity, acceleration, etc.","Unlike the classic rectified flow formulation, which formulates a single ODE in the location domain and only captures the expected velocity field (sufficient to capture a multi-modal data distribution), the hierarchical rectified flow formulation models the multi-modal random velocity field, acceleration field, etc., in their entirety.","This more faithful modeling of the random velocity field enables integration paths to intersect when the underlying ODE is solved during data generation.","Intersecting paths in turn lead to integration trajectories that are more straight than those obtained in the classic rectified flow formulation, where integration paths cannot intersect.","This leads to modeling of data distributions with fewer neural function evaluations.","We empirically verify this on synthetic 1D and 2D data as well as MNIST, CIFAR-10, and ImageNet-32 data.","Code is available at: https://riccizz.github.io/HRF/."],"url":"http://arxiv.org/abs/2502.17436v1"}
{"created":"2025-02-24 18:59:54","title":"GCC: Generative Color Constancy via Diffusing a Color Checker","abstract":"Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in bi-directional evaluations. These results highlight our method's stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications.","sentences":["Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities.","We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation.","Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations.","GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in bi-directional evaluations.","These results highlight our method's stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications."],"url":"http://arxiv.org/abs/2502.17435v1"}
{"created":"2025-02-24 18:59:07","title":"FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning","abstract":"Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on force feedback for reliable execution. However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning. Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not require intricate force-feedback. In this paper, we first present a low-cost, intuitive, bilateral teleoperation setup that relays external forces of the follower arm back to the teacher arm, facilitating data collection for complex, contact-rich tasks. We then introduce FACTR, a policy learning method that employs a curriculum which corrupts the visual input with decreasing intensity throughout training. The curriculum prevents our transformer-based policy from over-fitting to the visual input and guides the policy to properly attend to the force modality. We demonstrate that by fully utilizing the force information, our method significantly improves generalization to unseen objects by 43\\% compared to baseline approaches without a curriculum. Video results and instructions at https://jasonjzliu.com/factr/","sentences":["Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on force feedback for reliable execution.","However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning.","Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not require intricate force-feedback.","In this paper, we first present a low-cost, intuitive, bilateral teleoperation setup that relays external forces of the follower arm back to the teacher arm, facilitating data collection for complex, contact-rich tasks.","We then introduce FACTR, a policy learning method that employs a curriculum which corrupts the visual input with decreasing intensity throughout training.","The curriculum prevents our transformer-based policy from over-fitting to the visual input and guides the policy to properly attend to the force modality.","We demonstrate that by fully utilizing the force information, our method significantly improves generalization to unseen objects by 43\\% compared to baseline approaches without a curriculum.","Video results and instructions at https://jasonjzliu.com/factr/"],"url":"http://arxiv.org/abs/2502.17432v1"}
{"created":"2025-02-24 18:55:54","title":"S4S: Solving for a Diffusion Model Solver","abstract":"Diffusion models (DMs) create samples from a data distribution by starting from random noise and iteratively solving a reverse-time ordinary differential equation (ODE). Because each step in the iterative solution requires an expensive neural function evaluation (NFE), there has been significant interest in approximately solving these diffusion ODEs with only a few NFEs without modifying the underlying model. However, in the few NFE regime, we observe that tracking the true ODE evolution is fundamentally impossible using traditional ODE solvers. In this work, we propose a new method that learns a good solver for the DM, which we call Solving for the Solver (S4S). S4S directly optimizes a solver to obtain good generation quality by learning to match the output of a strong teacher solver. We evaluate S4S on six different pre-trained DMs, including pixel-space and latent-space DMs for both conditional and unconditional sampling. In all settings, S4S uniformly improves the sample quality relative to traditional ODE solvers. Moreover, our method is lightweight, data-free, and can be plugged in black-box on top of any discretization schedule or architecture to improve performance. Building on top of this, we also propose S4S-Alt, which optimizes both the solver and the discretization schedule. By exploiting the full design space of DM solvers, with 5 NFEs, we achieve an FID of 3.73 on CIFAR10 and 13.26 on MS-COCO, representing a $1.5\\times$ improvement over previous training-free ODE methods.","sentences":["Diffusion models (DMs) create samples from a data distribution by starting from random noise and iteratively solving a reverse-time ordinary differential equation (ODE).","Because each step in the iterative solution requires an expensive neural function evaluation (NFE), there has been significant interest in approximately solving these diffusion ODEs with only a few NFEs without modifying the underlying model.","However, in the few NFE regime, we observe that tracking the true ODE evolution is fundamentally impossible using traditional ODE solvers.","In this work, we propose a new method that learns a good solver for the DM, which we call Solving for the Solver (S4S).","S4S directly optimizes a solver to obtain good generation quality by learning to match the output of a strong teacher solver.","We evaluate S4S on six different pre-trained DMs, including pixel-space and latent-space DMs for both conditional and unconditional sampling.","In all settings, S4S uniformly improves the sample quality relative to traditional ODE solvers.","Moreover, our method is lightweight, data-free, and can be plugged in black-box on top of any discretization schedule or architecture to improve performance.","Building on top of this, we also propose S4S-Alt, which optimizes both the solver and the discretization schedule.","By exploiting the full design space of DM solvers, with 5 NFEs, we achieve an FID of 3.73 on CIFAR10 and 13.26 on MS-COCO, representing a $1.5\\times$ improvement over previous training-free ODE methods."],"url":"http://arxiv.org/abs/2502.17423v1"}
{"created":"2025-02-24 18:53:31","title":"LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification","abstract":"Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at https://github.com/sail-sg/LongSpec.","sentences":["Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs).","Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation.","In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges.","First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache.","Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference.","Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding.","Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction.","The code is available at https://github.com/sail-sg/LongSpec."],"url":"http://arxiv.org/abs/2502.17421v1"}
{"created":"2025-02-24 18:47:54","title":"X-Dancer: Expressive Music to Human Dance Video Generation","abstract":"We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.","sentences":["We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image.","As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames.","Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos.","To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands).","We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context.","Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework.","Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism.","Code and model will be available for research purposes."],"url":"http://arxiv.org/abs/2502.17414v1"}
{"created":"2025-02-24 18:30:36","title":"Large Language Models are Powerful EHR Encoders","abstract":"Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications.","sentences":["Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches.","Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices.","In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders.","By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets.","We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines.","Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window.","Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications."],"url":"http://arxiv.org/abs/2502.17403v1"}
{"created":"2025-02-24 18:28:22","title":"Enriching Physical-Virtual Interaction in AR Gaming by Tracking Identical Real Objects","abstract":"Augmented reality (AR) games, particularly those designed for headsets, have become increasingly prevalent with advancements in both hardware and software. However, the majority of AR games still rely on pre-scanned or static scenes, and interaction mechanisms are often limited to controllers or hand-tracking. Additionally, the presence of identical objects in AR games poses challenges for conventional object tracking techniques, which often struggle to differentiate between identical objects or necessitate the installation of fixed cameras for global object movement tracking. In response to these limitations, we present a novel approach to address the tracking of identical objects in an AR scene to enrich physical-virtual interaction. Our method leverages partial scene observations captured by an AR headset, utilizing the perspective and spatial data provided by this technology. Object identities within the scene are determined through the solution of a label assignment problem using integer programming. To enhance computational efficiency, we incorporate a Voronoi diagram-based pruning method into our approach. Our implementation of this approach in a farm-to-table AR game demonstrates its satisfactory performance and robustness. Furthermore, we showcase the versatility and practicality of our method through applications in AR storytelling and a simulated gaming robot. Our video demo is available at: https://youtu.be/rPGkLYuKvCQ.","sentences":["Augmented reality (AR) games, particularly those designed for headsets, have become increasingly prevalent with advancements in both hardware and software.","However, the majority of AR games still rely on pre-scanned or static scenes, and interaction mechanisms are often limited to controllers or hand-tracking.","Additionally, the presence of identical objects in AR games poses challenges for conventional object tracking techniques, which often struggle to differentiate between identical objects or necessitate the installation of fixed cameras for global object movement tracking.","In response to these limitations, we present a novel approach to address the tracking of identical objects in an AR scene to enrich physical-virtual interaction.","Our method leverages partial scene observations captured by an AR headset, utilizing the perspective and spatial data provided by this technology.","Object identities within the scene are determined through the solution of a label assignment problem using integer programming.","To enhance computational efficiency, we incorporate a Voronoi diagram-based pruning method into our approach.","Our implementation of this approach in a farm-to-table AR game demonstrates its satisfactory performance and robustness.","Furthermore, we showcase the versatility and practicality of our method through applications in AR storytelling and a simulated gaming robot.","Our video demo is available at: https://youtu.be/rPGkLYuKvCQ."],"url":"http://arxiv.org/abs/2502.17399v1"}
{"created":"2025-02-24 18:26:22","title":"Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded Heterogeneous SoCs","abstract":"Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific hardware accelerators to improve performance and energy efficiency. In particular, programmable multi-core accelerators feature a cluster of processing elements and tightly coupled scratchpad memories to balance performance, energy efficiency, and flexibility. In embedded systems running a general-purpose OS, accelerators access data via dedicated, physically addressed memory regions. This negatively impacts memory utilization and performance by requiring a copy from the virtual host address to the physical accelerator address space. Input-Output Memory Management Units (IOMMUs) overcome this limitation by allowing devices and hosts to use a shared virtual paged address space. However, resolving IO virtual addresses can be particularly costly on high-latency memory systems as it requires up to three sequential memory accesses on IOTLB miss. In this work, we present a quantitative evaluation of shared virtual addressing in RISC-V heterogeneous embedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V SoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated the system performance by emulating the design on FPGA and implementing compute kernels from the RajaPERF benchmark suite using heterogeneous OpenMP programming. We measure the transfers and computation time on the host and accelerators for systems with different DRAM access latencies. We first show that IO virtual address translation can account for 4.2% up to 17.6% of the accelerator's runtime for gemm (General Matrix Multiplication) at low and high memory bandwidth. Then, we show that in systems containing a last-level cache, this IO address translation cost falls to 0.4% and 0.7% under the same conditions, making shared virtual addressing and zero-copy offloading suitable for such RISC-V heterogeneous SoCs.","sentences":["Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific hardware accelerators to improve performance and energy efficiency.","In particular, programmable multi-core accelerators feature a cluster of processing elements and tightly coupled scratchpad memories to balance performance, energy efficiency, and flexibility.","In embedded systems running a general-purpose OS, accelerators access data via dedicated, physically addressed memory regions.","This negatively impacts memory utilization and performance by requiring a copy from the virtual host address to the physical accelerator address space.","Input-Output Memory Management Units (IOMMUs) overcome this limitation by allowing devices and hosts to use a shared virtual paged address space.","However, resolving IO virtual addresses can be particularly costly on high-latency memory systems as it requires up to three sequential memory accesses on IOTLB miss.","In this work, we present a quantitative evaluation of shared virtual addressing in RISC-V heterogeneous embedded systems.","We integrate an IOMMU in an open-source heterogeneous RISC-V SoC consisting of a 64-bit host with a 32-bit accelerator cluster.","We evaluated the system performance by emulating the design on FPGA and implementing compute kernels from the RajaPERF benchmark suite using heterogeneous OpenMP programming.","We measure the transfers and computation time on the host and accelerators for systems with different DRAM access latencies.","We first show that IO virtual address translation can account for 4.2% up to 17.6% of the accelerator's runtime for gemm (General Matrix Multiplication) at low and high memory bandwidth.","Then, we show that in systems containing a last-level cache, this IO address translation cost falls to 0.4% and 0.7% under the same conditions, making shared virtual addressing and zero-copy offloading suitable for such RISC-V heterogeneous SoCs."],"url":"http://arxiv.org/abs/2502.17398v1"}
{"created":"2025-02-24 18:20:42","title":"FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event Detection","abstract":"Event Detection (ED) is the task of identifying typed event mentions of interest from natural language text, which benefits domain-specific reasoning in biomedical, legal, and epidemiological domains. However, procuring supervised data for thousands of events for various domains is a laborious and expensive task. To this end, existing works have explored synthetic data generation via forward (generating labels for unlabeled sentences) and inverse (generating sentences from generated labels) generations. However, forward generation often produces noisy labels, while inverse generation struggles with domain drift and incomplete event annotations. To address these challenges, we introduce FIG, a hybrid approach that leverages inverse generation for high-quality data synthesis while anchoring it to domain-specific cues extracted via forward generation on unlabeled target data. FIG further enhances its synthetic data by adding missing annotations through forward generation-based refinement. Experimentation on three ED datasets from diverse domains reveals that FIG outperforms the best baseline achieving average gains of 3.3% F1 and 5.4% F1 in the zero-shot and few-shot settings respectively. Analyzing the generated trigger hit rate and human evaluation substantiates FIG's superior domain alignment and data quality compared to existing baselines.","sentences":["Event Detection (ED) is the task of identifying typed event mentions of interest from natural language text, which benefits domain-specific reasoning in biomedical, legal, and epidemiological domains.","However, procuring supervised data for thousands of events for various domains is a laborious and expensive task.","To this end, existing works have explored synthetic data generation via forward (generating labels for unlabeled sentences) and inverse (generating sentences from generated labels) generations.","However, forward generation often produces noisy labels, while inverse generation struggles with domain drift and incomplete event annotations.","To address these challenges, we introduce FIG, a hybrid approach that leverages inverse generation for high-quality data synthesis while anchoring it to domain-specific cues extracted via forward generation on unlabeled target data.","FIG further enhances its synthetic data by adding missing annotations through forward generation-based refinement.","Experimentation on three ED datasets from diverse domains reveals that FIG outperforms the best baseline achieving average gains of 3.3% F1 and 5.4% F1 in the zero-shot and few-shot settings respectively.","Analyzing the generated trigger hit rate and human evaluation substantiates FIG's superior domain alignment and data quality compared to existing baselines."],"url":"http://arxiv.org/abs/2502.17394v1"}
{"created":"2025-02-24 18:20:41","title":"Evolving Form and Function: Dual-Objective Optimization in Neural Symbolic Regression Networks","abstract":"Data increasingly abounds, but distilling their underlying relationships down to something interpretable remains challenging. One approach is genetic programming, which `symbolically regresses' a data set down into an equation.   However, symbolic regression (SR) faces the issue of requiring training from scratch for each new dataset. To generalize across all datasets, deep learning techniques have been applied to SR.   These networks, however, are only able to be trained using a symbolic objective: NN-generated and target equations are symbolically compared. But this does not consider the predictive power of these equations, which could be measured by a behavioral objective that compares the generated equation's predictions to actual data.   Here we introduce a method that combines gradient descent and evolutionary computation to yield neural networks that minimize the symbolic and behavioral errors of the equations they generate from data.   As a result, these evolved networks are shown to generate more symbolically and behaviorally accurate equations than those generated by networks trained by state-of-the-art gradient based neural symbolic regression methods.   We hope this method suggests that evolutionary algorithms, combined with gradient descent, can improve SR results by yielding equations with more accurate form and function.","sentences":["Data increasingly abounds, but distilling their underlying relationships down to something interpretable remains challenging.","One approach is genetic programming, which `symbolically regresses' a data set down into an equation.   ","However, symbolic regression (SR) faces the issue of requiring training from scratch for each new dataset.","To generalize across all datasets, deep learning techniques have been applied to SR.   ","These networks, however, are only able to be trained using a symbolic objective: NN-generated and target equations are symbolically compared.","But this does not consider the predictive power of these equations, which could be measured by a behavioral objective that compares the generated equation's predictions to actual data.   ","Here we introduce a method that combines gradient descent and evolutionary computation to yield neural networks that minimize the symbolic and behavioral errors of the equations they generate from data.   ","As a result, these evolved networks are shown to generate more symbolically and behaviorally accurate equations than those generated by networks trained by state-of-the-art gradient based neural symbolic regression methods.   ","We hope this method suggests that evolutionary algorithms, combined with gradient descent, can improve SR results by yielding equations with more accurate form and function."],"url":"http://arxiv.org/abs/2502.17393v1"}
{"created":"2025-02-24 18:14:01","title":"Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models","abstract":"Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity. In this work, we present Big-Math, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL). To create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution. To ensure the quality of Big-Math, we manually verify each step in our filtering process. Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm. Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger, while our rigorous filtering ensures that we maintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that Big-Math contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements. By bridging the gap between data quality and quantity, Big-Math establish a robust foundation for advancing reasoning in LLMs.","sentences":["Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements.","However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity.","In this work, we present Big-Math, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL).","To create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution.","To ensure the quality of Big-Math, we manually verify each step in our filtering process.","Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm.","Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger, while our rigorous filtering ensures that we maintain the questions most suitable for RL.","We also provide a rigorous analysis of the dataset, finding that Big-Math contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements.","By bridging the gap between data quality and quantity, Big-Math establish a robust foundation for advancing reasoning in LLMs."],"url":"http://arxiv.org/abs/2502.17387v1"}
{"created":"2025-02-24 18:01:50","title":"Continuous Integration Practices in Machine Learning Projects: The Practitioners` Perspective","abstract":"Continuous Integration (CI) is a cornerstone of modern software development. However, while widely adopted in traditional software projects, applying CI practices to Machine Learning (ML) projects presents distinctive characteristics. For example, our previous work revealed that ML projects often experience longer build durations and lower test coverage rates compared to their non-ML counterparts. Building on these quantitative findings, this study surveys 155 practitioners from 47 ML projects to investigate the underlying reasons for these distinctive characteristics through a qualitative perspective. Practitioners highlighted eight key differences, including test complexity, infrastructure requirements, and build duration and stability. Common challenges mentioned by practitioners include higher project complexity, model training demands, extensive data handling, increased computational resource needs, and dependency management, all contributing to extended build durations. Furthermore, ML systems' non-deterministic nature, data dependencies, and computational constraints were identified as significant barriers to effective testing. The key takeaway from this study is that while foundational CI principles remain valuable, ML projects require tailored approaches to address their unique challenges. To bridge this gap, we propose a set of ML-specific CI practices, including tracking model performance metrics and prioritizing test execution within CI pipelines. Additionally, our findings highlight the importance of fostering interdisciplinary collaboration to strengthen the testing culture in ML projects. By bridging quantitative findings with practitioners' insights, this study provides a deeper understanding of the interplay between CI practices and the unique demands of ML projects, laying the groundwork for more efficient and robust CI strategies in this domain.","sentences":["Continuous Integration (CI) is a cornerstone of modern software development.","However, while widely adopted in traditional software projects, applying CI practices to Machine Learning (ML) projects presents distinctive characteristics.","For example, our previous work revealed that ML projects often experience longer build durations and lower test coverage rates compared to their non-ML counterparts.","Building on these quantitative findings, this study surveys 155 practitioners from 47 ML projects to investigate the underlying reasons for these distinctive characteristics through a qualitative perspective.","Practitioners highlighted eight key differences, including test complexity, infrastructure requirements, and build duration and stability.","Common challenges mentioned by practitioners include higher project complexity, model training demands, extensive data handling, increased computational resource needs, and dependency management, all contributing to extended build durations.","Furthermore, ML systems' non-deterministic nature, data dependencies, and computational constraints were identified as significant barriers to effective testing.","The key takeaway from this study is that while foundational CI principles remain valuable, ML projects require tailored approaches to address their unique challenges.","To bridge this gap, we propose a set of ML-specific CI practices, including tracking model performance metrics and prioritizing test execution within CI pipelines.","Additionally, our findings highlight the importance of fostering interdisciplinary collaboration to strengthen the testing culture in ML projects.","By bridging quantitative findings with practitioners' insights, this study provides a deeper understanding of the interplay between CI practices and the unique demands of ML projects, laying the groundwork for more efficient and robust CI strategies in this domain."],"url":"http://arxiv.org/abs/2502.17378v1"}
{"created":"2025-02-24 17:52:01","title":"Sustainable Greenhouse Management: A Comparative Analysis of Recurrent and Graph Neural Networks","abstract":"The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation. However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both spatial dependencies and their directionality. Using high-frequency data collected at 15-minute intervals from a greenhouse in Volos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter conditions (R^2 = 0.985) but show limitations during summer cooling system operation. Though STGNNs currently show lower performance (winter R^2 = 0.947), their architecture offers greater potential for integrating additional variables such as PV generation and crop growth indicators.","sentences":["The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation.","However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production.","This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs).","While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables.","Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both spatial dependencies and their directionality.","Using high-frequency data collected at 15-minute intervals from a greenhouse in Volos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter conditions (R^2 = 0.985) but show limitations during summer cooling system operation.","Though STGNNs currently show lower performance (winter R^2 = 0.947), their architecture offers greater potential for integrating additional variables such as PV generation and crop growth indicators."],"url":"http://arxiv.org/abs/2502.17371v1"}
{"created":"2025-02-24 17:38:42","title":"A Closer Look at TabPFN v2: Strength, Limitation, and Extension","abstract":"Tabular datasets are inherently heterogeneous, posing significant challenges for developing pre-trained foundation models. The recently introduced transformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves unprecedented in-context learning accuracy across multiple tabular datasets, marking a pivotal advancement in tabular foundation models. In this paper, we comprehensively evaluate TabPFN v2 on over 300 datasets, confirming its exceptional generalization capabilities on small- to medium-scale tasks. Our analysis identifies randomized feature tokens as a key factor behind TabPFN v2's success, as they unify heterogeneous datasets into a fixed-dimensional representation, enabling more effective training and inference. To further understand TabPFN v2's predictions, we propose a leave-one-fold-out approach, transforming TabPFN v2 into a feature extractor and revealing its capability to simplify data distributions and boost accuracy. Lastly, to address TabPFN v2's limitations in high-dimensional, large-scale, and many-category tasks, we introduce a divide-and-conquer mechanism inspired by Chain-of-Thought prompting, enabling scalable inference. By uncovering the mechanisms behind TabPFN v2's success and introducing strategies to expand its applicability, this study provides key insights into the future of tabular foundation models.","sentences":["Tabular datasets are inherently heterogeneous, posing significant challenges for developing pre-trained foundation models.","The recently introduced transformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves unprecedented in-context learning accuracy across multiple tabular datasets, marking a pivotal advancement in tabular foundation models.","In this paper, we comprehensively evaluate TabPFN v2 on over 300 datasets, confirming its exceptional generalization capabilities on small- to medium-scale tasks.","Our analysis identifies randomized feature tokens as a key factor behind TabPFN v2's success, as they unify heterogeneous datasets into a fixed-dimensional representation, enabling more effective training and inference.","To further understand TabPFN v2's predictions, we propose a leave-one-fold-out approach, transforming TabPFN v2 into a feature extractor and revealing its capability to simplify data distributions and boost accuracy.","Lastly, to address TabPFN v2's limitations in high-dimensional, large-scale, and many-category tasks, we introduce a divide-and-conquer mechanism inspired by Chain-of-Thought prompting, enabling scalable inference.","By uncovering the mechanisms behind TabPFN v2's success and introducing strategies to expand its applicability, this study provides key insights into the future of tabular foundation models."],"url":"http://arxiv.org/abs/2502.17361v1"}
{"created":"2025-02-24 17:36:49","title":"DIS-CO: Discovering Copyrighted Content in VLMs Training Data","abstract":"How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data? Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model's training cutoff. Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. Our code and data are available at https://github.com/avduarte333/DIS-CO","sentences":["How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data?","Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development.","By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions.","To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model's training cutoff.","Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available.","Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content.","Our code and data are available at https://github.com/avduarte333/DIS-CO"],"url":"http://arxiv.org/abs/2502.17358v1"}
{"created":"2025-02-24 17:29:10","title":"Leveraging Procedural Knowledge and Task Hierarchies for Efficient Instructional Video Pre-training","abstract":"Instructional videos provide a convenient modality to learn new tasks (ex. cooking a recipe, or assembling furniture). A viewer will want to find a corresponding video that reflects both the overall task they are interested in as well as contains the relevant steps they need to carry out the task. To perform this, an instructional video model should be capable of inferring both the tasks and the steps that occur in an input video. Doing this efficiently and in a generalizable fashion is key when compute or relevant video topics used to train this model are limited. To address these requirements we explicitly mine task hierarchies and the procedural steps associated with instructional videos. We use this prior knowledge to pre-train our model, $\\texttt{Pivot}$, for step and task prediction. During pre-training, we also provide video augmentation and early stopping strategies to optimally identify which model to use for downstream tasks. We test this pre-trained model on task recognition, step recognition, and step prediction tasks on two downstream datasets. When pre-training data and compute are limited, we outperform previous baselines along these tasks. Therefore, leveraging prior task and step structures enables efficient training of $\\texttt{Pivot}$ for instructional video recommendation.","sentences":["Instructional videos provide a convenient modality to learn new tasks (ex. cooking a recipe, or assembling furniture).","A viewer will want to find a corresponding video that reflects both the overall task they are interested in as well as contains the relevant steps they need to carry out the task.","To perform this, an instructional video model should be capable of inferring both the tasks and the steps that occur in an input video.","Doing this efficiently and in a generalizable fashion is key when compute or relevant video topics used to train this model are limited.","To address these requirements we explicitly mine task hierarchies and the procedural steps associated with instructional videos.","We use this prior knowledge to pre-train our model, $\\texttt{Pivot}$, for step and task prediction.","During pre-training, we also provide video augmentation and early stopping strategies to optimally identify which model to use for downstream tasks.","We test this pre-trained model on task recognition, step recognition, and step prediction tasks on two downstream datasets.","When pre-training data and compute are limited, we outperform previous baselines along these tasks.","Therefore, leveraging prior task and step structures enables efficient training of $\\texttt{Pivot}$ for instructional video recommendation."],"url":"http://arxiv.org/abs/2502.17352v1"}
{"created":"2025-02-24 17:23:12","title":"How Scientists Use Large Language Models to Program","abstract":"Scientists across disciplines write code for critical activities like data collection and generation, statistical modeling, and visualization. As large language models that can generate code have become widely available, scientists may increasingly use these models during research software development. We investigate the characteristics of scientists who are early-adopters of code generating models and conduct interviews with scientists at a public, research-focused university. Through interviews and reviews of user interaction logs, we see that scientists often use code generating models as an information retrieval tool for navigating unfamiliar programming languages and libraries. We present findings about their verification strategies and discuss potential vulnerabilities that may emerge from code generation practices unknowingly influencing the parameters of scientific analyses.","sentences":["Scientists across disciplines write code for critical activities like data collection and generation, statistical modeling, and visualization.","As large language models that can generate code have become widely available, scientists may increasingly use these models during research software development.","We investigate the characteristics of scientists who are early-adopters of code generating models and conduct interviews with scientists at a public, research-focused university.","Through interviews and reviews of user interaction logs, we see that scientists often use code generating models as an information retrieval tool for navigating unfamiliar programming languages and libraries.","We present findings about their verification strategies and discuss potential vulnerabilities that may emerge from code generation practices unknowingly influencing the parameters of scientific analyses."],"url":"http://arxiv.org/abs/2502.17348v1"}
{"created":"2025-02-24 17:19:15","title":"SoFFT: Spatial Fourier Transform for Modeling Continuum Soft Robots","abstract":"Continuum soft robots, composed of flexible materials, exhibit theoretically infinite degrees of freedom, enabling notable adaptability in unstructured environments. Cosserat Rod Theory has emerged as a prominent framework for modeling these robots efficiently, representing continuum soft robots as time-varying curves, known as backbones. In this work, we propose viewing the robot's backbone as a signal in space and time, applying the Fourier transform to describe its deformation compactly. This approach unifies existing modeling strategies within the Cosserat Rod Theory framework, offering insights into commonly used heuristic methods. Moreover, the Fourier transform enables the development of a data-driven methodology to experimentally capture the robot's deformation. The proposed approach is validated through numerical simulations and experiments on a real-world prototype, demonstrating a reduction in the degrees of freedom while preserving the accuracy of the deformation representation.","sentences":["Continuum soft robots, composed of flexible materials, exhibit theoretically infinite degrees of freedom, enabling notable adaptability in unstructured environments.","Cosserat Rod Theory has emerged as a prominent framework for modeling these robots efficiently, representing continuum soft robots as time-varying curves, known as backbones.","In this work, we propose viewing the robot's backbone as a signal in space and time, applying the Fourier transform to describe its deformation compactly.","This approach unifies existing modeling strategies within the Cosserat Rod Theory framework, offering insights into commonly used heuristic methods.","Moreover, the Fourier transform enables the development of a data-driven methodology to experimentally capture the robot's deformation.","The proposed approach is validated through numerical simulations and experiments on a real-world prototype, demonstrating a reduction in the degrees of freedom while preserving the accuracy of the deformation representation."],"url":"http://arxiv.org/abs/2502.17347v1"}
{"created":"2025-02-24 17:04:24","title":"Tokenized SAEs: Disentangling SAE Reconstructions","abstract":"Sparse auto-encoders (SAEs) have become a prevalent tool for interpreting language models' inner workings. However, it is unknown how tightly SAE features correspond to computationally important directions in the model. This work empirically shows that many RES-JB SAE features predominantly correspond to simple input statistics. We hypothesize this is caused by a large class imbalance in training data combined with a lack of complex error signals. To reduce this behavior, we propose a method that disentangles token reconstruction from feature reconstruction. This improvement is achieved by introducing a per-token bias, which provides an enhanced baseline for interesting reconstruction. As a result, significantly more interesting features and improved reconstruction in sparse regimes are learned.","sentences":["Sparse auto-encoders (SAEs) have become a prevalent tool for interpreting language models' inner workings.","However, it is unknown how tightly SAE features correspond to computationally important directions in the model.","This work empirically shows that many RES-JB SAE features predominantly correspond to simple input statistics.","We hypothesize this is caused by a large class imbalance in training data combined with a lack of complex error signals.","To reduce this behavior, we propose a method that disentangles token reconstruction from feature reconstruction.","This improvement is achieved by introducing a per-token bias, which provides an enhanced baseline for interesting reconstruction.","As a result, significantly more interesting features and improved reconstruction in sparse regimes are learned."],"url":"http://arxiv.org/abs/2502.17332v1"}
{"created":"2025-02-24 17:01:48","title":"Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization","abstract":"In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.","sentences":["In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task.","Unlike prior methods that require external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances.","The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability.","The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability.","By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset.","Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings.","Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks."],"url":"http://arxiv.org/abs/2502.17328v1"}
{"created":"2025-02-24 17:00:36","title":"AnyTop: Character Animation Diffusion with Any Topology","abstract":"Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data. In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input. Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism. Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons. Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well. Furthermore, our model's latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation and motion editing. Our webpage, https://anytop2025.github.io/Anytop-page, includes links to videos and code.","sentences":["Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data.","In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input.","Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism.","Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons.","Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well.","Furthermore, our model's latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation and motion editing.","Our webpage, https://anytop2025.github.io/Anytop-page, includes links to videos and code."],"url":"http://arxiv.org/abs/2502.17327v1"}
{"created":"2025-02-24 16:55:27","title":"TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control","abstract":"Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets. In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow. For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks. Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region. Building on this insight, we propose the $\\textbf{S}$elf-$\\textbf{I}$mitative $\\textbf{R}$einforcement $\\textbf{L}$earning ($\\textbf{SIRL}$) framework, where the RL algorithm also imitates potentially task-relevant trajectories. Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return. As a result, our proposed algorithm achieves 120% performance improvement on the challenging HumanoidBench with 5% extra computation overhead. With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully.","sentences":["Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets.","In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow.","For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks.","Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region.","Building on this insight, we propose the $\\textbf{S}$elf-$\\textbf{I}$mitative $\\textbf{R}$einforcement $\\textbf{L}$earning ($\\textbf{SIRL}$) framework, where the RL algorithm also imitates potentially task-relevant trajectories.","Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return.","As a result, our proposed algorithm achieves 120% performance improvement on the challenging HumanoidBench with 5% extra computation overhead.","With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully."],"url":"http://arxiv.org/abs/2502.17322v1"}
{"created":"2025-02-24 16:50:55","title":"HIPPO: Enhancing the Table Understanding Capability of Large Language Models through Hybrid-Modal Preference Optimization","abstract":"Tabular data contains rich structural semantics and plays a crucial role in organizing and manipulating information. To better capture these structural semantics, this paper introduces the HybrId-modal Preference oPtimizatiOn (HIPPO) model, which represents tables using both text and image, and optimizes MLLMs to effectively learn more comprehensive table information from these multiple modalities. Specifically, HIPPO samples model responses from hybrid-modal table representations and designs a modality-consistent sampling strategy to enhance response diversity and mitigate modality bias during DPO training. Experimental results on table question answering and table fact verification tasks demonstrate the effectiveness of HIPPO, achieving a 4% improvement over various table reasoning models. Further analysis reveals that HIPPO not only enhances reasoning abilities based on unimodal table representations but also facilitates the extraction of crucial and distinct semantics from different modal representations. All data and codes are available at https://github.com/NEUIR/HIPPO.","sentences":["Tabular data contains rich structural semantics and plays a crucial role in organizing and manipulating information.","To better capture these structural semantics, this paper introduces the HybrId-modal Preference oPtimizatiOn (HIPPO) model, which represents tables using both text and image, and optimizes MLLMs to effectively learn more comprehensive table information from these multiple modalities.","Specifically, HIPPO samples model responses from hybrid-modal table representations and designs a modality-consistent sampling strategy to enhance response diversity and mitigate modality bias during DPO training.","Experimental results on table question answering and table fact verification tasks demonstrate the effectiveness of HIPPO, achieving a 4% improvement over various table reasoning models.","Further analysis reveals that HIPPO not only enhances reasoning abilities based on unimodal table representations but also facilitates the extraction of crucial and distinct semantics from different modal representations.","All data and codes are available at https://github.com/NEUIR/HIPPO."],"url":"http://arxiv.org/abs/2502.17315v1"}
{"created":"2025-02-24 16:44:20","title":"Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver Assistance Systems","abstract":"Accurate environmental perception is critical for advanced driver assistance systems (ADAS). Light detection and ranging (LiDAR) systems play a crucial role in ADAS; they can reliably detect obstacles and help ensure traffic safety. Existing research on LiDAR sensing has demonstrated that adapting the LiDAR's resolution and range based on environmental characteristics can improve machine perception. However, current adaptive LiDAR approaches for ADAS have not explored the possibility of combining the perception abilities of the vehicle and the human driver, which can potentially further enhance the detection performance. In this paper, we propose a novel system that adapts LiDAR characteristics to human driver's visual perception to enhance LiDAR sensing outside human's field of view. We develop a proof-of-concept prototype of the system in the virtual environment CARLA. Our system integrates real-time data on the driver's gaze to identify regions in the environment that the driver is monitoring. This allows the system to optimize LiDAR resources by dynamically increasing the LiDAR's range and resolution in peripheral areas that the driver may not be attending to. Our simulations show that this gaze-aware LiDAR enhances detection performance compared to a baseline standalone LiDAR, particularly in challenging environmental conditions like fog. Our hybrid human-machine sensing approach potentially offers improved safety and situational awareness in real-time driving scenarios for ADAS applications.","sentences":["Accurate environmental perception is critical for advanced driver assistance systems (ADAS).","Light detection and ranging (LiDAR) systems play a crucial role in ADAS; they can reliably detect obstacles and help ensure traffic safety.","Existing research on LiDAR sensing has demonstrated that adapting the LiDAR's resolution and range based on environmental characteristics can improve machine perception.","However, current adaptive LiDAR approaches for ADAS have not explored the possibility of combining the perception abilities of the vehicle and the human driver, which can potentially further enhance the detection performance.","In this paper, we propose a novel system that adapts LiDAR characteristics to human driver's visual perception to enhance LiDAR sensing outside human's field of view.","We develop a proof-of-concept prototype of the system in the virtual environment CARLA.","Our system integrates real-time data on the driver's gaze to identify regions in the environment that the driver is monitoring.","This allows the system to optimize LiDAR resources by dynamically increasing the LiDAR's range and resolution in peripheral areas that the driver may not be attending to.","Our simulations show that this gaze-aware LiDAR enhances detection performance compared to a baseline standalone LiDAR, particularly in challenging environmental conditions like fog.","Our hybrid human-machine sensing approach potentially offers improved safety and situational awareness in real-time driving scenarios for ADAS applications."],"url":"http://arxiv.org/abs/2502.17309v1"}
{"created":"2025-02-24 16:43:05","title":"Implicit Word Reordering with Knowledge Distillation for Cross-Lingual Dependency Parsing","abstract":"Word order difference between source and target languages is a major obstacle to cross-lingual transfer, especially in the dependency parsing task. Current works are mostly based on order-agnostic models or word reordering to mitigate this problem. However, such methods either do not leverage grammatical information naturally contained in word order or are computationally expensive as the permutation space grows exponentially with the sentence length. Moreover, the reordered source sentence with an unnatural word order may be a form of noising that harms the model learning. To this end, we propose an Implicit Word Reordering framework with Knowledge Distillation (IWR-KD). This framework is inspired by that deep networks are good at learning feature linearization corresponding to meaningful data transformation, e.g. word reordering. To realize this idea, we introduce a knowledge distillation framework composed of a word-reordering teacher model and a dependency parsing student model. We verify our proposed method on Universal Dependency Treebanks across 31 different languages and show it outperforms a series of competitors, together with experimental analysis to illustrate how our method works towards training a robust parser.","sentences":["Word order difference between source and target languages is a major obstacle to cross-lingual transfer, especially in the dependency parsing task.","Current works are mostly based on order-agnostic models or word reordering to mitigate this problem.","However, such methods either do not leverage grammatical information naturally contained in word order or are computationally expensive as the permutation space grows exponentially with the sentence length.","Moreover, the reordered source sentence with an unnatural word order may be a form of noising that harms the model learning.","To this end, we propose an Implicit Word Reordering framework with Knowledge Distillation (IWR-KD).","This framework is inspired by that deep networks are good at learning feature linearization corresponding to meaningful data transformation, e.g. word reordering.","To realize this idea, we introduce a knowledge distillation framework composed of a word-reordering teacher model and a dependency parsing student model.","We verify our proposed method on Universal Dependency Treebanks across 31 different languages and show it outperforms a series of competitors, together with experimental analysis to illustrate how our method works towards training a robust parser."],"url":"http://arxiv.org/abs/2502.17308v1"}
{"created":"2025-02-24 16:25:25","title":"Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts","abstract":"This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts. All data and code are available at https://github.com/NEUIR/M2RAG.","sentences":["This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents.","The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking.","All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling.","To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts.","Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts.","All data and code are available at https://github.com/NEUIR/M2RAG."],"url":"http://arxiv.org/abs/2502.17297v1"}
{"created":"2025-02-24 16:16:01","title":"GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow","abstract":"Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.","sentences":["Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community.","In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation.","Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces.","GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods.","Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR).","Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA."],"url":"http://arxiv.org/abs/2502.17288v1"}
{"created":"2025-02-24 15:51:42","title":"MonoTODia: Translating Monologue Requests to Task-Oriented Dialogues","abstract":"Data scarcity is one of the main problems when it comes to real-world applications of transformer-based models. This is especially evident for task-oriented dialogue (TOD) systems, which require specialized datasets, that are usually not readily available. This can hinder companies from adding TOD systems to their services. This study therefore investigates a novel approach to sourcing annotated dialogues from existing German monologue material. Focusing on a real-world example, we investigate whether these monologues can be transformed into dialogue formats suitable for training TOD systems. We show the approach with the concrete example of a company specializing in travel bookings via e-mail. We fine-tune state-of-the-art Large Language Models for the task of rewriting e-mails as dialogues and annotating them. To ensure the quality and validity of the generated data, we employ crowd workers to evaluate the dialogues across multiple criteria and to provide gold-standard annotations for the test dataset. We further evaluate the usefulness of the dialogues for training TOD systems. Our evaluation shows that the dialogues and annotations are of high quality and can serve as a valuable starting point for training TOD systems. Finally, we make the annotated dataset publicly available to foster future research.","sentences":["Data scarcity is one of the main problems when it comes to real-world applications of transformer-based models.","This is especially evident for task-oriented dialogue (TOD) systems, which require specialized datasets, that are usually not readily available.","This can hinder companies from adding TOD systems to their services.","This study therefore investigates a novel approach to sourcing annotated dialogues from existing German monologue material.","Focusing on a real-world example, we investigate whether these monologues can be transformed into dialogue formats suitable for training TOD systems.","We show the approach with the concrete example of a company specializing in travel bookings via e-mail.","We fine-tune state-of-the-art Large Language Models for the task of rewriting e-mails as dialogues and annotating them.","To ensure the quality and validity of the generated data, we employ crowd workers to evaluate the dialogues across multiple criteria and to provide gold-standard annotations for the test dataset.","We further evaluate the usefulness of the dialogues for training TOD systems.","Our evaluation shows that the dialogues and annotations are of high quality and can serve as a valuable starting point for training TOD systems.","Finally, we make the annotated dataset publicly available to foster future research."],"url":"http://arxiv.org/abs/2502.17268v1"}
{"created":"2025-02-24 15:44:02","title":"Robust Federated Learning in Unreliable Wireless Networks: A Client Selection Approach","abstract":"Federated learning (FL) has emerged as a promising distributed learning paradigm for training deep neural networks (DNNs) at the wireless edge, but its performance can be severely hindered by unreliable wireless transmission and inherent data heterogeneity among clients. Existing solutions primarily address these challenges by incorporating wireless resource optimization strategies, often focusing on uplink resource allocation across clients under the assumption of homogeneous client-server network standards. However, these approaches overlooked the fact that mobile clients may connect to the server via diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized configurations, limiting the flexibility of server-side modifications and restricting applicability in real-world commercial networks. This paper presents a novel theoretical analysis about how transmission failures in unreliable networks distort the effective label distributions of local samples, causing deviations from the global data distribution and introducing convergence bias in FL. Our analysis reveals that a carefully designed client selection strategy can mitigate biases induced by network unreliability and data heterogeneity. Motivated by this insight, we propose FedCote, a client selection approach that optimizes client selection probabilities without relying on wireless resource scheduling. Experimental results demonstrate the robustness of FedCote in DNN-based classification tasks under unreliable networks with frequent transmission failures.","sentences":["Federated learning (FL) has emerged as a promising distributed learning paradigm for training deep neural networks (DNNs) at the wireless edge, but its performance can be severely hindered by unreliable wireless transmission and inherent data heterogeneity among clients.","Existing solutions primarily address these challenges by incorporating wireless resource optimization strategies, often focusing on uplink resource allocation across clients under the assumption of homogeneous client-server network standards.","However, these approaches overlooked the fact that mobile clients may connect to the server via diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized configurations, limiting the flexibility of server-side modifications and restricting applicability in real-world commercial networks.","This paper presents a novel theoretical analysis about how transmission failures in unreliable networks distort the effective label distributions of local samples, causing deviations from the global data distribution and introducing convergence bias in FL.","Our analysis reveals that a carefully designed client selection strategy can mitigate biases induced by network unreliability and data heterogeneity.","Motivated by this insight, we propose FedCote, a client selection approach that optimizes client selection probabilities without relying on wireless resource scheduling.","Experimental results demonstrate the robustness of FedCote in DNN-based classification tasks under unreliable networks with frequent transmission failures."],"url":"http://arxiv.org/abs/2502.17260v1"}
{"created":"2025-02-24 15:39:14","title":"VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing","abstract":"Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios. Our code, data, and demos are available at https://knightyxp.github.io/VideoGrain_project_page/","sentences":["Recent advancements in diffusion models have significantly improved video generation and editing capabilities.","However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge.","The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model.","To address these difficulties, we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content.","We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention.","Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention.","Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios.","Our code, data, and demos are available at https://knightyxp.github.io/VideoGrain_project_page/"],"url":"http://arxiv.org/abs/2502.17258v1"}
{"created":"2025-02-24 15:34:09","title":"MULTITAT: Benchmarking Multilingual Table-and-Text Question Answering","abstract":"Question answering on the hybrid context of tables and text (TATQA) is a critical task, with broad applications in data-intensive domains. However, existing TATQA datasets are limited to English, leading to several drawbacks: (i) They overlook the challenges of multilingual TAT-QA and cannot assess model performance in the multilingual setting. (ii) They do not reflect real-world scenarios where tables and texts frequently appear in non-English languages. To address the limitations, we propose the first multilingual TATQA dataset (MULTITAT). Specifically, we sample data from 3 mainstream TATQA datasets and translate it into 10 diverse languages. To align the model TATQA capabilities in English with other languages, we develop a baseline, Ours. Experimental results reveal that the performance on non-English data in MULTITAT drops by an average of 19.4% compared to English, proving the necessity of MULTITAT. We further analyze the reasons for this performance gap. Furthermore, Ours outperforms other baselines by an average of 3.3, demonstrating its effectiveness.","sentences":["Question answering on the hybrid context of tables and text (TATQA) is a critical task, with broad applications in data-intensive domains.","However, existing TATQA datasets are limited to English, leading to several drawbacks: (i) They overlook the challenges of multilingual TAT-QA and cannot assess model performance in the multilingual setting.","(ii) They do not reflect real-world scenarios where tables and texts frequently appear in non-English languages.","To address the limitations, we propose the first multilingual TATQA dataset (MULTITAT).","Specifically, we sample data from 3 mainstream TATQA datasets and translate it into 10 diverse languages.","To align the model TATQA capabilities in English with other languages, we develop a baseline, Ours.","Experimental results reveal that the performance on non-English data in MULTITAT drops by an average of 19.4% compared to English, proving the necessity of MULTITAT.","We further analyze the reasons for this performance gap.","Furthermore, Ours outperforms other baselines by an average of 3.3, demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2502.17253v1"}
{"created":"2025-02-24 15:28:55","title":"CAR-LOAM: Color-Assisted Robust LiDAR Odometry and Mapping","abstract":"In this letter, we propose a color-assisted robust framework for accurate LiDAR odometry and mapping (LOAM). Simultaneously receiving data from both the LiDAR and the camera, the framework utilizes the color information from the camera images to colorize the LiDAR point clouds and then performs iterative pose optimization. For each LiDAR scan, the edge and planar features are extracted and colored using the corresponding image and then matched to a global map. Specifically, we adopt a perceptually uniform color difference weighting strategy to exclude color correspondence outliers and a robust error metric based on the Welsch's function to mitigate the impact of positional correspondence outliers during the pose optimization process. As a result, the system achieves accurate localization and reconstructs dense, accurate, colored and three-dimensional (3D) maps of the environment. Thorough experiments with challenging scenarios, including complex forests and a campus, show that our method provides higher robustness and accuracy compared with current state-of-the-art methods.","sentences":["In this letter, we propose a color-assisted robust framework for accurate LiDAR odometry and mapping (LOAM).","Simultaneously receiving data from both the LiDAR and the camera, the framework utilizes the color information from the camera images to colorize the LiDAR point clouds and then performs iterative pose optimization.","For each LiDAR scan, the edge and planar features are extracted and colored using the corresponding image and then matched to a global map.","Specifically, we adopt a perceptually uniform color difference weighting strategy to exclude color correspondence outliers and a robust error metric based on the Welsch's function to mitigate the impact of positional correspondence outliers during the pose optimization process.","As a result, the system achieves accurate localization and reconstructs dense, accurate, colored and three-dimensional (3D) maps of the environment.","Thorough experiments with challenging scenarios, including complex forests and a campus, show that our method provides higher robustness and accuracy compared with current state-of-the-art methods."],"url":"http://arxiv.org/abs/2502.17249v1"}
{"created":"2025-02-24 15:21:02","title":"UNB StepUP: A footStep database for gait analysis and recognition using Underfoot Pressure","abstract":"Gait refers to the patterns of limb movement generated during walking, which are unique to each individual due to both physical and behavioural traits. Walking patterns have been widely studied in biometrics, biomechanics, sports, and rehabilitation. While traditional methods rely on video and motion capture, advances in underfoot pressure sensing technology now offer deeper insights into gait. However, underfoot pressures during walking remain underexplored due to the lack of large, publicly accessible datasets. To address this, the UNB StepUP database was created, featuring gait pressure data collected with high-resolution pressure sensing tiles (4 sensors/cm\\textsuperscript{2}, 1.2m by 3.6m). Its first release, UNB StepUP-P150, includes over 200,000 footsteps from 150 individuals across various walking speeds (preferred, slow-to-stop, fast, and slow) and footwear types (barefoot, standard shoes, and two personal shoes). As the largest and most comprehensive dataset of its kind, it supports biometric gait recognition while presenting new research opportunities in biomechanics and deep learning. The UNB StepUP-P150 dataset sets a new benchmark for pressure-based gait analysis and recognition.","sentences":["Gait refers to the patterns of limb movement generated during walking, which are unique to each individual due to both physical and behavioural traits.","Walking patterns have been widely studied in biometrics, biomechanics, sports, and rehabilitation.","While traditional methods rely on video and motion capture, advances in underfoot pressure sensing technology now offer deeper insights into gait.","However, underfoot pressures during walking remain underexplored due to the lack of large, publicly accessible datasets.","To address this, the UNB StepUP database was created, featuring gait pressure data collected with high-resolution pressure sensing tiles (4 sensors/cm\\textsuperscript{2}, 1.2m by 3.6m).","Its first release, UNB StepUP-P150, includes over 200,000 footsteps from 150 individuals across various walking speeds (preferred, slow-to-stop, fast, and slow) and footwear types (barefoot, standard shoes, and two personal shoes).","As the largest and most comprehensive dataset of its kind, it supports biometric gait recognition while presenting new research opportunities in biomechanics and deep learning.","The UNB StepUP-P150 dataset sets a new benchmark for pressure-based gait analysis and recognition."],"url":"http://arxiv.org/abs/2502.17244v1"}
{"created":"2025-02-24 15:16:34","title":"Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction","abstract":"We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio","sentences":["We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation.","It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities.","Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz.","This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information.","To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics.","To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling.","Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency.","The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities.","Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio"],"url":"http://arxiv.org/abs/2502.17239v1"}
{"created":"2025-02-24 15:14:55","title":"MegaLoc: One Retrieval to Place Them All","abstract":"Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these tasks, and are known to fail when the requirements slightly change or when they meet out-of-distribution data. In this paper we combine a variety of existing methods, training techniques, and datasets to train a retrieval model, called MegaLoc, that is performant on multiple tasks. We find that MegaLoc (1) achieves state of the art on a large number of Visual Place Recognition datasets, (2) impressive results on common Landmark Retrieval datasets, and (3) sets a new state of the art for Visual Localization on the LaMAR datasets, where we only changed the retrieval method to the existing localization pipeline. The code for MegaLoc is available at https://github.com/gmberton/MegaLoc","sentences":["Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM.","However, existing solutions are built to specifically work for one of these tasks, and are known to fail when the requirements slightly change or when they meet out-of-distribution data.","In this paper we combine a variety of existing methods, training techniques, and datasets to train a retrieval model, called MegaLoc, that is performant on multiple tasks.","We find that MegaLoc (1) achieves state of the art on a large number of Visual Place Recognition datasets, (2) impressive results on common Landmark Retrieval datasets, and (3) sets a new state of the art for Visual Localization on the LaMAR datasets, where we only changed the retrieval method to the existing localization pipeline.","The code for MegaLoc is available at https://github.com/gmberton/MegaLoc"],"url":"http://arxiv.org/abs/2502.17237v1"}
{"created":"2025-02-24 15:04:29","title":"Electrical Load Forecasting over Multihop Smart Metering Networks with Federated Learning","abstract":"Electric load forecasting is essential for power management and stability in smart grids. This is mainly achieved via advanced metering infrastructure, where smart meters (SMs) record household energy data. Traditional machine learning (ML) methods are often employed for load forecasting but require data sharing which raises data privacy concerns. Federated learning (FL) can address this issue by running distributed ML models at local SMs without data exchange. However, current FL-based approaches struggle to achieve efficient load forecasting due to imbalanced data distribution across heterogeneous SMs. This paper presents a novel personalized federated learning (PFL) method for high-quality load forecasting in metering networks. A meta-learning-based strategy is developed to address data heterogeneity at local SMs in the collaborative training of local load forecasting models. Moreover, to minimize the load forecasting delays in our PFL model, we study a new latency optimization problem based on optimal resource allocation at SMs. A theoretical convergence analysis is also conducted to provide insights into FL design for federated load forecasting. Extensive simulations from real-world datasets show that our method outperforms existing approaches in terms of better load forecasting and reduced operational latency costs.","sentences":["Electric load forecasting is essential for power management and stability in smart grids.","This is mainly achieved via advanced metering infrastructure, where smart meters (SMs) record household energy data.","Traditional machine learning (ML) methods are often employed for load forecasting but require data sharing which raises data privacy concerns.","Federated learning (FL) can address this issue by running distributed ML models at local SMs without data exchange.","However, current FL-based approaches struggle to achieve efficient load forecasting due to imbalanced data distribution across heterogeneous SMs.","This paper presents a novel personalized federated learning (PFL) method for high-quality load forecasting in metering networks.","A meta-learning-based strategy is developed to address data heterogeneity at local SMs in the collaborative training of local load forecasting models.","Moreover, to minimize the load forecasting delays in our PFL model, we study a new latency optimization problem based on optimal resource allocation at SMs.","A theoretical convergence analysis is also conducted to provide insights into FL design for federated load forecasting.","Extensive simulations from real-world datasets show that our method outperforms existing approaches in terms of better load forecasting and reduced operational latency costs."],"url":"http://arxiv.org/abs/2502.17226v1"}
{"created":"2025-02-24 14:33:14","title":"Minimizers in Semi-Dynamic Strings","abstract":"Minimizers sampling is one of the most widely-used mechanisms for sampling strings. Let $S=S[0]\\ldots S[n-1]$ be a string over an alphabet $\\Sigma$. In addition, let $w\\geq 2$ and $k\\geq 1$ be two integers and $\\rho=(\\Sigma^k,\\leq)$ be a total order on $\\Sigma^k$. The minimizer of window $X=S[i\\mathinner{.\\,.} i+w+k-2]$ is the smallest position in $[i,i+w-1]$ where the smallest length-$k$ substring of $S[i\\mathinner{.\\,.} i+w+k-2]$ based on $\\rho$ starts. The set of minimizers for all $i\\in[0,n-w-k+1]$ is the set $\\mathcal{M}_{w,k,\\rho}(S)$ of the minimizers of $S$. The set $\\mathcal{M}_{w,k,\\rho}(S)$ can be computed in $\\mathcal{O}(n)$ time. The folklore algorithm for this computation computes the minimizer of every window in $\\mathcal{O}(1)$ amortized time using $\\mathcal{O}(w)$ working space. It is thus natural to pose the following two questions:   Question 1: Can we efficiently support other dynamic updates on the window?   Question 2: Can we improve on the $\\mathcal{O}(w)$ working space?   We answer both questions in the affirmative:   1. We term a string $X$ semi-dynamic when one is allowed to insert or delete a letter at any of its ends. We show a data structure that maintains a semi-dynamic string $X$ and supports minimizer queries in $X$ in $\\mathcal{O}(1)$ time with amortized $\\mathcal{O}(1)$ time per update operation.   2. We show that this data structure can be modified to occupy strongly sublinear space without increasing the asymptotic complexity of its operations. To the best of our knowledge, this yields the first algorithm for computing $\\mathcal{M}_{w,k,\\rho}(S)$ in $\\mathcal{O}(n)$ time using $\\mathcal{O}(\\sqrt{w})$ working space.   We complement our theoretical results with a concrete application and an experimental evaluation.","sentences":["Minimizers sampling is one of the most widely-used mechanisms for sampling strings.","Let $S=S[0]\\ldots S[n-1]$ be a string over an alphabet $\\Sigma$. In addition, let $w\\geq 2$ and $k\\geq 1$ be two integers and $\\rho=(\\Sigma^k,\\leq)$ be a total order on $\\Sigma^k$. The minimizer of window $X=S[i\\mathinner{.\\,.","}","i+w+k-2]$ is the smallest position in $[i,i+w-1]$ where the smallest length-$k$ substring of $S[i\\mathinner{.\\,.} i+w+k-2]$ based on $\\rho$ starts.","The set of minimizers for all $i\\in[0,n-w-k+1]$ is the set $\\mathcal{M}_{w,k,\\rho}(S)$ of the minimizers of $S$. The set $\\mathcal{M}_{w,k,\\rho}(S)$ can be computed in $\\mathcal{O}(n)$ time.","The folklore algorithm for this computation computes the minimizer of every window in $\\mathcal{O}(1)$ amortized time using $\\mathcal{O}(w)$ working space.","It is thus natural to pose the following two questions:   Question 1: Can we efficiently support other dynamic updates on the window?   ","Question 2: Can we improve on the $\\mathcal{O}(w)$ working space?   ","We answer both questions in the affirmative:   1.","We term a string $X$ semi-dynamic when one is allowed to insert or delete a letter at any of its ends.","We show a data structure that maintains a semi-dynamic string $X$ and supports minimizer queries in $X$ in $\\mathcal{O}(1)$ time with amortized $\\mathcal{O}(1)$ time per update operation.   ","2.","We show that this data structure can be modified to occupy strongly sublinear space without increasing the asymptotic complexity of its operations.","To the best of our knowledge, this yields the first algorithm for computing $\\mathcal{M}_{w,k,\\rho}(S)$ in $\\mathcal{O}(n)$ time using $\\mathcal{O}(\\sqrt{w})$ working space.   ","We complement our theoretical results with a concrete application and an experimental evaluation."],"url":"http://arxiv.org/abs/2502.17199v1"}
{"created":"2025-02-24 14:24:27","title":"IGDA: Interactive Graph Discovery through Large Language Model Agents","abstract":"Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.","sentences":["Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for discovery.","Instead of utilizing numerical data, LLMs utilize associated variable $\\textit{semantic metadata}$ to predict variable relationships.","Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials.","We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$ at the end of the $R$-th round.","To solve this task we propose $\\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges.","Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery.","Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component.","Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible.","Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches."],"url":"http://arxiv.org/abs/2502.17189v1"}
{"created":"2025-02-24 14:20:22","title":"Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric","abstract":"Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by assessing their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level \"novelty.\" Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric.","sentences":["Data diversity is crucial for the instruction tuning of large language models.","Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance.","However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering.","To address this, we systematically analyze 11 existing diversity measurement methods by assessing their correlation with model performance through extensive fine-tuning experiments.","Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space.","Building on this, we propose NovelSum, a new diversity metric based on sample-level \"novelty.\"","Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices.","With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric."],"url":"http://arxiv.org/abs/2502.17184v1"}
{"created":"2025-02-24 14:09:45","title":"Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch","abstract":"Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.","sentences":["Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences.","However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese.","To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training.","We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios.","Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training.","Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development."],"url":"http://arxiv.org/abs/2502.17173v1"}
{"created":"2025-02-24 13:58:42","title":"MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation","abstract":"Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.   In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. We release our benchmark to support the community developing accurate evaluation methods for multilingual RAG systems.","sentences":["Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators.","Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement.","However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances.","A native approach provides a better representation of the end user experience.   ","In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG).","Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance.","We describe our annotation process and show that it achieves high inter-annotator agreement.","We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators.","Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge).","We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs.","We release our benchmark to support the community developing accurate evaluation methods for multilingual RAG systems."],"url":"http://arxiv.org/abs/2502.17163v1"}
{"created":"2025-02-24 13:54:57","title":"A Pragmatic Note on Evaluating Generative Models with Fr\u00e9chet Inception Distance for Retinal Image Synthesis","abstract":"Fr\\'echet Inception Distance (FID), computed with an ImageNet pretrained Inception-v3 network, is widely used as a state-of-the-art evaluation metric for generative models. It assumes that feature vectors from Inception-v3 follow a multivariate Gaussian distribution and calculates the 2-Wasserstein distance based on their means and covariances. While FID effectively measures how closely synthetic data match real data in many image synthesis tasks, the primary goal in biomedical generative models is often to enrich training datasets ideally with corresponding annotations. For this purpose, the gold standard for evaluating generative models is to incorporate synthetic data into downstream task training, such as classification and segmentation, to pragmatically assess its performance. In this paper, we examine cases from retinal imaging modalities, including color fundus photography and optical coherence tomography, where FID and its related metrics misalign with task-specific evaluation goals in classification and segmentation. We highlight the limitations of using various metrics, represented by FID and its variants, as evaluation criteria for these applications and address their potential caveats in broader biomedical imaging modalities and downstream tasks.","sentences":["Fr\\'echet Inception Distance (FID), computed with an ImageNet pretrained Inception-v3 network, is widely used as a state-of-the-art evaluation metric for generative models.","It assumes that feature vectors from Inception-v3 follow a multivariate Gaussian distribution and calculates the 2-Wasserstein distance based on their means and covariances.","While FID effectively measures how closely synthetic data match real data in many image synthesis tasks, the primary goal in biomedical generative models is often to enrich training datasets ideally with corresponding annotations.","For this purpose, the gold standard for evaluating generative models is to incorporate synthetic data into downstream task training, such as classification and segmentation, to pragmatically assess its performance.","In this paper, we examine cases from retinal imaging modalities, including color fundus photography and optical coherence tomography, where FID and its related metrics misalign with task-specific evaluation goals in classification and segmentation.","We highlight the limitations of using various metrics, represented by FID and its variants, as evaluation criteria for these applications and address their potential caveats in broader biomedical imaging modalities and downstream tasks."],"url":"http://arxiv.org/abs/2502.17160v1"}
{"created":"2025-02-24 13:52:05","title":"Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation","abstract":"Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity. With the expansion in data and model size, parameter efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, we observe that existing methods designed for full fine-tuning merging fail under efficient tuning. To address the issues, we analyze from low-rank decomposition and reveal that maintaining direction and compensating for gap between singular values are crucial for efficient model merging. Consequently, we propose CoPA-Merging, a training-free parameter efficient merging method with complementary parameter adaptation. Specifically, we (1) prune parameters and construct scaling coefficients from inter-parameter relation to compensate for performance drop from task interference and (2) perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certificate the outstanding performance and generalizability of our method. Additional study and extensive analyses further showcase the effectiveness.","sentences":["Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks.","Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity.","With the expansion in data and model size, parameter efficient tuning becomes the common practice for obtaining task-specific models efficiently.","However, we observe that existing methods designed for full fine-tuning merging fail under efficient tuning.","To address the issues, we analyze from low-rank decomposition and reveal that maintaining direction and compensating for gap between singular values are crucial for efficient model merging.","Consequently, we propose CoPA-Merging, a training-free parameter efficient merging method with complementary parameter adaptation.","Specifically, we (1) prune parameters and construct scaling coefficients from inter-parameter relation to compensate for performance drop from task interference and (2) perform cross-task normalization to enhance unseen task generalization.","We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certificate the outstanding performance and generalizability of our method.","Additional study and extensive analyses further showcase the effectiveness."],"url":"http://arxiv.org/abs/2502.17159v1"}
{"created":"2025-02-24 13:51:06","title":"DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks","abstract":"Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models.","sentences":["Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data.","To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images.","Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models.","We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images).","Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation.","Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models.","Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch.","When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters.","DICEPTION provides valuable insights and a more promising solution for visual generalist models."],"url":"http://arxiv.org/abs/2502.17157v1"}
{"created":"2025-02-24 13:34:35","title":"Sentiment analysis of texts from social networks based on machine learning methods for monitoring public sentiment","abstract":"A sentiment analysis system powered by machine learning was created in this study to improve real-time social network public opinion monitoring. For sophisticated sentiment identification, the suggested approach combines cutting-edge transformer-based architectures (DistilBERT, RoBERTa) with traditional machine learning models (Logistic Regression, SVM, Naive Bayes). The system achieved an accuracy of up to 80-85% using transformer models in real-world scenarios after being tested using both deep learning techniques and standard machine learning processes on annotated social media datasets. According to experimental results, deep learning models perform noticeably better than lexicon-based and conventional rule-based classifiers, lowering misclassification rates and enhancing the ability to recognize nuances like sarcasm. According to feature importance analysis, context tokens, sentiment-bearing keywords, and part-of-speech structure are essential for precise categorization. The findings confirm that AI-driven sentiment frameworks can provide a more adaptive and efficient approach to modern sentiment challenges. Despite the system's impressive performance, issues with computing overhead, data quality, and domain-specific terminology still exist. In order to monitor opinions on a broad scale, future research will investigate improving computing performance, extending coverage to various languages, and integrating real-time streaming APIs. The results demonstrate that governments, corporations, and social researchers looking for more in-depth understanding of public mood on digital platforms can find a reliable and adaptable answer in AI-powered sentiment analysis.","sentences":["A sentiment analysis system powered by machine learning was created in this study to improve real-time social network public opinion monitoring.","For sophisticated sentiment identification, the suggested approach combines cutting-edge transformer-based architectures (DistilBERT, RoBERTa) with traditional machine learning models (Logistic Regression, SVM, Naive Bayes).","The system achieved an accuracy of up to 80-85% using transformer models in real-world scenarios after being tested using both deep learning techniques and standard machine learning processes on annotated social media datasets.","According to experimental results, deep learning models perform noticeably better than lexicon-based and conventional rule-based classifiers, lowering misclassification rates and enhancing the ability to recognize nuances like sarcasm.","According to feature importance analysis, context tokens, sentiment-bearing keywords, and part-of-speech structure are essential for precise categorization.","The findings confirm that AI-driven sentiment frameworks can provide a more adaptive and efficient approach to modern sentiment challenges.","Despite the system's impressive performance, issues with computing overhead, data quality, and domain-specific terminology still exist.","In order to monitor opinions on a broad scale, future research will investigate improving computing performance, extending coverage to various languages, and integrating real-time streaming APIs.","The results demonstrate that governments, corporations, and social researchers looking for more in-depth understanding of public mood on digital platforms can find a reliable and adaptable answer in AI-powered sentiment analysis."],"url":"http://arxiv.org/abs/2502.17143v1"}
{"created":"2025-02-24 13:29:34","title":"Analyzing a Two-Tier Disaggregated Memory Protection Scheme Based on Memory Replication","abstract":"As memory technologies continue to shrink and memory error rates increase, the demand for stronger reliability becomes increasingly critical. Fine-grain memory replication has emerged as an appealing approach to improving memory fault tolerance by augmenting conventional memory protection based on error-correcting codes with an additional layer of redundancy that replicates data across independent failure domains, such as replicating memory pages across different NUMA sockets. This method can tolerate a broad spectrum of memory errors, from individual memory cell failures to more complex memory controller failures. However, applying memory replication without a holistic consideration of the interaction between error-correcting codes and replication can result in redundant duplication and unnecessary storage overhead. We propose Replication-Aware Memory-error Protection (RAMP), a model that helps explore error protection strategies to improve the storage efficiency of memory protection in memory systems that utilize memory replication for performance and availability. We use RAMP to determine a protection strategy that can lower the storage cost of individual replicas while still ensuring robust protection through the collective protection conferred by multiple replicas. Our evaluation shows that a solution derived with RAMP enhances the storage efficiency of a state-of-the-art memory protection mechanism when paired with rack-level replication for disaggregated memory. Specifically, we can reduce the storage cost of memory protection from 27% down to 17.7% with minimal performance overhead.","sentences":["As memory technologies continue to shrink and memory error rates increase, the demand for stronger reliability becomes increasingly critical.","Fine-grain memory replication has emerged as an appealing approach to improving memory fault tolerance by augmenting conventional memory protection based on error-correcting codes with an additional layer of redundancy that replicates data across independent failure domains, such as replicating memory pages across different NUMA sockets.","This method can tolerate a broad spectrum of memory errors, from individual memory cell failures to more complex memory controller failures.","However, applying memory replication without a holistic consideration of the interaction between error-correcting codes and replication can result in redundant duplication and unnecessary storage overhead.","We propose Replication-Aware Memory-error Protection (RAMP), a model that helps explore error protection strategies to improve the storage efficiency of memory protection in memory systems that utilize memory replication for performance and availability.","We use RAMP to determine a protection strategy that can lower the storage cost of individual replicas while still ensuring robust protection through the collective protection conferred by multiple replicas.","Our evaluation shows that a solution derived with RAMP enhances the storage efficiency of a state-of-the-art memory protection mechanism when paired with rack-level replication for disaggregated memory.","Specifically, we can reduce the storage cost of memory protection from 27% down to 17.7% with minimal performance overhead."],"url":"http://arxiv.org/abs/2502.17138v1"}
{"created":"2025-02-24 13:27:46","title":"Evaluating the Effectiveness of Large Language Models in Automated News Article Summarization","abstract":"The automation of news analysis and summarization presents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in today's information society. Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily comprehensible summaries, offering an effective solution to the problem of information overload and providing users with a quick overview of relevant information. A particularly significant application of this technology lies in supply chain risk analysis. Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, including compliance with laws and regulations, risk management, and maintaining supply chain resilience. This paper develops an automated news summarization system for supply chain risk analysis using LLMs. The proposed solution aggregates news from various sources, summarizes them using LLMs, and presents the condensed information to users in a clear and concise format. This approach enables companies to optimize their information processing and make informed decisions. Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of supply chain risk analysis? (2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summarization quality? In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further. Our results demonstrate that LLMs, particularly Few-Shot GPT-4o mini, offer significant improvements in summary quality and risk identification.","sentences":["The automation of news analysis and summarization presents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in today's information society.","Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily comprehensible summaries, offering an effective solution to the problem of information overload and providing users with a quick overview of relevant information.","A particularly significant application of this technology lies in supply chain risk analysis.","Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, including compliance with laws and regulations, risk management, and maintaining supply chain resilience.","This paper develops an automated news summarization system for supply chain risk analysis using LLMs.","The proposed solution aggregates news from various sources, summarizes them using LLMs, and presents the condensed information to users in a clear and concise format.","This approach enables companies to optimize their information processing and make informed decisions.","Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of supply chain risk analysis?","(2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summarization quality?","In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further.","Our results demonstrate that LLMs, particularly Few-Shot GPT-4o mini, offer significant improvements in summary quality and risk identification."],"url":"http://arxiv.org/abs/2502.17136v1"}
{"created":"2025-02-24 13:19:59","title":"Low-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space","abstract":"Embedding tree-like data, from hierarchies to ontologies and taxonomies, forms a well-studied problem for representing knowledge across many domains. Hyperbolic geometry provides a natural solution for embedding trees, with vastly superior performance over Euclidean embeddings. Recent literature has shown that hyperbolic tree embeddings can even be placed on top of neural networks for hierarchical knowledge integration in deep learning settings. For all applications, a faithful embedding of trees is needed, with combinatorial constructions emerging as the most effective direction. This paper identifies and solves two key limitations of existing works. First, the combinatorial construction hinges on finding highly separated points on a hypersphere, a notoriously difficult problem. Current approaches achieve poor separation, degrading the quality of the corresponding hyperbolic embedding. We propose highly separated Delaunay tree embeddings (HS-DTE), which integrates angular separation in a generalized formulation of Delaunay embeddings, leading to lower embedding distortion. Second, low-distortion requires additional precision. The current approach for increasing precision is to use multiple precision arithmetic, which renders the embeddings useless on GPUs in deep learning settings. We reformulate the combinatorial construction using floating point expansion arithmetic, leading to superior embedding quality while retaining utility on accelerated hardware.","sentences":["Embedding tree-like data, from hierarchies to ontologies and taxonomies, forms a well-studied problem for representing knowledge across many domains.","Hyperbolic geometry provides a natural solution for embedding trees, with vastly superior performance over Euclidean embeddings.","Recent literature has shown that hyperbolic tree embeddings can even be placed on top of neural networks for hierarchical knowledge integration in deep learning settings.","For all applications, a faithful embedding of trees is needed, with combinatorial constructions emerging as the most effective direction.","This paper identifies and solves two key limitations of existing works.","First, the combinatorial construction hinges on finding highly separated points on a hypersphere, a notoriously difficult problem.","Current approaches achieve poor separation, degrading the quality of the corresponding hyperbolic embedding.","We propose highly separated Delaunay tree embeddings (HS-DTE), which integrates angular separation in a generalized formulation of Delaunay embeddings, leading to lower embedding distortion.","Second, low-distortion requires additional precision.","The current approach for increasing precision is to use multiple precision arithmetic, which renders the embeddings useless on GPUs in deep learning settings.","We reformulate the combinatorial construction using floating point expansion arithmetic, leading to superior embedding quality while retaining utility on accelerated hardware."],"url":"http://arxiv.org/abs/2502.17130v1"}
{"created":"2025-02-24 13:03:19","title":"Adversarial Training for Defense Against Label Poisoning Attacks","abstract":"As machine learning models grow in complexity and increasingly rely on publicly sourced data, such as the human-annotated labels used in training large language models, they become more vulnerable to label poisoning attacks. These attacks, in which adversaries subtly alter the labels within a training dataset, can severely degrade model performance, posing significant risks in critical applications. In this paper, we propose FLORAL, a novel adversarial training defense strategy based on support vector machines (SVMs) to counter these threats. Utilizing a bilevel optimization framework, we cast the training process as a non-zero-sum Stackelberg game between an attacker, who strategically poisons critical training labels, and the model, which seeks to recover from such attacks. Our approach accommodates various model architectures and employs a projected gradient descent algorithm with kernel SVMs for adversarial training. We provide a theoretical analysis of our algorithm's convergence properties and empirically evaluate FLORAL's effectiveness across diverse classification tasks. Compared to robust baselines and foundation models such as RoBERTa, FLORAL consistently achieves higher robust accuracy under increasing attacker budgets. These results underscore the potential of FLORAL to enhance the resilience of machine learning models against label poisoning threats, thereby ensuring robust classification in adversarial settings.","sentences":["As machine learning models grow in complexity and increasingly rely on publicly sourced data, such as the human-annotated labels used in training large language models, they become more vulnerable to label poisoning attacks.","These attacks, in which adversaries subtly alter the labels within a training dataset, can severely degrade model performance, posing significant risks in critical applications.","In this paper, we propose FLORAL, a novel adversarial training defense strategy based on support vector machines (SVMs) to counter these threats.","Utilizing a bilevel optimization framework, we cast the training process as a non-zero-sum Stackelberg game between an attacker, who strategically poisons critical training labels, and the model, which seeks to recover from such attacks.","Our approach accommodates various model architectures and employs a projected gradient descent algorithm with kernel SVMs for adversarial training.","We provide a theoretical analysis of our algorithm's convergence properties and empirically evaluate FLORAL's effectiveness across diverse classification tasks.","Compared to robust baselines and foundation models such as RoBERTa, FLORAL consistently achieves higher robust accuracy under increasing attacker budgets.","These results underscore the potential of FLORAL to enhance the resilience of machine learning models against label poisoning threats, thereby ensuring robust classification in adversarial settings."],"url":"http://arxiv.org/abs/2502.17121v1"}
{"created":"2025-02-24 13:03:00","title":"Semantic-Aware Dynamic and Distributed Power Allocation: a Multi-UAV Area Coverage Use Case","abstract":"The advancement towards 6G technology leverages improvements in aerial-terrestrial networking, where one of the critical challenges is the efficient allocation of transmit power. Although existing studies have shown commendable performance in addressing this challenge, a revolutionary breakthrough is anticipated to meet the demands and dynamism of 6G. Potential solutions include: 1) semantic communication and orchestration, which transitions the focus from mere transmission of bits to the communication of intended meanings of data and their integration into the network orchestration process; and 2) distributed machine learning techniques to develop adaptable and scalable solutions. In this context, this paper introduces a power allocation framework specifically designed for semantic-aware networks. The framework addresses a scenario involving multiple Unmanned Aerial Vehicles (UAVs) that collaboratively transmit observations over a multi-channel uplink medium to a central server, aiming to maximise observation quality. To tackle this problem, we present the Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) algorithm, which utilizes the data quality of observing areas as reward feedback during the training phase, thereby constituting a semantic-aware learning mechanism. Simulation results substantiate the efficacy and scalability of our approach, demonstrating its superior performance compared to traditional bit-oriented learning and heuristic algorithms.","sentences":["The advancement towards 6G technology leverages improvements in aerial-terrestrial networking, where one of the critical challenges is the efficient allocation of transmit power.","Although existing studies have shown commendable performance in addressing this challenge, a revolutionary breakthrough is anticipated to meet the demands and dynamism of 6G. Potential solutions include: 1) semantic communication and orchestration, which transitions the focus from mere transmission of bits to the communication of intended meanings of data and their integration into the network orchestration process; and 2) distributed machine learning techniques to develop adaptable and scalable solutions.","In this context, this paper introduces a power allocation framework specifically designed for semantic-aware networks.","The framework addresses a scenario involving multiple Unmanned Aerial Vehicles (UAVs) that collaboratively transmit observations over a multi-channel uplink medium to a central server, aiming to maximise observation quality.","To tackle this problem, we present the Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) algorithm, which utilizes the data quality of observing areas as reward feedback during the training phase, thereby constituting a semantic-aware learning mechanism.","Simulation results substantiate the efficacy and scalability of our approach, demonstrating its superior performance compared to traditional bit-oriented learning and heuristic algorithms."],"url":"http://arxiv.org/abs/2502.17120v1"}
{"created":"2025-02-24 13:01:33","title":"Diffusion Models for Tabular Data: Challenges, Current Progress, and Future Directions","abstract":"In recent years, generative models have achieved remarkable performance across diverse applications, including image generation, text synthesis, audio creation, video generation, and data augmentation. Diffusion models have emerged as superior alternatives to Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) by addressing their limitations, such as training instability, mode collapse, and poor representation of multimodal distributions. This success has spurred widespread research interest. In the domain of tabular data, diffusion models have begun to showcase similar advantages over GANs and VAEs, achieving significant performance breakthroughs and demonstrating their potential for addressing unique challenges in tabular data modeling. However, while domains like images and time series have numerous surveys summarizing advancements in diffusion models, there remains a notable gap in the literature for tabular data. Despite the increasing interest in diffusion models for tabular data, there has been little effort to systematically review and summarize these developments. This lack of a dedicated survey limits a clear understanding of the challenges, progress, and future directions in this critical area. This survey addresses this gap by providing a comprehensive review of diffusion models for tabular data. Covering works from June 2015, when diffusion models emerged, to December 2024, we analyze nearly all relevant studies, with updates maintained in a \\href{https://github.com/Diffusion-Model-Leiden/awesome-diffusion-models-for-tabular-data}{GitHub repository}. Assuming readers possess foundational knowledge of statistics and diffusion models, we employ mathematical formulations to deliver a rigorous and detailed review, aiming to promote developments in this emerging and exciting area.","sentences":["In recent years, generative models have achieved remarkable performance across diverse applications, including image generation, text synthesis, audio creation, video generation, and data augmentation.","Diffusion models have emerged as superior alternatives to Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) by addressing their limitations, such as training instability, mode collapse, and poor representation of multimodal distributions.","This success has spurred widespread research interest.","In the domain of tabular data, diffusion models have begun to showcase similar advantages over GANs and VAEs, achieving significant performance breakthroughs and demonstrating their potential for addressing unique challenges in tabular data modeling.","However, while domains like images and time series have numerous surveys summarizing advancements in diffusion models, there remains a notable gap in the literature for tabular data.","Despite the increasing interest in diffusion models for tabular data, there has been little effort to systematically review and summarize these developments.","This lack of a dedicated survey limits a clear understanding of the challenges, progress, and future directions in this critical area.","This survey addresses this gap by providing a comprehensive review of diffusion models for tabular data.","Covering works from June 2015, when diffusion models emerged, to December 2024, we analyze nearly all relevant studies, with updates maintained in a \\href{https://github.com/Diffusion-Model-Leiden/awesome-diffusion-models-for-tabular-data}{GitHub repository}.","Assuming readers possess foundational knowledge of statistics and diffusion models, we employ mathematical formulations to deliver a rigorous and detailed review, aiming to promote developments in this emerging and exciting area."],"url":"http://arxiv.org/abs/2502.17119v1"}
{"created":"2025-02-24 12:31:28","title":"Generative Models in Decision Making: A Survey","abstract":"In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in their integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals. This paper presents a comprehensive review of the application of generative models in decision-making tasks. We classify seven fundamental types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. Regarding their applications, we categorize their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making. Furthermore, we examine the deployment of these models across five critical real-world decision-making scenarios. Finally, we summarize the strengths and limitations of current approaches and propose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive models.","sentences":["In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in their integration into decision-making processes.","Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals.","This paper presents a comprehensive review of the application of generative models in decision-making tasks.","We classify seven fundamental types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models.","Regarding their applications, we categorize their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making.","Furthermore, we examine the deployment of these models across five critical real-world decision-making scenarios.","Finally, we summarize the strengths and limitations of current approaches and propose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive models."],"url":"http://arxiv.org/abs/2502.17100v1"}
{"created":"2025-02-24 12:29:16","title":"Improved Diffusion-based Generative Model with Better Adversarial Robustness","abstract":"Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation. To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs. Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well. Based on these insights, we propose to conduct efficient AT on both DPM and CM. Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models. The code is available at https://github.com/kugwzk/AT_Diff.","sentences":["Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks.","However, their training and sampling processes suffer from the issue of distribution mismatch.","During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation.","To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs.","Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue.","Fortunately, this issue can be mitigated by AT as well.","Based on these insights, we propose to conduct efficient AT on both DPM and CM.","Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models.","The code is available at https://github.com/kugwzk/AT_Diff."],"url":"http://arxiv.org/abs/2502.17099v1"}
{"created":"2025-02-24 12:15:07","title":"Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI","abstract":"We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to attain competitive results with fewer tokens. Key advancements include QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding. A three-stage training strategy further optimizes learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR extraction, and general multimodal reasoning. Our results highlight that high performance can be achieved through model design and training strategy rather than sheer data volume, making Shakti an efficient solution for enterprise-scale multimodal tasks.","sentences":["We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning.","While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to attain competitive results with fewer tokens.","Key advancements include QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding.","A three-stage training strategy further optimizes learning efficiency.","Evaluations show that Shakti-Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR extraction, and general multimodal reasoning.","Our results highlight that high performance can be achieved through model design and training strategy rather than sheer data volume, making Shakti an efficient solution for enterprise-scale multimodal tasks."],"url":"http://arxiv.org/abs/2502.17092v1"}
{"created":"2025-02-24 12:14:05","title":"WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring Texts","abstract":"Humans are influenced by how information is presented, a phenomenon known as the framing effect. Previous work has shown that LLMs may also be susceptible to framing but has done so on synthetic data and did not compare to human behavior. We introduce WildFrame, a dataset for evaluating LLM responses to positive and negative framing, in naturally-occurring sentences, and compare humans on the same data. WildFrame consists of 1,000 texts, first selecting real-world statements with clear sentiment, then reframing them in either positive or negative light, and lastly, collecting human sentiment annotations. By evaluating eight state-of-the-art LLMs on WildFrame, we find that all models exhibit framing effects similar to humans ($r\\geq0.57$), with both humans and models being more influenced by positive rather than negative reframing. Our findings benefit model developers, who can either harness framing or mitigate its effects, depending on the downstream application.","sentences":["Humans are influenced by how information is presented, a phenomenon known as the framing effect.","Previous work has shown that LLMs may also be susceptible to framing but has done so on synthetic data and did not compare to human behavior.","We introduce WildFrame, a dataset for evaluating LLM responses to positive and negative framing, in naturally-occurring sentences, and compare humans on the same data.","WildFrame consists of 1,000 texts, first selecting real-world statements with clear sentiment, then reframing them in either positive or negative light, and lastly, collecting human sentiment annotations.","By evaluating eight state-of-the-art LLMs on WildFrame, we find that all models exhibit framing effects similar to humans ($r\\geq0.57$), with both humans and models being more influenced by positive rather than negative reframing.","Our findings benefit model developers, who can either harness framing or mitigate its effects, depending on the downstream application."],"url":"http://arxiv.org/abs/2502.17091v1"}
{"created":"2025-02-24 11:52:35","title":"Forgetting Any Data at Any Time: A Theoretically Certified Unlearning Framework for Vertical Federated Learning","abstract":"Privacy concerns in machine learning are heightened by regulations such as the GDPR, which enforces the \"right to be forgotten\" (RTBF), driving the emergence of machine unlearning as a critical research field. Vertical Federated Learning (VFL) enables collaborative model training by aggregating a sample's features across distributed parties while preserving data privacy at each source. This paradigm has seen widespread adoption in healthcare, finance, and other privacy-sensitive domains. However, existing VFL systems lack robust mechanisms to comply with RTBF requirements, as unlearning methodologies for VFL remain underexplored. In this work, we introduce the first VFL framework with theoretically guaranteed unlearning capabilities, enabling the removal of any data at any time. Unlike prior approaches -- which impose restrictive assumptions on model architectures or data types for removal -- our solution is model- and data-agnostic, offering universal compatibility. Moreover, our framework supports asynchronous unlearning, eliminating the need for all parties to be simultaneously online during the forgetting process. These advancements address critical gaps in current VFL systems, ensuring compliance with RTBF while maintaining operational flexibility.We make all our implementations publicly available at https://github.com/wangln19/vertical-federated-unlearning.","sentences":["Privacy concerns in machine learning are heightened by regulations such as the GDPR, which enforces the \"right to be forgotten\" (RTBF), driving the emergence of machine unlearning as a critical research field.","Vertical Federated Learning (VFL) enables collaborative model training by aggregating a sample's features across distributed parties while preserving data privacy at each source.","This paradigm has seen widespread adoption in healthcare, finance, and other privacy-sensitive domains.","However, existing VFL systems lack robust mechanisms to comply with RTBF requirements, as unlearning methodologies for VFL remain underexplored.","In this work, we introduce the first VFL framework with theoretically guaranteed unlearning capabilities, enabling the removal of any data at any time.","Unlike prior approaches -- which impose restrictive assumptions on model architectures or data types for removal -- our solution is model- and data-agnostic, offering universal compatibility.","Moreover, our framework supports asynchronous unlearning, eliminating the need for all parties to be simultaneously online during the forgetting process.","These advancements address critical gaps in current VFL systems, ensuring compliance with RTBF while maintaining operational flexibility.","We make all our implementations publicly available at https://github.com/wangln19/vertical-federated-unlearning."],"url":"http://arxiv.org/abs/2502.17081v1"}
{"created":"2025-02-24 11:31:13","title":"A type-theoretic approach to semistrict higher categories","abstract":"Weak $\\infty$-categories are known to be more expressive than their strict counterparts, but are more difficult to work with, as constructions in such a category involve the manipulation of explicit coherence data. This motivates the search for definitions of semistrict $\\infty$-categories, where some, but not all, of the operations have been strictified.   We introduce a general framework for adding definitional equality to the type theory $\\mathsf{Catt}$, a type theory whose models correspond to globular weak $\\infty$-categories, which was introduced by Finster and Mimram. Adding equality to this theory causes the models to exhibit semistrict behaviour, trivialising some operations while leaving others weak. The framework consists of a generalisation of $\\mathsf{Catt}$ extended with an equality relation generated by an arbitrary set of equality rules $\\mathcal{R}$, which we name $\\mathsf{Catt}_{\\mathcal{R}}$. We study this framework in detail, formalising much of its metatheory in the proof assistant Agda, and studying how certain operations of $\\mathsf{Catt}$ behave in the presence of definitional equality.   We use this framework to introduce two type theories, $\\mathsf{Catt}_{\\mathsf{su}}$ and $\\mathsf{Catt}_{\\mathsf{sua}}$, which are instances of this general framework. Further, we provide terminating and confluent reduction systems that generate the equality of both systems. We therefore prove that the equality, and hence typechecking, of both theories is decidable. This is used to give an implementation of these type theories, which uses an approach inspired by normalisation by evaluation to efficiently find normal forms for terms. We further introduce a bidirectional typechecking algorithm used by the implementation which allows for terms to be defined in a convenient syntax where many arguments can be left implicit.","sentences":["Weak $\\infty$-categories are known to be more expressive than their strict counterparts, but are more difficult to work with, as constructions in such a category involve the manipulation of explicit coherence data.","This motivates the search for definitions of semistrict $\\infty$-categories, where some, but not all, of the operations have been strictified.   ","We introduce a general framework for adding definitional equality to the type theory $\\mathsf{Catt}$, a type theory whose models correspond to globular weak $\\infty$-categories, which was introduced by Finster and Mimram.","Adding equality to this theory causes the models to exhibit semistrict behaviour, trivialising some operations while leaving others weak.","The framework consists of a generalisation of $\\mathsf{Catt}$ extended with an equality relation generated by an arbitrary set of equality rules $\\mathcal{R}$, which we name $\\mathsf{Catt}_{\\mathcal{R}}$. We study this framework in detail, formalising much of its metatheory in the proof assistant Agda, and studying how certain operations of $\\mathsf{Catt}$ behave in the presence of definitional equality.   ","We use this framework to introduce two type theories, $\\mathsf{Catt}_{\\mathsf{su}}$ and $\\mathsf{Catt}_{\\mathsf{sua}}$, which are instances of this general framework.","Further, we provide terminating and confluent reduction systems that generate the equality of both systems.","We therefore prove that the equality, and hence typechecking, of both theories is decidable.","This is used to give an implementation of these type theories, which uses an approach inspired by normalisation by evaluation to efficiently find normal forms for terms.","We further introduce a bidirectional typechecking algorithm used by the implementation which allows for terms to be defined in a convenient syntax where many arguments can be left implicit."],"url":"http://arxiv.org/abs/2502.17068v1"}
{"created":"2025-02-24 11:28:00","title":"DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications","abstract":"Significant efforts have been directed towards adapting self-supervised multimodal learning for Earth observation applications. However, existing methods produce coarse patch-sized embeddings, limiting their effectiveness and integration with other modalities like LiDAR. To close this gap, we present DUNIA, an approach to learn pixel-sized embeddings through cross-modal alignment between images and full-waveform LiDAR data. As the model is trained in a contrastive manner, the embeddings can be directly leveraged in the context of a variety of environmental monitoring tasks in a zero-shot setting. In our experiments, we demonstrate the effectiveness of the embeddings for seven such tasks (canopy height mapping, fractional canopy cover, land cover mapping, tree species identification, plant area index, crop type classification, and per-pixel waveform-based vertical structure mapping). The results show that the embeddings, along with zero-shot classifiers, often outperform specialized supervised models, even in low data regimes. In the fine-tuning setting, we show strong low-shot capabilities with performances near or better than state-of-the-art on five out of six tasks.","sentences":["Significant efforts have been directed towards adapting self-supervised multimodal learning for Earth observation applications.","However, existing methods produce coarse patch-sized embeddings, limiting their effectiveness and integration with other modalities like LiDAR.","To close this gap, we present DUNIA, an approach to learn pixel-sized embeddings through cross-modal alignment between images and full-waveform LiDAR data.","As the model is trained in a contrastive manner, the embeddings can be directly leveraged in the context of a variety of environmental monitoring tasks in a zero-shot setting.","In our experiments, we demonstrate the effectiveness of the embeddings for seven such tasks (canopy height mapping, fractional canopy cover, land cover mapping, tree species identification, plant area index, crop type classification, and per-pixel waveform-based vertical structure mapping).","The results show that the embeddings, along with zero-shot classifiers, often outperform specialized supervised models, even in low data regimes.","In the fine-tuning setting, we show strong low-shot capabilities with performances near or better than state-of-the-art on five out of six tasks."],"url":"http://arxiv.org/abs/2502.17066v1"}
{"created":"2025-02-24 11:21:08","title":"Data Analysis Prediction over Multiple Unseen Datasets: A Vector Embedding Approach","abstract":"The massive increase in the data volume and dataset availability for analysts compels researchers to focus on data content and select high-quality datasets to enhance the performance of analytics operators. While selecting the highest quality data for analysis highly increases task accuracy and efficiency, it is still a hard task, especially when the number of available inputs is very large. To address this issue, we propose a novel methodology that infers the outcome of analytics operators by creating a model from datasets similar to the queried one. Dataset similarity is performed via projecting each dataset to a vector embedding representation. The vectorization process is performed using our proposed deep learning model NumTabData2Vec, which takes a whole dataset and projects it into a lower vector embedding representation space. Through experimental evaluation, we compare the prediction performance and the execution time of our framework to another state-of-the-art modelling operator framework, illustrating that our approach predicts analytics outcomes accurately. Furthermore, our vectorization model can project different real-world scenarios to a lower vector embedding representation and distinguish between them.","sentences":["The massive increase in the data volume and dataset availability for analysts compels researchers to focus on data content and select high-quality datasets to enhance the performance of analytics operators.","While selecting the highest quality data for analysis highly increases task accuracy and efficiency, it is still a hard task, especially when the number of available inputs is very large.","To address this issue, we propose a novel methodology that infers the outcome of analytics operators by creating a model from datasets similar to the queried one.","Dataset similarity is performed via projecting each dataset to a vector embedding representation.","The vectorization process is performed using our proposed deep learning model NumTabData2Vec, which takes a whole dataset and projects it into a lower vector embedding representation space.","Through experimental evaluation, we compare the prediction performance and the execution time of our framework to another state-of-the-art modelling operator framework, illustrating that our approach predicts analytics outcomes accurately.","Furthermore, our vectorization model can project different real-world scenarios to a lower vector embedding representation and distinguish between them."],"url":"http://arxiv.org/abs/2502.17060v1"}
{"created":"2025-02-24 11:13:37","title":"SpecDM: Hyperspectral Dataset Synthesis with Pixel-level Semantic Annotations","abstract":"In hyperspectral remote sensing field, some downstream dense prediction tasks, such as semantic segmentation (SS) and change detection (CD), rely on supervised learning to improve model performance and require a large amount of manually annotated data for training. However, due to the needs of specific equipment and special application scenarios, the acquisition and annotation of hyperspectral images (HSIs) are often costly and time-consuming. To this end, our work explores the potential of generative diffusion model in synthesizing HSIs with pixel-level annotations. The main idea is to utilize a two-stream VAE to learn the latent representations of images and corresponding masks respectively, learn their joint distribution during the diffusion model training, and finally obtain the image and mask through their respective decoders. To the best of our knowledge, it is the first work to generate high-dimensional HSIs with annotations. Our proposed approach can be applied in various kinds of dataset generation. We select two of the most widely used dense prediction tasks: semantic segmentation and change detection, and generate datasets suitable for these tasks. Experiments demonstrate that our synthetic datasets have a positive impact on the improvement of these downstream tasks.","sentences":["In hyperspectral remote sensing field, some downstream dense prediction tasks, such as semantic segmentation (SS) and change detection (CD), rely on supervised learning to improve model performance and require a large amount of manually annotated data for training.","However, due to the needs of specific equipment and special application scenarios, the acquisition and annotation of hyperspectral images (HSIs) are often costly and time-consuming.","To this end, our work explores the potential of generative diffusion model in synthesizing HSIs with pixel-level annotations.","The main idea is to utilize a two-stream VAE to learn the latent representations of images and corresponding masks respectively, learn their joint distribution during the diffusion model training, and finally obtain the image and mask through their respective decoders.","To the best of our knowledge, it is the first work to generate high-dimensional HSIs with annotations.","Our proposed approach can be applied in various kinds of dataset generation.","We select two of the most widely used dense prediction tasks: semantic segmentation and change detection, and generate datasets suitable for these tasks.","Experiments demonstrate that our synthetic datasets have a positive impact on the improvement of these downstream tasks."],"url":"http://arxiv.org/abs/2502.17056v1"}
{"created":"2025-02-24 11:08:08","title":"Optimizing Urban Mobility Through Complex Network Analysis and Big Data from Smart Cards","abstract":"This study investigates the network characteristics of high-frequency (HF) and low-frequency (LF) travelers in urban public transport systems by analyzing 20 million smart card records from Beijing's transit network. A novel methodology integrates advanced data preprocessing, clustering techniques, and complex network analysis to differentiate HF and LF passenger behaviors and their impacts on network structure, robustness, and efficiency. The primary challenge is accurately segmenting and modeling the behaviors of diverse passenger groups within a large-scale, noisy dataset while maintaining computational efficiency and scalability. HF networks, representing the top 25% of travelers by usage frequency, exhibit high connectivity with an average clustering coefficient of 0.72 and greater node degree centrality. However, they have lower robustness, with efficiency declining by 35% under targeted disruptions and longer average path lengths of 6.2 during peak hours. In contrast, LF networks, which include 75% of travelers, are more dispersed yet resilient, with efficiency declining by only 10% under similar disruptions and stronger intracommunity connectivity. Temporal analysis reveals that HF passengers significantly contribute to peak-hour congestion, with 57.4% of HF trips occurring between 6:00 and 10:00 AM, while LF passengers show a broader temporal distribution, helping to mitigate congestion hotspots. Understanding these travel patterns is crucial for optimizing public transit systems. The findings suggest targeted strategies such as enhancing robustness in HF networks by diversifying key routes and improving accessibility in LF-dominated areas. This research provides a scalable framework for analyzing smart card data and offers actionable insights for optimizing transit networks, improving congestion management, and advancing sustainable urban mobility planning.","sentences":["This study investigates the network characteristics of high-frequency (HF) and low-frequency (LF) travelers in urban public transport systems by analyzing 20 million smart card records from Beijing's transit network.","A novel methodology integrates advanced data preprocessing, clustering techniques, and complex network analysis to differentiate HF and LF passenger behaviors and their impacts on network structure, robustness, and efficiency.","The primary challenge is accurately segmenting and modeling the behaviors of diverse passenger groups within a large-scale, noisy dataset while maintaining computational efficiency and scalability.","HF networks, representing the top 25% of travelers by usage frequency, exhibit high connectivity with an average clustering coefficient of 0.72 and greater node degree centrality.","However, they have lower robustness, with efficiency declining by 35% under targeted disruptions and longer average path lengths of 6.2 during peak hours.","In contrast, LF networks, which include 75% of travelers, are more dispersed yet resilient, with efficiency declining by only 10% under similar disruptions and stronger intracommunity connectivity.","Temporal analysis reveals that HF passengers significantly contribute to peak-hour congestion, with 57.4% of HF trips occurring between 6:00 and 10:00 AM, while LF passengers show a broader temporal distribution, helping to mitigate congestion hotspots.","Understanding these travel patterns is crucial for optimizing public transit systems.","The findings suggest targeted strategies such as enhancing robustness in HF networks by diversifying key routes and improving accessibility in LF-dominated areas.","This research provides a scalable framework for analyzing smart card data and offers actionable insights for optimizing transit networks, improving congestion management, and advancing sustainable urban mobility planning."],"url":"http://arxiv.org/abs/2502.17054v1"}
{"created":"2025-02-24 11:07:00","title":"PointSea: Point Cloud Completion via Self-structure Augmentation","abstract":"Point cloud completion is a fundamental yet not well-solved problem in 3D vision. Current approaches often rely on 3D coordinate information and/or additional data (e.g., images and scanning viewpoints) to fill in missing parts. Unlike these methods, we explore self-structure augmentation and propose PointSea for global-to-local point cloud completion. In the global stage, consider how we inspect a defective region of a physical object, we may observe it from various perspectives for a better understanding. Inspired by this, PointSea augments data representation by leveraging self-projected depth images from multiple views. To reconstruct a compact global shape from the cross-modal input, we incorporate a feature fusion module to fuse features at both intra-view and inter-view levels. In the local stage, to reveal highly detailed structures, we introduce a point generator called the self-structure dual-generator. This generator integrates both learned shape priors and geometric self-similarities for shape refinement. Unlike existing efforts that apply a unified strategy for all points, our dual-path design adapts refinement strategies conditioned on the structural type of each point, addressing the specific incompleteness of each point. Comprehensive experiments on widely-used benchmarks demonstrate that PointSea effectively understands global shapes and generates local details from incomplete input, showing clear improvements over existing methods.","sentences":["Point cloud completion is a fundamental yet not well-solved problem in 3D vision.","Current approaches often rely on 3D coordinate information and/or additional data (e.g., images and scanning viewpoints) to fill in missing parts.","Unlike these methods, we explore self-structure augmentation and propose PointSea for global-to-local point cloud completion.","In the global stage, consider how we inspect a defective region of a physical object, we may observe it from various perspectives for a better understanding.","Inspired by this, PointSea augments data representation by leveraging self-projected depth images from multiple views.","To reconstruct a compact global shape from the cross-modal input, we incorporate a feature fusion module to fuse features at both intra-view and inter-view levels.","In the local stage, to reveal highly detailed structures, we introduce a point generator called the self-structure dual-generator.","This generator integrates both learned shape priors and geometric self-similarities for shape refinement.","Unlike existing efforts that apply a unified strategy for all points, our dual-path design adapts refinement strategies conditioned on the structural type of each point, addressing the specific incompleteness of each point.","Comprehensive experiments on widely-used benchmarks demonstrate that PointSea effectively understands global shapes and generates local details from incomplete input, showing clear improvements over existing methods."],"url":"http://arxiv.org/abs/2502.17053v1"}
{"created":"2025-02-24 11:01:07","title":"TabulaTime: A Novel Multimodal Deep Learning Framework for Advancing Acute Coronary Syndrome Prediction through Environmental and Clinical Data Integration","abstract":"Acute Coronary Syndromes (ACS), including ST-segment elevation myocardial infarctions (STEMI) and non-ST-segment elevation myocardial infarctions (NSTEMI), remain a leading cause of mortality worldwide. Traditional cardiovascular risk scores rely primarily on clinical data, often overlooking environmental influences like air pollution that significantly impact heart health. Moreover, integrating complex time-series environmental data with clinical records is challenging.   We introduce TabulaTime, a multimodal deep learning framework that enhances ACS risk prediction by combining clinical risk factors with air pollution data. TabulaTime features three key innovations: First, it integrates time-series air pollution data with clinical tabular data to improve prediction accuracy. Second, its PatchRWKV module automatically extracts complex temporal patterns, overcoming limitations of traditional feature engineering while maintaining linear computational complexity. Third, attention mechanisms enhance interpretability by revealing interactions between clinical and environmental factors.   Experimental results show that TabulaTime improves prediction accuracy by over 20% compared to conventional models such as CatBoost, Random Forest, and LightGBM, with air pollution data alone contributing over a 10% improvement. Feature importance analysis identifies critical predictors including previous angina, systolic blood pressure, PM10, and NO2. Overall, TabulaTime bridges clinical and environmental insights, supporting personalized prevention strategies and informing public health policies to mitigate ACS risk.","sentences":["Acute Coronary Syndromes (ACS), including ST-segment elevation myocardial infarctions (STEMI) and non-ST-segment elevation myocardial infarctions (NSTEMI), remain a leading cause of mortality worldwide.","Traditional cardiovascular risk scores rely primarily on clinical data, often overlooking environmental influences like air pollution that significantly impact heart health.","Moreover, integrating complex time-series environmental data with clinical records is challenging.   ","We introduce TabulaTime, a multimodal deep learning framework that enhances ACS risk prediction by combining clinical risk factors with air pollution data.","TabulaTime features three key innovations: First, it integrates time-series air pollution data with clinical tabular data to improve prediction accuracy.","Second, its PatchRWKV module automatically extracts complex temporal patterns, overcoming limitations of traditional feature engineering while maintaining linear computational complexity.","Third, attention mechanisms enhance interpretability by revealing interactions between clinical and environmental factors.   ","Experimental results show that TabulaTime improves prediction accuracy by over 20% compared to conventional models such as CatBoost, Random Forest, and LightGBM, with air pollution data alone contributing over a 10% improvement.","Feature importance analysis identifies critical predictors including previous angina, systolic blood pressure, PM10, and NO2.","Overall, TabulaTime bridges clinical and environmental insights, supporting personalized prevention strategies and informing public health policies to mitigate ACS risk."],"url":"http://arxiv.org/abs/2502.17049v1"}
{"created":"2025-02-24 10:49:34","title":"PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance","abstract":"Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance.","sentences":["Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility.","However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy.","Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations.","Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII).","In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows.","We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance.","We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1.","Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance."],"url":"http://arxiv.org/abs/2502.17041v1"}
{"created":"2025-02-24 10:46:28","title":"LCV2I: Communication-Efficient and High-Performance Collaborative Perception Framework with Low-Resolution LiDAR","abstract":"Vehicle-to-Infrastructure (V2I) collaborative perception leverages data collected by infrastructure's sensors to enhance vehicle perceptual capabilities. LiDAR, as a commonly used sensor in cooperative perception, is widely equipped in intelligent vehicles and infrastructure. However, its superior performance comes with a correspondingly high cost. To achieve low-cost V2I, reducing the cost of LiDAR is crucial. Therefore, we study adopting low-resolution LiDAR on the vehicle to minimize cost as much as possible. However, simply reducing the resolution of vehicle's LiDAR results in sparse point clouds, making distant small objects even more blurred. Additionally, traditional communication methods have relatively low bandwidth utilization efficiency. These factors pose challenges for us. To balance cost and perceptual accuracy, we propose a new collaborative perception framework, namely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDAR as input. It also employs feature offset correction modules and regional feature enhancement algorithms to improve feature representation. Finally, we use regional difference map and regional score map to assess the value of collaboration content, thereby improving communication bandwidth efficiency. In summary, our approach achieves high perceptual performance while substantially reducing the demand for high-resolution sensors on the vehicle. To evaluate this algorithm, we conduct 3D object detection in the real-world scenario of DAIR-V2X, demonstrating that the performance of LCV2I consistently surpasses currently existing algorithms.","sentences":["Vehicle-to-Infrastructure (V2I) collaborative perception leverages data collected by infrastructure's sensors to enhance vehicle perceptual capabilities.","LiDAR, as a commonly used sensor in cooperative perception, is widely equipped in intelligent vehicles and infrastructure.","However, its superior performance comes with a correspondingly high cost.","To achieve low-cost V2I, reducing the cost of LiDAR is crucial.","Therefore, we study adopting low-resolution LiDAR on the vehicle to minimize cost as much as possible.","However, simply reducing the resolution of vehicle's LiDAR results in sparse point clouds, making distant small objects even more blurred.","Additionally, traditional communication methods have relatively low bandwidth utilization efficiency.","These factors pose challenges for us.","To balance cost and perceptual accuracy, we propose a new collaborative perception framework, namely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDAR as input.","It also employs feature offset correction modules and regional feature enhancement algorithms to improve feature representation.","Finally, we use regional difference map and regional score map to assess the value of collaboration content, thereby improving communication bandwidth efficiency.","In summary, our approach achieves high perceptual performance while substantially reducing the demand for high-resolution sensors on the vehicle.","To evaluate this algorithm, we conduct 3D object detection in the real-world scenario of DAIR-V2X, demonstrating that the performance of LCV2I consistently surpasses currently existing algorithms."],"url":"http://arxiv.org/abs/2502.17039v1"}
{"created":"2025-02-24 10:43:57","title":"Multi-modal and Metadata Capture Model for Micro Video Popularity Prediction","abstract":"As short videos have become the primary form of content consumption across various industries, accurately predicting their popularity has become key to enhancing user engagement and optimizing business strategies. This report presents a solution for the 2024 INFORMS Data Mining Challenge, focusing on our developed 3M model (Multi-modal and Metadata Capture Model), which is a multi-modal popularity prediction model. The 3M model integrates video, audio, descriptions, and metadata to fully explore the multidimensional information of short videos. We employ a retriever-based method to retrieve relevant instances from a multi-modal memory bank, filtering similar videos based on visual, acoustic, and text-based features for prediction. Additionally, we apply a random masking method combined with a semi-supervised model for incomplete multi-modalities to leverage the metadata of videos. Ultimately, we use a network to synthesize both approaches, significantly improving the accuracy of predictions. Compared to traditional tag-based algorithms, our model outperforms existing methods on the validation set, showing a notable increase in prediction accuracy. Our research not only offers a new perspective on understanding the drivers of short video popularity but also provides valuable data support for identifying market opportunities, optimizing advertising strategies, and enhancing content creation. We believe that the innovative methodology proposed in this report provides practical tools and valuable insights for professionals in the field of short video popularity prediction, helping them effectively address future challenges.","sentences":["As short videos have become the primary form of content consumption across various industries, accurately predicting their popularity has become key to enhancing user engagement and optimizing business strategies.","This report presents a solution for the 2024 INFORMS Data Mining Challenge, focusing on our developed 3M model (Multi-modal and Metadata Capture Model), which is a multi-modal popularity prediction model.","The 3M model integrates video, audio, descriptions, and metadata to fully explore the multidimensional information of short videos.","We employ a retriever-based method to retrieve relevant instances from a multi-modal memory bank, filtering similar videos based on visual, acoustic, and text-based features for prediction.","Additionally, we apply a random masking method combined with a semi-supervised model for incomplete multi-modalities to leverage the metadata of videos.","Ultimately, we use a network to synthesize both approaches, significantly improving the accuracy of predictions.","Compared to traditional tag-based algorithms, our model outperforms existing methods on the validation set, showing a notable increase in prediction accuracy.","Our research not only offers a new perspective on understanding the drivers of short video popularity but also provides valuable data support for identifying market opportunities, optimizing advertising strategies, and enhancing content creation.","We believe that the innovative methodology proposed in this report provides practical tools and valuable insights for professionals in the field of short video popularity prediction, helping them effectively address future challenges."],"url":"http://arxiv.org/abs/2502.17038v1"}
{"created":"2025-02-24 10:41:27","title":"Subspace and DOA estimation under coarse quantization","abstract":"We study direction-of-arrival (DOA) estimation from coarsely quantized data. We focus on a two-step approach which first estimates the signal subspace via covariance estimation and then extracts DOA angles by the ESPRIT algorithm. In particular, we analyze two stochastic quantization schemes which use dithering: a one-bit quantizer combined with rectangular dither and a multi-bit quantizer with triangular dither. For each quantizer, we derive rigorous high probability bounds for the distances between the true and estimated signal subspaces and DOA angles. Using our analysis, we identify scenarios in which subspace and DOA estimation via triangular dithering qualitatively outperforms rectangular dithering. We verify in numerical simulations that our estimates are optimal in their dependence on the smallest non-zero eigenvalue of the target matrix. The resulting subspace estimation guarantees are equally applicable in the analysis of other spectral estimation algorithms and related problems.","sentences":["We study direction-of-arrival (DOA) estimation from coarsely quantized data.","We focus on a two-step approach which first estimates the signal subspace via covariance estimation and then extracts DOA angles by the ESPRIT algorithm.","In particular, we analyze two stochastic quantization schemes which use dithering: a one-bit quantizer combined with rectangular dither and a multi-bit quantizer with triangular dither.","For each quantizer, we derive rigorous high probability bounds for the distances between the true and estimated signal subspaces and DOA angles.","Using our analysis, we identify scenarios in which subspace and DOA estimation via triangular dithering qualitatively outperforms rectangular dithering.","We verify in numerical simulations that our estimates are optimal in their dependence on the smallest non-zero eigenvalue of the target matrix.","The resulting subspace estimation guarantees are equally applicable in the analysis of other spectral estimation algorithms and related problems."],"url":"http://arxiv.org/abs/2502.17037v1"}
{"created":"2025-02-24 10:34:35","title":"Evolution 6.0: Evolving Robotic Capabilities Through Generative Design","abstract":"We propose a new concept, Evolution 6.0, which represents the evolution of robotics driven by Generative AI. When a robot lacks the necessary tools to accomplish a task requested by a human, it autonomously designs the required instruments and learns how to use them to achieve the goal. Evolution 6.0 is an autonomous robotic system powered by Vision-Language Models (VLMs), Vision-Language Action (VLA) models, and Text-to-3D generative models for tool design and task execution. The system comprises two key modules: the Tool Generation Module, which fabricates task-specific tools from visual and textual data, and the Action Generation Module, which converts natural language instructions into robotic actions. It integrates QwenVLM for environmental understanding, OpenVLA for task execution, and Llama-Mesh for 3D tool generation. Evaluation results demonstrate a 90% success rate for tool generation with a 10-second inference time, and action generation achieving 83.5% in physical and visual generalization, 70% in motion generalization, and 37% in semantic generalization. Future improvements will focus on bimanual manipulation, expanded task capabilities, and enhanced environmental interpretation to improve real-world adaptability.","sentences":["We propose a new concept, Evolution 6.0, which represents the evolution of robotics driven by Generative AI.","When a robot lacks the necessary tools to accomplish a task requested by a human, it autonomously designs the required instruments and learns how to use them to achieve the goal.","Evolution 6.0 is an autonomous robotic system powered by Vision-Language Models (VLMs), Vision-Language Action (VLA) models, and Text-to-3D generative models for tool design and task execution.","The system comprises two key modules: the Tool Generation Module, which fabricates task-specific tools from visual and textual data, and the Action Generation Module, which converts natural language instructions into robotic actions.","It integrates QwenVLM for environmental understanding, OpenVLA for task execution, and Llama-Mesh for 3D tool generation.","Evaluation results demonstrate a 90% success rate for tool generation with a 10-second inference time, and action generation achieving 83.5% in physical and visual generalization, 70% in motion generalization, and 37% in semantic generalization.","Future improvements will focus on bimanual manipulation, expanded task capabilities, and enhanced environmental interpretation to improve real-world adaptability."],"url":"http://arxiv.org/abs/2502.17034v1"}
{"created":"2025-02-24 10:29:15","title":"Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence","abstract":"Multimodal alignment is crucial for various downstream tasks such as cross-modal generation and retrieval. Previous multimodal approaches like CLIP maximize the mutual information mainly by aligning pairwise samples across modalities while overlooking the distributional differences, leading to suboptimal alignment with modality gaps. In this paper, to overcome the limitation, we propose CS-Aligner, a novel and straightforward framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information. In the proposed framework, we find that the CS divergence and mutual information serve complementary roles in multimodal alignment, capturing both the global distribution information of each modality and the pairwise semantic relationships, yielding tighter and more precise alignment. Moreover, CS-Aligher enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice. Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment.","sentences":["Multimodal alignment is crucial for various downstream tasks such as cross-modal generation and retrieval.","Previous multimodal approaches like CLIP maximize the mutual information mainly by aligning pairwise samples across modalities while overlooking the distributional differences, leading to suboptimal alignment with modality gaps.","In this paper, to overcome the limitation, we propose CS-Aligner, a novel and straightforward framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information.","In the proposed framework, we find that the CS divergence and mutual information serve complementary roles in multimodal alignment, capturing both the global distribution information of each modality and the pairwise semantic relationships, yielding tighter and more precise alignment.","Moreover, CS-Aligher enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice.","Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment."],"url":"http://arxiv.org/abs/2502.17028v1"}
{"created":"2025-02-24 10:26:29","title":"Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges from Generalization","abstract":"Large language models (LLMs) have demonstrated remarkable in-context learning (ICL) abilities. However, existing theoretical analysis of ICL primarily exhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on supervised function learning tasks where prompts are constructed with i.i.d. input-label pairs. This i.i.d. assumption diverges significantly from real language learning scenarios where prompt tokens are interdependent. (b) Lack of Emergence Explanation. Most literature answers what ICL does from an implicit optimization perspective but falls short in elucidating how ICL emerges and the impact of pre-training phase on ICL. In our paper, to extend (a), we adopt a more practical paradigm, auto-regressive next-token prediction (AR-NTP), which closely aligns with the actual training of language models. Specifically, within AR-NTP, we emphasize prompt token-dependency, which involves predicting each subsequent token based on the preceding sequence. To address (b), we formalize a systematic pre-training and ICL framework, highlighting the layer-wise structure of sequences and topics, alongside a two-level expectation. In conclusion, we present data-dependent, topic-dependent and optimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs, investigating that ICL emerges from the generalization of sequences and topics. Our theory is supported by experiments on numerical linear dynamic systems, synthetic GINC and real-world language datasets.","sentences":["Large language models (LLMs) have demonstrated remarkable in-context learning (ICL) abilities.","However, existing theoretical analysis of ICL primarily exhibits two limitations: (a) Limited i.i.d.","Setting.","Most studies focus on supervised function learning tasks where prompts are constructed with i.i.d. input-label pairs.","This i.i.d. assumption diverges significantly from real language learning scenarios where prompt tokens are interdependent.","(b) Lack of Emergence Explanation.","Most literature answers what ICL does from an implicit optimization perspective but falls short in elucidating how ICL emerges and the impact of pre-training phase on ICL.","In our paper, to extend (a), we adopt a more practical paradigm, auto-regressive next-token prediction (AR-NTP), which closely aligns with the actual training of language models.","Specifically, within AR-NTP, we emphasize prompt token-dependency, which involves predicting each subsequent token based on the preceding sequence.","To address (b), we formalize a systematic pre-training and ICL framework, highlighting the layer-wise structure of sequences and topics, alongside a two-level expectation.","In conclusion, we present data-dependent, topic-dependent and optimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs, investigating that ICL emerges from the generalization of sequences and topics.","Our theory is supported by experiments on numerical linear dynamic systems, synthetic GINC and real-world language datasets."],"url":"http://arxiv.org/abs/2502.17024v1"}
{"created":"2025-02-24 10:22:17","title":"Advancing Eurasia Fire Understanding Through Machine Learning Techniques","abstract":"Modern fire management systems increasingly rely on satellite data and weather forecasting; however, access to comprehensive datasets remains limited due to proprietary restrictions. Despite the ecological significance of wildfires, large-scale, multi-regional research is constrained by data scarcity. Russian diverse ecosystems play a crucial role in shaping Eurasian fire dynamics, yet they remain underexplored. This study addresses existing gaps by introducing an open-access dataset that captures detailed fire incidents alongside corresponding meteorological conditions. We present one of the most extensive datasets available for wildfire analysis in Russia, covering 13 consecutive months of observations. Leveraging machine learning techniques, we conduct exploratory data analysis and develop predictive models to identify key fire behavior patterns across different fire categories and ecosystems. Our results highlight the critical influence of environmental factor patterns on fire occurrence and spread behavior. By improving the understanding of wildfire dynamics in Eurasia, this work contributes to more effective, data-driven approaches for proactive fire management in the face of evolving environmental conditions.","sentences":["Modern fire management systems increasingly rely on satellite data and weather forecasting; however, access to comprehensive datasets remains limited due to proprietary restrictions.","Despite the ecological significance of wildfires, large-scale, multi-regional research is constrained by data scarcity.","Russian diverse ecosystems play a crucial role in shaping Eurasian fire dynamics, yet they remain underexplored.","This study addresses existing gaps by introducing an open-access dataset that captures detailed fire incidents alongside corresponding meteorological conditions.","We present one of the most extensive datasets available for wildfire analysis in Russia, covering 13 consecutive months of observations.","Leveraging machine learning techniques, we conduct exploratory data analysis and develop predictive models to identify key fire behavior patterns across different fire categories and ecosystems.","Our results highlight the critical influence of environmental factor patterns on fire occurrence and spread behavior.","By improving the understanding of wildfire dynamics in Eurasia, this work contributes to more effective, data-driven approaches for proactive fire management in the face of evolving environmental conditions."],"url":"http://arxiv.org/abs/2502.17023v1"}
{"created":"2025-02-24 10:22:03","title":"Class-Dependent Perturbation Effects in Evaluating Time Series Attributions","abstract":"As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contributed the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through empirical analysis across multiple datasets, model architectures, and perturbation strategies, we identify important class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common.","sentences":["As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions.","Within XAI, feature attribution methods aim to identify which input features contributed the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics.","Through empirical analysis across multiple datasets, model architectures, and perturbation strategies, we identify important class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others.","In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences.","Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality.","We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions.","Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common."],"url":"http://arxiv.org/abs/2502.17022v1"}
{"created":"2025-02-24 09:31:18","title":"An Enhanced Large Language Model For Cross Modal Query Understanding System Using DL-KeyBERT Based CAZSSCL-MPGPT","abstract":"Large Language Models (LLMs) are advanced deep-learning models designed to understand and generate human language. They work together with models that process data like images, enabling cross-modal understanding. However, existing approaches often suffer from the echo chamber effect, where redundant visual patterns reduce model generalization and accuracy. Thus, the proposed system considered this limitation and developed an enhanced LLM-based framework for cross-modal query understanding using DL-KeyBERT-based CAZSSCL-MPGPT. The collected dataset consists of pre-processed images and texts. The preprocessed images then undergo object segmentation using Easom-You Only Look Once (E-YOLO). The object skeleton is generated, along with the knowledge graph using a Conditional Random Knowledge Graph (CRKG) technique. Further, features are extracted from the knowledge graph, generated skeletons, and segmented objects. The optimal features are then selected using the Fossa Optimization Algorithm (FOA). Meanwhile, the text undergoes word embedding using DL-KeyBERT. Finally, the cross-modal query understanding system utilizes CAZSSCL-MPGPT to generate accurate and contextually relevant image descriptions as text. The proposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362% in the COCO dataset 2017 and 98.43224393% in the vqav2-val dataset.","sentences":["Large Language Models (LLMs) are advanced deep-learning models designed to understand and generate human language.","They work together with models that process data like images, enabling cross-modal understanding.","However, existing approaches often suffer from the echo chamber effect, where redundant visual patterns reduce model generalization and accuracy.","Thus, the proposed system considered this limitation and developed an enhanced LLM-based framework for cross-modal query understanding using DL-KeyBERT-based CAZSSCL-MPGPT.","The collected dataset consists of pre-processed images and texts.","The preprocessed images then undergo object segmentation using Easom-You Only Look Once (E-YOLO).","The object skeleton is generated, along with the knowledge graph using a Conditional Random Knowledge Graph (CRKG) technique.","Further, features are extracted from the knowledge graph, generated skeletons, and segmented objects.","The optimal features are then selected using the Fossa Optimization Algorithm (FOA).","Meanwhile, the text undergoes word embedding using DL-KeyBERT.","Finally, the cross-modal query understanding system utilizes CAZSSCL-MPGPT to generate accurate and contextually relevant image descriptions as text.","The proposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362% in the COCO dataset 2017 and 98.43224393% in the vqav2-val dataset."],"url":"http://arxiv.org/abs/2502.17000v1"}
{"created":"2025-02-24 09:26:48","title":"Semantic Neural Radiance Fields for Multi-Date Satellite Data","abstract":"In this work we propose a satellite specific Neural Radiance Fields (NeRF) model capable to obtain a three-dimensional semantic representation (neural semantic field) of the scene. The model derives the output from a set of multi-date satellite images with corresponding pixel-wise semantic labels. We demonstrate the robustness of our approach and its capability to improve noisy input labels. We enhance the color prediction by utilizing the semantic information to address temporal image inconsistencies caused by non-stationary categories such as vehicles. To facilitate further research in this domain, we present a dataset comprising manually generated labels for popular multi-view satellite images. Our code and dataset are available at https://github.com/wagnva/semantic-nerf-for-satellite-data.","sentences":["In this work we propose a satellite specific Neural Radiance Fields (NeRF) model capable to obtain a three-dimensional semantic representation (neural semantic field) of the scene.","The model derives the output from a set of multi-date satellite images with corresponding pixel-wise semantic labels.","We demonstrate the robustness of our approach and its capability to improve noisy input labels.","We enhance the color prediction by utilizing the semantic information to address temporal image inconsistencies caused by non-stationary categories such as vehicles.","To facilitate further research in this domain, we present a dataset comprising manually generated labels for popular multi-view satellite images.","Our code and dataset are available at https://github.com/wagnva/semantic-nerf-for-satellite-data."],"url":"http://arxiv.org/abs/2502.16992v1"}
{"created":"2025-02-24 09:25:51","title":"All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark","abstract":"We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.","sentences":["We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos.","MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos.","It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions.","It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it.","Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric.","Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers."],"url":"http://arxiv.org/abs/2502.16989v1"}
{"created":"2025-02-24 08:54:39","title":"LongSafety: Evaluating Long-Context Safety of Large Language Models","abstract":"As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at https://github.com/thu-coai/LongSafety.","sentences":["As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context.","However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety.","To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks.","LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context.","Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%.","Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety.","Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models.","Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges.","Our code and data are available at https://github.com/thu-coai/LongSafety."],"url":"http://arxiv.org/abs/2502.16971v1"}
{"created":"2025-02-24 08:41:19","title":"Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM","abstract":"The billion-scale Large Language Models (LLMs) need deployment on expensive server-grade GPUs with large-storage HBMs and abundant computation capability. As LLM-assisted services become popular, achieving cost-effective LLM inference on budget-friendly hardware becomes the trend. Extensive researches relocate LLM parameters from expensive GPUs to host memory. However, the restricted bandwidth between the host and GPU memory limits the inference performance.   This work introduces Hermes, a budget-friendly system that leverages the near-data processing (NDP) within commodity DRAM DIMMs to enhance the performance of a single consumer-grade GPU, achieving efficient LLM inference. The inherent activation sparsity in LLMs naturally divides weight parameters into two categories, termed ``hot\" and ``cold\" neurons, respectively. Hot neurons, which consist of only approximately 20\\% of all weight parameters, account for 80\\% of the total computational load, while cold neurons make up the other 80\\% of parameters but are responsible for just 20\\% of the computational load. Therefore, we propose a heterogeneous computing strategy: mapping hot neurons to a single computation-efficient GPU, while offloading cold neurons to NDP-DIMMs, which offer large memory size but limited computation capabilities. Meanwhile, the dynamic nature of activation sparsity needs a real-time partition of hot/cold neurons and adaptive remapping of cold neurons across multiple NDP-DIMM modules. Therefore, we introduce a lightweight predictor optimizing real-time neuron partition and adjustment between GPU and NDP-DIMMs. We also utilize a window-based online scheduling mechanism to maintain load balance among NDP-DIMM modules. Hermes facilitates the deployment of LLaMA2-70B on consumer-grade hardware at 13.75 tokens/s and realizes an average 75.24$\\times$ speedup over the state-of-the-art offloading-based inference system.","sentences":["The billion-scale Large Language Models (LLMs) need deployment on expensive server-grade GPUs with large-storage HBMs and abundant computation capability.","As LLM-assisted services become popular, achieving cost-effective LLM inference on budget-friendly hardware becomes the trend.","Extensive researches relocate LLM parameters from expensive GPUs to host memory.","However, the restricted bandwidth between the host and GPU memory limits the inference performance.   ","This work introduces Hermes, a budget-friendly system that leverages the near-data processing (NDP) within commodity DRAM DIMMs to enhance the performance of a single consumer-grade GPU, achieving efficient LLM inference.","The inherent activation sparsity in LLMs naturally divides weight parameters into two categories, termed ``hot\" and ``cold\" neurons, respectively.","Hot neurons, which consist of only approximately 20\\% of all weight parameters, account for 80\\% of the total computational load, while cold neurons make up the other 80\\% of parameters but are responsible for just 20\\% of the computational load.","Therefore, we propose a heterogeneous computing strategy: mapping hot neurons to a single computation-efficient GPU, while offloading cold neurons to NDP-DIMMs, which offer large memory size but limited computation capabilities.","Meanwhile, the dynamic nature of activation sparsity needs a real-time partition of hot/cold neurons and adaptive remapping of cold neurons across multiple NDP-DIMM modules.","Therefore, we introduce a lightweight predictor optimizing real-time neuron partition and adjustment between GPU and NDP-DIMMs.","We also utilize a window-based online scheduling mechanism to maintain load balance among NDP-DIMM modules.","Hermes facilitates the deployment of LLaMA2-70B on consumer-grade hardware at 13.75 tokens/s and realizes an average 75.24$\\times$ speedup over the state-of-the-art offloading-based inference system."],"url":"http://arxiv.org/abs/2502.16963v1"}
{"created":"2025-02-24 08:38:21","title":"UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings","abstract":"Multilingual Large Language Models (LLMs) often provide suboptimal performance on low-resource languages like Urdu. This paper introduces UrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct architecture and continually pre-trained on 128 million Urdu tokens, capturing the rich diversity of the language. To enhance instruction-following and translation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune the model on 41,000 Urdu instructions and approximately 50,000 English-Urdu translation pairs. Evaluation across three machine translation datasets demonstrates significant performance improvements compared to state-of-the-art (SOTA) models, establishing a new benchmark for Urdu LLMs. These findings underscore the potential of targeted adaptation strategies with limited data and computational resources to address the unique challenges of low-resource languages.","sentences":["Multilingual Large Language Models (LLMs) often provide suboptimal performance on low-resource languages like Urdu.","This paper introduces UrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct architecture and continually pre-trained on 128 million Urdu tokens, capturing the rich diversity of the language.","To enhance instruction-following and translation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune the model on 41,000 Urdu instructions and approximately 50,000 English-Urdu translation pairs.","Evaluation across three machine translation datasets demonstrates significant performance improvements compared to state-of-the-art (SOTA) models, establishing a new benchmark for Urdu LLMs.","These findings underscore the potential of targeted adaptation strategies with limited data and computational resources to address the unique challenges of low-resource languages."],"url":"http://arxiv.org/abs/2502.16961v1"}
{"created":"2025-02-24 08:37:38","title":"Efficiency in the Roommates Problem","abstract":"We propose an $O(n^2)$-time algorithm to determine whether a given matching is efficient in the roommates problem.","sentences":["We propose an $O(n^2)$-time algorithm to determine whether a given matching is efficient in the roommates problem."],"url":"http://arxiv.org/abs/2502.16960v1"}
{"created":"2025-02-24 08:31:51","title":"Atten-Transformer: A Deep Learning Framework for User App Usage Prediction","abstract":"Accurately predicting smartphone app usage patterns is crucial for user experience optimization and targeted marketing. However, existing methods struggle to capture intricate dependencies in user behavior, particularly in sparse or complex usage scenarios. To address these challenges, we introduce Atten-Transformer, a novel model that integrates temporal attention with a Transformer network to dynamically identify and leverage key app usage patterns. Unlike conventional methods that primarily consider app order and duration, our approach employs a multi-dimensional feature representation, incorporating both feature encoding and temporal encoding to enhance predictive accuracy. The proposed attention mechanism effectively assigns importance to critical app usage moments, improving both model interpretability and generalization. Extensive experiments on multiple smartphone usage datasets, including LSapp and Tsinghua App Usage datasets, demonstrate that Atten-Transformer consistently outperforms state-of-the-art models across different data splits. Specifically, our model achieves a 45.24\\% improvement in HR@1 on the Tsinghua dataset (Time-based Split) and a 18.25\\% improvement in HR@1 on the LSapp dataset (Cold Start Split), showcasing its robustness across diverse app usage scenarios. These findings highlight the potential of integrating adaptive attention mechanisms in mobile usage forecasting, paving the way for enhanced user engagement and resource allocation.","sentences":["Accurately predicting smartphone app usage patterns is crucial for user experience optimization and targeted marketing.","However, existing methods struggle to capture intricate dependencies in user behavior, particularly in sparse or complex usage scenarios.","To address these challenges, we introduce Atten-Transformer, a novel model that integrates temporal attention with a Transformer network to dynamically identify and leverage key app usage patterns.","Unlike conventional methods that primarily consider app order and duration, our approach employs a multi-dimensional feature representation, incorporating both feature encoding and temporal encoding to enhance predictive accuracy.","The proposed attention mechanism effectively assigns importance to critical app usage moments, improving both model interpretability and generalization.","Extensive experiments on multiple smartphone usage datasets, including LSapp and Tsinghua App Usage datasets, demonstrate that Atten-Transformer consistently outperforms state-of-the-art models across different data splits.","Specifically, our model achieves a 45.24\\% improvement in HR@1 on the Tsinghua dataset (Time-based Split) and a 18.25\\% improvement in HR@1 on the LSapp dataset (Cold Start Split), showcasing its robustness across diverse app usage scenarios.","These findings highlight the potential of integrating adaptive attention mechanisms in mobile usage forecasting, paving the way for enhanced user engagement and resource allocation."],"url":"http://arxiv.org/abs/2502.16957v1"}
{"created":"2025-02-24 08:30:53","title":"MTVHunter: Smart Contracts Vulnerability Detection Based on Multi-Teacher Knowledge Translation","abstract":"Smart contracts, closely intertwined with cryptocurrency transactions, have sparked widespread concerns about considerable financial losses of security issues. To counteract this, a variety of tools have been developed to identify vulnerability in smart contract. However, they fail to overcome two challenges at the same time when faced with smart contract bytecode: (i) strong interference caused by enormous non-relevant instructions; (ii) missing semantics of bytecode due to incomplete data and control flow dependencies. In this paper, we propose a multi-teacher based bytecode vulnerability detection method, namely \\textbf{M}ulti-\\textbf{T}eacher \\textbf{V}ulnerability \\textbf{Hunter} (\\textbf{MTVHunter}), which delivers effective denoising and missing semantic to bytecode under multi-teacher guidance. Specifically, we first propose an instruction denoising teacher to eliminate noise interference by abstract vulnerability pattern and further reflect in contract embeddings. Secondly, we design a novel semantic complementary teacher with neuron distillation, which effectively extracts necessary semantic from source code to replenish the bytecode. Particularly, the proposed neuron distillation accelerate this semantic filling by turning the knowledge transition into a regression task. We conduct experiments on 229,178 real-world smart contracts that concerns four types of common vulnerabilities. Extensive experiments show MTVHunter achieves significantly performance gains over state-of-the-art approaches.","sentences":["Smart contracts, closely intertwined with cryptocurrency transactions, have sparked widespread concerns about considerable financial losses of security issues.","To counteract this, a variety of tools have been developed to identify vulnerability in smart contract.","However, they fail to overcome two challenges at the same time when faced with smart contract bytecode: (i) strong interference caused by enormous non-relevant instructions; (ii) missing semantics of bytecode due to incomplete data and control flow dependencies.","In this paper, we propose a multi-teacher based bytecode vulnerability detection method, namely \\textbf{M}ulti-\\textbf{T}eacher \\textbf{V}ulnerability \\textbf{Hunter} (\\textbf{MTVHunter}), which delivers effective denoising and missing semantic to bytecode under multi-teacher guidance.","Specifically, we first propose an instruction denoising teacher to eliminate noise interference by abstract vulnerability pattern and further reflect in contract embeddings.","Secondly, we design a novel semantic complementary teacher with neuron distillation, which effectively extracts necessary semantic from source code to replenish the bytecode.","Particularly, the proposed neuron distillation accelerate this semantic filling by turning the knowledge transition into a regression task.","We conduct experiments on 229,178 real-world smart contracts that concerns four types of common vulnerabilities.","Extensive experiments show MTVHunter achieves significantly performance gains over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2502.16955v1"}
{"created":"2025-02-24 08:27:36","title":"A survey of datasets for computer vision in agriculture","abstract":"In agricultural research, there has been a recent surge in the amount of Computer Vision (CV) focused work. But unlike general CV research, large high-quality public datasets are sparsely available. This can be partially attributed to the high variability between different agricultural tasks, crops and environments as well as the complexity of data collection, but it is also influenced by the reticence to publish datasets by many authors. This, as well as the lack of a widely used agricultural data repository, are impactful factors that hinder research in applied CV for agriculture as well as the usage of agricultural data in general-purpose CV research. In this survey, we provide a large number of high-quality datasets of images taken on fields. Overall, we find 45 datasets, which are listed in this paper as well as in an online catalog on the project website: https://smartfarminglab.github.io/field_dataset_survey/.","sentences":["In agricultural research, there has been a recent surge in the amount of Computer Vision (CV) focused work.","But unlike general CV research, large high-quality public datasets are sparsely available.","This can be partially attributed to the high variability between different agricultural tasks, crops and environments as well as the complexity of data collection, but it is also influenced by the reticence to publish datasets by many authors.","This, as well as the lack of a widely used agricultural data repository, are impactful factors that hinder research in applied CV for agriculture as well as the usage of agricultural data in general-purpose CV research.","In this survey, we provide a large number of high-quality datasets of images taken on fields.","Overall, we find 45 datasets, which are listed in this paper as well as in an online catalog on the project website: https://smartfarminglab.github.io/field_dataset_survey/."],"url":"http://arxiv.org/abs/2502.16950v1"}
{"created":"2025-02-24 08:20:02","title":"Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of Minority Samples","abstract":"The concept of a minimax classifier is well-established in statistical decision theory, but its implementation via neural networks remains challenging, particularly in scenarios with imbalanced training data having a limited number of samples for minority classes. To address this issue, we propose a novel minimax learning algorithm designed to minimize the risk of worst-performing classes. Our algorithm iterates through two steps: a minimization step that trains the model based on a selected target prior, and a maximization step that updates the target prior towards the adversarial prior for the trained model. In the minimization, we introduce a targeted logit-adjustment loss function that efficiently identifies optimal decision boundaries under the target prior. Moreover, based on a new prior-dependent generalization bound that we obtained, we theoretically prove that our loss function has a better generalization capability than existing loss functions. During the maximization, we refine the target prior by shifting it towards the adversarial prior, depending on the worst-performing classes rather than on per-class risk estimates. Our maximization method is particularly robust in the regime of a small number of samples. Additionally, to adapt to overparameterized neural networks, we partition the entire training dataset into two subsets: one for model training during the minimization step and the other for updating the target prior during the maximization step. Our proposed algorithm has a provable convergence property, and empirical results indicate that our algorithm performs better than or is comparable to existing methods. All codes are publicly available at https://github.com/hansung-choi/TLA-linear-ascent.","sentences":["The concept of a minimax classifier is well-established in statistical decision theory, but its implementation via neural networks remains challenging, particularly in scenarios with imbalanced training data having a limited number of samples for minority classes.","To address this issue, we propose a novel minimax learning algorithm designed to minimize the risk of worst-performing classes.","Our algorithm iterates through two steps: a minimization step that trains the model based on a selected target prior, and a maximization step that updates the target prior towards the adversarial prior for the trained model.","In the minimization, we introduce a targeted logit-adjustment loss function that efficiently identifies optimal decision boundaries under the target prior.","Moreover, based on a new prior-dependent generalization bound that we obtained, we theoretically prove that our loss function has a better generalization capability than existing loss functions.","During the maximization, we refine the target prior by shifting it towards the adversarial prior, depending on the worst-performing classes rather than on per-class risk estimates.","Our maximization method is particularly robust in the regime of a small number of samples.","Additionally, to adapt to overparameterized neural networks, we partition the entire training dataset into two subsets: one for model training during the minimization step and the other for updating the target prior during the maximization step.","Our proposed algorithm has a provable convergence property, and empirical results indicate that our algorithm performs better than or is comparable to existing methods.","All codes are publicly available at https://github.com/hansung-choi/TLA-linear-ascent."],"url":"http://arxiv.org/abs/2502.16948v1"}
{"created":"2025-02-24 08:17:54","title":"Using Machine Learning to Detect Fraudulent SMSs in Chichewa","abstract":"SMS enabled fraud is of great concern globally. Building classifiers based on machine learning for SMS fraud requires the use of suitable datasets for model training and validation. Most research has centred on the use of datasets of SMSs in English. This paper introduces a first dataset for SMS fraud detection in Chichewa, a major language in Africa, and reports on experiments with machine learning algorithms for classifying SMSs in Chichewa as fraud or non-fraud. We answer the broader research question of how feasible it is to develop machine learning classification models for Chichewa SMSs. To do that, we created three datasets. A small dataset of SMS in Chichewa was collected through primary research from a segment of the young population. We applied a label-preserving text transformations to increase its size. The enlarged dataset was translated into English using two approaches: human translation and machine translation. The Chichewa and the translated datasets were subjected to machine classification using random forest and logistic regression. Our findings indicate that both models achieved a promising accuracy of over 96% on the Chichewa dataset. There was a drop in performance when moving from the Chichewa to the translated dataset. This highlights the importance of data preprocessing, especially in multilingual or cross-lingual NLP tasks, and shows the challenges of relying on machine-translated text for training machine learning models. Our results underscore the importance of developing language specific models for SMS fraud detection to optimise accuracy and performance. Since most machine learning models require data preprocessing, it is essential to investigate the impact of the reliance on English-specific tools for data preprocessing.","sentences":["SMS enabled fraud is of great concern globally.","Building classifiers based on machine learning for SMS fraud requires the use of suitable datasets for model training and validation.","Most research has centred on the use of datasets of SMSs in English.","This paper introduces a first dataset for SMS fraud detection in Chichewa, a major language in Africa, and reports on experiments with machine learning algorithms for classifying SMSs in Chichewa as fraud or non-fraud.","We answer the broader research question of how feasible it is to develop machine learning classification models for Chichewa SMSs.","To do that, we created three datasets.","A small dataset of SMS in Chichewa was collected through primary research from a segment of the young population.","We applied a label-preserving text transformations to increase its size.","The enlarged dataset was translated into English using two approaches: human translation and machine translation.","The Chichewa and the translated datasets were subjected to machine classification using random forest and logistic regression.","Our findings indicate that both models achieved a promising accuracy of over 96% on the Chichewa dataset.","There was a drop in performance when moving from the Chichewa to the translated dataset.","This highlights the importance of data preprocessing, especially in multilingual or cross-lingual NLP tasks, and shows the challenges of relying on machine-translated text for training machine learning models.","Our results underscore the importance of developing language specific models for SMS fraud detection to optimise accuracy and performance.","Since most machine learning models require data preprocessing, it is essential to investigate the impact of the reliance on English-specific tools for data preprocessing."],"url":"http://arxiv.org/abs/2502.16947v1"}
{"created":"2025-02-24 08:11:29","title":"MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection","abstract":"Unsupervised anomaly detection in brain images is crucial for identifying injuries and pathologies without access to labels. However, the accurate localization of anomalies in medical images remains challenging due to the inherent complexity and variability of brain structures and the scarcity of annotated abnormal data. To address this challenge, we propose a novel approach that incorporates masking within diffusion models, leveraging their generative capabilities to learn robust representations of normal brain anatomy. During training, our model processes only normal brain MRI scans and performs a forward diffusion process in the latent space that adds noise to the features of randomly-selected patches. Following a dual objective, the model learns to identify which patches are noisy and recover their original features. This strategy ensures that the model captures intricate patterns of normal brain structures while isolating potential anomalies as noise in the latent space. At inference, the model identifies noisy patches corresponding to anomalies and generates a normal counterpart for these patches by applying a reverse diffusion process. Our method surpasses existing unsupervised anomaly detection techniques, demonstrating superior performance in generating accurate normal counterparts and localizing anomalies. The code is available at hhttps://github.com/farzad-bz/MAD-AD.","sentences":["Unsupervised anomaly detection in brain images is crucial for identifying injuries and pathologies without access to labels.","However, the accurate localization of anomalies in medical images remains challenging due to the inherent complexity and variability of brain structures and the scarcity of annotated abnormal data.","To address this challenge, we propose a novel approach that incorporates masking within diffusion models, leveraging their generative capabilities to learn robust representations of normal brain anatomy.","During training, our model processes only normal brain MRI scans and performs a forward diffusion process in the latent space that adds noise to the features of randomly-selected patches.","Following a dual objective, the model learns to identify which patches are noisy and recover their original features.","This strategy ensures that the model captures intricate patterns of normal brain structures while isolating potential anomalies as noise in the latent space.","At inference, the model identifies noisy patches corresponding to anomalies and generates a normal counterpart for these patches by applying a reverse diffusion process.","Our method surpasses existing unsupervised anomaly detection techniques, demonstrating superior performance in generating accurate normal counterparts and localizing anomalies.","The code is available at hhttps://github.com/farzad-bz/MAD-AD."],"url":"http://arxiv.org/abs/2502.16943v1"}
{"created":"2025-02-24 08:01:03","title":"SUSTeR: Sparse Unstructured Spatio Temporal Reconstruction on Traffic Prediction","abstract":"Mining spatio-temporal correlation patterns for traffic prediction is a well-studied field. However, most approaches are based on the assumption of the availability of and accessibility to a sufficiently dense data source, which is rather the rare case in reality. Traffic sensors in road networks are generally highly sparse in their distribution: fleet-based traffic sensing is sparse in space but also sparse in time. There are also other traffic application, besides road traffic, like moving objects in the marine space, where observations are sparsely and arbitrarily distributed in space. In this paper, we tackle the problem of traffic prediction on sparse and spatially irregular and non-deterministic traffic observations. We draw a border between imputations and this work as we consider high sparsity rates and no fixed sensor locations. We advance correlation mining methods with a Sparse Unstructured Spatio Temporal Reconstruction (SUSTeR) framework that reconstructs traffic states from sparse non-stationary observations. For the prediction the framework creates a hidden context traffic state which is enriched in a residual fashion with each observation. Such an assimilated hidden traffic state can be used by existing traffic prediction methods to predict future traffic states. We query these states with query locations from the spatial domain.","sentences":["Mining spatio-temporal correlation patterns for traffic prediction is a well-studied field.","However, most approaches are based on the assumption of the availability of and accessibility to a sufficiently dense data source, which is rather the rare case in reality.","Traffic sensors in road networks are generally highly sparse in their distribution: fleet-based traffic sensing is sparse in space but also sparse in time.","There are also other traffic application, besides road traffic, like moving objects in the marine space, where observations are sparsely and arbitrarily distributed in space.","In this paper, we tackle the problem of traffic prediction on sparse and spatially irregular and non-deterministic traffic observations.","We draw a border between imputations and this work as we consider high sparsity rates and no fixed sensor locations.","We advance correlation mining methods with a Sparse Unstructured Spatio Temporal Reconstruction (SUSTeR) framework that reconstructs traffic states from sparse non-stationary observations.","For the prediction the framework creates a hidden context traffic state which is enriched in a residual fashion with each observation.","Such an assimilated hidden traffic state can be used by existing traffic prediction methods to predict future traffic states.","We query these states with query locations from the spatial domain."],"url":"http://arxiv.org/abs/2502.16935v1"}
{"created":"2025-02-24 07:51:02","title":"Achieving Fair PCA Using Joint Eigenvalue Decomposition","abstract":"Principal Component Analysis (PCA) is a widely used method for dimensionality reduction, but it often overlooks fairness, especially when working with data that includes demographic characteristics. This can lead to biased representations that disproportionately affect certain groups. To address this issue, our approach incorporates Joint Eigenvalue Decomposition (JEVD), a technique that enables the simultaneous diagonalization of multiple matrices, ensuring fair and efficient representations. We formally show that the optimal solution of JEVD leads to a fair PCA solution. By integrating JEVD with PCA, we strike an optimal balance between preserving data structure and promoting fairness across diverse groups. We demonstrate that our method outperforms existing baseline approaches in fairness and representational quality on various datasets. It retains the core advantages of PCA while ensuring that sensitive demographic attributes do not create disparities in the reduced representation.","sentences":["Principal Component Analysis (PCA) is a widely used method for dimensionality reduction, but it often overlooks fairness, especially when working with data that includes demographic characteristics.","This can lead to biased representations that disproportionately affect certain groups.","To address this issue, our approach incorporates Joint Eigenvalue Decomposition (JEVD), a technique that enables the simultaneous diagonalization of multiple matrices, ensuring fair and efficient representations.","We formally show that the optimal solution of JEVD leads to a fair PCA solution.","By integrating JEVD with PCA, we strike an optimal balance between preserving data structure and promoting fairness across diverse groups.","We demonstrate that our method outperforms existing baseline approaches in fairness and representational quality on various datasets.","It retains the core advantages of PCA while ensuring that sensitive demographic attributes do not create disparities in the reduced representation."],"url":"http://arxiv.org/abs/2502.16933v1"}
{"created":"2025-02-24 07:50:01","title":"DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning","abstract":"Visuomotor policies have shown great promise in robotic manipulation but often require substantial amounts of human-collected data for effective performance. A key reason underlying the data demands is their limited spatial generalization capability, which necessitates extensive data collection across different object configurations. In this work, we present DemoGen, a low-cost, fully synthetic approach for automatic demonstration generation. Using only one human-collected demonstration per task, DemoGen generates spatially augmented demonstrations by adapting the demonstrated action trajectory to novel object configurations. Visual observations are synthesized by leveraging 3D point clouds as the modality and rearranging the subjects in the scene via 3D editing. Empirically, DemoGen significantly enhances policy performance across a diverse range of real-world manipulation tasks, showing its applicability even in challenging scenarios involving deformable objects, dexterous hand end-effectors, and bimanual platforms. Furthermore, DemoGen can be extended to enable additional out-of-distribution capabilities, including disturbance resistance and obstacle avoidance.","sentences":["Visuomotor policies have shown great promise in robotic manipulation but often require substantial amounts of human-collected data for effective performance.","A key reason underlying the data demands is their limited spatial generalization capability, which necessitates extensive data collection across different object configurations.","In this work, we present DemoGen, a low-cost, fully synthetic approach for automatic demonstration generation.","Using only one human-collected demonstration per task, DemoGen generates spatially augmented demonstrations by adapting the demonstrated action trajectory to novel object configurations.","Visual observations are synthesized by leveraging 3D point clouds as the modality and rearranging the subjects in the scene via 3D editing.","Empirically, DemoGen significantly enhances policy performance across a diverse range of real-world manipulation tasks, showing its applicability even in challenging scenarios involving deformable objects, dexterous hand end-effectors, and bimanual platforms.","Furthermore, DemoGen can be extended to enable additional out-of-distribution capabilities, including disturbance resistance and obstacle avoidance."],"url":"http://arxiv.org/abs/2502.16932v1"}
{"created":"2025-02-24 07:49:49","title":"Machine learning and high dimensional vector search","abstract":"Machine learning and vector search are two research topics that developed in parallel in nearby communities. However, unlike many other fields related to big data, machine learning has not significantly impacted vector search. In this opinion paper we attempt to explain this oddity. Along the way, we wander over the numerous bridges between the two fields.","sentences":["Machine learning and vector search are two research topics that developed in parallel in nearby communities.","However, unlike many other fields related to big data, machine learning has not significantly impacted vector search.","In this opinion paper we attempt to explain this oddity.","Along the way, we wander over the numerous bridges between the two fields."],"url":"http://arxiv.org/abs/2502.16931v1"}
{"created":"2025-02-24 07:15:05","title":"SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models","abstract":"Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs) faces significant challenges without training data, model tuning, or architectural modifications. Existing approaches require prompt tuning or architectural adaptations, limiting zero-shot applicability. Our work proposes a novel solution treating VLMs as black boxes, leveraging scores without training data or ground truth. Using large language model insights on object co-occurrence, we introduce compound prompts grounded in realistic object combinations. Analysis of these prompt scores reveals VLM biases and ``AND''/``OR'' signal ambiguities, notably that maximum compound scores are surprisingly suboptimal compared to second-highest scores. We address these through a debiasing and score-fusion algorithm that corrects image bias and clarifies VLM response behaviors. Our method enhances other zero-shot approaches, consistently improving their results. Experiments show superior mean Average Precision (mAP) compared to methods requiring training data, achieved through refined object ranking for robust zero-shot MLR.","sentences":["Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs) faces significant challenges without training data, model tuning, or architectural modifications.","Existing approaches require prompt tuning or architectural adaptations, limiting zero-shot applicability.","Our work proposes a novel solution treating VLMs as black boxes, leveraging scores without training data or ground truth.","Using large language model insights on object co-occurrence, we introduce compound prompts grounded in realistic object combinations.","Analysis of these prompt scores reveals VLM biases and ``AND''/``OR'' signal ambiguities, notably that maximum compound scores are surprisingly suboptimal compared to second-highest scores.","We address these through a debiasing and score-fusion algorithm that corrects image bias and clarifies VLM response behaviors.","Our method enhances other zero-shot approaches, consistently improving their results.","Experiments show superior mean Average Precision (mAP) compared to methods requiring training data, achieved through refined object ranking for robust zero-shot MLR."],"url":"http://arxiv.org/abs/2502.16911v1"}
{"created":"2025-02-24 07:02:31","title":"AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models","abstract":"While logical reasoning evaluation of Large Language Models (LLMs) has attracted significant attention, existing benchmarks predominantly rely on multiple-choice formats that are vulnerable to random guessing, leading to overestimated performance and substantial performance fluctuations. To obtain more accurate assessments of models' reasoning capabilities, we propose an automated method for synthesizing open-ended logic puzzles, and use it to develop a bilingual benchmark, AutoLogi. Our approach features program-based verification and controllable difficulty levels, enabling more reliable evaluation that better distinguishes models' reasoning abilities. Extensive evaluation of eight modern LLMs shows that AutoLogi can better reflect true model capabilities, with performance scores spanning from 35% to 73% compared to the narrower range of 21% to 37% on the source multiple-choice dataset. Beyond benchmark creation, this synthesis method can generate high-quality training data by incorporating program verifiers into the rejection sampling process, enabling systematic enhancement of LLMs' reasoning capabilities across diverse datasets.","sentences":["While logical reasoning evaluation of Large Language Models (LLMs) has attracted significant attention, existing benchmarks predominantly rely on multiple-choice formats that are vulnerable to random guessing, leading to overestimated performance and substantial performance fluctuations.","To obtain more accurate assessments of models' reasoning capabilities, we propose an automated method for synthesizing open-ended logic puzzles, and use it to develop a bilingual benchmark, AutoLogi.","Our approach features program-based verification and controllable difficulty levels, enabling more reliable evaluation that better distinguishes models' reasoning abilities.","Extensive evaluation of eight modern LLMs shows that AutoLogi can better reflect true model capabilities, with performance scores spanning from 35% to 73% compared to the narrower range of 21% to 37% on the source multiple-choice dataset.","Beyond benchmark creation, this synthesis method can generate high-quality training data by incorporating program verifiers into the rejection sampling process, enabling systematic enhancement of LLMs' reasoning capabilities across diverse datasets."],"url":"http://arxiv.org/abs/2502.16906v1"}
{"created":"2025-02-24 06:54:50","title":"Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs","abstract":"We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare tokens serving as specific effective triggers. Our findings expose a critical vulnerability in the fundamental architecture that enables cross-lingual transfer in these models. Our code and data are publicly available at https://github.com/himanshubeniwal/X-BAT.","sentences":["We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces.","Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare tokens serving as specific effective triggers.","Our findings expose a critical vulnerability in the fundamental architecture that enables cross-lingual transfer in these models.","Our code and data are publicly available at https://github.com/himanshubeniwal/X-BAT."],"url":"http://arxiv.org/abs/2502.16901v1"}
{"created":"2025-02-24 06:50:26","title":"Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning","abstract":"The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers. Traditional forecasting methods heavily rely on historical data and exhibit limited transferability across different scenarios, posing significant challenges for emerging applications in smart grids and energy internet. This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges. The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting. The framework's effectiveness was validated on a real-world dataset comprising load profiles from 20 Australian solar-powered households, demonstrating superior performance in both conventional and zero-shot scenarios. In conventional testing, our method achieved a Mean Squared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming existing approaches by at least 8\\%. In zero-shot prediction experiments across 19 households, the framework maintained consistent accuracy with a total MSE of 11.2712 and MAE of 7.6709, showing at least 12\\% improvement over current methods. The results validate the framework's potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications.","sentences":["The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers.","Traditional forecasting methods heavily rely on historical data and exhibit limited transferability across different scenarios, posing significant challenges for emerging applications in smart grids and energy internet.","This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges.","The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting.","The framework's effectiveness was validated on a real-world dataset comprising load profiles from 20 Australian solar-powered households, demonstrating superior performance in both conventional and zero-shot scenarios.","In conventional testing, our method achieved a Mean Squared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming existing approaches by at least 8\\%.","In zero-shot prediction experiments across 19 households, the framework maintained consistent accuracy with a total MSE of 11.2712 and MAE of 7.6709, showing at least 12\\% improvement over current methods.","The results validate the framework's potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications."],"url":"http://arxiv.org/abs/2502.16896v1"}
{"created":"2025-02-24 06:43:19","title":"Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data","abstract":"Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification. However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs. To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework. Our approach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa), Generative Pre-trained Transformer (GPT), and active learning, achieving high cross-task text classification performance without the need for any manually labeled data. Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency. These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application.","sentences":["Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification.","However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs.","To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework.","Our approach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa), Generative Pre-trained Transformer (GPT), and active learning, achieving high cross-task text classification performance without the need for any manually labeled data.","Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency.","These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application."],"url":"http://arxiv.org/abs/2502.16892v1"}
{"created":"2025-02-24 06:26:42","title":"APINT: A Full-Stack Framework for Acceleration of Privacy-Preserving Inference of Transformers based on Garbled Circuits","abstract":"As the importance of Privacy-Preserving Inference of Transformers (PiT) increases, a hybrid protocol that integrates Garbled Circuits (GC) and Homomorphic Encryption (HE) is emerging for its implementation. While this protocol is preferred for its ability to maintain accuracy, it has a severe drawback of excessive latency. To address this, existing protocols primarily focused on reducing HE latency, thus making GC the new latency bottleneck. Furthermore, previous studies only focused on individual computing layers, such as protocol or hardware accelerator, lacking a comprehensive solution at the system level. This paper presents APINT, a full-stack framework designed to reduce PiT's overall latency by addressing the latency problem of GC through both software and hardware solutions. APINT features a novel protocol that reallocates possible GC workloads to alternative methods (i.e., HE or standard matrix operation), substantially decreasing the GC workload. It also suggests GC-friendly circuit generation that reduces the number of AND gates at the most, which is the expensive operator in GC. Furthermore, APINT proposes an innovative netlist scheduling that combines coarse-grained operation mapping and fine-grained scheduling for maximal data reuse and minimal dependency. Finally, APINT's hardware accelerator, combined with its compiler speculation, effectively resolves the memory stall issue. Putting it all together, APINT achieves a remarkable end-to-end reduction in latency, outperforming the existing protocol on CPU platform by 12.2x online and 2.2x offline. Meanwhile, the APINT accelerator not only reduces its latency by 3.3x but also saves energy consumption by 4.6x while operating PiT compared to the state-of-the-art GC accelerator.","sentences":["As the importance of Privacy-Preserving Inference of Transformers (PiT) increases, a hybrid protocol that integrates Garbled Circuits (GC) and Homomorphic Encryption (HE) is emerging for its implementation.","While this protocol is preferred for its ability to maintain accuracy, it has a severe drawback of excessive latency.","To address this, existing protocols primarily focused on reducing HE latency, thus making GC the new latency bottleneck.","Furthermore, previous studies only focused on individual computing layers, such as protocol or hardware accelerator, lacking a comprehensive solution at the system level.","This paper presents APINT, a full-stack framework designed to reduce PiT's overall latency by addressing the latency problem of GC through both software and hardware solutions.","APINT features a novel protocol that reallocates possible GC workloads to alternative methods (i.e., HE or standard matrix operation), substantially decreasing the GC workload.","It also suggests GC-friendly circuit generation that reduces the number of AND gates at the most, which is the expensive operator in GC.","Furthermore, APINT proposes an innovative netlist scheduling that combines coarse-grained operation mapping and fine-grained scheduling for maximal data reuse and minimal dependency.","Finally, APINT's hardware accelerator, combined with its compiler speculation, effectively resolves the memory stall issue.","Putting it all together, APINT achieves a remarkable end-to-end reduction in latency, outperforming the existing protocol on CPU platform by 12.2x online and 2.2x offline.","Meanwhile, the APINT accelerator not only reduces its latency by 3.3x but also saves energy consumption by 4.6x while operating PiT compared to the state-of-the-art GC accelerator."],"url":"http://arxiv.org/abs/2502.16877v1"}
{"created":"2025-02-24 06:19:54","title":"Mitigating Hallucinations in Diffusion Models through Adaptive Attention Modulation","abstract":"Diffusion models, while increasingly adept at generating realistic images, are notably hindered by hallucinations -- unrealistic or incorrect features inconsistent with the trained data distribution. In this work, we propose Adaptive Attention Modulation (AAM), a novel approach to mitigate hallucinations by analyzing and modulating the self-attention mechanism in diffusion models. We hypothesize that self-attention during early denoising steps may inadvertently amplify or suppress features, contributing to hallucinations. To counter this, AAM introduces a temperature scaling mechanism within the softmax operation of the self-attention layers, dynamically modulating the attention distribution during inference. Additionally, AAM employs a masked perturbation technique to disrupt early-stage noise that may otherwise propagate into later stages as hallucinations. Extensive experiments demonstrate that AAM effectively reduces hallucinatory artifacts, enhancing both the fidelity and reliability of generated images. For instance, the proposed approach improves the FID score by 20.8% and reduces the percentage of hallucinated images by 12.9% (in absolute terms) on the Hands dataset.","sentences":["Diffusion models, while increasingly adept at generating realistic images, are notably hindered by hallucinations -- unrealistic or incorrect features inconsistent with the trained data distribution.","In this work, we propose Adaptive Attention Modulation (AAM), a novel approach to mitigate hallucinations by analyzing and modulating the self-attention mechanism in diffusion models.","We hypothesize that self-attention during early denoising steps may inadvertently amplify or suppress features, contributing to hallucinations.","To counter this, AAM introduces a temperature scaling mechanism within the softmax operation of the self-attention layers, dynamically modulating the attention distribution during inference.","Additionally, AAM employs a masked perturbation technique to disrupt early-stage noise that may otherwise propagate into later stages as hallucinations.","Extensive experiments demonstrate that AAM effectively reduces hallucinatory artifacts, enhancing both the fidelity and reliability of generated images.","For instance, the proposed approach improves the FID score by 20.8% and reduces the percentage of hallucinated images by 12.9% (in absolute terms) on the Hands dataset."],"url":"http://arxiv.org/abs/2502.16872v1"}
{"created":"2025-02-24 06:15:39","title":"Utilizing Social Media Analytics to Detect Trends in Saudi Arabias Evolving Market","abstract":"Saudi Arabia faced a swift economic growth and societal transformation under Vision 2030. This offers a unique opportunity to track emerging trends in the region, which will ultimately pave the way for new business and investment possibilities. This paper explores how AI and social media analytics can identify and track trends across sectors such as construction, food and beverage, tourism, technology, and entertainment thereby helping the businesses make informed decisions. By leveraging a tailored AI-driven methodology, we analyzed millions of social media posts each month, classifying discussions and calculating scores to track the trends. The approach not only uncovered the emerging trends but also shows diminishing trends. Our methodology is able to predict the emergence and growth of trends by utilizing social media data. This approach has potential for adaptation in other regions. Ultimately, our findings highlight how ongoing, AI-powered trend analysis can enable more effective, data-informed business and development strategies in an increasingly dynamic environment.","sentences":["Saudi Arabia faced a swift economic growth and societal transformation under Vision 2030.","This offers a unique opportunity to track emerging trends in the region, which will ultimately pave the way for new business and investment possibilities.","This paper explores how AI and social media analytics can identify and track trends across sectors such as construction, food and beverage, tourism, technology, and entertainment thereby helping the businesses make informed decisions.","By leveraging a tailored AI-driven methodology, we analyzed millions of social media posts each month, classifying discussions and calculating scores to track the trends.","The approach not only uncovered the emerging trends but also shows diminishing trends.","Our methodology is able to predict the emergence and growth of trends by utilizing social media data.","This approach has potential for adaptation in other regions.","Ultimately, our findings highlight how ongoing, AI-powered trend analysis can enable more effective, data-informed business and development strategies in an increasingly dynamic environment."],"url":"http://arxiv.org/abs/2502.16871v1"}
{"created":"2025-02-24 06:14:27","title":"Distributionally Robust Active Learning for Gaussian Process Regression","abstract":"Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction. Therefore, active learning (AL) for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem. However, existing AL methods do not theoretically guarantee prediction accuracy for target distribution. Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult. Thus, this paper proposes two AL methods that effectively reduce the worst-case expected error for GPR, which is the worst-case expectation in target distribution candidates. We show an upper bound of the worst-case expected squared error, which suggests that the error will be arbitrarily small by a finite number of data labels under mild conditions. Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets.","sentences":["Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction.","Therefore, active learning (AL) for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem.","However, existing AL methods do not theoretically guarantee prediction accuracy for target distribution.","Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult.","Thus, this paper proposes two AL methods that effectively reduce the worst-case expected error for GPR, which is the worst-case expectation in target distribution candidates.","We show an upper bound of the worst-case expected squared error, which suggests that the error will be arbitrarily small by a finite number of data labels under mild conditions.","Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2502.16870v1"}
{"created":"2025-02-24 06:10:49","title":"Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating Report from Raw Data","abstract":"Large Language Models (LLMs) have recently demonstrated remarkable performance in tasks such as Retrieval-Augmented Generation (RAG) and autonomous AI agent workflows. Yet, when faced with large sets of unstructured documents requiring progressive exploration, analysis, and synthesis, such as conducting literature survey, existing approaches often fall short. We address this challenge -- termed Progressive Document Investigation -- by introducing Graphy, an end-to-end platform that automates data modeling, exploration and high-quality report generation in a user-friendly manner. Graphy comprises an offline Scrapper that transforms raw documents into a structured graph of Fact and Dimension nodes, and an online Surveyor that enables iterative exploration and LLM-driven report generation. We showcase a pre-scrapped graph of over 50,000 papers -- complete with their references -- demonstrating how Graphy facilitates the literature-survey scenario. The demonstration video can be found at https://youtu.be/uM4nzkAdGlM.","sentences":["Large Language Models (LLMs) have recently demonstrated remarkable performance in tasks such as Retrieval-Augmented Generation (RAG) and autonomous AI agent workflows.","Yet, when faced with large sets of unstructured documents requiring progressive exploration, analysis, and synthesis, such as conducting literature survey, existing approaches often fall short.","We address this challenge -- termed Progressive Document Investigation -- by introducing Graphy, an end-to-end platform that automates data modeling, exploration and high-quality report generation in a user-friendly manner.","Graphy comprises an offline Scrapper that transforms raw documents into a structured graph of Fact and Dimension nodes, and an online Surveyor that enables iterative exploration and LLM-driven report generation.","We showcase a pre-scrapped graph of over 50,000 papers -- complete with their references -- demonstrating how Graphy facilitates the literature-survey scenario.","The demonstration video can be found at https://youtu.be/uM4nzkAdGlM."],"url":"http://arxiv.org/abs/2502.16868v1"}
{"created":"2025-02-24 06:00:17","title":"Multimodal Search in Chemical Documents and Reactions","abstract":"We present a multimodal search tool that facilitates retrieval of chemical reactions, molecular structures, and associated text from scientific literature. Queries may combine molecular diagrams, textual descriptions, and reaction data, allowing users to connect different representations of chemical information. To support this, the indexing process includes chemical diagram extraction and parsing, extraction of reaction data from text in tabular form, and cross-modal linking of diagrams and their mentions in text. We describe the system's architecture, key functionalities, and retrieval process, along with expert assessments of the system. This demo highlights the workflow and technical components of the search system.","sentences":["We present a multimodal search tool that facilitates retrieval of chemical reactions, molecular structures, and associated text from scientific literature.","Queries may combine molecular diagrams, textual descriptions, and reaction data, allowing users to connect different representations of chemical information.","To support this, the indexing process includes chemical diagram extraction and parsing, extraction of reaction data from text in tabular form, and cross-modal linking of diagrams and their mentions in text.","We describe the system's architecture, key functionalities, and retrieval process, along with expert assessments of the system.","This demo highlights the workflow and technical components of the search system."],"url":"http://arxiv.org/abs/2502.16865v1"}
{"created":"2025-02-24 05:56:46","title":"Potential-Based Greedy Matching for Dynamic Delivery Pooling","abstract":"We study the problem of pooling together delivery orders into a single trip, a strategy widely adopted by platforms to reduce total travel distance. Similar to other dynamic matching settings, the pooling decisions involve a trade-off between immediate reward and holding jobs for potentially better opportunities in the future. In this paper, we introduce a new heuristic dubbed potential-based greedy (PB), which aims to keep longer-distance jobs in the system, as they have higher potential reward (distance savings) from being pooled with other jobs in the future. This algorithm is simple in that it depends solely on the topology of the space, and does not rely on forecasts or partial information about future demand arrivals. We prove that PB significantly improves upon a naive greedy approach in terms of worst-case performance on the line. Moreover, we conduct extensive numerical experiments using both synthetic and real-world order-level data from the Meituan platform. Our simulations show that PB consistently outperforms not only the naive greedy heuristic but a number of benchmark algorithms, including (i) batching-based heuristics that are widely used in practice, and (ii) forecast-aware heuristics that are given the correct probability distributions (in synthetic data) or a best-effort forecast (in real data). We attribute the surprising unbeatability of PB to the fact that it is specialized for rewards defined by distance saved in delivery pooling.","sentences":["We study the problem of pooling together delivery orders into a single trip, a strategy widely adopted by platforms to reduce total travel distance.","Similar to other dynamic matching settings, the pooling decisions involve a trade-off between immediate reward and holding jobs for potentially better opportunities in the future.","In this paper, we introduce a new heuristic dubbed potential-based greedy (PB), which aims to keep longer-distance jobs in the system, as they have higher potential reward (distance savings) from being pooled with other jobs in the future.","This algorithm is simple in that it depends solely on the topology of the space, and does not rely on forecasts or partial information about future demand arrivals.","We prove that PB significantly improves upon a naive greedy approach in terms of worst-case performance on the line.","Moreover, we conduct extensive numerical experiments using both synthetic and real-world order-level data from the Meituan platform.","Our simulations show that PB consistently outperforms not only the naive greedy heuristic but a number of benchmark algorithms, including (i) batching-based heuristics that are widely used in practice, and (ii) forecast-aware heuristics that are given the correct probability distributions (in synthetic data) or a best-effort forecast (in real data).","We attribute the surprising unbeatability of PB to the fact that it is specialized for rewards defined by distance saved in delivery pooling."],"url":"http://arxiv.org/abs/2502.16862v1"}
{"created":"2025-02-24 05:53:04","title":"A Survey of fMRI to Image Reconstruction","abstract":"Functional magnetic resonance imaging (fMRI) based image reconstruction plays a pivotal role in decoding human perception, with applications in neuroscience and brain-computer interfaces. While recent advancements in deep learning and large-scale datasets have driven progress, challenges such as data scarcity, cross-subject variability, and low semantic consistency persist. To address these issues, we introduce the concept of fMRI-to-Image Learning (fMRI2Image) and present the first systematic review in this field. This review highlights key challenges, categorizes methodologies such as fMRI signal encoding, feature mapping, and image generator. Finally, promising research directions are proposed to advance this emerging frontier, providing a reference for future studies.","sentences":["Functional magnetic resonance imaging (fMRI) based image reconstruction plays a pivotal role in decoding human perception, with applications in neuroscience and brain-computer interfaces.","While recent advancements in deep learning and large-scale datasets have driven progress, challenges such as data scarcity, cross-subject variability, and low semantic consistency persist.","To address these issues, we introduce the concept of fMRI-to-Image Learning (fMRI2Image) and present the first systematic review in this field.","This review highlights key challenges, categorizes methodologies such as fMRI signal encoding, feature mapping, and image generator.","Finally, promising research directions are proposed to advance this emerging frontier, providing a reference for future studies."],"url":"http://arxiv.org/abs/2502.16861v1"}
{"created":"2025-02-24 05:51:53","title":"LongAttn: Selecting Long-context Training Data via Token-level Attention","abstract":"With the development of large language models (LLMs), there has been an increasing need for significant advancements in handling long contexts. To enhance long-context capabilities, constructing high-quality training data with long-range dependencies is crucial. Existing methods to select long-context data often rely on sentence-level analysis, which can be greatly optimized in both performance and efficiency. In this paper, we propose a novel token-level framework, LongAttn, which leverages the self-attention mechanism of LLMs to measure the long-range dependencies for the data. By calculating token-level dependency strength and distribution uniformity of token scores, LongAttn effectively quantifies long-range dependencies, enabling more accurate and efficient data selection. We filter LongABC-32K from open-source long-context datasets (ArXiv, Book, and Code). Through our comprehensive experiments, LongAttn has demonstrated its excellent effectiveness, scalability, and efficiency. To facilitate future research in long-context data, we released our code and the high-quality long-context training data LongABC-32K.","sentences":["With the development of large language models (LLMs), there has been an increasing need for significant advancements in handling long contexts.","To enhance long-context capabilities, constructing high-quality training data with long-range dependencies is crucial.","Existing methods to select long-context data often rely on sentence-level analysis, which can be greatly optimized in both performance and efficiency.","In this paper, we propose a novel token-level framework, LongAttn, which leverages the self-attention mechanism of LLMs to measure the long-range dependencies for the data.","By calculating token-level dependency strength and distribution uniformity of token scores, LongAttn effectively quantifies long-range dependencies, enabling more accurate and efficient data selection.","We filter LongABC-32K from open-source long-context datasets (ArXiv, Book, and Code).","Through our comprehensive experiments, LongAttn has demonstrated its excellent effectiveness, scalability, and efficiency.","To facilitate future research in long-context data, we released our code and the high-quality long-context training data LongABC-32K."],"url":"http://arxiv.org/abs/2502.16860v1"}
{"created":"2025-02-24 05:32:00","title":"Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models","abstract":"This paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection. The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively. The methodology involves adding noise to the dataset to improve model robustness and generalization. We used an ensemble of DeBERTa models to effectively capture complex patterns in the text. The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments.","sentences":["This paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection.","The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text.","Our team (Sarang) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively.","The methodology involves adding noise to the dataset to improve model robustness and generalization.","We used an ensemble of DeBERTa models to effectively capture complex patterns in the text.","The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments."],"url":"http://arxiv.org/abs/2502.16857v1"}
{"created":"2025-02-24 05:30:43","title":"SLABIM: A SLAM-BIM Coupled Dataset in HKUST Main Building","abstract":"Existing indoor SLAM datasets primarily focus on robot sensing, often lacking building architectures. To address this gap, we design and construct the first dataset to couple the SLAM and BIM, named SLABIM. This dataset provides BIM and SLAM-oriented sensor data, both modeling a university building at HKUST. The as-designed BIM is decomposed and converted for ease of use. We employ a multi-sensor suite for multi-session data collection and mapping to obtain the as-built model. All the related data are timestamped and organized, enabling users to deploy and test effectively. Furthermore, we deploy advanced methods and report the experimental results on three tasks: registration, localization and semantic mapping, demonstrating the effectiveness and practicality of SLABIM. We make our dataset open-source at https://github.com/HKUST-Aerial-Robotics/SLABIM.","sentences":["Existing indoor SLAM datasets primarily focus on robot sensing, often lacking building architectures.","To address this gap, we design and construct the first dataset to couple the SLAM and BIM, named SLABIM.","This dataset provides BIM and SLAM-oriented sensor data, both modeling a university building at HKUST.","The as-designed BIM is decomposed and converted for ease of use.","We employ a multi-sensor suite for multi-session data collection and mapping to obtain the as-built model.","All the related data are timestamped and organized, enabling users to deploy and test effectively.","Furthermore, we deploy advanced methods and report the experimental results on three tasks: registration, localization and semantic mapping, demonstrating the effectiveness and practicality of SLABIM.","We make our dataset open-source at https://github.com/HKUST-Aerial-Robotics/SLABIM."],"url":"http://arxiv.org/abs/2502.16856v1"}
{"created":"2025-02-24 05:10:04","title":"PulseBat: A field-accessible dataset for second-life battery diagnostics from realistic histories using multidimensional rapid pulse test","abstract":"As electric vehicles (EVs) approach the end of their operational life, their batteries retain significant economic value and present promising opportunities for second-life use and material recycling. This is particularly compelling for Global South and other underdeveloped regions, where reliable energy storage is vital to addressing critical challenges posed by weak and even nonexistent power grid and energy infrastructures. However, despite this potential, widespread adoption has been hindered by critical uncertainties surrounding the technical performance, safety, and recertification of second-life batteries. In cases where they have been redeployed, mismatches between estimated and actual performance often render batteries technically unsuitable or hazardous, turning them into liabilities for communities they were intended to benefit. This considerable misalignment exacerbates energy access disparities and undermines the broader vision of energy justice, highlighting an urgent need for robust and scalable solutions to unlock the potential. In the PulseBat Dataset, the authors tested 464 retired lithium-ion batteries, covering 3 cathode material types, 6 historical usages, 3 physical formats, and 6 capacity designs. The pulse test experiments were performed repeatedly for each second-life battery with 10 pulse width, 10 pulse magnitude, multiple state-of-charge, and state-of-health conditions, e.g., from 0.37 to 1.03. The PulseBat Dataset recorded these test conditions and the voltage response as well as the temperature signals that were subject to the injected pulse current, which could be used as a valuable data resource for critical diagnostics tasks such as state-of-charge estimation, state-of-health estimation, cathode material type identification, open-circuit voltage reconstruction, thermal management, and beyond.","sentences":["As electric vehicles (EVs) approach the end of their operational life, their batteries retain significant economic value and present promising opportunities for second-life use and material recycling.","This is particularly compelling for Global South and other underdeveloped regions, where reliable energy storage is vital to addressing critical challenges posed by weak and even nonexistent power grid and energy infrastructures.","However, despite this potential, widespread adoption has been hindered by critical uncertainties surrounding the technical performance, safety, and recertification of second-life batteries.","In cases where they have been redeployed, mismatches between estimated and actual performance often render batteries technically unsuitable or hazardous, turning them into liabilities for communities they were intended to benefit.","This considerable misalignment exacerbates energy access disparities and undermines the broader vision of energy justice, highlighting an urgent need for robust and scalable solutions to unlock the potential.","In the PulseBat Dataset, the authors tested 464 retired lithium-ion batteries, covering 3 cathode material types, 6 historical usages, 3 physical formats, and 6 capacity designs.","The pulse test experiments were performed repeatedly for each second-life battery with 10 pulse width, 10 pulse magnitude, multiple state-of-charge, and state-of-health conditions, e.g., from 0.37 to 1.03.","The PulseBat Dataset recorded these test conditions and the voltage response as well as the temperature signals that were subject to the injected pulse current, which could be used as a valuable data resource for critical diagnostics tasks such as state-of-charge estimation, state-of-health estimation, cathode material type identification, open-circuit voltage reconstruction, thermal management, and beyond."],"url":"http://arxiv.org/abs/2502.16848v1"}
{"created":"2025-02-24 05:01:37","title":"Online Friction Coefficient Identification for Legged Robots on Slippery Terrain Using Smoothed Contact Gradients","abstract":"This paper proposes an online friction coefficient identification framework for legged robots on slippery terrain. The approach formulates the optimization problem to minimize the sum of residuals between actual and predicted states parameterized by the friction coefficient in rigid body contact dynamics. Notably, the proposed framework leverages the analytic smoothed gradient of contact impulses, obtained by smoothing the complementarity condition of Coulomb friction, to solve the issue of non-informative gradients induced from the nonsmooth contact dynamics. Moreover, we introduce the rejection method to filter out data with high normal contact velocity following contact initiations during friction coefficient identification for legged robots. To validate the proposed framework, we conduct the experiments using a quadrupedal robot platform, KAIST HOUND, on slippery and nonslippery terrain. We observe that our framework achieves fast and consistent friction coefficient identification within various initial conditions.","sentences":["This paper proposes an online friction coefficient identification framework for legged robots on slippery terrain.","The approach formulates the optimization problem to minimize the sum of residuals between actual and predicted states parameterized by the friction coefficient in rigid body contact dynamics.","Notably, the proposed framework leverages the analytic smoothed gradient of contact impulses, obtained by smoothing the complementarity condition of Coulomb friction, to solve the issue of non-informative gradients induced from the nonsmooth contact dynamics.","Moreover, we introduce the rejection method to filter out data with high normal contact velocity following contact initiations during friction coefficient identification for legged robots.","To validate the proposed framework, we conduct the experiments using a quadrupedal robot platform, KAIST HOUND, on slippery and nonslippery terrain.","We observe that our framework achieves fast and consistent friction coefficient identification within various initial conditions."],"url":"http://arxiv.org/abs/2502.16843v1"}
{"created":"2025-02-24 04:54:49","title":"Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives","abstract":"Ensuring equitable Artificial Intelligence (AI) in healthcare demands systems that make unbiased decisions across all demographic groups, bridging technical innovation with ethical principles. Foundation Models (FMs), trained on vast datasets through self-supervised learning, enable efficient adaptation across medical imaging tasks while reducing dependency on labeled data. These models demonstrate potential for enhancing fairness, though significant challenges remain in achieving consistent performance across demographic groups. Our review indicates that effective bias mitigation in FMs requires systematic interventions throughout all stages of development. While previous approaches focused primarily on model-level bias mitigation, our analysis reveals that fairness in FMs requires integrated interventions throughout the development pipeline, from data documentation to deployment protocols. This comprehensive framework advances current knowledge by demonstrating how systematic bias mitigation, combined with policy engagement, can effectively address both technical and institutional barriers to equitable AI in healthcare. The development of equitable FMs represents a critical step toward democratizing advanced healthcare technologies, particularly for underserved populations and regions with limited medical infrastructure and computational resources.","sentences":["Ensuring equitable Artificial Intelligence (AI) in healthcare demands systems that make unbiased decisions across all demographic groups, bridging technical innovation with ethical principles.","Foundation Models (FMs), trained on vast datasets through self-supervised learning, enable efficient adaptation across medical imaging tasks while reducing dependency on labeled data.","These models demonstrate potential for enhancing fairness, though significant challenges remain in achieving consistent performance across demographic groups.","Our review indicates that effective bias mitigation in FMs requires systematic interventions throughout all stages of development.","While previous approaches focused primarily on model-level bias mitigation, our analysis reveals that fairness in FMs requires integrated interventions throughout the development pipeline, from data documentation to deployment protocols.","This comprehensive framework advances current knowledge by demonstrating how systematic bias mitigation, combined with policy engagement, can effectively address both technical and institutional barriers to equitable AI in healthcare.","The development of equitable FMs represents a critical step toward democratizing advanced healthcare technologies, particularly for underserved populations and regions with limited medical infrastructure and computational resources."],"url":"http://arxiv.org/abs/2502.16841v1"}
{"created":"2025-02-24 04:52:35","title":"In-context learning of evolving data streams with tabular foundational models","abstract":"State-of-the-art data stream mining in supervised classification has traditionally relied on ensembles of incremental decision trees. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees across all non-stationary benchmarks. Several promising research directions are outlined in the paper. The authors urge the community to explore these ideas, offering valuable opportunities to advance in-context stream learning.","sentences":["State-of-the-art data stream mining in supervised classification has traditionally relied on ensembles of incremental decision trees.","However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift.","These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning.","By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing.","This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments.","Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees across all non-stationary benchmarks.","Several promising research directions are outlined in the paper.","The authors urge the community to explore these ideas, offering valuable opportunities to advance in-context stream learning."],"url":"http://arxiv.org/abs/2502.16840v1"}
{"created":"2025-02-24 04:38:59","title":"A Novel Multi-Task Teacher-Student Architecture with Self-Supervised Pretraining for 48-Hour Vasoactive-Inotropic Trend Analysis in Sepsis Mortality Prediction","abstract":"Sepsis is a major cause of ICU mortality, where early recognition and effective interventions are essential for improving patient outcomes. However, the vasoactive-inotropic score (VIS) varies dynamically with a patient's hemodynamic status, complicated by irregular medication patterns, missing data, and confounders, making sepsis prediction challenging. To address this, we propose a novel Teacher-Student multitask framework with self-supervised VIS pretraining via a Masked Autoencoder (MAE). The teacher model performs mortality classification and severity-score regression, while the student distills robust time-series representations, enhancing adaptation to heterogeneous VIS data. Compared to LSTM-based methods, our approach achieves an AUROC of 0.82 on MIMIC-IV 3.0 (9,476 patients), outperforming the baseline (0.74). SHAP analysis revealed that SOFA score (0.147) had the greatest impact on ICU mortality, followed by LODS (0.033), single marital status (0.031), and Medicaid insurance (0.023), highlighting the role of sociodemographic factors. SAPSII (0.020) also contributed significantly. These findings suggest that both clinical and social factors should be considered in ICU decision-making. Our novel multitask and distillation strategies enable earlier identification of high-risk patients, improving prediction accuracy and disease management, offering new tools for ICU decision support.","sentences":["Sepsis is a major cause of ICU mortality, where early recognition and effective interventions are essential for improving patient outcomes.","However, the vasoactive-inotropic score (VIS) varies dynamically with a patient's hemodynamic status, complicated by irregular medication patterns, missing data, and confounders, making sepsis prediction challenging.","To address this, we propose a novel Teacher-Student multitask framework with self-supervised VIS pretraining via a Masked Autoencoder (MAE).","The teacher model performs mortality classification and severity-score regression, while the student distills robust time-series representations, enhancing adaptation to heterogeneous VIS data.","Compared to LSTM-based methods, our approach achieves an AUROC of 0.82 on MIMIC-IV 3.0 (9,476 patients), outperforming the baseline (0.74).","SHAP analysis revealed that SOFA score (0.147) had the greatest impact on ICU mortality, followed by LODS (0.033), single marital status (0.031), and Medicaid insurance (0.023), highlighting the role of sociodemographic factors.","SAPSII (0.020) also contributed significantly.","These findings suggest that both clinical and social factors should be considered in ICU decision-making.","Our novel multitask and distillation strategies enable earlier identification of high-risk patients, improving prediction accuracy and disease management, offering new tools for ICU decision support."],"url":"http://arxiv.org/abs/2502.16834v1"}
{"created":"2025-02-24 04:35:48","title":"FedBM: Stealing Knowledge from Pre-trained Language Models for Heterogeneous Federated Learning","abstract":"Federated learning (FL) has shown great potential in medical image computing since it provides a decentralized learning paradigm that allows multiple clients to train a model collaboratively without privacy leakage. However, current studies have shown that data heterogeneity incurs local learning bias in classifiers and feature extractors of client models during local training, leading to the performance degradation of a federation system. To address these issues, we propose a novel framework called Federated Bias eliMinating (FedBM) to get rid of local learning bias in heterogeneous federated learning (FL), which mainly consists of two modules, i.e., Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE). Specifically, LKCC exploits class concepts, prompts and pre-trained language models (PLMs) to obtain concept embeddings. These embeddings are used to estimate the latent concept distribution of each class in the linguistic space. Based on the theoretical derivation, we can rely on these distributions to pre-construct a high-quality classifier for clients to achieve classification optimization, which is frozen to avoid classifier bias during local training. CGDE samples probabilistic concept embeddings from the latent concept distributions to learn a conditional generator to capture the input space of the global model. Three regularization terms are introduced to improve the quality and utility of the generator. The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors. Extensive comparison experiments and ablation studies on public datasets demonstrate the superior performance of FedBM over state-of-the-arts and confirm the effectiveness of each module, respectively. The code is available at https://github.com/CUHK-AIM-Group/FedBM.","sentences":["Federated learning (FL) has shown great potential in medical image computing since it provides a decentralized learning paradigm that allows multiple clients to train a model collaboratively without privacy leakage.","However, current studies have shown that data heterogeneity incurs local learning bias in classifiers and feature extractors of client models during local training, leading to the performance degradation of a federation system.","To address these issues, we propose a novel framework called Federated Bias eliMinating (FedBM) to get rid of local learning bias in heterogeneous federated learning (FL), which mainly consists of two modules, i.e., Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE).","Specifically, LKCC exploits class concepts, prompts and pre-trained language models (PLMs) to obtain concept embeddings.","These embeddings are used to estimate the latent concept distribution of each class in the linguistic space.","Based on the theoretical derivation, we can rely on these distributions to pre-construct a high-quality classifier for clients to achieve classification optimization, which is frozen to avoid classifier bias during local training.","CGDE samples probabilistic concept embeddings from the latent concept distributions to learn a conditional generator to capture the input space of the global model.","Three regularization terms are introduced to improve the quality and utility of the generator.","The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors.","Extensive comparison experiments and ablation studies on public datasets demonstrate the superior performance of FedBM over state-of-the-arts and confirm the effectiveness of each module, respectively.","The code is available at https://github.com/CUHK-AIM-Group/FedBM."],"url":"http://arxiv.org/abs/2502.16832v1"}
{"created":"2025-02-24 04:23:21","title":"Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising","abstract":"Building on recent advances in Bayesian statistics and image denoising, we propose Noise2Score3D, a fully unsupervised framework for point cloud denoising that addresses the critical challenge of limited availability of clean data. Noise2Score3D learns the gradient of the underlying point cloud distribution directly from noisy data, eliminating the need for clean data during training. By leveraging Tweedie's formula, our method performs inference in a single step, avoiding the iterative processes used in existing unsupervised methods, thereby improving both performance and efficiency. Experimental results demonstrate that Noise2Score3D achieves state-of-the-art performance on standard benchmarks, outperforming other unsupervised methods in Chamfer distance and point-to-mesh metrics, and rivaling some supervised approaches. Furthermore, Noise2Score3D demonstrates strong generalization ability beyond training datasets. Additionally, we introduce Total Variation for Point Cloud, a criterion that allows for the estimation of unknown noise parameters, which further enhances the method's versatility and real-world utility.","sentences":["Building on recent advances in Bayesian statistics and image denoising, we propose Noise2Score3D, a fully unsupervised framework for point cloud denoising that addresses the critical challenge of limited availability of clean data.","Noise2Score3D learns the gradient of the underlying point cloud distribution directly from noisy data, eliminating the need for clean data during training.","By leveraging Tweedie's formula, our method performs inference in a single step, avoiding the iterative processes used in existing unsupervised methods, thereby improving both performance and efficiency.","Experimental results demonstrate that Noise2Score3D achieves state-of-the-art performance on standard benchmarks, outperforming other unsupervised methods in Chamfer distance and point-to-mesh metrics, and rivaling some supervised approaches.","Furthermore, Noise2Score3D demonstrates strong generalization ability beyond training datasets.","Additionally, we introduce Total Variation for Point Cloud, a criterion that allows for the estimation of unknown noise parameters, which further enhances the method's versatility and real-world utility."],"url":"http://arxiv.org/abs/2502.16826v1"}
{"created":"2025-02-24 04:22:57","title":"Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization","abstract":"Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to \\emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a \\emph{decline} in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $\\mu - 2\\sigma$ rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.","sentences":["Iterative data generation and model retraining are widely used to align large language models (LLMs).","It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection.","Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses.","In this work, we aim to \\emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance.","Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO.","However, our experiments reveal that this strategy leads to a \\emph{decline} in performance as the sample size increases.","To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards.","We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations.","Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $\\mu - 2\\sigma$ rather than the minimum reward, is crucial for optimal performance.","We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases."],"url":"http://arxiv.org/abs/2502.16825v1"}
{"created":"2025-02-24 04:19:15","title":"Posterior Inference with Diffusion Models for High-dimensional Black-box Optimization","abstract":"Optimizing high-dimensional and complex black-box functions is crucial in numerous scientific applications. While Bayesian optimization (BO) is a powerful method for sample-efficient optimization, it struggles with the curse of dimensionality and scaling to thousands of evaluations. Recently, leveraging generative models to solve black-box optimization problems has emerged as a promising framework. However, those methods often underperform compared to BO methods due to limited expressivity and difficulty of uncertainty estimation in high-dimensional spaces. To overcome these issues, we introduce \\textbf{DiBO}, a novel framework for solving high-dimensional black-box optimization problems. Our method iterates two stages. First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification. Second, we cast the candidate selection as a posterior inference problem to balance exploration and exploitation in high-dimensional spaces. Concretely, we fine-tune diffusion models to amortize posterior inference. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines across various synthetic and real-world black-box optimization tasks. Our code is publicly available \\href{https://github.com/umkiyoung/DiBO}{here}","sentences":["Optimizing high-dimensional and complex black-box functions is crucial in numerous scientific applications.","While Bayesian optimization (BO) is a powerful method for sample-efficient optimization, it struggles with the curse of dimensionality and scaling to thousands of evaluations.","Recently, leveraging generative models to solve black-box optimization problems has emerged as a promising framework.","However, those methods often underperform compared to BO methods due to limited expressivity and difficulty of uncertainty estimation in high-dimensional spaces.","To overcome these issues, we introduce \\textbf{DiBO}, a novel framework for solving high-dimensional black-box optimization problems.","Our method iterates two stages.","First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification.","Second, we cast the candidate selection as a posterior inference problem to balance exploration and exploitation in high-dimensional spaces.","Concretely, we fine-tune diffusion models to amortize posterior inference.","Extensive experiments demonstrate that our method outperforms state-of-the-art baselines across various synthetic and real-world black-box optimization tasks.","Our code is publicly available \\href{https://github.com/umkiyoung/DiBO}{here}"],"url":"http://arxiv.org/abs/2502.16824v1"}
{"created":"2025-02-24 04:02:16","title":"Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization","abstract":"Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data. Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, often sacrificing efficiency and interpretability. We consider the problem of efficiently denoising a new noisy data point sampled from an unknown $d$-dimensional manifold $M \\in \\mathbb{R}^D$, using only noisy samples. This work proposes a framework for test-time efficient manifold denoising, by framing the concept of \"learning-to-denoise\" as \"learning-to-optimize\". We have two technical innovations: (i) online learning methods which learn to optimize over the manifold of clean signals using only noisy data, effectively \"growing\" an optimizer one sample at a time. (ii) mixed-order methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance. We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal. Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search.","sentences":["Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data.","Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, often sacrificing efficiency and interpretability.","We consider the problem of efficiently denoising a new noisy data point sampled from an unknown $d$-dimensional manifold $M \\in \\mathbb{R}^D$, using only noisy samples.","This work proposes a framework for test-time efficient manifold denoising, by framing the concept of \"learning-to-denoise\" as \"learning-to-optimize\".","We have two technical innovations: (i) online learning methods which learn to optimize over the manifold of clean signals using only noisy data, effectively \"growing\" an optimizer one sample at a time.","(ii) mixed-order methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance.","We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal.","Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search."],"url":"http://arxiv.org/abs/2502.16819v1"}
