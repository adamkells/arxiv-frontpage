{"created":"2024-01-03 18:24:18","title":"On the hardness of learning under symmetries","abstract":"We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries (\"equivariance\") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension. Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard.","sentences":["We study the problem of learning equivariant neural networks via gradient descent.","The incorporation of known symmetries (\"equivariance\") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision.","However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent.","In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent?","We answer this question in the negative.","In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension.","Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard."],"url":"http://arxiv.org/abs/2401.01869v1"}
{"created":"2024-01-03 17:44:17","title":"The Power of Training: How Different Neural Network Setups Influence the Energy Demand","abstract":"This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.","sentences":["This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption.","While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission.","Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer.","Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results.","Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning."],"url":"http://arxiv.org/abs/2401.01851v1"}
{"created":"2024-01-03 17:36:27","title":"DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction","abstract":"Forecasting future stock trends remains challenging for academia and industry due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics influencing stock prices. In recent years, graph neural networks have achieved remarkable performance in this problem by formulating multiple stocks as graph-structured data. However, most of these approaches rely on artificially defined factors to construct static stock graphs, which fail to capture the intrinsic interdependencies between stocks that rapidly evolve. In addition, these methods often ignore the hierarchical features of the stocks and lose distinctive information within. In this work, we propose a novel graph learning approach implemented without expert knowledge to address these issues. First, our approach automatically constructs dynamic stock graphs by entropy-driven edge generation from a signal processing perspective. Then, we further learn task-optimal dependencies between stocks via a generalized graph diffusion process on constructed stock graphs. Last, a decoupled representation learning scheme is adopted to capture distinctive hierarchical intra-stock features. Experimental results demonstrate substantial improvements over state-of-the-art baselines on real-world datasets. Moreover, the ablation study and sensitivity study further illustrate the effectiveness of the proposed method in modeling the time-evolving inter-stock and intra-stock dynamics.","sentences":["Forecasting future stock trends remains challenging for academia and industry due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics influencing stock prices.","In recent years, graph neural networks have achieved remarkable performance in this problem by formulating multiple stocks as graph-structured data.","However, most of these approaches rely on artificially defined factors to construct static stock graphs, which fail to capture the intrinsic interdependencies between stocks that rapidly evolve.","In addition, these methods often ignore the hierarchical features of the stocks and lose distinctive information within.","In this work, we propose a novel graph learning approach implemented without expert knowledge to address these issues.","First, our approach automatically constructs dynamic stock graphs by entropy-driven edge generation from a signal processing perspective.","Then, we further learn task-optimal dependencies between stocks via a generalized graph diffusion process on constructed stock graphs.","Last, a decoupled representation learning scheme is adopted to capture distinctive hierarchical intra-stock features.","Experimental results demonstrate substantial improvements over state-of-the-art baselines on real-world datasets.","Moreover, the ablation study and sensitivity study further illustrate the effectiveness of the proposed method in modeling the time-evolving inter-stock and intra-stock dynamics."],"url":"http://arxiv.org/abs/2401.01846v1"}
{"created":"2024-01-03 17:22:48","title":"Investigating Semi-Supervised Learning Algorithms in Text Datasets","abstract":"Using large training datasets enhances the generalization capabilities of neural networks. Semi-supervised learning (SSL) is useful when there are few labeled data and a lot of unlabeled data. SSL methods that use data augmentation are most successful for image datasets. In contrast, texts do not have consistent augmentation methods as images. Consequently, methods that use augmentation are not as effective in text data as they are in image data. In this study, we compared SSL algorithms that do not require augmentation; these are self-training, co-training, tri-training, and tri-training with disagreement. In the experiments, we used 4 different text datasets for different tasks. We examined the algorithms from a variety of perspectives by asking experiment questions and suggested several improvements. Among the algorithms, tri-training with disagreement showed the closest performance to the Oracle; however, performance gap shows that new semi-supervised algorithms or improvements in existing methods are needed.","sentences":["Using large training datasets enhances the generalization capabilities of neural networks.","Semi-supervised learning (SSL) is useful when there are few labeled data and a lot of unlabeled data.","SSL methods that use data augmentation are most successful for image datasets.","In contrast, texts do not have consistent augmentation methods as images.","Consequently, methods that use augmentation are not as effective in text data as they are in image data.","In this study, we compared SSL algorithms that do not require augmentation; these are self-training, co-training, tri-training, and tri-training with disagreement.","In the experiments, we used 4 different text datasets for different tasks.","We examined the algorithms from a variety of perspectives by asking experiment questions and suggested several improvements.","Among the algorithms, tri-training with disagreement showed the closest performance to the Oracle; however, performance gap shows that new semi-supervised algorithms or improvements in existing methods are needed."],"url":"http://arxiv.org/abs/2401.01843v1"}
{"created":"2024-01-03 17:20:27","title":"Wasserstein Nonnegative Tensor Factorization with Manifold Regularization","abstract":"Nonnegative tensor factorization (NTF) has become an important tool for feature extraction and part-based representation with preserved intrinsic structure information from nonnegative high-order data. However, the original NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss function which treats each feature equally leading to the neglect of the side-information of features. To utilize correlation information of features and manifold information of samples, we introduce Wasserstein manifold nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein distance between the distribution of input tensorial data and the distribution of reconstruction. Although some researches about Wasserstein distance have been proposed in nonnegative matrix factorization (NMF), they ignore the spatial structure information of higher-order data. We use Wasserstein distance (a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and add a graph regularizer to a latent factor. Experimental results demonstrate the effectiveness of the proposed method compared with other NMF and NTF methods.","sentences":["Nonnegative tensor factorization (NTF) has become an important tool for feature extraction and part-based representation with preserved intrinsic structure information from nonnegative high-order data.","However, the original NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss function which treats each feature equally leading to the neglect of the side-information of features.","To utilize correlation information of features and manifold information of samples, we introduce Wasserstein manifold nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein distance between the distribution of input tensorial data and the distribution of reconstruction.","Although some researches about Wasserstein distance have been proposed in nonnegative matrix factorization (NMF), they ignore the spatial structure information of higher-order data.","We use Wasserstein distance (a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and add a graph regularizer to a latent factor.","Experimental results demonstrate the effectiveness of the proposed method compared with other NMF and NTF methods."],"url":"http://arxiv.org/abs/2401.01842v1"}
{"created":"2024-01-03 17:05:17","title":"NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems","abstract":"Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknown dynamical system towards target states. Our experiments demonstrate the effectiveness and data efficiency of NODEC for learning optimal control of unknown dynamical systems.","sentences":["Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework.","For systems with unknown dynamics, an additional step of dynamics modeling is required.","However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function.","Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency.","To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model.","Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknown dynamical system towards target states.","Our experiments demonstrate the effectiveness and data efficiency of NODEC for learning optimal control of unknown dynamical systems."],"url":"http://arxiv.org/abs/2401.01836v1"}
{"created":"2024-01-03 16:47:13","title":"Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling","abstract":"Data augmentation is an effective technique for improving the performance of machine learning models. However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision. In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model. Our method involves iteratively masking words in a sentence and replacing them with language model predictions. We have tested our proposed method on various NLP tasks and found it to be effective in many cases. Our results are presented along with a comparison to existing augmentation methods. Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets.","sentences":["Data augmentation is an effective technique for improving the performance of machine learning models.","However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision.","In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model.","Our method involves iteratively masking words in a sentence and replacing them with language model predictions.","We have tested our proposed method on various NLP tasks and found it to be effective in many cases.","Our results are presented along with a comparison to existing augmentation methods.","Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets."],"url":"http://arxiv.org/abs/2401.01830v1"}
{"created":"2024-01-03 16:43:09","title":"Data-Driven Power Modeling and Monitoring via Hardware Performance Counters Tracking","abstract":"In the current high-performance and embedded computing era, full-stack energy-centric design is paramount. Use cases require increasingly high performance at an affordable power budget, often under real-time constraints. Extreme heterogeneity and parallelism address these issues but greatly complicate online power consumption assessment, which is essential for dynamic hardware and software stack adaptations. We introduce a novel architecture-agnostic power modeling methodology with state-of-the-art accuracy, low overhead, and high responsiveness. Our methodology identifies the best Performance Monitoring Counters (PMCs) to model the power consumption of each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS) state. The individual linear models are combined into a complete model that effectively describes the power consumption of the whole system, achieving high accuracy and low overhead. Our evaluation reports an average estimation error of 7.5 % for power consumption and 1.3 % for energy. Furthermore, we propose Runmeter, an open-source, PMC-based monitoring framework integrated into the Linux kernel. Runmeter manages PMC samples collection and manipulation, efficiently evaluating our power models at runtime. With a time overhead of only 0.7 % in the worst case, Runmeter provides responsive and accurate power measurements directly in the kernel, which can be employed for actuation policies such as Dynamic Power Management (DPM) and power-aware task scheduling.","sentences":["In the current high-performance and embedded computing era, full-stack energy-centric design is paramount.","Use cases require increasingly high performance at an affordable power budget, often under real-time constraints.","Extreme heterogeneity and parallelism address these issues but greatly complicate online power consumption assessment, which is essential for dynamic hardware and software stack adaptations.","We introduce a novel architecture-agnostic power modeling methodology with state-of-the-art accuracy, low overhead, and high responsiveness.","Our methodology identifies the best Performance Monitoring Counters (PMCs) to model the power consumption of each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS) state.","The individual linear models are combined into a complete model that effectively describes the power consumption of the whole system, achieving high accuracy and low overhead.","Our evaluation reports an average estimation error of 7.5 % for power consumption and 1.3 % for energy.","Furthermore, we propose Runmeter, an open-source, PMC-based monitoring framework integrated into the Linux kernel.","Runmeter manages PMC samples collection and manipulation, efficiently evaluating our power models at runtime.","With a time overhead of only 0.7 % in the worst case, Runmeter provides responsive and accurate power measurements directly in the kernel, which can be employed for actuation policies such as Dynamic Power Management (DPM) and power-aware task scheduling."],"url":"http://arxiv.org/abs/2401.01826v1"}
{"created":"2024-01-03 16:38:56","title":"HawkRover: An Autonomous mmWave Vehicular Communication Testbed with Multi-sensor Fusion and Deep Learning","abstract":"Connected and automated vehicles (CAVs) have become a transformative technology that can change our daily life. Currently, millimeter-wave (mmWave) bands are identified as the promising CAV connectivity solution. While it can provide high data rate, their realization faces many challenges such as high attenuation during mmWave signal propagation and mobility management. Existing solution has to initiate pilot signal to measure channel information, then apply signal processing to calculate the best narrow beam towards the receiver end to guarantee sufficient signal power. This process takes significant overhead and time, hence not suitable for vehicles. In this study, we propose an autonomous and low-cost testbed to collect extensive co-located mmWave signal and other sensors data such as LiDAR (Light Detection and Ranging), cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave vehicular communications. Intuitively, these sensors can build a 3D map around the vehicle and signal propagation path can be estimated, eliminating iterative the process via pilot signals. This multimodal data fusion, together with AI, is expected to bring significant advances in ``connected'' research.","sentences":["Connected and automated vehicles (CAVs) have become a transformative technology that can change our daily life.","Currently, millimeter-wave (mmWave) bands are identified as the promising CAV connectivity solution.","While it can provide high data rate, their realization faces many challenges such as high attenuation during mmWave signal propagation and mobility management.","Existing solution has to initiate pilot signal to measure channel information, then apply signal processing to calculate the best narrow beam towards the receiver end to guarantee sufficient signal power.","This process takes significant overhead and time, hence not suitable for vehicles.","In this study, we propose an autonomous and low-cost testbed to collect extensive co-located mmWave signal and other sensors data such as LiDAR (Light Detection and Ranging), cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave vehicular communications.","Intuitively, these sensors can build a 3D map around the vehicle and signal propagation path can be estimated, eliminating iterative the process via pilot signals.","This multimodal data fusion, together with AI, is expected to bring significant advances in ``connected'' research."],"url":"http://arxiv.org/abs/2401.01822v2"}
{"created":"2024-01-03 16:38:56","title":"Detours for Navigating Instructional Videos","abstract":"We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.","sentences":["We introduce the video detours problem for navigating instructional videos.","Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration.","To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries.","Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data.","We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques.","Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%."],"url":"http://arxiv.org/abs/2401.01823v1"}
{"created":"2024-01-03 16:21:46","title":"SENS3: Multisensory Database of Finger-Surface Interactions and Corresponding Sensations","abstract":"The growing demand for natural interactions with technology underscores the importance of achieving realistic touch sensations in digital environments. Realizing this goal highly depends on comprehensive databases of finger-surface interactions, which need further development. Here, we present SENS3, an extensive open-access repository of multisensory data acquired from fifty surfaces when two participants explored them with their fingertips through static contact, pressing, tapping, and sliding. SENS3 encompasses high-fidelity visual, audio, and haptic information recorded during these interactions, including videos, sounds, contact forces, torques, positions, accelerations, skin temperature, heat flux, and surface photographs. Additionally, it incorporates thirteen participants' psychophysical sensation ratings while exploring these surfaces freely. We anticipate that SENS3 will be valuable for advancing multisensory texture rendering, user experience development, and touch sensing in robotics.","sentences":["The growing demand for natural interactions with technology underscores the importance of achieving realistic touch sensations in digital environments.","Realizing this goal highly depends on comprehensive databases of finger-surface interactions, which need further development.","Here, we present SENS3, an extensive open-access repository of multisensory data acquired from fifty surfaces when two participants explored them with their fingertips through static contact, pressing, tapping, and sliding.","SENS3 encompasses high-fidelity visual, audio, and haptic information recorded during these interactions, including videos, sounds, contact forces, torques, positions, accelerations, skin temperature, heat flux, and surface photographs.","Additionally, it incorporates thirteen participants' psychophysical sensation ratings while exploring these surfaces freely.","We anticipate that SENS3 will be valuable for advancing multisensory texture rendering, user experience development, and touch sensing in robotics."],"url":"http://arxiv.org/abs/2401.01818v1"}
{"created":"2024-01-03 16:15:22","title":"Signal Processing in the Retina: Interpretable Graph Classifier to Predict Ganglion Cell Responses","abstract":"It is a popular hypothesis in neuroscience that ganglion cells in the retina are activated by selectively detecting visual features in an observed scene. While ganglion cell firings can be predicted via data-trained deep neural nets, the networks remain indecipherable, thus providing little understanding of the cells' underlying operations. To extract knowledge from the cell firings, in this paper we learn an interpretable graph-based classifier from data to predict the firings of ganglion cells in response to visual stimuli. Specifically, we learn a positive semi-definite (PSD) metric matrix $\\mathbf{M} \\succeq 0$ that defines Mahalanobis distances between graph nodes (visual events) endowed with pre-computed feature vectors; the computed inter-node distances lead to edge weights and a combinatorial graph that is amenable to binary classification. Mathematically, we define the objective of metric matrix $\\mathbf{M}$ optimization using a graph adaptation of large margin nearest neighbor (LMNN), which is rewritten as a semi-definite programming (SDP) problem. We solve it efficiently via a fast approximation called Gershgorin disc perfect alignment (GDPA) linearization. The learned metric matrix $\\mathbf{M}$ provides interpretability: important features are identified along $\\mathbf{M}$'s diagonal, and their mutual relationships are inferred from off-diagonal terms. Our fast metric learning framework can be applied to other biological systems with pre-chosen features that require interpretation.","sentences":["It is a popular hypothesis in neuroscience that ganglion cells in the retina are activated by selectively detecting visual features in an observed scene.","While ganglion cell firings can be predicted via data-trained deep neural nets, the networks remain indecipherable, thus providing little understanding of the cells' underlying operations.","To extract knowledge from the cell firings, in this paper we learn an interpretable graph-based classifier from data to predict the firings of ganglion cells in response to visual stimuli.","Specifically, we learn a positive semi-definite (PSD) metric matrix $\\mathbf{M} \\succeq 0$ that defines Mahalanobis distances between graph nodes (visual events) endowed with pre-computed feature vectors; the computed inter-node distances lead to edge weights and a combinatorial graph that is amenable to binary classification.","Mathematically, we define the objective of metric matrix $\\mathbf{M}$ optimization using a graph adaptation of large margin nearest neighbor (LMNN), which is rewritten as a semi-definite programming (SDP) problem.","We solve it efficiently via a fast approximation called Gershgorin disc perfect alignment (GDPA) linearization.","The learned metric matrix $\\mathbf{M}$ provides interpretability: important features are identified along $\\mathbf{M}$'s diagonal, and their mutual relationships are inferred from off-diagonal terms.","Our fast metric learning framework can be applied to other biological systems with pre-chosen features that require interpretation."],"url":"http://arxiv.org/abs/2401.01813v1"}
{"created":"2024-01-03 15:36:33","title":"Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review","abstract":"According to the World Health Organization (WHO), air pollution kills seven million people every year. Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries. In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction. The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used. Two research questions were formulated for this review. 1086 publications were collected in the initial PRISMA stage. After the screening and eligibility phases, 37 papers were selected for inclusion. A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction. Three methods of prediction were identified: time series, feature-based and spatio-temporal. This review's findings identify major limitations in applications found in the literature, namely lack of coverage, lack of diversity of data and lack of inclusion of context-specific features. This review proposes directions for future research and underlines practical implications in healthcare, urban planning, global synergy and smart cities.","sentences":["According to the World Health Organization (WHO), air pollution kills seven million people every year.","Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries.","In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction.","The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used.","Two research questions were formulated for this review.","1086 publications were collected in the initial PRISMA stage.","After the screening and eligibility phases, 37 papers were selected for inclusion.","A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction.","Three methods of prediction were identified: time series, feature-based and spatio-temporal.","This review's findings identify major limitations in applications found in the literature, namely lack of coverage, lack of diversity of data and lack of inclusion of context-specific features.","This review proposes directions for future research and underlines practical implications in healthcare, urban planning, global synergy and smart cities."],"url":"http://arxiv.org/abs/2401.01788v1"}
{"created":"2024-01-03 15:27:10","title":"An experimental sorting method for improving metagenomic data encoding","abstract":"Minimizing data storage poses a significant challenge in large-scale metagenomic projects. In this paper, we present a new method for improving the encoding of FASTQ files generated by metagenomic sequencing. This method incorporates metagenomic classification followed by a recursive filter for clustering reads by DNA sequence similarity to improve the overall reference-free compression. In the results, we show an overall improvement in the compression of several datasets. As hypothesized, we show a progressive compression gain for higher coverage depth and number of identified species. Additionally, we provide an implementation that is freely available at https://github.com/cobilab/mizar and can be customized to work with other FASTQ compression tools.","sentences":["Minimizing data storage poses a significant challenge in large-scale metagenomic projects.","In this paper, we present a new method for improving the encoding of FASTQ files generated by metagenomic sequencing.","This method incorporates metagenomic classification followed by a recursive filter for clustering reads by DNA sequence similarity to improve the overall reference-free compression.","In the results, we show an overall improvement in the compression of several datasets.","As hypothesized, we show a progressive compression gain for higher coverage depth and number of identified species.","Additionally, we provide an implementation that is freely available at https://github.com/cobilab/mizar and can be customized to work with other FASTQ compression tools."],"url":"http://arxiv.org/abs/2401.01786v1"}
{"created":"2024-01-03 15:12:42","title":"Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering","abstract":"While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data. Our model directly provides answers for $78.2\\%$ of the known queries and opts to search for $77.2\\%$ of the unknown ones. This results in the API being utilized only $62\\%$ of the time.","sentences":["While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination.","Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers.","Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information.","However, searching in a large collection of documents introduces additional computational/time costs.","An optimal behavior would be to query external resources only when the LLM is not confident about answers.","In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool.","We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task.","In addition, we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data.","Our model directly provides answers for $78.2\\%$ of the known queries and opts to search for $77.2\\%$ of the unknown ones.","This results in the API being utilized only $62\\%$ of the time."],"url":"http://arxiv.org/abs/2401.01780v1"}
{"created":"2024-01-03 14:52:18","title":"A Novel Paradigm for Neural Computation: X-Net with Learnable Neurons and Adaptable Structure","abstract":"Artificial neural networks (ANNs) have permeated various disciplinary domains, ranging from bioinformatics to financial analytics, where their application has become an indispensable facet of contemporary scientific research endeavors. However, the inherent limitations of traditional neural networks arise due to their relatively fixed network structures and activation functions. 1, The type of activation function is single and relatively fixed, which leads to poor \"unit representation ability\" of the network, and it is often used to solve simple problems with very complex networks; 2, the network structure is not adaptive, it is easy to cause network structure redundant or insufficient. To address the aforementioned issues, this study proposes a novel neural network called X-Net. By utilizing our designed Alternating Backpropagation mechanism, X-Net dynamically selects appropriate activation functions based on derivative information during training to enhance the network's representation capability for specific tasks. Simultaneously, it accurately adjusts the network structure at the neuron level to accommodate tasks of varying complexities and reduce computational costs. Through a series of experiments, we demonstrate the dual advantages of X-Net in terms of reducing model size and improving representation power. Specifically, in terms of the number of parameters, X-Net is only 3$\\%$ of baselines on average, and only 1.4$\\%$ under some tasks. In terms of representation ability, X-Net can achieve an average $R^2$=0.985 on the fitting task by only optimizing the activation function without introducing any parameters. Finally, we also tested the ability of X-Net to help scientific discovery on data from multiple disciplines such as society, energy, environment, and aerospace, and achieved concise and good results.","sentences":["Artificial neural networks (ANNs) have permeated various disciplinary domains, ranging from bioinformatics to financial analytics, where their application has become an indispensable facet of contemporary scientific research endeavors.","However, the inherent limitations of traditional neural networks arise due to their relatively fixed network structures and activation functions.","1, The type of activation function is single and relatively fixed, which leads to poor \"unit representation ability\" of the network, and it is often used to solve simple problems with very complex networks; 2, the network structure is not adaptive, it is easy to cause network structure redundant or insufficient.","To address the aforementioned issues, this study proposes a novel neural network called X-Net.","By utilizing our designed Alternating Backpropagation mechanism, X-Net dynamically selects appropriate activation functions based on derivative information during training to enhance the network's representation capability for specific tasks.","Simultaneously, it accurately adjusts the network structure at the neuron level to accommodate tasks of varying complexities and reduce computational costs.","Through a series of experiments, we demonstrate the dual advantages of X-Net in terms of reducing model size and improving representation power.","Specifically, in terms of the number of parameters, X-Net is only 3$\\%$ of baselines on average, and only 1.4$\\%$ under some tasks.","In terms of representation ability, X-Net can achieve an average $R^2$=0.985 on the fitting task by only optimizing the activation function without introducing any parameters.","Finally, we also tested the ability of X-Net to help scientific discovery on data from multiple disciplines such as society, energy, environment, and aerospace, and achieved concise and good results."],"url":"http://arxiv.org/abs/2401.01772v1"}
{"created":"2024-01-03 14:28:55","title":"Cross-target Stance Detection by Exploiting Target Analytical Perspectives","abstract":"Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target. One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets. However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge. In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge. First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM). Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments results demonstrate the superiority of MPPT against the state-of-the-art baseline methods.","sentences":["Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target.","One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets.","However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge.","In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge.","First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM).","Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor.","Extensive experiments results demonstrate the superiority of MPPT against the state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2401.01761v2"}
{"created":"2024-01-03 14:19:04","title":"Fuzzy Logic Controller Design for Mobile Robot Outdoor Navigation","abstract":"Many researchers around the world are researching to get control solutions that enhance robots' ability to navigate in dynamic environments autonomously. However, until these days robots have limited capability and many navigation tasks on Earth and other planets have been difficult so far. This paperwork presents the development of a control system for a differential drive-wheeled mobile robot that autonomously controls its position, heading, and speed based on destination information given and surrounding data gathered through mounted proximity and GPS sensors. The intelligence of this control system is implemented by using a fuzzy logic algorithm which is a very powerful tool to handle un-modeled systems like the dynamically changing environment dealt with in this research. The fuzzy controller is used to address the problems associated with navigation in an obstacle-strewn environment. Such issues include position estimation, path planning, and obstacle avoidance. In this study modeling, design, and simulation of the system have been done. The simulation result shows that the developed mobile robot travels successfully from any location to the destination location without colliding with obstacles.","sentences":["Many researchers around the world are researching to get control solutions that enhance robots' ability to navigate in dynamic environments autonomously.","However, until these days robots have limited capability and many navigation tasks on Earth and other planets have been difficult so far.","This paperwork presents the development of a control system for a differential drive-wheeled mobile robot that autonomously controls its position, heading, and speed based on destination information given and surrounding data gathered through mounted proximity and GPS sensors.","The intelligence of this control system is implemented by using a fuzzy logic algorithm which is a very powerful tool to handle un-modeled systems like the dynamically changing environment dealt with in this research.","The fuzzy controller is used to address the problems associated with navigation in an obstacle-strewn environment.","Such issues include position estimation, path planning, and obstacle avoidance.","In this study modeling, design, and simulation of the system have been done.","The simulation result shows that the developed mobile robot travels successfully from any location to the destination location without colliding with obstacles."],"url":"http://arxiv.org/abs/2401.01756v1"}
{"created":"2024-01-03 13:57:09","title":"Few-shot Image Generation via Information Transfer from the Built Geodesic Surface","abstract":"Images generated by most of generative models trained with limited data often exhibit deficiencies in either fidelity, diversity, or both. One effective solution to address the limitation is few-shot generative model adaption. However, the type of approaches typically rely on a large-scale pre-trained model, serving as a source domain, to facilitate information transfer to the target domain. In this paper, we propose a method called Information Transfer from the Built Geodesic Surface (ITBGS), which contains two module: Feature Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization (I\\&R). With the FAGS module, a pseudo-source domain is created by projecting image features from the training dataset into the Pre-Shape Space, subsequently generating new features on the Geodesic surface. Thus, no pre-trained models is needed for the adaption process during the training of generative models with FAGS. I\\&R module are introduced for supervising the interpolated images and regularizing their relative distances, respectively, to further enhance the quality of generated images. Through qualitative and quantitative experiments, we demonstrate that the proposed method consistently achieves optimal or comparable results across a diverse range of semantically distinct datasets, even in extremely few-shot scenarios.","sentences":["Images generated by most of generative models trained with limited data often exhibit deficiencies in either fidelity, diversity, or both.","One effective solution to address the limitation is few-shot generative model adaption.","However, the type of approaches typically rely on a large-scale pre-trained model, serving as a source domain, to facilitate information transfer to the target domain.","In this paper, we propose a method called Information Transfer from the Built Geodesic Surface (ITBGS), which contains two module: Feature Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization (I\\&R).","With the FAGS module, a pseudo-source domain is created by projecting image features from the training dataset into the Pre-Shape Space, subsequently generating new features on the Geodesic surface.","Thus, no pre-trained models is needed for the adaption process during the training of generative models with FAGS.","I\\&R module are introduced for supervising the interpolated images and regularizing their relative distances, respectively, to further enhance the quality of generated images.","Through qualitative and quantitative experiments, we demonstrate that the proposed method consistently achieves optimal or comparable results across a diverse range of semantically distinct datasets, even in extremely few-shot scenarios."],"url":"http://arxiv.org/abs/2401.01749v1"}
{"created":"2024-01-03 13:37:01","title":"Minimizing the Weighted Number of Tardy Jobs is W[1]-hard","abstract":"We consider the $1||\\sum w_J U_j$ problem, the problem of minimizing the weighted number of tardy jobs on a single machine. This problem is one of the most basic and fundamental problems in scheduling theory, with several different applications both in theory and practice. We prove that $1||\\sum w_J U_j$ is W[1]-hard with respect to the number $p_{\\#}$ of different processing times in the input, as well as with respect to the number $w_{\\#}$ of different weights in the input. This, along with previous work, provides a complete picture for $1||\\sum w_J U_j$ from the perspective of parameterized complexity, as well as almost tight complexity bounds for the problem under the Exponential Time Hypothesis (ETH).","sentences":["We consider the $1||\\sum w_J U_j$ problem, the problem of minimizing the weighted number of tardy jobs on a single machine.","This problem is one of the most basic and fundamental problems in scheduling theory, with several different applications both in theory and practice.","We prove that $1||\\sum w_J U_j$ is W[1]-hard with respect to the number $p_{\\#}$ of different processing times in the input, as well as with respect to the number $w_{\\#}$ of different weights in the input.","This, along with previous work, provides a complete picture for $1||\\sum w_J U_j$ from the perspective of parameterized complexity, as well as almost tight complexity bounds for the problem under the Exponential Time Hypothesis (ETH)."],"url":"http://arxiv.org/abs/2401.01740v1"}
{"created":"2024-01-03 13:16:38","title":"Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data","abstract":"Assistive robots should be able to wash, fold or iron clothes. However, due to the variety, deformability and self-occlusions of clothes, creating general-purpose robot systems for cloth manipulation is challenging. Synthetic data is a promising direction to improve generalization, though its usability is often limited by the sim-to-real gap. To advance the use of synthetic data for cloth manipulation and to enable tasks such as robotic folding, we present a synthetic data pipeline to train keypoint detectors for almost flattened cloth items. To test its performance, we have also collected a real-world dataset. We train detectors for both T-shirts, towels and shorts and obtain an average precision of 64.3%. Fine-tuning on real-world data improves performance to 74.2%. Additional insight is provided by discussing various failure modes of the keypoint detectors and by comparing different approaches to obtain cloth meshes and materials. We also quantify the remaining sim-to-real gap and argue that further improvements to the fidelity of cloth assets will be required to further reduce this gap. The code, dataset and trained models are available online.","sentences":["Assistive robots should be able to wash, fold or iron clothes.","However, due to the variety, deformability and self-occlusions of clothes, creating general-purpose robot systems for cloth manipulation is challenging.","Synthetic data is a promising direction to improve generalization, though its usability is often limited by the sim-to-real gap.","To advance the use of synthetic data for cloth manipulation and to enable tasks such as robotic folding, we present a synthetic data pipeline to train keypoint detectors for almost flattened cloth items.","To test its performance, we have also collected a real-world dataset.","We train detectors for both T-shirts, towels and shorts and obtain an average precision of 64.3%.","Fine-tuning on real-world data improves performance to 74.2%.","Additional insight is provided by discussing various failure modes of the keypoint detectors and by comparing different approaches to obtain cloth meshes and materials.","We also quantify the remaining sim-to-real gap and argue that further improvements to the fidelity of cloth assets will be required to further reduce this gap.","The code, dataset and trained models are available online."],"url":"http://arxiv.org/abs/2401.01734v1"}
{"created":"2024-01-03 13:12:04","title":"Investigating the Suitability of Concept Drift Detection for Detecting Leakages in Water Distribution Networks","abstract":"Leakages are a major risk in water distribution networks as they cause water loss and increase contamination risks. Leakage detection is a difficult task due to the complex dynamics of water distribution networks. In particular, small leakages are hard to detect. From a machine-learning perspective, leakages can be modeled as concept drift. Thus, a wide variety of drift detection schemes seems to be a suitable choice for detecting leakages. In this work, we explore the potential of model-loss-based and distribution-based drift detection methods to tackle leakage detection. We additionally discuss the issue of temporal dependencies in the data and propose a way to cope with it when applying distribution-based detection. We evaluate different methods systematically for leakages of different sizes and detection times. Additionally, we propose a first drift-detection-based technique for localizing leakages.","sentences":["Leakages are a major risk in water distribution networks as they cause water loss and increase contamination risks.","Leakage detection is a difficult task due to the complex dynamics of water distribution networks.","In particular, small leakages are hard to detect.","From a machine-learning perspective, leakages can be modeled as concept drift.","Thus, a wide variety of drift detection schemes seems to be a suitable choice for detecting leakages.","In this work, we explore the potential of model-loss-based and distribution-based drift detection methods to tackle leakage detection.","We additionally discuss the issue of temporal dependencies in the data and propose a way to cope with it when applying distribution-based detection.","We evaluate different methods systematically for leakages of different sizes and detection times.","Additionally, we propose a first drift-detection-based technique for localizing leakages."],"url":"http://arxiv.org/abs/2401.01733v1"}
{"created":"2024-01-03 13:07:07","title":"Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices","abstract":"Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively execute global parameter averaging across all clusters. We have framed our asynchronous SGD loss function as a block structured optimization problem with delayed updates and derived an optimal convergence rate of $O\\left(\\frac{1}{\\sqrt{K}}\\right)$. We further discuss linear speedup with respect to the number of participating clusters and the bound on the staleness parameter.","sentences":["Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets.","This trend is expected to continue.","However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales.","This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics.","Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model.","These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively execute global parameter averaging across all clusters.","We have framed our asynchronous SGD loss function as a block structured optimization problem with delayed updates and derived an optimal convergence rate of $O\\left(\\frac{1}{\\sqrt{K}}\\right)$. We further discuss linear speedup with respect to the number of participating clusters and the bound on the staleness parameter."],"url":"http://arxiv.org/abs/2401.01728v1"}
{"created":"2024-01-03 12:55:24","title":"Limited Feedback on Measurements: Sharing a Codebook or a Generative Model?","abstract":"Discrete Fourier transform (DFT) codebook-based solutions are well-established for limited feedback schemes in frequency division duplex (FDD) systems. In recent years, data-aided solutions have been shown to achieve higher performance, enabled by the adaptivity of the feedback scheme to the propagation environment of the base station (BS) cell. In particular, a versatile limited feedback scheme utilizing Gaussian mixture models (GMMs) was recently introduced. The scheme supports multi-user communications, exhibits low complexity, supports parallelization, and offers significant flexibility concerning various system parameters. Conceptually, a GMM captures environment knowledge and is subsequently transferred to the mobile terminals (MTs) for online inference of feedback information. Afterward, the BS designs precoders using either directional information or a generative modeling-based approach. A major shortcoming of recent works is that the assessed system performance is only evaluated through synthetic simulation data that is generally unable to fully characterize the features of real-world environments. It raises the question of how the GMM-based feedback scheme performs on real-world measurement data, especially compared to the well-established DFT-based solution. Our experiments reveal that the GMM-based feedback scheme tremendously improves the system performance measured in terms of sum-rate, allowing to deploy systems with fewer pilots or feedback bits.","sentences":["Discrete Fourier transform (DFT) codebook-based solutions are well-established for limited feedback schemes in frequency division duplex (FDD) systems.","In recent years, data-aided solutions have been shown to achieve higher performance, enabled by the adaptivity of the feedback scheme to the propagation environment of the base station (BS) cell.","In particular, a versatile limited feedback scheme utilizing Gaussian mixture models (GMMs) was recently introduced.","The scheme supports multi-user communications, exhibits low complexity, supports parallelization, and offers significant flexibility concerning various system parameters.","Conceptually, a GMM captures environment knowledge and is subsequently transferred to the mobile terminals (MTs) for online inference of feedback information.","Afterward, the BS designs precoders using either directional information or a generative modeling-based approach.","A major shortcoming of recent works is that the assessed system performance is only evaluated through synthetic simulation data that is generally unable to fully characterize the features of real-world environments.","It raises the question of how the GMM-based feedback scheme performs on real-world measurement data, especially compared to the well-established DFT-based solution.","Our experiments reveal that the GMM-based feedback scheme tremendously improves the system performance measured in terms of sum-rate, allowing to deploy systems with fewer pilots or feedback bits."],"url":"http://arxiv.org/abs/2401.01721v1"}
{"created":"2024-01-03 12:05:38","title":"Patterns of Persistence and Diffusibility across World's Languages","abstract":"Language similarities can be caused by genetic relatedness, areal contact, universality, or chance. Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored. In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility). We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages. We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones. Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another. Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics.","sentences":["Language similarities can be caused by genetic relatedness, areal contact, universality, or chance.","Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored.","In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility).","We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages.","We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones.","Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another.","Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics."],"url":"http://arxiv.org/abs/2401.01698v1"}
{"created":"2024-01-03 11:54:48","title":"AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with Detail-Preserving Model-based Deep Learning","abstract":"Deep learning has shown great potential in accelerating diffusion tensor imaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise and detail loss in reconstructing the DTI-derived parametric maps especially when sparsely sampled q-space data are used. This paper proposes a novel method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to facilitate fast and accurate DTI with only six measurements. AID-DTI is equipped with a newly designed Singular Value Decomposition (SVD)-based regularizer, which can effectively capture fine details while suppressing noise during network training. Experimental results on Human Connectome Project (HCP) data consistently demonstrate that the proposed method estimates DTI parameter maps with fine-grained details and outperforms three state-of-the-art methods both quantitatively and qualitatively.","sentences":["Deep learning has shown great potential in accelerating diffusion tensor imaging (DTI).","Nevertheless, existing methods tend to suffer from Rician noise and detail loss in reconstructing the DTI-derived parametric maps especially when sparsely sampled q-space data are used.","This paper proposes a novel method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to facilitate fast and accurate DTI with only six measurements.","AID-DTI is equipped with a newly designed Singular Value Decomposition (SVD)-based regularizer, which can effectively capture fine details while suppressing noise during network training.","Experimental results on Human Connectome Project (HCP) data consistently demonstrate that the proposed method estimates DTI parameter maps with fine-grained details and outperforms three state-of-the-art methods both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2401.01693v1"}
{"created":"2024-01-03 11:49:07","title":"Zero-shot Active Learning Using Self Supervised Learning","abstract":"Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.","sentences":["Deep learning algorithms are often said to be data hungry.","The performance of such algorithms generally improve as more and more annotated data is fed into the model.","While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task.","Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget.","In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process.","We aim to leverage self-supervised learnt features for the task of Active Learning.","The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation."],"url":"http://arxiv.org/abs/2401.01690v1"}
{"created":"2024-01-03 10:47:20","title":"Simultaneous q-Space Sampling Optimization and Reconstruction for Fast and High-fidelity Diffusion Magnetic Resonance Imaging","abstract":"Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the noninvasive investigation of tissue microstructural properties and structural connectivity in the \\textit{in vivo} human brain. However, to effectively capture the intricate characteristics of water diffusion at various directions and scales, it is important to employ comprehensive q-space sampling. Unfortunately, this requirement leads to long scan times, limiting the clinical applicability of dMRI. To address this challenge, we propose SSOR, a Simultaneous q-Space sampling Optimization and Reconstruction framework. We jointly optimize a subset of q-space samples using a continuous representation of spherical harmonic functions and a reconstruction network. Additionally, we integrate the unique properties of diffusion magnetic resonance imaging (dMRI) in both the q-space and image domains by applying $l1$-norm and total-variation regularization. The experiments conducted on HCP data demonstrate that SSOR has promising strengths both quantitatively and qualitatively and exhibits robustness to noise.","sentences":["Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the noninvasive investigation of tissue microstructural properties and structural connectivity in the \\textit{in vivo} human brain.","However, to effectively capture the intricate characteristics of water diffusion at various directions and scales, it is important to employ comprehensive q-space sampling.","Unfortunately, this requirement leads to long scan times, limiting the clinical applicability of dMRI.","To address this challenge, we propose SSOR, a Simultaneous q-Space sampling Optimization and Reconstruction framework.","We jointly optimize a subset of q-space samples using a continuous representation of spherical harmonic functions and a reconstruction network.","Additionally, we integrate the unique properties of diffusion magnetic resonance imaging (dMRI) in both the q-space and image domains by applying $l1$-norm and total-variation regularization.","The experiments conducted on HCP data demonstrate that SSOR has promising strengths both quantitatively and qualitatively and exhibits robustness to noise."],"url":"http://arxiv.org/abs/2401.01662v1"}
{"created":"2024-01-03 10:18:55","title":"Near Real-Time Data-Driven Control of Virtual Reality Traffic in Open Radio Access Network","abstract":"In mobile networks, Open Radio Access Network (ORAN) provides a framework for implementing network slicing that interacts with the resources at the lower layers. Both monitoring and Radio Access Network (RAN) control is feasible for both 4G and 5G systems. In this work, we consider how data-driven resource allocation in a 4G context can enable adaptive slice allocation to steer the experienced latency of Virtual Reality (VR) traffic towards a requested latency. We develop an xApp for the near real-time RAN Intelligent Controller (RIC) that embeds a heuristic algorithm for latency control, aiming to: (1) maintain latency of a VR stream around a requested value; and (2) improve the available RAN allocation to offer higher bit rate to another user. We have experimentally demonstrated the proposed approach in an ORAN testbed. Our results show that the data-driven approach can dynamically follow the variation of the traffic load while satisfying the required latency. This results in 15.8% more resources to secondary users than a latency-equivalent static allocation.","sentences":["In mobile networks, Open Radio Access Network (ORAN) provides a framework for implementing network slicing that interacts with the resources at the lower layers.","Both monitoring and Radio Access Network (RAN) control is feasible for both 4G and 5G systems.","In this work, we consider how data-driven resource allocation in a 4G context can enable adaptive slice allocation to steer the experienced latency of Virtual Reality (VR) traffic towards a requested latency.","We develop an xApp for the near real-time RAN Intelligent Controller (RIC) that embeds a heuristic algorithm for latency control, aiming to: (1) maintain latency of a VR stream around a requested value; and (2) improve the available RAN allocation to offer higher bit rate to another user.","We have experimentally demonstrated the proposed approach in an ORAN testbed.","Our results show that the data-driven approach can dynamically follow the variation of the traffic load while satisfying the required latency.","This results in 15.8% more resources to secondary users than a latency-equivalent static allocation."],"url":"http://arxiv.org/abs/2401.01652v1"}
{"created":"2024-01-03 10:07:11","title":"De-Confusing Pseudo-Labels in Source-Free Domain Adaptation","abstract":"Source-free domain adaptation (SFDA) aims to transfer knowledge learned from a source domain to an unlabeled target domain, where the source data is unavailable during adaptation. Existing approaches for SFDA focus on self-training usually including well-established entropy minimization and pseudo-labeling techniques. Recent work suggested a co-learning strategy to improve the quality of the generated target pseudo-labels using robust pretrained networks such as Swin-B. However, since the generated pseudo-labels depend on the source model, they may be noisy due to domain shift. In this paper, we view SFDA from the perspective of label noise learning and learn to de-confuse the pseudo-labels. More specifically, we learn a noise transition matrix of the pseudo-labels to capture the label corruption of each class and learn the underlying true label distribution. Estimating the noise transition matrix enables a better true class-posterior estimation results with better prediction accuracy. We demonstrate the effectiveness of our approach applied with several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art results on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome.","sentences":["Source-free domain adaptation (SFDA) aims to transfer knowledge learned from a source domain to an unlabeled target domain, where the source data is unavailable during adaptation.","Existing approaches for SFDA focus on self-training usually including well-established entropy minimization and pseudo-labeling techniques.","Recent work suggested a co-learning strategy to improve the quality of the generated target pseudo-labels using robust pretrained networks such as Swin-B. However, since the generated pseudo-labels depend on the source model, they may be noisy due to domain shift.","In this paper, we view SFDA from the perspective of label noise learning and learn to de-confuse the pseudo-labels.","More specifically, we learn a noise transition matrix of the pseudo-labels to capture the label corruption of each class and learn the underlying true label distribution.","Estimating the noise transition matrix enables a better true class-posterior estimation results with better prediction accuracy.","We demonstrate the effectiveness of our approach applied with several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art results on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome."],"url":"http://arxiv.org/abs/2401.01650v1"}
{"created":"2024-01-03 09:39:36","title":"Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction","abstract":"Multimodal learning significantly benefits cancer survival prediction, especially the integration of pathological images and genomic data. Despite advantages of multimodal learning for cancer survival prediction, massive redundancy in multimodal data prevents it from extracting discriminative and compact information: (1) An extensive amount of intra-modal task-unrelated information blurs discriminability, especially for gigapixel whole slide images (WSIs) with many patches in pathology and thousands of pathways in genomic data, leading to an ``intra-modal redundancy\" issue. (2) Duplicated information among modalities dominates the representation of multimodal data, which makes modality-specific information prone to being ignored, resulting in an ``inter-modal redundancy\" issue. To address these, we propose a new framework, Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and Prototypical Information Disentanglement (PID) module for inter-modal redundancy. Specifically, a variant of information bottleneck, PIB, is proposed to model prototypes approximating a bunch of instances for different risk levels, which can be used for selection of discriminative instances within modality. PID module decouples entangled multimodal data into compact distinct components: modality-common and modality-specific knowledge, under the guidance of the joint prototypical distribution. Extensive experiments on five cancer benchmark datasets demonstrated our superiority over other methods.","sentences":["Multimodal learning significantly benefits cancer survival prediction, especially the integration of pathological images and genomic data.","Despite advantages of multimodal learning for cancer survival prediction, massive redundancy in multimodal data prevents it from extracting discriminative and compact information: (1) An extensive amount of intra-modal task-unrelated information blurs discriminability, especially for gigapixel whole slide images (WSIs) with many patches in pathology and thousands of pathways in genomic data, leading to an ``intra-modal redundancy\" issue.","(2) Duplicated information among modalities dominates the representation of multimodal data, which makes modality-specific information prone to being ignored, resulting in an ``inter-modal redundancy\" issue.","To address these, we propose a new framework, Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and Prototypical Information Disentanglement (PID) module for inter-modal redundancy.","Specifically, a variant of information bottleneck, PIB, is proposed to model prototypes approximating a bunch of instances for different risk levels, which can be used for selection of discriminative instances within modality.","PID module decouples entangled multimodal data into compact distinct components: modality-common and modality-specific knowledge, under the guidance of the joint prototypical distribution.","Extensive experiments on five cancer benchmark datasets demonstrated our superiority over other methods."],"url":"http://arxiv.org/abs/2401.01646v1"}
{"created":"2024-01-03 09:32:48","title":"Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences","abstract":"Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly improves value detection rate at high precision thresholds and transfers well to out-of-domain distributions.","sentences":["Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction.","Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data.","Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions.","In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions.","Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks.","We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets.","The embedding model significantly improves value detection rate at high precision thresholds and transfers well to out-of-domain distributions."],"url":"http://arxiv.org/abs/2401.01641v2"}
{"created":"2024-01-03 09:31:43","title":"Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data","abstract":"Self-supervised learning (SSL) has become the de facto training paradigm of large models where pre-training is followed by supervised fine-tuning using domain-specific data and labels. Hypothesizing that SSL models would learn more generic, hence less biased, representations, this study explores the impact of pre-training and fine-tuning strategies on fairness (i.e., performing equally on different demographic breakdowns). Motivated by human-centric applications on real-world timeseries data, we interpret inductive biases on the model, layer, and metric levels by systematically comparing SSL models to their supervised counterparts. Our findings demonstrate that SSL has the capacity to achieve performance on par with supervised methods while significantly enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1% loss in performance through self-supervision. Ultimately, this work underscores SSL's potential in human-centric computing, particularly high-stakes, data-scarce application domains like healthcare.","sentences":["Self-supervised learning (SSL) has become the de facto training paradigm of large models where pre-training is followed by supervised fine-tuning using domain-specific data and labels.","Hypothesizing that SSL models would learn more generic, hence less biased, representations, this study explores the impact of pre-training and fine-tuning strategies on fairness (i.e., performing equally on different demographic breakdowns).","Motivated by human-centric applications on real-world timeseries data, we interpret inductive biases on the model, layer, and metric levels by systematically comparing SSL models to their supervised counterparts.","Our findings demonstrate that SSL has the capacity to achieve performance on par with supervised methods while significantly enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1% loss in performance through self-supervision.","Ultimately, this work underscores SSL's potential in human-centric computing, particularly high-stakes, data-scarce application domains like healthcare."],"url":"http://arxiv.org/abs/2401.01640v1"}
{"created":"2024-01-03 09:03:30","title":"Synthetic Data in AI: Challenges, Applications, and Ethical Implications","abstract":"In the rapidly evolving field of artificial intelligence, the creation and utilization of synthetic datasets have become increasingly significant. This report delves into the multifaceted aspects of synthetic data, particularly emphasizing the challenges and potential biases these datasets may harbor. It explores the methodologies behind synthetic data generation, spanning traditional statistical models to advanced deep learning techniques, and examines their applications across diverse domains. The report also critically addresses the ethical considerations and legal implications associated with synthetic datasets, highlighting the urgent need for mechanisms to ensure fairness, mitigate biases, and uphold ethical standards in AI development.","sentences":["In the rapidly evolving field of artificial intelligence, the creation and utilization of synthetic datasets have become increasingly significant.","This report delves into the multifaceted aspects of synthetic data, particularly emphasizing the challenges and potential biases these datasets may harbor.","It explores the methodologies behind synthetic data generation, spanning traditional statistical models to advanced deep learning techniques, and examines their applications across diverse domains.","The report also critically addresses the ethical considerations and legal implications associated with synthetic datasets, highlighting the urgent need for mechanisms to ensure fairness, mitigate biases, and uphold ethical standards in AI development."],"url":"http://arxiv.org/abs/2401.01629v1"}
{"created":"2024-01-03 08:54:56","title":"On the Expressive Power of Graph Neural Networks","abstract":"The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.   While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph structures, relating this to the graph isomorphism test. Besides, a GNN's ability to compute graph properties such as graph moments has been suggested as another form of expressiveness. All of these different definitions are complementary and have yielded different recommendations for GNN architecture choices. In this paper, we would like to give an overview of the notion of \"expressive power\" of GNNs and provide some valuable insights regarding the design choices of GNNs.","sentences":["The study of Graph Neural Networks has received considerable interest in the past few years.","By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine.","The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification.","However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.   ","While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives.","Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs.","Another direction focuses on the extent to which GNNs can distinguish between different graph structures, relating this to the graph isomorphism test.","Besides, a GNN's ability to compute graph properties such as graph moments has been suggested as another form of expressiveness.","All of these different definitions are complementary and have yielded different recommendations for GNN architecture choices.","In this paper, we would like to give an overview of the notion of \"expressive power\" of GNNs and provide some valuable insights regarding the design choices of GNNs."],"url":"http://arxiv.org/abs/2401.01626v1"}
{"created":"2024-01-03 08:51:18","title":"SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks","abstract":"Anomaly detection on attributed networks aims to find the nodes whose behaviors are significantly different from other majority nodes. Generally, network data contains information about relationships between entities, and the anomaly is usually embodied in these relationships. Therefore, how to comprehensively model complex interaction patterns in networks is still a major focus. It can be observed that anomalies in networks violate the homophily assumption. However, most existing studies only considered this phenomenon obliquely rather than explicitly. Besides, the node representation of normal entities can be perturbed easily by the noise relationships introduced by anomalous nodes. To address the above issues, we present a novel contrastive learning framework for anomaly detection on attributed networks, \\textbf{SCALA}, aiming to improve the embedding quality of the network and provide a new measurement of qualifying the anomaly score for each node by introducing sparsification into the conventional method. Extensive experiments are conducted on five benchmark real-world datasets and the results show that SCALA consistently outperforms all baseline methods significantly.","sentences":["Anomaly detection on attributed networks aims to find the nodes whose behaviors are significantly different from other majority nodes.","Generally, network data contains information about relationships between entities, and the anomaly is usually embodied in these relationships.","Therefore, how to comprehensively model complex interaction patterns in networks is still a major focus.","It can be observed that anomalies in networks violate the homophily assumption.","However, most existing studies only considered this phenomenon obliquely rather than explicitly.","Besides, the node representation of normal entities can be perturbed easily by the noise relationships introduced by anomalous nodes.","To address the above issues, we present a novel contrastive learning framework for anomaly detection on attributed networks, \\textbf{SCALA}, aiming to improve the embedding quality of the network and provide a new measurement of qualifying the anomaly score for each node by introducing sparsification into the conventional method.","Extensive experiments are conducted on five benchmark real-world datasets and the results show that SCALA consistently outperforms all baseline methods significantly."],"url":"http://arxiv.org/abs/2401.01625v1"}
{"created":"2024-01-03 08:41:10","title":"Several new classes of MDS symbol-pair codes derived from matrix-product codes","abstract":"In order to correct the pair-errors generated during the transmission of modern high-density data storage that the outputs of the channels consist of overlapping pairs of symbols, a new coding scheme named symbol-pair code is proposed. The error-correcting capability of the symbol-pair code is determined by its minimum symbol-pair distance. For such codes, the larger the minimum symbol-pair distance, the better. It is a challenging task to construct symbol-pair codes with optimal parameters, especially, maximum-distance-separable (MDS) symbol-pair codes. In this paper, the permutation equivalence codes of matrix-product codes with underlying matrixes of orders 3 and 4 are used to extend the minimum symbol-pair distance, and six new classes of MDS symbol-pair codes are derived.","sentences":["In order to correct the pair-errors generated during the transmission of modern high-density data storage that the outputs of the channels consist of overlapping pairs of symbols, a new coding scheme named symbol-pair code is proposed.","The error-correcting capability of the symbol-pair code is determined by its minimum symbol-pair distance.","For such codes, the larger the minimum symbol-pair distance, the better.","It is a challenging task to construct symbol-pair codes with optimal parameters, especially, maximum-distance-separable (MDS) symbol-pair codes.","In this paper, the permutation equivalence codes of matrix-product codes with underlying matrixes of orders 3 and 4 are used to extend the minimum symbol-pair distance, and six new classes of MDS symbol-pair codes are derived."],"url":"http://arxiv.org/abs/2401.01619v1"}
{"created":"2024-01-03 07:59:17","title":"Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning","abstract":"Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered. Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods. Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session. To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach. Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE. When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge. Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is publicly available at https://github.com/1170300714/LP-DiF.","sentences":["Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered.","Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF).","We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods.","Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session.","To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach.","Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE.","When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge.","Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL.","Code is publicly available at https://github.com/1170300714/LP-DiF."],"url":"http://arxiv.org/abs/2401.01598v1"}
{"created":"2024-01-03 07:58:25","title":"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries","abstract":"In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries. This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care. Our dataset, code, and pre-trained models will be made publicly available.","sentences":["In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making.","Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues.","Also prior works in the area of medical question summarisation have been limited to the English language.","This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting.","To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids.","This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective.","We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task.","By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries.","This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care.","Our dataset, code, and pre-trained models will be made publicly available."],"url":"http://arxiv.org/abs/2401.01596v1"}
{"created":"2024-01-03 07:54:13","title":"MLIP: Medical Language-Image Pre-training with Masked Local Representation Learning","abstract":"Existing contrastive language-image pre-training aims to learn a joint representation by matching abundant image-text pairs. However, the number of image-text pairs in medical datasets is usually orders of magnitude smaller than that in natural datasets. Besides, medical image-text pairs often involve numerous complex fine-grained correspondences. This paper aims to enhance the data efficiency by introducing multiple-to-multiple local relationship modeling to capture denser supervisions. More specifically, we propose a Medical Language-Image Pre-training (MLIP) framework, which exploits the limited image-text medical data more efficiently through patch-sentence matching. Furthermore, we introduce a masked contrastive learning strategy with semantic integrity estimation to reduce redundancy in images while preserving the underlying semantics. Our evaluation results show that MLIP outperforms previous work in zero/few-shot classification and few-shot segmentation tasks by a large margin.","sentences":["Existing contrastive language-image pre-training aims to learn a joint representation by matching abundant image-text pairs.","However, the number of image-text pairs in medical datasets is usually orders of magnitude smaller than that in natural datasets.","Besides, medical image-text pairs often involve numerous complex fine-grained correspondences.","This paper aims to enhance the data efficiency by introducing multiple-to-multiple local relationship modeling to capture denser supervisions.","More specifically, we propose a Medical Language-Image Pre-training (MLIP) framework, which exploits the limited image-text medical data more efficiently through patch-sentence matching.","Furthermore, we introduce a masked contrastive learning strategy with semantic integrity estimation to reduce redundancy in images while preserving the underlying semantics.","Our evaluation results show that MLIP outperforms previous work in zero/few-shot classification and few-shot segmentation tasks by a large margin."],"url":"http://arxiv.org/abs/2401.01591v1"}
{"created":"2024-01-03 07:47:22","title":"The Security and Privacy of Mobile Edge Computing: An Artificial Intelligence Perspective","abstract":"Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud computing and information technology (IT) services to be delivered at the network's edge. By shifting the load of cloud computing to individual local servers, MEC helps meet the requirements of ultralow latency, localized data processing, and extends the potential of Internet of Things (IoT) for end-users. However, the crosscutting nature of MEC and the multidisciplinary components necessary for its deployment have presented additional security and privacy concerns. Fortunately, Artificial Intelligence (AI) algorithms can cope with excessively unpredictable and complex data, which offers a distinct advantage in dealing with sophisticated and developing adversaries in the security industry. Hence, in this paper we comprehensively provide a survey of security and privacy in MEC from the perspective of AI. On the one hand, we use European Telecommunications Standards Institute (ETSI) MEC reference architecture as our based framework while merging the Software Defined Network (SDN) and Network Function Virtualization (NFV) to better illustrate a serviceable platform of MEC. On the other hand, we focus on new security and privacy issues, as well as potential solutions from the viewpoints of AI. Finally, we comprehensively discuss the opportunities and challenges associated with applying AI to MEC security and privacy as possible future research directions.","sentences":["Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud computing and information technology (IT) services to be delivered at the network's edge.","By shifting the load of cloud computing to individual local servers, MEC helps meet the requirements of ultralow latency, localized data processing, and extends the potential of Internet of Things (IoT) for end-users.","However, the crosscutting nature of MEC and the multidisciplinary components necessary for its deployment have presented additional security and privacy concerns.","Fortunately, Artificial Intelligence (AI) algorithms can cope with excessively unpredictable and complex data, which offers a distinct advantage in dealing with sophisticated and developing adversaries in the security industry.","Hence, in this paper we comprehensively provide a survey of security and privacy in MEC from the perspective of AI.","On the one hand, we use European Telecommunications Standards Institute (ETSI) MEC reference architecture as our based framework while merging the Software Defined Network (SDN) and Network Function Virtualization (NFV) to better illustrate a serviceable platform of MEC.","On the other hand, we focus on new security and privacy issues, as well as potential solutions from the viewpoints of AI.","Finally, we comprehensively discuss the opportunities and challenges associated with applying AI to MEC security and privacy as possible future research directions."],"url":"http://arxiv.org/abs/2401.01589v1"}
