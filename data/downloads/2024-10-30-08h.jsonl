{"created":"2024-10-29 17:58:13","title":"Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset","abstract":"The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). Interestingly, we find that the \"manipulation centricity\" is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose Manipulation Centric Representation (MCR), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We introduce a novel contrastive loss that aligns visual observations with the robot's proprioceptive state-action dynamics, combined with a behavior cloning (BC)-like actor loss to predict actions during pre-training, along with a time contrastive loss. Empirical results across 4 simulation domains with 20 tasks verify that MCR outperforms the strongest baseline method by 14.8%. Moreover, MCR boosts the performance of data-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%. Project website: https://robots-pretrain-robots.github.io/.","sentences":["The pre-training of visual representations has enhanced the efficiency of robot learning.","Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation.","Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion.","We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity).","Interestingly, we find that the \"manipulation centricity\" is a strong indicator of success rates when applied to downstream tasks.","Drawing from these findings, we propose Manipulation Centric Representation (MCR), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity.","Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions.","We introduce a novel contrastive loss that aligns visual observations with the robot's proprioceptive state-action dynamics, combined with a behavior cloning (BC)-like actor loss to predict actions during pre-training, along with a time contrastive loss.","Empirical results across 4 simulation domains with 20 tasks verify that MCR outperforms the strongest baseline method by 14.8%.","Moreover, MCR boosts the performance of data-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%.","Project website: https://robots-pretrain-robots.github.io/."],"url":"http://arxiv.org/abs/2410.22325v1"}
{"created":"2024-10-29 17:55:00","title":"Understanding Synthetic Context Extension via Retrieval Heads","abstract":"Long-context LLMs are increasingly in demand for applications such as retrieval-augmented generation. To defray the cost of pretraining LLMs over long contexts, recent work takes an approach of synthetic context extension: fine-tuning LLMs with synthetically generated long-context data in a post-training stage. However, it remains unclear how and why this synthetic context extension imparts abilities for downstream long-context tasks. In this paper, we investigate fine-tuning on synthetic data for three long-context tasks that require retrieval and reasoning. We vary the realism of \"needle\" concepts to be retrieved and diversity of the surrounding \"haystack\" context, from using LLMs to construct synthetic documents to using templated relations and creating symbolic datasets. We find that models trained on synthetic data fall short of the real data, but surprisingly, the mismatch can be interpreted and even predicted in terms of a special set of attention heads that are responsible for retrieval over long context: retrieval heads (Wu et al., 2024). The retrieval heads learned on synthetic data are mostly subsets of the retrieval heads learned on real data, and there is a strong correlation between the recall of heads learned and the downstream performance of a model. Furthermore, with attention knockout and activation patching, we mechanistically show that retrieval heads are necessary and explain model performance, although they are not totally sufficient. Our results shed light on how to interpret synthetic data fine-tuning performance and how to approach creating better data for learning real-world capabilities over long contexts.","sentences":["Long-context LLMs are increasingly in demand for applications such as retrieval-augmented generation.","To defray the cost of pretraining LLMs over long contexts, recent work takes an approach of synthetic context extension: fine-tuning LLMs with synthetically generated long-context data in a post-training stage.","However, it remains unclear how and why this synthetic context extension imparts abilities for downstream long-context tasks.","In this paper, we investigate fine-tuning on synthetic data for three long-context tasks that require retrieval and reasoning.","We vary the realism of \"needle\" concepts to be retrieved and diversity of the surrounding \"haystack\" context, from using LLMs to construct synthetic documents to using templated relations and creating symbolic datasets.","We find that models trained on synthetic data fall short of the real data, but surprisingly, the mismatch can be interpreted and even predicted in terms of a special set of attention heads that are responsible for retrieval over long context: retrieval heads (Wu et al., 2024).","The retrieval heads learned on synthetic data are mostly subsets of the retrieval heads learned on real data, and there is a strong correlation between the recall of heads learned and the downstream performance of a model.","Furthermore, with attention knockout and activation patching, we mechanistically show that retrieval heads are necessary and explain model performance, although they are not totally sufficient.","Our results shed light on how to interpret synthetic data fine-tuning performance and how to approach creating better data for learning real-world capabilities over long contexts."],"url":"http://arxiv.org/abs/2410.22316v1"}
{"created":"2024-10-29 17:54:17","title":"Natural Language Inference Improves Compositionality in Vision-Language Models","abstract":"Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the textual description, using Large Language Models (LLMs) to break them down into subsets of questions and answers. However, these methods primarily operate on the surface level, failing to incorporate deeper lexical understanding while introducing incorrect assumptions generated by the LLM. In response to these issues, we present Caption Expansion with Contradictions and Entailments (CECE), a principled approach that leverages Natural Language Inference (NLI) to generate entailments and contradictions from a given premise. CECE produces lexically diverse sentences while maintaining their core meaning. Through extensive experiments, we show that CECE enhances interpretability and reduces overreliance on biased or superficial features. By balancing CECE along the original premise, we achieve significant improvements over previous methods without requiring additional fine-tuning, producing state-of-the-art results on benchmarks that score agreement with human judgments for image-text alignment, and achieving an increase in performance on Winoground of +19.2% (group score) and +12.9% on EqBen (group score) over the best prior work (finetuned with targeted data).","sentences":["Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships.","Recent methods aim to address these limitations by relying on the semantics of the textual description, using Large Language Models (LLMs) to break them down into subsets of questions and answers.","However, these methods primarily operate on the surface level, failing to incorporate deeper lexical understanding while introducing incorrect assumptions generated by the LLM.","In response to these issues, we present Caption Expansion with Contradictions and Entailments (CECE), a principled approach that leverages Natural Language Inference (NLI) to generate entailments and contradictions from a given premise.","CECE produces lexically diverse sentences while maintaining their core meaning.","Through extensive experiments, we show that CECE enhances interpretability and reduces overreliance on biased or superficial features.","By balancing CECE along the original premise, we achieve significant improvements over previous methods without requiring additional fine-tuning, producing state-of-the-art results on benchmarks that score agreement with human judgments for image-text alignment, and achieving an increase in performance on Winoground of +19.2% (group score) and +12.9% on EqBen (group score) over the best prior work (finetuned with targeted data)."],"url":"http://arxiv.org/abs/2410.22315v1"}
{"created":"2024-10-29 17:54:02","title":"An Efficient Approach to Generate Safe Drivable Space by LiDAR-Camera-HDmap Fusion","abstract":"In this paper, we propose an accurate and robust perception module for Autonomous Vehicles (AVs) for drivable space extraction. Perception is crucial in autonomous driving, where many deep learning-based methods, while accurate on benchmark datasets, fail to generalize effectively, especially in diverse and unpredictable environments. Our work introduces a robust easy-to-generalize perception module that leverages LiDAR, camera, and HD map data fusion to deliver a safe and reliable drivable space in all weather conditions. We present an adaptive ground removal and curb detection method integrated with HD map data for enhanced obstacle detection reliability. Additionally, we propose an adaptive DBSCAN clustering algorithm optimized for precipitation noise, and a cost-effective LiDAR-camera frustum association that is resilient to calibration discrepancies. Our comprehensive drivable space representation incorporates all perception data, ensuring compatibility with vehicle dimensions and road regulations. This approach not only improves generalization and efficiency, but also significantly enhances safety in autonomous vehicle operations. Our approach is tested on a real dataset and its reliability is verified during the daily (including harsh snowy weather) operation of our autonomous shuttle, WATonoBus","sentences":["In this paper, we propose an accurate and robust perception module for Autonomous Vehicles (AVs) for drivable space extraction.","Perception is crucial in autonomous driving, where many deep learning-based methods, while accurate on benchmark datasets, fail to generalize effectively, especially in diverse and unpredictable environments.","Our work introduces a robust easy-to-generalize perception module that leverages LiDAR, camera, and HD map data fusion to deliver a safe and reliable drivable space in all weather conditions.","We present an adaptive ground removal and curb detection method integrated with HD map data for enhanced obstacle detection reliability.","Additionally, we propose an adaptive DBSCAN clustering algorithm optimized for precipitation noise, and a cost-effective LiDAR-camera frustum association that is resilient to calibration discrepancies.","Our comprehensive drivable space representation incorporates all perception data, ensuring compatibility with vehicle dimensions and road regulations.","This approach not only improves generalization and efficiency, but also significantly enhances safety in autonomous vehicle operations.","Our approach is tested on a real dataset and its reliability is verified during the daily (including harsh snowy weather) operation of our autonomous shuttle, WATonoBus"],"url":"http://arxiv.org/abs/2410.22314v1"}
{"created":"2024-10-29 17:53:56","title":"Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving","abstract":"End-to-end autonomous driving demonstrates strong planning capabilities with large-scale data but still struggles in complex, rare scenarios due to limited commonsense. In contrast, Large Vision-Language Models (LVLMs) excel in scene understanding and reasoning. The path forward lies in merging the strengths of both approaches. Previous methods using LVLMs to predict trajectories or control signals yield suboptimal results, as LVLMs are not well-suited for precise numerical predictions. This paper presents Senna, an autonomous driving system combining an LVLM (Senna-VLM) with an end-to-end model (Senna-E2E). Senna decouples high-level planning from low-level trajectory prediction. Senna-VLM generates planning decisions in natural language, while Senna-E2E predicts precise trajectories. Senna-VLM utilizes a multi-image encoding approach and multi-view prompts for efficient scene understanding. Besides, we introduce planning-oriented QAs alongside a three-stage training strategy, which enhances Senna-VLM's planning performance while preserving commonsense. Extensive experiments on two datasets show that Senna achieves state-of-the-art planning performance. Notably, with pre-training on a large-scale dataset DriveX and fine-tuning on nuScenes, Senna significantly reduces average planning error by 27.12% and collision rate by 33.33% over model without pre-training. We believe Senna's cross-scenario generalization and transferability are essential for achieving fully autonomous driving. Code and models will be released at https://github.com/hustvl/Senna.","sentences":["End-to-end autonomous driving demonstrates strong planning capabilities with large-scale data but still struggles in complex, rare scenarios due to limited commonsense.","In contrast, Large Vision-Language Models (LVLMs) excel in scene understanding and reasoning.","The path forward lies in merging the strengths of both approaches.","Previous methods using LVLMs to predict trajectories or control signals yield suboptimal results, as LVLMs are not well-suited for precise numerical predictions.","This paper presents Senna, an autonomous driving system combining an LVLM (Senna-VLM) with an end-to-end model (Senna-E2E).","Senna decouples high-level planning from low-level trajectory prediction.","Senna-VLM generates planning decisions in natural language, while Senna-E2E predicts precise trajectories.","Senna-VLM utilizes a multi-image encoding approach and multi-view prompts for efficient scene understanding.","Besides, we introduce planning-oriented QAs alongside a three-stage training strategy, which enhances Senna-VLM's planning performance while preserving commonsense.","Extensive experiments on two datasets show that Senna achieves state-of-the-art planning performance.","Notably, with pre-training on a large-scale dataset DriveX and fine-tuning on nuScenes, Senna significantly reduces average planning error by 27.12% and collision rate by 33.33% over model without pre-training.","We believe Senna's cross-scenario generalization and transferability are essential for achieving fully autonomous driving.","Code and models will be released at https://github.com/hustvl/Senna."],"url":"http://arxiv.org/abs/2410.22313v1"}
{"created":"2024-10-29 17:39:31","title":"Motion Graph Unleashed: A Novel Approach to Video Prediction","abstract":"We introduce motion graph, a novel approach to the video prediction problem, which predicts future video frames from limited past data. The motion graph transforms patches of video frames into interconnected graph nodes, to comprehensively describe the spatial-temporal relationships among them. This representation overcomes the limitations of existing motion representations such as image differences, optical flow, and motion matrix that either fall short in capturing complex motion patterns or suffer from excessive memory consumption. We further present a video prediction pipeline empowered by motion graph, exhibiting substantial performance improvements and cost reductions. Experiments on various datasets, including UCF Sports, KITTI and Cityscapes, highlight the strong representative ability of motion graph. Especially on UCF Sports, our method matches and outperforms the SOTA methods with a significant reduction in model size by 78% and a substantial decrease in GPU memory utilization by 47%.","sentences":["We introduce motion graph, a novel approach to the video prediction problem, which predicts future video frames from limited past data.","The motion graph transforms patches of video frames into interconnected graph nodes, to comprehensively describe the spatial-temporal relationships among them.","This representation overcomes the limitations of existing motion representations such as image differences, optical flow, and motion matrix that either fall short in capturing complex motion patterns or suffer from excessive memory consumption.","We further present a video prediction pipeline empowered by motion graph, exhibiting substantial performance improvements and cost reductions.","Experiments on various datasets, including UCF Sports, KITTI and Cityscapes, highlight the strong representative ability of motion graph.","Especially on UCF Sports, our method matches and outperforms the SOTA methods with a significant reduction in model size by 78% and a substantial decrease in GPU memory utilization by 47%."],"url":"http://arxiv.org/abs/2410.22288v1"}
{"created":"2024-10-29 17:34:01","title":"Active Event Alignment for Monocular Distance Estimation","abstract":"Event cameras provide a natural and data efficient representation of visual information, motivating novel computational strategies towards extracting visual information. Inspired by the biological vision system, we propose a behavior driven approach for object-wise distance estimation from event camera data. This behavior-driven method mimics how biological systems, like the human eye, stabilize their view based on object distance: distant objects require minimal compensatory rotation to stay in focus, while nearby objects demand greater adjustments to maintain alignment. This adaptive strategy leverages natural stabilization behaviors to estimate relative distances effectively. Unlike traditional vision algorithms that estimate depth across the entire image, our approach targets local depth estimation within a specific region of interest. By aligning events within a small region, we estimate the angular velocity required to stabilize the image motion. We demonstrate that, under certain assumptions, the compensatory rotational flow is inversely proportional to the object's distance. The proposed approach achieves new state-of-the-art accuracy in distance estimation - a performance gain of 16% on EVIMO2. EVIMO2 event sequences comprise complex camera motion and substantial variance in depth of static real world scenes.","sentences":["Event cameras provide a natural and data efficient representation of visual information, motivating novel computational strategies towards extracting visual information.","Inspired by the biological vision system, we propose a behavior driven approach for object-wise distance estimation from event camera data.","This behavior-driven method mimics how biological systems, like the human eye, stabilize their view based on object distance: distant objects require minimal compensatory rotation to stay in focus, while nearby objects demand greater adjustments to maintain alignment.","This adaptive strategy leverages natural stabilization behaviors to estimate relative distances effectively.","Unlike traditional vision algorithms that estimate depth across the entire image, our approach targets local depth estimation within a specific region of interest.","By aligning events within a small region, we estimate the angular velocity required to stabilize the image motion.","We demonstrate that, under certain assumptions, the compensatory rotational flow is inversely proportional to the object's distance.","The proposed approach achieves new state-of-the-art accuracy in distance estimation - a performance gain of 16% on EVIMO2.","EVIMO2 event sequences comprise complex camera motion and substantial variance in depth of static real world scenes."],"url":"http://arxiv.org/abs/2410.22280v1"}
{"created":"2024-10-29 17:27:58","title":"Fourier Head: Helping Large Language Models Learn Complex Probability Distributions","abstract":"As the quality of large language models has improved, there has been increased interest in using them to model non-linguistic tokens. For example, the Decision Transformer recasts agentic decision making as a sequence modeling problem, using a decoder-only LLM to model the distribution over the discrete action space for an Atari agent. However, when adapting LLMs to non-linguistic domains, it remains unclear if softmax over discrete bins captures the continuous structure of the tokens and the potentially complex distributions needed for high quality token generation. We introduce a neural network layer, constructed using Fourier series, which we can easily substitute for any linear layer if we want the outputs to have a more continuous structure. We perform extensive analysis on synthetic datasets, as well as on large-scale decision making and time series forecasting tasks. We also provide theoretical evidence that this layer can better learn signal from data while ignoring high-frequency noise. All of our results support the effectiveness of our proposed Fourier head in scenarios where the underlying data distribution has a natural continuous structure. For example, the Fourier head improves a Decision Transformer agent's returns by 46% on the Atari Seaquest game, and increases a state-of-the-art times series foundation model's forecasting performance by 3.5% across 20 benchmarks unseen during training.","sentences":["As the quality of large language models has improved, there has been increased interest in using them to model non-linguistic tokens.","For example, the Decision Transformer recasts agentic decision making as a sequence modeling problem, using a decoder-only LLM to model the distribution over the discrete action space for an Atari agent.","However, when adapting LLMs to non-linguistic domains, it remains unclear if softmax over discrete bins captures the continuous structure of the tokens and the potentially complex distributions needed for high quality token generation.","We introduce a neural network layer, constructed using Fourier series, which we can easily substitute for any linear layer if we want the outputs to have a more continuous structure.","We perform extensive analysis on synthetic datasets, as well as on large-scale decision making and time series forecasting tasks.","We also provide theoretical evidence that this layer can better learn signal from data while ignoring high-frequency noise.","All of our results support the effectiveness of our proposed Fourier head in scenarios where the underlying data distribution has a natural continuous structure.","For example, the Fourier head improves a Decision Transformer agent's returns by 46% on the Atari Seaquest game, and increases a state-of-the-art times series foundation model's forecasting performance by 3.5% across 20 benchmarks unseen during training."],"url":"http://arxiv.org/abs/2410.22269v1"}
{"created":"2024-10-29 17:27:43","title":"Approximately Counting Knapsack Solutions in Subquadratic Time","abstract":"We revisit the classic #Knapsack problem, which asks to count the Boolean points $(x_1,\\dots,x_n)\\in\\{0,1\\}^n$ in a given half-space $\\sum_{i=1}^nW_ix_i\\le T$. This #P-complete problem admits $(1\\pm\\epsilon)$-approximation. Before this work, [Dyer, STOC 2003]'s $\\tilde{O}(n^{2.5}+n^2{\\epsilon^{-2}})$-time randomized approximation scheme remains the fastest known in the natural regime of $\\epsilon\\ge 1/polylog(n)$. In this paper, we give a randomized $(1\\pm\\epsilon)$-approximation algorithm in $\\tilde{O}(n^{1.5}{\\epsilon^{-2}})$ time (in the standard word-RAM model), achieving the first sub-quadratic dependence on $n$. Such sub-quadratic running time is rare in the approximate counting literature in general, as a large class of algorithms naturally faces a quadratic-time barrier.   Our algorithm follows Dyer's framework, which reduces #Knapsack to the task of sampling (and approximately counting) solutions in a randomly rounded instance with poly(n)-bounded integer weights. We refine Dyer's framework using the following ideas:   - We decrease the sample complexity of Dyer's Monte Carlo method, by proving some structural lemmas for typical points near the input hyperplane via hitting-set arguments, and appropriately setting the rounding scale.   - Instead of running a vanilla dynamic program on the rounded instance, we employ techniques from the growing field of pseudopolynomial-time Subset Sum algorithms, such as FFT, divide-and-conquer, and balls-into-bins hashing of [Bringmann, SODA 2017].   We also need other ingredients, including a surprising application of the recent Bounded Monotone (max,+)-Convolution algorithm by [Chi-Duan-Xie-Zhang, STOC 2022] (adapted by [Bringmann-D\\\"urr-Polak, ESA 2024]), the notion of sum-approximation from [Gawrychowski-Markin-Weimann, ICALP 2018]'s #Knapsack approximation scheme, and a two-phase extension of Dyer's framework for handling tiny weights.","sentences":["We revisit the classic #Knapsack problem, which asks to count the Boolean points $(x_1,\\dots,x_n)\\in\\{0,1\\}^n$ in a given half-space $\\sum_{i=1}^nW_ix_i\\le T$. This #P-complete problem admits $(1\\pm\\epsilon)$-approximation.","Before this work, [Dyer, STOC 2003]'s $\\tilde{O}(n^{2.5}+n^2{\\epsilon^{-2}})$-time randomized approximation scheme remains the fastest known in the natural regime of $\\epsilon\\ge 1/polylog(n)$. In this paper, we give a randomized $(1\\pm\\epsilon)$-approximation algorithm in $\\tilde{O}(n^{1.5}{\\epsilon^{-2}})$ time (in the standard word-RAM model), achieving the first sub-quadratic dependence on $n$. Such sub-quadratic running time is rare in the approximate counting literature in general, as a large class of algorithms naturally faces a quadratic-time barrier.   ","Our algorithm follows Dyer's framework, which reduces #Knapsack to the task of sampling (and approximately counting) solutions in a randomly rounded instance with poly(n)-bounded integer weights.","We refine Dyer's framework using the following ideas:   - We decrease the sample complexity of Dyer's Monte Carlo method, by proving some structural lemmas for typical points near the input hyperplane via hitting-set arguments, and appropriately setting the rounding scale.   ","- Instead of running a vanilla dynamic program on the rounded instance, we employ techniques from the growing field of pseudopolynomial-time Subset Sum algorithms, such as FFT, divide-and-conquer, and balls-into-bins hashing of [Bringmann, SODA 2017].   ","We also need other ingredients, including a surprising application of the recent Bounded Monotone (max,+)-Convolution algorithm by [Chi-Duan-Xie-Zhang, STOC 2022] (adapted by [Bringmann-D\\\"urr-Polak, ESA 2024]), the notion of sum-approximation from [Gawrychowski-Markin-Weimann, ICALP 2018]'s #Knapsack approximation scheme, and a two-phase extension of Dyer's framework for handling tiny weights."],"url":"http://arxiv.org/abs/2410.22267v1"}
{"created":"2024-10-29 17:23:25","title":"Communication Characterization of AI Workloads for Large-scale Multi-chiplet Accelerators","abstract":"Next-generation artificial intelligence (AI) workloads are posing challenges of scalability and robustness in terms of execution time due to their intrinsic evolving data-intensive characteristics. In this paper, we aim to analyse the potential bottlenecks caused due to data movement characteristics of AI workloads on scale-out accelerator architectures composed of multiple chiplets. Our methodology captures the unicast and multicast communication traffic of a set of AI workloads and assesses aspects such as the time spent in such communications and the amount of multicast messages as a function of the number of employed chiplets. Our studies reveal that some AI workloads are potentially vulnerable to the dominant effects of communication, especially multicast traffic, which can become a performance bottleneck and limit their scalability. Workload profiling insights suggest to architect a flexible interconnect solution at chiplet level in order to improve the performance, efficiency and scalability of next-generation AI accelerators.","sentences":["Next-generation artificial intelligence (AI) workloads are posing challenges of scalability and robustness in terms of execution time due to their intrinsic evolving data-intensive characteristics.","In this paper, we aim to analyse the potential bottlenecks caused due to data movement characteristics of AI workloads on scale-out accelerator architectures composed of multiple chiplets.","Our methodology captures the unicast and multicast communication traffic of a set of AI workloads and assesses aspects such as the time spent in such communications and the amount of multicast messages as a function of the number of employed chiplets.","Our studies reveal that some AI workloads are potentially vulnerable to the dominant effects of communication, especially multicast traffic, which can become a performance bottleneck and limit their scalability.","Workload profiling insights suggest to architect a flexible interconnect solution at chiplet level in order to improve the performance, efficiency and scalability of next-generation AI accelerators."],"url":"http://arxiv.org/abs/2410.22262v1"}
{"created":"2024-10-29 17:19:56","title":"FactBench: A Dynamic Benchmark for In-the-Wild Language Model Factuality Evaluation","abstract":"Language models (LMs) are widely used by an increasing number of users, underscoring the challenge of maintaining factuality across a broad range of topics. We first present VERIFY (Verification and Evidence RetrIeval for FactualitY evaluation), a pipeline to evaluate LMs' factuality in real-world user interactions. VERIFY considers the verifiability of LM-generated content and categorizes content units as supported, unsupported, or undecidable based on the retrieved evidence from the Web. Importantly, factuality judgment by VERIFY correlates better with human evaluations than existing methods. Using VERIFY, we identify \"hallucination prompts\" across diverse topics, i.e., those eliciting the highest rates of incorrect and inconclusive LM responses. These prompts form FactBench, a dataset of 1K prompts across 150 fine-grained topics. Our dataset captures emerging factuality challenges in real-world LM interactions and can be regularly updated with new prompts. We benchmark widely-used LMs from GPT, Gemini, and Llama3.1 family on FactBench, yielding the following key findings: (i) Proprietary models exhibit better factuality, with performance declining from Easy to Hard hallucination prompts. (ii) Llama3.1-405B-Instruct shows comparable or lower factual accuracy than Llama3.1-70B-Instruct across all evaluation methods due to its higher subjectivity that leads to more content labeled as undecidable. (iii) Gemini1.5-Pro shows a significantly higher refusal rate, with over-refusal in 25% of cases. Our code and data are publicly available at https://huggingface.co/spaces/launch/factbench.","sentences":["Language models (LMs) are widely used by an increasing number of users, underscoring the challenge of maintaining factuality across a broad range of topics.","We first present VERIFY (Verification and Evidence RetrIeval for FactualitY evaluation), a pipeline to evaluate LMs' factuality in real-world user interactions.","VERIFY considers the verifiability of LM-generated content and categorizes content units as supported, unsupported, or undecidable based on the retrieved evidence from the Web.","Importantly, factuality judgment by VERIFY correlates better with human evaluations than existing methods.","Using VERIFY, we identify \"hallucination prompts\" across diverse topics, i.e., those eliciting the highest rates of incorrect and inconclusive LM responses.","These prompts form FactBench, a dataset of 1K prompts across 150 fine-grained topics.","Our dataset captures emerging factuality challenges in real-world LM interactions and can be regularly updated with new prompts.","We benchmark widely-used LMs from GPT, Gemini, and Llama3.1 family on FactBench, yielding the following key findings: (i) Proprietary models exhibit better factuality, with performance declining from Easy to Hard hallucination prompts.","(ii) Llama3.1-405B-Instruct shows comparable or lower factual accuracy than Llama3.1-70B-Instruct across all evaluation methods due to its higher subjectivity that leads to more content labeled as undecidable.","(iii) Gemini1.5-Pro shows a significantly higher refusal rate, with over-refusal in 25% of cases.","Our code and data are publicly available at https://huggingface.co/spaces/launch/factbench."],"url":"http://arxiv.org/abs/2410.22257v1"}
{"created":"2024-10-29 17:19:18","title":"Hypergraph-based multi-scale spatio-temporal graph convolution network for Time-Series anomaly detection","abstract":"Multivariate time series anomaly detection technology plays an important role in many fields including aerospace, water treatment, cloud service providers, etc. Excellent anomaly detection models can greatly improve work efficiency and avoid major economic losses. However, with the development of technology, the increasing size and complexity of data, and the lack of labels for relevant abnormal data, it is becoming increasingly challenging to perform effective and accurate anomaly detection in high-dimensional and complex data sets. In this paper, we propose a hypergraph based spatiotemporal graph convolutional neural network model STGCN_Hyper, which explicitly captures high-order, multi-hop correlations between multiple variables through a hypergraph based dynamic graph structure learning module. On this basis, we further use the hypergraph based spatiotemporal graph convolutional network to utilize the learned hypergraph structure to effectively propagate and aggregate one-hop and multi-hop related node information in the convolutional network, thereby obtaining rich spatial information. Furthermore, through the multi-scale TCN dilated convolution module, the STGCN_hyper model can also capture the dependencies of features at different scales in the temporal dimension. An unsupervised anomaly detector based on PCA and GMM is also integrated into the STGCN_hyper model. Through the anomaly score of the detector, the model can detect the anomalies in an unsupervised way. Experimental results on multiple time series datasets show that our model can flexibly learn the multi-scale time series features in the data and the dependencies between features, and outperforms most existing baseline models in terms of precision, recall, F1-score on anomaly detection tasks. Our code is available on: https://git.ecdf.ed.ac.uk/msc-23-24/s2044819","sentences":["Multivariate time series anomaly detection technology plays an important role in many fields including aerospace, water treatment, cloud service providers, etc.","Excellent anomaly detection models can greatly improve work efficiency and avoid major economic losses.","However, with the development of technology, the increasing size and complexity of data, and the lack of labels for relevant abnormal data, it is becoming increasingly challenging to perform effective and accurate anomaly detection in high-dimensional and complex data sets.","In this paper, we propose a hypergraph based spatiotemporal graph convolutional neural network model STGCN_Hyper, which explicitly captures high-order, multi-hop correlations between multiple variables through a hypergraph based dynamic graph structure learning module.","On this basis, we further use the hypergraph based spatiotemporal graph convolutional network to utilize the learned hypergraph structure to effectively propagate and aggregate one-hop and multi-hop related node information in the convolutional network, thereby obtaining rich spatial information.","Furthermore, through the multi-scale TCN dilated convolution module, the STGCN_hyper model can also capture the dependencies of features at different scales in the temporal dimension.","An unsupervised anomaly detector based on PCA and GMM is also integrated into the STGCN_hyper model.","Through the anomaly score of the detector, the model can detect the anomalies in an unsupervised way.","Experimental results on multiple time series datasets show that our model can flexibly learn the multi-scale time series features in the data and the dependencies between features, and outperforms most existing baseline models in terms of precision, recall, F1-score on anomaly detection tasks.","Our code is available on: https://git.ecdf.ed.ac.uk/msc-23-24/s2044819"],"url":"http://arxiv.org/abs/2410.22256v1"}
{"created":"2024-10-29 17:10:23","title":"Optimizing and Managing Wireless Backhaul for Resilient Next-Generation Cellular Networks","abstract":"Next-generation wireless networks target high network availability, ubiquitous coverage, and extremely high data rates for mobile users. This requires exploring new frequency bands, e.g., mmWaves, moving toward ultra-dense deployments in urban locations, and providing ad hoc, resilient connectivity in rural scenarios. The design of the backhaul network plays a key role in advancing how the access part of the wireless system supports next-generation use cases. Wireless backhauling, such as the newly introduced Integrated Access and Backhaul (IAB) concept in 5G, provides a promising solution, also leveraging the mmWave technology and steerable beams to mitigate interference and scalability issues. At the same time, however, managing and optimizing a complex wireless backhaul introduces additional challenges for the operation of cellular systems. This paper presents a strategy for the optimal creation of the backhaul network considering various constraints related to network topology, robustness, and flow management. We evaluate its feasibility and efficiency using synthetic and realistic network scenarios based on 3D modeling of buildings and ray tracing. We implement and prototype our solution as a dynamic IAB control framework based on the Open Radio Access Network (RAN) architecture, and demonstrate its functionality in Colosseum, a large-scale wireless network emulator with hardware in the loop.","sentences":["Next-generation wireless networks target high network availability, ubiquitous coverage, and extremely high data rates for mobile users.","This requires exploring new frequency bands, e.g., mmWaves, moving toward ultra-dense deployments in urban locations, and providing ad hoc, resilient connectivity in rural scenarios.","The design of the backhaul network plays a key role in advancing how the access part of the wireless system supports next-generation use cases.","Wireless backhauling, such as the newly introduced Integrated Access and Backhaul (IAB) concept in 5G, provides a promising solution, also leveraging the mmWave technology and steerable beams to mitigate interference and scalability issues.","At the same time, however, managing and optimizing a complex wireless backhaul introduces additional challenges for the operation of cellular systems.","This paper presents a strategy for the optimal creation of the backhaul network considering various constraints related to network topology, robustness, and flow management.","We evaluate its feasibility and efficiency using synthetic and realistic network scenarios based on 3D modeling of buildings and ray tracing.","We implement and prototype our solution as a dynamic IAB control framework based on the Open Radio Access Network (RAN) architecture, and demonstrate its functionality in Colosseum, a large-scale wireless network emulator with hardware in the loop."],"url":"http://arxiv.org/abs/2410.22246v1"}
{"created":"2024-10-29 17:06:50","title":"Computing Betti tables and minimal presentations of zero-dimensional persistent homology","abstract":"The Betti tables of a multigraded module encode the grades at which there is an algebraic change in the module. Multigraded modules show up in many areas of pure and applied mathematics, and in particular in topological data analysis, where they are known as persistence modules, and where their Betti tables describe the places at which the homology of filtered simplicial complexes change. Although Betti tables of singly and bigraded modules are already being used in applications of topological data analysis, their computation in the bigraded case (which relies on an algorithm that is cubic in the size of the filtered simplicial complex) is a bottleneck when working with large datasets. We show that, in the special case of $0$-dimensional homology (which is relevant for clustering and graph classification) the Betti tables of a bigraded module can be computed in log-linear time. We also consider the problem of computing minimal presentations, and show that a minimal presentation of $0$-dimensional persistent homology can be computed in quadratic time, regardless of the grading poset.","sentences":["The Betti tables of a multigraded module encode the grades at which there is an algebraic change in the module.","Multigraded modules show up in many areas of pure and applied mathematics, and in particular in topological data analysis, where they are known as persistence modules, and where their Betti tables describe the places at which the homology of filtered simplicial complexes change.","Although Betti tables of singly and bigraded modules are already being used in applications of topological data analysis, their computation in the bigraded case (which relies on an algorithm that is cubic in the size of the filtered simplicial complex) is a bottleneck when working with large datasets.","We show that, in the special case of $0$-dimensional homology (which is relevant for clustering and graph classification) the Betti tables of a bigraded module can be computed in log-linear time.","We also consider the problem of computing minimal presentations, and show that a minimal presentation of $0$-dimensional persistent homology can be computed in quadratic time, regardless of the grading poset."],"url":"http://arxiv.org/abs/2410.22242v1"}
{"created":"2024-10-29 17:05:25","title":"Are Decoder-Only Large Language Models the Silver Bullet for Code Search?","abstract":"Code search is crucial for code reuse, enabling developers to efficiently locate relevant snippets. Current methods rely on encoder-based models, which suffer from limitations such as poor generalization and restricted input lengths. Decoder-only large language models (LLMs), with their extensive pre-training, larger size, and longer input capabilities, offer potential solutions to these issues, yet their effectiveness in code search remains underexplored. To fill this gap, our study presents the first systematic exploration of decoder-only LLMs for code search. We evaluate nine state-of-the-art decoder-only models using two fine-tuning methods, two datasets (CSN and CoSQA$^+$), and three model sizes. Our findings reveal that fine-tuned CodeGemma significantly outperforms encoder-only models like UniXcoder, achieving a 5.57% improvement in MRR on CSN and a 49.6% increase in MAP on CoSQA$^+$ compared to zero-shot UniXcoder. These results highlight the superior performance and adaptability of decoder-only models. Additionally, we provide valuable insights into optimizing these models for code search, covering aspects such as model selection, fine-tuning methods, training data, and model size, and discussing their strengths and limitations.","sentences":["Code search is crucial for code reuse, enabling developers to efficiently locate relevant snippets.","Current methods rely on encoder-based models, which suffer from limitations such as poor generalization and restricted input lengths.","Decoder-only large language models (LLMs), with their extensive pre-training, larger size, and longer input capabilities, offer potential solutions to these issues, yet their effectiveness in code search remains underexplored.","To fill this gap, our study presents the first systematic exploration of decoder-only LLMs for code search.","We evaluate nine state-of-the-art decoder-only models using two fine-tuning methods, two datasets (CSN and CoSQA$^+$), and three model sizes.","Our findings reveal that fine-tuned CodeGemma significantly outperforms encoder-only models like UniXcoder, achieving a 5.57% improvement in MRR on CSN and a 49.6% increase in MAP on CoSQA$^+$ compared to zero-shot UniXcoder.","These results highlight the superior performance and adaptability of decoder-only models.","Additionally, we provide valuable insights into optimizing these models for code search, covering aspects such as model selection, fine-tuning methods, training data, and model size, and discussing their strengths and limitations."],"url":"http://arxiv.org/abs/2410.22240v1"}
{"created":"2024-10-29 17:02:26","title":"I/O complexity and pebble games with partial computations","abstract":"Optimizing data movements during program executions is essential for achieving high performance in modern computing systems. This has been classically modeled with the Red-Blue Pebble Game and its variants. In the existing models, it is typically assumed that the number of red pebbles, i.e., the size of the fast memory, is larger than the maximum in-degree in the computational graph (e.g. an arithmetic circuit). This assumption can be restrictive for many real applications, especially when dealing with \"big data\" in Machine Learning and Scientific Computing. In this work we study a generalization of the original Red-Blue Pebble Game to allow arbitrary in-degrees, that can be larger than the size of the fast memory. The objective is to minimize the I/O operations by allowing the computation of partial results in the fast memory. We show that this variant of the problem is NP-complete, even for the special case where the computational graph consists of a single level, and only two words fit in the fast memory. Approximation algorithms for a couple of special cases are also outlined.","sentences":["Optimizing data movements during program executions is essential for achieving high performance in modern computing systems.","This has been classically modeled with the Red-Blue Pebble Game and its variants.","In the existing models, it is typically assumed that the number of red pebbles, i.e., the size of the fast memory, is larger than the maximum in-degree in the computational graph (e.g. an arithmetic circuit).","This assumption can be restrictive for many real applications, especially when dealing with \"big data\" in Machine Learning and Scientific Computing.","In this work we study a generalization of the original Red-Blue Pebble Game to allow arbitrary in-degrees, that can be larger than the size of the fast memory.","The objective is to minimize the I/O operations by allowing the computation of partial results in the fast memory.","We show that this variant of the problem is NP-complete, even for the special case where the computational graph consists of a single level, and only two words fit in the fast memory.","Approximation algorithms for a couple of special cases are also outlined."],"url":"http://arxiv.org/abs/2410.22237v1"}
{"created":"2024-10-29 16:54:37","title":"Subgraph Aggregation for Out-of-Distribution Generalization on Graphs","abstract":"Out-of-distribution (OOD) generalization in Graph Neural Networks (GNNs) has gained significant attention due to its critical importance in graph-based predictions in real-world scenarios. Existing methods primarily focus on extracting a single causal subgraph from the input graph to achieve generalizable predictions. However, relying on a single subgraph can lead to susceptibility to spurious correlations and is insufficient for learning invariant patterns behind graph data. Moreover, in many real-world applications, such as molecular property prediction, multiple critical subgraphs may influence the target label property. To address these challenges, we propose a novel framework, SubGraph Aggregation (SuGAr), designed to learn a diverse set of subgraphs that are crucial for OOD generalization on graphs. Specifically, SuGAr employs a tailored subgraph sampler and diversity regularizer to extract a diverse set of invariant subgraphs. These invariant subgraphs are then aggregated by averaging their representations, which enriches the subgraph signals and enhances coverage of the underlying causal structures, thereby improving OOD generalization. Extensive experiments on both synthetic and real-world datasets demonstrate that \\ours outperforms state-of-the-art methods, achieving up to a 24% improvement in OOD generalization on graphs. To the best of our knowledge, this is the first work to study graph OOD generalization by learning multiple invariant subgraphs.","sentences":["Out-of-distribution (OOD) generalization in Graph Neural Networks (GNNs) has gained significant attention due to its critical importance in graph-based predictions in real-world scenarios.","Existing methods primarily focus on extracting a single causal subgraph from the input graph to achieve generalizable predictions.","However, relying on a single subgraph can lead to susceptibility to spurious correlations and is insufficient for learning invariant patterns behind graph data.","Moreover, in many real-world applications, such as molecular property prediction, multiple critical subgraphs may influence the target label property.","To address these challenges, we propose a novel framework, SubGraph Aggregation (SuGAr), designed to learn a diverse set of subgraphs that are crucial for OOD generalization on graphs.","Specifically, SuGAr employs a tailored subgraph sampler and diversity regularizer to extract a diverse set of invariant subgraphs.","These invariant subgraphs are then aggregated by averaging their representations, which enriches the subgraph signals and enhances coverage of the underlying causal structures, thereby improving OOD generalization.","Extensive experiments on both synthetic and real-world datasets demonstrate that \\ours outperforms state-of-the-art methods, achieving up to a 24% improvement in OOD generalization on graphs.","To the best of our knowledge, this is the first work to study graph OOD generalization by learning multiple invariant subgraphs."],"url":"http://arxiv.org/abs/2410.22228v1"}
{"created":"2024-10-29 16:38:34","title":"Drone Acoustic Analysis for Predicting Psychoacoustic Annoyance via Artificial Neural Networks","abstract":"Unmanned Aerial Vehicles (UAVs) have become widely used in various fields and industrial applications thanks to their low operational cost, compact size and wide accessibility. However, the noise generated by drone propellers has emerged as a significant concern. This may affect the public willingness to implement these vehicles in services that require operation in proximity to residential areas. The standard approaches to address this challenge include sound pressure measurements and noise characteristic analyses. The integration of Artificial Intelligence models in recent years has further streamlined the process by enhancing complex feature detection in drone acoustics data. This study builds upon prior research by examining the efficacy of various Deep Learning models in predicting Psychoacoustic Annoyance, an effective index for measuring perceived annoyance by human ears, based on multiple drone characteristics as input. This is accomplished by constructing a training dataset using precise measurements of various drone models with multiple microphones and analyzing flight data, maneuvers, drone physical characteristics, and perceived annoyance under realistic conditions. The aim of this research is to improve our understanding of drone noise, aid in the development of noise reduction techniques, and encourage the acceptance of drone usage on public spaces.","sentences":["Unmanned Aerial Vehicles (UAVs) have become widely used in various fields and industrial applications thanks to their low operational cost, compact size and wide accessibility.","However, the noise generated by drone propellers has emerged as a significant concern.","This may affect the public willingness to implement these vehicles in services that require operation in proximity to residential areas.","The standard approaches to address this challenge include sound pressure measurements and noise characteristic analyses.","The integration of Artificial Intelligence models in recent years has further streamlined the process by enhancing complex feature detection in drone acoustics data.","This study builds upon prior research by examining the efficacy of various Deep Learning models in predicting Psychoacoustic Annoyance, an effective index for measuring perceived annoyance by human ears, based on multiple drone characteristics as input.","This is accomplished by constructing a training dataset using precise measurements of various drone models with multiple microphones and analyzing flight data, maneuvers, drone physical characteristics, and perceived annoyance under realistic conditions.","The aim of this research is to improve our understanding of drone noise, aid in the development of noise reduction techniques, and encourage the acceptance of drone usage on public spaces."],"url":"http://arxiv.org/abs/2410.22208v1"}
{"created":"2024-10-29 16:36:14","title":"EnvoDat: A Large-Scale Multisensory Dataset for Robotic Spatial Awareness and Semantic Reasoning in Heterogeneous Environments","abstract":"To ensure the efficiency of robot autonomy under diverse real-world conditions, a high-quality heterogeneous dataset is essential to benchmark the operating algorithms' performance and robustness. Current benchmarks predominantly focus on urban terrains, specifically for on-road autonomous driving, leaving multi-degraded, densely vegetated, dynamic and feature-sparse environments, such as underground tunnels, natural fields, and modern indoor spaces underrepresented. To fill this gap, we introduce EnvoDat, a large-scale, multi-modal dataset collected in diverse environments and conditions, including high illumination, fog, rain, and zero visibility at different times of the day. Overall, EnvoDat contains 26 sequences from 13 scenes, 10 sensing modalities, over 1.9TB of data, and over 89K fine-grained polygon-based annotations for more than 82 object and terrain classes. We post-processed EnvoDat in different formats that support benchmarking SLAM and supervised learning algorithms, and fine-tuning multimodal vision models. With EnvoDat, we contribute to environment-resilient robotic autonomy in areas where the conditions are extremely challenging. The datasets and other relevant resources can be accessed through https://linusnep.github.io/EnvoDat/.","sentences":["To ensure the efficiency of robot autonomy under diverse real-world conditions, a high-quality heterogeneous dataset is essential to benchmark the operating algorithms' performance and robustness.","Current benchmarks predominantly focus on urban terrains, specifically for on-road autonomous driving, leaving multi-degraded, densely vegetated, dynamic and feature-sparse environments, such as underground tunnels, natural fields, and modern indoor spaces underrepresented.","To fill this gap, we introduce EnvoDat, a large-scale, multi-modal dataset collected in diverse environments and conditions, including high illumination, fog, rain, and zero visibility at different times of the day.","Overall, EnvoDat contains 26 sequences from 13 scenes, 10 sensing modalities, over 1.9TB of data, and over 89K fine-grained polygon-based annotations for more than 82 object and terrain classes.","We post-processed EnvoDat in different formats that support benchmarking SLAM and supervised learning algorithms, and fine-tuning multimodal vision models.","With EnvoDat, we contribute to environment-resilient robotic autonomy in areas where the conditions are extremely challenging.","The datasets and other relevant resources can be accessed through https://linusnep.github.io/EnvoDat/."],"url":"http://arxiv.org/abs/2410.22200v1"}
{"created":"2024-10-29 16:34:08","title":"Class-Aware Contrastive Optimization for Imbalanced Text Classification","abstract":"The unique characteristics of text data make classification tasks a complex problem. Advances in unsupervised and semi-supervised learning and autoencoder architectures addressed several challenges. However, they still struggle with imbalanced text classification tasks, a common scenario in real-world applications, demonstrating a tendency to produce embeddings with unfavorable properties, such as class overlap. In this paper, we show that leveraging class-aware contrastive optimization combined with denoising autoencoders can successfully tackle imbalanced text classification tasks, achieving better performance than the current state-of-the-art. Concretely, our proposal combines reconstruction loss with contrastive class separation in the embedding space, allowing a better balance between the truthfulness of the generated embeddings and the model's ability to separate different classes. Compared with an extensive set of traditional and state-of-the-art competing methods, our proposal demonstrates a notable increase in performance across a wide variety of text datasets.","sentences":["The unique characteristics of text data make classification tasks a complex problem.","Advances in unsupervised and semi-supervised learning and autoencoder architectures addressed several challenges.","However, they still struggle with imbalanced text classification tasks, a common scenario in real-world applications, demonstrating a tendency to produce embeddings with unfavorable properties, such as class overlap.","In this paper, we show that leveraging class-aware contrastive optimization combined with denoising autoencoders can successfully tackle imbalanced text classification tasks, achieving better performance than the current state-of-the-art.","Concretely, our proposal combines reconstruction loss with contrastive class separation in the embedding space, allowing a better balance between the truthfulness of the generated embeddings and the model's ability to separate different classes.","Compared with an extensive set of traditional and state-of-the-art competing methods, our proposal demonstrates a notable increase in performance across a wide variety of text datasets."],"url":"http://arxiv.org/abs/2410.22197v1"}
{"created":"2024-10-29 16:30:34","title":"$r$Age-$k$: Communication-Efficient Federated Learning Using Age Factor","abstract":"Federated learning (FL) is a collaborative approach where multiple clients, coordinated by a parameter server (PS), train a unified machine-learning model. The approach, however, suffers from two key challenges: data heterogeneity and communication overhead. Data heterogeneity refers to inconsistencies in model training arising from heterogeneous data at different clients. Communication overhead arises from the large volumes of parameter updates exchanged between the PS and clients. Existing solutions typically address these challenges separately. This paper introduces a new communication-efficient algorithm that uses the age of information metric to simultaneously tackle both limitations of FL. We introduce age vectors at the PS, which keep track of how often the different model parameters are updated from the clients. The PS uses this to selectively request updates for specific gradient indices from each client. Further, the PS employs age vectors to identify clients with statistically similar data and group them into clusters. The PS combines the age vectors of the clustered clients to efficiently coordinate gradient index updates among clients within a cluster. We evaluate our approach using the MNIST and CIFAR10 datasets in highly non-i.i.d. settings. The experimental results show that our proposed method can expedite training, surpassing other communication-efficient strategies in efficiency.","sentences":["Federated learning (FL) is a collaborative approach where multiple clients, coordinated by a parameter server (PS), train a unified machine-learning model.","The approach, however, suffers from two key challenges: data heterogeneity and communication overhead.","Data heterogeneity refers to inconsistencies in model training arising from heterogeneous data at different clients.","Communication overhead arises from the large volumes of parameter updates exchanged between the PS and clients.","Existing solutions typically address these challenges separately.","This paper introduces a new communication-efficient algorithm that uses the age of information metric to simultaneously tackle both limitations of FL.","We introduce age vectors at the PS, which keep track of how often the different model parameters are updated from the clients.","The PS uses this to selectively request updates for specific gradient indices from each client.","Further, the PS employs age vectors to identify clients with statistically similar data and group them into clusters.","The PS combines the age vectors of the clustered clients to efficiently coordinate gradient index updates among clients within a cluster.","We evaluate our approach using the MNIST and CIFAR10 datasets in highly non-i.i.d. settings.","The experimental results show that our proposed method can expedite training, surpassing other communication-efficient strategies in efficiency."],"url":"http://arxiv.org/abs/2410.22192v1"}
{"created":"2024-10-29 16:25:50","title":"Active Learning for Vision-Language Models","abstract":"Pre-trained vision-language models (VLMs) like CLIP have demonstrated impressive zero-shot performance on a wide range of downstream computer vision tasks. However, there still exists a considerable performance gap between these models and a supervised deep model trained on a downstream dataset. To bridge this gap, we propose a novel active learning (AL) framework that enhances the zero-shot classification performance of VLMs by selecting only a few informative samples from the unlabeled data for annotation during training. To achieve this, our approach first calibrates the predicted entropy of VLMs and then utilizes a combination of self-uncertainty and neighbor-aware uncertainty to calculate a reliable uncertainty measure for active sample selection. Our extensive experiments show that the proposed approach outperforms existing AL approaches on several image classification datasets, and significantly enhances the zero-shot performance of VLMs.","sentences":["Pre-trained vision-language models (VLMs) like CLIP have demonstrated impressive zero-shot performance on a wide range of downstream computer vision tasks.","However, there still exists a considerable performance gap between these models and a supervised deep model trained on a downstream dataset.","To bridge this gap, we propose a novel active learning (AL) framework that enhances the zero-shot classification performance of VLMs by selecting only a few informative samples from the unlabeled data for annotation during training.","To achieve this, our approach first calibrates the predicted entropy of VLMs and then utilizes a combination of self-uncertainty and neighbor-aware uncertainty to calculate a reliable uncertainty measure for active sample selection.","Our extensive experiments show that the proposed approach outperforms existing AL approaches on several image classification datasets, and significantly enhances the zero-shot performance of VLMs."],"url":"http://arxiv.org/abs/2410.22187v1"}
{"created":"2024-10-29 16:23:20","title":"Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets","abstract":"We propose a novel teacher-student framework to distill knowledge from multiple teachers trained on distinct datasets. Each teacher is first trained from scratch on its own dataset. Then, the teachers are combined into a joint architecture, which fuses the features of all teachers at multiple representation levels. The joint teacher architecture is fine-tuned on samples from all datasets, thus gathering useful generic information from all data samples. Finally, we employ a multi-level feature distillation procedure to transfer the knowledge to a student model for each of the considered datasets. We conduct image classification experiments on seven benchmarks, and action recognition experiments on three benchmarks. To illustrate the power of our feature distillation procedure, the student architectures are chosen to be identical to those of the individual teachers. To demonstrate the flexibility of our approach, we combine teachers with distinct architectures. We show that our novel Multi-Level Feature Distillation (MLFD) can significantly surpass equivalent architectures that are either trained on individual datasets, or jointly trained on all datasets at once. Furthermore, we confirm that each step of the proposed training procedure is well motivated by a comprehensive ablation study. We publicly release our code at https://github.com/AdrianIordache/MLFD.","sentences":["We propose a novel teacher-student framework to distill knowledge from multiple teachers trained on distinct datasets.","Each teacher is first trained from scratch on its own dataset.","Then, the teachers are combined into a joint architecture, which fuses the features of all teachers at multiple representation levels.","The joint teacher architecture is fine-tuned on samples from all datasets, thus gathering useful generic information from all data samples.","Finally, we employ a multi-level feature distillation procedure to transfer the knowledge to a student model for each of the considered datasets.","We conduct image classification experiments on seven benchmarks, and action recognition experiments on three benchmarks.","To illustrate the power of our feature distillation procedure, the student architectures are chosen to be identical to those of the individual teachers.","To demonstrate the flexibility of our approach, we combine teachers with distinct architectures.","We show that our novel Multi-Level Feature Distillation (MLFD) can significantly surpass equivalent architectures that are either trained on individual datasets, or jointly trained on all datasets at once.","Furthermore, we confirm that each step of the proposed training procedure is well motivated by a comprehensive ablation study.","We publicly release our code at https://github.com/AdrianIordache/MLFD."],"url":"http://arxiv.org/abs/2410.22184v1"}
{"created":"2024-10-29 16:19:08","title":"Synthetic Data Generation with Large Language Models for Personalized Community Question Answering","abstract":"Personalization in Information Retrieval (IR) is a topic studied by the research community since a long time. However, there is still a lack of datasets to conduct large-scale evaluations of personalized IR; this is mainly due to the fact that collecting and curating high-quality user-related information requires significant costs and time investment. Furthermore, the creation of datasets for Personalized IR (PIR) tasks is affected by both privacy concerns and the need for accurate user-related data, which are often not publicly available. Recently, researchers have started to explore the use of Large Language Models (LLMs) to generate synthetic datasets, which is a possible solution to generate data for low-resource tasks. In this paper, we investigate the potential of Large Language Models (LLMs) for generating synthetic documents to train an IR system for a Personalized Community Question Answering task. To study the effectiveness of IR models fine-tuned on LLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build Sy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and answers posted on the popular StackExchange communities. Starting from questions in SE-PQA, we generate synthetic answers using different prompt techniques and LLMs. Our findings suggest that LLMs have high potential in generating data tailored to users' needs. The synthetic data can replace human-written training data, even if the generated data may contain incorrect information.","sentences":["Personalization in Information Retrieval (IR) is a topic studied by the research community since a long time.","However, there is still a lack of datasets to conduct large-scale evaluations of personalized IR; this is mainly due to the fact that collecting and curating high-quality user-related information requires significant costs and time investment.","Furthermore, the creation of datasets for Personalized IR (PIR) tasks is affected by both privacy concerns and the need for accurate user-related data, which are often not publicly available.","Recently, researchers have started to explore the use of Large Language Models (LLMs) to generate synthetic datasets, which is a possible solution to generate data for low-resource tasks.","In this paper, we investigate the potential of Large Language Models (LLMs) for generating synthetic documents to train an IR system for a Personalized Community Question Answering task.","To study the effectiveness of IR models fine-tuned on LLM-generated data, we introduce a new dataset, named Sy-SE-PQA.","We build Sy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and answers posted on the popular StackExchange communities.","Starting from questions in SE-PQA, we generate synthetic answers using different prompt techniques and LLMs.","Our findings suggest that LLMs have high potential in generating data tailored to users' needs.","The synthetic data can replace human-written training data, even if the generated data may contain incorrect information."],"url":"http://arxiv.org/abs/2410.22182v1"}
{"created":"2024-10-29 16:17:07","title":"Natural Language Processing for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review","abstract":"Objective: This review aims to analyze the application of natural language processing (NLP) techniques in cancer research using electronic health records (EHRs) and clinical notes. This review addresses gaps in the existing literature by providing a broader perspective than previous studies focused on specific cancer types or applications. Methods: A comprehensive literature search was conducted using the Scopus database, identifying 94 relevant studies published between 2019 and 2024. Data extraction included study characteristics, cancer types, NLP methodologies, dataset information, performance metrics, challenges, and future directions. Studies were categorized based on cancer types and NLP applications. Results: The results showed a growing trend in NLP applications for cancer research, with breast, lung, and colorectal cancers being the most studied. Information extraction and text classification emerged as predominant NLP tasks. A shift from rule-based to advanced machine learning techniques, particularly transformer-based models, was observed. The Dataset sizes used in existing studies varied widely. Key challenges included the limited generalizability of proposed solutions and the need for improved integration into clinical workflows. Conclusion: NLP techniques show significant potential in analyzing EHRs and clinical notes for cancer research. However, future work should focus on improving model generalizability, enhancing robustness in handling complex clinical language, and expanding applications to understudied cancer types. Integration of NLP tools into clinical practice and addressing ethical considerations remain crucial for utilizing the full potential of NLP in enhancing cancer diagnosis, treatment, and patient outcomes.","sentences":["Objective: This review aims to analyze the application of natural language processing (NLP) techniques in cancer research using electronic health records (EHRs) and clinical notes.","This review addresses gaps in the existing literature by providing a broader perspective than previous studies focused on specific cancer types or applications.","Methods: A comprehensive literature search was conducted using the Scopus database, identifying 94 relevant studies published between 2019 and 2024.","Data extraction included study characteristics, cancer types, NLP methodologies, dataset information, performance metrics, challenges, and future directions.","Studies were categorized based on cancer types and NLP applications.","Results:","The results showed a growing trend in NLP applications for cancer research, with breast, lung, and colorectal cancers being the most studied.","Information extraction and text classification emerged as predominant NLP tasks.","A shift from rule-based to advanced machine learning techniques, particularly transformer-based models, was observed.","The Dataset sizes used in existing studies varied widely.","Key challenges included the limited generalizability of proposed solutions and the need for improved integration into clinical workflows.","Conclusion: NLP techniques show significant potential in analyzing EHRs and clinical notes for cancer research.","However, future work should focus on improving model generalizability, enhancing robustness in handling complex clinical language, and expanding applications to understudied cancer types.","Integration of NLP tools into clinical practice and addressing ethical considerations remain crucial for utilizing the full potential of NLP in enhancing cancer diagnosis, treatment, and patient outcomes."],"url":"http://arxiv.org/abs/2410.22180v1"}
{"created":"2024-10-29 16:15:59","title":"Analyzing Multimodal Interaction Strategies for LLM-Assisted Manipulation of 3D Scenes","abstract":"As more applications of large language models (LLMs) for 3D content for immersive environments emerge, it is crucial to study user behaviour to identify interaction patterns and potential barriers to guide the future design of immersive content creation and editing systems which involve LLMs. In an empirical user study with 12 participants, we combine quantitative usage data with post-experience questionnaire feedback to reveal common interaction patterns and key barriers in LLM-assisted 3D scene editing systems. We identify opportunities for improving natural language interfaces in 3D design tools and propose design recommendations for future LLM-integrated 3D content creation systems. Through an empirical study, we demonstrate that LLM-assisted interactive systems can be used productively in immersive environments.","sentences":["As more applications of large language models (LLMs) for 3D content for immersive environments emerge, it is crucial to study user behaviour to identify interaction patterns and potential barriers to guide the future design of immersive content creation and editing systems which involve LLMs.","In an empirical user study with 12 participants, we combine quantitative usage data with post-experience questionnaire feedback to reveal common interaction patterns and key barriers in LLM-assisted 3D scene editing systems.","We identify opportunities for improving natural language interfaces in 3D design tools and propose design recommendations for future LLM-integrated 3D content creation systems.","Through an empirical study, we demonstrate that LLM-assisted interactive systems can be used productively in immersive environments."],"url":"http://arxiv.org/abs/2410.22177v1"}
{"created":"2024-10-29 15:54:09","title":"Training LLMs for Generating IEC 61131-3 Structured Text with Online Feedback","abstract":"The advent of large language models (LLMs), such as GPT-4, has enabled significant advancements in generating code across various domains. However, these models face unique challenges when generating IEC 61131-3 Structured Text (ST) code due to limited data in public training datasets and the complexity of ST language syntax. This paper proposes a novel approach to training LLMs that emphasizes improving the quality of learning data through an online process involving compiler feedback and evaluation from a secondary LLM. In this framework, the primary LLM generates new training samples, which are subsequently evaluated by a compiler for syntactical correctness and by a specialized LLM that excels at assessing semantic accuracy, though it is not optimized for code generation itself. Through iterative refinement of the training data, this approach results in marked improvements for the trained LLM, leading to higher compilation success rates and better semantic precision. As a result, the framework proves highly suitable for industrial automation applications and outperforms state-of-the-art models.","sentences":["The advent of large language models (LLMs), such as GPT-4, has enabled significant advancements in generating code across various domains.","However, these models face unique challenges when generating IEC 61131-3 Structured Text (ST) code due to limited data in public training datasets and the complexity of ST language syntax.","This paper proposes a novel approach to training LLMs that emphasizes improving the quality of learning data through an online process involving compiler feedback and evaluation from a secondary LLM.","In this framework, the primary LLM generates new training samples, which are subsequently evaluated by a compiler for syntactical correctness and by a specialized LLM that excels at assessing semantic accuracy, though it is not optimized for code generation itself.","Through iterative refinement of the training data, this approach results in marked improvements for the trained LLM, leading to higher compilation success rates and better semantic precision.","As a result, the framework proves highly suitable for industrial automation applications and outperforms state-of-the-art models."],"url":"http://arxiv.org/abs/2410.22159v1"}
{"created":"2024-10-29 15:48:55","title":"Shining a Light on Hurricane Damage Estimation via Nighttime Light Data: Pre-processing Matters","abstract":"Amidst escalating climate change, hurricanes are inflicting severe socioeconomic impacts, marked by heightened economic losses and increased displacement. Previous research utilized nighttime light data to predict the impact of hurricanes on economic losses. However, prior work did not provide a thorough analysis of the impact of combining different techniques for pre-processing nighttime light (NTL) data. Addressing this gap, our research explores a variety of NTL pre-processing techniques, including value thresholding, built masking, and quality filtering and imputation, applied to two distinct datasets, VSC-NTL and VNP46A2, at the zip code level. Experiments evaluate the correlation of the denoised NTL data with economic damages of Category 4-5 hurricanes in Florida. They reveal that the quality masking and imputation technique applied to VNP46A2 show a substantial correlation with economic damage data.","sentences":["Amidst escalating climate change, hurricanes are inflicting severe socioeconomic impacts, marked by heightened economic losses and increased displacement.","Previous research utilized nighttime light data to predict the impact of hurricanes on economic losses.","However, prior work did not provide a thorough analysis of the impact of combining different techniques for pre-processing nighttime light (NTL) data.","Addressing this gap, our research explores a variety of NTL pre-processing techniques, including value thresholding, built masking, and quality filtering and imputation, applied to two distinct datasets, VSC-NTL and VNP46A2, at the zip code level.","Experiments evaluate the correlation of the denoised NTL data with economic damages of Category 4-5 hurricanes in Florida.","They reveal that the quality masking and imputation technique applied to VNP46A2 show a substantial correlation with economic damage data."],"url":"http://arxiv.org/abs/2410.22150v1"}
{"created":"2024-10-29 15:39:45","title":"A Data-Driven Analysis of the Sovereign Citizens Movement on Telegram","abstract":"Online communities of known extremist groups like the alt-right and QAnon have been well explored in past work. However, we find that an extremist group called Sovereign Citizens is relatively unexplored despite its existence since the 1970s. Their main belief is delegitimizing the established government with a tactic called paper terrorism, clogging courts with pseudolegal claims. In recent years, their activities have escalated to threats like forcefully claiming property ownership and participating in the Capitol Riot. This paper aims to shed light on Sovereign Citizens' online activities by examining two Telegram channels, each belonging to an identified Sovereign Citizen individual. We collect over 888K text messages and apply NLP techniques. We find that the two channels differ in the topics they discussed, demonstrating different focuses. Further, the two channels exhibit less toxic content compared to other extremist groups like QAnon. Finally, we find indications of overlapping beliefs between the two channels and QAnon, suggesting a merging or complementing of beliefs.","sentences":["Online communities of known extremist groups like the alt-right and QAnon have been well explored in past work.","However, we find that an extremist group called Sovereign Citizens is relatively unexplored despite its existence since the 1970s.","Their main belief is delegitimizing the established government with a tactic called paper terrorism, clogging courts with pseudolegal claims.","In recent years, their activities have escalated to threats like forcefully claiming property ownership and participating in the Capitol Riot.","This paper aims to shed light on Sovereign Citizens' online activities by examining two Telegram channels, each belonging to an identified Sovereign Citizen individual.","We collect over 888K text messages and apply NLP techniques.","We find that the two channels differ in the topics they discussed, demonstrating different focuses.","Further, the two channels exhibit less toxic content compared to other extremist groups like QAnon.","Finally, we find indications of overlapping beliefs between the two channels and QAnon, suggesting a merging or complementing of beliefs."],"url":"http://arxiv.org/abs/2410.22142v1"}
{"created":"2024-10-29 15:32:36","title":"SimRec: Mitigating the Cold-Start Problem in Sequential Recommendation by Integrating Item Similarity","abstract":"Sequential recommendation systems often struggle to make predictions or take action when dealing with cold-start items that have limited amount of interactions. In this work, we propose SimRec - a new approach to mitigate the cold-start problem in sequential recommendation systems. SimRec addresses this challenge by leveraging the inherent similarity among items, incorporating item similarities into the training process through a customized loss function. Importantly, this enhancement is attained with identical model architecture and the same amount of trainable parameters, resulting in the same inference time and requiring minimal additional effort. This novel approach results in a robust contextual sequential recommendation model capable of effectively handling rare items, including those that were not explicitly seen during training, thereby enhancing overall recommendation performance. Rigorous evaluations against multiple baselines on diverse datasets showcase SimRec's superiority, particularly in scenarios involving items occurring less than 10 times in the training data. The experiments reveal an impressive improvement, with SimRec achieving up to 78% higher HR@10 compared to SASRec. Notably, SimRec outperforms strong baselines on sparse datasets while delivering on-par performance on dense datasets. Our code is available at https://github.com/amazon-science/sequential-recommendation-using-similarity.","sentences":["Sequential recommendation systems often struggle to make predictions or take action when dealing with cold-start items that have limited amount of interactions.","In this work, we propose SimRec - a new approach to mitigate the cold-start problem in sequential recommendation systems.","SimRec addresses this challenge by leveraging the inherent similarity among items, incorporating item similarities into the training process through a customized loss function.","Importantly, this enhancement is attained with identical model architecture and the same amount of trainable parameters, resulting in the same inference time and requiring minimal additional effort.","This novel approach results in a robust contextual sequential recommendation model capable of effectively handling rare items, including those that were not explicitly seen during training, thereby enhancing overall recommendation performance.","Rigorous evaluations against multiple baselines on diverse datasets showcase SimRec's superiority, particularly in scenarios involving items occurring less than 10 times in the training data.","The experiments reveal an impressive improvement, with SimRec achieving up to 78% higher HR@10 compared to SASRec.","Notably, SimRec outperforms strong baselines on sparse datasets while delivering on-par performance on dense datasets.","Our code is available at https://github.com/amazon-science/sequential-recommendation-using-similarity."],"url":"http://arxiv.org/abs/2410.22136v1"}
{"created":"2024-10-29 15:31:27","title":"Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic Segmentation","abstract":"Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a large-scale source-domain dataset, and then transfer the model to data-scarce target-domain datasets for pixel-level segmentation. The significant domain gap between the source and target datasets leads to a sharp decline in the performance of existing few-shot segmentation (FSS) methods in cross-domain scenarios. In this work, we discover an intriguing phenomenon: simply filtering different frequency components for target domains can lead to a significant performance improvement, sometimes even as high as 14% mIoU. Then, we delve into this phenomenon for an interpretation, and find such improvements stem from the reduced inter-channel correlation in feature maps, which benefits CD-FSS with enhanced robustness against domain gaps and larger activated regions for segmentation. Based on this, we propose a lightweight frequency masker, which further reduces channel correlations by an amplitude-phase-masker (APM) module and an Adaptive Channel Phase Attention (ACPA) module. Notably, APM introduces only 0.01% additional parameters but improves the average performance by over 10%, and ACPA imports only 2.5% parameters but further improves the performance by over 1.5%, which significantly surpasses the state-of-the-art CD-FSS methods.","sentences":["Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a large-scale source-domain dataset, and then transfer the model to data-scarce target-domain datasets for pixel-level segmentation.","The significant domain gap between the source and target datasets leads to a sharp decline in the performance of existing few-shot segmentation (FSS) methods in cross-domain scenarios.","In this work, we discover an intriguing phenomenon: simply filtering different frequency components for target domains can lead to a significant performance improvement, sometimes even as high as 14% mIoU. Then, we delve into this phenomenon for an interpretation, and find such improvements stem from the reduced inter-channel correlation in feature maps, which benefits CD-FSS with enhanced robustness against domain gaps and larger activated regions for segmentation.","Based on this, we propose a lightweight frequency masker, which further reduces channel correlations by an amplitude-phase-masker (APM) module and an Adaptive Channel Phase Attention (ACPA) module.","Notably, APM introduces only 0.01% additional parameters but improves the average performance by over 10%, and ACPA imports only 2.5% parameters but further improves the performance by over 1.5%, which significantly surpasses the state-of-the-art CD-FSS methods."],"url":"http://arxiv.org/abs/2410.22135v1"}
{"created":"2024-10-29 15:31:03","title":"Learning Successor Features the Simple Way","abstract":"In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporal-difference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs. We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Miniworld) mazes and Mujoco, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required.","sentences":["In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments.","Successor Features (SFs) offer a potential solution to this challenge.","However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data.","More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency.","We introduce a novel, simple method for learning SFs directly from pixels.","Our approach uses a combination of a Temporal-difference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs.","We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Miniworld) mazes and Mujoco, for both single and continual learning scenarios.","As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches.","Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required."],"url":"http://arxiv.org/abs/2410.22133v1"}
{"created":"2024-10-29 15:25:21","title":"RankUp: Boosting Semi-Supervised Regression with an Auxiliary Ranking Classifier","abstract":"State-of-the-art (SOTA) semi-supervised learning techniques, such as FixMatch and it's variants, have demonstrated impressive performance in classification tasks. However, these methods are not directly applicable to regression tasks. In this paper, we present RankUp, a simple yet effective approach that adapts existing semi-supervised classification techniques to enhance the performance of regression tasks. RankUp achieves this by converting the original regression task into a ranking problem and training it concurrently with the original regression objective. This auxiliary ranking classifier outputs a classification result, thus enabling integration with existing semi-supervised classification methods. Moreover, we introduce regression distribution alignment (RDA), a complementary technique that further enhances RankUp's performance by refining pseudo-labels through distribution alignment. Despite its simplicity, RankUp, with or without RDA, achieves SOTA results in across a range of regression benchmarks, including computer vision, audio, and natural language processing tasks. Our code and log data are open-sourced at https://github.com/pm25/semi-supervised-regression.","sentences":["State-of-the-art (SOTA) semi-supervised learning techniques, such as FixMatch and it's variants, have demonstrated impressive performance in classification tasks.","However, these methods are not directly applicable to regression tasks.","In this paper, we present RankUp, a simple yet effective approach that adapts existing semi-supervised classification techniques to enhance the performance of regression tasks.","RankUp achieves this by converting the original regression task into a ranking problem and training it concurrently with the original regression objective.","This auxiliary ranking classifier outputs a classification result, thus enabling integration with existing semi-supervised classification methods.","Moreover, we introduce regression distribution alignment (RDA), a complementary technique that further enhances RankUp's performance by refining pseudo-labels through distribution alignment.","Despite its simplicity, RankUp, with or without RDA, achieves SOTA results in across a range of regression benchmarks, including computer vision, audio, and natural language processing tasks.","Our code and log data are open-sourced at https://github.com/pm25/semi-supervised-regression."],"url":"http://arxiv.org/abs/2410.22124v1"}
{"created":"2024-10-29 15:24:27","title":"Testing Identity of Distributions under Kolmogorov Distance in Polylogarithmic Space","abstract":"Suppose we have a sample from a distribution $D$ and we want to test whether $D = D^*$ for a fixed distribution $D^*$. Specifically, we want to reject with constant probability, if the distance of $D$ from $D^*$ is $\\geq \\varepsilon$ in a given metric. In the case of continuous distributions, this has been studied thoroughly in the statistics literature. Namely, for the well-studied Kolmogorov metric a test is known that uses the optimal $O(1/\\varepsilon^2)$ samples.   However, this test naively uses also space $O(1/\\varepsilon^2)$, and previous work improved this to $O(1/\\varepsilon)$. In this paper, we show that much less space suffices -- we give an algorithm that uses space $O(\\log^4 \\varepsilon^{-1})$ in the streaming setting while also using an asymptotically optimal number of samples. This is in contrast with the standard total variation distance on discrete distributions for which such space reduction is known to be impossible. Finally, we state 9 related open problems that we hope will spark interest in this and related problems.","sentences":["Suppose we have a sample from a distribution $D$ and we want to test whether $D = D^*$ for a fixed distribution $D^*$. Specifically, we want to reject with constant probability, if the distance of $D$ from $D^*$ is $\\geq \\varepsilon$ in a given metric.","In the case of continuous distributions, this has been studied thoroughly in the statistics literature.","Namely, for the well-studied Kolmogorov metric a test is known that uses the optimal $O(1/\\varepsilon^2)$ samples.   ","However, this test naively uses also space $O(1/\\varepsilon^2)$, and previous work improved this to $O(1/\\varepsilon)$. In this paper, we show that much less space suffices -- we give an algorithm that uses space $O(\\log^4 \\varepsilon^{-1})$ in the streaming setting while also using an asymptotically optimal number of samples.","This is in contrast with the standard total variation distance on discrete distributions for which such space reduction is known to be impossible.","Finally, we state 9 related open problems that we hope will spark interest in this and related problems."],"url":"http://arxiv.org/abs/2410.22123v1"}
{"created":"2024-10-29 15:22:45","title":"Vision Paper: Designing Graph Neural Networks in Compliance with the European Artificial Intelligence Act","abstract":"The European Union's Artificial Intelligence Act (AI Act) introduces comprehensive guidelines for the development and oversight of Artificial Intelligence (AI) and Machine Learning (ML) systems, with significant implications for Graph Neural Networks (GNNs). This paper addresses the unique challenges posed by the AI Act for GNNs, which operate on complex graph-structured data. The legislation's requirements for data management, data governance, robustness, human oversight, and privacy necessitate tailored strategies for GNNs. Our study explores the impact of these requirements on GNN training and proposes methods to ensure compliance. We provide an in-depth analysis of bias, robustness, explainability, and privacy in the context of GNNs, highlighting the need for fair sampling strategies and effective interpretability techniques. Our contributions fill the research gap by offering specific guidance for GNNs under the new legislative framework and identifying open questions and future research directions.","sentences":["The European Union's Artificial Intelligence Act (AI Act) introduces comprehensive guidelines for the development and oversight of Artificial Intelligence (AI) and Machine Learning (ML) systems, with significant implications for Graph Neural Networks (GNNs).","This paper addresses the unique challenges posed by the AI Act for GNNs, which operate on complex graph-structured data.","The legislation's requirements for data management, data governance, robustness, human oversight, and privacy necessitate tailored strategies for GNNs.","Our study explores the impact of these requirements on GNN training and proposes methods to ensure compliance.","We provide an in-depth analysis of bias, robustness, explainability, and privacy in the context of GNNs, highlighting the need for fair sampling strategies and effective interpretability techniques.","Our contributions fill the research gap by offering specific guidance for GNNs under the new legislative framework and identifying open questions and future research directions."],"url":"http://arxiv.org/abs/2410.22120v1"}
{"created":"2024-10-29 15:14:37","title":"Where Do Large Learning Rates Lead Us?","abstract":"It is generally accepted that starting neural networks training with large learning rates (LRs) improves generalization. Following a line of research devoted to understanding this effect, we conduct an empirical study in a controlled setting focusing on two questions: 1) how large an initial LR is required for obtaining optimal quality, and 2) what are the key differences between models trained with different LRs? We discover that only a narrow range of initial LRs slightly above the convergence threshold lead to optimal results after fine-tuning with a small LR or weight averaging. By studying the local geometry of reached minima, we observe that using LRs from this optimal range allows for the optimization to locate a basin that only contains high-quality minima. Additionally, we show that these initial LRs result in a sparse set of learned features, with a clear focus on those most relevant for the task. In contrast, starting training with too small LRs leads to unstable minima and attempts to learn all features simultaneously, resulting in poor generalization. Conversely, using initial LRs that are too large fails to detect a basin with good solutions and extract meaningful patterns from the data.","sentences":["It is generally accepted that starting neural networks training with large learning rates (LRs) improves generalization.","Following a line of research devoted to understanding this effect, we conduct an empirical study in a controlled setting focusing on two questions: 1) how large an initial LR is required for obtaining optimal quality, and 2) what are the key differences between models trained with different LRs?","We discover that only a narrow range of initial LRs slightly above the convergence threshold lead to optimal results after fine-tuning with a small LR or weight averaging.","By studying the local geometry of reached minima, we observe that using LRs from this optimal range allows for the optimization to locate a basin that only contains high-quality minima.","Additionally, we show that these initial LRs result in a sparse set of learned features, with a clear focus on those most relevant for the task.","In contrast, starting training with too small LRs leads to unstable minima and attempts to learn all features simultaneously, resulting in poor generalization.","Conversely, using initial LRs that are too large fails to detect a basin with good solutions and extract meaningful patterns from the data."],"url":"http://arxiv.org/abs/2410.22113v1"}
{"created":"2024-10-29 15:11:45","title":"Multimodal Semantic Communication for Generative Audio-Driven Video Conferencing","abstract":"This paper studies an efficient multimodal data communication scheme for video conferencing. In our considered system, a speaker gives a talk to the audiences, with talking head video and audio being transmitted. Since the speaker does not frequently change posture and high-fidelity transmission of audio (speech and music) is required, redundant visual video data exists and can be removed by generating the video from the audio. To this end, we propose a wave-to-video (Wav2Vid) system, an efficient video transmission framework that reduces transmitted data by generating talking head video from audio. In particular, full-duration audio and short-duration video data are synchronously transmitted through a wireless channel, with neural networks (NNs) extracting and encoding audio and video semantics. The receiver then combines the decoded audio and video data, as well as uses a generative adversarial network (GAN) based model to generate the lip movement videos of the speaker. Simulation results show that the proposed Wav2Vid system can reduce the amount of transmitted data by up to 83% while maintaining the perceptual quality of the generated conferencing video.","sentences":["This paper studies an efficient multimodal data communication scheme for video conferencing.","In our considered system, a speaker gives a talk to the audiences, with talking head video and audio being transmitted.","Since the speaker does not frequently change posture and high-fidelity transmission of audio (speech and music) is required, redundant visual video data exists and can be removed by generating the video from the audio.","To this end, we propose a wave-to-video (Wav2Vid) system, an efficient video transmission framework that reduces transmitted data by generating talking head video from audio.","In particular, full-duration audio and short-duration video data are synchronously transmitted through a wireless channel, with neural networks (NNs) extracting and encoding audio and video semantics.","The receiver then combines the decoded audio and video data, as well as uses a generative adversarial network (GAN) based model to generate the lip movement videos of the speaker.","Simulation results show that the proposed Wav2Vid system can reduce the amount of transmitted data by up to 83% while maintaining the perceptual quality of the generated conferencing video."],"url":"http://arxiv.org/abs/2410.22112v1"}
{"created":"2024-10-29 15:08:50","title":"Data Generation for Hardware-Friendly Post-Training Quantization","abstract":"Zero-shot quantization (ZSQ) using synthetic data is a key approach for post-training quantization (PTQ) under privacy and security constraints. However, existing data generation methods often struggle to effectively generate data suitable for hardware-friendly quantization, where all model layers are quantized. We analyze existing data generation methods based on batch normalization (BN) matching and identify several gaps between synthetic and real data: 1) Current generation algorithms do not optimize the entire synthetic dataset simultaneously; 2) Data augmentations applied during training are often overlooked; and 3) A distribution shift occurs in the final model layers due to the absence of BN in those layers. These gaps negatively impact ZSQ performance, particularly in hardware-friendly quantization scenarios. In this work, we propose Data Generation for Hardware-friendly quantization (DGH), a novel method that addresses these gaps. DGH jointly optimizes all generated images, regardless of the image set size or GPU memory constraints. To address data augmentation mismatches, DGH includes a preprocessing stage that mimics the augmentation process and enhances image quality by incorporating natural image priors. Finally, we propose a new distribution-stretching loss that aligns the support of the feature map distribution between real and synthetic data. This loss is applied to the model's output and can be adapted to various tasks. DGH demonstrates significant improvements in quantization performance across multiple tasks, achieving up to a 30% increase in accuracy for hardware-friendly ZSQ in both classification and object detection, often performing on par with real data.","sentences":["Zero-shot quantization (ZSQ) using synthetic data is a key approach for post-training quantization (PTQ) under privacy and security constraints.","However, existing data generation methods often struggle to effectively generate data suitable for hardware-friendly quantization, where all model layers are quantized.","We analyze existing data generation methods based on batch normalization (BN) matching and identify several gaps between synthetic and real data: 1) Current generation algorithms do not optimize the entire synthetic dataset simultaneously; 2) Data augmentations applied during training are often overlooked; and 3) A distribution shift occurs in the final model layers due to the absence of BN in those layers.","These gaps negatively impact ZSQ performance, particularly in hardware-friendly quantization scenarios.","In this work, we propose Data Generation for Hardware-friendly quantization (DGH), a novel method that addresses these gaps.","DGH jointly optimizes all generated images, regardless of the image set size or GPU memory constraints.","To address data augmentation mismatches, DGH includes a preprocessing stage that mimics the augmentation process and enhances image quality by incorporating natural image priors.","Finally, we propose a new distribution-stretching loss that aligns the support of the feature map distribution between real and synthetic data.","This loss is applied to the model's output and can be adapted to various tasks.","DGH demonstrates significant improvements in quantization performance across multiple tasks, achieving up to a 30% increase in accuracy for hardware-friendly ZSQ in both classification and object detection, often performing on par with real data."],"url":"http://arxiv.org/abs/2410.22110v1"}
{"created":"2024-10-29 15:08:13","title":"Faster two-dimensional pattern matching with $k$ mismatches","abstract":"The classical pattern matching asks for locating all occurrences of one string, called the pattern, in another, called the text, where a string is simply a sequence of characters. Due to the potential practical applications, it is desirable to seek approximate occurrences, for example by bounding the number of mismatches. This problem has been extensively studied, and by now we have a good understanding of the best possible time complexity as a function of $n$ (length of the text), $m$ (length of the pattern), and $k$ (number of mismatches). In particular, we know that for $k=\\mathcal{O}(\\sqrt{m})$, we can achieve quasi-linear time complexity [Gawrychowski and Uzna\\'nski, ICALP 2018].   We consider a natural generalisation of the approximate pattern matching problem to two-dimensional strings, which are simply square arrays of characters. The exact version of this problem has been extensively studied in the early 90s. While periodicity, which is the basic tool for one-dimensional pattern matching, admits a natural extension to two dimensions, it turns out to become significantly more challenging to work with, and it took some time until an alphabet-independent linear-time algorithm has been obtained by Galil and Park [SICOMP 1996].   In the approximate two-dimensional pattern matching, we are given a pattern of size $m\\times m$ and a text of size $n\\times n$, and ask for all locations in the text where the pattern matches with at most $k$ mismatches. The asymptotically fastest algorithm for this algorithm works in $\\mathcal{O}(kn^{2})$ time [Amir and Landau, TCS 1991]. We provide a new insight into two-dimensional periodicity to improve on these 30-years old bounds. Our algorithm works in $\\tilde{\\mathcal{O}}((m^{2}+mk^{5/4})n^{2}/m^{2})$ time, which is $\\tilde{\\mathcal{O}}(n^{2})$ for $k=\\mathcal{O}(m^{4/5})$.","sentences":["The classical pattern matching asks for locating all occurrences of one string, called the pattern, in another, called the text, where a string is simply a sequence of characters.","Due to the potential practical applications, it is desirable to seek approximate occurrences, for example by bounding the number of mismatches.","This problem has been extensively studied, and by now we have a good understanding of the best possible time complexity as a function of $n$ (length of the text), $m$ (length of the pattern), and $k$ (number of mismatches).","In particular, we know that for $k=\\mathcal{O}(\\sqrt{m})$, we can achieve quasi-linear time complexity","[Gawrychowski and Uzna\\'nski, ICALP 2018].   ","We consider a natural generalisation of the approximate pattern matching problem to two-dimensional strings, which are simply square arrays of characters.","The exact version of this problem has been extensively studied in the early 90s.","While periodicity, which is the basic tool for one-dimensional pattern matching, admits a natural extension to two dimensions, it turns out to become significantly more challenging to work with, and it took some time until an alphabet-independent linear-time algorithm has been obtained by Galil and Park","[SICOMP 1996].   ","In the approximate two-dimensional pattern matching, we are given a pattern of size $m\\times m$ and a text of size $n\\times n$, and ask for all locations in the text where the pattern matches with at most $k$ mismatches.","The asymptotically fastest algorithm for this algorithm works in $\\mathcal{O}(kn^{2})$ time [Amir and Landau, TCS 1991].","We provide a new insight into two-dimensional periodicity to improve on these 30-years old bounds.","Our algorithm works in $\\tilde{\\mathcal{O}}((m^{2}+mk^{5/4})n^{2}/m^{2})$ time, which is $\\tilde{\\mathcal{O}}(n^{2})$ for $k=\\mathcal{O}(m^{4/5})$."],"url":"http://arxiv.org/abs/2410.22109v1"}
{"created":"2024-10-29 15:07:23","title":"Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench","abstract":"Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals' confidential and private data, raising legal and ethical concerns. While many previous works have addressed this issue in LLM via machine unlearning, it remains largely unexplored for MLLMs. To tackle this challenge, we introduce Multimodal Large Language Model Unlearning Benchmark (MLLMU-Bench), a novel benchmark aimed at advancing the understanding of multimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles and 153 profiles for public celebrities, each profile feature over 14 customized question-answer pairs, evaluated from both multimodal (image+text) and unimodal (text) perspectives. The benchmark is divided into four sets to assess unlearning algorithms in terms of efficacy, generalizability, and model utility. Finally, we provide baseline results using existing generative model unlearning algorithms. Surprisingly, our experiments show that unimodal unlearning algorithms excel in generation and cloze tasks, while multimodal unlearning approaches perform better in classification tasks with multimodal inputs.","sentences":["Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals' confidential and private data, raising legal and ethical concerns.","While many previous works have addressed this issue in LLM via machine unlearning, it remains largely unexplored for MLLMs.","To tackle this challenge, we introduce Multimodal Large Language Model Unlearning Benchmark (MLLMU-Bench), a novel benchmark aimed at advancing the understanding of multimodal machine unlearning.","MLLMU-Bench consists of 500 fictitious profiles and 153 profiles for public celebrities, each profile feature over 14 customized question-answer pairs, evaluated from both multimodal (image+text) and unimodal (text) perspectives.","The benchmark is divided into four sets to assess unlearning algorithms in terms of efficacy, generalizability, and model utility.","Finally, we provide baseline results using existing generative model unlearning algorithms.","Surprisingly, our experiments show that unimodal unlearning algorithms excel in generation and cloze tasks, while multimodal unlearning approaches perform better in classification tasks with multimodal inputs."],"url":"http://arxiv.org/abs/2410.22108v1"}
{"created":"2024-10-29 15:00:36","title":"Ideal Membership Problem for Boolean Minority and Dual Discriminator","abstract":"We consider the polynomial Ideal Membership Problem (IMP) for ideals encoding combinatorial problems that are instances of CSPs over a finite language. In this paper, the input polynomial $f$ has degree at most $d=O(1)$ (we call this problem IMP$_d$). We bridge the gap in \\cite{MonaldoMastrolilli2019} by proving that the IMP$_d$ for Boolean combinatorial ideals whose constraints are closed under the minority polymorphism can be solved in polynomial time. This completes the identification of the tractability for the Boolean IMP$_d$. We also prove that the proof of membership for the IMP$_d$ for problems constrained by the dual discriminator polymorphism over any finite domain can be found in polynomial time. Our results can be used in applications such as Nullstellensatz and Sum-of-Squares proofs.","sentences":["We consider the polynomial Ideal Membership Problem (IMP) for ideals encoding combinatorial problems that are instances of CSPs over a finite language.","In this paper, the input polynomial $f$ has degree at most $d=O(1)$ (we call this problem IMP$_d$).","We bridge the gap in \\cite{MonaldoMastrolilli2019} by proving that the IMP$_d$ for Boolean combinatorial ideals whose constraints are closed under the minority polymorphism can be solved in polynomial time.","This completes the identification of the tractability for the Boolean IMP$_d$. We also prove that the proof of membership for the IMP$_d$ for problems constrained by the dual discriminator polymorphism over any finite domain can be found in polynomial time.","Our results can be used in applications such as Nullstellensatz and Sum-of-Squares proofs."],"url":"http://arxiv.org/abs/2410.22102v1"}
{"created":"2024-10-29 14:46:49","title":"InLINE: Inner-Layer Information Exchange for Multi-task Learning on Heterogeneous Graphs","abstract":"Heterogeneous graph is an important structure for modeling complex relational data in real-world scenarios and usually involves various node prediction tasks within a single graph. Training these tasks separately may neglect beneficial information sharing, hence a preferred way is to learn several tasks in a same model by Multi-Task Learning (MTL). However, MTL introduces the issue of negative transfer, where the training of different tasks interferes with each other as they may focus on different information from the data, resulting in suboptimal performance. To solve the issue, existing MTL methods use separate backbones for each task, then selectively exchange beneficial features through interactions among the output embeddings from each layer of different backbones, which we refer to as outer-layer exchange. However, the negative transfer in heterogeneous graphs arises not simply from the varying importance of an individual node feature across tasks, but also from the varying importance of inter-relation between two nodes across tasks. These inter-relations are entangled in the output embedding, making it difficult for existing methods to discriminate beneficial information from the embedding. To address this challenge, we propose the Inner-Layer Information Exchange (InLINE) model that facilitate fine-grained information exchanges within each graph layer rather than through output embeddings. Specifically, InLINE consists of (1) Structure Disentangled Experts for layer-wise structure disentanglement, (2) Structure Disentangled Gates for assigning disentangled information to different tasks. Evaluations on two public datasets and a large industry dataset show that our model effectively alleviates the significant performance drop on specific tasks caused by negative transfer, improving Macro F1 by 6.3% on DBLP dataset and AUC by 3.6% on the industry dataset compared to SoA methods.","sentences":["Heterogeneous graph is an important structure for modeling complex relational data in real-world scenarios and usually involves various node prediction tasks within a single graph.","Training these tasks separately may neglect beneficial information sharing, hence a preferred way is to learn several tasks in a same model by Multi-Task Learning (MTL).","However, MTL introduces the issue of negative transfer, where the training of different tasks interferes with each other as they may focus on different information from the data, resulting in suboptimal performance.","To solve the issue, existing MTL methods use separate backbones for each task, then selectively exchange beneficial features through interactions among the output embeddings from each layer of different backbones, which we refer to as outer-layer exchange.","However, the negative transfer in heterogeneous graphs arises not simply from the varying importance of an individual node feature across tasks, but also from the varying importance of inter-relation between two nodes across tasks.","These inter-relations are entangled in the output embedding, making it difficult for existing methods to discriminate beneficial information from the embedding.","To address this challenge, we propose the Inner-Layer Information Exchange (InLINE) model that facilitate fine-grained information exchanges within each graph layer rather than through output embeddings.","Specifically, InLINE consists of (1) Structure Disentangled Experts for layer-wise structure disentanglement, (2) Structure Disentangled Gates for assigning disentangled information to different tasks.","Evaluations on two public datasets and a large industry dataset show that our model effectively alleviates the significant performance drop on specific tasks caused by negative transfer, improving Macro F1 by 6.3% on DBLP dataset and AUC by 3.6% on the industry dataset compared to SoA methods."],"url":"http://arxiv.org/abs/2410.22089v1"}
{"created":"2024-10-29 14:36:11","title":"A New Broadcast Primitive for BFT Protocols","abstract":"Byzantine fault tolerant (BFT) protocol descriptions often assume application-layer networking primitives, such as best-effort and reliable broadcast, which are impossible to implement in practice in a Byzantine environment as they require either unbounded buffering of messages or giving up liveness, under certain circumstances. However, many of these protocols do not (or can be modified to not) need such strong networking primitives. In this paper, we define a new, slightly weaker networking primitive that we call abortable broadcast. We describe an implementation of this new primitive and show that it (1) still provides strong delivery guarantees, even in the case of network congestion, link or peer failure, and backpressure, (2) preserves bandwidth, and (3) enforces all data structures to be bounded even in the presence of malicious peers. The latter prevents out-of-memory DoS attacks by malicious peers, an issue often overlooked in the literature. The new primitive and its implementation are not just theoretical. We use them to implement the BFT protocols in the IPC (InProductionChain), a publicly available blockchain network that enables replicated execution of general-purpose computation, serving hundreds of thousands of applications and their users.","sentences":["Byzantine fault tolerant (BFT) protocol descriptions often assume application-layer networking primitives, such as best-effort and reliable broadcast, which are impossible to implement in practice in a Byzantine environment as they require either unbounded buffering of messages or giving up liveness, under certain circumstances.","However, many of these protocols do not (or can be modified to not) need such strong networking primitives.","In this paper, we define a new, slightly weaker networking primitive that we call abortable broadcast.","We describe an implementation of this new primitive and show that it (1) still provides strong delivery guarantees, even in the case of network congestion, link or peer failure, and backpressure, (2) preserves bandwidth, and (3) enforces all data structures to be bounded even in the presence of malicious peers.","The latter prevents out-of-memory DoS attacks by malicious peers, an issue often overlooked in the literature.","The new primitive and its implementation are not just theoretical.","We use them to implement the BFT protocols in the IPC (InProductionChain), a publicly available blockchain network that enables replicated execution of general-purpose computation, serving hundreds of thousands of applications and their users."],"url":"http://arxiv.org/abs/2410.22080v1"}
{"created":"2024-10-29 14:34:41","title":"USpeech: Ultrasound-Enhanced Speech with Minimal Human Effort via Cross-Modal Synthesis","abstract":"Speech enhancement is crucial in human-computer interaction, especially for ubiquitous devices. Ultrasound-based speech enhancement has emerged as an attractive choice because of its superior ubiquity and performance. However, inevitable interference from unexpected and unintended sources during audio-ultrasound data acquisition makes existing solutions rely heavily on human effort for data collection and processing. This leads to significant data scarcity that limits the full potential of ultrasound-based speech enhancement. To address this, we propose USpeech, a cross-modal ultrasound synthesis framework for speech enhancement with minimal human effort. At its core is a two-stage framework that establishes correspondence between visual and ultrasonic modalities by leveraging audible audio as a bridge. This approach overcomes challenges from the lack of paired video-ultrasound datasets and the inherent heterogeneity between video and ultrasound data. Our framework incorporates contrastive video-audio pre-training to project modalities into a shared semantic space and employs an audio-ultrasound encoder-decoder for ultrasound synthesis. We then present a speech enhancement network that enhances speech in the time-frequency domain and recovers the clean speech waveform via a neural vocoder. Comprehensive experiments show USpeech achieves remarkable performance using synthetic ultrasound data comparable to physical data, significantly outperforming state-of-the-art ultrasound-based speech enhancement baselines. USpeech is open-sourced at https://github.com/aiot-lab/USpeech/.","sentences":["Speech enhancement is crucial in human-computer interaction, especially for ubiquitous devices.","Ultrasound-based speech enhancement has emerged as an attractive choice because of its superior ubiquity and performance.","However, inevitable interference from unexpected and unintended sources during audio-ultrasound data acquisition makes existing solutions rely heavily on human effort for data collection and processing.","This leads to significant data scarcity that limits the full potential of ultrasound-based speech enhancement.","To address this, we propose USpeech, a cross-modal ultrasound synthesis framework for speech enhancement with minimal human effort.","At its core is a two-stage framework that establishes correspondence between visual and ultrasonic modalities by leveraging audible audio as a bridge.","This approach overcomes challenges from the lack of paired video-ultrasound datasets and the inherent heterogeneity between video and ultrasound data.","Our framework incorporates contrastive video-audio pre-training to project modalities into a shared semantic space and employs an audio-ultrasound encoder-decoder for ultrasound synthesis.","We then present a speech enhancement network that enhances speech in the time-frequency domain and recovers the clean speech waveform via a neural vocoder.","Comprehensive experiments show USpeech achieves remarkable performance using synthetic ultrasound data comparable to physical data, significantly outperforming state-of-the-art ultrasound-based speech enhancement baselines.","USpeech is open-sourced at https://github.com/aiot-lab/USpeech/."],"url":"http://arxiv.org/abs/2410.22076v1"}
{"created":"2024-10-29 13:53:09","title":"CHORDONOMICON: A Dataset of 666,000 Songs and their Chord Progressions","abstract":"Chord progressions encapsulate important information about music, pertaining to its structure and conveyed emotions. They serve as the backbone of musical composition, and in many cases, they are the sole information required for a musician to play along and follow the music. Despite their importance, chord progressions as a data domain remain underexplored. There is a lack of large-scale datasets suitable for deep learning applications, and limited research exploring chord progressions as an input modality. In this work, we present Chordonomicon, a dataset of over 666,000 songs and their chord progressions, annotated with structural parts, genre, and release date - created by scraping various sources of user-generated progressions and associated metadata. We demonstrate the practical utility of the Chordonomicon dataset for classification and generation tasks, and discuss its potential to provide valuable insights to the research community. Chord progressions are unique in their ability to be represented in multiple formats (e.g. text, graph) and the wealth of information chords convey in given contexts, such as their harmonic function . These characteristics make the Chordonomicon an ideal testbed for exploring advanced machine learning techniques, including transformers, graph machine learning, and hybrid systems that combine knowledge representation and machine learning.","sentences":["Chord progressions encapsulate important information about music, pertaining to its structure and conveyed emotions.","They serve as the backbone of musical composition, and in many cases, they are the sole information required for a musician to play along and follow the music.","Despite their importance, chord progressions as a data domain remain underexplored.","There is a lack of large-scale datasets suitable for deep learning applications, and limited research exploring chord progressions as an input modality.","In this work, we present Chordonomicon, a dataset of over 666,000 songs and their chord progressions, annotated with structural parts, genre, and release date - created by scraping various sources of user-generated progressions and associated metadata.","We demonstrate the practical utility of the Chordonomicon dataset for classification and generation tasks, and discuss its potential to provide valuable insights to the research community.","Chord progressions are unique in their ability to be represented in multiple formats (e.g. text, graph) and the wealth of information chords convey in given contexts, such as their harmonic function .","These characteristics make the Chordonomicon an ideal testbed for exploring advanced machine learning techniques, including transformers, graph machine learning, and hybrid systems that combine knowledge representation and machine learning."],"url":"http://arxiv.org/abs/2410.22046v1"}
{"created":"2024-10-29 13:46:52","title":"An LLM-based Simulation Framework for Embodied Conversational Agents in Psychological Counseling","abstract":"Simulation is crucial for validating algorithmic strategies in real-world scenarios. While LLM-based social simulation shows promise as a mainstream tool, simulating complex scenarios like psychological counseling remains challenging. We present ECAs (short for Embodied Conversational Agents), a framework for simulating psychological counseling clients' embodied memory, integrating embodied cognition and counseling theories. We formulate six design goals based on a comprehensive review of psychological counseling theories. Using LLMs, we expand real counseling case data into a nuanced embodied cognitive memory space and generate dialogues based on high-frequency counseling questions. We validate our framework using the D4 dataset, with evaluations by licensed counselors. Results show our approach significantly outperforms baselines in simulation authenticity and necessity. To demonstrate scalability, we created a public ECAs dataset through batch simulations. This research provides valuable insights for future social simulation studies in psychological counseling and Embodied Counseling Agents research.","sentences":["Simulation is crucial for validating algorithmic strategies in real-world scenarios.","While LLM-based social simulation shows promise as a mainstream tool, simulating complex scenarios like psychological counseling remains challenging.","We present ECAs (short for Embodied Conversational Agents), a framework for simulating psychological counseling clients' embodied memory, integrating embodied cognition and counseling theories.","We formulate six design goals based on a comprehensive review of psychological counseling theories.","Using LLMs, we expand real counseling case data into a nuanced embodied cognitive memory space and generate dialogues based on high-frequency counseling questions.","We validate our framework using the D4 dataset, with evaluations by licensed counselors.","Results show our approach significantly outperforms baselines in simulation authenticity and necessity.","To demonstrate scalability, we created a public ECAs dataset through batch simulations.","This research provides valuable insights for future social simulation studies in psychological counseling and Embodied Counseling Agents research."],"url":"http://arxiv.org/abs/2410.22041v1"}
{"created":"2024-10-29 12:45:10","title":"Critical Semantic Properties of Music Notation Datasets","abstract":"The semantics of notation systems can naturally be meta-modelled as a network of transformations, starting with the syntactic elements of the notation and ending with the parameters of an execution. In this context, a digital encoding format for music notation can be seen as selecting a subset of the data nodes of this network for storage, leaving others to evaluation. For such a selection, semantic properties are defined which have impact on the practical costs of maintenance, migration, extension, etc.","sentences":["The semantics of notation systems can naturally be meta-modelled as a network of transformations, starting with the syntactic elements of the notation and ending with the parameters of an execution.","In this context, a digital encoding format for music notation can be seen as selecting a subset of the data nodes of this network for storage, leaving others to evaluation.","For such a selection, semantic properties are defined which have impact on the practical costs of maintenance, migration, extension, etc."],"url":"http://arxiv.org/abs/2410.22002v1"}
{"created":"2024-10-29 12:25:00","title":"A Machine Learning-Based Secure Face Verification Scheme and Its Applications to Digital Surveillance","abstract":"Face verification is a well-known image analysis application and is widely used to recognize individuals in contemporary society. However, most real-world recognition systems ignore the importance of protecting the identity-sensitive facial images that are used for verification. To address this problem, we investigate how to implement a secure face verification system that protects the facial images from being imitated. In our work, we use the DeepID2 convolutional neural network to extract the features of a facial image and an EM algorithm to solve the facial verification problem. To maintain the privacy of facial images, we apply homomorphic encryption schemes to encrypt the facial data and compute the EM algorithm in the ciphertext domain. We develop three face verification systems for surveillance (or entrance) control of a local community based on three levels of privacy concerns. The associated timing performances are presented to demonstrate their feasibility for practical implementation.","sentences":["Face verification is a well-known image analysis application and is widely used to recognize individuals in contemporary society.","However, most real-world recognition systems ignore the importance of protecting the identity-sensitive facial images that are used for verification.","To address this problem, we investigate how to implement a secure face verification system that protects the facial images from being imitated.","In our work, we use the DeepID2 convolutional neural network to extract the features of a facial image and an EM algorithm to solve the facial verification problem.","To maintain the privacy of facial images, we apply homomorphic encryption schemes to encrypt the facial data and compute the EM algorithm in the ciphertext domain.","We develop three face verification systems for surveillance (or entrance) control of a local community based on three levels of privacy concerns.","The associated timing performances are presented to demonstrate their feasibility for practical implementation."],"url":"http://arxiv.org/abs/2410.21993v1"}
{"created":"2024-10-29 12:22:07","title":"From Explicit Rules to Implicit Reasoning in an Interpretable Violence Monitoring System","abstract":"Recently, research based on pre-trained models has demonstrated outstanding performance in violence surveillance tasks. However, these black-box systems face challenges regarding explainability during training and inference processes. An important question is how to incorporate explicit knowledge into these implicit models, thereby designing expert-driven and interpretable violence surveillance systems. This paper proposes a new paradigm for weakly supervised violence monitoring (WSVM) called Rule base Violence monitoring (RuleVM). The proposed RuleVM uses a dual-branch structure for different designs for images and text. One of the branches is called the implicit branch, which uses only visual features for coarse-grained binary classification. In this branch, image feature extraction is divided into two channels: one responsible for extracting scene frames and the other focusing on extracting actions. The other branch is called the explicit branch, which utilizes language-image alignment to perform fine-grained classification. For the language channel design in the explicit branch, the proposed RuleCLIP uses the state-of-the-art YOLO-World model to detect objects and actions in video frames, and association rules are identified through data mining methods as descriptions of the video. Leveraging the dual?branch architecture, RuleVM achieves interpretable coarse?grained and fine-grained violence surveillance. Extensive experiments were conducted on two commonly used benchmarks, and the results show that RuleCLIP achieved the best performance in both coarse-grained and fine-grained detection, significantly outperforming existing state-of-the-art methods. Moreover, interpretability experiments uncovered some interesting rules, such as the observation that as the number of people increases, the risk level of violent behavior also rises.","sentences":["Recently, research based on pre-trained models has demonstrated outstanding performance in violence surveillance tasks.","However, these black-box systems face challenges regarding explainability during training and inference processes.","An important question is how to incorporate explicit knowledge into these implicit models, thereby designing expert-driven and interpretable violence surveillance systems.","This paper proposes a new paradigm for weakly supervised violence monitoring (WSVM) called Rule base Violence monitoring (RuleVM).","The proposed RuleVM uses a dual-branch structure for different designs for images and text.","One of the branches is called the implicit branch, which uses only visual features for coarse-grained binary classification.","In this branch, image feature extraction is divided into two channels: one responsible for extracting scene frames and the other focusing on extracting actions.","The other branch is called the explicit branch, which utilizes language-image alignment to perform fine-grained classification.","For the language channel design in the explicit branch, the proposed RuleCLIP uses the state-of-the-art YOLO-World model to detect objects and actions in video frames, and association rules are identified through data mining methods as descriptions of the video.","Leveraging the dual?branch architecture, RuleVM achieves interpretable coarse?grained and fine-grained violence surveillance.","Extensive experiments were conducted on two commonly used benchmarks, and the results show that RuleCLIP achieved the best performance in both coarse-grained and fine-grained detection, significantly outperforming existing state-of-the-art methods.","Moreover, interpretability experiments uncovered some interesting rules, such as the observation that as the number of people increases, the risk level of violent behavior also rises."],"url":"http://arxiv.org/abs/2410.21991v1"}
{"created":"2024-10-29 12:21:23","title":"Understanding Code Understandability Improvements in Code Reviews","abstract":"Motivation: Code understandability is crucial in software development, as developers spend 58% to 70% of their time reading source code. Improving it can improve productivity and reduce maintenance costs. Problem: Experimental studies often identify factors influencing code understandability in controlled settings but overlook real-world influences like project culture, guidelines, and developers' backgrounds. Ignoring these factors may yield results with limited external validity. Objective: This study investigates how developers enhance code understandability through code review comments, assuming that code reviewers are specialists in code quality. Method and Results: We analyzed 2,401 code review comments from Java open-source projects on GitHub, finding that over 42% focus on improving code understandability. We further examined 385 comments specifically related to this aspect and identified eight categories of concerns, such as inadequate documentation and poor identifiers. Notably, 83.9% of suggestions for improvement were accepted and integrated, with fewer than 1% later reverted. We identified various types of patches that enhance understandability, from simple changes like removing unused code to context-dependent improvements such as optimizing method calls. Additionally, we evaluated four well-known linters for their ability to flag these issues, finding they cover less than 30%, although many could be easily added as new rules. Implications: Our findings encourage the development of tools to enhance code understandability, as accepted changes can serve as reliable training data for specialized machine-learning models. Our dataset supports this training and can inform the development of evidence-based code style guides. Data Availability: Our data is publicly available at https://codeupcrc.github.io.","sentences":["Motivation: Code understandability is crucial in software development, as developers spend 58% to 70% of their time reading source code.","Improving it can improve productivity and reduce maintenance costs.","Problem: Experimental studies often identify factors influencing code understandability in controlled settings but overlook real-world influences like project culture, guidelines, and developers' backgrounds.","Ignoring these factors may yield results with limited external validity.","Objective:","This study investigates how developers enhance code understandability through code review comments, assuming that code reviewers are specialists in code quality.","Method and Results: We analyzed 2,401 code review comments from Java open-source projects on GitHub, finding that over 42% focus on improving code understandability.","We further examined 385 comments specifically related to this aspect and identified eight categories of concerns, such as inadequate documentation and poor identifiers.","Notably, 83.9% of suggestions for improvement were accepted and integrated, with fewer than 1% later reverted.","We identified various types of patches that enhance understandability, from simple changes like removing unused code to context-dependent improvements such as optimizing method calls.","Additionally, we evaluated four well-known linters for their ability to flag these issues, finding they cover less than 30%, although many could be easily added as new rules.","Implications: Our findings encourage the development of tools to enhance code understandability, as accepted changes can serve as reliable training data for specialized machine-learning models.","Our dataset supports this training and can inform the development of evidence-based code style guides.","Data Availability: Our data is publicly available at https://codeupcrc.github.io."],"url":"http://arxiv.org/abs/2410.21990v1"}
{"created":"2024-10-29 12:06:24","title":"VaultFS: Write-once Software Support at the File System Level Against Ransomware Attacks","abstract":"The demand for data protection measures against unauthorized changes or deletions is steadily increasing. These measures are essential for maintaining the integrity and accessibility of data, effectively guarding against threats like ransomware attacks that focus on encrypting large volumes of stored data, as well as insider threats that involve tampering with or erasing system and access logs. Such protection measures have become crucial in today's landscape, and hardware-based solutions like Write-Once Read-Many (WORM) storage devices, have been put forth as viable options, which however impose hardware-level investments, and the impossibility to reuse the blocks of the storage devices after they have been written. In this article we propose VaultFS, a Linux-suited file system oriented to the maintenance of cold-data, namely data that are written using a common file system interface, are kept accessible, but are not modifiable, even by threads running with (effective)root-id. Essentially, these files are supported via the write-once semantic, and cannot be subject to the rewriting (or deletion) of their content up to the end of their (potentially infinite) protection life time. Hence they cannot be subject to ransomware attacks even under privilege escalation. This takes place with no need for any underlying WORM device -- since ValutFS is a pure software solution working with common read/write devices (e.g., hard disks and SSD). Also, VaultFS offers the possibility to protect the storage against Denial-of-Service (DOS) attacks, possibly caused by un-trusted applications that simply write on the file system to make its device blocks busy with non-removable content.","sentences":["The demand for data protection measures against unauthorized changes or deletions is steadily increasing.","These measures are essential for maintaining the integrity and accessibility of data, effectively guarding against threats like ransomware attacks that focus on encrypting large volumes of stored data, as well as insider threats that involve tampering with or erasing system and access logs.","Such protection measures have become crucial in today's landscape, and hardware-based solutions like Write-Once Read-Many (WORM) storage devices, have been put forth as viable options, which however impose hardware-level investments, and the impossibility to reuse the blocks of the storage devices after they have been written.","In this article we propose VaultFS, a Linux-suited file system oriented to the maintenance of cold-data, namely data that are written using a common file system interface, are kept accessible, but are not modifiable, even by threads running with (effective)root-id.","Essentially, these files are supported via the write-once semantic, and cannot be subject to the rewriting (or deletion) of their content up to the end of their (potentially infinite) protection life time.","Hence they cannot be subject to ransomware attacks even under privilege escalation.","This takes place with no need for any underlying WORM device -- since ValutFS is a pure software solution working with common read/write devices (e.g., hard disks and SSD).","Also, VaultFS offers the possibility to protect the storage against Denial-of-Service (DOS) attacks, possibly caused by un-trusted applications that simply write on the file system to make its device blocks busy with non-removable content."],"url":"http://arxiv.org/abs/2410.21979v1"}
{"created":"2024-10-29 11:53:18","title":"BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays","abstract":"Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.","sentences":["Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports.","MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples.","However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations.","This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark.","To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets.","Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively.","Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP.","Our code are available at https://github.com/yangzhou12/BenchX."],"url":"http://arxiv.org/abs/2410.21969v1"}
{"created":"2024-10-29 11:47:01","title":"SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types","abstract":"Ensuring the safety of large language model (LLM) applications is essential for developing trustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First, they focus solely on either discriminative or generative evaluation paradigms while ignoring their interconnection. Second, they rely on standardized inputs, overlooking the effects of widespread prompting techniques, such as system prompts, few-shot demonstrations, and chain-of-thought prompting. To overcome these issues, we developed SG-Bench, a novel benchmark to assess the generalization of LLM safety across various tasks and prompt types. This benchmark integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak on LLM safety. Our assessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the benchmark reveals that most LLMs perform worse on discriminative tasks than generative ones, and are highly susceptible to prompts, indicating poor generalization in safety alignment. We also explain these findings quantitatively and qualitatively to provide insights for future research.","sentences":["Ensuring the safety of large language model (LLM) applications is essential for developing trustworthy artificial intelligence.","Current LLM safety benchmarks have two limitations.","First, they focus solely on either discriminative or generative evaluation paradigms while ignoring their interconnection.","Second, they rely on standardized inputs, overlooking the effects of widespread prompting techniques, such as system prompts, few-shot demonstrations, and chain-of-thought prompting.","To overcome these issues, we developed SG-Bench, a novel benchmark to assess the generalization of LLM safety across various tasks and prompt types.","This benchmark integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak on LLM safety.","Our assessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the benchmark reveals that most LLMs perform worse on discriminative tasks than generative ones, and are highly susceptible to prompts, indicating poor generalization in safety alignment.","We also explain these findings quantitatively and qualitatively to provide insights for future research."],"url":"http://arxiv.org/abs/2410.21965v1"}
{"created":"2024-10-29 11:25:59","title":"Online Alignment and Addition in Multi-Term Floating-Point Adders","abstract":"Multi-term floating-point addition appears in vector dot-product computations, matrix multiplications, and other forms of floating-point data aggregation. A critical step in multi-term floating point addition is the alignment of fractions of the floating-point terms before adding them. Alignment is executed serially by identifying first the maximum of all exponents and then shifting the fraction of each term according to the difference of its exponent from the maximum one. Contrary to common practice, this work proposes a new online algorithm that splits the identification of the maximum exponent, the alignment shift for each fraction, and their addition to multiple fused incremental steps that can be computed in parallel. Each fused step is implemented by a new associative operator that allows the incremental alignment and addition for arbitrary number of operands. Experimental results show that employing the proposed align-and-add operators for the implementation of multi-term floating point adders can improve delay or save significant area and power. The achieved area and power savings range between 3%-23% and 4%-26%, respectively.","sentences":["Multi-term floating-point addition appears in vector dot-product computations, matrix multiplications, and other forms of floating-point data aggregation.","A critical step in multi-term floating point addition is the alignment of fractions of the floating-point terms before adding them.","Alignment is executed serially by identifying first the maximum of all exponents and then shifting the fraction of each term according to the difference of its exponent from the maximum one.","Contrary to common practice, this work proposes a new online algorithm that splits the identification of the maximum exponent, the alignment shift for each fraction, and their addition to multiple fused incremental steps that can be computed in parallel.","Each fused step is implemented by a new associative operator that allows the incremental alignment and addition for arbitrary number of operands.","Experimental results show that employing the proposed align-and-add operators for the implementation of multi-term floating point adders can improve delay or save significant area and power.","The achieved area and power savings range between 3%-23% and 4%-26%, respectively."],"url":"http://arxiv.org/abs/2410.21959v1"}
{"created":"2024-10-29 11:23:09","title":"Spatio-temporal Transformers for Action Unit Classification with Event Cameras","abstract":"Face analysis has been studied from different angles to infer emotion, poses, shapes, and landmarks. Traditionally RGB cameras are used, yet for fine-grained tasks standard sensors might not be up to the task due to their latency, making it impossible to record and detect micro-movements that carry a highly informative signal, which is necessary for inferring the true emotions of a subject. Event cameras have been increasingly gaining interest as a possible solution to this and similar high-frame rate tasks. We propose a novel spatiotemporal Vision Transformer model that uses Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA) to enhance the accuracy of Action Unit classification from event streams. We also address the lack of labeled event data in the literature, which can be considered one of the main causes of an existing gap between the maturity of RGB and neuromorphic vision models. Gathering data is harder in the event domain since it cannot be crawled from the web and labeling frames should take into account event aggregation rates and the fact that static parts might not be visible in certain frames. To this end, we present FACEMORPHIC, a temporally synchronized multimodal face dataset composed of RGB videos and event streams. The dataset is annotated at a video level with facial Action Units and contains streams collected with various possible applications, ranging from 3D shape estimation to lip-reading. We then show how temporal synchronization can allow effective neuromorphic face analysis without the need to manually annotate videos: we instead leverage cross-modal supervision bridging the domain gap by representing face shapes in a 3D space. Our proposed model outperforms baseline methods by effectively capturing spatial and temporal information, crucial for recognizing subtle facial micro-expressions.","sentences":["Face analysis has been studied from different angles to infer emotion, poses, shapes, and landmarks.","Traditionally RGB cameras are used, yet for fine-grained tasks standard sensors might not be up to the task due to their latency, making it impossible to record and detect micro-movements that carry a highly informative signal, which is necessary for inferring the true emotions of a subject.","Event cameras have been increasingly gaining interest as a possible solution to this and similar high-frame rate tasks.","We propose a novel spatiotemporal Vision Transformer model that uses Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA) to enhance the accuracy of Action Unit classification from event streams.","We also address the lack of labeled event data in the literature, which can be considered one of the main causes of an existing gap between the maturity of RGB and neuromorphic vision models.","Gathering data is harder in the event domain since it cannot be crawled from the web and labeling frames should take into account event aggregation rates and the fact that static parts might not be visible in certain frames.","To this end, we present FACEMORPHIC, a temporally synchronized multimodal face dataset composed of RGB videos and event streams.","The dataset is annotated at a video level with facial Action Units and contains streams collected with various possible applications, ranging from 3D shape estimation to lip-reading.","We then show how temporal synchronization can allow effective neuromorphic face analysis without the need to manually annotate videos: we instead leverage cross-modal supervision bridging the domain gap by representing face shapes in a 3D space.","Our proposed model outperforms baseline methods by effectively capturing spatial and temporal information, crucial for recognizing subtle facial micro-expressions."],"url":"http://arxiv.org/abs/2410.21958v1"}
{"created":"2024-10-29 11:18:04","title":"ActiveSplat: High-Fidelity Scene Reconstruction through Active Gaussian Splatting","abstract":"We propose ActiveSplat, an autonomous high-fidelity reconstruction system leveraging Gaussian splatting. Taking advantage of efficient and realistic rendering, the system establishes a unified framework for online mapping, viewpoint selection, and path planning. The key to ActiveSplat is a hybrid map representation that integrates both dense information about the environment and a sparse abstraction of the workspace. Therefore, the system leverages sparse topology for efficient viewpoint sampling and path planning, while exploiting view-dependent dense prediction for viewpoint selection, facilitating efficient decision-making with promising accuracy and completeness. A hierarchical planning strategy based on the topological map is adopted to mitigate repetitive trajectories and improve local granularity given limited budgets, ensuring high-fidelity reconstruction with photorealistic view synthesis. Extensive experiments and ablation studies validate the efficacy of the proposed method in terms of reconstruction accuracy, data coverage, and exploration efficiency. Project page: https://li-yuetao.github.io/ActiveSplat/.","sentences":["We propose ActiveSplat, an autonomous high-fidelity reconstruction system leveraging Gaussian splatting.","Taking advantage of efficient and realistic rendering, the system establishes a unified framework for online mapping, viewpoint selection, and path planning.","The key to ActiveSplat is a hybrid map representation that integrates both dense information about the environment and a sparse abstraction of the workspace.","Therefore, the system leverages sparse topology for efficient viewpoint sampling and path planning, while exploiting view-dependent dense prediction for viewpoint selection, facilitating efficient decision-making with promising accuracy and completeness.","A hierarchical planning strategy based on the topological map is adopted to mitigate repetitive trajectories and improve local granularity given limited budgets, ensuring high-fidelity reconstruction with photorealistic view synthesis.","Extensive experiments and ablation studies validate the efficacy of the proposed method in terms of reconstruction accuracy, data coverage, and exploration efficiency.","Project page: https://li-yuetao.github.io/ActiveSplat/."],"url":"http://arxiv.org/abs/2410.21955v1"}
{"created":"2024-10-29 11:12:44","title":"Sumsets, 3SUM, Subset Sum: Now for Real!","abstract":"We study a broad class of algorithmic problems with an \"additive flavor\" such as computing sumsets, 3SUM, Subset Sum and geometric pattern matching. Our starting point is that these problems can often be solved efficiently for integers, owed to the rich available tool set including bit-tricks, linear hashing, and the Fast Fourier Transform. However, for real numbers these tools are not available, leading to significant gaps in the best-known running times for integer inputs versus for real inputs. In this work our goal is to close this gap.   As our key contribution we design a new technique for computing real sumsets. It is based on a surprising blend of algebraic ideas (like Prony's method and coprime factorizations) with combinatorial tricks. We then apply our new algorithm to the aforementioned problems and successfully obtain, in all cases, equally fast algorithms for real inputs. Specifically, we replicate the running times of the following landmark results by randomized algorithms in the standard real RAM model:   - Sumsets: Given two sets $A,B$, their sumset $A+B=\\{a+b:a\\in A,b\\in B\\}$ can be computed in time $\\tilde O(|A+B|)$ [Cole, Hariharan; STOC'02].   - Geometric pattern matching: Given two sets $A,B$, we can test whether there is some shift such that $A+s\\subseteq B$ in time $\\tilde O(|A|+|B|)$ [Cardoze, Schulman; FOCS'98].   - 3SUM with preprocessing: We can preprocess three size-$n$ sets $A,B,C$ in time $\\tilde O(n^2)$ such that upon query of sets $A'\\subseteq A,B'\\subseteq B,C'\\subseteq C$, the 3SUM instance $(A',B',C')$ can be decided in time $\\tilde O(n^{13/7})$ [Chan, Lewenstein; STOC'15].   - Output-sensitive Subset Sum: Given a size-$n$ (multi-)set $X$ and a target $t$, we can compute the set of subset sums $\\{\\Sigma(X'):X'\\subseteq X,\\Sigma(X')\\leq t\\}$ in output-sensitive time $\\tilde O(n+\\mathrm{out}^{4/3})$ [Bringmann, Nakos; STOC'20].","sentences":["We study a broad class of algorithmic problems with an \"additive flavor\" such as computing sumsets, 3SUM, Subset Sum and geometric pattern matching.","Our starting point is that these problems can often be solved efficiently for integers, owed to the rich available tool set including bit-tricks, linear hashing, and the Fast Fourier Transform.","However, for real numbers these tools are not available, leading to significant gaps in the best-known running times for integer inputs versus for real inputs.","In this work our goal is to close this gap.   ","As our key contribution we design a new technique for computing real sumsets.","It is based on a surprising blend of algebraic ideas (like Prony's method and coprime factorizations) with combinatorial tricks.","We then apply our new algorithm to the aforementioned problems and successfully obtain, in all cases, equally fast algorithms for real inputs.","Specifically, we replicate the running times of the following landmark results by randomized algorithms in the standard real RAM model:   - Sumsets: Given two sets $A,B$, their sumset $A+B=\\{a+b:a\\in A,b\\in B\\}$ can be computed in time","$\\tilde O(|A+B|)$","[Cole, Hariharan; STOC'02].   - Geometric pattern matching: Given two sets $A,B$, we can test whether there is some shift such that $A+s\\subseteq B$ in time $\\tilde O(|A|+|B|)$ [Cardoze, Schulman; FOCS'98].   ","- 3SUM with preprocessing: We can preprocess three size-$n$ sets $A,B,C$ in time $\\tilde O(n^2)$ such that upon query of sets $A'\\subseteq A,B'\\subseteq B,C'\\subseteq C$, the 3SUM instance $(A',B',C')$ can be decided in time $\\tilde O(n^{13/7})$","[Chan, Lewenstein; STOC'15].   - Output-sensitive Subset Sum: Given a size-$n$ (multi-)set $X$ and a target $t$, we can compute the set of subset sums $\\{\\Sigma(X'):X'\\subseteq X,\\Sigma(X')\\leq t\\}$ in output-sensitive time $\\tilde O(n+\\mathrm{out}^{4/3})$","[Bringmann, Nakos; STOC'20]."],"url":"http://arxiv.org/abs/2410.21953v1"}
{"created":"2024-10-29 11:01:35","title":"Beating Bellman's Algorithm for Subset Sum","abstract":"Bellman's algorithm for Subset Sum is one of the earliest and simplest examples of dynamic programming, dating back to 1957. For a given set of $n$ integers $X$ and a target $t$, it computes the set of subset sums $\\mathcal S(X, t)$ (i.e., the set of integers $s \\in [0\\ldots t]$ for which there is a subset of $X$ summing to $s$) in time $O(|\\mathcal S(X, t)| \\cdot n)$. Since then, it has been an important question whether Bellman's seminal algorithm can be improved.   This question is addressed in many recent works. And yet, while some algorithms improve upon Bellman's algorithm in specific parameter regimes, such as Bringmann's $\\tilde O(t + n)$-time algorithm [SODA '17] and Bringmann and Nakos' $\\tilde O(|\\mathcal S(X, t)|^{4/3})$-time algorithm [STOC '20], none of the known algorithms beats Bellman's algorithm in all regimes. In particular, it remained open whether Subset Sum is in time $\\tilde O(|\\mathcal S(X, t)| \\cdot n^{1-\\epsilon})$ (for some $\\epsilon > 0$).   In this work we positively resolve this question and design an algorithm that outperforms Bellman's algorithm in all regimes. Our algorithm runs in time $\\tilde O(|\\mathcal S(X, t)| \\cdot \\sqrt{n})$, thus improving the time complexity by a factor of nearly $\\sqrt n$. Our key innovation is the use of a result from additive combinatorics, which has not been applied in an algorithmic context before and which we believe to be of further independent interest for algorithm design. To demonstrate the broader applicability of our approach, we extend our ideas to a variant of Subset Sum on vectors as well as to Unbounded Subset Sum.","sentences":["Bellman's algorithm for Subset Sum is one of the earliest and simplest examples of dynamic programming, dating back to 1957.","For a given set of $n$ integers $X$ and a target $t$, it computes the set of subset sums $\\mathcal S(X, t)$ (i.e., the set of integers $s \\in","[0\\ldots t]$ for which there is a subset of $X$ summing to $s$) in time $O(|\\mathcal S(X, t)| \\cdot n)$.","Since then, it has been an important question whether Bellman's seminal algorithm can be improved.   ","This question is addressed in many recent works.","And yet, while some algorithms improve upon Bellman's algorithm in specific parameter regimes, such as Bringmann's $\\tilde O(t","+","n)$-time algorithm","[SODA '17] and Bringmann and Nakos' $\\tilde O(|\\mathcal S(X, t)|^{4/3})$-time algorithm","[STOC '20], none of the known algorithms beats Bellman's algorithm in all regimes.","In particular, it remained open whether Subset Sum is in time $\\tilde O(|\\mathcal S(X, t)| \\cdot n^{1-\\epsilon})$ (for some $\\epsilon > 0$).   ","In this work we positively resolve this question and design an algorithm that outperforms Bellman's algorithm in all regimes.","Our algorithm runs in time $\\tilde O(|\\mathcal S(X, t)| \\cdot \\sqrt{n})$, thus improving the time complexity by a factor of nearly $\\sqrt n$. Our key innovation is the use of a result from additive combinatorics, which has not been applied in an algorithmic context before and which we believe to be of further independent interest for algorithm design.","To demonstrate the broader applicability of our approach, we extend our ideas to a variant of Subset Sum on vectors as well as to Unbounded Subset Sum."],"url":"http://arxiv.org/abs/2410.21942v1"}
{"created":"2024-10-29 10:57:03","title":"ReMix: Training Generalized Person Re-identification on a Mixture of Data","abstract":"Modern person re-identification (Re-ID) methods have a weak generalization ability and experience a major accuracy drop when capturing environments change. This is because existing multi-camera Re-ID datasets are limited in size and diversity, since such data is difficult to obtain. At the same time, enormous volumes of unlabeled single-camera records are available. Such data can be easily collected, and therefore, it is more diverse. Currently, single-camera data is used only for self-supervised pre-training of Re-ID methods. However, the diversity of single-camera data is suppressed by fine-tuning on limited multi-camera data after pre-training. In this paper, we propose ReMix, a generalized Re-ID method jointly trained on a mixture of limited labeled multi-camera and large unlabeled single-camera data. Effective training of our method is achieved through a novel data sampling strategy and new loss functions that are adapted for joint use with both types of data. Experiments show that ReMix has a high generalization ability and outperforms state-of-the-art methods in generalizable person Re-ID. To the best of our knowledge, this is the first work that explores joint training on a mixture of multi-camera and single-camera data in person Re-ID.","sentences":["Modern person re-identification (Re-ID) methods have a weak generalization ability and experience a major accuracy drop when capturing environments change.","This is because existing multi-camera Re-ID datasets are limited in size and diversity, since such data is difficult to obtain.","At the same time, enormous volumes of unlabeled single-camera records are available.","Such data can be easily collected, and therefore, it is more diverse.","Currently, single-camera data is used only for self-supervised pre-training of Re-ID methods.","However, the diversity of single-camera data is suppressed by fine-tuning on limited multi-camera data after pre-training.","In this paper, we propose ReMix, a generalized Re-ID method jointly trained on a mixture of limited labeled multi-camera and large unlabeled single-camera data.","Effective training of our method is achieved through a novel data sampling strategy and new loss functions that are adapted for joint use with both types of data.","Experiments show that ReMix has a high generalization ability and outperforms state-of-the-art methods in generalizable person Re-ID.","To the best of our knowledge, this is the first work that explores joint training on a mixture of multi-camera and single-camera data in person Re-ID."],"url":"http://arxiv.org/abs/2410.21938v1"}
{"created":"2024-10-29 10:52:43","title":"LogSHIELD: A Graph-based Real-time Anomaly Detection Framework using Frequency Analysis","abstract":"Anomaly-based cyber threat detection using deep learning is on a constant growth in popularity for novel cyber-attack detection and forensics. A robust, efficient, and real-time threat detector in a large-scale operational enterprise network requires high accuracy, high fidelity, and a high throughput model to detect malicious activities. Traditional anomaly-based detection models, however, suffer from high computational overhead and low detection accuracy, making them unsuitable for real-time threat detection. In this work, we propose LogSHIELD, a highly effective graph-based anomaly detection model in host data. We present a real-time threat detection approach using frequency-domain analysis of provenance graphs. To demonstrate the significance of graph-based frequency analysis we proposed two approaches. Approach-I uses a Graph Neural Network (GNN) LogGNN and approach-II performs frequency domain analysis on graph node samples for graph embedding. Both approaches use a statistical clustering algorithm for anomaly detection. The proposed models are evaluated using a large host log dataset consisting of 774M benign logs and 375K malware logs. LogSHIELD explores the provenance graph to extract contextual and causal relationships among logs, exposing abnormal activities. It can detect stealthy and sophisticated attacks with over 98% average AUC and F1 scores. It significantly improves throughput, achieves an average detection latency of 0.13 seconds, and outperforms state-of-the-art models in detection time.","sentences":["Anomaly-based cyber threat detection using deep learning is on a constant growth in popularity for novel cyber-attack detection and forensics.","A robust, efficient, and real-time threat detector in a large-scale operational enterprise network requires high accuracy, high fidelity, and a high throughput model to detect malicious activities.","Traditional anomaly-based detection models, however, suffer from high computational overhead and low detection accuracy, making them unsuitable for real-time threat detection.","In this work, we propose LogSHIELD, a highly effective graph-based anomaly detection model in host data.","We present a real-time threat detection approach using frequency-domain analysis of provenance graphs.","To demonstrate the significance of graph-based frequency analysis we proposed two approaches.","Approach-I uses a Graph Neural Network (GNN) LogGNN and approach-II performs frequency domain analysis on graph node samples for graph embedding.","Both approaches use a statistical clustering algorithm for anomaly detection.","The proposed models are evaluated using a large host log dataset consisting of 774M benign logs and 375K malware logs.","LogSHIELD explores the provenance graph to extract contextual and causal relationships among logs, exposing abnormal activities.","It can detect stealthy and sophisticated attacks with over 98% average AUC and F1 scores.","It significantly improves throughput, achieves an average detection latency of 0.13 seconds, and outperforms state-of-the-art models in detection time."],"url":"http://arxiv.org/abs/2410.21936v1"}
{"created":"2024-10-29 10:50:27","title":"Data streaming platform for crowd-sourced vehicle dataset generation","abstract":"Vehicles are sophisticated machines equipped with sensors that provide real-time data for onboard driving assistance systems. Due to the wide variety of traffic, road, and weather conditions, continuous system enhancements are essential. Connectivity allows vehicles to transmit previously unknown data, expanding datasets and accelerating the development of new data models. This enables faster identification and integration of novel data, improving system reliability and reducing time to market. Data Spaces aim to create a data-driven, interconnected, and innovative data economy, where edge and cloud infrastructures support a virtualised IoT platform that connects data sources and development servers. This paper proposes an edge-cloud data platform to connect car data producers with multiple and heterogeneous services, addressing key challenges in Data Spaces, such as data sovereignty, governance, interoperability, and privacy. The paper also evaluates the data platform's performance limits for text, image, and video data workloads, examines the impact of connectivity technologies, and assesses latencies. The results show that latencies drop to 33ms with 5G connectivity when pipelining data to consuming applications hosted at the edge, compared to around 77ms when crossing both edge and cloud processing infrastructures. The results offer guidance on the necessary processing assets to avoid bottlenecks in car data platforms.","sentences":["Vehicles are sophisticated machines equipped with sensors that provide real-time data for onboard driving assistance systems.","Due to the wide variety of traffic, road, and weather conditions, continuous system enhancements are essential.","Connectivity allows vehicles to transmit previously unknown data, expanding datasets and accelerating the development of new data models.","This enables faster identification and integration of novel data, improving system reliability and reducing time to market.","Data Spaces aim to create a data-driven, interconnected, and innovative data economy, where edge and cloud infrastructures support a virtualised IoT platform that connects data sources and development servers.","This paper proposes an edge-cloud data platform to connect car data producers with multiple and heterogeneous services, addressing key challenges in Data Spaces, such as data sovereignty, governance, interoperability, and privacy.","The paper also evaluates the data platform's performance limits for text, image, and video data workloads, examines the impact of connectivity technologies, and assesses latencies.","The results show that latencies drop to 33ms with 5G connectivity when pipelining data to consuming applications hosted at the edge, compared to around 77ms when crossing both edge and cloud processing infrastructures.","The results offer guidance on the necessary processing assets to avoid bottlenecks in car data platforms."],"url":"http://arxiv.org/abs/2410.21934v1"}
{"created":"2024-10-29 10:37:37","title":"Reliable Semantic Understanding for Real World Zero-shot Object Goal Navigation","abstract":"We introduce an innovative approach to advancing semantic understanding in zero-shot object goal navigation (ZS-OGN), enhancing the autonomy of robots in unfamiliar environments. Traditional reliance on labeled data has been a limitation for robotic adaptability, which we address by employing a dual-component framework that integrates a GLIP Vision Language Model for initial detection and an InstructionBLIP model for validation. This combination not only refines object and environmental recognition but also fortifies the semantic interpretation, pivotal for navigational decision-making. Our method, rigorously tested in both simulated and real-world settings, exhibits marked improvements in navigation precision and reliability.","sentences":["We introduce an innovative approach to advancing semantic understanding in zero-shot object goal navigation (ZS-OGN), enhancing the autonomy of robots in unfamiliar environments.","Traditional reliance on labeled data has been a limitation for robotic adaptability, which we address by employing a dual-component framework that integrates a GLIP Vision Language Model for initial detection and an InstructionBLIP model for validation.","This combination not only refines object and environmental recognition but also fortifies the semantic interpretation, pivotal for navigational decision-making.","Our method, rigorously tested in both simulated and real-world settings, exhibits marked improvements in navigation precision and reliability."],"url":"http://arxiv.org/abs/2410.21926v1"}
{"created":"2024-10-29 10:21:04","title":"On Eigenvector Approximation of Diagonalizable Random Matrices with Random Perturbations: Properties and Applications","abstract":"We extend the result on the top eigenvalue of the i.i.d.\\ matrix with fixed perturbations by Tao to random perturbations. In particular, we consider a setup that $\\mathbf{M}=\\mathbf{W}+\\lambda\\mathbf{u}\\mathbf{u}^*$ with $\\mathbf{W}$ drawn from a Ginibre Orthogonal Ensemble and the perturbation $\\mathbf{u}$ drawn uniformly from $\\mathcal{S}^{d-1}$. We provide several asymptotic properties about the eigenvalues and the top eigenvector of the random matrix, which can not be obtained trivially from the deterministic perturbation case.   We also apply our results to extend the work of Max Simchowitz, which provides an optimal lower bound for approximating the eigenspace of a symmetric matrix. We present a \\textit{query complexity} lower bound for approximating the eigenvector of any asymmetric but diagonalizable matrix $\\mathbf{M}$ corresponding to the largest eigenvalue. We show that for every $\\operatorname{gap}\\in (0,1/2]$ and large enough dimension $d$, there exists a random matrix $\\mathbf{M}$ with $\\operatorname{gap}(\\mathbf{M})=\\Omega(\\operatorname{gap})$, such that if a matrix-vector query product algorithm can identity a vector $\\hat{\\mathbf{v}}$ which satisfies $\\left\\|\\hat{\\mathbf{v}}-\\mathbf{v}_1(\\mathbf{M}) \\right\\|_2^2\\le \\operatorname{const}\\times \\operatorname{gap}$, it needs at least $\\mathcal{O}\\left(\\frac{\\log d}{\\operatorname{gap}}\\right)$ queries of matrix-vector products. In the inverse polynomial accuracy regime where $\\epsilon \\ge \\frac{1}{\\operatorname{poly}(d)}$, the complexity matches the upper bounds $\\mathcal{O}\\left(\\frac{\\log(d/\\epsilon)}{\\operatorname{gap}}\\right)$, which can be obtained via the power method. As far as we know, it is the first lower bound for computing the eigenvector of an asymmetric matrix, which is far more complicated than in the symmetric case.","sentences":["We extend the result on the top eigenvalue of the i.i.d.\\ matrix with fixed perturbations by Tao to random perturbations.","In particular, we consider a setup that $\\mathbf{M}=\\mathbf{W}+\\lambda\\mathbf{u}\\mathbf{u}^*$ with $\\mathbf{W}$ drawn from a Ginibre Orthogonal Ensemble and the perturbation $\\mathbf{u}$ drawn uniformly from $\\mathcal{S}^{d-1}$. We provide several asymptotic properties about the eigenvalues and the top eigenvector of the random matrix, which can not be obtained trivially from the deterministic perturbation case.   ","We also apply our results to extend the work of Max Simchowitz, which provides an optimal lower bound for approximating the eigenspace of a symmetric matrix.","We present a \\textit{query complexity} lower bound for approximating the eigenvector of any asymmetric but diagonalizable matrix $\\mathbf{M}$ corresponding to the largest eigenvalue.","We show that for every $\\operatorname{gap}\\in (0,1/2]$ and large enough dimension $d$, there exists a random matrix $\\mathbf{M}$ with $\\operatorname{gap}(\\mathbf{M})=\\Omega(\\operatorname{gap})$, such that if a matrix-vector query product algorithm can identity a vector $\\hat{\\mathbf{v}}$ which satisfies $\\left\\|\\hat{\\mathbf{v}}-\\mathbf{v}_1(\\mathbf{M})","\\right\\|_2^2\\le \\operatorname{const}\\times \\operatorname{gap}$, it needs at least $\\mathcal{O}\\left(\\frac{\\log d}{\\operatorname{gap}}\\right)$ queries of matrix-vector products.","In the inverse polynomial accuracy regime where $\\epsilon \\ge \\frac{1}{\\operatorname{poly}(d)}$, the complexity matches the upper bounds $\\mathcal{O}\\left(\\frac{\\log(d/\\epsilon)}{\\operatorname{gap}}\\right)$, which can be obtained via the power method.","As far as we know, it is the first lower bound for computing the eigenvector of an asymmetric matrix, which is far more complicated than in the symmetric case."],"url":"http://arxiv.org/abs/2410.21919v1"}
{"created":"2024-10-29 10:15:26","title":"Cognitive Semantic Augmentation LEO Satellite Networks for Earth Observation","abstract":"Earth observation (EO) systems are essential for mapping, catastrophe monitoring, and resource management, but they have trouble processing and sending large amounts of EO data efficiently, especially for specialized applications like agriculture and real-time disaster response. This paper presents a novel framework for semantic communication in EO satellite networks, aimed at enhancing data transmission efficiency and system performance through cognitive processing techniques. The proposed system leverages Discrete Task-Oriented Joint Source-Channel Coding (DT-JSCC) and Semantic Data Augmentation (SA) integrate cognitive semantic processing with inter-satellite links, enabling efficient analysis and transmission of multispectral imagery for improved object detection, pattern recognition, and real-time decision-making. Cognitive Semantic Augmentation (CSA) is introduced to enhance a system's capability to process and transmit semantic information, improving feature prioritization, consistency, and adaptation to changing communication and application needs. The end-to-end architecture is designed for next-generation satellite networks, such as those supporting 6G, demonstrating significant improvements in fewer communication rounds and better accuracy over federated learning.","sentences":["Earth observation (EO) systems are essential for mapping, catastrophe monitoring, and resource management, but they have trouble processing and sending large amounts of EO data efficiently, especially for specialized applications like agriculture and real-time disaster response.","This paper presents a novel framework for semantic communication in EO satellite networks, aimed at enhancing data transmission efficiency and system performance through cognitive processing techniques.","The proposed system leverages Discrete Task-Oriented Joint Source-Channel Coding (DT-JSCC) and Semantic Data Augmentation (SA) integrate cognitive semantic processing with inter-satellite links, enabling efficient analysis and transmission of multispectral imagery for improved object detection, pattern recognition, and real-time decision-making.","Cognitive Semantic Augmentation (CSA) is introduced to enhance a system's capability to process and transmit semantic information, improving feature prioritization, consistency, and adaptation to changing communication and application needs.","The end-to-end architecture is designed for next-generation satellite networks, such as those supporting 6G, demonstrating significant improvements in fewer communication rounds and better accuracy over federated learning."],"url":"http://arxiv.org/abs/2410.21916v1"}
{"created":"2024-10-29 10:01:40","title":"SceneGenAgent: Precise Industrial Scene Generation with Coding Agent","abstract":"The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at https://github.com/THUDM/SceneGenAgent .","sentences":["The modeling of industrial scenes is essential for simulations in industrial manufacturing.","While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement.","To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code.","SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios.","Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements.","To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent.","Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o.","Our code and data are available at https://github.com/THUDM/SceneGenAgent ."],"url":"http://arxiv.org/abs/2410.21909v1"}
{"created":"2024-10-29 09:42:07","title":"Semi-Supervised Self-Learning Enhanced Music Emotion Recognition","abstract":"Music emotion recognition (MER) aims to identify the emotions conveyed in a given musical piece. But currently in the field of MER, the available public datasets have limited sample sizes. Recently, segment-based methods for emotion-related tasks have been proposed, which train backbone networks on shorter segments instead of entire audio clips, thereby naturally augmenting training samples without requiring additional resources. Then, the predicted segment-level results are aggregated to obtain the entire song prediction. The most commonly used method is that segment inherits the label of the clip containing it, but music emotion is not constant during the whole clip. Doing so will introduce label noise and make the training overfit easily. To handle the noisy label issue, we propose a semi-supervised self-learning (SSSL) method, which can differentiate between samples with correct and incorrect labels in a self-learning manner, thus effectively utilizing the augmented segment-level data. Experiments on three public emotional datasets demonstrate that the proposed method can achieve better or comparable performance.","sentences":["Music emotion recognition (MER) aims to identify the emotions conveyed in a given musical piece.","But currently in the field of MER, the available public datasets have limited sample sizes.","Recently, segment-based methods for emotion-related tasks have been proposed, which train backbone networks on shorter segments instead of entire audio clips, thereby naturally augmenting training samples without requiring additional resources.","Then, the predicted segment-level results are aggregated to obtain the entire song prediction.","The most commonly used method is that segment inherits the label of the clip containing it, but music emotion is not constant during the whole clip.","Doing so will introduce label noise and make the training overfit easily.","To handle the noisy label issue, we propose a semi-supervised self-learning (SSSL) method, which can differentiate between samples with correct and incorrect labels in a self-learning manner, thus effectively utilizing the augmented segment-level data.","Experiments on three public emotional datasets demonstrate that the proposed method can achieve better or comparable performance."],"url":"http://arxiv.org/abs/2410.21897v1"}
{"created":"2024-10-29 09:39:54","title":"Evaluating K-Fold Cross Validation for Transformer Based Symbolic Regression Models","abstract":"Symbolic Regression remains an NP-Hard problem, with extensive research focusing on AI models for this task. Transformer models have shown promise in Symbolic Regression, but performance suffers with smaller datasets. We propose applying k-fold cross-validation to a transformer-based symbolic regression model trained on a significantly reduced dataset (15,000 data points, down from 500,000). This technique partitions the training data into multiple subsets (folds), iteratively training on some while validating on others. Our aim is to provide an estimate of model generalization and mitigate overfitting issues associated with smaller datasets. Results show that this process improves the model's output consistency and generalization by a relative improvement in validation loss of 53.31%. Potentially enabling more efficient and accessible symbolic regression in resource-constrained environments.","sentences":["Symbolic Regression remains an NP-Hard problem, with extensive research focusing on AI models for this task.","Transformer models have shown promise in Symbolic Regression, but performance suffers with smaller datasets.","We propose applying k-fold cross-validation to a transformer-based symbolic regression model trained on a significantly reduced dataset (15,000 data points, down from 500,000).","This technique partitions the training data into multiple subsets (folds), iteratively training on some while validating on others.","Our aim is to provide an estimate of model generalization and mitigate overfitting issues associated with smaller datasets.","Results show that this process improves the model's output consistency and generalization by a relative improvement in validation loss of 53.31%.","Potentially enabling more efficient and accessible symbolic regression in resource-constrained environments."],"url":"http://arxiv.org/abs/2410.21896v1"}
{"created":"2024-10-29 09:36:59","title":"Guided Diffusion-based Counterfactual Augmentation for Robust Session-based Recommendation","abstract":"Session-based recommendation (SR) models aim to recommend top-K items to a user, based on the user's behaviour during the current session. Several SR models are proposed in the literature, however,concerns have been raised about their susceptibility to inherent biases in the training data (observed data) such as popularity bias. SR models when trained on the biased training data may encounter performance challenges on out-of-distribution data in real-world scenarios. One way to mitigate popularity bias is counterfactual data augmentation. Compared to prior works that rely on generating data using SR models, we focus on utilizing the capabilities of state-of-the art diffusion models for generating counterfactual data. We propose a guided diffusion-based counterfactual augmentation framework for SR. Through a combination of offline and online experiments on a real-world and simulated dataset, respectively, we show that our approach performs significantly better than the baseline SR models and other state-of-the art augmentation frameworks. More importantly, our framework shows significant improvement on less popular target items, by achieving up to 20% gain in Recall and 13% gain in CTR on real-world and simulated datasets,respectively.","sentences":["Session-based recommendation (SR) models aim to recommend top-K items to a user, based on the user's behaviour during the current session.","Several SR models are proposed in the literature, however,concerns have been raised about their susceptibility to inherent biases in the training data (observed data) such as popularity bias.","SR models when trained on the biased training data may encounter performance challenges on out-of-distribution data in real-world scenarios.","One way to mitigate popularity bias is counterfactual data augmentation.","Compared to prior works that rely on generating data using SR models, we focus on utilizing the capabilities of state-of-the art diffusion models for generating counterfactual data.","We propose a guided diffusion-based counterfactual augmentation framework for SR.","Through a combination of offline and online experiments on a real-world and simulated dataset, respectively, we show that our approach performs significantly better than the baseline SR models and other state-of-the art augmentation frameworks.","More importantly, our framework shows significant improvement on less popular target items, by achieving up to 20% gain in Recall and 13% gain in CTR on real-world and simulated datasets,respectively."],"url":"http://arxiv.org/abs/2410.21892v1"}
{"created":"2024-10-29 09:09:08","title":"SCGNet-Stacked Convolution with Gated Recurrent Unit Network for Cyber Network Intrusion Detection and Intrusion Type Classification","abstract":"Intrusion detection system (IDS) is a piece of hardware or software that looks for malicious activity or policy violations in a network. It looks for malicious activity or security flaws on a network or system. IDS protects hosts or networks by looking for indications of known attacks or deviations from normal behavior (Network-based intrusion detection system, or NIDS for short). Due to the rapidly increasing amount of network data, traditional intrusion detection systems (IDSs) are far from being able to quickly and efficiently identify complex and varied network attacks, especially those linked to low-frequency attacks. The SCGNet (Stacked Convolution with Gated Recurrent Unit Network) is a novel deep learning architecture that we propose in this study. It exhibits promising results on the NSL-KDD dataset in both task, network attack detection, and attack type classification with 99.76% and 98.92% accuracy, respectively. We have also introduced a general data preprocessing pipeline that is easily applicable to other similar datasets. We have also experimented with conventional machine-learning techniques to evaluate the performance of the data processing pipeline.","sentences":["Intrusion detection system (IDS) is a piece of hardware or software that looks for malicious activity or policy violations in a network.","It looks for malicious activity or security flaws on a network or system.","IDS protects hosts or networks by looking for indications of known attacks or deviations from normal behavior (Network-based intrusion detection system, or NIDS for short).","Due to the rapidly increasing amount of network data, traditional intrusion detection systems (IDSs) are far from being able to quickly and efficiently identify complex and varied network attacks, especially those linked to low-frequency attacks.","The SCGNet (Stacked Convolution with Gated Recurrent Unit Network) is a novel deep learning architecture that we propose in this study.","It exhibits promising results on the NSL-KDD dataset in both task, network attack detection, and attack type classification with 99.76% and 98.92% accuracy, respectively.","We have also introduced a general data preprocessing pipeline that is easily applicable to other similar datasets.","We have also experimented with conventional machine-learning techniques to evaluate the performance of the data processing pipeline."],"url":"http://arxiv.org/abs/2410.21873v1"}
{"created":"2024-10-29 09:08:57","title":"Advancing Efficient Brain Tumor Multi-Class Classification -- New Insights from the Vision Mamba Model in Transfer Learning","abstract":"Early and accurate diagnosis of brain tumors is crucial for improving patient survival rates. However, the detection and classification of brain tumors are challenging due to their diverse types and complex morphological characteristics. This study investigates the application of pre-trained models for brain tumor classification, with a particular focus on deploying the Mamba model. We fine-tuned several mainstream transfer learning models and applied them to the multi-class classification of brain tumors. By comparing these models to those trained from scratch, we demonstrated the significant advantages of transfer learning, especially in the medical imaging field, where annotated data is often limited. Notably, we introduced the Vision Mamba (Vim), a novel network architecture, and applied it for the first time in brain tumor classification, achieving exceptional classification accuracy. Experimental results indicate that the Vim model achieved 100% classification accuracy on an independent test set, emphasizing its potential for tumor classification tasks. These findings underscore the effectiveness of transfer learning in brain tumor classification and reveal that, compared to existing state-of-the-art models, the Vim model is lightweight, efficient, and highly accurate, offering a new perspective for clinical applications. Furthermore, the framework proposed in this study for brain tumor classification, based on transfer learning and the Vision Mamba model, is broadly applicable to other medical imaging classification problems.","sentences":["Early and accurate diagnosis of brain tumors is crucial for improving patient survival rates.","However, the detection and classification of brain tumors are challenging due to their diverse types and complex morphological characteristics.","This study investigates the application of pre-trained models for brain tumor classification, with a particular focus on deploying the Mamba model.","We fine-tuned several mainstream transfer learning models and applied them to the multi-class classification of brain tumors.","By comparing these models to those trained from scratch, we demonstrated the significant advantages of transfer learning, especially in the medical imaging field, where annotated data is often limited.","Notably, we introduced the Vision Mamba (Vim), a novel network architecture, and applied it for the first time in brain tumor classification, achieving exceptional classification accuracy.","Experimental results indicate that the Vim model achieved 100% classification accuracy on an independent test set, emphasizing its potential for tumor classification tasks.","These findings underscore the effectiveness of transfer learning in brain tumor classification and reveal that, compared to existing state-of-the-art models, the Vim model is lightweight, efficient, and highly accurate, offering a new perspective for clinical applications.","Furthermore, the framework proposed in this study for brain tumor classification, based on transfer learning and the Vision Mamba model, is broadly applicable to other medical imaging classification problems."],"url":"http://arxiv.org/abs/2410.21872v1"}
{"created":"2024-10-29 09:03:57","title":"Cross-Entropy Is All You Need To Invert the Data Generating Process","abstract":"Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion. Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process. We extend these identifiability results to parametric instance discrimination, then show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization. We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation. We corroborate our theoretical contribution with a series of empirical studies. First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors. Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations. Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation. Together, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks. This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised deep learning.","sentences":["Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive.","Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion.","Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process.","We extend these identifiability results to parametric instance discrimination, then show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization.","We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation.","We corroborate our theoretical contribution with a series of empirical studies.","First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors.","Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations.","Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation.","Together, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks.","This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised deep learning."],"url":"http://arxiv.org/abs/2410.21869v1"}
{"created":"2024-10-29 09:02:37","title":"Improving In-Context Learning with Small Language Model Ensembles","abstract":"Large language models (LLMs) have shown impressive capabilities across various tasks, but their performance on domain-specific tasks remains limited. While methods like retrieval augmented generation and fine-tuning can help to address this, they require significant resources. In-context learning (ICL) is a cheap and efficient alternative but cannot match the accuracies of advanced methods. We present Ensemble SuperICL, a novel approach that enhances ICL by leveraging the expertise of multiple fine-tuned small language models (SLMs). Ensemble SuperICL achieves state of the art (SoTA) results on several natural language understanding benchmarks. Additionally, we test it on a medical-domain labelling task and showcase its practicality by using off-the-shelf SLMs fine-tuned on a general language task, achieving superior accuracy in large-scale data labelling compared to all baselines. Finally, we conduct an ablation study and sensitivity analyses to elucidate the underlying mechanism of Ensemble SuperICL. Our research contributes to the growing demand for efficient domain specialisation methods in LLMs, offering a cheap and effective method for practitioners.","sentences":["Large language models (LLMs) have shown impressive capabilities across various tasks, but their performance on domain-specific tasks remains limited.","While methods like retrieval augmented generation and fine-tuning can help to address this, they require significant resources.","In-context learning (ICL) is a cheap and efficient alternative but cannot match the accuracies of advanced methods.","We present Ensemble SuperICL, a novel approach that enhances ICL by leveraging the expertise of multiple fine-tuned small language models (SLMs).","Ensemble SuperICL achieves state of the art (SoTA) results on several natural language understanding benchmarks.","Additionally, we test it on a medical-domain labelling task and showcase its practicality by using off-the-shelf SLMs fine-tuned on a general language task, achieving superior accuracy in large-scale data labelling compared to all baselines.","Finally, we conduct an ablation study and sensitivity analyses to elucidate the underlying mechanism of Ensemble SuperICL.","Our research contributes to the growing demand for efficient domain specialisation methods in LLMs, offering a cheap and effective method for practitioners."],"url":"http://arxiv.org/abs/2410.21868v1"}
{"created":"2024-10-29 09:00:01","title":"Token-based identity management in the distributed cloud","abstract":"The immense shift to cloud computing has brought changes in security and privacy requirements, impacting critical Identity Management services. Currently, many IdM systems and solutions are accessible as cloud services, delivering identity services for applications in closed domains and the public cloud. This research paper centres on identity management in distributed environments, emphasising the importance of robust up to date authorisation mechanisms. The paper concentrates on implementing robust security paradigms to minimise communication overhead among services while preserving privacy and access control. The key contribution focuses on solving the problem of restricted access to resources in cases when the authentication token is still valid, but permissions are updated. The proposed solution incorporates an Identity and Access Management server as a component that authenticates all external requests. The IAM server key responsibilities include maintaining user data, assigning privileges within the system, and authorisation. Furthermore, it empowers users by offering an Application Programming Interface for managing users and their rights within the same organisation, providing finer granularity in authorisation. The IAM server has been integrated with a configuration dissemination tool designed as a distributed cloud infrastructure to evaluate the solution.","sentences":["The immense shift to cloud computing has brought changes in security and privacy requirements, impacting critical Identity Management services.","Currently, many IdM systems and solutions are accessible as cloud services, delivering identity services for applications in closed domains and the public cloud.","This research paper centres on identity management in distributed environments, emphasising the importance of robust up to date authorisation mechanisms.","The paper concentrates on implementing robust security paradigms to minimise communication overhead among services while preserving privacy and access control.","The key contribution focuses on solving the problem of restricted access to resources in cases when the authentication token is still valid, but permissions are updated.","The proposed solution incorporates an Identity and Access Management server as a component that authenticates all external requests.","The IAM server key responsibilities include maintaining user data, assigning privileges within the system, and authorisation.","Furthermore, it empowers users by offering an Application Programming Interface for managing users and their rights within the same organisation, providing finer granularity in authorisation.","The IAM server has been integrated with a configuration dissemination tool designed as a distributed cloud infrastructure to evaluate the solution."],"url":"http://arxiv.org/abs/2410.21865v1"}
{"created":"2024-10-29 08:36:23","title":"Micro-Structures Graph-Based Point Cloud Registration for Balancing Efficiency and Accuracy","abstract":"Point Cloud Registration (PCR) is a fundamental and significant issue in photogrammetry and remote sensing, aiming to seek the optimal rigid transformation between sets of points. Achieving efficient and precise PCR poses a considerable challenge. We propose a novel micro-structures graph-based global point cloud registration method. The overall method is comprised of two stages. 1) Coarse registration (CR): We develop a graph incorporating micro-structures, employing an efficient graph-based hierarchical strategy to remove outliers for obtaining the maximal consensus set. We propose a robust GNC-Welsch estimator for optimization derived from a robust estimator to the outlier process in the Lie algebra space, achieving fast and robust alignment. 2) Fine registration (FR): To refine local alignment further, we use the octree approach to adaptive search plane features in the micro-structures. By minimizing the distance from the point-to-plane, we can obtain a more precise local alignment, and the process will also be addressed effectively by being treated as a planar adjustment algorithm combined with Anderson accelerated optimization (PA-AA). After extensive experiments on real data, our proposed method performs well on the 3DMatch and ETH datasets compared to the most advanced methods, achieving higher accuracy metrics and reducing the time cost by at least one-third.","sentences":["Point Cloud Registration (PCR) is a fundamental and significant issue in photogrammetry and remote sensing, aiming to seek the optimal rigid transformation between sets of points.","Achieving efficient and precise PCR poses a considerable challenge.","We propose a novel micro-structures graph-based global point cloud registration method.","The overall method is comprised of two stages.","1) Coarse registration (CR): We develop a graph incorporating micro-structures, employing an efficient graph-based hierarchical strategy to remove outliers for obtaining the maximal consensus set.","We propose a robust GNC-Welsch estimator for optimization derived from a robust estimator to the outlier process in the Lie algebra space, achieving fast and robust alignment.","2) Fine registration (FR): To refine local alignment further, we use the octree approach to adaptive search plane features in the micro-structures.","By minimizing the distance from the point-to-plane, we can obtain a more precise local alignment, and the process will also be addressed effectively by being treated as a planar adjustment algorithm combined with Anderson accelerated optimization (PA-AA).","After extensive experiments on real data, our proposed method performs well on the 3DMatch and ETH datasets compared to the most advanced methods, achieving higher accuracy metrics and reducing the time cost by at least one-third."],"url":"http://arxiv.org/abs/2410.21857v1"}
{"created":"2024-10-29 08:28:23","title":"Learning Infinitesimal Generators of Continuous Symmetries from Data","abstract":"Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages. Our codes are available at \\url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.","sentences":["Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models.","When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies.","Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous.","To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data.","However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations.","In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators.","Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators.","To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task.","The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data.","We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages.","Our codes are available at \\url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}."],"url":"http://arxiv.org/abs/2410.21853v1"}
{"created":"2024-10-29 08:17:31","title":"Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone Meeting Transcription","abstract":"Distant-microphone meeting transcription is a challenging task. State-of-the-art end-to-end speaker-attributed automatic speech recognition (SA-ASR) architectures lack a multichannel noise and reverberation reduction front-end, which limits their performance. In this paper, we introduce a joint beamforming and SA-ASR approach for real meeting transcription. We first describe a data alignment and augmentation method to pretrain a neural beamformer on real meeting data. We then compare fixed, hybrid, and fully neural beamformers as front-ends to the SA-ASR model. Finally, we jointly optimize the fully neural beamformer and the SA-ASR model. Experiments on the real AMI corpus show that,while state-of-the-art multi-frame cross-channel attention based channel fusion fails to improve ASR performance, fine-tuning SA-ASR on the fixed beamformer's output and jointly fine-tuning SA-ASR with the neural beamformer reduce the word error rate by 8% and 9% relative, respectively.","sentences":["Distant-microphone meeting transcription is a challenging task.","State-of-the-art end-to-end speaker-attributed automatic speech recognition (SA-ASR) architectures lack a multichannel noise and reverberation reduction front-end, which limits their performance.","In this paper, we introduce a joint beamforming and SA-ASR approach for real meeting transcription.","We first describe a data alignment and augmentation method to pretrain a neural beamformer on real meeting data.","We then compare fixed, hybrid, and fully neural beamformers as front-ends to the SA-ASR model.","Finally, we jointly optimize the fully neural beamformer and the SA-ASR model.","Experiments on the real AMI corpus show that,while state-of-the-art multi-frame cross-channel attention based channel fusion fails to improve ASR performance, fine-tuning SA-ASR on the fixed beamformer's output and jointly fine-tuning SA-ASR with the neural beamformer reduce the word error rate by 8% and 9% relative, respectively."],"url":"http://arxiv.org/abs/2410.21849v1"}
{"created":"2024-10-29 07:56:04","title":"Enhanced Survival Prediction in Head and Neck Cancer Using Convolutional Block Attention and Multimodal Data Fusion","abstract":"Accurate survival prediction in head and neck cancer (HNC) is essential for guiding clinical decision-making and optimizing treatment strategies. Traditional models, such as Cox proportional hazards, have been widely used but are limited in their ability to handle complex multi-modal data. This paper proposes a deep learning-based approach leveraging CT and PET imaging modalities to predict survival outcomes in HNC patients. Our method integrates feature extraction with a Convolutional Block Attention Module (CBAM) and a multi-modal data fusion layer that combines imaging data to generate a compact feature representation. The final prediction is achieved through a fully parametric discrete-time survival model, allowing for flexible hazard functions that overcome the limitations of traditional survival models. We evaluated our approach using the HECKTOR and HEAD-NECK-RADIOMICS- HN1 datasets, demonstrating its superior performance compared to conconventional statistical and machine learning models. The results indicate that our deep learning model significantly improves survival prediction accuracy, offering a robust tool for personalized treatment planning in HNC","sentences":["Accurate survival prediction in head and neck cancer (HNC) is essential for guiding clinical decision-making and optimizing treatment strategies.","Traditional models, such as Cox proportional hazards, have been widely used but are limited in their ability to handle complex multi-modal data.","This paper proposes a deep learning-based approach leveraging CT and PET imaging modalities to predict survival outcomes in HNC patients.","Our method integrates feature extraction with a Convolutional Block Attention Module (CBAM) and a multi-modal data fusion layer that combines imaging data to generate a compact feature representation.","The final prediction is achieved through a fully parametric discrete-time survival model, allowing for flexible hazard functions that overcome the limitations of traditional survival models.","We evaluated our approach using the HECKTOR and HEAD-NECK-RADIOMICS- HN1 datasets, demonstrating its superior performance compared to conconventional statistical and machine learning models.","The results indicate that our deep learning model significantly improves survival prediction accuracy, offering a robust tool for personalized treatment planning in HNC"],"url":"http://arxiv.org/abs/2410.21831v1"}
{"created":"2024-10-29 07:48:52","title":"Volumetric Conditioning Module to Control Pretrained Diffusion Models for 3D Medical Images","abstract":"Spatial control methods using additional modules on pretrained diffusion models have gained attention for enabling conditional generation in natural images. These methods guide the generation process with new conditions while leveraging the capabilities of large models. They could be beneficial as training strategies in the context of 3D medical imaging, where training a diffusion model from scratch is challenging due to high computational costs and data scarcity. However, the potential application of spatial control methods with additional modules to 3D medical images has not yet been explored. In this paper, we present a tailored spatial control method for 3D medical images with a novel lightweight module, Volumetric Conditioning Module (VCM). Our VCM employs an asymmetric U-Net architecture to effectively encode complex information from various levels of 3D conditions, providing detailed guidance in image synthesis. To examine the applicability of spatial control methods and the effectiveness of VCM for 3D medical data, we conduct experiments under single- and multimodal conditions scenarios across a wide range of dataset sizes, from extremely small datasets with 10 samples to large datasets with 500 samples. The experimental results show that the VCM is effective for conditional generation and efficient in terms of requiring less training data and computational resources. We further investigate the potential applications for our spatial control method through axial super-resolution for medical images. Our code is available at \\url{https://github.com/Ahn-Ssu/VCM}","sentences":["Spatial control methods using additional modules on pretrained diffusion models have gained attention for enabling conditional generation in natural images.","These methods guide the generation process with new conditions while leveraging the capabilities of large models.","They could be beneficial as training strategies in the context of 3D medical imaging, where training a diffusion model from scratch is challenging due to high computational costs and data scarcity.","However, the potential application of spatial control methods with additional modules to 3D medical images has not yet been explored.","In this paper, we present a tailored spatial control method for 3D medical images with a novel lightweight module, Volumetric Conditioning Module (VCM).","Our VCM employs an asymmetric U-Net architecture to effectively encode complex information from various levels of 3D conditions, providing detailed guidance in image synthesis.","To examine the applicability of spatial control methods and the effectiveness of VCM for 3D medical data, we conduct experiments under single- and multimodal conditions scenarios across a wide range of dataset sizes, from extremely small datasets with 10 samples to large datasets with 500 samples.","The experimental results show that the VCM is effective for conditional generation and efficient in terms of requiring less training data and computational resources.","We further investigate the potential applications for our spatial control method through axial super-resolution for medical images.","Our code is available at \\url{https://github.com/Ahn-Ssu/VCM}"],"url":"http://arxiv.org/abs/2410.21826v1"}
{"created":"2024-10-29 07:24:11","title":"A Fresh Look at Generalized Category Discovery through Non-negative Matrix Factorization","abstract":"Generalized Category Discovery (GCD) aims to classify both base and novel images using labeled base data. However, current approaches inadequately address the intrinsic optimization of the co-occurrence matrix $\\bar{A}$ based on cosine similarity, failing to achieve zero base-novel regions and adequate sparsity in base and novel domains. To address these deficiencies, we propose a Non-Negative Generalized Category Discovery (NN-GCD) framework. It employs Symmetric Non-negative Matrix Factorization (SNMF) as a mathematical medium to prove the equivalence of optimal K-means with optimal SNMF, and the equivalence of SNMF solver with non-negative contrastive learning (NCL) optimization. Utilizing these theoretical equivalences, it reframes the optimization of $\\bar{A}$ and K-means clustering as an NCL optimization problem. Moreover, to satisfy the non-negative constraints and make a GCD model converge to a near-optimal region, we propose a GELU activation function and an NMF NCE loss. To transition $\\bar{A}$ from a suboptimal state to the desired $\\bar{A}^*$, we introduce a hybrid sparse regularization approach to impose sparsity constraints. Experimental results show NN-GCD outperforms state-of-the-art methods on GCD benchmarks, achieving an average accuracy of 66.1\\% on the Semantic Shift Benchmark, surpassing prior counterparts by 4.7\\%.","sentences":["Generalized Category Discovery (GCD) aims to classify both base and novel images using labeled base data.","However, current approaches inadequately address the intrinsic optimization of the co-occurrence matrix $\\bar{A}$ based on cosine similarity, failing to achieve zero base-novel regions and adequate sparsity in base and novel domains.","To address these deficiencies, we propose a Non-Negative Generalized Category Discovery (NN-GCD) framework.","It employs Symmetric Non-negative Matrix Factorization (SNMF) as a mathematical medium to prove the equivalence of optimal K-means with optimal SNMF, and the equivalence of SNMF solver with non-negative contrastive learning (NCL) optimization.","Utilizing these theoretical equivalences, it reframes the optimization of $\\bar{A}$ and K-means clustering as an NCL optimization problem.","Moreover, to satisfy the non-negative constraints and make a GCD model converge to a near-optimal region, we propose a GELU activation function and an NMF NCE loss.","To transition $\\bar{A}$ from a suboptimal state to the desired $\\bar{A}^*$, we introduce a hybrid sparse regularization approach to impose sparsity constraints.","Experimental results show NN-GCD outperforms state-of-the-art methods on GCD benchmarks, achieving an average accuracy of 66.1\\% on the Semantic Shift Benchmark, surpassing prior counterparts by 4.7\\%."],"url":"http://arxiv.org/abs/2410.21807v1"}
{"created":"2024-10-29 07:16:20","title":"SimSiam Naming Game: A Unified Approach for Representation Learning and Emergent Communication","abstract":"Emergent communication, driven by generative models, enables agents to develop a shared language for describing their individual views of the same objects through interactions. Meanwhile, self-supervised learning (SSL), particularly SimSiam, uses discriminative representation learning to make representations of augmented views of the same data point closer in the representation space. Building on the prior work of VI-SimSiam, which incorporates a generative and Bayesian perspective into the SimSiam framework via variational inference (VI) interpretation, we propose SimSiam+VAE, a unified approach for both representation learning and emergent communication. SimSiam+VAE integrates a variational autoencoder (VAE) into the predictor of the SimSiam network to enhance representation learning and capture uncertainty. Experimental results show that SimSiam+VAE outperforms both SimSiam and VI-SimSiam. We further extend this model into a communication framework called the SimSiam Naming Game (SSNG), which applies the generative and Bayesian approach based on VI to develop internal representations and emergent language, while utilizing the discriminative process of SimSiam to facilitate mutual understanding between agents. In experiments with established models, despite the dynamic alternation of agent roles during interactions, SSNG demonstrates comparable performance to the referential game and slightly outperforms the Metropolis-Hastings naming game.","sentences":["Emergent communication, driven by generative models, enables agents to develop a shared language for describing their individual views of the same objects through interactions.","Meanwhile, self-supervised learning (SSL), particularly SimSiam, uses discriminative representation learning to make representations of augmented views of the same data point closer in the representation space.","Building on the prior work of VI-SimSiam, which incorporates a generative and Bayesian perspective into the SimSiam framework via variational inference (VI) interpretation, we propose SimSiam+VAE, a unified approach for both representation learning and emergent communication.","SimSiam+VAE integrates a variational autoencoder (VAE) into the predictor of the SimSiam network to enhance representation learning and capture uncertainty.","Experimental results show that SimSiam+VAE outperforms both SimSiam and VI-SimSiam.","We further extend this model into a communication framework called the SimSiam Naming Game (SSNG), which applies the generative and Bayesian approach based on VI to develop internal representations and emergent language, while utilizing the discriminative process of SimSiam to facilitate mutual understanding between agents.","In experiments with established models, despite the dynamic alternation of agent roles during interactions, SSNG demonstrates comparable performance to the referential game and slightly outperforms the Metropolis-Hastings naming game."],"url":"http://arxiv.org/abs/2410.21803v1"}
{"created":"2024-10-29 07:13:47","title":"PerSRV: Personalized Sticker Retrieval with Vision-Language Model","abstract":"Instant Messaging is a popular means for daily communication, allowing users to send text and stickers. As the saying goes, \"a picture is worth a thousand words\", so developing an effective sticker retrieval technique is crucial for enhancing user experience. However, existing sticker retrieval methods rely on labeled data to interpret stickers, and general-purpose Vision-Language Models (VLMs) often struggle to capture the unique semantics of stickers. Additionally, relevant-based sticker retrieval methods lack personalization, creating a gap between diverse user expectations and retrieval results. To address these, we propose the Personalized Sticker Retrieval with Vision-Language Model framework, namely PerSRV, structured into offline calculations and online processing modules. The online retrieval part follows the paradigm of relevant recall and personalized ranking, supported by the offline pre-calculation parts, which are sticker semantic understanding, utility evaluation and personalization modules. Firstly, for sticker-level semantic understanding, we supervised fine-tuned LLaVA-1.5-7B to generate human-like sticker semantics, complemented by textual content extracted from figures and historical interaction queries. Secondly, we investigate three crowd-sourcing metrics for sticker utility evaluation. Thirdly, we cluster style centroids based on users' historical interactions to achieve personal preference modeling. Finally, we evaluate our proposed PerSRV method on a public sticker retrieval dataset from WeChat, containing 543,098 candidates and 12,568 interactions. Experimental results show that PerSRV significantly outperforms existing methods in multi-modal sticker retrieval. Additionally, our fine-tuned VLM delivers notable improvements in sticker semantic understandings.","sentences":["Instant Messaging is a popular means for daily communication, allowing users to send text and stickers.","As the saying goes, \"a picture is worth a thousand words\", so developing an effective sticker retrieval technique is crucial for enhancing user experience.","However, existing sticker retrieval methods rely on labeled data to interpret stickers, and general-purpose Vision-Language Models (VLMs) often struggle to capture the unique semantics of stickers.","Additionally, relevant-based sticker retrieval methods lack personalization, creating a gap between diverse user expectations and retrieval results.","To address these, we propose the Personalized Sticker Retrieval with Vision-Language Model framework, namely PerSRV, structured into offline calculations and online processing modules.","The online retrieval part follows the paradigm of relevant recall and personalized ranking, supported by the offline pre-calculation parts, which are sticker semantic understanding, utility evaluation and personalization modules.","Firstly, for sticker-level semantic understanding, we supervised fine-tuned LLaVA-1.5-7B to generate human-like sticker semantics, complemented by textual content extracted from figures and historical interaction queries.","Secondly, we investigate three crowd-sourcing metrics for sticker utility evaluation.","Thirdly, we cluster style centroids based on users' historical interactions to achieve personal preference modeling.","Finally, we evaluate our proposed PerSRV method on a public sticker retrieval dataset from WeChat, containing 543,098 candidates and 12,568 interactions.","Experimental results show that PerSRV significantly outperforms existing methods in multi-modal sticker retrieval.","Additionally, our fine-tuned VLM delivers notable improvements in sticker semantic understandings."],"url":"http://arxiv.org/abs/2410.21801v1"}
{"created":"2024-10-29 07:06:14","title":"Efficient Incremental Code Coverage Analysis for Regression Test Suites","abstract":"Code coverage analysis has been widely adopted in the continuous integration of open-source and industry software repositories to monitor the adequacy of regression test suites. However, computing code coverage can be costly, introducing significant overhead during test execution. Plus, re-collecting code coverage for the entire test suite is usually unnecessary when only a part of the coverage data is affected by code changes. While regression test selection (RTS) techniques exist to select a subset of tests whose behaviors may be affected by code changes, they are not compatible with code coverage analysis techniques -- that is, simply executing RTS-selected tests leads to incorrect code coverage results. In this paper, we present the first incremental code coverage analysis technique, which speeds up code coverage analysis by executing a minimal subset of tests to update the coverage data affected by code changes. We implement our technique in a tool dubbed iJaCoCo, which builds on Ekstazi and JaCoCo -- the state-of-the-art RTS and code coverage analysis tools for Java. We evaluate iJaCoCo on 1,122 versions from 22 open-source repositories and show that iJaCoCo can speed up code coverage analysis time by an average of 1.86x and up to 8.20x compared to JaCoCo.","sentences":["Code coverage analysis has been widely adopted in the continuous integration of open-source and industry software repositories to monitor the adequacy of regression test suites.","However, computing code coverage can be costly, introducing significant overhead during test execution.","Plus, re-collecting code coverage for the entire test suite is usually unnecessary when only a part of the coverage data is affected by code changes.","While regression test selection (RTS) techniques exist to select a subset of tests whose behaviors may be affected by code changes, they are not compatible with code coverage analysis techniques -- that is, simply executing RTS-selected tests leads to incorrect code coverage results.","In this paper, we present the first incremental code coverage analysis technique, which speeds up code coverage analysis by executing a minimal subset of tests to update the coverage data affected by code changes.","We implement our technique in a tool dubbed iJaCoCo, which builds on Ekstazi and JaCoCo -- the state-of-the-art RTS and code coverage analysis tools for Java.","We evaluate iJaCoCo on 1,122 versions from 22 open-source repositories and show that iJaCoCo can speed up code coverage analysis time by an average of 1.86x and up to 8.20x compared to JaCoCo."],"url":"http://arxiv.org/abs/2410.21798v1"}
{"created":"2024-10-29 05:46:16","title":"DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial Information for Dynamics Model Learning","abstract":"This work proposes DOFS, a pilot dataset of 3D deformable objects (DOs) (e.g., elasto-plastic objects) with full spatial information (i.e., top, side, and bottom information) using a novel and low-cost data collection platform with a transparent operating plane. The dataset consists of active manipulation action, multi-view RGB-D images, well-registered point clouds, 3D deformed mesh, and 3D occupancy with semantics, using a pinching strategy with a two-parallel-finger gripper. In addition, we trained a neural network with the down-sampled 3D occupancy and action as input to model the dynamics of an elasto-plastic object. Our dataset and all CADs of the data collection system will be released soon on our website.","sentences":["This work proposes DOFS, a pilot dataset of 3D deformable objects (DOs) (e.g., elasto-plastic objects) with full spatial information (i.e., top, side, and bottom information) using a novel and low-cost data collection platform with a transparent operating plane.","The dataset consists of active manipulation action, multi-view RGB-D images, well-registered point clouds, 3D deformed mesh, and 3D occupancy with semantics, using a pinching strategy with a two-parallel-finger gripper.","In addition, we trained a neural network with the down-sampled 3D occupancy and action as input to model the dynamics of an elasto-plastic object.","Our dataset and all CADs of the data collection system will be released soon on our website."],"url":"http://arxiv.org/abs/2410.21758v1"}
{"created":"2024-10-29 05:33:14","title":"Learning and Unlearning of Fabricated Knowledge in Language Models","abstract":"What happens when a new piece of knowledge is introduced into the training data and how long does it last while a large language model (LM) continues to train? We investigate this question by injecting facts into LMs from a new probing dataset, \"Outlandish\", which is designed to permit the testing of a spectrum of different fact types. When studying how robust these memories are, there appears to be a sweet spot in the spectrum of fact novelty between consistency with world knowledge and total randomness, where the injected memory is the most enduring. Specifically we show that facts that conflict with common knowledge are remembered for tens of thousands of training steps, while prompts not conflicting with common knowledge (mundane), as well as scrambled prompts (randomly jumbled) are both forgotten much more rapidly. Further, knowledge-conflicting facts can \"prime'' how the language model hallucinates on logically unrelated prompts, showing their propensity for non-target generalization, while both mundane and randomly jumbled facts prime significantly less. Finally, we show that impacts of knowledge-conflicting facts in LMs, though they can be long lasting, can be largely erased by novel application of multi-step sparse updates, even while the training ability of the model is preserved. As such, this very simple procedure has direct implications for mitigating the effects of data poisoning in training.","sentences":["What happens when a new piece of knowledge is introduced into the training data and how long does it last while a large language model (LM) continues to train?","We investigate this question by injecting facts into LMs from a new probing dataset, \"Outlandish\", which is designed to permit the testing of a spectrum of different fact types.","When studying how robust these memories are, there appears to be a sweet spot in the spectrum of fact novelty between consistency with world knowledge and total randomness, where the injected memory is the most enduring.","Specifically we show that facts that conflict with common knowledge are remembered for tens of thousands of training steps, while prompts not conflicting with common knowledge (mundane), as well as scrambled prompts (randomly jumbled) are both forgotten much more rapidly.","Further, knowledge-conflicting facts can \"prime'' how the language model hallucinates on logically unrelated prompts, showing their propensity for non-target generalization, while both mundane and randomly jumbled facts prime significantly less.","Finally, we show that impacts of knowledge-conflicting facts in LMs, though they can be long lasting, can be largely erased by novel application of multi-step sparse updates, even while the training ability of the model is preserved.","As such, this very simple procedure has direct implications for mitigating the effects of data poisoning in training."],"url":"http://arxiv.org/abs/2410.21750v1"}
{"created":"2024-10-29 05:10:34","title":"EI-Nexus: Towards Unmediated and Flexible Inter-Modality Local Feature Extraction and Matching for Event-Image Data","abstract":"Event cameras, with high temporal resolution and high dynamic range, have limited research on the inter-modality local feature extraction and matching of event-image data. We propose EI-Nexus, an unmediated and flexible framework that integrates two modality-specific keypoint extractors and a feature matcher. To achieve keypoint extraction across viewpoint and modality changes, we bring Local Feature Distillation (LFD), which transfers the viewpoint consistency from a well-learned image extractor to the event extractor, ensuring robust feature correspondence. Furthermore, with the help of Context Aggregation (CA), a remarkable enhancement is observed in feature matching. We further establish the first two inter-modality feature matching benchmarks, MVSEC-RPE and EC-RPE, to assess relative pose estimation on event-image data. Our approach outperforms traditional methods that rely on explicit modal transformation, offering more unmediated and adaptable feature extraction and matching, achieving better keypoint similarity and state-of-the-art results on the MVSEC-RPE and EC-RPE benchmarks. The source code and benchmarks will be made publicly available at https://github.com/ZhonghuaYi/EI-Nexus_official.","sentences":["Event cameras, with high temporal resolution and high dynamic range, have limited research on the inter-modality local feature extraction and matching of event-image data.","We propose EI-Nexus, an unmediated and flexible framework that integrates two modality-specific keypoint extractors and a feature matcher.","To achieve keypoint extraction across viewpoint and modality changes, we bring Local Feature Distillation (LFD), which transfers the viewpoint consistency from a well-learned image extractor to the event extractor, ensuring robust feature correspondence.","Furthermore, with the help of Context Aggregation (CA), a remarkable enhancement is observed in feature matching.","We further establish the first two inter-modality feature matching benchmarks, MVSEC-RPE and EC-RPE, to assess relative pose estimation on event-image data.","Our approach outperforms traditional methods that rely on explicit modal transformation, offering more unmediated and adaptable feature extraction and matching, achieving better keypoint similarity and state-of-the-art results on the MVSEC-RPE and EC-RPE benchmarks.","The source code and benchmarks will be made publicly available at https://github.com/ZhonghuaYi/EI-Nexus_official."],"url":"http://arxiv.org/abs/2410.21743v1"}
{"created":"2024-10-29 04:58:07","title":"Enhancing Financial Question Answering with a Multi-Agent Reflection Framework","abstract":"While Large Language Models (LLMs) have shown impressive capabilities in numerous Natural Language Processing (NLP) tasks, they still struggle with financial question answering (QA), particularly when numerical reasoning is required. Recently, LLM-based multi-agent frameworks have demonstrated remarkable effectiveness in multi-step reasoning, which is crucial for financial QA tasks as it involves extracting relevant information from tables and text and then performing numerical reasoning on the extracted data to infer answers. In this study, we propose a multi-agent framework incorporating a critic agent that reflects on the reasoning steps and final answers for each question. Additionally, we enhance our system by adding multiple critic agents, each focusing on a specific aspect of the answer. Our results indicate that this framework significantly improves performance compared to single-agent reasoning, with an average performance increase of 15% for the LLaMA3-8B model and 5% for the LLaMA3-70B model. Furthermore, our framework performs on par with, and in some cases surpasses, larger single-agent LLMs such as LLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to Claude-3.5 Sonnet. Overall, our framework presents an effective solution to enhance open-source LLMs for financial QA tasks, offering a cost-effective alternative to larger models like Claude-3.5 Sonnet.","sentences":["While Large Language Models (LLMs) have shown impressive capabilities in numerous Natural Language Processing (NLP) tasks, they still struggle with financial question answering (QA), particularly when numerical reasoning is required.","Recently, LLM-based multi-agent frameworks have demonstrated remarkable effectiveness in multi-step reasoning, which is crucial for financial QA tasks as it involves extracting relevant information from tables and text and then performing numerical reasoning on the extracted data to infer answers.","In this study, we propose a multi-agent framework incorporating a critic agent that reflects on the reasoning steps and final answers for each question.","Additionally, we enhance our system by adding multiple critic agents, each focusing on a specific aspect of the answer.","Our results indicate that this framework significantly improves performance compared to single-agent reasoning, with an average performance increase of 15% for the LLaMA3-8B model and 5% for the LLaMA3-70B model.","Furthermore, our framework performs on par with, and in some cases surpasses, larger single-agent LLMs such as LLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to Claude-3.5 Sonnet.","Overall, our framework presents an effective solution to enhance open-source LLMs for financial QA tasks, offering a cost-effective alternative to larger models like Claude-3.5 Sonnet."],"url":"http://arxiv.org/abs/2410.21741v1"}
{"created":"2024-10-29 04:56:34","title":"Building Castles in the Cloud: Architecting Resilient and Scalable Infrastructure","abstract":"In the contemporary world of dynamic digital solutions and services, the significance of effective and stable cloud solutions cannot be overestimated. The cloud adaptation is becoming more popular due to mobile advantages, including flexibility, cheaper costs and scalability. However, creating a fail-proof architecture that can accommodate scale-up and enable high data availability and security is not an easy task. In this paper, a discussion will be made regarding significant measures required in designing contexts inside the cloud environment. It explores the need for replicate servers, fault tolerance, disaster backup and load balancing for high availability. Further, the paper also discusses the optimum strategy for designing cloud infrastructures such as microservices, containerization, and serverless. Based on the literature review, we analyze various approaches that are used to improve cloud reliability and elasticity. The paper also provides a best practice guide for designing a cloud infrastructure for these requirements concerning cases. The results and discussion section outlines the improvement in business continuity and operational efficiency when using the proposed architecture. This paper concludes with recommendations for future studies and the successful application of the elaborated matters.","sentences":["In the contemporary world of dynamic digital solutions and services, the significance of effective and stable cloud solutions cannot be overestimated.","The cloud adaptation is becoming more popular due to mobile advantages, including flexibility, cheaper costs and scalability.","However, creating a fail-proof architecture that can accommodate scale-up and enable high data availability and security is not an easy task.","In this paper, a discussion will be made regarding significant measures required in designing contexts inside the cloud environment.","It explores the need for replicate servers, fault tolerance, disaster backup and load balancing for high availability.","Further, the paper also discusses the optimum strategy for designing cloud infrastructures such as microservices, containerization, and serverless.","Based on the literature review, we analyze various approaches that are used to improve cloud reliability and elasticity.","The paper also provides a best practice guide for designing a cloud infrastructure for these requirements concerning cases.","The results and discussion section outlines the improvement in business continuity and operational efficiency when using the proposed architecture.","This paper concludes with recommendations for future studies and the successful application of the elaborated matters."],"url":"http://arxiv.org/abs/2410.21740v1"}
{"created":"2024-10-29 04:54:45","title":"SS3DM: Benchmarking Street-View Surface Reconstruction with a Synthetic 3D Mesh Dataset","abstract":"Reconstructing accurate 3D surfaces for street-view scenarios is crucial for applications such as digital entertainment and autonomous driving simulation. However, existing street-view datasets, including KITTI, Waymo, and nuScenes, only offer noisy LiDAR points as ground-truth data for geometric evaluation of reconstructed surfaces. These geometric ground-truths often lack the necessary precision to evaluate surface positions and do not provide data for assessing surface normals. To overcome these challenges, we introduce the SS3DM dataset, comprising precise \\textbf{S}ynthetic \\textbf{S}treet-view \\textbf{3D} \\textbf{M}esh models exported from the CARLA simulator. These mesh models facilitate accurate position evaluation and include normal vectors for evaluating surface normal. To simulate the input data in realistic driving scenarios for 3D reconstruction, we virtually drive a vehicle equipped with six RGB cameras and five LiDAR sensors in diverse outdoor scenes. Leveraging this dataset, we establish a benchmark for state-of-the-art surface reconstruction methods, providing a comprehensive evaluation of the associated challenges.   For more information, visit our homepage at https://ss3dm.top.","sentences":["Reconstructing accurate 3D surfaces for street-view scenarios is crucial for applications such as digital entertainment and autonomous driving simulation.","However, existing street-view datasets, including KITTI, Waymo, and nuScenes, only offer noisy LiDAR points as ground-truth data for geometric evaluation of reconstructed surfaces.","These geometric ground-truths often lack the necessary precision to evaluate surface positions and do not provide data for assessing surface normals.","To overcome these challenges, we introduce the SS3DM dataset, comprising precise \\textbf{S}ynthetic \\textbf{S}treet-view \\textbf{3D} \\textbf{M}esh models exported from the CARLA simulator.","These mesh models facilitate accurate position evaluation and include normal vectors for evaluating surface normal.","To simulate the input data in realistic driving scenarios for 3D reconstruction, we virtually drive a vehicle equipped with six RGB cameras and five LiDAR sensors in diverse outdoor scenes.","Leveraging this dataset, we establish a benchmark for state-of-the-art surface reconstruction methods, providing a comprehensive evaluation of the associated challenges.   ","For more information, visit our homepage at https://ss3dm.top."],"url":"http://arxiv.org/abs/2410.21739v1"}
{"created":"2024-10-29 04:28:49","title":"Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models","abstract":"While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved. Existing endeavors have focused on bridging these gaps; however, these approaches either hinge on external data and cannot completely eliminate manual effort, or they fall short in effectively directing LLMs to generate high-quality exemplary prompts. To address the said pitfalls, we propose a novel prompt approach for automatic reasoning named \\textbf{LBS3}, inspired by curriculum learning which better reflects human learning habits. Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries that are pertinent to the target query. Following this, it invokes a progressive strategy that utilizes exemplary prompts stemmed from easy-proxy queries to direct LLMs in solving hard-proxy queries, enabling the high-quality of the proxy solutions. Finally, our extensive experiments in various reasoning-intensive tasks with varying open- and closed-source LLMs show that LBS3 achieves strongly competitive performance compared to the SOTA baselines.","sentences":["While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved.","Existing endeavors have focused on bridging these gaps; however, these approaches either hinge on external data and cannot completely eliminate manual effort, or they fall short in effectively directing LLMs to generate high-quality exemplary prompts.","To address the said pitfalls, we propose a novel prompt approach for automatic reasoning named \\textbf{LBS3}, inspired by curriculum learning which better reflects human learning habits.","Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries that are pertinent to the target query.","Following this, it invokes a progressive strategy that utilizes exemplary prompts stemmed from easy-proxy queries to direct LLMs in solving hard-proxy queries, enabling the high-quality of the proxy solutions.","Finally, our extensive experiments in various reasoning-intensive tasks with varying open- and closed-source LLMs show that LBS3 achieves strongly competitive performance compared to the SOTA baselines."],"url":"http://arxiv.org/abs/2410.21728v1"}
{"created":"2024-10-29 04:27:12","title":"Edge Arrival Online Matching: The Power of Free Disposal on Acyclic Graphs","abstract":"Online matching is a fundamental problem in the study of online algorithms. We study the problem under a very general arrival model: the edge arrival model. Free disposal is an important notion in the online matching literature, which allows the algorithm to dispose of the previously matched edges. Without free disposal, we cannot achieve any bounded ratio, even with randomized algorithms, when edges are weighted.   Our paper focuses on clarifying the power of free disposal in both the unweighted and the weighted setting. As far as we know, it's still uncertain if free disposal can give us extra leverage to enhance the competitive ratio in the unweighted scenario, even in specific instances such as Growing Trees, where every new edge adds a new leaf to the graph. Our study serves as a valuable initial exploration of this open question. The results are listed as follows:   1. With free disposal, we improve the competitive ratio for unweighted online matching on Growing Trees from $5/9$ to $2/3 \\approx 0.66$, and show that the ratio is tight. For Forests, a more general setting where the underlying graph is a forest and edges may arrive in arbitrary order, we improve the competitive ratio from $5/9$ to $5/8 = 0.625$.   2. Both the ratios of $2/3$ and $0.625$ show a separation to the upper bound of the competitive ratio without free disposal on Growing Trees ($0.5914$). Therefore, we demonstrate the additional power of free disposal for the unweighted setting for the first time, at least in the special setting of Growing Trees and Forests.   3. We improve the competitive ratio for weighted online matching on Growing Trees from $1/3$ to $1/2$ using a very simple ordinal algorithm, and show that it is optimal among ordinal algorithms.","sentences":["Online matching is a fundamental problem in the study of online algorithms.","We study the problem under a very general arrival model: the edge arrival model.","Free disposal is an important notion in the online matching literature, which allows the algorithm to dispose of the previously matched edges.","Without free disposal, we cannot achieve any bounded ratio, even with randomized algorithms, when edges are weighted.   ","Our paper focuses on clarifying the power of free disposal in both the unweighted and the weighted setting.","As far as we know, it's still uncertain if free disposal can give us extra leverage to enhance the competitive ratio in the unweighted scenario, even in specific instances such as Growing Trees, where every new edge adds a new leaf to the graph.","Our study serves as a valuable initial exploration of this open question.","The results are listed as follows:   1.","With free disposal, we improve the competitive ratio for unweighted online matching on Growing Trees from $5/9$ to $2/3 \\approx 0.66$, and show that the ratio is tight.","For Forests, a more general setting where the underlying graph is a forest and edges may arrive in arbitrary order, we improve the competitive ratio from $5/9$ to $5/8 = 0.625$.   2.","Both the ratios of $2/3$ and $0.625$ show a separation to the upper bound of the competitive ratio without free disposal on Growing Trees ($0.5914$).","Therefore, we demonstrate the additional power of free disposal for the unweighted setting for the first time, at least in the special setting of Growing Trees and Forests.   ","3.","We improve the competitive ratio for weighted online matching on Growing Trees from $1/3$ to $1/2$ using a very simple ordinal algorithm, and show that it is optimal among ordinal algorithms."],"url":"http://arxiv.org/abs/2410.21727v1"}
{"created":"2024-10-29 04:22:28","title":"Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection","abstract":"Domain Generation Algorithms (DGAs) are malicious techniques used by malware to dynamically generate seemingly random domain names for communication with Command & Control (C&C) servers. Due to the fast and simple generation of DGA domains, detection methods must be highly efficient and precise to be effective. Large Language Models (LLMs) have demonstrated their proficiency in real-time detection tasks, making them ideal candidates for detecting DGAs. Our work validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS exfiltration attacks. We developed LLM models and conducted comprehensive evaluation using a diverse dataset comprising 59 distinct real-world DGA malware families and normal domain data. Our LLM model significantly outperformed traditional natural language processing techniques, especially in detecting unknown DGAs. We also evaluated its performance on DNS exfiltration datasets, demonstrating its effectiveness in enhancing cybersecurity measures. To the best of our knowledge, this is the first work that empirically applies LLMs for DGA and DNS exfiltration detection.","sentences":["Domain Generation Algorithms (DGAs) are malicious techniques used by malware to dynamically generate seemingly random domain names for communication with Command & Control (C&C) servers.","Due to the fast and simple generation of DGA domains, detection methods must be highly efficient and precise to be effective.","Large Language Models (LLMs) have demonstrated their proficiency in real-time detection tasks, making them ideal candidates for detecting DGAs.","Our work validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS exfiltration attacks.","We developed LLM models and conducted comprehensive evaluation using a diverse dataset comprising 59 distinct real-world DGA malware families and normal domain data.","Our LLM model significantly outperformed traditional natural language processing techniques, especially in detecting unknown DGAs.","We also evaluated its performance on DNS exfiltration datasets, demonstrating its effectiveness in enhancing cybersecurity measures.","To the best of our knowledge, this is the first work that empirically applies LLMs for DGA and DNS exfiltration detection."],"url":"http://arxiv.org/abs/2410.21723v1"}
{"created":"2024-10-29 04:14:32","title":"Generating Realistic Tabular Data with Large Language Models","abstract":"While most generative models show achievements in image data generation, few are developed for tabular data generation. Recently, due to success of large language models (LLM) in diverse tasks, they have also been used for tabular data generation. However, these methods do not capture the correct correlation between the features and the target variable, hindering their applications in downstream predictive tasks. To address this problem, we propose a LLM-based method with three important improvements to correctly capture the ground-truth feature-class correlation in the real data. First, we propose a novel permutation strategy for the input data in the fine-tuning phase. Second, we propose a feature-conditional sampling approach to generate synthetic samples. Finally, we generate the labels by constructing prompts based on the generated samples to query our fine-tuned LLM. Our extensive experiments show that our method significantly outperforms 10 SOTA baselines on 20 datasets in downstream tasks. It also produces highly realistic synthetic samples in terms of quality and diversity. More importantly, classifiers trained with our synthetic data can even compete with classifiers trained with the original data on half of the benchmark datasets, which is a significant achievement in tabular data generation.","sentences":["While most generative models show achievements in image data generation, few are developed for tabular data generation.","Recently, due to success of large language models (LLM) in diverse tasks, they have also been used for tabular data generation.","However, these methods do not capture the correct correlation between the features and the target variable, hindering their applications in downstream predictive tasks.","To address this problem, we propose a LLM-based method with three important improvements to correctly capture the ground-truth feature-class correlation in the real data.","First, we propose a novel permutation strategy for the input data in the fine-tuning phase.","Second, we propose a feature-conditional sampling approach to generate synthetic samples.","Finally, we generate the labels by constructing prompts based on the generated samples to query our fine-tuned LLM.","Our extensive experiments show that our method significantly outperforms 10 SOTA baselines on 20 datasets in downstream tasks.","It also produces highly realistic synthetic samples in terms of quality and diversity.","More importantly, classifiers trained with our synthetic data can even compete with classifiers trained with the original data on half of the benchmark datasets, which is a significant achievement in tabular data generation."],"url":"http://arxiv.org/abs/2410.21717v1"}
{"created":"2024-10-29 04:14:23","title":"A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution","abstract":"Authorship attribution aims to identify the origin or author of a document. Traditional approaches have heavily relied on manual features and fail to capture long-range correlations, limiting their effectiveness. Recent advancements leverage text embeddings from pre-trained language models, which require significant fine-tuning on labeled data, posing challenges in data dependency and limited interpretability. Large Language Models (LLMs), with their deep reasoning capabilities and ability to maintain long-range textual associations, offer a promising alternative. This study explores the potential of pre-trained LLMs in one-shot authorship attribution, specifically utilizing Bayesian approaches and probability outputs of LLMs. Our methodology calculates the probability that a text entails previous writings of an author, reflecting a more nuanced understanding of authorship. By utilizing only pre-trained models such as Llama-3-70B, our results on the IMDb and blog datasets show an impressive 85\\% accuracy in one-shot authorship classification across ten authors. Our findings set new baselines for one-shot authorship analysis using LLMs and expand the application scope of these models in forensic linguistics. This work also includes extensive ablation studies to validate our approach.","sentences":["Authorship attribution aims to identify the origin or author of a document.","Traditional approaches have heavily relied on manual features and fail to capture long-range correlations, limiting their effectiveness.","Recent advancements leverage text embeddings from pre-trained language models, which require significant fine-tuning on labeled data, posing challenges in data dependency and limited interpretability.","Large Language Models (LLMs), with their deep reasoning capabilities and ability to maintain long-range textual associations, offer a promising alternative.","This study explores the potential of pre-trained LLMs in one-shot authorship attribution, specifically utilizing Bayesian approaches and probability outputs of LLMs.","Our methodology calculates the probability that a text entails previous writings of an author, reflecting a more nuanced understanding of authorship.","By utilizing only pre-trained models such as Llama-3-70B, our results on the IMDb and blog datasets show an impressive 85\\% accuracy in one-shot authorship classification across ten authors.","Our findings set new baselines for one-shot authorship analysis using LLMs and expand the application scope of these models in forensic linguistics.","This work also includes extensive ablation studies to validate our approach."],"url":"http://arxiv.org/abs/2410.21716v1"}
{"created":"2024-10-29 03:53:03","title":"Multi-view clustering integrating anchor attribute and structural information","abstract":"Multisource data has spurred the development of advanced clustering algorithms, such as multi-view clustering, which critically relies on constructing similarity matrices. Traditional algorithms typically generate these matrices from sample attributes alone. However, real-world networks often include pairwise directed topological structures critical for clustering. This paper introduces a novel multi-view clustering algorithm, AAS. It utilizes a two-step proximity approach via anchors in each view, integrating attribute and directed structural information. This approach enhances the clarity of category characteristics in the similarity matrices. The anchor structural similarity matrix leverages strongly connected components of directed graphs. The entire process-from similarity matrices construction to clustering - is consolidated into a unified optimization framework. Comparative experiments on the modified Attribute SBM dataset against eight algorithms affirm the effectiveness and superiority of AAS.","sentences":["Multisource data has spurred the development of advanced clustering algorithms, such as multi-view clustering, which critically relies on constructing similarity matrices.","Traditional algorithms typically generate these matrices from sample attributes alone.","However, real-world networks often include pairwise directed topological structures critical for clustering.","This paper introduces a novel multi-view clustering algorithm, AAS.","It utilizes a two-step proximity approach via anchors in each view, integrating attribute and directed structural information.","This approach enhances the clarity of category characteristics in the similarity matrices.","The anchor structural similarity matrix leverages strongly connected components of directed graphs.","The entire process-from similarity matrices construction to clustering - is consolidated into a unified optimization framework.","Comparative experiments on the modified Attribute SBM dataset against eight algorithms affirm the effectiveness and superiority of AAS."],"url":"http://arxiv.org/abs/2410.21711v1"}
{"created":"2024-10-29 03:49:40","title":"Unsupervised Modality Adaptation with Text-to-Image Diffusion Models for Semantic Segmentation","abstract":"Despite their success, unsupervised domain adaptation methods for semantic segmentation primarily focus on adaptation between image domains and do not utilize other abundant visual modalities like depth, infrared and event. This limitation hinders their performance and restricts their application in real-world multimodal scenarios. To address this issue, we propose Modality Adaptation with text-to-image Diffusion Models (MADM) for semantic segmentation task which utilizes text-to-image diffusion models pre-trained on extensive image-text pairs to enhance the model's cross-modality capabilities. Specifically, MADM comprises two key complementary components to tackle major challenges. First, due to the large modality gap, using one modal data to generate pseudo labels for another modality suffers from a significant drop in accuracy. To address this, MADM designs diffusion-based pseudo-label generation which adds latent noise to stabilize pseudo-labels and enhance label accuracy. Second, to overcome the limitations of latent low-resolution features in diffusion models, MADM introduces the label palette and latent regression which converts one-hot encoded labels into the RGB form by palette and regresses them in the latent space, thus ensuring the pre-trained decoder for up-sampling to obtain fine-grained features. Extensive experimental results demonstrate that MADM achieves state-of-the-art adaptation performance across various modality tasks, including images to depth, infrared, and event modalities. We open-source our code and models at https://github.com/XiaRho/MADM.","sentences":["Despite their success, unsupervised domain adaptation methods for semantic segmentation primarily focus on adaptation between image domains and do not utilize other abundant visual modalities like depth, infrared and event.","This limitation hinders their performance and restricts their application in real-world multimodal scenarios.","To address this issue, we propose Modality Adaptation with text-to-image Diffusion Models (MADM) for semantic segmentation task which utilizes text-to-image diffusion models pre-trained on extensive image-text pairs to enhance the model's cross-modality capabilities.","Specifically, MADM comprises two key complementary components to tackle major challenges.","First, due to the large modality gap, using one modal data to generate pseudo labels for another modality suffers from a significant drop in accuracy.","To address this, MADM designs diffusion-based pseudo-label generation which adds latent noise to stabilize pseudo-labels and enhance label accuracy.","Second, to overcome the limitations of latent low-resolution features in diffusion models, MADM introduces the label palette and latent regression which converts one-hot encoded labels into the RGB form by palette and regresses them in the latent space, thus ensuring the pre-trained decoder for up-sampling to obtain fine-grained features.","Extensive experimental results demonstrate that MADM achieves state-of-the-art adaptation performance across various modality tasks, including images to depth, infrared, and event modalities.","We open-source our code and models at https://github.com/XiaRho/MADM."],"url":"http://arxiv.org/abs/2410.21708v1"}
{"created":"2024-10-29 03:41:47","title":"AdaptGCD: Multi-Expert Adapter Tuning for Generalized Category Discovery","abstract":"Different from the traditional semi-supervised learning paradigm that is constrained by the close-world assumption, Generalized Category Discovery (GCD) presumes that the unlabeled dataset contains new categories not appearing in the labeled set, and aims to not only classify old categories but also discover new categories in the unlabeled data. Existing studies on GCD typically devote to transferring the general knowledge from the self-supervised pretrained model to the target GCD task via some fine-tuning strategies, such as partial tuning and prompt learning. Nevertheless, these fine-tuning methods fail to make a sound balance between the generalization capacity of pretrained backbone and the adaptability to the GCD task. To fill this gap, in this paper, we propose a novel adapter-tuning-based method named AdaptGCD, which is the first work to introduce the adapter tuning into the GCD task and provides some key insights expected to enlighten future research. Furthermore, considering the discrepancy of supervision information between the old and new classes, a multi-expert adapter structure equipped with a route assignment constraint is elaborately devised, such that the data from old and new classes are separated into different expert groups. Extensive experiments are conducted on 7 widely-used datasets. The remarkable improvements in performance highlight the effectiveness of our proposals.","sentences":["Different from the traditional semi-supervised learning paradigm that is constrained by the close-world assumption, Generalized Category Discovery (GCD) presumes that the unlabeled dataset contains new categories not appearing in the labeled set, and aims to not only classify old categories but also discover new categories in the unlabeled data.","Existing studies on GCD typically devote to transferring the general knowledge from the self-supervised pretrained model to the target GCD task via some fine-tuning strategies, such as partial tuning and prompt learning.","Nevertheless, these fine-tuning methods fail to make a sound balance between the generalization capacity of pretrained backbone and the adaptability to the GCD task.","To fill this gap, in this paper, we propose a novel adapter-tuning-based method named AdaptGCD, which is the first work to introduce the adapter tuning into the GCD task and provides some key insights expected to enlighten future research.","Furthermore, considering the discrepancy of supervision information between the old and new classes, a multi-expert adapter structure equipped with a route assignment constraint is elaborately devised, such that the data from old and new classes are separated into different expert groups.","Extensive experiments are conducted on 7 widely-used datasets.","The remarkable improvements in performance highlight the effectiveness of our proposals."],"url":"http://arxiv.org/abs/2410.21705v1"}
{"created":"2024-10-29 03:27:56","title":"On the Role of Depth and Looping for In-Context Learning with Task Diversity","abstract":"The intriguing in-context learning (ICL) abilities of deep Transformer models have lately garnered significant attention. By studying in-context linear regression on unimodal Gaussian data, recent empirical and theoretical works have argued that ICL emerges from Transformers' abilities to simulate learning algorithms like gradient descent. However, these works fail to capture the remarkable ability of Transformers to learn multiple tasks in context. To this end, we study in-context learning for linear regression with diverse tasks, characterized by data covariance matrices with condition numbers ranging from $[1, \\kappa]$, and highlight the importance of depth in this setting. More specifically, (a) we show theoretical lower bounds of $\\log(\\kappa)$ (or $\\sqrt{\\kappa}$) linear attention layers in the unrestricted (or restricted) attention setting and, (b) we show that multilayer Transformers can indeed solve such tasks with a number of layers that matches the lower bounds. However, we show that this expressivity of multilayer Transformer comes at the price of robustness. In particular, multilayer Transformers are not robust to even distributional shifts as small as $O(e^{-L})$ in Wasserstein distance, where $L$ is the depth of the network. We then demonstrate that Looped Transformers -- a special class of multilayer Transformers with weight-sharing -- not only exhibit similar expressive power but are also provably robust under mild assumptions. Besides out-of-distribution generalization, we also show that Looped Transformers are the only models that exhibit a monotonic behavior of loss with respect to depth.","sentences":["The intriguing in-context learning (ICL) abilities of deep Transformer models have lately garnered significant attention.","By studying in-context linear regression on unimodal Gaussian data, recent empirical and theoretical works have argued that ICL emerges from Transformers' abilities to simulate learning algorithms like gradient descent.","However, these works fail to capture the remarkable ability of Transformers to learn multiple tasks in context.","To this end, we study in-context learning for linear regression with diverse tasks, characterized by data covariance matrices with condition numbers ranging from $[1, \\kappa]$, and highlight the importance of depth in this setting.","More specifically, (a) we show theoretical lower bounds of $\\log(\\kappa)$ (or $\\sqrt{\\kappa}$) linear attention layers in the unrestricted (or restricted) attention setting and, (b) we show that multilayer Transformers can indeed solve such tasks with a number of layers that matches the lower bounds.","However, we show that this expressivity of multilayer Transformer comes at the price of robustness.","In particular, multilayer Transformers are not robust to even distributional shifts as small as $O(e^{-L})$ in Wasserstein distance, where $L$ is the depth of the network.","We then demonstrate that Looped Transformers -- a special class of multilayer Transformers with weight-sharing -- not only exhibit similar expressive power but are also provably robust under mild assumptions.","Besides out-of-distribution generalization, we also show that Looped Transformers are the only models that exhibit a monotonic behavior of loss with respect to depth."],"url":"http://arxiv.org/abs/2410.21698v1"}
{"created":"2024-10-29 03:25:20","title":"CFSafety: Comprehensive Fine-grained Safety Assessment for LLMs","abstract":"As large language models (LLMs) rapidly evolve, they bring significant conveniences to our work and daily lives, but also introduce considerable safety risks. These models can generate texts with social biases or unethical content, and under specific adversarial instructions, may even incite illegal activities. Therefore, rigorous safety assessments of LLMs are crucial. In this work, we introduce a safety assessment benchmark, CFSafety, which integrates 5 classic safety scenarios and 5 types of instruction attacks, totaling 10 categories of safety questions, to form a test set with 25k prompts. This test set was used to evaluate the natural language generation (NLG) capabilities of LLMs, employing a combination of simple moral judgment and a 1-5 safety rating scale for scoring. Using this benchmark, we tested eight popular LLMs, including the GPT series. The results indicate that while GPT-4 demonstrated superior safety performance, the safety effectiveness of LLMs, including this model, still requires improvement. The data and code associated with this study are available on GitHub.","sentences":["As large language models (LLMs) rapidly evolve, they bring significant conveniences to our work and daily lives, but also introduce considerable safety risks.","These models can generate texts with social biases or unethical content, and under specific adversarial instructions, may even incite illegal activities.","Therefore, rigorous safety assessments of LLMs are crucial.","In this work, we introduce a safety assessment benchmark, CFSafety, which integrates 5 classic safety scenarios and 5 types of instruction attacks, totaling 10 categories of safety questions, to form a test set with 25k prompts.","This test set was used to evaluate the natural language generation (NLG) capabilities of LLMs, employing a combination of simple moral judgment and a 1-5 safety rating scale for scoring.","Using this benchmark, we tested eight popular LLMs, including the GPT series.","The results indicate that while GPT-4 demonstrated superior safety performance, the safety effectiveness of LLMs, including this model, still requires improvement.","The data and code associated with this study are available on GitHub."],"url":"http://arxiv.org/abs/2410.21695v1"}
{"created":"2024-10-29 03:18:31","title":"Improved Spectral Density Estimation via Explicit and Implicit Deflation","abstract":"We study algorithms for approximating the spectral density of a symmetric matrix $A$ that is accessed through matrix-vector product queries. By combining a previously studied Chebyshev polynomial moment matching method with a deflation step that approximately projects off the largest magnitude eigendirections of $A$ before estimating the spectral density, we give an $\\epsilon\\cdot\\sigma_\\ell(A)$ error approximation to the spectral density in the Wasserstein-$1$ metric using $O(\\ell\\log n+ 1/\\epsilon)$ matrix-vector products, where $\\sigma_\\ell(A)$ is the $\\ell^{th}$ largest singular value of $A$. In the common case when $A$ exhibits fast singular value decay, our bound can be much stronger than prior work, which gives an error bound of $\\epsilon \\cdot ||A||_2$ using $O(1/\\epsilon)$ matrix-vector products. We also show that it is nearly tight: any algorithm giving error $\\epsilon \\cdot \\sigma_\\ell(A)$ must use $\\Omega(\\ell+1/\\epsilon)$ matrix-vector products.   We further show that the popular Stochastic Lanczos Quadrature (SLQ) method matches the above bound, even though SLQ itself is parameter-free and performs no explicit deflation. This bound explains the strong practical performance of SLQ, and motivates a simple variant of SLQ that achieves an even tighter error bound. Our error bound for SLQ leverages an analysis that views it as an implicit polynomial moment matching method, along with recent results on low-rank approximation with single-vector Krylov methods. We use these results to show that the method can perform implicit deflation as part of moment matching.","sentences":["We study algorithms for approximating the spectral density of a symmetric matrix $A$ that is accessed through matrix-vector product queries.","By combining a previously studied Chebyshev polynomial moment matching method with a deflation step that approximately projects off the largest magnitude eigendirections of $A$ before estimating the spectral density, we give an $\\epsilon\\cdot\\sigma_\\ell(A)$ error approximation to the spectral density in the Wasserstein-$1$ metric using $O(\\ell\\log n+ 1/\\epsilon)$ matrix-vector products, where $\\sigma_\\ell(A)$ is the $\\ell^{th}$ largest singular value of $A$.","In the common case when $A$ exhibits fast singular value decay, our bound can be much stronger than prior work, which gives an error bound of $\\epsilon \\cdot ||A||_2$ using $O(1/\\epsilon)$ matrix-vector products.","We also show that it is nearly tight: any algorithm giving error $\\epsilon \\cdot \\sigma_\\ell(A)$ must use $\\Omega(\\ell+1/\\epsilon)$ matrix-vector products.   ","We further show that the popular Stochastic Lanczos Quadrature (SLQ) method matches the above bound, even though SLQ itself is parameter-free and performs no explicit deflation.","This bound explains the strong practical performance of SLQ, and motivates a simple variant of SLQ that achieves an even tighter error bound.","Our error bound for SLQ leverages an analysis that views it as an implicit polynomial moment matching method, along with recent results on low-rank approximation with single-vector Krylov methods.","We use these results to show that the method can perform implicit deflation as part of moment matching."],"url":"http://arxiv.org/abs/2410.21690v1"}
{"created":"2024-10-29 03:07:33","title":"Pushing the Limits of All-Atom Geometric Graph Neural Networks: Pre-Training, Scaling and Zero-Shot Transfer","abstract":"Constructing transferable descriptors for conformation representation of molecular and biological systems finds numerous applications in drug discovery, learning-based molecular dynamics, and protein mechanism analysis. Geometric graph neural networks (Geom-GNNs) with all-atom information have transformed atomistic simulations by serving as a general learnable geometric descriptors for downstream tasks including prediction of interatomic potential and molecular properties. However, common practices involve supervising Geom-GNNs on specific downstream tasks, which suffer from the lack of high-quality data and inaccurate labels leading to poor generalization and performance degradation on out-of-distribution (OOD) scenarios. In this work, we explored the possibility of using pre-trained Geom-GNNs as transferable and highly effective geometric descriptors for improved generalization. To explore their representation power, we studied the scaling behaviors of Geom-GNNs under self-supervised pre-training, supervised and unsupervised learning setups. We find that the expressive power of different architectures can differ on the pre-training task. Interestingly, Geom-GNNs do not follow the power-law scaling on the pre-training task, and universally lack predictable scaling behavior on the supervised tasks with quantum chemical labels important for screening and design of novel molecules. More importantly, we demonstrate how all-atom graph embedding can be organically combined with other neural architectures to enhance the expressive power. Meanwhile, the low-dimensional projection of the latent space shows excellent agreement with conventional geometrical descriptors.","sentences":["Constructing transferable descriptors for conformation representation of molecular and biological systems finds numerous applications in drug discovery, learning-based molecular dynamics, and protein mechanism analysis.","Geometric graph neural networks (Geom-GNNs) with all-atom information have transformed atomistic simulations by serving as a general learnable geometric descriptors for downstream tasks including prediction of interatomic potential and molecular properties.","However, common practices involve supervising Geom-GNNs on specific downstream tasks, which suffer from the lack of high-quality data and inaccurate labels leading to poor generalization and performance degradation on out-of-distribution (OOD) scenarios.","In this work, we explored the possibility of using pre-trained Geom-GNNs as transferable and highly effective geometric descriptors for improved generalization.","To explore their representation power, we studied the scaling behaviors of Geom-GNNs under self-supervised pre-training, supervised and unsupervised learning setups.","We find that the expressive power of different architectures can differ on the pre-training task.","Interestingly, Geom-GNNs do not follow the power-law scaling on the pre-training task, and universally lack predictable scaling behavior on the supervised tasks with quantum chemical labels important for screening and design of novel molecules.","More importantly, we demonstrate how all-atom graph embedding can be organically combined with other neural architectures to enhance the expressive power.","Meanwhile, the low-dimensional projection of the latent space shows excellent agreement with conventional geometrical descriptors."],"url":"http://arxiv.org/abs/2410.21683v1"}
{"created":"2024-10-29 03:02:53","title":"Revisiting Reliability in Large-Scale Machine Learning Research Clusters","abstract":"Reliability is a fundamental challenge in operating large-scale machine learning (ML) infrastructures, particularly as the scale of ML models and training clusters continues to grow. Despite decades of research on infrastructure failures, the impact of job failures across different scales remains unclear. This paper presents a view of managing two large, multi-tenant ML clusters, providing quantitative analysis, operational experience, and our own perspective in understanding and addressing reliability concerns at scale. Our analysis reveals that while large jobs are most vulnerable to failures, smaller jobs make up the majority of jobs in the clusters and should be incorporated into optimization objectives. We identify key workload properties, compare them across clusters, and demonstrate essential reliability requirements for pushing the boundaries of ML training at scale.   We hereby introduce a taxonomy of failures and key reliability metrics, analyze 11 months of data from two state-of-the-art ML environments with over 150 million A100 GPU hours and 4 million jobs. Building on our data, we fit a failure model to project Mean Time to Failure for various GPU scales. We further propose a method to estimate a related metric, Effective Training Time Ratio, as a function of job parameters, and we use this model to gauge the efficacy of potential software mitigations at scale. Our work provides valuable insights and future research directions for improving the reliability of AI supercomputer clusters, emphasizing the need for flexible, workload-agnostic, and reliability-aware infrastructure, system software, and algorithms.","sentences":["Reliability is a fundamental challenge in operating large-scale machine learning (ML) infrastructures, particularly as the scale of ML models and training clusters continues to grow.","Despite decades of research on infrastructure failures, the impact of job failures across different scales remains unclear.","This paper presents a view of managing two large, multi-tenant ML clusters, providing quantitative analysis, operational experience, and our own perspective in understanding and addressing reliability concerns at scale.","Our analysis reveals that while large jobs are most vulnerable to failures, smaller jobs make up the majority of jobs in the clusters and should be incorporated into optimization objectives.","We identify key workload properties, compare them across clusters, and demonstrate essential reliability requirements for pushing the boundaries of ML training at scale.   ","We hereby introduce a taxonomy of failures and key reliability metrics, analyze 11 months of data from two state-of-the-art ML environments with over 150 million A100 GPU hours and 4 million jobs.","Building on our data, we fit a failure model to project Mean Time to Failure for various GPU scales.","We further propose a method to estimate a related metric, Effective Training Time Ratio, as a function of job parameters, and we use this model to gauge the efficacy of potential software mitigations at scale.","Our work provides valuable insights and future research directions for improving the reliability of AI supercomputer clusters, emphasizing the need for flexible, workload-agnostic, and reliability-aware infrastructure, system software, and algorithms."],"url":"http://arxiv.org/abs/2410.21680v1"}
{"created":"2024-10-29 02:54:06","title":"How Does Critical Batch Size Scale in Pre-training?","abstract":"Training large-scale models under given resources requires careful design of parallelism strategies. In particular, the efficiency notion of critical batch size, concerning the compromise between time and compute, marks the threshold beyond which greater data parallelism leads to diminishing returns. To operationalize it, we propose a measure of CBS and pre-train a series of auto-regressive language models, ranging from 85 million to 1.2 billion parameters, on the C4 dataset. Through extensive hyper-parameter sweeps and careful control on factors such as batch size, momentum, and learning rate along with its scheduling, we systematically investigate the impact of scale on CBS. Then we fit scaling laws with respect to model and data sizes to decouple their effects. Overall, our results demonstrate that CBS scales primarily with data size rather than model size, a finding we justify theoretically through the analysis of infinite-width limits of neural networks and infinite-dimensional least squares regression. Of independent interest, we highlight the importance of common hyper-parameter choices and strategies for studying large-scale pre-training beyond fixed training durations.","sentences":["Training large-scale models under given resources requires careful design of parallelism strategies.","In particular, the efficiency notion of critical batch size, concerning the compromise between time and compute, marks the threshold beyond which greater data parallelism leads to diminishing returns.","To operationalize it, we propose a measure of CBS and pre-train a series of auto-regressive language models, ranging from 85 million to 1.2 billion parameters, on the C4 dataset.","Through extensive hyper-parameter sweeps and careful control on factors such as batch size, momentum, and learning rate along with its scheduling, we systematically investigate the impact of scale on CBS.","Then we fit scaling laws with respect to model and data sizes to decouple their effects.","Overall, our results demonstrate that CBS scales primarily with data size rather than model size, a finding we justify theoretically through the analysis of infinite-width limits of neural networks and infinite-dimensional least squares regression.","Of independent interest, we highlight the importance of common hyper-parameter choices and strategies for studying large-scale pre-training beyond fixed training durations."],"url":"http://arxiv.org/abs/2410.21676v1"}
{"created":"2024-10-29 02:48:41","title":"Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review","abstract":"Public Code Review (PCR) is an assistant to the internal code review of the development team, in the form of a public Software Question Answering (SQA) community, to help developers access high-quality and efficient review services. Current methods on PCR mainly focus on the reviewer's perspective, including finding a capable reviewer, predicting comment quality, and recommending/generating review comments. However, it is not well studied that how to satisfy the review necessity requests posted by developers which can increase their visibility, which in turn acts as a prerequisite for better review responses. To this end, we propose a Knowledge-guided Prompt learning for Public Code Review (KP-PCR) to achieve developer-based code review request quality assurance (i.e., predicting request necessity and recommending tags subtask). Specifically, we reformulate the two subtasks via 1) text prompt tuning which converts both of them into a Masked Language Model (MLM) by constructing prompt templates using hard prompt; 2) knowledge and code prefix tuning which introduces external knowledge by soft prompt, and uses data flow diagrams to characterize code snippets. Finally, both of the request necessity prediction and tag recommendation subtasks output predicted results through an answer engineering module. In addition, we further analysis the time complexity of our KP-PCR that has lightweight prefix based the operation of introducing knowledge. Experimental results on the PCR dataset for the period 2011-2023 demonstrate that our KP-PCR outperforms baselines by 8.3%-28.8% in the request necessity prediction and by 0.1%-29.5% in the tag recommendation. The code implementation is released at https://github.com/WUT-IDEA/KP-PCR.","sentences":["Public Code Review (PCR) is an assistant to the internal code review of the development team, in the form of a public Software Question Answering (SQA) community, to help developers access high-quality and efficient review services.","Current methods on PCR mainly focus on the reviewer's perspective, including finding a capable reviewer, predicting comment quality, and recommending/generating review comments.","However, it is not well studied that how to satisfy the review necessity requests posted by developers which can increase their visibility, which in turn acts as a prerequisite for better review responses.","To this end, we propose a Knowledge-guided Prompt learning for Public Code Review (KP-PCR) to achieve developer-based code review request quality assurance (i.e., predicting request necessity and recommending tags subtask).","Specifically, we reformulate the two subtasks via 1) text prompt tuning which converts both of them into a Masked Language Model (MLM) by constructing prompt templates using hard prompt; 2) knowledge and code prefix tuning which introduces external knowledge by soft prompt, and uses data flow diagrams to characterize code snippets.","Finally, both of the request necessity prediction and tag recommendation subtasks output predicted results through an answer engineering module.","In addition, we further analysis the time complexity of our KP-PCR that has lightweight prefix based the operation of introducing knowledge.","Experimental results on the PCR dataset for the period 2011-2023 demonstrate that our KP-PCR outperforms baselines by 8.3%-28.8% in the request necessity prediction and by 0.1%-29.5% in the tag recommendation.","The code implementation is released at https://github.com/WUT-IDEA/KP-PCR."],"url":"http://arxiv.org/abs/2410.21673v1"}
{"created":"2024-10-29 02:35:21","title":"Sequential choice in ordered bundles","abstract":"Experience goods such as sporting and artistic events, songs, videos, news stories, podcasts, and television series, are often packaged and consumed in bundles. Many such bundles are ordered in the sense that the individual items are consumed sequentially, one at a time. We examine if an individual's decision to consume the next item in an ordered bundle can be predicted based on his/her consumption pattern for the preceding items. We evaluate several predictive models, including two custom Transformers using decoder-only and encoder-decoder architectures, fine-tuned GPT-3, a custom LSTM model, a reinforcement learning model, two Markov models, and a zero-order model. Using data from Spotify, we find that the custom Transformer with a decoder-only architecture provides the most accurate predictions, both for individual choices and aggregate demand. This model captures a general form of state dependence. Analysis of Transformer attention weights suggests that the consumption of the next item in a bundle is based on approximately equal weighting of all preceding choices. Our results indicate that the Transformer can assist in queuing the next item that an individual is likely to consume from an ordered bundle, predicting the demand for individual items, and personalizing promotions to increase demand.","sentences":["Experience goods such as sporting and artistic events, songs, videos, news stories, podcasts, and television series, are often packaged and consumed in bundles.","Many such bundles are ordered in the sense that the individual items are consumed sequentially, one at a time.","We examine if an individual's decision to consume the next item in an ordered bundle can be predicted based on his/her consumption pattern for the preceding items.","We evaluate several predictive models, including two custom Transformers using decoder-only and encoder-decoder architectures, fine-tuned GPT-3, a custom LSTM model, a reinforcement learning model, two Markov models, and a zero-order model.","Using data from Spotify, we find that the custom Transformer with a decoder-only architecture provides the most accurate predictions, both for individual choices and aggregate demand.","This model captures a general form of state dependence.","Analysis of Transformer attention weights suggests that the consumption of the next item in a bundle is based on approximately equal weighting of all preceding choices.","Our results indicate that the Transformer can assist in queuing the next item that an individual is likely to consume from an ordered bundle, predicting the demand for individual items, and personalizing promotions to increase demand."],"url":"http://arxiv.org/abs/2410.21670v1"}
{"created":"2024-10-29 02:34:06","title":"Investigating Memorization in Video Diffusion Models","abstract":"Diffusion models, widely used for image and video generation, face a significant limitation: the risk of memorizing and reproducing training data during inference, potentially generating unauthorized copyrighted content. While prior research has focused on image diffusion models (IDMs), video diffusion models (VDMs) remain underexplored. To address this gap, we first formally define the two types of memorization in VDMs (content memorization and motion memorization) in a practical way that focuses on privacy preservation and applies to all generation types. We then introduce new metrics specifically designed to separately assess content and motion memorization in VDMs. Additionally, we curate a dataset of text prompts that are most prone to triggering memorization when used as conditioning in VDMs. By leveraging these prompts, we generate diverse videos from various open-source VDMs, successfully extracting numerous training videos from each tested model. Through the application of our proposed metrics, we systematically analyze memorization across various pretrained VDMs, including text-conditional and unconditional models, on a variety of datasets. Our comprehensive study reveals that memorization is widespread across all tested VDMs, indicating that VDMs can also memorize image training data in addition to video datasets. Finally, we propose efficient and effective detection strategies for both content and motion memorization, offering a foundational approach for improving privacy in VDMs.","sentences":["Diffusion models, widely used for image and video generation, face a significant limitation: the risk of memorizing and reproducing training data during inference, potentially generating unauthorized copyrighted content.","While prior research has focused on image diffusion models (IDMs), video diffusion models (VDMs) remain underexplored.","To address this gap, we first formally define the two types of memorization in VDMs (content memorization and motion memorization) in a practical way that focuses on privacy preservation and applies to all generation types.","We then introduce new metrics specifically designed to separately assess content and motion memorization in VDMs.","Additionally, we curate a dataset of text prompts that are most prone to triggering memorization when used as conditioning in VDMs.","By leveraging these prompts, we generate diverse videos from various open-source VDMs, successfully extracting numerous training videos from each tested model.","Through the application of our proposed metrics, we systematically analyze memorization across various pretrained VDMs, including text-conditional and unconditional models, on a variety of datasets.","Our comprehensive study reveals that memorization is widespread across all tested VDMs, indicating that VDMs can also memorize image training data in addition to video datasets.","Finally, we propose efficient and effective detection strategies for both content and motion memorization, offering a foundational approach for improving privacy in VDMs."],"url":"http://arxiv.org/abs/2410.21669v1"}
{"created":"2024-10-29 02:16:01","title":"Exploring Local Memorization in Diffusion Models via Bright Ending Attention","abstract":"In this paper, we identify and leverage a novel `bright ending' (BE) anomaly in diffusion models prone to memorizing training images to address a new task: locating localized memorization regions within these models. BE refers to a distinct cross-attention pattern observed in text-to-image generations using diffusion models. Specifically, memorized image patches exhibit significantly greater attention to the end token during the final inference step compared to non-memorized patches. This attention map effectively highlights regions where the generated image replicates training data. Furthermore, driven by our observation that local memorization significantly underperforms in existing tasks of measuring, detecting, and mitigating memorization in diffusion models compared to global memorization, we propose a simple yet effective method to integrate BE and the results of the new localization task into these existing frameworks. This integration effectively improves their performances by narrowing the performance gap caused by local memorization. Our results not only demonstrate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon.","sentences":["In this paper, we identify and leverage a novel `bright ending' (BE) anomaly in diffusion models prone to memorizing training images to address a new task: locating localized memorization regions within these models.","BE refers to a distinct cross-attention pattern observed in text-to-image generations using diffusion models.","Specifically, memorized image patches exhibit significantly greater attention to the end token during the final inference step compared to non-memorized patches.","This attention map effectively highlights regions where the generated image replicates training data.","Furthermore, driven by our observation that local memorization significantly underperforms in existing tasks of measuring, detecting, and mitigating memorization in diffusion models compared to global memorization, we propose a simple yet effective method to integrate BE and the results of the new localization task into these existing frameworks.","This integration effectively improves their performances by narrowing the performance gap caused by local memorization.","Our results not only demonstrate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon."],"url":"http://arxiv.org/abs/2410.21665v1"}
{"created":"2024-10-29 00:54:00","title":"Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis","abstract":"Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis. Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose. For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually. While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes. To overcome these limitations, this work introduces a new family of \\textit{Factor Graph Diffusion Models} (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition. This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, text-based editing of conditions with noise inversion, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure. We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\\% higher recall than SD while retaining its generalization ability. We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image.","sentences":["Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis.","Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose.","For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually.","While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes.","To overcome these limitations, this work introduces a new family of \\textit{Factor Graph Diffusion Models} (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition.","This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, text-based editing of conditions with noise inversion, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure.","We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\\% higher recall than SD while retaining its generalization ability.","We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image."],"url":"http://arxiv.org/abs/2410.21638v1"}
{"created":"2024-10-28 23:47:43","title":"Identifying Selections for Unsupervised Subtask Discovery","abstract":"When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a $\\textit{selection}$ mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at https://anonymous.4open.science/r/Identifying\\_Selections\\_for\\_Unsupervised\\_Subtask\\_Discovery/README.md.","sentences":["When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks.","Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems.","However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a $\\textit{selection}$ mechanism on actions, rather than possible underlying confounders or intermediates.","Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data.","These selections serve as subgoals that indicate subtasks and guide policy.","In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks.","Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios.","The codes are provided at https://anonymous.4open.science/r/Identifying\\_Selections\\_for\\_Unsupervised\\_Subtask\\_Discovery/README.md."],"url":"http://arxiv.org/abs/2410.21616v1"}
