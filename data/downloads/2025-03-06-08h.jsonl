{"created":"2025-03-05 18:58:58","title":"PacketCLIP: Multi-Modal Embedding of Network Traffic and Language for Cybersecurity Reasoning","abstract":"Traffic classification is vital for cybersecurity, yet encrypted traffic poses significant challenges. We present PacketCLIP, a multi-modal framework combining packet data with natural language semantics through contrastive pretraining and hierarchical Graph Neural Network (GNN) reasoning. PacketCLIP integrates semantic reasoning with efficient classification, enabling robust detection of anomalies in encrypted network flows. By aligning textual descriptions with packet behaviors, it offers enhanced interpretability, scalability, and practical applicability across diverse security scenarios. PacketCLIP achieves a 95% mean AUC, outperforms baselines by 11.6%, and reduces model size by 92%, making it ideal for real-time anomaly detection. By bridging advanced machine learning techniques and practical cybersecurity needs, PacketCLIP provides a foundation for scalable, efficient, and interpretable solutions to tackle encrypted traffic classification and network intrusion detection challenges in resource-constrained environments.","sentences":["Traffic classification is vital for cybersecurity, yet encrypted traffic poses significant challenges.","We present PacketCLIP, a multi-modal framework combining packet data with natural language semantics through contrastive pretraining and hierarchical Graph Neural Network (GNN) reasoning.","PacketCLIP integrates semantic reasoning with efficient classification, enabling robust detection of anomalies in encrypted network flows.","By aligning textual descriptions with packet behaviors, it offers enhanced interpretability, scalability, and practical applicability across diverse security scenarios.","PacketCLIP achieves a 95% mean AUC, outperforms baselines by 11.6%, and reduces model size by 92%, making it ideal for real-time anomaly detection.","By bridging advanced machine learning techniques and practical cybersecurity needs, PacketCLIP provides a foundation for scalable, efficient, and interpretable solutions to tackle encrypted traffic classification and network intrusion detection challenges in resource-constrained environments."],"url":"http://arxiv.org/abs/2503.03747v1"}
{"created":"2025-03-05 18:58:44","title":"Process-based Self-Rewarding Language Models","abstract":"Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.","sentences":["Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios.","Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance.","Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs.","However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance.","In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm.","Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities."],"url":"http://arxiv.org/abs/2503.03746v1"}
{"created":"2025-03-05 18:37:52","title":"Graph-Augmented LSTM for Forecasting Sparse Anomalies in Graph-Structured Time Series","abstract":"Detecting anomalies in time series data is a critical task across many domains. The challenge intensifies when anomalies are sparse and the data are multivariate with relational dependencies across sensors or nodes. Traditional univariate anomaly detectors struggle to capture such cross-node dependencies, particularly in sparse anomaly settings. To address this, we propose a graph-augmented time series forecasting approach that explicitly integrates the graph of relationships among time series into an LSTM forecasting model. This enables the model to detect rare anomalies that might otherwise go unnoticed in purely univariate approaches. We evaluate the approach on two benchmark datasets - the Yahoo Webscope S5 anomaly dataset and the METR-LA traffic sensor network - and compare the performance of the Graph-Augmented LSTM against LSTM-only, ARIMA, and Prophet baselines. Results demonstrate that the graph-augmented model achieves significantly higher precision and recall, improving F1-score by up to 10% over the best baseline","sentences":["Detecting anomalies in time series data is a critical task across many domains.","The challenge intensifies when anomalies are sparse and the data are multivariate with relational dependencies across sensors or nodes.","Traditional univariate anomaly detectors struggle to capture such cross-node dependencies, particularly in sparse anomaly settings.","To address this, we propose a graph-augmented time series forecasting approach that explicitly integrates the graph of relationships among time series into an LSTM forecasting model.","This enables the model to detect rare anomalies that might otherwise go unnoticed in purely univariate approaches.","We evaluate the approach on two benchmark datasets - the Yahoo Webscope S5 anomaly dataset and the METR-LA traffic sensor network - and compare the performance of the Graph-Augmented LSTM against LSTM-only, ARIMA, and Prophet baselines.","Results demonstrate that the graph-augmented model achieves significantly higher precision and recall, improving F1-score by up to 10% over the best baseline"],"url":"http://arxiv.org/abs/2503.03729v1"}
{"created":"2025-03-05 18:21:34","title":"When Radiation Meets Linux: Analyzing Soft Errors in Linux on COTS SoCs under Proton Irradiation","abstract":"The increasing use of Linux on commercial off-the-shelf (COTS) system-on-chip (SoC) in spaceborne computing inherits COTS susceptibility to radiation-induced failures like soft errors. Modern SoCs exacerbate this issue as aggressive transistor scaling reduces critical charge thresholds to induce soft errors and increases radiation effects within densely packed transistors, degrading overall reliability. Linux's monolithic architecture amplifies these risks, as tightly coupled kernel subsystems propagate errors to critical components (e.g., memory management), while limited error-correcting code (ECC) offers minimal mitigation. Furthermore, the lack of public soft error data from irradiation tests on COTS SoCs running Linux hinders reliability improvements. This study evaluates proton irradiation effects (20-50 MeV) on Linux across three COTS SoC architectures: Raspberry Pi Zero 2 W (40 nm CMOS, Cortex-A53), NXP i.MX 8M Plus (14 nm FinFET, Cortex-A53), and OrangeCrab (40 nm FPGA, RISC-V). Irradiation results show the 14 nm FinFET NXP SoC achieved 2-3x longer Linux uptime without ECC memory versus both 40 nm CMOS counterparts, partially due to FinFET's reduced charge collection. Additionally, this work presents the first cross-architecture analysis of soft error-prone Linux kernel components in modern SoCs to develop targeted mitigations. The findings establish foundational data on Linux's soft error sensitivity in COTS SoCs, guiding mission readiness for space applications.","sentences":["The increasing use of Linux on commercial off-the-shelf (COTS) system-on-chip (SoC) in spaceborne computing inherits COTS susceptibility to radiation-induced failures like soft errors.","Modern SoCs exacerbate this issue as aggressive transistor scaling reduces critical charge thresholds to induce soft errors and increases radiation effects within densely packed transistors, degrading overall reliability.","Linux's monolithic architecture amplifies these risks, as tightly coupled kernel subsystems propagate errors to critical components (e.g., memory management), while limited error-correcting code (ECC) offers minimal mitigation.","Furthermore, the lack of public soft error data from irradiation tests on COTS SoCs running Linux hinders reliability improvements.","This study evaluates proton irradiation effects (20-50 MeV) on Linux across three COTS SoC architectures: Raspberry Pi Zero 2 W (40 nm CMOS, Cortex-A53), NXP i.MX 8M Plus (14 nm FinFET, Cortex-A53), and OrangeCrab (40 nm FPGA, RISC-V).","Irradiation results show the 14 nm FinFET NXP SoC achieved 2-3x longer Linux uptime without ECC memory versus both 40 nm CMOS counterparts, partially due to FinFET's reduced charge collection.","Additionally, this work presents the first cross-architecture analysis of soft error-prone Linux kernel components in modern SoCs to develop targeted mitigations.","The findings establish foundational data on Linux's soft error sensitivity in COTS SoCs, guiding mission readiness for space applications."],"url":"http://arxiv.org/abs/2503.03722v1"}
{"created":"2025-03-05 18:10:11","title":"Machine Learning in Biomechanics: Key Applications and Limitations in Walking, Running, and Sports Movements","abstract":"This chapter provides an overview of recent and promising Machine Learning applications, i.e. pose estimation, feature estimation, event detection, data exploration & clustering, and automated classification, in gait (walking and running) and sports biomechanics. It explores the potential of Machine Learning methods to address challenges in biomechanical workflows, highlights central limitations, i.e. data and annotation availability and explainability, that need to be addressed, and emphasises the importance of interdisciplinary approaches for fully harnessing the potential of Machine Learning in gait and sports biomechanics.","sentences":["This chapter provides an overview of recent and promising Machine Learning applications, i.e. pose estimation, feature estimation, event detection, data exploration & clustering, and automated classification, in gait (walking and running) and sports biomechanics.","It explores the potential of Machine Learning methods to address challenges in biomechanical workflows, highlights central limitations, i.e. data and annotation availability and explainability, that need to be addressed, and emphasises the importance of interdisciplinary approaches for fully harnessing the potential of Machine Learning in gait and sports biomechanics."],"url":"http://arxiv.org/abs/2503.03717v1"}
{"created":"2025-03-05 18:04:30","title":"Handling Uncertainty in Health Data using Generative Algorithms","abstract":"Understanding and managing uncertainty is crucial in machine learning, especially in high-stakes domains like healthcare, where class imbalance can impact predictions. This paper introduces RIGA, a novel pipeline that mitigates class imbalance using generative AI. By converting tabular healthcare data into images, RIGA leverages models like cGAN, VQVAE, and VQGAN to generate balanced samples, improving classification performance. These representations are processed by CNNs and later transformed back into tabular format for seamless integration. This approach enhances traditional classifiers like XGBoost, improves Bayesian structure learning, and strengthens ML model robustness by generating realistic synthetic data for underrepresented classes.","sentences":["Understanding and managing uncertainty is crucial in machine learning, especially in high-stakes domains like healthcare, where class imbalance can impact predictions.","This paper introduces RIGA, a novel pipeline that mitigates class imbalance using generative AI.","By converting tabular healthcare data into images, RIGA leverages models like cGAN, VQVAE, and VQGAN to generate balanced samples, improving classification performance.","These representations are processed by CNNs and later transformed back into tabular format for seamless integration.","This approach enhances traditional classifiers like XGBoost, improves Bayesian structure learning, and strengthens ML model robustness by generating realistic synthetic data for underrepresented classes."],"url":"http://arxiv.org/abs/2503.03715v1"}
{"created":"2025-03-05 17:58:16","title":"Curating Demonstrations using Online Experience","abstract":"Many robot demonstration datasets contain heterogeneous demonstrations of varying quality. This heterogeneity may benefit policy pre-training, but can hinder robot performance when used with a final imitation learning objective. In particular, some strategies in the data may be less reliable than others or may be underrepresented in the data, leading to poor performance when such strategies are sampled at test time. Moreover, such unreliable or underrepresented strategies can be difficult even for people to discern, and sifting through demonstration datasets is time-consuming and costly. On the other hand, policy performance when trained on such demonstrations can reflect the reliability of different strategies. We thus propose for robots to self-curate based on online robot experience (Demo-SCORE). More specifically, we train and cross-validate a classifier to discern successful policy roll-outs from unsuccessful ones and use the classifier to filter heterogeneous demonstration datasets. Our experiments in simulation and the real world show that Demo-SCORE can effectively identify suboptimal demonstrations without manual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute success rate in the resulting policy compared to the base policy trained with all original demonstrations.","sentences":["Many robot demonstration datasets contain heterogeneous demonstrations of varying quality.","This heterogeneity may benefit policy pre-training, but can hinder robot performance when used with a final imitation learning objective.","In particular, some strategies in the data may be less reliable than others or may be underrepresented in the data, leading to poor performance when such strategies are sampled at test time.","Moreover, such unreliable or underrepresented strategies can be difficult even for people to discern, and sifting through demonstration datasets is time-consuming and costly.","On the other hand, policy performance when trained on such demonstrations can reflect the reliability of different strategies.","We thus propose for robots to self-curate based on online robot experience (Demo-SCORE).","More specifically, we train and cross-validate a classifier to discern successful policy roll-outs from unsuccessful ones and use the classifier to filter heterogeneous demonstration datasets.","Our experiments in simulation and the real world show that Demo-SCORE can effectively identify suboptimal demonstrations without manual curation.","Notably, Demo-SCORE achieves over 15-35% higher absolute success rate in the resulting policy compared to the base policy trained with all original demonstrations."],"url":"http://arxiv.org/abs/2503.03707v1"}
{"created":"2025-03-05 17:56:49","title":"An Automated Computational Pipeline for Generating Large-Scale Cohorts of Patient-Specific Ventricular Models in Electromechanical In Silico Trials","abstract":"In recent years, human in silico trials have gained significant traction as a powerful approach to evaluate the effects of drugs, clinical interventions, and medical devices. In silico trials not only minimise patient risks but also reduce reliance on animal testing. However, the implementation of in silico trials presents several time-consuming challenges. It requires the creation of large cohorts of virtual patients. Each virtual patient is described by their anatomy with a volumetric mesh and electrophysiological and mechanical dynamics through mathematical equations and parameters. Furthermore, simulated conditions need definition including stimulation protocols and therapy evaluation. For large virtual cohorts, this requires automatic and efficient pipelines for generation of corresponding files. In this work, we present a computational pipeline to automatically create large virtual patient cohort files to conduct large-scale in silico trials through cardiac electromechanical simulations. The pipeline generates the files describing meshes, labels, and data required for the simulations directly from unprocessed surface meshes. We applied the pipeline to generate over 100 virtual patients from various datasets and performed simulations to demonstrate capacity to conduct in silico trials for virtual patients using verified and validated electrophysiology and electromechanics models for the context of use. The proposed pipeline is adaptable to accommodate different types of ventricular geometries and mesh processing tools, ensuring its versatility in handling diverse clinical datasets. By establishing an automated framework for large scale simulation studies as required for in silico trials and providing open-source code, our work aims to support scalable, personalised cardiac simulations in research and clinical applications.","sentences":["In recent years, human in silico trials have gained significant traction as a powerful approach to evaluate the effects of drugs, clinical interventions, and medical devices.","In silico trials not only minimise patient risks but also reduce reliance on animal testing.","However, the implementation of in silico trials presents several time-consuming challenges.","It requires the creation of large cohorts of virtual patients.","Each virtual patient is described by their anatomy with a volumetric mesh and electrophysiological and mechanical dynamics through mathematical equations and parameters.","Furthermore, simulated conditions need definition including stimulation protocols and therapy evaluation.","For large virtual cohorts, this requires automatic and efficient pipelines for generation of corresponding files.","In this work, we present a computational pipeline to automatically create large virtual patient cohort files to conduct large-scale in silico trials through cardiac electromechanical simulations.","The pipeline generates the files describing meshes, labels, and data required for the simulations directly from unprocessed surface meshes.","We applied the pipeline to generate over 100 virtual patients from various datasets and performed simulations to demonstrate capacity to conduct in silico trials for virtual patients using verified and validated electrophysiology and electromechanics models for the context of use.","The proposed pipeline is adaptable to accommodate different types of ventricular geometries and mesh processing tools, ensuring its versatility in handling diverse clinical datasets.","By establishing an automated framework for large scale simulation studies as required for in silico trials and providing open-source code, our work aims to support scalable, personalised cardiac simulations in research and clinical applications."],"url":"http://arxiv.org/abs/2503.03706v1"}
{"created":"2025-03-05 17:56:20","title":"Effective LLM Knowledge Learning via Model Generalization","abstract":"Large language models (LLMs) are trained on enormous documents that contain extensive world knowledge. However, it is still not well-understood how knowledge is acquired via autoregressive pre-training. This lack of understanding greatly hinders effective knowledge learning, especially for continued pretraining on up-to-date information, as this evolving information often lacks diverse repetitions like foundational knowledge. In this paper, we focus on understanding and improving LLM knowledge learning. We found and verified that knowledge learning for LLMs can be deemed as an implicit supervised task hidden in the autoregressive pre-training objective. Our findings suggest that knowledge learning for LLMs would benefit from methods designed to improve generalization ability for supervised tasks. Based on our analysis, we propose the formatting-based data augmentation to grow in-distribution samples, which does not present the risk of altering the facts embedded in documents as text paraphrasing. We also introduce sharpness-aware minimization as an effective optimization algorithm to better improve generalization. Moreover, our analysis and method can be readily extended to instruction tuning. Extensive experiment results validate our findings and demonstrate our methods' effectiveness in both continued pre-training and instruction tuning. This paper offers new perspectives and insights to interpret and design effective strategies for LLM knowledge learning.","sentences":["Large language models (LLMs) are trained on enormous documents that contain extensive world knowledge.","However, it is still not well-understood how knowledge is acquired via autoregressive pre-training.","This lack of understanding greatly hinders effective knowledge learning, especially for continued pretraining on up-to-date information, as this evolving information often lacks diverse repetitions like foundational knowledge.","In this paper, we focus on understanding and improving LLM knowledge learning.","We found and verified that knowledge learning for LLMs can be deemed as an implicit supervised task hidden in the autoregressive pre-training objective.","Our findings suggest that knowledge learning for LLMs would benefit from methods designed to improve generalization ability for supervised tasks.","Based on our analysis, we propose the formatting-based data augmentation to grow in-distribution samples, which does not present the risk of altering the facts embedded in documents as text paraphrasing.","We also introduce sharpness-aware minimization as an effective optimization algorithm to better improve generalization.","Moreover, our analysis and method can be readily extended to instruction tuning.","Extensive experiment results validate our findings and demonstrate our methods' effectiveness in both continued pre-training and instruction tuning.","This paper offers new perspectives and insights to interpret and design effective strategies for LLM knowledge learning."],"url":"http://arxiv.org/abs/2503.03705v1"}
{"created":"2025-03-05 17:53:07","title":"Developing and Utilizing a Large-Scale Cantonese Dataset for Multi-Tasking in Large Language Models","abstract":"High-quality data resources play a crucial role in learning large language models (LLMs), particularly for low-resource languages like Cantonese. Despite having more than 85 million native speakers, Cantonese is still considered a low-resource language in the field of natural language processing (NLP) due to factors such as the dominance of Mandarin, lack of cohesion within the Cantonese-speaking community, diversity in character encoding and input methods, and the tendency of overseas Cantonese speakers to prefer using English. In addition, rich colloquial vocabulary of Cantonese, English loanwords, and code-switching characteristics add to the complexity of corpus collection and processing. To address these challenges, we collect Cantonese texts from a variety of sources, including open source corpora, Hong Kong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous data processing through language filtering, quality filtering, content filtering, and de-duplication steps, successfully constructing a high-quality Cantonese corpus of over 2 billion tokens for training large language models. We further refined the model through supervised fine-tuning (SFT) on curated Cantonese tasks, enhancing its ability to handle specific applications. Upon completion of the training, the model achieves state-of-the-art (SOTA) performance on four Cantonese benchmarks. After training on our dataset, the model also exhibits improved performance on other mainstream language tasks.","sentences":["High-quality data resources play a crucial role in learning large language models (LLMs), particularly for low-resource languages like Cantonese.","Despite having more than 85 million native speakers, Cantonese is still considered a low-resource language in the field of natural language processing (NLP) due to factors such as the dominance of Mandarin, lack of cohesion within the Cantonese-speaking community, diversity in character encoding and input methods, and the tendency of overseas Cantonese speakers to prefer using English.","In addition, rich colloquial vocabulary of Cantonese, English loanwords, and code-switching characteristics add to the complexity of corpus collection and processing.","To address these challenges, we collect Cantonese texts from a variety of sources, including open source corpora, Hong Kong-specific forums, Wikipedia, and Common Crawl data.","We conduct rigorous data processing through language filtering, quality filtering, content filtering, and de-duplication steps, successfully constructing a high-quality Cantonese corpus of over 2 billion tokens for training large language models.","We further refined the model through supervised fine-tuning (SFT) on curated Cantonese tasks, enhancing its ability to handle specific applications.","Upon completion of the training, the model achieves state-of-the-art (SOTA) performance on four Cantonese benchmarks.","After training on our dataset, the model also exhibits improved performance on other mainstream language tasks."],"url":"http://arxiv.org/abs/2503.03702v1"}
{"created":"2025-03-05 17:28:16","title":"Addressing Overprescribing Challenges: Fine-Tuning Large Language Models for Medication Recommendation Tasks","abstract":"Medication recommendation systems have garnered attention within healthcare for their potential to deliver personalized and efficacious drug combinations based on patient's clinical data. However, existing methodologies encounter challenges in adapting to diverse Electronic Health Records (EHR) systems and effectively utilizing unstructured data, resulting in limited generalization capabilities and suboptimal performance. Recently, interest is growing in harnessing Large Language Models (LLMs) in the medical domain to support healthcare professionals and enhance patient care. Despite the emergence of medical LLMs and their promising results in tasks like medical question answering, their practical applicability in clinical settings, particularly in medication recommendation, often remains underexplored.   In this study, we evaluate both general-purpose and medical-specific LLMs for medication recommendation tasks. Our findings reveal that LLMs frequently encounter the challenge of overprescribing, leading to heightened clinical risks and diminished medication recommendation accuracy. To address this issue, we propose Language-Assisted Medication Recommendation (LAMO), which employs a parameter-efficient fine-tuning approach to tailor open-source LLMs for optimal performance in medication recommendation scenarios. LAMO leverages the wealth of clinical information within clinical notes, a resource often underutilized in traditional methodologies. As a result of our approach, LAMO outperforms previous state-of-the-art methods by over 10% in internal validation accuracy. Furthermore, temporal and external validations demonstrate LAMO's robust generalization capabilities across various temporal and hospital contexts. Additionally, an out-of-distribution medication recommendation experiment demonstrates LAMO's remarkable accuracy even with medications outside the training data.","sentences":["Medication recommendation systems have garnered attention within healthcare for their potential to deliver personalized and efficacious drug combinations based on patient's clinical data.","However, existing methodologies encounter challenges in adapting to diverse Electronic Health Records (EHR) systems and effectively utilizing unstructured data, resulting in limited generalization capabilities and suboptimal performance.","Recently, interest is growing in harnessing Large Language Models (LLMs) in the medical domain to support healthcare professionals and enhance patient care.","Despite the emergence of medical LLMs and their promising results in tasks like medical question answering, their practical applicability in clinical settings, particularly in medication recommendation, often remains underexplored.   ","In this study, we evaluate both general-purpose and medical-specific LLMs for medication recommendation tasks.","Our findings reveal that LLMs frequently encounter the challenge of overprescribing, leading to heightened clinical risks and diminished medication recommendation accuracy.","To address this issue, we propose Language-Assisted Medication Recommendation (LAMO), which employs a parameter-efficient fine-tuning approach to tailor open-source LLMs for optimal performance in medication recommendation scenarios.","LAMO leverages the wealth of clinical information within clinical notes, a resource often underutilized in traditional methodologies.","As a result of our approach, LAMO outperforms previous state-of-the-art methods by over 10% in internal validation accuracy.","Furthermore, temporal and external validations demonstrate LAMO's robust generalization capabilities across various temporal and hospital contexts.","Additionally, an out-of-distribution medication recommendation experiment demonstrates LAMO's remarkable accuracy even with medications outside the training data."],"url":"http://arxiv.org/abs/2503.03687v1"}
{"created":"2025-03-05 17:27:59","title":"MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems","abstract":"LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs. In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS. To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high effectiveness, efficiency and strong generalization ability. Code will be available at https://github.com/rui-ye/MAS-GPT.","sentences":["LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks.","However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs.","In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS.","To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs.","Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference.","The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses.","Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high effectiveness, efficiency and strong generalization ability.","Code will be available at https://github.com/rui-ye/MAS-GPT."],"url":"http://arxiv.org/abs/2503.03686v1"}
{"created":"2025-03-05 17:25:20","title":"Towards Trustworthy Federated Learning","abstract":"This paper develops a comprehensive framework to address three critical trustworthy challenges in federated learning (FL): robustness against Byzantine attacks, fairness, and privacy preservation. To improve the system's defense against Byzantine attacks that send malicious information to bias the system's performance, we develop a Two-sided Norm Based Screening (TNBS) mechanism, which allows the central server to crop the gradients that have the l lowest norms and h highest norms. TNBS functions as a screening tool to filter out potential malicious participants whose gradients are far from the honest ones. To promote egalitarian fairness, we adopt the q-fair federated learning (q-FFL). Furthermore, we adopt a differential privacy-based scheme to prevent raw data at local clients from being inferred by curious parties. Convergence guarantees are provided for the proposed framework under different scenarios. Experimental results on real datasets demonstrate that the proposed framework effectively improves robustness and fairness while managing the trade-off between privacy and accuracy. This work appears to be the first study that experimentally and theoretically addresses fairness, privacy, and robustness in trustworthy FL.","sentences":["This paper develops a comprehensive framework to address three critical trustworthy challenges in federated learning (FL): robustness against Byzantine attacks, fairness, and privacy preservation.","To improve the system's defense against Byzantine attacks that send malicious information to bias the system's performance, we develop a Two-sided Norm Based Screening (TNBS) mechanism, which allows the central server to crop the gradients that have the l lowest norms and h highest norms.","TNBS functions as a screening tool to filter out potential malicious participants whose gradients are far from the honest ones.","To promote egalitarian fairness, we adopt the q-fair federated learning (q-FFL).","Furthermore, we adopt a differential privacy-based scheme to prevent raw data at local clients from being inferred by curious parties.","Convergence guarantees are provided for the proposed framework under different scenarios.","Experimental results on real datasets demonstrate that the proposed framework effectively improves robustness and fairness while managing the trade-off between privacy and accuracy.","This work appears to be the first study that experimentally and theoretically addresses fairness, privacy, and robustness in trustworthy FL."],"url":"http://arxiv.org/abs/2503.03684v1"}
{"created":"2025-03-05 16:54:15","title":"A Generative Approach to High Fidelity 3D Reconstruction from Text Data","abstract":"The convergence of generative artificial intelligence and advanced computer vision technologies introduces a groundbreaking approach to transforming textual descriptions into three-dimensional representations. This research proposes a fully automated pipeline that seamlessly integrates text-to-image generation, various image processing techniques, and deep learning methods for reflection removal and 3D reconstruction. By leveraging state-of-the-art generative models like Stable Diffusion, the methodology translates natural language inputs into detailed 3D models through a multi-stage workflow.   The reconstruction process begins with the generation of high-quality images from textual prompts, followed by enhancement by a reinforcement learning agent and reflection removal using the Stable Delight model. Advanced image upscaling and background removal techniques are then applied to further enhance visual fidelity. These refined two-dimensional representations are subsequently transformed into volumetric 3D models using sophisticated machine learning algorithms, capturing intricate spatial relationships and geometric characteristics. This process achieves a highly structured and detailed output, ensuring that the final 3D models reflect both semantic accuracy and geometric precision.   This approach addresses key challenges in generative reconstruction, such as maintaining semantic coherence, managing geometric complexity, and preserving detailed visual information. Comprehensive experimental evaluations will assess reconstruction quality, semantic accuracy, and geometric fidelity across diverse domains and varying levels of complexity. By demonstrating the potential of AI-driven 3D reconstruction techniques, this research offers significant implications for fields such as augmented reality (AR), virtual reality (VR), and digital content creation.","sentences":["The convergence of generative artificial intelligence and advanced computer vision technologies introduces a groundbreaking approach to transforming textual descriptions into three-dimensional representations.","This research proposes a fully automated pipeline that seamlessly integrates text-to-image generation, various image processing techniques, and deep learning methods for reflection removal and 3D reconstruction.","By leveraging state-of-the-art generative models like Stable Diffusion, the methodology translates natural language inputs into detailed 3D models through a multi-stage workflow.   ","The reconstruction process begins with the generation of high-quality images from textual prompts, followed by enhancement by a reinforcement learning agent and reflection removal using the Stable Delight model.","Advanced image upscaling and background removal techniques are then applied to further enhance visual fidelity.","These refined two-dimensional representations are subsequently transformed into volumetric 3D models using sophisticated machine learning algorithms, capturing intricate spatial relationships and geometric characteristics.","This process achieves a highly structured and detailed output, ensuring that the final 3D models reflect both semantic accuracy and geometric precision.   ","This approach addresses key challenges in generative reconstruction, such as maintaining semantic coherence, managing geometric complexity, and preserving detailed visual information.","Comprehensive experimental evaluations will assess reconstruction quality, semantic accuracy, and geometric fidelity across diverse domains and varying levels of complexity.","By demonstrating the potential of AI-driven 3D reconstruction techniques, this research offers significant implications for fields such as augmented reality (AR), virtual reality (VR), and digital content creation."],"url":"http://arxiv.org/abs/2503.03664v1"}
{"created":"2025-03-05 16:52:34","title":"LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant","abstract":"First-person video assistants are highly anticipated to enhance our daily lives through online video dialogue. However, existing online video assistants often sacrifice assistant efficacy for real-time efficiency by processing low-frame-rate videos with coarse-grained visual features.To overcome the trade-off between efficacy and efficiency, we propose \"Fast & Slow Video-Language Thinker\" as an onLIne videO assistaNt, LION-FS, achieving real-time, proactive, temporally accurate, and contextually precise responses. LION-FS adopts a two-stage optimization strategy: 1)Fast Path: Routing-Based Response Determination evaluates frame-by-frame whether an immediate response is necessary. To enhance response determination accuracy and handle higher frame-rate inputs efficiently, we employ Token Aggregation Routing to dynamically fuse spatiotemporal features without increasing token numbers, while utilizing Token Dropping Routing to eliminate redundant features. 2)Slow Path: Multi-granularity Keyframe Augmentation optimizes keyframes during response generation. To provide comprehensive and detailed responses beyond atomic actions constrained by training data, fine-grained spatial features and human-environment interaction features are extracted through multi-granular pooling. These features are further integrated into a meticulously designed multimodal Thinking Template to guide more precise response generation. Comprehensive evaluations on online video tasks demonstrate that LION-FS achieves state-of-the-art efficacy and efficiency.","sentences":["First-person video assistants are highly anticipated to enhance our daily lives through online video dialogue.","However, existing online video assistants often sacrifice assistant efficacy for real-time efficiency by processing low-frame-rate videos with coarse-grained visual features.","To overcome the trade-off between efficacy and efficiency, we propose \"Fast & Slow Video-Language Thinker\" as an onLIne videO assistaNt, LION-FS, achieving real-time, proactive, temporally accurate, and contextually precise responses.","LION-FS adopts a two-stage optimization strategy: 1)Fast Path: Routing-Based Response Determination evaluates frame-by-frame whether an immediate response is necessary.","To enhance response determination accuracy and handle higher frame-rate inputs efficiently, we employ Token Aggregation Routing to dynamically fuse spatiotemporal features without increasing token numbers, while utilizing Token Dropping Routing to eliminate redundant features.","2)Slow Path: Multi-granularity Keyframe Augmentation optimizes keyframes during response generation.","To provide comprehensive and detailed responses beyond atomic actions constrained by training data, fine-grained spatial features and human-environment interaction features are extracted through multi-granular pooling.","These features are further integrated into a meticulously designed multimodal Thinking Template to guide more precise response generation.","Comprehensive evaluations on online video tasks demonstrate that LION-FS achieves state-of-the-art efficacy and efficiency."],"url":"http://arxiv.org/abs/2503.03663v1"}
{"created":"2025-03-05 16:39:04","title":"Robust Learning of Diverse Code Edits","abstract":"Software engineering activities frequently involve edits to existing code. However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements. In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm. Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity. Today's code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning. To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting. Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones. We show the generality of our approach on two model families (DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation abilities post adaptation.","sentences":["Software engineering activities frequently involve edits to existing code.","However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements.","In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm.","Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity.","Today's code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning.","To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting.","Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones.","We show the generality of our approach on two model families (DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation abilities post adaptation."],"url":"http://arxiv.org/abs/2503.03656v1"}
{"created":"2025-03-05 16:27:25","title":"Token-Level Privacy in Large Language Models","abstract":"The use of language models as remote services requires transmitting private information to external providers, raising significant privacy concerns. This process not only risks exposing sensitive data to untrusted service providers but also leaves it vulnerable to interception by eavesdroppers. Existing privacy-preserving methods for natural language processing (NLP) interactions primarily rely on semantic similarity, overlooking the role of contextual information. In this work, we introduce dchi-stencil, a novel token-level privacy-preserving mechanism that integrates contextual and semantic information while ensuring strong privacy guarantees under the dchi differential privacy framework, achieving 2epsilon-dchi-privacy. By incorporating both semantic and contextual nuances, dchi-stencil achieves a robust balance between privacy and utility. We evaluate dchi-stencil using state-of-the-art language models and diverse datasets, achieving comparable and even better trade-off between utility and privacy compared to existing methods. This work highlights the potential of dchi-stencil to set a new standard for privacy-preserving NLP in modern, high-risk applications.","sentences":["The use of language models as remote services requires transmitting private information to external providers, raising significant privacy concerns.","This process not only risks exposing sensitive data to untrusted service providers but also leaves it vulnerable to interception by eavesdroppers.","Existing privacy-preserving methods for natural language processing (NLP) interactions primarily rely on semantic similarity, overlooking the role of contextual information.","In this work, we introduce dchi-stencil, a novel token-level privacy-preserving mechanism that integrates contextual and semantic information while ensuring strong privacy guarantees under the dchi differential privacy framework, achieving 2epsilon-dchi-privacy.","By incorporating both semantic and contextual nuances, dchi-stencil achieves a robust balance between privacy and utility.","We evaluate dchi-stencil using state-of-the-art language models and diverse datasets, achieving comparable and even better trade-off between utility and privacy compared to existing methods.","This work highlights the potential of dchi-stencil to set a new standard for privacy-preserving NLP in modern, high-risk applications."],"url":"http://arxiv.org/abs/2503.03652v1"}
{"created":"2025-03-05 16:26:58","title":"DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles","abstract":"Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle.","sentences":["Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements.","However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions.","Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation.","Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image.","The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality.","This facilitates self-evolution of the model without reliance on annotated text-image pairs.","Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data.","For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation.","The code will be released at https://github.com/showlab/DoraCycle."],"url":"http://arxiv.org/abs/2503.03651v1"}
{"created":"2025-03-05 16:20:40","title":"Improved FPT Approximation Algorithms for TSP","abstract":"TSP is a classic and extensively studied problem with numerous real-world applications in artificial intelligence and operations research. It is well-known that TSP admits a constant approximation ratio on metric graphs but becomes NP-hard to approximate within any computable function $f(n)$ on general graphs. This disparity highlights a significant gap between the results on metric graphs and general graphs. Recent research has introduced some parameters to measure the ``distance'' of general graphs from being metric and explored FPT approximation algorithms parameterized by these parameters. Two commonly studied parameters are $p$, the number of vertices in triangles violating the triangle inequality, and $q$, the minimum number of vertices whose removal results in a metric graph. In this paper, we present improved FPT approximation algorithms with respect to these two parameters. For $p$, we propose an FPT algorithm with a 1.5-approximation ratio, improving upon the previous ratio of 2.5. For $q$, we significantly enhance the approximation ratio from 11 to 3, advancing the state of the art in both cases.","sentences":["TSP is a classic and extensively studied problem with numerous real-world applications in artificial intelligence and operations research.","It is well-known that TSP admits a constant approximation ratio on metric graphs but becomes NP-hard to approximate within any computable function $f(n)$ on general graphs.","This disparity highlights a significant gap between the results on metric graphs and general graphs.","Recent research has introduced some parameters to measure the ``distance'' of general graphs from being metric and explored FPT approximation algorithms parameterized by these parameters.","Two commonly studied parameters are $p$, the number of vertices in triangles violating the triangle inequality, and $q$, the minimum number of vertices whose removal results in a metric graph.","In this paper, we present improved FPT approximation algorithms with respect to these two parameters.","For $p$, we propose an FPT algorithm with a 1.5-approximation ratio, improving upon the previous ratio of 2.5.","For $q$, we significantly enhance the approximation ratio from 11 to 3, advancing the state of the art in both cases."],"url":"http://arxiv.org/abs/2503.03642v1"}
{"created":"2025-03-05 16:16:46","title":"4D Radar Ground Truth Augmentation with LiDAR-to-4D Radar Data Synthesis","abstract":"Ground truth augmentation (GT-Aug) is a common method for LiDAR-based object detection, as it enhances object density by leveraging ground truth bounding boxes (GT bboxes). However, directly applying GT-Aug to 4D Radar tensor data overlooks important measurements outside the GT bboxes-such as sidelobes-leading to synthetic distributions that deviate from real-world 4D Radar data. To address this limitation, we propose 4D Radar Ground Truth Augmentation (4DR GT-Aug). Our approach first augments LiDAR data and then converts it to 4D Radar data via a LiDAR-to-4D Radar data synthesis (L2RDaS) module, which explicitly accounts for measurements both inside and outside GT bboxes. In doing so, it produces 4D Radar data distributions that more closely resemble real-world measurements, thereby improving object detection accuracy. Experiments on the K-Radar dataset show that the proposed method achieves improved performance compared to conventional GT-Aug in object detection for 4D Radar. The implementation code is available at https://github.com/kaist-avelab/K-Radar.","sentences":["Ground truth augmentation (GT-Aug) is a common method for LiDAR-based object detection, as it enhances object density by leveraging ground truth bounding boxes (GT bboxes).","However, directly applying GT-Aug to 4D Radar tensor data overlooks important measurements outside the GT bboxes-such as sidelobes-leading to synthetic distributions that deviate from real-world 4D Radar data.","To address this limitation, we propose 4D Radar Ground Truth Augmentation (4DR GT-Aug).","Our approach first augments LiDAR data and then converts it to 4D Radar data via a LiDAR-to-4D Radar data synthesis (L2RDaS) module, which explicitly accounts for measurements both inside and outside GT bboxes.","In doing so, it produces 4D Radar data distributions that more closely resemble real-world measurements, thereby improving object detection accuracy.","Experiments on the K-Radar dataset show that the proposed method achieves improved performance compared to conventional GT-Aug in object detection for 4D Radar.","The implementation code is available at https://github.com/kaist-avelab/K-Radar."],"url":"http://arxiv.org/abs/2503.03637v1"}
{"created":"2025-03-05 16:14:36","title":"Motion Planning and Control with Unknown Nonlinear Dynamics through Predicted Reachability","abstract":"Autonomous motion planning under unknown nonlinear dynamics presents significant challenges. An agent needs to continuously explore the system dynamics to acquire its properties, such as reachability, in order to guide system navigation adaptively. In this paper, we propose a hybrid planning-control framework designed to compute a feasible trajectory toward a target. Our approach involves partitioning the state space and approximating the system by a piecewise affine (PWA) system with constrained control inputs. By abstracting the PWA system into a directed weighted graph, we incrementally update the existence of its edges via affine system identification and reach control theory, introducing a predictive reachability condition by exploiting prior information of the unknown dynamics. Heuristic weights are assigned to edges based on whether their existence is certain or remains indeterminate. Consequently, we propose a framework that adaptively collects and analyzes data during mission execution, continually updates the predictive graph, and synthesizes a controller online based on the graph search outcomes. We demonstrate the efficacy of our approach through simulation scenarios involving a mobile robot operating in unknown terrains, with its unknown dynamics abstracted as a single integrator model.","sentences":["Autonomous motion planning under unknown nonlinear dynamics presents significant challenges.","An agent needs to continuously explore the system dynamics to acquire its properties, such as reachability, in order to guide system navigation adaptively.","In this paper, we propose a hybrid planning-control framework designed to compute a feasible trajectory toward a target.","Our approach involves partitioning the state space and approximating the system by a piecewise affine (PWA) system with constrained control inputs.","By abstracting the PWA system into a directed weighted graph, we incrementally update the existence of its edges via affine system identification and reach control theory, introducing a predictive reachability condition by exploiting prior information of the unknown dynamics.","Heuristic weights are assigned to edges based on whether their existence is certain or remains indeterminate.","Consequently, we propose a framework that adaptively collects and analyzes data during mission execution, continually updates the predictive graph, and synthesizes a controller online based on the graph search outcomes.","We demonstrate the efficacy of our approach through simulation scenarios involving a mobile robot operating in unknown terrains, with its unknown dynamics abstracted as a single integrator model."],"url":"http://arxiv.org/abs/2503.03633v1"}
{"created":"2025-03-05 16:09:30","title":"TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation","abstract":"Traffic simulation is essential for autonomous vehicle (AV) development, enabling comprehensive safety evaluation across diverse driving conditions. However, traditional rule-based simulators struggle to capture complex human interactions, while data-driven approaches often fail to maintain long-term behavioral realism or generate diverse safety-critical events. To address these challenges, we propose TeraSim, an open-source, high-fidelity traffic simulation platform designed to uncover unknown unsafe events and efficiently estimate AV statistical performance metrics, such as crash rates. TeraSim is designed for seamless integration with third-party physics simulators and standalone AV stacks, to construct a complete AV simulation system. Experimental results demonstrate its effectiveness in generating diverse safety-critical events involving both static and dynamic agents, identifying hidden deficiencies in AV systems, and enabling statistical performance evaluation. These findings highlight TeraSim's potential as a practical tool for AV safety assessment, benefiting researchers, developers, and policymakers. The code is available at https://github.com/mcity/TeraSim.","sentences":["Traffic simulation is essential for autonomous vehicle (AV) development, enabling comprehensive safety evaluation across diverse driving conditions.","However, traditional rule-based simulators struggle to capture complex human interactions, while data-driven approaches often fail to maintain long-term behavioral realism or generate diverse safety-critical events.","To address these challenges, we propose TeraSim, an open-source, high-fidelity traffic simulation platform designed to uncover unknown unsafe events and efficiently estimate AV statistical performance metrics, such as crash rates.","TeraSim is designed for seamless integration with third-party physics simulators and standalone AV stacks, to construct a complete AV simulation system.","Experimental results demonstrate its effectiveness in generating diverse safety-critical events involving both static and dynamic agents, identifying hidden deficiencies in AV systems, and enabling statistical performance evaluation.","These findings highlight TeraSim's potential as a practical tool for AV safety assessment, benefiting researchers, developers, and policymakers.","The code is available at https://github.com/mcity/TeraSim."],"url":"http://arxiv.org/abs/2503.03629v1"}
{"created":"2025-03-05 16:02:09","title":"It's My Data Too: Private ML for Datasets with Multi-User Training Examples","abstract":"We initiate a study of algorithms for model training with user-level differential privacy (DP), where each example may be attributed to multiple users, which we call the multi-attribution model. We first provide a carefully chosen definition of user-level DP under the multi-attribution model. Training in the multi-attribution model is facilitated by solving the contribution bounding problem, i.e. the problem of selecting a subset of the dataset for which each user is associated with a limited number of examples. We propose a greedy baseline algorithm for the contribution bounding problem. We then empirically study this algorithm for a synthetic logistic regression task and a transformer training task, including studying variants of this baseline algorithm that optimize the subset chosen using different techniques and criteria. We find that the baseline algorithm remains competitive with its variants in most settings, and build a better understanding of the practical importance of a bias-variance tradeoff inherent in solutions to the contribution bounding problem.","sentences":["We initiate a study of algorithms for model training with user-level differential privacy (DP), where each example may be attributed to multiple users, which we call the multi-attribution model.","We first provide a carefully chosen definition of user-level DP under the multi-attribution model.","Training in the multi-attribution model is facilitated by solving the contribution bounding problem, i.e. the problem of selecting a subset of the dataset for which each user is associated with a limited number of examples.","We propose a greedy baseline algorithm for the contribution bounding problem.","We then empirically study this algorithm for a synthetic logistic regression task and a transformer training task, including studying variants of this baseline algorithm that optimize the subset chosen using different techniques and criteria.","We find that the baseline algorithm remains competitive with its variants in most settings, and build a better understanding of the practical importance of a bias-variance tradeoff inherent in solutions to the contribution bounding problem."],"url":"http://arxiv.org/abs/2503.03622v1"}
{"created":"2025-03-05 15:58:24","title":"Design and Implementation of an IoT Cluster with Raspberry Pi Powered by Solar Energy: A Theoretical Approach","abstract":"This document presents the design and implementation of a low-power IoT server cluster, based on Raspberry Pi 3 Model B and powered by solar energy. The proposed architecture integrates Kubernetes (K3s) and Docker, providing an efficient, scalable, and high-performance computing environment. The cluster is designed to optimize energy consumption, leveraging a 200W solar panel system and a 100Ah lithium-ion battery to support continuous operation under favorable environmental conditions. Performance analysis was conducted based on theoretical inferences and data obtained from external sources, evaluating resource allocation, power consumption, and service availability. These analyses provide theoretical estimates of the system's operational feasibility under different scenarios. The results suggest that this system can serve as a viable and sustainable alternative for edge computing applications and cloud services, reducing dependence on traditional data centers. In addition to its positive impact on environmental sustainability by significantly reducing the carbon footprint, this solution also addresses economic concerns, as conventional data centers consume enormous amounts of energy, leading to increased demand on the power grid and higher operational costs.","sentences":["This document presents the design and implementation of a low-power IoT server cluster, based on Raspberry Pi 3 Model B and powered by solar energy.","The proposed architecture integrates Kubernetes (K3s) and Docker, providing an efficient, scalable, and high-performance computing environment.","The cluster is designed to optimize energy consumption, leveraging a 200W solar panel system and a 100Ah lithium-ion battery to support continuous operation under favorable environmental conditions.","Performance analysis was conducted based on theoretical inferences and data obtained from external sources, evaluating resource allocation, power consumption, and service availability.","These analyses provide theoretical estimates of the system's operational feasibility under different scenarios.","The results suggest that this system can serve as a viable and sustainable alternative for edge computing applications and cloud services, reducing dependence on traditional data centers.","In addition to its positive impact on environmental sustainability by significantly reducing the carbon footprint, this solution also addresses economic concerns, as conventional data centers consume enormous amounts of energy, leading to increased demand on the power grid and higher operational costs."],"url":"http://arxiv.org/abs/2503.03618v1"}
{"created":"2025-03-05 15:28:50","title":"Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias","abstract":"Score-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data. While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no meaning. This paper focuses on textual hallucinations, where diffusion models correctly generate individual symbols but assemble them in a nonsensical manner. Through experimental probing, we consistently observe that such phenomenon is attributed it to the network's local generation bias. Denoising networks tend to produce outputs that rely heavily on highly correlated local regions, particularly when different dimensions of the data distribution are nearly pairwise independent. This behavior leads to a generation process that decomposes the global distribution into separate, independent distributions for each symbol, ultimately failing to capture the global structure, including underlying grammar. Intriguingly, this bias persists across various denoising network architectures including MLP and transformers which have the structure to model global dependency. These findings also provide insights into understanding other types of hallucinations, extending beyond text, as a result of implicit biases in the denoising models. Additionally, we theoretically analyze the training dynamics for a specific case involving a two-layer MLP learning parity points on a hypercube, offering an explanation of its underlying mechanism.","sentences":["Score-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data.","While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no meaning.","This paper focuses on textual hallucinations, where diffusion models correctly generate individual symbols but assemble them in a nonsensical manner.","Through experimental probing, we consistently observe that such phenomenon is attributed it to the network's local generation bias.","Denoising networks tend to produce outputs that rely heavily on highly correlated local regions, particularly when different dimensions of the data distribution are nearly pairwise independent.","This behavior leads to a generation process that decomposes the global distribution into separate, independent distributions for each symbol, ultimately failing to capture the global structure, including underlying grammar.","Intriguingly, this bias persists across various denoising network architectures including MLP and transformers which have the structure to model global dependency.","These findings also provide insights into understanding other types of hallucinations, extending beyond text, as a result of implicit biases in the denoising models.","Additionally, we theoretically analyze the training dynamics for a specific case involving a two-layer MLP learning parity points on a hypercube, offering an explanation of its underlying mechanism."],"url":"http://arxiv.org/abs/2503.03595v1"}
{"created":"2025-03-05 15:24:37","title":"Digital Twin-Enabled Blockage-Aware Dynamic mmWave Multi-Hop V2X Communication","abstract":"Millimeter wave (mmWave) technology in vehicle-to-everything (V2X) communication offers unprecedented data rates and low latency, but faces significant reliability challenges due to signal blockages and limited range. This paper introduces a novel system for managing dynamic multi-hop mmWave V2X communications in complex blocking environments. We present a system architecture that integrates a mobility digital twin (DT) with the multi-hop routing control plane, providing a comprehensive, real-time view of the network and its surrounding traffic environment. This integration enables the control plane to make informed routing decisions based on rich contextual data about vehicles, infrastructure, and potential signal blockages. Leveraging this DT-enhanced architecture, we propose an advanced routing algorithm that combines high-precision environmental data with trajectory prediction to achieve blockage-aware mmWave multi-hop V2X routing. Our algorithm anticipates network topology changes and adapts topology dynamically to maintain reliable connections. We evaluate our approach through proof-of-concept simulations using a mobility DT of the Nishishinjuku area. Results demonstrate that our DT-enabled routing strategy significantly outperforms conventional methods in maintaining reliable mmWave V2X connections across various traffic scenarios, including fully connected and mixed traffic environments.","sentences":["Millimeter wave (mmWave) technology in vehicle-to-everything (V2X) communication offers unprecedented data rates and low latency, but faces significant reliability challenges due to signal blockages and limited range.","This paper introduces a novel system for managing dynamic multi-hop mmWave V2X communications in complex blocking environments.","We present a system architecture that integrates a mobility digital twin (DT) with the multi-hop routing control plane, providing a comprehensive, real-time view of the network and its surrounding traffic environment.","This integration enables the control plane to make informed routing decisions based on rich contextual data about vehicles, infrastructure, and potential signal blockages.","Leveraging this DT-enhanced architecture, we propose an advanced routing algorithm that combines high-precision environmental data with trajectory prediction to achieve blockage-aware mmWave multi-hop V2X routing.","Our algorithm anticipates network topology changes and adapts topology dynamically to maintain reliable connections.","We evaluate our approach through proof-of-concept simulations using a mobility DT of the Nishishinjuku area.","Results demonstrate that our DT-enabled routing strategy significantly outperforms conventional methods in maintaining reliable mmWave V2X connections across various traffic scenarios, including fully connected and mixed traffic environments."],"url":"http://arxiv.org/abs/2503.03590v1"}
{"created":"2025-03-05 15:22:35","title":"\"You don't need a university degree to comprehend data protection this way\": LLM-Powered Interactive Privacy Policy Assessment","abstract":"Protecting online privacy requires users to engage with and comprehend website privacy policies, but many policies are difficult and tedious to read. We present the first qualitative user study on Large Language Model (LLM)-driven privacy policy assessment. To this end, we build and evaluate an LLM-based privacy policy assessment browser extension, which helps users understand the essence of a lengthy, complex privacy policy while browsing. The tool integrates a dashboard and an LLM chat. In our qualitative user study (N=22), we evaluate usability, understandability of the information our tool provides, and its impacts on awareness. While providing a comprehensible quick overview and a chat for in-depth discussion improves privacy awareness, users note issues with building trust in the tool. From our insights, we derive important design implications to guide future policy analysis tools.","sentences":["Protecting online privacy requires users to engage with and comprehend website privacy policies, but many policies are difficult and tedious to read.","We present the first qualitative user study on Large Language Model (LLM)-driven privacy policy assessment.","To this end, we build and evaluate an LLM-based privacy policy assessment browser extension, which helps users understand the essence of a lengthy, complex privacy policy while browsing.","The tool integrates a dashboard and an LLM chat.","In our qualitative user study (N=22), we evaluate usability, understandability of the information our tool provides, and its impacts on awareness.","While providing a comprehensible quick overview and a chat for in-depth discussion improves privacy awareness, users note issues with building trust in the tool.","From our insights, we derive important design implications to guide future policy analysis tools."],"url":"http://arxiv.org/abs/2503.03587v1"}
{"created":"2025-03-05 15:17:18","title":"Scaling Crowdsourced Election Monitoring: Construction and Evaluation of Classification Models for Multilingual and Cross-Domain Classification Settings","abstract":"The adoption of crowdsourced election monitoring as a complementary alternative to traditional election monitoring is on the rise. Yet, its reliance on digital response volunteers to manually process incoming election reports poses a significant scaling bottleneck. In this paper, we address the challenge of scaling crowdsourced election monitoring by advancing the task of automated classification of crowdsourced election reports to multilingual and cross-domain classification settings. We propose a two-step classification approach of first identifying informative reports and then categorising them into distinct information types. We conduct classification experiments using multilingual transformer models such as XLM-RoBERTa and multilingual embeddings such as SBERT, augmented with linguistically motivated features. Our approach achieves F1-Scores of 77\\% for informativeness detection and 75\\% for information type classification. We conduct cross-domain experiments, applying models trained in a source electoral domain to a new target electoral domain in zero-shot and few-shot classification settings. Our results show promising potential for model transfer across electoral domains, with F1-Scores of 59\\% in zero-shot and 63\\% in few-shot settings. However, our analysis also reveals a performance bias in detecting informative English reports over Swahili, likely due to imbalances in the training data, indicating a need for caution when deploying classification models in real-world election scenarios.","sentences":["The adoption of crowdsourced election monitoring as a complementary alternative to traditional election monitoring is on the rise.","Yet, its reliance on digital response volunteers to manually process incoming election reports poses a significant scaling bottleneck.","In this paper, we address the challenge of scaling crowdsourced election monitoring by advancing the task of automated classification of crowdsourced election reports to multilingual and cross-domain classification settings.","We propose a two-step classification approach of first identifying informative reports and then categorising them into distinct information types.","We conduct classification experiments using multilingual transformer models such as XLM-RoBERTa and multilingual embeddings such as SBERT, augmented with linguistically motivated features.","Our approach achieves F1-Scores of 77\\% for informativeness detection and 75\\% for information type classification.","We conduct cross-domain experiments, applying models trained in a source electoral domain to a new target electoral domain in zero-shot and few-shot classification settings.","Our results show promising potential for model transfer across electoral domains, with F1-Scores of 59\\% in zero-shot and 63\\% in few-shot settings.","However, our analysis also reveals a performance bias in detecting informative English reports over Swahili, likely due to imbalances in the training data, indicating a need for caution when deploying classification models in real-world election scenarios."],"url":"http://arxiv.org/abs/2503.03582v1"}
{"created":"2025-03-05 15:13:54","title":"A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery","abstract":"We propose a novel system for robot-to-human object handover that emulates human coworker interactions. Unlike most existing studies that focus primarily on grasping strategies and motion planning, our system focus on 1. inferring human handover intents, 2. imagining spatial handover configuration. The first one integrates multimodal perception-combining visual and verbal cues-to infer human intent. The second one using a diffusion-based model to generate the handover configuration, involving the spacial relationship among robot's gripper, the object, and the human hand, thereby mimicking the cognitive process of motor imagery. Experimental results demonstrate that our approach effectively interprets human cues and achieves fluent, human-like handovers, offering a promising solution for collaborative robotics. Code, videos, and data are available at: https://i3handover.github.io.","sentences":["We propose a novel system for robot-to-human object handover that emulates human coworker interactions.","Unlike most existing studies that focus primarily on grasping strategies and motion planning, our system focus on 1.","inferring human handover intents, 2.","imagining spatial handover configuration.","The first one integrates multimodal perception-combining visual and verbal cues-to infer human intent.","The second one using a diffusion-based model to generate the handover configuration, involving the spacial relationship among robot's gripper, the object, and the human hand, thereby mimicking the cognitive process of motor imagery.","Experimental results demonstrate that our approach effectively interprets human cues and achieves fluent, human-like handovers, offering a promising solution for collaborative robotics.","Code, videos, and data are available at: https://i3handover.github.io."],"url":"http://arxiv.org/abs/2503.03579v1"}
{"created":"2025-03-05 15:00:39","title":"Domain Consistent Industrial Decarbonisation of Global Coal Power Plants","abstract":"Machine learning and optimisation techniques (MLOPT) hold significant potential to accelerate the decarbonisation of industrial systems by enabling data-driven operational improvements. However, the practical application of MLOPT in industrial settings is often hindered by a lack of domain compliance and system-specific consistency, resulting in suboptimal solutions with limited real-world applicability. To address this challenge, we propose a novel human-in-the-loop (HITL) constraint-based optimisation framework that integrates domain expertise with data-driven methods, ensuring solutions are both technically sound and operationally feasible. We demonstrate the efficacy of this framework through a case study focused on enhancing the thermal efficiency and reducing the turbine heat rate of a 660 MW supercritical coal-fired power plant. By embedding domain knowledge as constraints within the optimisation process, our approach yields solutions that align with the plant's operational patterns and are seamlessly integrated into its control systems. Empirical validation confirms a mean improvement in thermal efficiency of 0.64\\% and a mean reduction in turbine heat rate of 93 kJ/kWh. Scaling our analysis to 59 global coal power plants with comparable capacity and fuel type, we estimate a cumulative lifetime reduction of 156.4 million tons of carbon emissions. These results underscore the transformative potential of our HITL-MLOPT framework in delivering domain-compliant, implementable solutions for industrial decarbonisation, offering a scalable pathway to mitigate the environmental impact of coal-based power generation worldwide.","sentences":["Machine learning and optimisation techniques (MLOPT) hold significant potential to accelerate the decarbonisation of industrial systems by enabling data-driven operational improvements.","However, the practical application of MLOPT in industrial settings is often hindered by a lack of domain compliance and system-specific consistency, resulting in suboptimal solutions with limited real-world applicability.","To address this challenge, we propose a novel human-in-the-loop (HITL) constraint-based optimisation framework that integrates domain expertise with data-driven methods, ensuring solutions are both technically sound and operationally feasible.","We demonstrate the efficacy of this framework through a case study focused on enhancing the thermal efficiency and reducing the turbine heat rate of a 660 MW supercritical coal-fired power plant.","By embedding domain knowledge as constraints within the optimisation process, our approach yields solutions that align with the plant's operational patterns and are seamlessly integrated into its control systems.","Empirical validation confirms a mean improvement in thermal efficiency of 0.64\\% and a mean reduction in turbine heat rate of 93 kJ/kWh.","Scaling our analysis to 59 global coal power plants with comparable capacity and fuel type, we estimate a cumulative lifetime reduction of 156.4 million tons of carbon emissions.","These results underscore the transformative potential of our HITL-MLOPT framework in delivering domain-compliant, implementable solutions for industrial decarbonisation, offering a scalable pathway to mitigate the environmental impact of coal-based power generation worldwide."],"url":"http://arxiv.org/abs/2503.03571v1"}
{"created":"2025-03-05 14:56:39","title":"Novel Complexity Results for Temporal Separators with Deadlines","abstract":"We consider two variants, (s,z,l)-Temporal Separator and (s,z,l)-Temporal Cut, respectively, of the vertex separator and the edge cut problem in temporal graphs. The goal is to remove the minimum number of vertices (temporal edges, respectively) in order to delete all the temporal paths that have time travel at most l between a source vertex s and target vertex z. First, we solve an open problem in the literature showing that (s,z,l)-Temporal Separator is NP-complete even when the underlying graph has pathwidth bounded by four. We complement this result showing that (s,z,l)-Temporal Separator can be solved in polynomial time for graphs of pathwidth bounded by three. Then we consider the approximability of (s,z,l)-Temporal Separator and we show that it cannot be approximated within factor$2^{\\Omega(\\log^{1-\\varepsilon}|V|)}$ for any constant $\\varepsilon> 0$, unless $NP \\subseteq ZPP$ (V is the vertex set of the input temporal graph) and that the strict version is approximable within factor l - 1 (we show also that it is unliklely that this factor can be improved). Then we consider the (s,z,l)-Temporal Cut problem, we show that it is APX-hard and we present a $2 \\log_2(2\\ell)$ approximation algorithm.","sentences":["We consider two variants, (s,z,l)-Temporal Separator and (s,z,l)-Temporal Cut, respectively, of the vertex separator and the edge cut problem in temporal graphs.","The goal is to remove the minimum number of vertices (temporal edges, respectively) in order to delete all the temporal paths that have time travel at most l between a source vertex s and target vertex z.","First, we solve an open problem in the literature showing that (s,z,l)-Temporal Separator is NP-complete even when the underlying graph has pathwidth bounded by four.","We complement this result showing that (s,z,l)-Temporal Separator can be solved in polynomial time for graphs of pathwidth bounded by three.","Then we consider the approximability of (s,z,l)-Temporal Separator and we show that it cannot be approximated within factor$2^{\\Omega(\\log^{1-\\varepsilon}|V|)}$ for any constant $\\varepsilon> 0$, unless $NP \\subseteq ZPP$ (V is the vertex set of the input temporal graph) and that the strict version is approximable within factor l - 1 (we show also that it is unliklely that this factor can be improved).","Then we consider the (s,z,l)-Temporal Cut problem, we show that it is APX-hard and we present a $2 \\log_2(2\\ell)$ approximation algorithm."],"url":"http://arxiv.org/abs/2503.03568v1"}
{"created":"2025-03-05 14:51:46","title":"A Conceptual Model for Attributions in Event-Centric Knowledge Graphs","abstract":"The use of narratives as a means of fusing information from knowledge graphs (KGs) into a coherent line of argumentation has been the subject of recent investigation. Narratives are especially useful in event-centric knowledge graphs in that they provide a means to connect different real-world events and categorize them by well-known narrations. However, specifically for controversial events, a problem in information fusion arises, namely, multiple viewpoints regarding the validity of certain event aspects, e.g., regarding the role a participant takes in an event, may exist. Expressing those viewpoints in KGs is challenging because disputed information provided by different viewpoints may introduce inconsistencies. Hence, most KGs only feature a single view on the contained information, hampering the effectiveness of narrative information access. This paper is an extension of our original work and introduces attributions, i.e., parameterized predicates that allow for the representation of facts that are only valid in a specific viewpoint. For this, we develop a conceptual model that allows for the representation of viewpoint-dependent information. As an extension, we enhance the model by a conception of viewpoint-compatibility. Based on this, we deepen our original deliberations on the model's effects on information fusion and provide additional grounding in the literature.","sentences":["The use of narratives as a means of fusing information from knowledge graphs (KGs) into a coherent line of argumentation has been the subject of recent investigation.","Narratives are especially useful in event-centric knowledge graphs in that they provide a means to connect different real-world events and categorize them by well-known narrations.","However, specifically for controversial events, a problem in information fusion arises, namely, multiple viewpoints regarding the validity of certain event aspects, e.g., regarding the role a participant takes in an event, may exist.","Expressing those viewpoints in KGs is challenging because disputed information provided by different viewpoints may introduce inconsistencies.","Hence, most KGs only feature a single view on the contained information, hampering the effectiveness of narrative information access.","This paper is an extension of our original work and introduces attributions, i.e., parameterized predicates that allow for the representation of facts that are only valid in a specific viewpoint.","For this, we develop a conceptual model that allows for the representation of viewpoint-dependent information.","As an extension, we enhance the model by a conception of viewpoint-compatibility.","Based on this, we deepen our original deliberations on the model's effects on information fusion and provide additional grounding in the literature."],"url":"http://arxiv.org/abs/2503.03563v1"}
{"created":"2025-03-05 14:41:59","title":"A Graph Width Perspective on Partially Ordered Hamiltonian Paths","abstract":"We consider the problem of finding a Hamiltonian path with precedence constraints in the form of a partial order on the vertex set. This problem is known as Partially Ordered Hamiltonian Path Problem (POHPP). Here, we study the complexity for graph width parameters for which the ordinary Hamiltonian Path problem is in $\\mathsf{FPT}$. We show that POHPP is $\\mathsf{NP}$-complete for graphs of pathwidth 4. We complement this result by giving polynomial-time algorithms for graphs of pathwidth 3 and treewidth 2. Furthermore, we show that POHPP is $\\mathsf{NP}$-hard for graphs of clique cover number 2 and $\\mathsf{W[1]}$-hard for some distance-to-$\\mathcal{G}$ parameters, including distance to path and distance to clique. In addition, we present $\\mathsf{XP}$ and $\\mathsf{FPT}$ algorithms for parameters such as distance to block and feedback edge set number.","sentences":["We consider the problem of finding a Hamiltonian path with precedence constraints in the form of a partial order on the vertex set.","This problem is known as Partially Ordered Hamiltonian Path Problem (POHPP).","Here, we study the complexity for graph width parameters for which the ordinary Hamiltonian Path problem is in $\\mathsf{FPT}$. We show that POHPP is $\\mathsf{NP}$-complete for graphs of pathwidth 4.","We complement this result by giving polynomial-time algorithms for graphs of pathwidth 3 and treewidth 2.","Furthermore, we show that POHPP is $\\mathsf{NP}$-hard for graphs of clique cover number 2 and $\\mathsf{W[1]}$-hard for some distance-to-$\\mathcal{G}$ parameters, including distance to path and distance to clique.","In addition, we present $\\mathsf{XP}$ and $\\mathsf{FPT}$ algorithms for parameters such as distance to block and feedback edge set number."],"url":"http://arxiv.org/abs/2503.03553v1"}
{"created":"2025-03-05 14:28:01","title":"A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction","abstract":"Generating novel views from recorded videos is crucial for enabling autonomous UAV navigation. Recent advancements in neural rendering have facilitated the rapid development of methods capable of rendering new trajectories. However, these methods often fail to generalize well to regions far from the training data without an optimized flight path, leading to suboptimal reconstructions. We propose a self-supervised cyclic neural-analytic pipeline that combines high-quality neural rendering outputs with precise geometric insights from analytical methods. Our solution improves RGB and mesh reconstructions for novel view synthesis, especially in undersampled areas and regions that are completely different from the training dataset. We use an effective transformer-based architecture for image reconstruction to refine and adapt the synthesis process, enabling effective handling of novel, unseen poses without relying on extensive labeled datasets. Our findings demonstrate substantial improvements in rendering views of novel and also 3D reconstruction, which to the best of our knowledge is a first, setting a new standard for autonomous navigation in complex outdoor environments.","sentences":["Generating novel views from recorded videos is crucial for enabling autonomous UAV navigation.","Recent advancements in neural rendering have facilitated the rapid development of methods capable of rendering new trajectories.","However, these methods often fail to generalize well to regions far from the training data without an optimized flight path, leading to suboptimal reconstructions.","We propose a self-supervised cyclic neural-analytic pipeline that combines high-quality neural rendering outputs with precise geometric insights from analytical methods.","Our solution improves RGB and mesh reconstructions for novel view synthesis, especially in undersampled areas and regions that are completely different from the training dataset.","We use an effective transformer-based architecture for image reconstruction to refine and adapt the synthesis process, enabling effective handling of novel, unseen poses without relying on extensive labeled datasets.","Our findings demonstrate substantial improvements in rendering views of novel and also 3D reconstruction, which to the best of our knowledge is a first, setting a new standard for autonomous navigation in complex outdoor environments."],"url":"http://arxiv.org/abs/2503.03543v1"}
{"created":"2025-03-05 14:23:56","title":"Data Sharing, Privacy and Security Considerations in the Energy Sector: A Review from Technical Landscape to Regulatory Specifications","abstract":"Decarbonization, decentralization and digitalization are the three key elements driving the twin energy transition. The energy system is evolving to a more data driven ecosystem, leading to the need of communication and storage of large amount of data of different resolution from the prosumers and other stakeholders in the energy ecosystem. While the energy system is certainly advancing, this paradigm shift is bringing in new privacy and security issues related to collection, processing and storage of data - not only from the technical dimension, but also from the regulatory perspective. Understanding data privacy and security in the evolving energy system, regarding regulatory compliance, is an immature field of research. Contextualized knowledge of how related issues are regulated is still in its infancy, and the practical and technical basis for the regulatory framework for data privacy and security is not clear. To fill this gap, this paper conducts a comprehensive review of the data-related issues for the energy system by integrating both technical and regulatory dimensions. We start by reviewing open-access data, data communication and data-processing techniques for the energy system, and use it as the basis to connect the analysis of data-related issues from the integrated perspective. We classify the issues into three categories: (i) data-sharing among energy end users and stakeholders (ii) privacy of end users, and (iii) cyber security, and then explore these issues from a regulatory perspective. We analyze the evolution of related regulations, and introduce the relevant regulatory initiatives for the categorized issues in terms of regulatory definitions, concepts, principles, rights and obligations in the context of energy systems. Finally, we provide reflections on the gaps that still exist, and guidelines for regulatory frameworks for a truly participatory energy system.","sentences":["Decarbonization, decentralization and digitalization are the three key elements driving the twin energy transition.","The energy system is evolving to a more data driven ecosystem, leading to the need of communication and storage of large amount of data of different resolution from the prosumers and other stakeholders in the energy ecosystem.","While the energy system is certainly advancing, this paradigm shift is bringing in new privacy and security issues related to collection, processing and storage of data - not only from the technical dimension, but also from the regulatory perspective.","Understanding data privacy and security in the evolving energy system, regarding regulatory compliance, is an immature field of research.","Contextualized knowledge of how related issues are regulated is still in its infancy, and the practical and technical basis for the regulatory framework for data privacy and security is not clear.","To fill this gap, this paper conducts a comprehensive review of the data-related issues for the energy system by integrating both technical and regulatory dimensions.","We start by reviewing open-access data, data communication and data-processing techniques for the energy system, and use it as the basis to connect the analysis of data-related issues from the integrated perspective.","We classify the issues into three categories: (i) data-sharing among energy end users and stakeholders (ii) privacy of end users, and (iii) cyber security, and then explore these issues from a regulatory perspective.","We analyze the evolution of related regulations, and introduce the relevant regulatory initiatives for the categorized issues in terms of regulatory definitions, concepts, principles, rights and obligations in the context of energy systems.","Finally, we provide reflections on the gaps that still exist, and guidelines for regulatory frameworks for a truly participatory energy system."],"url":"http://arxiv.org/abs/2503.03539v1"}
{"created":"2025-03-05 14:19:41","title":"Using CognitIDE to Capture Developers' Cognitive Load via Physiological Activity During Everyday Software Development Tasks","abstract":"Integrated development environments (IDE) support developers in a variety of tasks. Unobtrusively capturing developers' cognitive load while working on different programming tasks could help optimize developers' work experience, increase their productivity, and positively impact code quality. In this paper, we propose a study in which the IntelliJ-based IDE plugin CognitIDE is used to collect, map, and visualize software developers' physiological activity data while they are working on various software development tasks. In a feasibility study, participants completed four simulated everyday working tasks of software developers - coding, debugging, code documentation, and email writing - based on Java open source code in the IDE whilst their physiological activity was recorded. Between the tasks, the participants' perceived workload was assessed. Feasibility testing showed that CognitIDE could successfully be used for data collection sessions of one hour, which was the most extended duration tested and was well-perceived by those working with it. Furthermore, the recorded physiological activity indicated higher cognitive load during working tasks compared to baseline recordings. This suggests that cognitive load can be assessed, mapped to code positions, visualized, and discussed with participants in such study setups with CognitIDE. These promising results indicate the usefulness of the plugin for diverse study workflows in a natural IDE environment.","sentences":["Integrated development environments (IDE) support developers in a variety of tasks.","Unobtrusively capturing developers' cognitive load while working on different programming tasks could help optimize developers' work experience, increase their productivity, and positively impact code quality.","In this paper, we propose a study in which the IntelliJ-based IDE plugin CognitIDE is used to collect, map, and visualize software developers' physiological activity data while they are working on various software development tasks.","In a feasibility study, participants completed four simulated everyday working tasks of software developers - coding, debugging, code documentation, and email writing - based on Java open source code in the IDE whilst their physiological activity was recorded.","Between the tasks, the participants' perceived workload was assessed.","Feasibility testing showed that CognitIDE could successfully be used for data collection sessions of one hour, which was the most extended duration tested and was well-perceived by those working with it.","Furthermore, the recorded physiological activity indicated higher cognitive load during working tasks compared to baseline recordings.","This suggests that cognitive load can be assessed, mapped to code positions, visualized, and discussed with participants in such study setups with CognitIDE.","These promising results indicate the usefulness of the plugin for diverse study workflows in a natural IDE environment."],"url":"http://arxiv.org/abs/2503.03537v1"}
{"created":"2025-03-05 14:18:39","title":"Unified Human Localization and Trajectory Prediction with Monocular Vision","abstract":"Conventional human trajectory prediction models rely on clean curated data, requiring specialized equipment or manual labeling, which is often impractical for robotic applications. The existing predictors tend to overfit to clean observation affecting their robustness when used with noisy inputs. In this work, we propose MonoTransmotion (MT), a Transformer-based framework that uses only a monocular camera to jointly solve localization and prediction tasks. Our framework has two main modules: Bird's Eye View (BEV) localization and trajectory prediction. The BEV localization module estimates the position of a person using 2D human poses, enhanced by a novel directional loss for smoother sequential localizations. The trajectory prediction module predicts future motion from these estimates. We show that by jointly training both tasks with our unified framework, our method is more robust in real-world scenarios made of noisy inputs. We validate our MT network on both curated and non-curated datasets. On the curated dataset, MT achieves around 12% improvement over baseline models on BEV localization and trajectory prediction. On real-world non-curated dataset, experimental results indicate that MT maintains similar performance levels, highlighting its robustness and generalization capability. The code is available at https://github.com/vita-epfl/MonoTransmotion.","sentences":["Conventional human trajectory prediction models rely on clean curated data, requiring specialized equipment or manual labeling, which is often impractical for robotic applications.","The existing predictors tend to overfit to clean observation affecting their robustness when used with noisy inputs.","In this work, we propose MonoTransmotion (MT), a Transformer-based framework that uses only a monocular camera to jointly solve localization and prediction tasks.","Our framework has two main modules: Bird's Eye View (BEV) localization and trajectory prediction.","The BEV localization module estimates the position of a person using 2D human poses, enhanced by a novel directional loss for smoother sequential localizations.","The trajectory prediction module predicts future motion from these estimates.","We show that by jointly training both tasks with our unified framework, our method is more robust in real-world scenarios made of noisy inputs.","We validate our MT network on both curated and non-curated datasets.","On the curated dataset, MT achieves around 12% improvement over baseline models on BEV localization and trajectory prediction.","On real-world non-curated dataset, experimental results indicate that MT maintains similar performance levels, highlighting its robustness and generalization capability.","The code is available at https://github.com/vita-epfl/MonoTransmotion."],"url":"http://arxiv.org/abs/2503.03535v1"}
{"created":"2025-03-05 14:14:25","title":"AI-Enabled Conversational Journaling for Advancing Parkinson's Disease Symptom Tracking","abstract":"Journaling plays a crucial role in managing chronic conditions by allowing patients to document symptoms and medication intake, providing essential data for long-term care. While valuable, traditional journaling methods often rely on static, self-directed entries, lacking interactive feedback and real-time guidance. This gap can result in incomplete or imprecise information, limiting its usefulness for effective treatment. To address this gap, we introduce PATRIKA, an AI-enabled prototype designed specifically for people with Parkinson's disease (PwPD). The system incorporates cooperative conversation principles, clinical interview simulations, and personalization to create a more effective and user-friendly journaling experience. Through two user studies with PwPD and iterative refinement of PATRIKA, we demonstrate conversational journaling's significant potential in patient engagement and collecting clinically valuable information. Our results showed that generating probing questions PATRIKA turned journaling into a bi-directional interaction. Additionally, we offer insights for designing journaling systems for healthcare and future directions for promoting sustained journaling.","sentences":["Journaling plays a crucial role in managing chronic conditions by allowing patients to document symptoms and medication intake, providing essential data for long-term care.","While valuable, traditional journaling methods often rely on static, self-directed entries, lacking interactive feedback and real-time guidance.","This gap can result in incomplete or imprecise information, limiting its usefulness for effective treatment.","To address this gap, we introduce PATRIKA, an AI-enabled prototype designed specifically for people with Parkinson's disease (PwPD).","The system incorporates cooperative conversation principles, clinical interview simulations, and personalization to create a more effective and user-friendly journaling experience.","Through two user studies with PwPD and iterative refinement of PATRIKA, we demonstrate conversational journaling's significant potential in patient engagement and collecting clinically valuable information.","Our results showed that generating probing questions PATRIKA turned journaling into a bi-directional interaction.","Additionally, we offer insights for designing journaling systems for healthcare and future directions for promoting sustained journaling."],"url":"http://arxiv.org/abs/2503.03532v1"}
{"created":"2025-03-05 14:11:19","title":"Higher Stakes, Healthier Trust? An Application-Grounded Approach to Assessing Healthy Trust in High-Stakes Human-AI Collaboration","abstract":"Human-AI collaboration is increasingly promoted to improve high-stakes decision-making, yet its benefits have not been fully realized. Application-grounded evaluations are needed to better evaluate methods for improving collaboration but often require domain experts, making studies costly and limiting their generalizability. Current evaluation methods are constrained by limited public datasets and reliance on proxy tasks. To address these challenges, we propose an application-grounded framework for large-scale, online evaluations of vision-based decision-making tasks. The framework introduces Blockies, a parametric approach for generating datasets of simulated diagnostic tasks, offering control over the traits and biases in the data used to train real-world models. These tasks are designed to be easy to learn but difficult to master, enabling participation by non-experts. The framework also incorporates storytelling and monetary incentives to manipulate perceived task stakes. An initial empirical study demonstrated that the high-stakes condition significantly reduced healthy distrust of AI, despite longer decision-making times. These findings underscore the importance of perceived stakes in fostering healthy distrust and demonstrate the framework's potential for scalable evaluation of high-stakes Human-AI collaboration.","sentences":["Human-AI collaboration is increasingly promoted to improve high-stakes decision-making, yet its benefits have not been fully realized.","Application-grounded evaluations are needed to better evaluate methods for improving collaboration but often require domain experts, making studies costly and limiting their generalizability.","Current evaluation methods are constrained by limited public datasets and reliance on proxy tasks.","To address these challenges, we propose an application-grounded framework for large-scale, online evaluations of vision-based decision-making tasks.","The framework introduces Blockies, a parametric approach for generating datasets of simulated diagnostic tasks, offering control over the traits and biases in the data used to train real-world models.","These tasks are designed to be easy to learn but difficult to master, enabling participation by non-experts.","The framework also incorporates storytelling and monetary incentives to manipulate perceived task stakes.","An initial empirical study demonstrated that the high-stakes condition significantly reduced healthy distrust of AI, despite longer decision-making times.","These findings underscore the importance of perceived stakes in fostering healthy distrust and demonstrate the framework's potential for scalable evaluation of high-stakes Human-AI collaboration."],"url":"http://arxiv.org/abs/2503.03529v1"}
{"created":"2025-03-05 14:07:29","title":"O-RAN xApps Conflict Management using Graph Convolutional Networks","abstract":"Open Radio Access Network (O-RAN) adopts a flexible, open, and virtualized structure with standardized interfaces, reducing dependency on a single supplier. Conflict management in O-RAN refers to the process of identifying and resolving conflicts between network applications. xApps are applications deployed at the RAN Intelligent Controller (RIC) that leverage advanced AI/ML algorithms to make dynamic decisions for network optimization. The lack of a unified mechanism to coordinate and prioritize the actions of different applications can create three types of conflicts (direct, indirect, and implicit). In our paper, we introduce a novel data-driven GCN-based method called Graph-based xApps Conflict and Root Cause Analysis Engine (GRACE) based on Graph Convolutional Network (GCN). It detects three types of conflicts (direct, indirect, and implicit) and pinpoints the root causes (xApps). GRACE captures the complex and hidden dependencies among the xApps, the controlled parameters, and the KPIs in O-RAN to detect possible conflicts. Then, it identifies the root causes (xApps) contributing to the detected conflicts. The proposed method was tested on highly imbalanced datasets where the number of conflict instances ranges from 40% to 10%. The model is tested in a setting that simulates real-world scenarios where conflicts are rare to assess its performance and generalizability. Experimental results demonstrate an exceptional performance, achieving a high F1-score greater than 98% for all the case studies.","sentences":["Open Radio Access Network (O-RAN) adopts a flexible, open, and virtualized structure with standardized interfaces, reducing dependency on a single supplier.","Conflict management in O-RAN refers to the process of identifying and resolving conflicts between network applications.","xApps are applications deployed at the RAN Intelligent Controller (RIC) that leverage advanced AI/ML algorithms to make dynamic decisions for network optimization.","The lack of a unified mechanism to coordinate and prioritize the actions of different applications can create three types of conflicts (direct, indirect, and implicit).","In our paper, we introduce a novel data-driven GCN-based method called Graph-based xApps Conflict and Root Cause Analysis Engine (GRACE) based on Graph Convolutional Network (GCN).","It detects three types of conflicts (direct, indirect, and implicit) and pinpoints the root causes (xApps).","GRACE captures the complex and hidden dependencies among the xApps, the controlled parameters, and the KPIs in O-RAN to detect possible conflicts.","Then, it identifies the root causes (xApps) contributing to the detected conflicts.","The proposed method was tested on highly imbalanced datasets where the number of conflict instances ranges from 40% to 10%.","The model is tested in a setting that simulates real-world scenarios where conflicts are rare to assess its performance and generalizability.","Experimental results demonstrate an exceptional performance, achieving a high F1-score greater than 98% for all the case studies."],"url":"http://arxiv.org/abs/2503.03523v1"}
{"created":"2025-03-05 13:55:26","title":"Mineral segmentation using electron microscope images and spectral sampling through multimodal graph neural networks","abstract":"We propose a novel Graph Neural Network-based method for segmentation based on data fusion of multimodal Scanning Electron Microscope (SEM) images. In most cases, Backscattered Electron (BSE) images obtained using SEM do not contain sufficient information for mineral segmentation. Therefore, imaging is often complemented with point-wise Energy-Dispersive X-ray Spectroscopy (EDS) spectral measurements that provide highly accurate information about the chemical composition but that are time-consuming to acquire. This motivates the use of sparse spectral data in conjunction with BSE images for mineral segmentation. The unstructured nature of the spectral data makes most traditional image fusion techniques unsuitable for BSE-EDS fusion. We propose using graph neural networks to fuse the two modalities and segment the mineral phases simultaneously. Our results demonstrate that providing EDS data for as few as 1% of BSE pixels produces accurate segmentation, enabling rapid analysis of mineral samples. The proposed data fusion pipeline is versatile and can be adapted to other domains that involve image data and point-wise measurements.","sentences":["We propose a novel Graph Neural Network-based method for segmentation based on data fusion of multimodal Scanning Electron Microscope (SEM) images.","In most cases, Backscattered Electron (BSE) images obtained using SEM do not contain sufficient information for mineral segmentation.","Therefore, imaging is often complemented with point-wise Energy-Dispersive X-ray Spectroscopy (EDS) spectral measurements that provide highly accurate information about the chemical composition but that are time-consuming to acquire.","This motivates the use of sparse spectral data in conjunction with BSE images for mineral segmentation.","The unstructured nature of the spectral data makes most traditional image fusion techniques unsuitable for BSE-EDS fusion.","We propose using graph neural networks to fuse the two modalities and segment the mineral phases simultaneously.","Our results demonstrate that providing EDS data for as few as 1% of BSE pixels produces accurate segmentation, enabling rapid analysis of mineral samples.","The proposed data fusion pipeline is versatile and can be adapted to other domains that involve image data and point-wise measurements."],"url":"http://arxiv.org/abs/2503.03507v1"}
{"created":"2025-03-05 13:54:13","title":"Rethinking Synthetic Data definitions: A privacy driven approach","abstract":"Synthetic data is gaining traction as a cost-effective solution for the increasing data demands of AI development and can be generated either from existing knowledge or derived data captured from real-world events. The source of the synthetic data generation and the technique used significantly impacts its residual privacy risk and therefore its opportunity for sharing. Traditional classification of synthetic data types no longer fit the newer generation techniques and there is a need to better align the classification with practical needs. We suggest a new way of grouping synthetic data types that better supports privacy evaluations to aid regulatory policymaking. Our novel classification provides flexibility to new advancements like deep generative methods and offers a more practical framework for future applications.","sentences":["Synthetic data is gaining traction as a cost-effective solution for the increasing data demands of AI development and can be generated either from existing knowledge or derived data captured from real-world events.","The source of the synthetic data generation and the technique used significantly impacts its residual privacy risk and therefore its opportunity for sharing.","Traditional classification of synthetic data types no longer fit the newer generation techniques and there is a need to better align the classification with practical needs.","We suggest a new way of grouping synthetic data types that better supports privacy evaluations to aid regulatory policymaking.","Our novel classification provides flexibility to new advancements like deep generative methods and offers a more practical framework for future applications."],"url":"http://arxiv.org/abs/2503.03506v1"}
{"created":"2025-03-05 13:46:39","title":"Topo Goes Political: TDA-Based Controversy Detection in Imbalanced Reddit Political Data","abstract":"The detection of controversial content in political discussions on the Internet is a critical challenge in maintaining healthy digital discourse. Unlike much of the existing literature that relies on synthetically balanced data, our work preserves the natural distribution of controversial and non-controversial posts. This real-world imbalance highlights a core challenge that needs to be addressed for practical deployment. Our study re-evaluates well-established methods for detecting controversial content. We curate our own dataset focusing on the Indian political context that preserves the natural distribution of controversial content, with only 12.9% of the posts in our dataset being controversial. This disparity reflects the true imbalance in real-world political discussions and highlights a critical limitation in the existing evaluation methods. Benchmarking on datasets that model data imbalance is vital for ensuring real-world applicability. Thus, in this work, (i) we release our dataset, with an emphasis on class imbalance, that focuses on the Indian political context, (ii) we evaluate existing methods from this domain on this dataset and demonstrate their limitations in the imbalanced setting, (iii) we introduce an intuitive metric to measure a model's robustness to class imbalance, (iv) we also incorporate ideas from the domain of Topological Data Analysis, specifically Persistent Homology, to curate features that provide richer representations of the data. Furthermore, we benchmark models trained with topological features against established baselines.","sentences":["The detection of controversial content in political discussions on the Internet is a critical challenge in maintaining healthy digital discourse.","Unlike much of the existing literature that relies on synthetically balanced data, our work preserves the natural distribution of controversial and non-controversial posts.","This real-world imbalance highlights a core challenge that needs to be addressed for practical deployment.","Our study re-evaluates well-established methods for detecting controversial content.","We curate our own dataset focusing on the Indian political context that preserves the natural distribution of controversial content, with only 12.9% of the posts in our dataset being controversial.","This disparity reflects the true imbalance in real-world political discussions and highlights a critical limitation in the existing evaluation methods.","Benchmarking on datasets that model data imbalance is vital for ensuring real-world applicability.","Thus, in this work, (i) we release our dataset, with an emphasis on class imbalance, that focuses on the Indian political context, (ii) we evaluate existing methods from this domain on this dataset and demonstrate their limitations in the imbalanced setting, (iii) we introduce an intuitive metric to measure a model's robustness to class imbalance, (iv) we also incorporate ideas from the domain of Topological Data Analysis, specifically Persistent Homology, to curate features that provide richer representations of the data.","Furthermore, we benchmark models trained with topological features against established baselines."],"url":"http://arxiv.org/abs/2503.03500v1"}
{"created":"2025-03-05 13:34:31","title":"Oblivious Digital Tokens","abstract":"A computing device typically identifies itself by exhibiting unique measurable behavior or by proving its knowledge of a secret. In both cases, the identifying device must reveal information to a verifier. Considerable research has focused on protecting identifying entities (provers) and reducing the amount of leaked data. However, little has been done to conceal the fact that the verification occurred.   We show how this problem naturally arises in the context of digital emblems, which were recently proposed by the International Committee of the Red Cross to protect digital resources during cyber-conflicts. To address this new and important open problem, we define a new primitive, called an Oblivious Digital Token (ODT) that can be verified obliviously. Verifiers can use this procedure to check whether a device has an ODT without revealing to any other parties (including the device itself) that this check occurred. We demonstrate the feasibility of ODTs and present a concrete construction that provably meets the ODT security requirements, even if the prover device's software is fully compromised. We also implement a prototype of the proposed construction and evaluate its performance, thereby confirming its practicality.","sentences":["A computing device typically identifies itself by exhibiting unique measurable behavior or by proving its knowledge of a secret.","In both cases, the identifying device must reveal information to a verifier.","Considerable research has focused on protecting identifying entities (provers) and reducing the amount of leaked data.","However, little has been done to conceal the fact that the verification occurred.   ","We show how this problem naturally arises in the context of digital emblems, which were recently proposed by the International Committee of the Red Cross to protect digital resources during cyber-conflicts.","To address this new and important open problem, we define a new primitive, called an Oblivious Digital Token (ODT) that can be verified obliviously.","Verifiers can use this procedure to check whether a device has an ODT without revealing to any other parties (including the device itself) that this check occurred.","We demonstrate the feasibility of ODTs and present a concrete construction that provably meets the ODT security requirements, even if the prover device's software is fully compromised.","We also implement a prototype of the proposed construction and evaluate its performance, thereby confirming its practicality."],"url":"http://arxiv.org/abs/2503.03494v1"}
{"created":"2025-03-05 13:29:23","title":"Federated Learning for Predicting Mild Cognitive Impairment to Dementia Conversion","abstract":"Dementia is a progressive condition that impairs an individual's cognitive health and daily functioning, with mild cognitive impairment (MCI) often serving as its precursor. The prediction of MCI to dementia conversion has been well studied, but previous studies have almost always focused on traditional Machine Learning (ML) based methods that require sharing sensitive clinical information to train predictive models. This study proposes a privacy-enhancing solution using Federated Learning (FL) to train predictive models for MCI to dementia conversion without sharing sensitive data, leveraging socio demographic and cognitive measures. We simulated and compared two network architectures, Peer to Peer (P2P) and client-server, to enable collaborative learning. Our results demonstrated that FL had comparable predictive performance to centralized ML, and each clinical site showed similar performance without sharing local data. Moreover, the predictive performance of FL models was superior to site specific models trained without collaboration. This work highlights that FL can eliminate the need for data sharing without compromising model efficacy.","sentences":["Dementia is a progressive condition that impairs an individual's cognitive health and daily functioning, with mild cognitive impairment (MCI) often serving as its precursor.","The prediction of MCI to dementia conversion has been well studied, but previous studies have almost always focused on traditional Machine Learning (ML) based methods that require sharing sensitive clinical information to train predictive models.","This study proposes a privacy-enhancing solution using Federated Learning (FL) to train predictive models for MCI to dementia conversion without sharing sensitive data, leveraging socio demographic and cognitive measures.","We simulated and compared two network architectures, Peer to Peer (P2P) and client-server, to enable collaborative learning.","Our results demonstrated that FL had comparable predictive performance to centralized ML, and each clinical site showed similar performance without sharing local data.","Moreover, the predictive performance of FL models was superior to site specific models trained without collaboration.","This work highlights that FL can eliminate the need for data sharing without compromising model efficacy."],"url":"http://arxiv.org/abs/2503.03489v1"}
{"created":"2025-03-05 13:29:01","title":"Logarithmic-Time Internal Pattern Matching Queries in Compressed and Dynamic Texts","abstract":"Internal Pattern Matching (IPM) queries on a text $T$, given two fragments $X$ and $Y$ of $T$ such that $|Y|<2|X|$, ask to compute all exact occurrences of $X$ within $Y$. IPM queries have been introduced by Kociumaka, Radoszewski, Rytter, and Wale\\'n [SODA'15 & SICOMP'24], who showed that they can be answered in $O(1)$ time using a data structure of size $O(n)$ and used this result to answer various queries about fragments of $T$.   In this work, we study IPM queries on compressed and dynamic strings. Our result is an $O(\\log n)$-time query algorithm applicable to any balanced recompression-based run-length straight-line program (RLSLP). In particular, one can use it on top of the RLSLP of Kociumaka, Navarro, and Prezza [IEEE TIT'23], whose size $O\\big(\\delta \\log \\frac{n\\log \\sigma}{\\delta \\log n}\\big)$ is optimal (among all text representations) as a function of the text length $n$, the alphabet size $\\sigma$, and the substring complexity $\\delta$. Our procedure does not rely on any preprocessing of the underlying RLSLP, which makes it readily applicable on top of the dynamic strings data structure of Gawrychowski, Karczmarz, Kociumaka, {\\L}\\k{a}cki and Sankowski [SODA'18], which supports fully persistent updates in logarithmic time with high probability.","sentences":["Internal Pattern Matching (IPM) queries on a text $T$, given two fragments $X$ and $Y$ of $T$ such that $|Y|<2|X|$, ask to compute all exact occurrences of $X$ within $Y$. IPM queries have been introduced by Kociumaka, Radoszewski, Rytter, and Wale\\'n","[SODA'15 & SICOMP'24], who showed that they can be answered in $O(1)$ time using a data structure of size $O(n)$ and used this result to answer various queries about fragments of $T$.   In this work, we study IPM queries on compressed and dynamic strings.","Our result is an $O(\\log n)$-time query algorithm applicable to any balanced recompression-based run-length straight-line program (RLSLP).","In particular, one can use it on top of the RLSLP of Kociumaka, Navarro, and Prezza","[IEEE TIT'23], whose size $O\\big(\\delta \\log \\frac{n\\log \\sigma}{\\delta \\log n}\\big)$ is optimal (among all text representations) as a function of the text length $n$, the alphabet size $\\sigma$, and the substring complexity $\\delta$.","Our procedure does not rely on any preprocessing of the underlying RLSLP, which makes it readily applicable on top of the dynamic strings data structure of Gawrychowski, Karczmarz, Kociumaka, {\\L}\\k{a}cki and Sankowski [SODA'18], which supports fully persistent updates in logarithmic time with high probability."],"url":"http://arxiv.org/abs/2503.03488v1"}
{"created":"2025-03-05 13:24:58","title":"Differentially Private Learners for Heterogeneous Treatment Effects","abstract":"Patient data is widely used to estimate heterogeneous treatment effects and thus understand the effectiveness and safety of drugs. Yet, patient data includes highly sensitive information that must be kept private. In this work, we aim to estimate the conditional average treatment effect (CATE) from observational data under differential privacy. Specifically, we present DP-CATE, a novel framework for CATE estimation that is Neyman-orthogonal and further ensures differential privacy of the estimates. Our framework is highly general: it applies to any two-stage CATE meta-learner with a Neyman-orthogonal loss function, and any machine learning model can be used for nuisance estimation. We further provide an extension of our DP-CATE, where we employ RKHS regression to release the complete CATE function while ensuring differential privacy. We demonstrate our DP-CATE across various experiments using synthetic and real-world datasets. To the best of our knowledge, we are the first to provide a framework for CATE estimation that is Neyman-orthogonal and differentially private.","sentences":["Patient data is widely used to estimate heterogeneous treatment effects and thus understand the effectiveness and safety of drugs.","Yet, patient data includes highly sensitive information that must be kept private.","In this work, we aim to estimate the conditional average treatment effect (CATE) from observational data under differential privacy.","Specifically, we present DP-CATE, a novel framework for CATE estimation that is Neyman-orthogonal and further ensures differential privacy of the estimates.","Our framework is highly general: it applies to any two-stage CATE meta-learner with a Neyman-orthogonal loss function, and any machine learning model can be used for nuisance estimation.","We further provide an extension of our DP-CATE, where we employ RKHS regression to release the complete CATE function while ensuring differential privacy.","We demonstrate our DP-CATE across various experiments using synthetic and real-world datasets.","To the best of our knowledge, we are the first to provide a framework for CATE estimation that is Neyman-orthogonal and differentially private."],"url":"http://arxiv.org/abs/2503.03486v1"}
{"created":"2025-03-05 13:24:57","title":"TEDDY: A Family Of Foundation Models For Understanding Single Cell Biology","abstract":"Understanding the biological mechanism of disease is critical for medicine, and in particular drug discovery. AI-powered analysis of genome-scale biological data hold great potential in this regard. The increasing availability of single-cell RNA sequencing data has enabled the development of large foundation models for disease biology. However, existing foundation models either do not improve or only modestly improve over task-specific models in downstream applications. Here, we explored two avenues for improving the state-of-the-art. First, we scaled the pre-training dataset to 116 million cells, which is larger than those used by previous models. Second, we leveraged the availability of large-scale biological annotations as a form of supervision during pre-training. We trained the TEDDY family of models comprising six transformer-based state-of-the-art single-cell foundation models with 70 million, 160 million, and 400 million parameters. We vetted our models on two downstream evaluation tasks -- identifying the underlying disease state of held-out donors not seen during training and distinguishing healthy cells from diseased ones for disease conditions and donors not seen during training. Scaling experiments showed that performance improved predictably with both data volume and parameter count. Our models showed substantial improvement over existing work on the first task and more muted improvements on the second.","sentences":["Understanding the biological mechanism of disease is critical for medicine, and in particular drug discovery.","AI-powered analysis of genome-scale biological data hold great potential in this regard.","The increasing availability of single-cell RNA sequencing data has enabled the development of large foundation models for disease biology.","However, existing foundation models either do not improve or only modestly improve over task-specific models in downstream applications.","Here, we explored two avenues for improving the state-of-the-art.","First, we scaled the pre-training dataset to 116 million cells, which is larger than those used by previous models.","Second, we leveraged the availability of large-scale biological annotations as a form of supervision during pre-training.","We trained the TEDDY family of models comprising six transformer-based state-of-the-art single-cell foundation models with 70 million, 160 million, and 400 million parameters.","We vetted our models on two downstream evaluation tasks -- identifying the underlying disease state of held-out donors not seen during training and distinguishing healthy cells from diseased ones for disease conditions and donors not seen during training.","Scaling experiments showed that performance improved predictably with both data volume and parameter count.","Our models showed substantial improvement over existing work on the first task and more muted improvements on the second."],"url":"http://arxiv.org/abs/2503.03485v1"}
{"created":"2025-03-05 13:16:55","title":"SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning","abstract":"Vision-language-action models (VLAs) have shown great potential as generalist robot policies. However, these models pose urgent safety challenges during deployment, including the risk of physical harm to the environment, the robot itself, and humans. How can safety be explicitly incorporated into VLAs? In this work, we propose SafeVLA, a novel algorithm designed to integrate safety into VLAs, ensuring the protection of the environment, robot hardware and humans in real-world settings. SafeVLA effectively balances safety and task performance by employing large-scale constrained learning within simulated environments. We demonstrate that SafeVLA outperforms the current state-of-the-art method in both safety and task performance, achieving average improvements of 83.58% and 3.85%, respectively, in simulation. By prioritizing safety, our approach eliminates high-risk behaviors and reduces the upper bound of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby significantly mitigating long-tail risks. Furthermore, the learned safety constraints generalize to diverse, unseen scenarios, including multiple out-of-distribution perturbations and tasks. Our data, models and newly proposed benchmark environment are available at https://sites.google.com/view/pku-safevla.","sentences":["Vision-language-action models (VLAs) have shown great potential as generalist robot policies.","However, these models pose urgent safety challenges during deployment, including the risk of physical harm to the environment, the robot itself, and humans.","How can safety be explicitly incorporated into VLAs?","In this work, we propose SafeVLA, a novel algorithm designed to integrate safety into VLAs, ensuring the protection of the environment, robot hardware and humans in real-world settings.","SafeVLA effectively balances safety and task performance by employing large-scale constrained learning within simulated environments.","We demonstrate that SafeVLA outperforms the current state-of-the-art method in both safety and task performance, achieving average improvements of 83.58% and 3.85%, respectively, in simulation.","By prioritizing safety, our approach eliminates high-risk behaviors and reduces the upper bound of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby significantly mitigating long-tail risks.","Furthermore, the learned safety constraints generalize to diverse, unseen scenarios, including multiple out-of-distribution perturbations and tasks.","Our data, models and newly proposed benchmark environment are available at https://sites.google.com/view/pku-safevla."],"url":"http://arxiv.org/abs/2503.03480v1"}
{"created":"2025-03-05 12:54:54","title":"Generative Artificial Intelligence in Robotic Manipulation: A Survey","abstract":"This survey provides a comprehensive review on recent advancements of generative learning models in robotic manipulation, addressing key challenges in the field. Robotic manipulation faces critical bottlenecks, including significant challenges in insufficient data and inefficient data acquisition, long-horizon and complex task planning, and the multi-modality reasoning ability for robust policy learning performance across diverse environments. To tackle these challenges, this survey introduces several generative model paradigms, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), diffusion models, probabilistic flow models, and autoregressive models, highlighting their strengths and limitations. The applications of these models are categorized into three hierarchical layers: the Foundation Layer, focusing on data generation and reward generation; the Intermediate Layer, covering language, code, visual, and state generation; and the Policy Layer, emphasizing grasp generation and trajectory generation. Each layer is explored in detail, along with notable works that have advanced the state of the art. Finally, the survey outlines future research directions and challenges, emphasizing the need for improved efficiency in data utilization, better handling of long-horizon tasks, and enhanced generalization across diverse robotic scenarios. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/GAI4Manipulation/AwesomeGAIManipulation","sentences":["This survey provides a comprehensive review on recent advancements of generative learning models in robotic manipulation, addressing key challenges in the field.","Robotic manipulation faces critical bottlenecks, including significant challenges in insufficient data and inefficient data acquisition, long-horizon and complex task planning, and the multi-modality reasoning ability for robust policy learning performance across diverse environments.","To tackle these challenges, this survey introduces several generative model paradigms, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), diffusion models, probabilistic flow models, and autoregressive models, highlighting their strengths and limitations.","The applications of these models are categorized into three hierarchical layers: the Foundation Layer, focusing on data generation and reward generation; the Intermediate Layer, covering language, code, visual, and state generation; and the Policy Layer, emphasizing grasp generation and trajectory generation.","Each layer is explored in detail, along with notable works that have advanced the state of the art.","Finally, the survey outlines future research directions and challenges, emphasizing the need for improved efficiency in data utilization, better handling of long-horizon tasks, and enhanced generalization across diverse robotic scenarios.","All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/GAI4Manipulation/AwesomeGAIManipulation"],"url":"http://arxiv.org/abs/2503.03464v1"}
{"created":"2025-03-05 12:52:14","title":"Open-Source Large Language Models as Multilingual Crowdworkers: Synthesizing Open-Domain Dialogues in Several Languages With No Examples in Targets and No Machine Translation","abstract":"The prevailing paradigm in the domain of Open-Domain Dialogue agents predominantly focuses on the English language, encompassing both models and datasets. Furthermore, the financial and temporal investments required for crowdsourcing such datasets for finetuning are substantial, particularly when multiple languages are involved. Fortunately, advancements in Large Language Models (LLMs) have unveiled a plethora of possibilities across diverse tasks. Specifically, instruction-tuning has enabled LLMs to execute tasks based on natural language instructions, occasionally surpassing the performance of human crowdworkers. Additionally, these models possess the capability to function in various languages within a single thread. Consequently, to generate new samples in different languages, we propose leveraging these capabilities to replicate the data collection process. We introduce a pipeline for generating Open-Domain Dialogue data in multiple Target Languages using LLMs, with demonstrations provided in a unique Source Language. By eschewing explicit Machine Translation in this approach, we enhance the adherence to language-specific nuances. We apply this methodology to the PersonaChat dataset. To enhance the openness of generated dialogues and mimic real life scenarii, we added the notion of speech events corresponding to the type of conversation the speakers are involved in and also that of common ground which represents the premises of a conversation.","sentences":["The prevailing paradigm in the domain of Open-Domain Dialogue agents predominantly focuses on the English language, encompassing both models and datasets.","Furthermore, the financial and temporal investments required for crowdsourcing such datasets for finetuning are substantial, particularly when multiple languages are involved.","Fortunately, advancements in Large Language Models (LLMs) have unveiled a plethora of possibilities across diverse tasks.","Specifically, instruction-tuning has enabled LLMs to execute tasks based on natural language instructions, occasionally surpassing the performance of human crowdworkers.","Additionally, these models possess the capability to function in various languages within a single thread.","Consequently, to generate new samples in different languages, we propose leveraging these capabilities to replicate the data collection process.","We introduce a pipeline for generating Open-Domain Dialogue data in multiple Target Languages using LLMs, with demonstrations provided in a unique Source Language.","By eschewing explicit Machine Translation in this approach, we enhance the adherence to language-specific nuances.","We apply this methodology to the PersonaChat dataset.","To enhance the openness of generated dialogues and mimic real life scenarii, we added the notion of speech events corresponding to the type of conversation the speakers are involved in and also that of common ground which represents the premises of a conversation."],"url":"http://arxiv.org/abs/2503.03462v1"}
{"created":"2025-03-05 12:41:37","title":"Towards Continuous Experiment-driven MLOps","abstract":"Despite advancements in MLOps and AutoML, ML development still remains challenging for data scientists. First, there is poor support for and limited control over optimizing and evolving ML models. Second, there is lack of efficient mechanisms for continuous evolution of ML models which would leverage the knowledge gained in previous optimizations of the same or different models. We propose an experiment-driven MLOps approach which tackles these problems. Our approach relies on the concept of an experiment, which embodies a fully controllable optimization process. It introduces full traceability and repeatability to the optimization process, allows humans to be in full control of it, and enables continuous improvement of the ML system. Importantly, it also establishes knowledge, which is carried over and built across a series of experiments and allows for improving the efficiency of experimentation over time. We demonstrate our approach through its realization and application in the ExtremeXP1 project (Horizon Europe).","sentences":["Despite advancements in MLOps and AutoML, ML development still remains challenging for data scientists.","First, there is poor support for and limited control over optimizing and evolving ML models.","Second, there is lack of efficient mechanisms for continuous evolution of ML models which would leverage the knowledge gained in previous optimizations of the same or different models.","We propose an experiment-driven MLOps approach which tackles these problems.","Our approach relies on the concept of an experiment, which embodies a fully controllable optimization process.","It introduces full traceability and repeatability to the optimization process, allows humans to be in full control of it, and enables continuous improvement of the ML system.","Importantly, it also establishes knowledge, which is carried over and built across a series of experiments and allows for improving the efficiency of experimentation over time.","We demonstrate our approach through its realization and application in the ExtremeXP1 project (Horizon Europe)."],"url":"http://arxiv.org/abs/2503.03455v1"}
{"created":"2025-03-05 12:40:34","title":"Data Poisoning Attacks to Locally Differentially Private Range Query Protocols","abstract":"Trajectory data, which tracks movements through geographic locations, is crucial for improving real-world applications. However, collecting such sensitive data raises considerable privacy concerns. Local differential privacy (LDP) offers a solution by allowing individuals to locally perturb their trajectory data before sharing it. Despite its privacy benefits, LDP protocols are vulnerable to data poisoning attacks, where attackers inject fake data to manipulate aggregated results. In this work, we make the first attempt to analyze vulnerabilities in several representative LDP trajectory protocols. We propose \\textsc{TraP}, a heuristic algorithm for data \\underline{P}oisoning attacks using a prefix-suffix method to optimize fake \\underline{Tra}jectory selection, significantly reducing computational complexity. Our experimental results demonstrate that our attack can substantially increase target pattern occurrences in the perturbed trajectory dataset with few fake users. This study underscores the urgent need for robust defenses and better protocol designs to safeguard LDP trajectory data against malicious manipulation.","sentences":["Trajectory data, which tracks movements through geographic locations, is crucial for improving real-world applications.","However, collecting such sensitive data raises considerable privacy concerns.","Local differential privacy (LDP) offers a solution by allowing individuals to locally perturb their trajectory data before sharing it.","Despite its privacy benefits, LDP protocols are vulnerable to data poisoning attacks, where attackers inject fake data to manipulate aggregated results.","In this work, we make the first attempt to analyze vulnerabilities in several representative LDP trajectory protocols.","We propose \\textsc{TraP}, a heuristic algorithm for data \\underline{P}oisoning attacks using a prefix-suffix method to optimize fake \\underline{Tra}jectory selection, significantly reducing computational complexity.","Our experimental results demonstrate that our attack can substantially increase target pattern occurrences in the perturbed trajectory dataset with few fake users.","This study underscores the urgent need for robust defenses and better protocol designs to safeguard LDP trajectory data against malicious manipulation."],"url":"http://arxiv.org/abs/2503.03454v1"}
{"created":"2025-03-05 12:35:54","title":"Active Learning for Deep Learning-Based Hemodynamic Parameter Estimation","abstract":"Hemodynamic parameters such as pressure and wall shear stress play an important role in diagnosis, prognosis, and treatment planning in cardiovascular diseases. These parameters can be accurately computed using computational fluid dynamics (CFD), but CFD is computationally intensive. Hence, deep learning methods have been adopted as a surrogate to rapidly estimate CFD outcomes. A drawback of such data-driven models is the need for time-consuming reference CFD simulations for training. In this work, we introduce an active learning framework to reduce the number of CFD simulations required for the training of surrogate models, lowering the barriers to their deployment in new applications. We propose three distinct querying strategies to determine for which unlabeled samples CFD simulations should be obtained. These querying strategies are based on geometrical variance, ensemble uncertainty, and adherence to the physics governing fluid dynamics. We benchmark these methods on velocity field estimation in synthetic coronary artery bifurcations and find that they allow for substantial reductions in annotation cost. Notably, we find that our strategies reduce the number of samples required by up to 50% and make the trained models more robust to difficult cases. Our results show that active learning is a feasible strategy to increase the potential of deep learning-based CFD surrogates.","sentences":["Hemodynamic parameters such as pressure and wall shear stress play an important role in diagnosis, prognosis, and treatment planning in cardiovascular diseases.","These parameters can be accurately computed using computational fluid dynamics (CFD), but CFD is computationally intensive.","Hence, deep learning methods have been adopted as a surrogate to rapidly estimate CFD outcomes.","A drawback of such data-driven models is the need for time-consuming reference CFD simulations for training.","In this work, we introduce an active learning framework to reduce the number of CFD simulations required for the training of surrogate models, lowering the barriers to their deployment in new applications.","We propose three distinct querying strategies to determine for which unlabeled samples CFD simulations should be obtained.","These querying strategies are based on geometrical variance, ensemble uncertainty, and adherence to the physics governing fluid dynamics.","We benchmark these methods on velocity field estimation in synthetic coronary artery bifurcations and find that they allow for substantial reductions in annotation cost.","Notably, we find that our strategies reduce the number of samples required by up to 50% and make the trained models more robust to difficult cases.","Our results show that active learning is a feasible strategy to increase the potential of deep learning-based CFD surrogates."],"url":"http://arxiv.org/abs/2503.03453v1"}
{"created":"2025-03-05 12:25:22","title":"Biased Heritage: How Datasets Shape Models in Facial Expression Recognition","abstract":"In recent years, the rapid development of artificial intelligence (AI) systems has raised concerns about our ability to ensure their fairness, that is, how to avoid discrimination based on protected characteristics such as gender, race, or age. While algorithmic fairness is well-studied in simple binary classification tasks on tabular data, its application to complex, real-world scenarios-such as Facial Expression Recognition (FER)-remains underexplored. FER presents unique challenges: it is inherently multiclass, and biases emerge across intersecting demographic variables, each potentially comprising multiple protected groups. We present a comprehensive framework to analyze bias propagation from datasets to trained models in image-based FER systems, while introducing new bias metrics specifically designed for multiclass problems with multiple demographic groups. Our methodology studies bias propagation by (1) inducing controlled biases in FER datasets, (2) training models on these biased datasets, and (3) analyzing the correlation between dataset bias metrics and model fairness notions. Our findings reveal that stereotypical biases propagate more strongly to model predictions than representational biases, suggesting that preventing emotion-specific demographic patterns should be prioritized over general demographic balance in FER datasets. Additionally, we observe that biased datasets lead to reduced model accuracy, challenging the assumed fairness-accuracy trade-off.","sentences":["In recent years, the rapid development of artificial intelligence (AI) systems has raised concerns about our ability to ensure their fairness, that is, how to avoid discrimination based on protected characteristics such as gender, race, or age.","While algorithmic fairness is well-studied in simple binary classification tasks on tabular data, its application to complex, real-world scenarios-such as Facial Expression Recognition (FER)-remains underexplored.","FER presents unique challenges: it is inherently multiclass, and biases emerge across intersecting demographic variables, each potentially comprising multiple protected groups.","We present a comprehensive framework to analyze bias propagation from datasets to trained models in image-based FER systems, while introducing new bias metrics specifically designed for multiclass problems with multiple demographic groups.","Our methodology studies bias propagation by (1) inducing controlled biases in FER datasets, (2) training models on these biased datasets, and (3) analyzing the correlation between dataset bias metrics and model fairness notions.","Our findings reveal that stereotypical biases propagate more strongly to model predictions than representational biases, suggesting that preventing emotion-specific demographic patterns should be prioritized over general demographic balance in FER datasets.","Additionally, we observe that biased datasets lead to reduced model accuracy, challenging the assumed fairness-accuracy trade-off."],"url":"http://arxiv.org/abs/2503.03446v1"}
{"created":"2025-03-05 12:24:12","title":"Conceptualizing Uncertainty","abstract":"Uncertainty in machine learning refers to the degree of confidence or lack thereof in a model's predictions. While uncertainty quantification methods exist, explanations of uncertainty, especially in high-dimensional settings, remain an open challenge. Existing work focuses on feature attribution approaches which are restricted to local explanations. Understanding uncertainty, its origins, and characteristics on a global scale is crucial for enhancing interpretability and trust in a model's predictions. In this work, we propose to explain the uncertainty in high-dimensional data classification settings by means of concept activation vectors which give rise to local and global explanations of uncertainty. We demonstrate the utility of the generated explanations by leveraging them to refine and improve our model.","sentences":["Uncertainty in machine learning refers to the degree of confidence or lack thereof in a model's predictions.","While uncertainty quantification methods exist, explanations of uncertainty, especially in high-dimensional settings, remain an open challenge.","Existing work focuses on feature attribution approaches which are restricted to local explanations.","Understanding uncertainty, its origins, and characteristics on a global scale is crucial for enhancing interpretability and trust in a model's predictions.","In this work, we propose to explain the uncertainty in high-dimensional data classification settings by means of concept activation vectors which give rise to local and global explanations of uncertainty.","We demonstrate the utility of the generated explanations by leveraging them to refine and improve our model."],"url":"http://arxiv.org/abs/2503.03443v1"}
{"created":"2025-03-05 12:10:43","title":"Nearest Neighbor Searching in a Dynamic Simple Polygon","abstract":"In the nearest neighbor problem, we are given a set $S$ of point sites that we want to store such that we can find the nearest neighbor of a (new) query point efficiently. In the dynamic version of the problem, the goal is to design a data structure that supports both efficient queries and updates, i.e. insertions and deletions in $S$. This problem has been widely studied in various settings, ranging from points in the plane to more general distance measures and even points within simple polygons. When the sites do not live in the plane but in some domain, another dynamic problem arises: what happens if not the sites, but the domain itself is subject to updates?   Updating sites often results in local changes to the solution or data structure, while updating the domain may incur many global changes. For example, in the closest pair problem, inserting a point only requires us to check if this point is in the new closest pair, while updating the domain might change the distances between most pairs of points in our set. Presumably, this is the reason that this form of dynamization has received much less attention. Only some basic problems, such as shortest paths and ray shooting, have been studied in this setting.   Here, we tackle the nearest neighbor problem in a dynamic simple polygon. We allow insertions into both the set of sites and the polygon. An insertion in the polygon is the addition of a line segment starting at the boundary of the polygon. We present a near-linear size --in both the number of sites and the complexity of the polygon-- data structure with sublinear update and query time. This is the first nearest neighbor data structure that allows for updates to the domain.","sentences":["In the nearest neighbor problem, we are given a set $S$ of point sites that we want to store such that we can find the nearest neighbor of a (new) query point efficiently.","In the dynamic version of the problem, the goal is to design a data structure that supports both efficient queries and updates, i.e. insertions and deletions in $S$. This problem has been widely studied in various settings, ranging from points in the plane to more general distance measures and even points within simple polygons.","When the sites do not live in the plane but in some domain, another dynamic problem arises: what happens if not the sites, but the domain itself is subject to updates?   ","Updating sites often results in local changes to the solution or data structure, while updating the domain may incur many global changes.","For example, in the closest pair problem, inserting a point only requires us to check if this point is in the new closest pair, while updating the domain might change the distances between most pairs of points in our set.","Presumably, this is the reason that this form of dynamization has received much less attention.","Only some basic problems, such as shortest paths and ray shooting, have been studied in this setting.   ","Here, we tackle the nearest neighbor problem in a dynamic simple polygon.","We allow insertions into both the set of sites and the polygon.","An insertion in the polygon is the addition of a line segment starting at the boundary of the polygon.","We present a near-linear size --in both the number of sites and the complexity of the polygon-- data structure with sublinear update and query time.","This is the first nearest neighbor data structure that allows for updates to the domain."],"url":"http://arxiv.org/abs/2503.03435v1"}
{"created":"2025-03-05 12:10:14","title":"RASD: Retrieval-Augmented Speculative Decoding","abstract":"Speculative decoding accelerates inference in large language models (LLMs) by generating draft tokens for target model verification. Current approaches for obtaining draft tokens rely on lightweight draft models or additional model structures to generate draft tokens and retrieve context from databases. Due to the draft model's small size and limited training data, model-based speculative decoding frequently becomes less effective in out-of-domain scenarios. Additionally, the time cost of the drafting phase results in a low upper limit on acceptance length during the verification step, limiting overall efficiency. This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which adopts retrieval methods to enhance model-based speculative decoding. We introduce tree pruning and tree fusion to achieve this. Specifically, we develop a pruning method based on the draft model's probability distribution to construct the optimal retrieval tree. Second, we employ the longest prefix matching algorithm to merge the tree generated by the draft model with the retrieval tree, resulting in a unified tree for verification. Experimental results demonstrate that RASD achieves state-of-the-art inference acceleration across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD exhibits strong scalability, seamlessly integrating with various speculative decoding approaches, including both generation-based and retrieval-based methods.","sentences":["Speculative decoding accelerates inference in large language models (LLMs) by generating draft tokens for target model verification.","Current approaches for obtaining draft tokens rely on lightweight draft models or additional model structures to generate draft tokens and retrieve context from databases.","Due to the draft model's small size and limited training data, model-based speculative decoding frequently becomes less effective in out-of-domain scenarios.","Additionally, the time cost of the drafting phase results in a low upper limit on acceptance length during the verification step, limiting overall efficiency.","This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which adopts retrieval methods to enhance model-based speculative decoding.","We introduce tree pruning and tree fusion to achieve this.","Specifically, we develop a pruning method based on the draft model's probability distribution to construct the optimal retrieval tree.","Second, we employ the longest prefix matching algorithm to merge the tree generated by the draft model with the retrieval tree, resulting in a unified tree for verification.","Experimental results demonstrate that RASD achieves state-of-the-art inference acceleration across tasks such as DocQA, Summary, Code, and In-Domain QA.","Moreover, RASD exhibits strong scalability, seamlessly integrating with various speculative decoding approaches, including both generation-based and retrieval-based methods."],"url":"http://arxiv.org/abs/2503.03434v1"}
{"created":"2025-03-05 12:01:22","title":"Privacy is All You Need: Revolutionizing Wearable Health Data with Advanced PETs","abstract":"In a world where data is the new currency, wearable health devices offer unprecedented insights into daily life, continuously monitoring vital signs and metrics. However, this convenience raises privacy concerns, as these devices collect sensitive data that can be misused or breached. Traditional measures often fail due to real-time data processing needs and limited device power. Users also lack awareness and control over data sharing and usage. We propose a Privacy-Enhancing Technology (PET) framework for wearable devices, integrating federated learning, lightweight cryptographic methods, and selectively deployed blockchain technology. The blockchain acts as a secure ledger triggered only upon data transfer requests, granting users real-time notifications and control. By dismantling data monopolies, this approach returns data sovereignty to individuals. Through real-world applications like secure medical data sharing, privacy-preserving fitness tracking, and continuous health monitoring, our framework reduces privacy risks by up to 70 percent while preserving data utility and performance. This innovation sets a new benchmark for wearable privacy and can scale to broader IoT ecosystems, including smart homes and industry. As data continues to shape our digital landscape, our research underscores the critical need to maintain privacy and user control at the forefront of technological progress.","sentences":["In a world where data is the new currency, wearable health devices offer unprecedented insights into daily life, continuously monitoring vital signs and metrics.","However, this convenience raises privacy concerns, as these devices collect sensitive data that can be misused or breached.","Traditional measures often fail due to real-time data processing needs and limited device power.","Users also lack awareness and control over data sharing and usage.","We propose a Privacy-Enhancing Technology (PET) framework for wearable devices, integrating federated learning, lightweight cryptographic methods, and selectively deployed blockchain technology.","The blockchain acts as a secure ledger triggered only upon data transfer requests, granting users real-time notifications and control.","By dismantling data monopolies, this approach returns data sovereignty to individuals.","Through real-world applications like secure medical data sharing, privacy-preserving fitness tracking, and continuous health monitoring, our framework reduces privacy risks by up to 70 percent while preserving data utility and performance.","This innovation sets a new benchmark for wearable privacy and can scale to broader IoT ecosystems, including smart homes and industry.","As data continues to shape our digital landscape, our research underscores the critical need to maintain privacy and user control at the forefront of technological progress."],"url":"http://arxiv.org/abs/2503.03428v1"}
{"created":"2025-03-05 11:49:32","title":"Automatic Drywall Analysis for Progress Tracking and Quality Control in Construction","abstract":"Digitalization in the construction industry has become essential, enabling centralized, easy access to all relevant information of a building. Automated systems can facilitate the timely and resource-efficient documentation of changes, which is crucial for key processes such as progress tracking and quality control. This paper presents a method for image-based automated drywall analysis enabling construction progress and quality assessment through on-site camera systems. Our proposed solution integrates a deep learning-based instance segmentation model to detect and classify various drywall elements with an analysis module to cluster individual wall segments, estimate camera perspective distortions, and apply the corresponding corrections. This system extracts valuable information from images, enabling more accurate progress tracking and quality assessment on construction sites. Our main contributions include a fully automated pipeline for drywall analysis, improving instance segmentation accuracy through architecture modifications and targeted data augmentation, and a novel algorithm to extract important information from the segmentation results. Our modified model, enhanced with data augmentation, achieves significantly higher accuracy compared to other architectures, offering more detailed and precise information than existing approaches. Combined with the proposed drywall analysis steps, it enables the reliable automation of construction progress and quality assessment.","sentences":["Digitalization in the construction industry has become essential, enabling centralized, easy access to all relevant information of a building.","Automated systems can facilitate the timely and resource-efficient documentation of changes, which is crucial for key processes such as progress tracking and quality control.","This paper presents a method for image-based automated drywall analysis enabling construction progress and quality assessment through on-site camera systems.","Our proposed solution integrates a deep learning-based instance segmentation model to detect and classify various drywall elements with an analysis module to cluster individual wall segments, estimate camera perspective distortions, and apply the corresponding corrections.","This system extracts valuable information from images, enabling more accurate progress tracking and quality assessment on construction sites.","Our main contributions include a fully automated pipeline for drywall analysis, improving instance segmentation accuracy through architecture modifications and targeted data augmentation, and a novel algorithm to extract important information from the segmentation results.","Our modified model, enhanced with data augmentation, achieves significantly higher accuracy compared to other architectures, offering more detailed and precise information than existing approaches.","Combined with the proposed drywall analysis steps, it enables the reliable automation of construction progress and quality assessment."],"url":"http://arxiv.org/abs/2503.03422v1"}
{"created":"2025-03-05 11:47:41","title":"Simplicial SMOTE: Oversampling Solution to the Imbalanced Learning Problem","abstract":"SMOTE (Synthetic Minority Oversampling Technique) is the established geometric approach to random oversampling to balance classes in the imbalanced learning problem, followed by many extensions. Its idea is to introduce synthetic data points of the minor class, with each new point being the convex combination of an existing data point and one of its k-nearest neighbors. In this paper, by viewing SMOTE as sampling from the edges of a geometric neighborhood graph and borrowing tools from the topological data analysis, we propose a novel technique, Simplicial SMOTE, that samples from the simplices of a geometric neighborhood simplicial complex. A new synthetic point is defined by the barycentric coordinates w.r.t. a simplex spanned by an arbitrary number of data points being sufficiently close rather than a pair. Such a replacement of the geometric data model results in better coverage of the underlying data distribution compared to existing geometric sampling methods and allows the generation of synthetic points of the minority class closer to the majority class on the decision boundary. We experimentally demonstrate that our Simplicial SMOTE outperforms several popular geometric sampling methods, including the original SMOTE. Moreover, we show that simplicial sampling can be easily integrated into existing SMOTE extensions. We generalize and evaluate simplicial extensions of the classic Borderline SMOTE, Safe-level SMOTE, and ADASYN algorithms, all of which outperform their graph-based counterparts.","sentences":["SMOTE (Synthetic Minority Oversampling Technique) is the established geometric approach to random oversampling to balance classes in the imbalanced learning problem, followed by many extensions.","Its idea is to introduce synthetic data points of the minor class, with each new point being the convex combination of an existing data point and one of its k-nearest neighbors.","In this paper, by viewing SMOTE as sampling from the edges of a geometric neighborhood graph and borrowing tools from the topological data analysis, we propose a novel technique, Simplicial SMOTE, that samples from the simplices of a geometric neighborhood simplicial complex.","A new synthetic point is defined by the barycentric coordinates w.r.t.","a simplex spanned by an arbitrary number of data points being sufficiently close rather than a pair.","Such a replacement of the geometric data model results in better coverage of the underlying data distribution compared to existing geometric sampling methods and allows the generation of synthetic points of the minority class closer to the majority class on the decision boundary.","We experimentally demonstrate that our Simplicial SMOTE outperforms several popular geometric sampling methods, including the original SMOTE.","Moreover, we show that simplicial sampling can be easily integrated into existing SMOTE extensions.","We generalize and evaluate simplicial extensions of the classic Borderline SMOTE, Safe-level SMOTE, and ADASYN algorithms, all of which outperform their graph-based counterparts."],"url":"http://arxiv.org/abs/2503.03418v1"}
{"created":"2025-03-05 11:24:55","title":"Evolutionary Prediction Games","abstract":"When users decide whether to use a system based on the quality of predictions they receive, learning has the capacity to shape the population of users it serves - for better or worse. This work aims to study the long-term implications of this process through the lens of evolutionary game theory. We introduce and study evolutionary prediction games, designed to capture the role of learning as a driver of natural selection between groups of users, and hence a determinant of evolutionary outcomes. Our main theoretical results show that: (i) in settings with unlimited data and compute, learning tends to reinforce the survival of the fittest, and (ii) in more realistic settings, opportunities for coexistence emerge. We analyze these opportunities in terms of their stability and feasibility, present several mechanisms that can sustain their existence, and empirically demonstrate our findings using real and synthetic data.","sentences":["When users decide whether to use a system based on the quality of predictions they receive, learning has the capacity to shape the population of users it serves - for better or worse.","This work aims to study the long-term implications of this process through the lens of evolutionary game theory.","We introduce and study evolutionary prediction games, designed to capture the role of learning as a driver of natural selection between groups of users, and hence a determinant of evolutionary outcomes.","Our main theoretical results show that: (i) in settings with unlimited data and compute, learning tends to reinforce the survival of the fittest, and (ii) in more realistic settings, opportunities for coexistence emerge.","We analyze these opportunities in terms of their stability and feasibility, present several mechanisms that can sustain their existence, and empirically demonstrate our findings using real and synthetic data."],"url":"http://arxiv.org/abs/2503.03401v1"}
{"created":"2025-03-05 11:21:37","title":"Predicting Practically? Domain Generalization for Predictive Analytics in Real-world Environments","abstract":"Predictive machine learning models are widely used in customer relationship management (CRM) to forecast customer behaviors and support decision-making. However, the dynamic nature of customer behaviors often results in significant distribution shifts between training data and serving data, leading to performance degradation in predictive models. Domain generalization, which aims to train models that can generalize to unseen environments without prior knowledge of their distributions, has become a critical area of research. In this work, we propose a novel domain generalization method tailored to handle complex distribution shifts, encompassing both covariate and concept shifts. Our method builds upon the Distributionally Robust Optimization framework, optimizing model performance over a set of hypothetical worst-case distributions rather than relying solely on the training data. Through simulation experiments, we demonstrate the working mechanism of the proposed method. We also conduct experiments on a real-world customer churn dataset, and validate its effectiveness in both temporal and spatial generalization settings. Finally, we discuss the broader implications of our method for advancing Information Systems (IS) design research, particularly in building robust predictive models for dynamic managerial environments.","sentences":["Predictive machine learning models are widely used in customer relationship management (CRM) to forecast customer behaviors and support decision-making.","However, the dynamic nature of customer behaviors often results in significant distribution shifts between training data and serving data, leading to performance degradation in predictive models.","Domain generalization, which aims to train models that can generalize to unseen environments without prior knowledge of their distributions, has become a critical area of research.","In this work, we propose a novel domain generalization method tailored to handle complex distribution shifts, encompassing both covariate and concept shifts.","Our method builds upon the Distributionally Robust Optimization framework, optimizing model performance over a set of hypothetical worst-case distributions rather than relying solely on the training data.","Through simulation experiments, we demonstrate the working mechanism of the proposed method.","We also conduct experiments on a real-world customer churn dataset, and validate its effectiveness in both temporal and spatial generalization settings.","Finally, we discuss the broader implications of our method for advancing Information Systems (IS) design research, particularly in building robust predictive models for dynamic managerial environments."],"url":"http://arxiv.org/abs/2503.03399v1"}
{"created":"2025-03-05 11:02:29","title":"GNNMerge: Merging of GNN Models Without Accessing Training Data","abstract":"Model merging has gained prominence in machine learning as a method to integrate multiple trained models into a single model without accessing the original training data. While existing approaches have demonstrated success in domains such as computer vision and NLP, their application to Graph Neural Networks (GNNs) remains unexplored. These methods often rely on the assumption of shared initialization, which is seldom applicable to GNNs. In this work, we undertake the first benchmarking study of model merging algorithms for GNNs, revealing their limited effectiveness in this context. To address these challenges, we propose GNNMerge, which utilizes a task-agnostic node embedding alignment strategy to merge GNNs. Furthermore, we establish that under a mild relaxation, the proposed optimization objective admits direct analytical solutions for widely used GNN architectures, significantly enhancing its computational efficiency. Empirical evaluations across diverse datasets, tasks, and architectures establish GNNMerge to be up to 24% more accurate than existing methods while delivering over 2 orders of magnitude speed-up compared to training from scratch.","sentences":["Model merging has gained prominence in machine learning as a method to integrate multiple trained models into a single model without accessing the original training data.","While existing approaches have demonstrated success in domains such as computer vision and NLP, their application to Graph Neural Networks (GNNs) remains unexplored.","These methods often rely on the assumption of shared initialization, which is seldom applicable to GNNs.","In this work, we undertake the first benchmarking study of model merging algorithms for GNNs, revealing their limited effectiveness in this context.","To address these challenges, we propose GNNMerge, which utilizes a task-agnostic node embedding alignment strategy to merge GNNs.","Furthermore, we establish that under a mild relaxation, the proposed optimization objective admits direct analytical solutions for widely used GNN architectures, significantly enhancing its computational efficiency.","Empirical evaluations across diverse datasets, tasks, and architectures establish GNNMerge to be up to 24% more accurate than existing methods while delivering over 2 orders of magnitude speed-up compared to training from scratch."],"url":"http://arxiv.org/abs/2503.03384v1"}
{"created":"2025-03-05 10:46:03","title":"MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for Microscopic Images","abstract":"Existing generic unsupervised domain adaptation approaches require access to both a large labeled source dataset and a sufficient unlabeled target dataset during adaptation. However, collecting a large dataset, even if unlabeled, is a challenging and expensive endeavor, especially in medical imaging. In addition, constraints such as privacy issues can result in cases where source data is unavailable. Taking in consideration these challenges, we propose MIAdapt, an adaptive approach for Microscopic Imagery Adaptation as a solution for Source-free Few-shot Domain Adaptive Object detection (SF-FSDA). We also define two competitive baselines (1) Faster-FreeShot and (2) MT-FreeShot. Extensive experiments on the challenging M5-Malaria and Raabin-WBC datasets validate the effectiveness of MIAdapt. Without using any image from the source domain MIAdapt surpasses state-of-the-art source-free UDA (SF-UDA) methods by +21.3% mAP and few-shot domain adaptation (FSDA) approaches by +4.7% mAP on Raabin-WBC. Our code and models will be publicly available.","sentences":["Existing generic unsupervised domain adaptation approaches require access to both a large labeled source dataset and a sufficient unlabeled target dataset during adaptation.","However, collecting a large dataset, even if unlabeled, is a challenging and expensive endeavor, especially in medical imaging.","In addition, constraints such as privacy issues can result in cases where source data is unavailable.","Taking in consideration these challenges, we propose MIAdapt, an adaptive approach for Microscopic Imagery Adaptation as a solution for Source-free Few-shot Domain Adaptive Object detection (SF-FSDA).","We also define two competitive baselines (1) Faster-FreeShot and (2) MT-FreeShot.","Extensive experiments on the challenging M5-Malaria and Raabin-WBC datasets validate the effectiveness of MIAdapt.","Without using any image from the source domain MIAdapt surpasses state-of-the-art source-free UDA (SF-UDA) methods by +21.3% mAP and few-shot domain adaptation (FSDA) approaches by +4.7% mAP on Raabin-WBC.","Our code and models will be publicly available."],"url":"http://arxiv.org/abs/2503.03370v1"}
{"created":"2025-03-05 10:42:41","title":"TopoMortar: A dataset to evaluate image segmentation methods focused on topology accuracy","abstract":"We present TopoMortar, a brick wall dataset that is the first dataset specifically designed to evaluate topology-focused image segmentation methods, such as topology loss functions. TopoMortar enables to investigate in two ways whether methods incorporate prior topological knowledge. First, by eliminating challenges seen in real-world data, such as small training set, noisy labels, and out-of-distribution test-set images, that, as we show, impact the effectiveness of topology losses. Second, by allowing to assess in the same dataset topology accuracy across dataset challenges, isolating dataset-related effects from the effect of incorporating prior topological knowledge. In these two experiments, it is deliberately difficult to improve topology accuracy without actually using topology information, thus, permitting to attribute an improvement in topology accuracy to the incorporation of prior topological knowledge. To this end, TopoMortar includes three types of labels (accurate, noisy, pseudo-labels), two fixed training sets (large and small), and in-distribution and out-of-distribution test-set images. We compared eight loss functions on TopoMortar, and we found that clDice achieved the most topologically accurate segmentations, Skeleton Recall loss performed best particularly with noisy labels, and the relative advantageousness of the other loss functions depended on the experimental setting. Additionally, we show that simple methods, such as data augmentation and self-distillation, can elevate Cross entropy Dice loss to surpass most topology loss functions, and that those simple methods can enhance topology loss functions as well. clDice and Skeleton Recall loss, both skeletonization-based loss functions, were also the fastest to train, making this type of loss function a promising research direction. TopoMortar and our code can be found at https://github.com/jmlipman/TopoMortar","sentences":["We present TopoMortar, a brick wall dataset that is the first dataset specifically designed to evaluate topology-focused image segmentation methods, such as topology loss functions.","TopoMortar enables to investigate in two ways whether methods incorporate prior topological knowledge.","First, by eliminating challenges seen in real-world data, such as small training set, noisy labels, and out-of-distribution test-set images, that, as we show, impact the effectiveness of topology losses.","Second, by allowing to assess in the same dataset topology accuracy across dataset challenges, isolating dataset-related effects from the effect of incorporating prior topological knowledge.","In these two experiments, it is deliberately difficult to improve topology accuracy without actually using topology information, thus, permitting to attribute an improvement in topology accuracy to the incorporation of prior topological knowledge.","To this end, TopoMortar includes three types of labels (accurate, noisy, pseudo-labels), two fixed training sets (large and small), and in-distribution and out-of-distribution test-set images.","We compared eight loss functions on TopoMortar, and we found that clDice achieved the most topologically accurate segmentations, Skeleton Recall loss performed best particularly with noisy labels, and the relative advantageousness of the other loss functions depended on the experimental setting.","Additionally, we show that simple methods, such as data augmentation and self-distillation, can elevate Cross entropy Dice loss to surpass most topology loss functions, and that those simple methods can enhance topology loss functions as well.","clDice and Skeleton Recall loss, both skeletonization-based loss functions, were also the fastest to train, making this type of loss function a promising research direction.","TopoMortar and our code can be found at https://github.com/jmlipman/TopoMortar"],"url":"http://arxiv.org/abs/2503.03365v1"}
{"created":"2025-03-05 10:40:19","title":"From Infants to AI: Incorporating Infant-like Learning in Models Boosts Efficiency and Generalization in Learning Social Prediction Tasks","abstract":"Early in development, infants learn a range of useful concepts, which can be challenging from a computational standpoint. This early learning comes together with an initial understanding of aspects of the meaning of concepts, e.g., their implications, causality, and using them to predict likely future events. All this is accomplished in many cases with little or no supervision, and from relatively few examples, compared with current network models. In learning about objects and human-object interactions, early acquired and possibly innate concepts are often used in the process of learning additional, more complex concepts. In the current work, we model how early-acquired concepts are used in the learning of subsequent concepts, and compare the results with standard deep network modeling. We focused in particular on the use of the concepts of animacy and goal attribution in learning to predict future events. We show that the use of early concepts in the learning of new concepts leads to better learning (higher accuracy) and more efficient learning (requiring less data). We further show that this integration of early and new concepts shapes the representation of the concepts acquired by the model. The results show that when the concepts were learned in a human-like manner, the emerging representation was more useful, as measured in terms of generalization to novel data and tasks. On a more general level, the results suggest that there are likely to be basic differences in the conceptual structures acquired by current network models compared to human learning.","sentences":["Early in development, infants learn a range of useful concepts, which can be challenging from a computational standpoint.","This early learning comes together with an initial understanding of aspects of the meaning of concepts, e.g., their implications, causality, and using them to predict likely future events.","All this is accomplished in many cases with little or no supervision, and from relatively few examples, compared with current network models.","In learning about objects and human-object interactions, early acquired and possibly innate concepts are often used in the process of learning additional, more complex concepts.","In the current work, we model how early-acquired concepts are used in the learning of subsequent concepts, and compare the results with standard deep network modeling.","We focused in particular on the use of the concepts of animacy and goal attribution in learning to predict future events.","We show that the use of early concepts in the learning of new concepts leads to better learning (higher accuracy) and more efficient learning (requiring less data).","We further show that this integration of early and new concepts shapes the representation of the concepts acquired by the model.","The results show that when the concepts were learned in a human-like manner, the emerging representation was more useful, as measured in terms of generalization to novel data and tasks.","On a more general level, the results suggest that there are likely to be basic differences in the conceptual structures acquired by current network models compared to human learning."],"url":"http://arxiv.org/abs/2503.03361v1"}
{"created":"2025-03-05 10:40:09","title":"Transformers for molecular property prediction: Domain adaptation efficiently improves performance","abstract":"Most of the current transformer-based chemical language models are pre-trained on millions to billions of molecules. However, the improvement from such scaling in dataset size is not confidently linked to improved molecular property prediction. The aim of this study is to investigate and overcome some of the limitations of transformer models in predicting molecular properties. Specifically, we examine the impact of pre-training dataset size and diversity on the performance of transformer models and investigate the use of domain adaptation as a technique for improving model performance. First, our findings indicate that increasing pretraining dataset size beyond 400K molecules from the GuacaMol dataset does not result in a significant improvement on four ADME endpoints, namely, solubility, permeability, microsomal stability, and plasma protein binding. Second, our results demonstrate that using domain adaptation by further training the transformer model on a small set of domain-relevant molecules, i.e., a few hundred to a few thousand, using multi-task regression of physicochemical properties was sufficient to significantly improve performance for three out of the four investigated ADME endpoints (P-value < 0.001). Finally, we observe that a model pre-trained on 400K molecules and domain adopted on a few hundred/thousand molecules performs similarly (P-value > 0.05) to more complicated transformer models like MolBERT(pre-trained on 1.3M molecules) and MolFormer (pre-trained on 100M molecules). A comparison to a random forest model trained on basic physicochemical properties showed similar performance to the examined transformer models. We believe that current transformer models can be improved through further systematic analysis of pre-training and downstream data, pre-training objectives, and scaling laws, ultimately leading to better and more helpful models.","sentences":["Most of the current transformer-based chemical language models are pre-trained on millions to billions of molecules.","However, the improvement from such scaling in dataset size is not confidently linked to improved molecular property prediction.","The aim of this study is to investigate and overcome some of the limitations of transformer models in predicting molecular properties.","Specifically, we examine the impact of pre-training dataset size and diversity on the performance of transformer models and investigate the use of domain adaptation as a technique for improving model performance.","First, our findings indicate that increasing pretraining dataset size beyond 400K molecules from the GuacaMol dataset does not result in a significant improvement on four ADME endpoints, namely, solubility, permeability, microsomal stability, and plasma protein binding.","Second, our results demonstrate that using domain adaptation by further training the transformer model on a small set of domain-relevant molecules, i.e., a few hundred to a few thousand, using multi-task regression of physicochemical properties was sufficient to significantly improve performance for three out of the four investigated ADME endpoints (P-value < 0.001).","Finally, we observe that a model pre-trained on 400K molecules and domain adopted on a few hundred/thousand molecules performs similarly (P-value > 0.05) to more complicated transformer models like MolBERT(pre-trained on 1.3M molecules) and MolFormer (pre-trained on 100M molecules).","A comparison to a random forest model trained on basic physicochemical properties showed similar performance to the examined transformer models.","We believe that current transformer models can be improved through further systematic analysis of pre-training and downstream data, pre-training objectives, and scaling laws, ultimately leading to better and more helpful models."],"url":"http://arxiv.org/abs/2503.03360v1"}
{"created":"2025-03-05 10:39:23","title":"Iterating Pointers: Enabling Static Analysis for Loop-based Pointers","abstract":"Pointers are an integral part of C and other programming languages. They enable substantial flexibility from the programmer's standpoint, allowing the user fine, unmediated control over data access patterns. However, accesses done through pointers are often hard to track, and challenging to understand for optimizers, compilers, and sometimes, even for the developers themselves because of the direct memory access they provide. We alleviate this problem by exposing additional information to analyzers and compilers. By separating the concept of a pointer into a data container and an offset, we can optimize C programs beyond what other state-of-the-art approaches are capable of, in some cases even enabling auto-parallelization. Using this process, we are able to successfully analyze and optimize code from OpenSSL, the Mantevo benchmark suite, and the Lempel-Ziv-Oberhumer compression algorithm. We provide the only automatic approach able to find all parallelization opportunities in the HPCCG benchmark from the Mantevo suite the developers identified and even outperform the reference implementation by up to 18%, as well as speed up the PBKDF2 algorithm implementation from OpenSSL by up to 11x.","sentences":["Pointers are an integral part of C and other programming languages.","They enable substantial flexibility from the programmer's standpoint, allowing the user fine, unmediated control over data access patterns.","However, accesses done through pointers are often hard to track, and challenging to understand for optimizers, compilers, and sometimes, even for the developers themselves because of the direct memory access they provide.","We alleviate this problem by exposing additional information to analyzers and compilers.","By separating the concept of a pointer into a data container and an offset, we can optimize C programs beyond what other state-of-the-art approaches are capable of, in some cases even enabling auto-parallelization.","Using this process, we are able to successfully analyze and optimize code from OpenSSL, the Mantevo benchmark suite, and the Lempel-Ziv-Oberhumer compression algorithm.","We provide the only automatic approach able to find all parallelization opportunities in the HPCCG benchmark from the Mantevo suite the developers identified and even outperform the reference implementation by up to 18%, as well as speed up the PBKDF2 algorithm implementation from OpenSSL by up to 11x."],"url":"http://arxiv.org/abs/2503.03359v1"}
{"created":"2025-03-05 10:37:51","title":"Video Super-Resolution: All You Need is a Video Diffusion Model","abstract":"We present a generic video super-resolution algorithm in this paper, based on the Diffusion Posterior Sampling framework with an unconditional video generation model in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Due to limited computational resources and training data, our experiments provide empirical evidence of the algorithm's strong super-resolution capabilities using synthetic data.","sentences":["We present a generic video super-resolution algorithm in this paper, based on the Diffusion Posterior Sampling framework with an unconditional video generation model in latent space.","The video generation model, a diffusion transformer, functions as a space-time model.","We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment.","Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training.","Due to limited computational resources and training data, our experiments provide empirical evidence of the algorithm's strong super-resolution capabilities using synthetic data."],"url":"http://arxiv.org/abs/2503.03355v1"}
{"created":"2025-03-05 10:07:48","title":"IoT Integration Protocol for Enhanced Hospital Care","abstract":"This paper introduces the \"IoT Integration Protocol for Enhanced Hospital Care\", a comprehensive framework designed to leverage Internet of Things (IoT) technology to enhance patient care, improve operational efficiency, and ensure data security in hospital settings. With the growing emphasis on utilizing advanced technologies in healthcare, this protocol aims to harness the potential of IoT devices to optimize patient monitoring, enable remote care, and support clinical decision-making. By integrating IoT seamlessly into nursing workflows and patient care plans, hospitals can achieve higher levels of patient-centric care and real-time data insights, leading to better treatment outcomes and resource allocation. This paper outlines the protocol's objectives, key components, and expected benefits, while emphasizing the importance of ethical considerations and ongoing evaluation to ensure successful implementation.","sentences":["This paper introduces the \"IoT Integration Protocol for Enhanced Hospital Care\", a comprehensive framework designed to leverage Internet of Things (IoT) technology to enhance patient care, improve operational efficiency, and ensure data security in hospital settings.","With the growing emphasis on utilizing advanced technologies in healthcare, this protocol aims to harness the potential of IoT devices to optimize patient monitoring, enable remote care, and support clinical decision-making.","By integrating IoT seamlessly into nursing workflows and patient care plans, hospitals can achieve higher levels of patient-centric care and real-time data insights, leading to better treatment outcomes and resource allocation.","This paper outlines the protocol's objectives, key components, and expected benefits, while emphasizing the importance of ethical considerations and ongoing evaluation to ensure successful implementation."],"url":"http://arxiv.org/abs/2503.03334v1"}
{"created":"2025-03-05 10:03:59","title":"Leap: Inductive Link Prediction via Learnable TopologyAugmentation","abstract":"Link prediction is a crucial task in many downstream applications of graph machine learning. To this end, Graph Neural Network (GNN) is a widely used technique for link prediction, mainly in transductive settings, where the goal is to predict missing links between existing nodes. However, many real-life applications require an inductive setting that accommodates for new nodes, coming into an existing graph. Thus, recently inductive link prediction has attracted considerable attention, and a multi-layer perceptron (MLP) is the popular choice of most studies to learn node representations. However, these approaches have limited expressivity and do not fully capture the graph's structural signal. Therefore, in this work we propose LEAP, an inductive link prediction method based on LEArnable toPology augmentation. Unlike previous methods, LEAP models the inductive bias from both the structure and node features, and hence is more expressive. To the best of our knowledge, this is the first attempt to provide structural contexts for new nodes via learnable augmentation in inductive settings. Extensive experiments on seven real-world homogeneous and heterogeneous graphs demonstrates that LEAP significantly surpasses SOTA methods. The improvements are up to 22\\% and 17\\% in terms of AUC and average precision, respectively. The code and datasets are available on GitHub (https://github.com/AhmedESamy/LEAP/)","sentences":["Link prediction is a crucial task in many downstream applications of graph machine learning.","To this end, Graph Neural Network (GNN) is a widely used technique for link prediction, mainly in transductive settings, where the goal is to predict missing links between existing nodes.","However, many real-life applications require an inductive setting that accommodates for new nodes, coming into an existing graph.","Thus, recently inductive link prediction has attracted considerable attention, and a multi-layer perceptron (MLP) is the popular choice of most studies to learn node representations.","However, these approaches have limited expressivity and do not fully capture the graph's structural signal.","Therefore, in this work we propose LEAP, an inductive link prediction method based on LEArnable toPology augmentation.","Unlike previous methods, LEAP models the inductive bias from both the structure and node features, and hence is more expressive.","To the best of our knowledge, this is the first attempt to provide structural contexts for new nodes via learnable augmentation in inductive settings.","Extensive experiments on seven real-world homogeneous and heterogeneous graphs demonstrates that LEAP significantly surpasses SOTA methods.","The improvements are up to 22\\% and 17\\% in terms of AUC and average precision, respectively.","The code and datasets are available on GitHub (https://github.com/AhmedESamy/LEAP/)"],"url":"http://arxiv.org/abs/2503.03331v1"}
{"created":"2025-03-05 10:03:21","title":"Automated Attendee Recognition System for Large-Scale Social Events or Conference Gathering","abstract":"Manual attendance tracking at large-scale events, such as marriage functions or conferences, is often inefficient and prone to human error. To address this challenge, we propose an automated, cloud-based attendance tracking system that uses cameras mounted at the entrance and exit gates. The mounted cameras continuously capture video and send the video data to cloud services to perform real-time face detection and recognition. Unlike existing solutions, our system accurately identifies attendees even when they are not looking directly at the camera, allowing natural movements, such as looking around or talking while walking. To the best of our knowledge, this is the first system to achieve high recognition rates under such dynamic conditions. Our system demonstrates overall 90% accuracy, with each video frame processed in 5 seconds, ensuring real time operation without frame loss. In addition, notifications are sent promptly to security personnel within the same latency. This system achieves 100% accuracy for individuals without facial obstructions and successfully recognizes all attendees appearing within the camera's field of view, providing a robust solution for attendee recognition in large-scale social events.","sentences":["Manual attendance tracking at large-scale events, such as marriage functions or conferences, is often inefficient and prone to human error.","To address this challenge, we propose an automated, cloud-based attendance tracking system that uses cameras mounted at the entrance and exit gates.","The mounted cameras continuously capture video and send the video data to cloud services to perform real-time face detection and recognition.","Unlike existing solutions, our system accurately identifies attendees even when they are not looking directly at the camera, allowing natural movements, such as looking around or talking while walking.","To the best of our knowledge, this is the first system to achieve high recognition rates under such dynamic conditions.","Our system demonstrates overall 90% accuracy, with each video frame processed in 5 seconds, ensuring real time operation without frame loss.","In addition, notifications are sent promptly to security personnel within the same latency.","This system achieves 100% accuracy for individuals without facial obstructions and successfully recognizes all attendees appearing within the camera's field of view, providing a robust solution for attendee recognition in large-scale social events."],"url":"http://arxiv.org/abs/2503.03330v1"}
{"created":"2025-03-05 09:39:51","title":"Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers","abstract":"For event cameras, current sparse geometric solvers for egomotion estimation assume that the rotational displacements are known, such as those provided by an IMU. Thus, they can only recover the translational motion parameters. Recovering full-DoF motion parameters using a sparse geometric solver is a more challenging task, and has not yet been investigated. In this paper, we propose several solvers to estimate both rotational and translational velocities within a unified framework. Our method leverages event manifolds induced by line segments. The problem formulations are based on either an incidence relation for lines or a novel coplanarity relation for normal vectors. We demonstrate the possibility of recovering full-DoF egomotion parameters for both angular and linear velocities without requiring extra sensor measurements or motion priors. To achieve efficient optimization, we exploit the Adam framework with a first-order approximation of rotations for quick initialization. Experiments on both synthetic and real-world data demonstrate the effectiveness of our method. The code is available at https://github.com/jizhaox/relpose-event.","sentences":["For event cameras, current sparse geometric solvers for egomotion estimation assume that the rotational displacements are known, such as those provided by an IMU.","Thus, they can only recover the translational motion parameters.","Recovering full-DoF motion parameters using a sparse geometric solver is a more challenging task, and has not yet been investigated.","In this paper, we propose several solvers to estimate both rotational and translational velocities within a unified framework.","Our method leverages event manifolds induced by line segments.","The problem formulations are based on either an incidence relation for lines or a novel coplanarity relation for normal vectors.","We demonstrate the possibility of recovering full-DoF egomotion parameters for both angular and linear velocities without requiring extra sensor measurements or motion priors.","To achieve efficient optimization, we exploit the Adam framework with a first-order approximation of rotations for quick initialization.","Experiments on both synthetic and real-world data demonstrate the effectiveness of our method.","The code is available at https://github.com/jizhaox/relpose-event."],"url":"http://arxiv.org/abs/2503.03307v1"}
{"created":"2025-03-05 09:36:57","title":"Differential Machine Learning for Time Series Prediction","abstract":"Accurate time series prediction is challenging due to the inherent nonlinearity and sensitivity to initial conditions. We propose a novel approach that enhances neural network predictions through differential learning, which involves training models on both the original time series and its differential series. Specifically, we develop a differential long short-term memory (Diff-LSTM) network that uses a shared LSTM cell to simultaneously process both data streams, effectively capturing intrinsic patterns and temporal dynamics. Evaluated on the Mackey-Glass, Lorenz, and R\\\"ossler chaotic time series, as well as a real-world financial dataset from ACI Worldwide Inc., our results demonstrate that the Diff- LSTM network outperforms prevalent models such as recurrent neural networks, convolutional neural networks, and bidirectional and encoder-decoder LSTM networks in both short-term and long-term predictions. This framework offers a promising solution for enhancing time series prediction, even when comprehensive knowledge of the underlying dynamics of the time series is not fully available.","sentences":["Accurate time series prediction is challenging due to the inherent nonlinearity and sensitivity to initial conditions.","We propose a novel approach that enhances neural network predictions through differential learning, which involves training models on both the original time series and its differential series.","Specifically, we develop a differential long short-term memory (Diff-LSTM) network that uses a shared LSTM cell to simultaneously process both data streams, effectively capturing intrinsic patterns and temporal dynamics.","Evaluated on the Mackey-Glass, Lorenz, and R\\\"ossler chaotic time series, as well as a real-world financial dataset from ACI Worldwide Inc., our results demonstrate that the Diff- LSTM network outperforms prevalent models such as recurrent neural networks, convolutional neural networks, and bidirectional and encoder-decoder LSTM networks in both short-term and long-term predictions.","This framework offers a promising solution for enhancing time series prediction, even when comprehensive knowledge of the underlying dynamics of the time series is not fully available."],"url":"http://arxiv.org/abs/2503.03302v1"}
{"created":"2025-03-05 09:30:49","title":"Label-Efficient LiDAR Semantic Segmentation with 2D-3D Vision Transformer Adapters","abstract":"LiDAR semantic segmentation models are typically trained from random initialization as universal pre-training is hindered by the lack of large, diverse datasets. Moreover, most point cloud segmentation architectures incorporate custom network layers, limiting the transferability of advances from vision-based architectures. Inspired by recent advances in universal foundation models, we propose BALViT, a novel approach that leverages frozen vision models as amodal feature encoders for learning strong LiDAR encoders. Specifically, BALViT incorporates both range-view and bird's-eye-view LiDAR encoding mechanisms, which we combine through a novel 2D-3D adapter. While the range-view features are processed through a frozen image backbone, our bird's-eye-view branch enhances them through multiple cross-attention interactions. Thereby, we continuously improve the vision network with domain-dependent knowledge, resulting in a strong label-efficient LiDAR encoding mechanism. Extensive evaluations of BALViT on the SemanticKITTI and nuScenes benchmarks demonstrate that it outperforms state-of-the-art methods on small data regimes. We make the code and models publicly available at: http://balvit.cs.uni-freiburg.de.","sentences":["LiDAR semantic segmentation models are typically trained from random initialization as universal pre-training is hindered by the lack of large, diverse datasets.","Moreover, most point cloud segmentation architectures incorporate custom network layers, limiting the transferability of advances from vision-based architectures.","Inspired by recent advances in universal foundation models, we propose BALViT, a novel approach that leverages frozen vision models as amodal feature encoders for learning strong LiDAR encoders.","Specifically, BALViT incorporates both range-view and bird's-eye-view LiDAR encoding mechanisms, which we combine through a novel 2D-3D adapter.","While the range-view features are processed through a frozen image backbone, our bird's-eye-view branch enhances them through multiple cross-attention interactions.","Thereby, we continuously improve the vision network with domain-dependent knowledge, resulting in a strong label-efficient LiDAR encoding mechanism.","Extensive evaluations of BALViT on the SemanticKITTI and nuScenes benchmarks demonstrate that it outperforms state-of-the-art methods on small data regimes.","We make the code and models publicly available at: http://balvit.cs.uni-freiburg.de."],"url":"http://arxiv.org/abs/2503.03299v1"}
{"created":"2025-03-05 09:13:40","title":"Deep Understanding of Sign Language for Sign to Subtitle Alignment","abstract":"The objective of this work is to align asynchronous subtitles in sign language videos with limited labelled data. To achieve this goal, we propose a novel framework with the following contributions: (1) we leverage fundamental grammatical rules of British Sign Language (BSL) to pre-process the input subtitles, (2) we design a selective alignment loss to optimise the model for predicting the temporal location of signs only when the queried sign actually occurs in a scene, and (3) we conduct self-training with refined pseudo-labels which are more accurate than the heuristic audio-aligned labels. From this, our model not only better understands the correlation between the text and the signs, but also holds potential for application in the translation of sign languages, particularly in scenarios where manual labelling of large-scale sign data is impractical or challenging. Extensive experimental results demonstrate that our approach achieves state-of-the-art results, surpassing previous baselines by substantial margins in terms of both frame-level accuracy and F1-score. This highlights the effectiveness and practicality of our framework in advancing the field of sign language video alignment and translation.","sentences":["The objective of this work is to align asynchronous subtitles in sign language videos with limited labelled data.","To achieve this goal, we propose a novel framework with the following contributions: (1) we leverage fundamental grammatical rules of British Sign Language (BSL) to pre-process the input subtitles, (2) we design a selective alignment loss to optimise the model for predicting the temporal location of signs only when the queried sign actually occurs in a scene, and (3) we conduct self-training with refined pseudo-labels which are more accurate than the heuristic audio-aligned labels.","From this, our model not only better understands the correlation between the text and the signs, but also holds potential for application in the translation of sign languages, particularly in scenarios where manual labelling of large-scale sign data is impractical or challenging.","Extensive experimental results demonstrate that our approach achieves state-of-the-art results, surpassing previous baselines by substantial margins in terms of both frame-level accuracy and F1-score.","This highlights the effectiveness and practicality of our framework in advancing the field of sign language video alignment and translation."],"url":"http://arxiv.org/abs/2503.03287v1"}
{"created":"2025-03-05 09:07:13","title":"Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments","abstract":"Unmanned Surface Vehicles (USVs) are increasingly applied to water operations such as environmental monitoring and river-map modeling. It faces a significant challenge in achieving precise autonomous docking at ports or stations, still relying on remote human control or external positioning systems for accuracy and safety which limits the full potential of human-out-of-loop deployment for USVs.This paper introduces a novel supervised learning pipeline with the auto-labeling technique for USVs autonomous visual docking. Firstly, we designed an auto-labeling data collection pipeline that appends relative pose and image pair to the dataset. This step does not require conventional manual labeling for supervised learning. Secondly, the Neural Dock Pose Estimator (NDPE) is proposed to achieve relative dock pose prediction without the need for hand-crafted feature engineering, camera calibration, and peripheral markers. Moreover, The NDPE can accurately predict the relative dock pose in real-world water environments, facilitating the implementation of Position-Based Visual Servo (PBVS) and low-level motion controllers for efficient and autonomous docking.Experiments show that the NDPE is robust to the disturbance of the distance and the USV velocity. The effectiveness of our proposed solution is tested and validated in real-world water environments, reflecting its capability to handle real-world autonomous docking tasks.","sentences":["Unmanned Surface Vehicles (USVs) are increasingly applied to water operations such as environmental monitoring and river-map modeling.","It faces a significant challenge in achieving precise autonomous docking at ports or stations, still relying on remote human control or external positioning systems for accuracy and safety which limits the full potential of human-out-of-loop deployment for USVs.","This paper introduces a novel supervised learning pipeline with the auto-labeling technique for USVs autonomous visual docking.","Firstly, we designed an auto-labeling data collection pipeline that appends relative pose and image pair to the dataset.","This step does not require conventional manual labeling for supervised learning.","Secondly, the Neural Dock Pose Estimator (NDPE) is proposed to achieve relative dock pose prediction without the need for hand-crafted feature engineering, camera calibration, and peripheral markers.","Moreover, The NDPE can accurately predict the relative dock pose in real-world water environments, facilitating the implementation of Position-Based Visual Servo (PBVS) and low-level motion controllers for efficient and autonomous docking.","Experiments show that the NDPE is robust to the disturbance of the distance and the USV velocity.","The effectiveness of our proposed solution is tested and validated in real-world water environments, reflecting its capability to handle real-world autonomous docking tasks."],"url":"http://arxiv.org/abs/2503.03282v1"}
{"created":"2025-03-05 09:02:33","title":"Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions","abstract":"Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.","sentences":["Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks.","However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored.","A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features.","In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge.","Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns.","This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.","We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models.","Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities."],"url":"http://arxiv.org/abs/2503.03278v1"}
{"created":"2025-03-05 08:59:06","title":"TrafficKAN-GCN: Graph Convolutional-based Kolmogorov-Arnold Network for Traffic Flow Optimization","abstract":"Urban traffic optimization is critical for improving transportation efficiency and alleviating congestion, particularly in large-scale dynamic networks. Traditional methods, such as Dijkstra's and Floyd's algorithms, provide effective solutions in static settings, but they struggle with the spatial-temporal complexity of real-world traffic flows. In this work, we propose TrafficKAN-GCN, a hybrid deep learning framework combining Kolmogorov-Arnold Networks (KAN) with Graph Convolutional Networks (GCN), designed to enhance urban traffic flow optimization. By integrating KAN's adaptive nonlinear function approximation with GCN's spatial graph learning capabilities, TrafficKAN-GCN captures both complex traffic patterns and topological dependencies. We evaluate the proposed framework using real-world traffic data from the Baltimore Metropolitan area. Compared with baseline models such as MLP-GCN, standard GCN, and Transformer-based approaches, TrafficKAN-GCN achieves competitive prediction accuracy while demonstrating improved robustness in handling noisy and irregular traffic data. Our experiments further highlight the framework's ability to redistribute traffic flow, mitigate congestion, and adapt to disruptive events, such as the Francis Scott Key Bridge collapse. This study contributes to the growing body of work on hybrid graph learning for intelligent transportation systems, highlighting the potential of combining KAN and GCN for real-time traffic optimization. Future work will focus on reducing computational overhead and integrating Transformer-based temporal modeling for enhanced long-term traffic prediction. The proposed TrafficKAN-GCN framework offers a promising direction for data-driven urban mobility management, balancing predictive accuracy, robustness, and computational efficiency.","sentences":["Urban traffic optimization is critical for improving transportation efficiency and alleviating congestion, particularly in large-scale dynamic networks.","Traditional methods, such as Dijkstra's and Floyd's algorithms, provide effective solutions in static settings, but they struggle with the spatial-temporal complexity of real-world traffic flows.","In this work, we propose TrafficKAN-GCN, a hybrid deep learning framework combining Kolmogorov-Arnold Networks (KAN) with Graph Convolutional Networks (GCN), designed to enhance urban traffic flow optimization.","By integrating KAN's adaptive nonlinear function approximation with GCN's spatial graph learning capabilities, TrafficKAN-GCN captures both complex traffic patterns and topological dependencies.","We evaluate the proposed framework using real-world traffic data from the Baltimore Metropolitan area.","Compared with baseline models such as MLP-GCN, standard GCN, and Transformer-based approaches, TrafficKAN-GCN achieves competitive prediction accuracy while demonstrating improved robustness in handling noisy and irregular traffic data.","Our experiments further highlight the framework's ability to redistribute traffic flow, mitigate congestion, and adapt to disruptive events, such as the Francis Scott Key Bridge collapse.","This study contributes to the growing body of work on hybrid graph learning for intelligent transportation systems, highlighting the potential of combining KAN and GCN for real-time traffic optimization.","Future work will focus on reducing computational overhead and integrating Transformer-based temporal modeling for enhanced long-term traffic prediction.","The proposed TrafficKAN-GCN framework offers a promising direction for data-driven urban mobility management, balancing predictive accuracy, robustness, and computational efficiency."],"url":"http://arxiv.org/abs/2503.03276v1"}
{"created":"2025-03-05 08:56:26","title":"Benchmarking Dynamic SLO Compliance in Distributed Computing Continuum Systems","abstract":"Ensuring Service Level Objectives (SLOs) in large-scale architectures, such as Distributed Computing Continuum Systems (DCCS), is challenging due to their heterogeneous nature and varying service requirements across different devices and applications. Additionally, unpredictable workloads and resource limitations lead to fluctuating performance and violated SLOs. To improve SLO compliance in DCCS, one possibility is to apply machine learning; however, the design choices are often left to the developer. To that extent, we provide a benchmark of Active Inference -- an emerging method from neuroscience -- against three established reinforcement learning algorithms (Deep Q-Network, Advantage Actor-Critic, and Proximal Policy Optimization). We consider a realistic DCCS use case: an edge device running a video conferencing application alongside a WebSocket server streaming videos. Using one of the respective algorithms, we continuously monitor key performance metrics, such as latency and bandwidth usage, to dynamically adjust parameters -- including the number of streams, frame rate, and resolution -- to optimize service quality and user experience. To test algorithms' adaptability to constant system changes, we simulate dynamically changing SLOs and both instant and gradual data-shift scenarios, such as network bandwidth limitations and fluctuating device thermal states. Although the evaluated algorithms all showed advantages and limitations, our findings demonstrate that Active Inference is a promising approach for ensuring SLO compliance in DCCS, offering lower memory usage, stable CPU utilization, and fast convergence.","sentences":["Ensuring Service Level Objectives (SLOs) in large-scale architectures, such as Distributed Computing Continuum Systems (DCCS), is challenging due to their heterogeneous nature and varying service requirements across different devices and applications.","Additionally, unpredictable workloads and resource limitations lead to fluctuating performance and violated SLOs.","To improve SLO compliance in DCCS, one possibility is to apply machine learning; however, the design choices are often left to the developer.","To that extent, we provide a benchmark of Active Inference -- an emerging method from neuroscience -- against three established reinforcement learning algorithms (Deep Q-Network, Advantage Actor-Critic, and Proximal Policy Optimization).","We consider a realistic DCCS use case: an edge device running a video conferencing application alongside a WebSocket server streaming videos.","Using one of the respective algorithms, we continuously monitor key performance metrics, such as latency and bandwidth usage, to dynamically adjust parameters -- including the number of streams, frame rate, and resolution -- to optimize service quality and user experience.","To test algorithms' adaptability to constant system changes, we simulate dynamically changing SLOs and both instant and gradual data-shift scenarios, such as network bandwidth limitations and fluctuating device thermal states.","Although the evaluated algorithms all showed advantages and limitations, our findings demonstrate that Active Inference is a promising approach for ensuring SLO compliance in DCCS, offering lower memory usage, stable CPU utilization, and fast convergence."],"url":"http://arxiv.org/abs/2503.03274v1"}
{"created":"2025-03-05 08:52:55","title":"Towards Effective and Sparse Adversarial Attack on Spiking Neural Networks via Breaking Invisible Surrogate Gradients","abstract":"Spiking neural networks (SNNs) have shown their competence in handling spatial-temporal event-based data with low energy consumption. Similar to conventional artificial neural networks (ANNs), SNNs are also vulnerable to gradient-based adversarial attacks, wherein gradients are calculated by spatial-temporal back-propagation (STBP) and surrogate gradients (SGs). However, the SGs may be invisible for an inference-only model as they do not influence the inference results, and current gradient-based attacks are ineffective for binary dynamic images captured by the dynamic vision sensor (DVS). While some approaches addressed the issue of invisible SGs through universal SGs, their SGs lack a correlation with the victim model, resulting in sub-optimal performance. Moreover, the imperceptibility of existing SNN-based binary attacks is still insufficient. In this paper, we introduce an innovative potential-dependent surrogate gradient (PDSG) method to establish a robust connection between the SG and the model, thereby enhancing the adaptability of adversarial attacks across various models with invisible SGs. Additionally, we propose the sparse dynamic attack (SDA) to effectively attack binary dynamic images. Utilizing a generation-reduction paradigm, SDA can fully optimize the sparsity of adversarial perturbations. Experimental results demonstrate that our PDSG and SDA outperform state-of-the-art SNN-based attacks across various models and datasets. Specifically, our PDSG achieves 100% attack success rate on ImageNet, and our SDA obtains 82% attack success rate by modifying only 0.24% of the pixels on CIFAR10DVS. The code is available at https://github.com/ryime/PDSG-SDA .","sentences":["Spiking neural networks (SNNs) have shown their competence in handling spatial-temporal event-based data with low energy consumption.","Similar to conventional artificial neural networks (ANNs), SNNs are also vulnerable to gradient-based adversarial attacks, wherein gradients are calculated by spatial-temporal back-propagation (STBP) and surrogate gradients (SGs).","However, the SGs may be invisible for an inference-only model as they do not influence the inference results, and current gradient-based attacks are ineffective for binary dynamic images captured by the dynamic vision sensor (DVS).","While some approaches addressed the issue of invisible SGs through universal SGs, their SGs lack a correlation with the victim model, resulting in sub-optimal performance.","Moreover, the imperceptibility of existing SNN-based binary attacks is still insufficient.","In this paper, we introduce an innovative potential-dependent surrogate gradient (PDSG) method to establish a robust connection between the SG and the model, thereby enhancing the adaptability of adversarial attacks across various models with invisible SGs.","Additionally, we propose the sparse dynamic attack (SDA) to effectively attack binary dynamic images.","Utilizing a generation-reduction paradigm, SDA can fully optimize the sparsity of adversarial perturbations.","Experimental results demonstrate that our PDSG and SDA outperform state-of-the-art SNN-based attacks across various models and datasets.","Specifically, our PDSG achieves 100% attack success rate on ImageNet, and our SDA obtains 82% attack success rate by modifying only 0.24% of the pixels on CIFAR10DVS.","The code is available at https://github.com/ryime/PDSG-SDA ."],"url":"http://arxiv.org/abs/2503.03272v1"}
{"created":"2025-03-05 08:50:53","title":"Conformal Transformations for Symmetric Power Transformers","abstract":"Transformers with linear attention offer significant computational advantages over softmax-based transformers but often suffer from degraded performance. The symmetric power (sympow) transformer, a particular type of linear transformer, addresses some of this performance gap by leveraging symmetric tensor embeddings, achieving comparable performance to softmax transformers. However, the finite capacity of the recurrent state in sympow transformers limits their ability to retain information, leading to performance degradation when scaling the training or evaluation context length. To address this issue, we propose the conformal-sympow transformer, which dynamically frees up capacity using data-dependent multiplicative gating and adaptively stores information using data-dependent rotary embeddings. Preliminary experiments on the LongCrawl64 dataset demonstrate that conformal-sympow overcomes the limitations of sympow transformers, achieving robust performance across scaled training and evaluation contexts.","sentences":["Transformers with linear attention offer significant computational advantages over softmax-based transformers but often suffer from degraded performance.","The symmetric power (sympow) transformer, a particular type of linear transformer, addresses some of this performance gap by leveraging symmetric tensor embeddings, achieving comparable performance to softmax transformers.","However, the finite capacity of the recurrent state in sympow transformers limits their ability to retain information, leading to performance degradation when scaling the training or evaluation context length.","To address this issue, we propose the conformal-sympow transformer, which dynamically frees up capacity using data-dependent multiplicative gating and adaptively stores information using data-dependent rotary embeddings.","Preliminary experiments on the LongCrawl64 dataset demonstrate that conformal-sympow overcomes the limitations of sympow transformers, achieving robust performance across scaled training and evaluation contexts."],"url":"http://arxiv.org/abs/2503.03269v1"}
{"created":"2025-03-05 08:49:31","title":"Quantum-Inspired Privacy-Preserving Federated Learning Framework for Secure Dementia Classification","abstract":"Dementia, a neurological disorder impacting millions globally, presents significant challenges in diagnosis and patient care. With the rise of privacy concerns and security threats in healthcare, federated learning (FL) has emerged as a promising approach to enable collaborative model training across decentralized datasets without exposing sensitive patient information. However, FL remains vulnerable to advanced security breaches such as gradient inversion and eavesdropping attacks. This paper introduces a novel framework that integrates federated learning with quantum-inspired encryption techniques for dementia classification, emphasizing privacy preservation and security. Leveraging quantum key distribution (QKD), the framework ensures secure transmission of model weights, protecting against unauthorized access and interception during training. The methodology utilizes a convolutional neural network (CNN) for dementia classification, with federated training conducted across distributed healthcare nodes, incorporating QKD-encrypted weight sharing to secure the aggregation process. Experimental evaluations conducted on MRI data from the OASIS dataset demonstrate that the proposed framework achieves identical accuracy levels to a baseline model while enhancing data security and reducing loss by almost 1% compared to the classical baseline model. The framework offers significant implications for democratizing access to AI-driven dementia diagnostics in low- and middle-income countries, addressing critical resource and privacy constraints. This work contributes a robust, scalable, and secure federated learning solution for healthcare applications, paving the way for broader adoption of quantum-inspired techniques in AI-driven medical research.","sentences":["Dementia, a neurological disorder impacting millions globally, presents significant challenges in diagnosis and patient care.","With the rise of privacy concerns and security threats in healthcare, federated learning (FL) has emerged as a promising approach to enable collaborative model training across decentralized datasets without exposing sensitive patient information.","However, FL remains vulnerable to advanced security breaches such as gradient inversion and eavesdropping attacks.","This paper introduces a novel framework that integrates federated learning with quantum-inspired encryption techniques for dementia classification, emphasizing privacy preservation and security.","Leveraging quantum key distribution (QKD), the framework ensures secure transmission of model weights, protecting against unauthorized access and interception during training.","The methodology utilizes a convolutional neural network (CNN) for dementia classification, with federated training conducted across distributed healthcare nodes, incorporating QKD-encrypted weight sharing to secure the aggregation process.","Experimental evaluations conducted on MRI data from the OASIS dataset demonstrate that the proposed framework achieves identical accuracy levels to a baseline model while enhancing data security and reducing loss by almost 1% compared to the classical baseline model.","The framework offers significant implications for democratizing access to AI-driven dementia diagnostics in low- and middle-income countries, addressing critical resource and privacy constraints.","This work contributes a robust, scalable, and secure federated learning solution for healthcare applications, paving the way for broader adoption of quantum-inspired techniques in AI-driven medical research."],"url":"http://arxiv.org/abs/2503.03267v1"}
{"created":"2025-03-05 08:47:36","title":"Optimizing for the Shortest Path in Denoising Diffusion Model","abstract":"In this research, we propose a novel denoising diffusion model based on shortest-path modeling that optimizes residual propagation to enhance both denoising efficiency and quality.Drawing on Denoising Diffusion Implicit Models (DDIM) and insights from graph theory, our model, termed the Shortest Path Diffusion Model (ShortDF), treats the denoising process as a shortest-path problem aimed at minimizing reconstruction error. By optimizing the initial residuals, we improve the efficiency of the reverse diffusion process and the quality of the generated samples.Extensive experiments on multiple standard benchmarks demonstrate that ShortDF significantly reduces diffusion time (or steps) while enhancing the visual fidelity of generated samples compared to prior arts.This work, we suppose, paves the way for interactive diffusion-based applications and establishes a foundation for rapid data generation. Code is available at https://github.com/UnicomAI/ShortDF.","sentences":["In this research, we propose a novel denoising diffusion model based on shortest-path modeling that optimizes residual propagation to enhance both denoising efficiency and quality.","Drawing on Denoising Diffusion Implicit Models (DDIM) and insights from graph theory, our model, termed the Shortest Path Diffusion Model (ShortDF), treats the denoising process as a shortest-path problem aimed at minimizing reconstruction error.","By optimizing the initial residuals, we improve the efficiency of the reverse diffusion process and the quality of the generated samples.","Extensive experiments on multiple standard benchmarks demonstrate that ShortDF significantly reduces diffusion time (or steps) while enhancing the visual fidelity of generated samples compared to prior arts.","This work, we suppose, paves the way for interactive diffusion-based applications and establishes a foundation for rapid data generation.","Code is available at https://github.com/UnicomAI/ShortDF."],"url":"http://arxiv.org/abs/2503.03265v1"}
{"created":"2025-03-05 08:37:10","title":"Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions","abstract":"Large language models (LLMs) can perform various natural language processing (NLP) tasks through in-context learning without relying on supervised data. However, multiple previous studies have reported suboptimal performance of LLMs in biological text mining. By analyzing failure patterns in these evaluations, we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs fail to learn implicit dataset-specific nuances from supervised data, (2) The common formatting requirements of discriminative tasks limit the reasoning capabilities of LLMs particularly for LLMs that lack test-time compute, and (3) LLMs struggle to adhere to annotation guidelines and match exact schemas, which hinders their ability to understand detailed annotation requirements which is essential in biomedical annotation workflow. To address these challenges, we experimented with prompt engineering techniques targeted to the above issues, and developed a pipeline that dynamically extracts instructions from annotation guidelines. Our findings show that frontier LLMs can approach or surpass the performance of state-of-the-art (SOTA) BERT-based models with minimal reliance on manually annotated data and without fine-tuning. Furthermore, we performed model distillation on a closed-source LLM, demonstrating that a BERT model trained exclusively on synthetic data annotated by LLMs can also achieve a practical performance. Based on these results, we explored the feasibility of partially replacing manual annotation with LLMs in production scenarios for biomedical text mining.","sentences":["Large language models (LLMs) can perform various natural language processing (NLP) tasks through in-context learning without relying on supervised data.","However, multiple previous studies have reported suboptimal performance of LLMs in biological text mining.","By analyzing failure patterns in these evaluations, we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs fail to learn implicit dataset-specific nuances from supervised data, (2) The common formatting requirements of discriminative tasks limit the reasoning capabilities of LLMs particularly for LLMs that lack test-time compute, and (3) LLMs struggle to adhere to annotation guidelines and match exact schemas, which hinders their ability to understand detailed annotation requirements which is essential in biomedical annotation workflow.","To address these challenges, we experimented with prompt engineering techniques targeted to the above issues, and developed a pipeline that dynamically extracts instructions from annotation guidelines.","Our findings show that frontier LLMs can approach or surpass the performance of state-of-the-art (SOTA) BERT-based models with minimal reliance on manually annotated data and without fine-tuning.","Furthermore, we performed model distillation on a closed-source LLM, demonstrating that a BERT model trained exclusively on synthetic data annotated by LLMs can also achieve a practical performance.","Based on these results, we explored the feasibility of partially replacing manual annotation with LLMs in production scenarios for biomedical text mining."],"url":"http://arxiv.org/abs/2503.03261v1"}
{"created":"2025-03-05 08:28:11","title":"Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs","abstract":"With the rise of large language models (LLMs), there has been growing interest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging LLMs as predictors, GFMs have demonstrated impressive generalizability across various tasks and datasets. However, existing research on LLMs as predictors has predominantly focused on static graphs, leaving their potential in dynamic graph prediction unexplored. In this work, we pioneer using LLMs for predictive tasks on dynamic graphs. We identify two key challenges: the constraints imposed by context length when processing large-scale historical data and the significant variability in domain characteristics, both of which complicate the development of a unified predictor. To address these challenges, we propose the GraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages collaborative LLMs. In contrast to using a single LLM as the predictor, GAD incorporates global and local summary agents to generate domain-specific knowledge, enhancing its transferability across domains. Additionally, knowledge reflection agents enable adaptive updates to GAD's knowledge, maintaining a unified and self-consistent architecture. In experiments, GAD demonstrates performance comparable to or even exceeds that of full-supervised graph neural networks without dataset-specific training. Finally, to enhance the task-specific performance of LLM-based predictors, we discuss potential improvements, such as dataset-specific fine-tuning to LLMs. By developing tailored strategies for different tasks, we provide new insights for the future design of LLM-based predictors.","sentences":["With the rise of large language models (LLMs), there has been growing interest in Graph Foundation Models (GFMs) for graph-based tasks.","By leveraging LLMs as predictors, GFMs have demonstrated impressive generalizability across various tasks and datasets.","However, existing research on LLMs as predictors has predominantly focused on static graphs, leaving their potential in dynamic graph prediction unexplored.","In this work, we pioneer using LLMs for predictive tasks on dynamic graphs.","We identify two key challenges: the constraints imposed by context length when processing large-scale historical data and the significant variability in domain characteristics, both of which complicate the development of a unified predictor.","To address these challenges, we propose the GraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages collaborative LLMs.","In contrast to using a single LLM as the predictor, GAD incorporates global and local summary agents to generate domain-specific knowledge, enhancing its transferability across domains.","Additionally, knowledge reflection agents enable adaptive updates to GAD's knowledge, maintaining a unified and self-consistent architecture.","In experiments, GAD demonstrates performance comparable to or even exceeds that of full-supervised graph neural networks without dataset-specific training.","Finally, to enhance the task-specific performance of LLM-based predictors, we discuss potential improvements, such as dataset-specific fine-tuning to LLMs.","By developing tailored strategies for different tasks, we provide new insights for the future design of LLM-based predictors."],"url":"http://arxiv.org/abs/2503.03258v1"}
{"created":"2025-03-05 08:20:16","title":"BAT: Learning Event-based Optical Flow with Bidirectional Adaptive Temporal Correlation","abstract":"Event cameras deliver visual information characterized by a high dynamic range and high temporal resolution, offering significant advantages in estimating optical flow for complex lighting conditions and fast-moving objects. Current advanced optical flow methods for event cameras largely adopt established image-based frameworks. However, the spatial sparsity of event data limits their performance. In this paper, we present BAT, an innovative framework that estimates event-based optical flow using bidirectional adaptive temporal correlation. BAT includes three novel designs: 1) a bidirectional temporal correlation that transforms bidirectional temporally dense motion cues into spatially dense ones, enabling accurate and spatially dense optical flow estimation; 2) an adaptive temporal sampling strategy for maintaining temporal consistency in correlation; 3) spatially adaptive temporal motion aggregation to efficiently and adaptively aggregate consistent target motion features into adjacent motion features while suppressing inconsistent ones. Our results rank $1^{st}$ on the DSEC-Flow benchmark, outperforming existing state-of-the-art methods by a large margin while also exhibiting sharp edges and high-quality details. Notably, our BAT can accurately predict future optical flow using only past events, significantly outperforming E-RAFT's warm-start approach. Code: \\textcolor{magenta}{https://github.com/gangweiX/BAT}.","sentences":["Event cameras deliver visual information characterized by a high dynamic range and high temporal resolution, offering significant advantages in estimating optical flow for complex lighting conditions and fast-moving objects.","Current advanced optical flow methods for event cameras largely adopt established image-based frameworks.","However, the spatial sparsity of event data limits their performance.","In this paper, we present BAT, an innovative framework that estimates event-based optical flow using bidirectional adaptive temporal correlation.","BAT includes three novel designs: 1) a bidirectional temporal correlation that transforms bidirectional temporally dense motion cues into spatially dense ones, enabling accurate and spatially dense optical flow estimation; 2) an adaptive temporal sampling strategy for maintaining temporal consistency in correlation; 3) spatially adaptive temporal motion aggregation to efficiently and adaptively aggregate consistent target motion features into adjacent motion features while suppressing inconsistent ones.","Our results rank $1^{st}$ on the DSEC-Flow benchmark, outperforming existing state-of-the-art methods by a large margin while also exhibiting sharp edges and high-quality details.","Notably, our BAT can accurately predict future optical flow using only past events, significantly outperforming E-RAFT's warm-start approach.","Code: \\textcolor{magenta}{https://github.com/gangweiX/BAT}."],"url":"http://arxiv.org/abs/2503.03256v1"}
{"created":"2025-03-05 08:08:32","title":"From Coverage to Prestige: A Comprehensive Assessment of Large-Scale Scientometric Data","abstract":"As research in the Scientometric deepens, the impact of data quality on research outcomes has garnered increasing attention. This study, based on Web of Science (WoS) and Crossref datasets, systematically evaluates the differences between data sources and the effects of data merging through matching, comparison, and integration. Two core metrics were employed: Reference Coverage Rate (RCR) and Article Scientific Prestige (ASP), which respectively measure citation completeness (quantity) and academic influence (quality). The results indicate that the WoS dataset outperforms Crossref in its coverage of high-impact literature and ASP scores, while the Crossref dataset provides complementary value through its broader coverage of literature. Data merging significantly improves the completeness of the citation network, with particularly pronounced benefits in smaller disciplinary clusters such as Education and Arts. However, data merging also introduces some low-quality citations, resulting in a polarization of overall data quality. Moreover, the impact of data merging varies across disciplines; high-impact clusters such as Science, Biology, and Medicine benefit the most, whereas clusters like Social Sciences and Arts are more vulnerable to negative effects. This study highlights the critical role of data sources in Scientometric research and provides a framework for assessing and improving data quality.","sentences":["As research in the Scientometric deepens, the impact of data quality on research outcomes has garnered increasing attention.","This study, based on Web of Science (WoS) and Crossref datasets, systematically evaluates the differences between data sources and the effects of data merging through matching, comparison, and integration.","Two core metrics were employed: Reference Coverage Rate (RCR) and Article Scientific Prestige (ASP), which respectively measure citation completeness (quantity) and academic influence (quality).","The results indicate that the WoS dataset outperforms Crossref in its coverage of high-impact literature and ASP scores, while the Crossref dataset provides complementary value through its broader coverage of literature.","Data merging significantly improves the completeness of the citation network, with particularly pronounced benefits in smaller disciplinary clusters such as Education and Arts.","However, data merging also introduces some low-quality citations, resulting in a polarization of overall data quality.","Moreover, the impact of data merging varies across disciplines; high-impact clusters such as Science, Biology, and Medicine benefit the most, whereas clusters like Social Sciences and Arts are more vulnerable to negative effects.","This study highlights the critical role of data sources in Scientometric research and provides a framework for assessing and improving data quality."],"url":"http://arxiv.org/abs/2503.03251v1"}
{"created":"2025-03-05 07:52:52","title":"Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection in Neonatal Care","abstract":"Around 10% of newborns require some help to initiate breathing, and 5\\% need ventilation assistance. Accurate Time of Birth (ToB) documentation is essential for optimizing neonatal care, as timely interventions are vital for proper resuscitation. However, current clinical methods for recording ToB often rely on manual processes, which can be prone to inaccuracies. In this study, we present a novel two-stream fusion system that combines the power of image and video analysis to accurately detect the ToB from thermal recordings in the delivery room and operating theater. By integrating static and dynamic streams, our approach captures richer birth-related spatiotemporal features, leading to more robust and precise ToB estimation. We demonstrate that this synergy between data modalities enhances performance over single-stream approaches. Our system achieves 95.7% precision and 84.8% recall in detecting birth within short video clips. Additionally, with the help of a score aggregation module, it successfully identifies ToB in 100% of test cases, with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.","sentences":["Around 10% of newborns require some help to initiate breathing, and 5\\% need ventilation assistance.","Accurate Time of Birth (ToB) documentation is essential for optimizing neonatal care, as timely interventions are vital for proper resuscitation.","However, current clinical methods for recording ToB often rely on manual processes, which can be prone to inaccuracies.","In this study, we present a novel two-stream fusion system that combines the power of image and video analysis to accurately detect the ToB from thermal recordings in the delivery room and operating theater.","By integrating static and dynamic streams, our approach captures richer birth-related spatiotemporal features, leading to more robust and precise ToB estimation.","We demonstrate that this synergy between data modalities enhances performance over single-stream approaches.","Our system achieves 95.7% precision and 84.8% recall in detecting birth within short video clips.","Additionally, with the help of a score aggregation module, it successfully identifies ToB in 100% of test cases, with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations."],"url":"http://arxiv.org/abs/2503.03244v1"}
{"created":"2025-03-05 07:47:57","title":"Structural Entropy Guided Unsupervised Graph Out-Of-Distribution Detection","abstract":"With the emerging of huge amount of unlabeled data, unsupervised out-of-distribution (OOD) detection is vital for ensuring the reliability of graph neural networks (GNNs) by identifying OOD samples from in-distribution (ID) ones during testing, where encountering novel or unknown data is inevitable. Existing methods often suffer from compromised performance due to redundant information in graph structures, which impairs their ability to effectively differentiate between ID and OOD data. To address this challenge, we propose SEGO, an unsupervised framework that integrates structural entropy into OOD detection regarding graph classification. Specifically, within the architecture of contrastive learning, SEGO introduces an anchor view in the form of coding tree by minimizing structural entropy. The obtained coding tree effectively removes redundant information from graphs while preserving essential structural information, enabling the capture of distinct graph patterns between ID and OOD samples. Furthermore, we present a multi-grained contrastive learning scheme at local, global, and tree levels using triplet views, where coding trees with essential information serve as the anchor view. Extensive experiments on real-world datasets validate the effectiveness of SEGO, demonstrating superior performance over state-of-the-art baselines in OOD detection. Specifically, our method achieves the best performance on 9 out of 10 dataset pairs, with an average improvement of 3.7\\% on OOD detection datasets, significantly surpassing the best competitor by 10.8\\% on the FreeSolv/ToxCast dataset pair.","sentences":["With the emerging of huge amount of unlabeled data, unsupervised out-of-distribution (OOD) detection is vital for ensuring the reliability of graph neural networks (GNNs) by identifying OOD samples from in-distribution (ID) ones during testing, where encountering novel or unknown data is inevitable.","Existing methods often suffer from compromised performance due to redundant information in graph structures, which impairs their ability to effectively differentiate between ID and OOD data.","To address this challenge, we propose SEGO, an unsupervised framework that integrates structural entropy into OOD detection regarding graph classification.","Specifically, within the architecture of contrastive learning, SEGO introduces an anchor view in the form of coding tree by minimizing structural entropy.","The obtained coding tree effectively removes redundant information from graphs while preserving essential structural information, enabling the capture of distinct graph patterns between ID and OOD samples.","Furthermore, we present a multi-grained contrastive learning scheme at local, global, and tree levels using triplet views, where coding trees with essential information serve as the anchor view.","Extensive experiments on real-world datasets validate the effectiveness of SEGO, demonstrating superior performance over state-of-the-art baselines in OOD detection.","Specifically, our method achieves the best performance on 9 out of 10 dataset pairs, with an average improvement of 3.7\\% on OOD detection datasets, significantly surpassing the best competitor by 10.8\\% on the FreeSolv/ToxCast dataset pair."],"url":"http://arxiv.org/abs/2503.03241v1"}
{"created":"2025-03-05 07:29:12","title":"GenColor: Generative Color-Concept Association in Visual Design","abstract":"Existing approaches for color-concept association typically rely on query-based image referencing, and color extraction from image references. However, these approaches are effective only for common concepts, and are vulnerable to unstable image referencing and varying image conditions. Our formative study with designers underscores the need for primary-accent color compositions and context-dependent colors (e.g., 'clear' vs. 'polluted' sky) in design. In response, we introduce a generative approach for mining semantically resonant colors leveraging images generated by text-to-image models. Our insight is that contemporary text-to-image models can resemble visual patterns from large-scale real-world data. The framework comprises three stages: concept instancing produces generative samples using diffusion models, text-guided image segmentation identifies concept-relevant regions within the image, and color association extracts primarily accompanied by accent colors. Quantitative comparisons with expert designs validate our approach's effectiveness, and we demonstrate the applicability through cases in various design scenarios and a gallery.","sentences":["Existing approaches for color-concept association typically rely on query-based image referencing, and color extraction from image references.","However, these approaches are effective only for common concepts, and are vulnerable to unstable image referencing and varying image conditions.","Our formative study with designers underscores the need for primary-accent color compositions and context-dependent colors (e.g., 'clear' vs. 'polluted' sky) in design.","In response, we introduce a generative approach for mining semantically resonant colors leveraging images generated by text-to-image models.","Our insight is that contemporary text-to-image models can resemble visual patterns from large-scale real-world data.","The framework comprises three stages: concept instancing produces generative samples using diffusion models, text-guided image segmentation identifies concept-relevant regions within the image, and color association extracts primarily accompanied by accent colors.","Quantitative comparisons with expert designs validate our approach's effectiveness, and we demonstrate the applicability through cases in various design scenarios and a gallery."],"url":"http://arxiv.org/abs/2503.03236v1"}
{"created":"2025-03-05 07:24:00","title":"Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile Sensing on Humanoid Robots","abstract":"Humans are able to convey different messages using only touch. Equipping robots with the ability to understand social touch adds another modality in which humans and robots can communicate. In this paper, we present a social gesture recognition system using a fabric-based, large-scale tactile sensor integrated onto the arms of a humanoid robot. We built a social gesture dataset using multiple participants and extracted temporal features for classification. By collecting real-world data on a humanoid robot, our system provides valuable insights into human-robot social touch, further advancing the development of spHRI systems for more natural and effective communication.","sentences":["Humans are able to convey different messages using only touch.","Equipping robots with the ability to understand social touch adds another modality in which humans and robots can communicate.","In this paper, we present a social gesture recognition system using a fabric-based, large-scale tactile sensor integrated onto the arms of a humanoid robot.","We built a social gesture dataset using multiple participants and extracted temporal features for classification.","By collecting real-world data on a humanoid robot, our system provides valuable insights into human-robot social touch, further advancing the development of spHRI systems for more natural and effective communication."],"url":"http://arxiv.org/abs/2503.03234v1"}
{"created":"2025-03-05 06:32:49","title":"Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture","abstract":"Recovering absolute poses in the world coordinate system from monocular views presents significant challenges. Two primary issues arise in this context. Firstly, existing methods rely on 3D motion data for training, which requires collection in limited environments. Acquiring such 3D labels for new actions in a timely manner is impractical, severely restricting the model's generalization capabilities. In contrast, 2D poses are far more accessible and easier to obtain. Secondly, estimating a person's absolute position in metric space from a single viewpoint is inherently more complex. To address these challenges, we introduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motions into 2D poses, leveraging 2D data to enhance 3D motion reconstruction in diverse scenarios and accurately predict absolute positions in the world coordinate system. We initially pretrain a single-view diffusion model with extensive 2D data, followed by fine-tuning a multi-view diffusion model for view consistency using publicly available 3D data. This strategy facilitates the effective use of large-scale 2D data. Additionally, we propose an innovative human motion representation that decouples local actions from global movements and encodes geometric priors of the ground, ensuring the generative model learns accurate motion priors from 2D data. During inference, this allows for the gradual recovery of global movements, resulting in more plausible positioning. We evaluate our model's performance on real-world datasets, demonstrating superior accuracy in motion and absolute human positioning compared to state-of-the-art methods, along with enhanced generalization and scalability. Our code will be made publicly available.","sentences":["Recovering absolute poses in the world coordinate system from monocular views presents significant challenges.","Two primary issues arise in this context.","Firstly, existing methods rely on 3D motion data for training, which requires collection in limited environments.","Acquiring such 3D labels for new actions in a timely manner is impractical, severely restricting the model's generalization capabilities.","In contrast, 2D poses are far more accessible and easier to obtain.","Secondly, estimating a person's absolute position in metric space from a single viewpoint is inherently more complex.","To address these challenges, we introduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motions into 2D poses, leveraging 2D data to enhance 3D motion reconstruction in diverse scenarios and accurately predict absolute positions in the world coordinate system.","We initially pretrain a single-view diffusion model with extensive 2D data, followed by fine-tuning a multi-view diffusion model for view consistency using publicly available 3D data.","This strategy facilitates the effective use of large-scale 2D data.","Additionally, we propose an innovative human motion representation that decouples local actions from global movements and encodes geometric priors of the ground, ensuring the generative model learns accurate motion priors from 2D data.","During inference, this allows for the gradual recovery of global movements, resulting in more plausible positioning.","We evaluate our model's performance on real-world datasets, demonstrating superior accuracy in motion and absolute human positioning compared to state-of-the-art methods, along with enhanced generalization and scalability.","Our code will be made publicly available."],"url":"http://arxiv.org/abs/2503.03222v1"}
{"created":"2025-03-05 06:16:15","title":"COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open Source Intelligence","abstract":"Open Source Intelligence (OSINT) requires the integration and reasoning of diverse multimodal data, presenting significant challenges in deriving actionable insights. Traditional approaches, including multimodal large language models (MLLMs), often struggle to infer complex contextual relationships or deliver comprehensive intelligence from unstructured data sources. In this paper, we introduce COSINT-Agent, a knowledge-driven multimodal agent tailored to address the challenges of OSINT in the Chinese domain. COSINT-Agent seamlessly integrates the perceptual capabilities of fine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene Knowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match framework, which bridges COSINT-MLLM and EES-KG, enabling systematic extraction, reasoning, and contextualization of multimodal insights. This integration facilitates precise entity recognition, event interpretation, and context retrieval, effectively transforming raw multimodal data into actionable intelligence. Extensive experiments validate the superior performance of COSINT-Agent across core OSINT tasks, including entity recognition, EES generation, and context matching. These results underscore its potential as a robust and scalable solution for advancing automated multimodal reasoning and enhancing the effectiveness of OSINT methodologies.","sentences":["Open Source Intelligence (OSINT) requires the integration and reasoning of diverse multimodal data, presenting significant challenges in deriving actionable insights.","Traditional approaches, including multimodal large language models (MLLMs), often struggle to infer complex contextual relationships or deliver comprehensive intelligence from unstructured data sources.","In this paper, we introduce COSINT-Agent, a knowledge-driven multimodal agent tailored to address the challenges of OSINT in the Chinese domain.","COSINT-Agent seamlessly integrates the perceptual capabilities of fine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene Knowledge Graph (EES-KG).","Central to COSINT-Agent is the innovative EES-Match framework, which bridges COSINT-MLLM and EES-KG, enabling systematic extraction, reasoning, and contextualization of multimodal insights.","This integration facilitates precise entity recognition, event interpretation, and context retrieval, effectively transforming raw multimodal data into actionable intelligence.","Extensive experiments validate the superior performance of COSINT-Agent across core OSINT tasks, including entity recognition, EES generation, and context matching.","These results underscore its potential as a robust and scalable solution for advancing automated multimodal reasoning and enhancing the effectiveness of OSINT methodologies."],"url":"http://arxiv.org/abs/2503.03215v1"}
{"created":"2025-03-05 05:50:38","title":"An Analytical Theory of Power Law Spectral Bias in the Learning Dynamics of Diffusion Models","abstract":"We developed an analytical framework for understanding how the learned distribution evolves during diffusion model training. Leveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one- or two-layer linear denoiser settings with arbitrary data. Remarkably, these solutions allowed us to derive the generated distribution in closed form and its KL divergence through training. These analytical results expose a pronounced power-law spectral bias, i.e., for weights and distributions, the convergence time of a mode follows an inverse power law of its variance. Empirical experiments on both Gaussian and image datasets demonstrate that the power-law spectral bias remains robust even when using deeper or convolutional architectures. Our results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations for why earlier stopping could lead to incorrect details in image generative models.","sentences":["We developed an analytical framework for understanding how the learned distribution evolves during diffusion model training.","Leveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one- or two-layer linear denoiser settings with arbitrary data.","Remarkably, these solutions allowed us to derive the generated distribution in closed form and its KL divergence through training.","These analytical results expose a pronounced power-law spectral bias, i.e., for weights and distributions, the convergence time of a mode follows an inverse power law of its variance.","Empirical experiments on both Gaussian and image datasets demonstrate that the power-law spectral bias remains robust even when using deeper or convolutional architectures.","Our results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations for why earlier stopping could lead to incorrect details in image generative models."],"url":"http://arxiv.org/abs/2503.03206v1"}
{"created":"2025-03-05 05:46:21","title":"Access Specification-Aware Software Transactional Memory Techniques for Efficient Execution of Smart Contract Transactions","abstract":"For a high-performance blockchain like Supra's Layer 1, minimizing latencies across key components is crucial-such as data dissemination, consensus (or ordering), and transaction execution. While through significant innovations we have improved the first two, transaction execution remains an area for further optimization. Software Transactional Memory (STM) is a widely used technique for parallel execution, with Aptos' BlockSTM pioneering its application of efficient blockchain transaction processing on multi-core validator nodes. Subsequently, PEVM [13] adapted BlockSTM for EVM transaction execution. However, we identified a gap in existing STM techniques-while access specifications have been used in industry (e.g., Solana's user-provided read-write sets), they have not been leveraged to enhance STM efficiency. Our experimental analysis demonstrates that specification-aware STMs outperform their plain counterparts on both EVM and MoveVM. To maximize these benefits, we have designed specification-aware SupraSTM (saSupraSTM), a novel algorithm that fully utilizes access specifications. Through extensive testing, saSupraSTM outperforms both our specification-aware adaptation of Aptos' BlockSTM and specification-aware PEVM, setting a new benchmark for transaction execution efficiency in the context of blockchain networks.","sentences":["For a high-performance blockchain like Supra's Layer 1, minimizing latencies across key components is crucial-such as data dissemination, consensus (or ordering), and transaction execution.","While through significant innovations we have improved the first two, transaction execution remains an area for further optimization.","Software Transactional Memory (STM) is a widely used technique for parallel execution, with Aptos' BlockSTM pioneering its application of efficient blockchain transaction processing on multi-core validator nodes.","Subsequently, PEVM [13] adapted BlockSTM for EVM transaction execution.","However, we identified a gap in existing STM techniques-while access specifications have been used in industry (e.g., Solana's user-provided read-write sets), they have not been leveraged to enhance STM efficiency.","Our experimental analysis demonstrates that specification-aware STMs outperform their plain counterparts on both EVM and MoveVM.","To maximize these benefits, we have designed specification-aware SupraSTM (saSupraSTM), a novel algorithm that fully utilizes access specifications.","Through extensive testing, saSupraSTM outperforms both our specification-aware adaptation of Aptos' BlockSTM and specification-aware PEVM, setting a new benchmark for transaction execution efficiency in the context of blockchain networks."],"url":"http://arxiv.org/abs/2503.03203v1"}
{"created":"2025-03-05 05:46:08","title":"Variance-Aware Loss Scheduling for Multimodal Alignment in Low-Data Settings","abstract":"Training vision-language models for image-text alignment typically requires large datasets to achieve robust performance. In low-data scenarios, standard contrastive learning can struggle to align modalities effectively due to overfitting and unstable training dynamics. In this paper, we propose a variance-aware loss scheduling approach that dynamically adjusts the weighting of the contrastive loss based on the statistical variability (uncertainty) in the model's alignment predictions. Using a subset of the Flickr8k image-caption dataset to simulate limited data conditions, we demonstrate that our approach improves image-text retrieval accuracy compared to a fixed-weight baseline. We also compare against other adaptive weighting strategies (using output entropy and cosine similarity spread) and find that variance-aware scheduling provides the best overall trade-off. Qualitatively, our method yields more distinct multimodal embeddings as shown by t-SNE visualizations. Moreover, in a stress test with noise-injected captions and images, the variance-guided loss proves more robust, maintaining higher recall when random perturbations are introduced. These results highlight the benefit of adaptive loss weighting for multimodal alignment in low-data regimes.","sentences":["Training vision-language models for image-text alignment typically requires large datasets to achieve robust performance.","In low-data scenarios, standard contrastive learning can struggle to align modalities effectively due to overfitting and unstable training dynamics.","In this paper, we propose a variance-aware loss scheduling approach that dynamically adjusts the weighting of the contrastive loss based on the statistical variability (uncertainty) in the model's alignment predictions.","Using a subset of the Flickr8k image-caption dataset to simulate limited data conditions, we demonstrate that our approach improves image-text retrieval accuracy compared to a fixed-weight baseline.","We also compare against other adaptive weighting strategies (using output entropy and cosine similarity spread) and find that variance-aware scheduling provides the best overall trade-off.","Qualitatively, our method yields more distinct multimodal embeddings as shown by t-SNE visualizations.","Moreover, in a stress test with noise-injected captions and images, the variance-guided loss proves more robust, maintaining higher recall when random perturbations are introduced.","These results highlight the benefit of adaptive loss weighting for multimodal alignment in low-data regimes."],"url":"http://arxiv.org/abs/2503.03202v1"}
{"created":"2025-03-05 05:39:29","title":"Towards Robust Universal Information Extraction: Benchmark, Evaluation, and Solution","abstract":"In this paper, we aim to enhance the robustness of Universal Information Extraction (UIE) by introducing a new benchmark dataset, a comprehensive evaluation, and a feasible solution. Existing robust benchmark datasets have two key limitations: 1) They generate only a limited range of perturbations for a single Information Extraction (IE) task, which fails to evaluate the robustness of UIE models effectively; 2) They rely on small models or handcrafted rules to generate perturbations, often resulting in unnatural adversarial examples. Considering the powerful generation capabilities of Large Language Models (LLMs), we introduce a new benchmark dataset for Robust UIE, called RUIE-Bench, which utilizes LLMs to generate more diverse and realistic perturbations across different IE tasks. Based on this dataset, we comprehensively evaluate existing UIE models and reveal that both LLM-based models and other models suffer from significant performance drops. To improve robustness and reduce training costs, we propose a data-augmentation solution that dynamically selects hard samples for iterative training based on the model's inference loss. Experimental results show that training with only \\textbf{15\\%} of the data leads to an average \\textbf{7.5\\%} relative performance improvement across three IE tasks.","sentences":["In this paper, we aim to enhance the robustness of Universal Information Extraction (UIE) by introducing a new benchmark dataset, a comprehensive evaluation, and a feasible solution.","Existing robust benchmark datasets have two key limitations: 1) They generate only a limited range of perturbations for a single Information Extraction (IE) task, which fails to evaluate the robustness of UIE models effectively; 2) They rely on small models or handcrafted rules to generate perturbations, often resulting in unnatural adversarial examples.","Considering the powerful generation capabilities of Large Language Models (LLMs), we introduce a new benchmark dataset for Robust UIE, called RUIE-Bench, which utilizes LLMs to generate more diverse and realistic perturbations across different IE tasks.","Based on this dataset, we comprehensively evaluate existing UIE models and reveal that both LLM-based models and other models suffer from significant performance drops.","To improve robustness and reduce training costs, we propose a data-augmentation solution that dynamically selects hard samples for iterative training based on the model's inference loss.","Experimental results show that training with only \\textbf{15\\%} of the data leads to an average \\textbf{7.5\\%} relative performance improvement across three IE tasks."],"url":"http://arxiv.org/abs/2503.03201v1"}
{"created":"2025-03-05 05:36:26","title":"Transformer-Based Spatio-Temporal Association of Apple Fruitlets","abstract":"In this paper, we present a transformer-based method to spatio-temporally associate apple fruitlets in stereo-images collected on different days and from different camera poses. State-of-the-art association methods in agriculture are dedicated towards matching larger crops using either high-resolution point clouds or temporally stable features, which are both difficult to obtain for smaller fruit in the field. To address these challenges, we propose a transformer-based architecture that encodes the shape and position of each fruitlet, and propagates and refines these features through a series of transformer encoder layers with alternating self and cross-attention. We demonstrate that our method is able to achieve an F1-score of 92.4% on data collected in a commercial apple orchard and outperforms all baselines and ablations.","sentences":["In this paper, we present a transformer-based method to spatio-temporally associate apple fruitlets in stereo-images collected on different days and from different camera poses.","State-of-the-art association methods in agriculture are dedicated towards matching larger crops using either high-resolution point clouds or temporally stable features, which are both difficult to obtain for smaller fruit in the field.","To address these challenges, we propose a transformer-based architecture that encodes the shape and position of each fruitlet, and propagates and refines these features through a series of transformer encoder layers with alternating self and cross-attention.","We demonstrate that our method is able to achieve an F1-score of 92.4% on data collected in a commercial apple orchard and outperforms all baselines and ablations."],"url":"http://arxiv.org/abs/2503.03200v1"}
{"created":"2025-03-05 05:30:26","title":"Directly Follows Graphs Go Predictive Process Monitoring With Graph Neural Networks","abstract":"In the past years, predictive process monitoring (PPM) techniques based on artificial neural networks have evolved as a method to monitor the future behavior of business processes. Existing approaches mostly focus on interpreting the processes as sequences, so-called traces, and feeding them to neural architectures designed to operate on sequential data such as recurrent neural networks (RNNs) or transformers. In this study, we investigate an alternative way to perform PPM: by transforming each process in its directly-follows-graph (DFG) representation we are able to apply graph neural networks (GNNs) for the prediction tasks. By this, we aim to develop models that are more suitable for complex processes that are long and contain an abundance of loops. In particular, we present different ways to create DFG representations depending on the particular GNN we use. The tested GNNs range from classical node-based to novel edge-based architectures. Further, we investigate the possibility of using multi-graphs. By these steps, we aim to design graph representations that minimize the information loss when transforming traces into graphs.","sentences":["In the past years, predictive process monitoring (PPM) techniques based on artificial neural networks have evolved as a method to monitor the future behavior of business processes.","Existing approaches mostly focus on interpreting the processes as sequences, so-called traces, and feeding them to neural architectures designed to operate on sequential data such as recurrent neural networks (RNNs) or transformers.","In this study, we investigate an alternative way to perform PPM: by transforming each process in its directly-follows-graph (DFG) representation we are able to apply graph neural networks (GNNs) for the prediction tasks.","By this, we aim to develop models that are more suitable for complex processes that are long and contain an abundance of loops.","In particular, we present different ways to create DFG representations depending on the particular GNN we use.","The tested GNNs range from classical node-based to novel edge-based architectures.","Further, we investigate the possibility of using multi-graphs.","By these steps, we aim to design graph representations that minimize the information loss when transforming traces into graphs."],"url":"http://arxiv.org/abs/2503.03197v1"}
{"created":"2025-03-05 05:25:54","title":"Online Bidding under RoS Constraints without Knowing the Value","abstract":"We consider the problem of bidding in online advertising, where an advertiser aims to maximize value while adhering to budget and Return-on-Spend (RoS) constraints. Unlike prior work that assumes knowledge of the value generated by winning each impression ({e.g.,} conversions), we address the more realistic setting where the advertiser must simultaneously learn the optimal bidding strategy and the value of each impression opportunity. This introduces a challenging exploration-exploitation dilemma: the advertiser must balance exploring different bids to estimate impression values with exploiting current knowledge to bid effectively. To address this, we propose a novel Upper Confidence Bound (UCB)-style algorithm that carefully manages this trade-off. Via a rigorous theoretical analysis, we prove that our algorithm achieves $\\widetilde{O}(\\sqrt{T\\log(|\\mathcal{B}|T)})$ regret and constraint violation, where $T$ is the number of bidding rounds and $\\mathcal{B}$ is the domain of possible bids. This establishes the first optimal regret and constraint violation bounds for bidding in the online setting with unknown impression values. Moreover, our algorithm is computationally efficient and simple to implement. We validate our theoretical findings through experiments on synthetic data, demonstrating that our algorithm exhibits strong empirical performance compared to existing approaches.","sentences":["We consider the problem of bidding in online advertising, where an advertiser aims to maximize value while adhering to budget and Return-on-Spend (RoS) constraints.","Unlike prior work that assumes knowledge of the value generated by winning each impression ({e.g.,} conversions), we address the more realistic setting where the advertiser must simultaneously learn the optimal bidding strategy and the value of each impression opportunity.","This introduces a challenging exploration-exploitation dilemma: the advertiser must balance exploring different bids to estimate impression values with exploiting current knowledge to bid effectively.","To address this, we propose a novel Upper Confidence Bound (UCB)-style algorithm that carefully manages this trade-off.","Via a rigorous theoretical analysis, we prove that our algorithm achieves $\\widetilde{O}(\\sqrt{T\\log(|\\mathcal{B}|T)})$ regret and constraint violation, where $T$ is the number of bidding rounds and $\\mathcal{B}$ is the domain of possible bids.","This establishes the first optimal regret and constraint violation bounds for bidding in the online setting with unknown impression values.","Moreover, our algorithm is computationally efficient and simple to implement.","We validate our theoretical findings through experiments on synthetic data, demonstrating that our algorithm exhibits strong empirical performance compared to existing approaches."],"url":"http://arxiv.org/abs/2503.03195v1"}
{"created":"2025-03-05 05:17:15","title":"Distributed Certifiably Correct Range-Aided SLAM","abstract":"Reliable simultaneous localization and mapping (SLAM) algorithms are necessary for safety-critical autonomous navigation. In the communication-constrained multi-agent setting, navigation systems increasingly use point-to-point range sensors as they afford measurements with low bandwidth requirements and known data association. The state estimation problem for these systems takes the form of range-aided (RA) SLAM. However, distributed algorithms for solving the RA-SLAM problem lack formal guarantees on the quality of the returned estimate. To this end, we present the first distributed algorithm for RA-SLAM that can efficiently recover certifiably globally optimal solutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA), achieves this via the Riemannian Staircase method, where computational procedures developed for distributed certifiably correct pose graph optimization are generalized to the RA-SLAM problem. We demonstrate DCORA's efficacy on real-world multi-agent datasets by achieving absolute trajectory errors comparable to those of a state-of-the-art centralized certifiably correct RA-SLAM algorithm. Additionally, we perform a parametric study on the structure of the RA-SLAM problem using synthetic data, revealing how common parameters affect DCORA's performance.","sentences":["Reliable simultaneous localization and mapping (SLAM) algorithms are necessary for safety-critical autonomous navigation.","In the communication-constrained multi-agent setting, navigation systems increasingly use point-to-point range sensors as they afford measurements with low bandwidth requirements and known data association.","The state estimation problem for these systems takes the form of range-aided (RA) SLAM.","However, distributed algorithms for solving the RA-SLAM problem lack formal guarantees on the quality of the returned estimate.","To this end, we present the first distributed algorithm for RA-SLAM that can efficiently recover certifiably globally optimal solutions.","Our algorithm, distributed certifiably correct RA-SLAM (DCORA), achieves this via the Riemannian Staircase method, where computational procedures developed for distributed certifiably correct pose graph optimization are generalized to the RA-SLAM problem.","We demonstrate DCORA's efficacy on real-world multi-agent datasets by achieving absolute trajectory errors comparable to those of a state-of-the-art centralized certifiably correct RA-SLAM algorithm.","Additionally, we perform a parametric study on the structure of the RA-SLAM problem using synthetic data, revealing how common parameters affect DCORA's performance."],"url":"http://arxiv.org/abs/2503.03192v1"}
{"created":"2025-03-05 04:53:07","title":"Enhancing Cybersecurity in Critical Infrastructure with LLM-Assisted Explainable IoT Systems","abstract":"Ensuring the security of critical infrastructure has become increasingly vital with the proliferation of Internet of Things (IoT) systems. However, the heterogeneous nature of IoT data and the lack of human-comprehensible insights from anomaly detection models remain significant challenges. This paper presents a hybrid framework that combines numerical anomaly detection using Autoencoders with Large Language Models (LLMs) for enhanced preprocessing and interpretability. Two preprocessing approaches are implemented: a traditional method utilizing Principal Component Analysis (PCA) to reduce dimensionality and an LLM-assisted method where GPT-4 dynamically recommends feature selection, transformation, and encoding strategies.   Experimental results on the KDDCup99 10% corrected dataset demonstrate that the LLM-assisted preprocessing pipeline significantly improves anomaly detection performance. The macro-average F1 score increased from 0.49 in the traditional PCA-based approach to 0.98 with LLM-driven insights. Additionally, the LLM generates natural language explanations for detected anomalies, providing contextual insights into their causes and implications. This framework highlights the synergy between numerical AI models and LLMs, delivering an accurate, interpretable, and efficient solution for IoT cybersecurity in critical infrastructure.","sentences":["Ensuring the security of critical infrastructure has become increasingly vital with the proliferation of Internet of Things (IoT) systems.","However, the heterogeneous nature of IoT data and the lack of human-comprehensible insights from anomaly detection models remain significant challenges.","This paper presents a hybrid framework that combines numerical anomaly detection using Autoencoders with Large Language Models (LLMs) for enhanced preprocessing and interpretability.","Two preprocessing approaches are implemented: a traditional method utilizing Principal Component Analysis (PCA) to reduce dimensionality and an LLM-assisted method where GPT-4 dynamically recommends feature selection, transformation, and encoding strategies.   ","Experimental results on the KDDCup99 10% corrected dataset demonstrate that the LLM-assisted preprocessing pipeline significantly improves anomaly detection performance.","The macro-average F1 score increased from 0.49 in the traditional PCA-based approach to 0.98 with LLM-driven insights.","Additionally, the LLM generates natural language explanations for detected anomalies, providing contextual insights into their causes and implications.","This framework highlights the synergy between numerical AI models and LLMs, delivering an accurate, interpretable, and efficient solution for IoT cybersecurity in critical infrastructure."],"url":"http://arxiv.org/abs/2503.03180v1"}
{"created":"2025-03-05 04:48:14","title":"Active operator learning with predictive uncertainty quantification for partial differential equations","abstract":"In this work, we develop a method for uncertainty quantification in deep operator networks (DeepONets) using predictive uncertainty estimates calibrated to model errors observed during training. The uncertainty framework operates using a single network, in contrast to existing ensemble approaches, and introduces minimal overhead during training and inference. We also introduce an optimized implementation for DeepONet inference (reducing evaluation times by a factor of five) to provide models well-suited for real-time applications. We evaluate the uncertainty-equipped models on a series of partial differential equation (PDE) problems, and show that the model predictions are unbiased, non-skewed, and accurately reproduce solutions to the PDEs. To assess how well the models generalize, we evaluate the network predictions and uncertainty estimates on in-distribution and out-of-distribution test datasets. We find the predictive uncertainties accurately reflect the observed model errors over a range of problems with varying complexity; simpler out-of-distribution examples are assigned low uncertainty estimates, consistent with the observed errors, while more complex out-of-distribution examples are properly assigned higher uncertainties. We also provide a statistical analysis of the predictive uncertainties and verify that these estimates are well-aligned with the observed error distributions at the tail-end of training. Finally, we demonstrate how predictive uncertainties can be used within an active learning framework to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures.","sentences":["In this work, we develop a method for uncertainty quantification in deep operator networks (DeepONets) using predictive uncertainty estimates calibrated to model errors observed during training.","The uncertainty framework operates using a single network, in contrast to existing ensemble approaches, and introduces minimal overhead during training and inference.","We also introduce an optimized implementation for DeepONet inference (reducing evaluation times by a factor of five) to provide models well-suited for real-time applications.","We evaluate the uncertainty-equipped models on a series of partial differential equation (PDE) problems, and show that the model predictions are unbiased, non-skewed, and accurately reproduce solutions to the PDEs.","To assess how well the models generalize, we evaluate the network predictions and uncertainty estimates on in-distribution and out-of-distribution test datasets.","We find the predictive uncertainties accurately reflect the observed model errors over a range of problems with varying complexity; simpler out-of-distribution examples are assigned low uncertainty estimates, consistent with the observed errors, while more complex out-of-distribution examples are properly assigned higher uncertainties.","We also provide a statistical analysis of the predictive uncertainties and verify that these estimates are well-aligned with the observed error distributions at the tail-end of training.","Finally, we demonstrate how predictive uncertainties can be used within an active learning framework to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures."],"url":"http://arxiv.org/abs/2503.03178v1"}
{"created":"2025-03-05 04:16:36","title":"A Predict-Then-Optimize Customer Allocation Framework for Online Fund Recommendation","abstract":"With the rapid growth of online investment platforms, funds can be distributed to individual customers online. The central issue is to match funds with potential customers under constraints. Most mainstream platforms adopt the recommendation formulation to tackle the problem. However, the traditional recommendation regime has its inherent drawbacks when applying the fund-matching problem with multiple constraints. In this paper, we model the fund matching under the allocation formulation. We design PTOFA, a Predict-Then-Optimize Fund Allocation framework. This data-driven framework consists of two stages, i.e., prediction and optimization, which aim to predict expected revenue based on customer behavior and optimize the impression allocation to achieve the maximum revenue under the necessary constraints, respectively. Extensive experiments on real-world datasets from an industrial online investment platform validate the effectiveness and efficiency of our solution. Additionally, the online A/B tests demonstrate PTOFA's effectiveness in the real-world fund recommendation scenario.","sentences":["With the rapid growth of online investment platforms, funds can be distributed to individual customers online.","The central issue is to match funds with potential customers under constraints.","Most mainstream platforms adopt the recommendation formulation to tackle the problem.","However, the traditional recommendation regime has its inherent drawbacks when applying the fund-matching problem with multiple constraints.","In this paper, we model the fund matching under the allocation formulation.","We design PTOFA, a Predict-Then-Optimize Fund Allocation framework.","This data-driven framework consists of two stages, i.e., prediction and optimization, which aim to predict expected revenue based on customer behavior and optimize the impression allocation to achieve the maximum revenue under the necessary constraints, respectively.","Extensive experiments on real-world datasets from an industrial online investment platform validate the effectiveness and efficiency of our solution.","Additionally, the online A/B tests demonstrate PTOFA's effectiveness in the real-world fund recommendation scenario."],"url":"http://arxiv.org/abs/2503.03165v1"}
{"created":"2025-03-05 04:05:09","title":"SpinML: Customized Synthetic Data Generation for Private Training of Specialized ML Models","abstract":"Specialized machine learning (ML) models tailored to users needs and requests are increasingly being deployed on smart devices with cameras, to provide personalized intelligent services taking advantage of camera data. However, two primary challenges hinder the training of such models: the lack of publicly available labeled data suitable for specialized tasks and the inaccessibility of labeled private data due to concerns about user privacy. To address these challenges, we propose a novel system SpinML, where the server generates customized Synthetic image data to Privately traIN a specialized ML model tailored to the user request, with the usage of only a few sanitized reference images from the user. SpinML offers users fine-grained, object-level control over the reference images, which allows user to trade between the privacy and utility of the generated synthetic data according to their privacy preferences. Through experiments on three specialized model training tasks, we demonstrate that our proposed system can enhance the performance of specialized models without compromising users privacy preferences.","sentences":["Specialized machine learning (ML) models tailored to users needs and requests are increasingly being deployed on smart devices with cameras, to provide personalized intelligent services taking advantage of camera data.","However, two primary challenges hinder the training of such models: the lack of publicly available labeled data suitable for specialized tasks and the inaccessibility of labeled private data due to concerns about user privacy.","To address these challenges, we propose a novel system SpinML, where the server generates customized Synthetic image data to Privately traIN a specialized ML model tailored to the user request, with the usage of only a few sanitized reference images from the user.","SpinML offers users fine-grained, object-level control over the reference images, which allows user to trade between the privacy and utility of the generated synthetic data according to their privacy preferences.","Through experiments on three specialized model training tasks, we demonstrate that our proposed system can enhance the performance of specialized models without compromising users privacy preferences."],"url":"http://arxiv.org/abs/2503.03160v1"}
{"created":"2025-03-05 03:56:01","title":"DiRe-JAX: A JAX based Dimensionality Reduction Algorithm for Large-scale Data","abstract":"DiRe-JAX is a new dimensionality reduction toolkit designed to address some of the challenges faced by traditional methods like UMAP and tSNE such as loss of global structure and computational efficiency. Built on the JAX framework, DiRe leverages modern hardware acceleration to provide an efficient, scalable, and interpretable solution for visualizing complex data structures, and for quantitative analysis of lower-dimensional embeddings. The toolkit shows considerable promise in preserving both local and global structures within the data as compare to state-of-the-art UMAP and tSNE implementations. This makes it suitable for a wide range of applications in machine learning, bioinformatics, and data science.","sentences":["DiRe-JAX is a new dimensionality reduction toolkit designed to address some of the challenges faced by traditional methods like UMAP and tSNE such as loss of global structure and computational efficiency.","Built on the JAX framework, DiRe leverages modern hardware acceleration to provide an efficient, scalable, and interpretable solution for visualizing complex data structures, and for quantitative analysis of lower-dimensional embeddings.","The toolkit shows considerable promise in preserving both local and global structures within the data as compare to state-of-the-art UMAP and tSNE implementations.","This makes it suitable for a wide range of applications in machine learning, bioinformatics, and data science."],"url":"http://arxiv.org/abs/2503.03156v1"}
{"created":"2025-03-05 03:54:51","title":"Dango: A Mixed-Initiative Data Wrangling System using Large Language Model","abstract":"Data wrangling is a time-consuming and challenging task in a data science pipeline. While many tools have been proposed to automate or facilitate data wrangling, they often misinterpret user intent, especially in complex tasks. We propose Dango, a mixed-initiative multi-agent system for data wrangling. Compared to existing tools, Dango enhances user communication of intent by allowing users to demonstrate on multiple tables and use natural language prompts in a conversation interface, enabling users to clarify their intent by answering LLM-posed multiple-choice clarification questions, and providing multiple forms of feedback such as step-by-step natural language explanations and data provenance to help users evaluate the data wrangling scripts. We conducted a within-subjects user study with 38 participants and demonstrated that Dango's features can significantly improve intent clarification, accuracy, and efficiency in data wrangling. Furthermore, we demonstrated the generalizability of Dango by applying it to a broader set of data wrangling tasks.","sentences":["Data wrangling is a time-consuming and challenging task in a data science pipeline.","While many tools have been proposed to automate or facilitate data wrangling, they often misinterpret user intent, especially in complex tasks.","We propose Dango, a mixed-initiative multi-agent system for data wrangling.","Compared to existing tools, Dango enhances user communication of intent by allowing users to demonstrate on multiple tables and use natural language prompts in a conversation interface, enabling users to clarify their intent by answering LLM-posed multiple-choice clarification questions, and providing multiple forms of feedback such as step-by-step natural language explanations and data provenance to help users evaluate the data wrangling scripts.","We conducted a within-subjects user study with 38 participants and demonstrated that Dango's features can significantly improve intent clarification, accuracy, and efficiency in data wrangling.","Furthermore, we demonstrated the generalizability of Dango by applying it to a broader set of data wrangling tasks."],"url":"http://arxiv.org/abs/2503.03154v1"}
{"created":"2025-03-05 03:47:17","title":"Position: Model Collapse Does Not Mean What You Think","abstract":"The proliferation of AI-generated content online has fueled concerns over \\emph{model collapse}, a degradation in future generative models' performance when trained on synthetic data generated by earlier models. Industry leaders, premier research journals and popular science publications alike have prophesied catastrophic societal consequences stemming from model collapse. In this position piece, we contend this widespread narrative fundamentally misunderstands the scientific evidence. We highlight that research on model collapse actually encompasses eight distinct and at times conflicting definitions of model collapse, and argue that inconsistent terminology within and between papers has hindered building a comprehensive understanding of model collapse. To assess how significantly different interpretations of model collapse threaten future generative models, we posit what we believe are realistic conditions for studying model collapse and then conduct a rigorous assessment of the literature's methodologies through this lens. While we leave room for reasonable disagreement, our analysis of research studies, weighted by how faithfully each study matches real-world conditions, leads us to conclude that certain predicted claims of model collapse rely on assumptions and conditions that poorly match real-world conditions, and in fact several prominent collapse scenarios are readily avoidable. Altogether, this position paper argues that model collapse has been warped from a nuanced multifaceted consideration into an oversimplified threat, and that the evidence suggests specific harms more likely under society's current trajectory have received disproportionately less attention.","sentences":["The proliferation of AI-generated content online has fueled concerns over \\emph{model collapse}, a degradation in future generative models' performance when trained on synthetic data generated by earlier models.","Industry leaders, premier research journals and popular science publications alike have prophesied catastrophic societal consequences stemming from model collapse.","In this position piece, we contend this widespread narrative fundamentally misunderstands the scientific evidence.","We highlight that research on model collapse actually encompasses eight distinct and at times conflicting definitions of model collapse, and argue that inconsistent terminology within and between papers has hindered building a comprehensive understanding of model collapse.","To assess how significantly different interpretations of model collapse threaten future generative models, we posit what we believe are realistic conditions for studying model collapse and then conduct a rigorous assessment of the literature's methodologies through this lens.","While we leave room for reasonable disagreement, our analysis of research studies, weighted by how faithfully each study matches real-world conditions, leads us to conclude that certain predicted claims of model collapse rely on assumptions and conditions that poorly match real-world conditions, and in fact several prominent collapse scenarios are readily avoidable.","Altogether, this position paper argues that model collapse has been warped from a nuanced multifaceted consideration into an oversimplified threat, and that the evidence suggests specific harms more likely under society's current trajectory have received disproportionately less attention."],"url":"http://arxiv.org/abs/2503.03150v1"}
{"created":"2025-03-05 03:41:57","title":"PriFFT: Privacy-preserving Federated Fine-tuning of Large Language Models via Function Secret Sharing","abstract":"Fine-tuning large language models (LLMs) raises privacy concerns due to the risk of exposing sensitive training data. Federated learning (FL) mitigates this risk by keeping training samples on local devices, but recent studies show that adversaries can still infer private information from model updates in FL. Additionally, LLM parameters are typically shared publicly during federated fine-tuning, while developers are often reluctant to disclose these parameters, posing further security challenges. Inspired by the above problems, we propose PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both the model updates and parameters. In PriFFT, clients and the server share model inputs and parameters by secret sharing, performing secure fine-tuning on shared values without accessing plaintext data. Due to considerable LLM parameters, privacy-preserving federated fine-tuning invokes complex secure calculations and requires substantial communication and computation resources. To optimize the efficiency of privacy-preserving federated fine-tuning of LLMs, we introduce function secret-sharing protocols for various operations, including reciprocal calculation, tensor products, natural exponentiation, softmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to 4.02X speed improvement and reduce 7.19X communication overhead compared to the implementation based on existing secret sharing methods. Besides, PriFFT achieves a 2.23X speed improvement and reduces 4.08X communication overhead in privacy-preserving fine-tuning without accuracy drop compared to the existing secret sharing methods.","sentences":["Fine-tuning large language models (LLMs) raises privacy concerns due to the risk of exposing sensitive training data.","Federated learning (FL) mitigates this risk by keeping training samples on local devices, but recent studies show that adversaries can still infer private information from model updates in FL.","Additionally, LLM parameters are typically shared publicly during federated fine-tuning, while developers are often reluctant to disclose these parameters, posing further security challenges.","Inspired by the above problems, we propose PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both the model updates and parameters.","In PriFFT, clients and the server share model inputs and parameters by secret sharing, performing secure fine-tuning on shared values without accessing plaintext data.","Due to considerable LLM parameters, privacy-preserving federated fine-tuning invokes complex secure calculations and requires substantial communication and computation resources.","To optimize the efficiency of privacy-preserving federated fine-tuning of LLMs, we introduce function secret-sharing protocols for various operations, including reciprocal calculation, tensor products, natural exponentiation, softmax, hyperbolic tangent, and dropout.","The proposed protocols achieve up to 4.02X speed improvement and reduce 7.19X communication overhead compared to the implementation based on existing secret sharing methods.","Besides, PriFFT achieves a 2.23X speed improvement and reduces 4.08X communication overhead in privacy-preserving fine-tuning without accuracy drop compared to the existing secret sharing methods."],"url":"http://arxiv.org/abs/2503.03146v1"}
{"created":"2025-03-05 03:33:31","title":"A Survey of Foundation Models for Environmental Science","abstract":"Modeling environmental ecosystems is essential for effective resource management, sustainable development, and understanding complex ecological processes. However, traditional methods frequently struggle with the inherent complexity, interconnectedness, and limited data of such systems. Foundation models, with their large-scale pre-training and universal representations, offer transformative opportunities by integrating diverse data sources, capturing spatiotemporal dependencies, and adapting to a broad range of tasks. This survey presents a comprehensive overview of foundation model applications in environmental science, highlighting advancements in forward prediction, data generation, data assimilation, downscaling, model ensembling, and decision-making across domains. We also detail the development process of these models, covering data collection, architecture design, training, tuning, and evaluation. By showcasing these emerging methods, we aim to foster interdisciplinary collaboration and advance the integration of cutting-edge machine learning for sustainable solutions in environmental science.","sentences":["Modeling environmental ecosystems is essential for effective resource management, sustainable development, and understanding complex ecological processes.","However, traditional methods frequently struggle with the inherent complexity, interconnectedness, and limited data of such systems.","Foundation models, with their large-scale pre-training and universal representations, offer transformative opportunities by integrating diverse data sources, capturing spatiotemporal dependencies, and adapting to a broad range of tasks.","This survey presents a comprehensive overview of foundation model applications in environmental science, highlighting advancements in forward prediction, data generation, data assimilation, downscaling, model ensembling, and decision-making across domains.","We also detail the development process of these models, covering data collection, architecture design, training, tuning, and evaluation.","By showcasing these emerging methods, we aim to foster interdisciplinary collaboration and advance the integration of cutting-edge machine learning for sustainable solutions in environmental science."],"url":"http://arxiv.org/abs/2503.03142v1"}
{"created":"2025-03-05 03:26:54","title":"Knowledge Augmentation in Federation: Rethinking What Collaborative Learning Can Bring Back to Decentralized Data","abstract":"Data, as an observable form of knowledge, has become one of the most important factors of production for the development of Artificial Intelligence (AI). Meanwhile, increasing legislation and regulations on private and proprietary information results in scattered data sources also known as the ``data islands''. Although some collaborative learning paradigms such as Federated Learning (FL) can enable privacy-preserving training over decentralized data, they have inherent deficiencies in fairness, costs and reproducibility because of being learning-centric, which greatly limits the way how participants cooperate with each other. In light of this, we present a knowledge-centric paradigm termed \\emph{Knowledge Augmentation in Federation} (KAF), with focus on how to enhance local knowledge through collaborative effort. We provide the suggested system architecture, formulate the prototypical optimization objective, and review emerging studies that employ methodologies suitable for KAF. On our roadmap, with a three-way categorization we describe the methods for knowledge expansion, knowledge filtering, and label and feature space correction in the federation. Further, we highlight several challenges and open questions that deserve more attention from the community. With our investigation, we intend to offer new insights for what collaborative learning can bring back to decentralized data.","sentences":["Data, as an observable form of knowledge, has become one of the most important factors of production for the development of Artificial Intelligence (AI).","Meanwhile, increasing legislation and regulations on private and proprietary information results in scattered data sources also known as the ``data islands''.","Although some collaborative learning paradigms such as Federated Learning (FL) can enable privacy-preserving training over decentralized data, they have inherent deficiencies in fairness, costs and reproducibility because of being learning-centric, which greatly limits the way how participants cooperate with each other.","In light of this, we present a knowledge-centric paradigm termed \\emph{Knowledge Augmentation in Federation} (KAF), with focus on how to enhance local knowledge through collaborative effort.","We provide the suggested system architecture, formulate the prototypical optimization objective, and review emerging studies that employ methodologies suitable for KAF.","On our roadmap, with a three-way categorization we describe the methods for knowledge expansion, knowledge filtering, and label and feature space correction in the federation.","Further, we highlight several challenges and open questions that deserve more attention from the community.","With our investigation, we intend to offer new insights for what collaborative learning can bring back to decentralized data."],"url":"http://arxiv.org/abs/2503.03140v1"}
{"created":"2025-03-05 03:26:48","title":"Convergence Analysis of Federated Learning Methods Using Backward Error Analysis","abstract":"Backward error analysis allows finding a modified loss function, which the parameter updates really follow under the influence of an optimization method. The additional loss terms included in this modified function is called implicit regularizer. In this paper, we attempt to find the implicit regularizer for various federated learning algorithms on non-IID data distribution, and explain why each method shows different convergence behavior. We first show that the implicit regularizer of FedAvg disperses the gradient of each client from the average gradient, thus increasing the gradient variance. We also empirically show that the implicit regularizer hampers its convergence. Similarly, we compute the implicit regularizers of FedSAM and SCAFFOLD, and explain why they converge better. While existing convergence analyses focus on pointing out the advantages of FedSAM and SCAFFOLD, our approach can explain their limitations in complex non-convex settings. In specific, we demonstrate that FedSAM can partially remove the bias in the first-order term of the implicit regularizer in FedAvg, whereas SCAFFOLD can fully eliminate the bias in the first-order term, but not in the second-order term. Consequently, the implicit regularizer can provide a useful insight on the convergence behavior of federated learning from a different theoretical perspective.","sentences":["Backward error analysis allows finding a modified loss function, which the parameter updates really follow under the influence of an optimization method.","The additional loss terms included in this modified function is called implicit regularizer.","In this paper, we attempt to find the implicit regularizer for various federated learning algorithms on non-IID data distribution, and explain why each method shows different convergence behavior.","We first show that the implicit regularizer of FedAvg disperses the gradient of each client from the average gradient, thus increasing the gradient variance.","We also empirically show that the implicit regularizer hampers its convergence.","Similarly, we compute the implicit regularizers of FedSAM and SCAFFOLD, and explain why they converge better.","While existing convergence analyses focus on pointing out the advantages of FedSAM and SCAFFOLD, our approach can explain their limitations in complex non-convex settings.","In specific, we demonstrate that FedSAM can partially remove the bias in the first-order term of the implicit regularizer in FedAvg, whereas SCAFFOLD can fully eliminate the bias in the first-order term, but not in the second-order term.","Consequently, the implicit regularizer can provide a useful insight on the convergence behavior of federated learning from a different theoretical perspective."],"url":"http://arxiv.org/abs/2503.03139v1"}
{"created":"2025-03-05 03:15:38","title":"Bridging Molecular Graphs and Large Language Models","abstract":"While Large Language Models (LLMs) have shown exceptional generalization capabilities, their ability to process graph data, such as molecular structures, remains limited. To bridge this gap, this paper proposes Graph2Token, an efficient solution that aligns graph tokens to LLM tokens. The key idea is to represent a graph token with the LLM token vocabulary, without fine-tuning the LLM backbone. To achieve this goal, we first construct a molecule-text paired dataset from multisources, including CHEBI and HMDB, to train a graph structure encoder, which reduces the distance between graphs and texts representations in the feature space. Then, we propose a novel alignment strategy that associates a graph token with LLM tokens. To further unleash the potential of LLMs, we collect molecular IUPAC name identifiers, which are incorporated into the LLM prompts. By aligning molecular graphs as special tokens, we can activate LLM generalization ability to molecular few-shot learning. Extensive experiments on molecular classification and regression tasks demonstrate the effectiveness of our proposed Graph2Token.","sentences":["While Large Language Models (LLMs) have shown exceptional generalization capabilities, their ability to process graph data, such as molecular structures, remains limited.","To bridge this gap, this paper proposes Graph2Token, an efficient solution that aligns graph tokens to LLM tokens.","The key idea is to represent a graph token with the LLM token vocabulary, without fine-tuning the LLM backbone.","To achieve this goal, we first construct a molecule-text paired dataset from multisources, including CHEBI and HMDB, to train a graph structure encoder, which reduces the distance between graphs and texts representations in the feature space.","Then, we propose a novel alignment strategy that associates a graph token with LLM tokens.","To further unleash the potential of LLMs, we collect molecular IUPAC name identifiers, which are incorporated into the LLM prompts.","By aligning molecular graphs as special tokens, we can activate LLM generalization ability to molecular few-shot learning.","Extensive experiments on molecular classification and regression tasks demonstrate the effectiveness of our proposed Graph2Token."],"url":"http://arxiv.org/abs/2503.03135v1"}
{"created":"2025-03-05 02:51:50","title":"Exploring Neural Ordinary Differential Equations as Interpretable Healthcare classifiers","abstract":"Deep Learning has emerged as one of the most significant innovations in machine learning. However, a notable limitation of this field lies in the ``black box\" decision-making processes, which have led to skepticism within groups like healthcare and scientific communities regarding its applicability. In response, this study introduces a interpretable approach using Neural Ordinary Differential Equations (NODEs), a category of neural network models that exploit the dynamics of differential equations for representation learning. Leveraging their foundation in differential equations, we illustrate the capability of these models to continuously process textual data, marking the first such model of its kind, and thereby proposing a promising direction for future research in this domain. The primary objective of this research is to propose a novel architecture for groups like healthcare that require the predictive capabilities of deep learning while emphasizing the importance of model transparency demonstrated in NODEs.","sentences":["Deep Learning has emerged as one of the most significant innovations in machine learning.","However, a notable limitation of this field lies in the ``black box\" decision-making processes, which have led to skepticism within groups like healthcare and scientific communities regarding its applicability.","In response, this study introduces a interpretable approach using Neural Ordinary Differential Equations (NODEs), a category of neural network models that exploit the dynamics of differential equations for representation learning.","Leveraging their foundation in differential equations, we illustrate the capability of these models to continuously process textual data, marking the first such model of its kind, and thereby proposing a promising direction for future research in this domain.","The primary objective of this research is to propose a novel architecture for groups like healthcare that require the predictive capabilities of deep learning while emphasizing the importance of model transparency demonstrated in NODEs."],"url":"http://arxiv.org/abs/2503.03129v1"}
{"created":"2025-03-05 02:37:41","title":"The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models","abstract":"Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling.","sentences":["Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data.","However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions.","To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations.","Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling."],"url":"http://arxiv.org/abs/2503.03122v1"}
{"created":"2025-03-05 02:22:01","title":"PromAssistant: Leveraging Large Language Models for Text-to-PromQL","abstract":"With the increasing complexity of modern online service systems, understanding the state and behavior of the systems is essential for ensuring their reliability and stability. Therefore, metric monitoring systems are widely used and become an important infrastructure in online service systems. Engineers usually interact with metrics data by manually writing domain-specific language (DSL) queries to achieve various analysis objectives. However, writing these queries can be challenging and time-consuming, as it requires engineers to have high programming skills and understand the context of the system. In this paper, we focus on PromQL, which is the metric query DSL provided by the widely used metric monitoring system Prometheus. We aim to simplify metrics querying by enabling engineers to interact with metrics data in Prometheus through natural language, and we call this task text-to-PromQL. Building upon the insight, this paper proposes PromAssistant, a Large Language Model-based text-to-PromQL framework. PromAssistant first uses a knowledge graph to describe the complex context of an online service system. Then, through the synergistic reasoning of LLMs and the knowledge graph, PromAssistant transforms engineers' natural language questions into PromQL queries. To evaluate PromAssistant, we manually construct the first text-to-PromQL benchmark dataset which contains 280 metric query questions. The experiment results show that PromAssistant is effective in text-to-PromQL and outperforms baseline approaches. To the best of our knowledge, this paper is the first study of text-to-PromQL, and PromAssistant pioneered the DSL generation framework for metric querying and analysis.","sentences":["With the increasing complexity of modern online service systems, understanding the state and behavior of the systems is essential for ensuring their reliability and stability.","Therefore, metric monitoring systems are widely used and become an important infrastructure in online service systems.","Engineers usually interact with metrics data by manually writing domain-specific language (DSL) queries to achieve various analysis objectives.","However, writing these queries can be challenging and time-consuming, as it requires engineers to have high programming skills and understand the context of the system.","In this paper, we focus on PromQL, which is the metric query DSL provided by the widely used metric monitoring system Prometheus.","We aim to simplify metrics querying by enabling engineers to interact with metrics data in Prometheus through natural language, and we call this task text-to-PromQL.","Building upon the insight, this paper proposes PromAssistant, a Large Language Model-based text-to-PromQL framework.","PromAssistant first uses a knowledge graph to describe the complex context of an online service system.","Then, through the synergistic reasoning of LLMs and the knowledge graph, PromAssistant transforms engineers' natural language questions into PromQL queries.","To evaluate PromAssistant, we manually construct the first text-to-PromQL benchmark dataset which contains 280 metric query questions.","The experiment results show that PromAssistant is effective in text-to-PromQL and outperforms baseline approaches.","To the best of our knowledge, this paper is the first study of text-to-PromQL, and PromAssistant pioneered the DSL generation framework for metric querying and analysis."],"url":"http://arxiv.org/abs/2503.03114v1"}
{"created":"2025-03-05 02:18:31","title":"Predicting Space Tourism Demand Using Explainable AI","abstract":"Comprehensive forecasts of space tourism demand are crucial for businesses to optimize strategies and customer experiences in this burgeoning industry. Traditional methods struggle to capture the complex factors influencing an individual's decision to travel to space. In this paper, we propose an explainable and trustworthy artificial intelligence framework to address the challenge of predicting space tourism demand by following the National Institute of Standards and Technology guidelines. We develop a novel machine learning network, called SpaceNet, capable of learning wide-range dependencies in data and allowing us to analyze the relationships between various factors such as age, income, and risk tolerance. We investigate space travel demand in the US, categorizing it into four types: no travel, moon travel, suborbital, and orbital travel. To this end, we collected 1860 data points in many states and cities with different ages and then conducted our experiment with the data. From our experiments, the SpaceNet achieves an average ROC-AUC of 0.82 $\\pm$ 0.088, indicating strong classification performance. Our investigation demonstrated that travel price, age, annual income, gender, and fatality probability are important features in deciding whether a person wants to travel or not. Beyond demand forecasting, we use explainable AI to provide interpretation for the travel-type decisions of an individual, offering insights into the factors driving interest in space travel, which is not possible with traditional classification methods. This knowledge enables businesses to tailor marketing strategies and optimize service offerings in this rapidly evolving market. To the best of our knowledge, this is the first work to implement an explainable and interpretable AI framework for investigating the factors influencing space tourism.","sentences":["Comprehensive forecasts of space tourism demand are crucial for businesses to optimize strategies and customer experiences in this burgeoning industry.","Traditional methods struggle to capture the complex factors influencing an individual's decision to travel to space.","In this paper, we propose an explainable and trustworthy artificial intelligence framework to address the challenge of predicting space tourism demand by following the National Institute of Standards and Technology guidelines.","We develop a novel machine learning network, called SpaceNet, capable of learning wide-range dependencies in data and allowing us to analyze the relationships between various factors such as age, income, and risk tolerance.","We investigate space travel demand in the US, categorizing it into four types: no travel, moon travel, suborbital, and orbital travel.","To this end, we collected 1860 data points in many states and cities with different ages and then conducted our experiment with the data.","From our experiments, the SpaceNet achieves an average ROC-AUC of 0.82 $\\pm$ 0.088, indicating strong classification performance.","Our investigation demonstrated that travel price, age, annual income, gender, and fatality probability are important features in deciding whether a person wants to travel or not.","Beyond demand forecasting, we use explainable AI to provide interpretation for the travel-type decisions of an individual, offering insights into the factors driving interest in space travel, which is not possible with traditional classification methods.","This knowledge enables businesses to tailor marketing strategies and optimize service offerings in this rapidly evolving market.","To the best of our knowledge, this is the first work to implement an explainable and interpretable AI framework for investigating the factors influencing space tourism."],"url":"http://arxiv.org/abs/2503.03113v1"}
{"created":"2025-03-05 01:32:56","title":"Car-STAGE: Automated framework for large-scale high-dimensional simulated time-series data generation based on user-defined criteria","abstract":"Generating large-scale sensing datasets through photo-realistic simulation is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of synchronous data collection from the open-source CARLA simulator using multiple sensors attached to vehicle based on user-defined criteria. We propose a novel, one-step framework that we refer to as Car-STAGE, based on CARLA simulator, to generate data using a graphical user interface (GUI) defining configuration parameters to data collection without any user intervention. This framework can utilize the user-defined configuration parameters such as choice of maps, number and configurations of sensors, environmental and lighting conditions etc. to run the simulation in the background, collecting high-dimensional sensor data from diverse sensors such as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor, Semantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow Camera along with the ground-truths of the individual actors and storing the sensor data as well as ground-truth labels in a local or cloud-based database. The framework uses multiple threads where a main thread runs the server, a worker thread deals with queue and frame number and the rest of the threads processes the sensor data. The other way we derive speed up over the native implementation is by memory mapping the raw binary data into the disk and then converting the data into known formats at the end of data collection. We show that using these techniques, we gain a significant speed up over frames, under an increasing set of sensors and over the number of spawned objects.","sentences":["Generating large-scale sensing datasets through photo-realistic simulation is an important aspect of many robotics applications such as autonomous driving.","In this paper, we consider the problem of synchronous data collection from the open-source CARLA simulator using multiple sensors attached to vehicle based on user-defined criteria.","We propose a novel, one-step framework that we refer to as Car-STAGE, based on CARLA simulator, to generate data using a graphical user interface (GUI) defining configuration parameters to data collection without any user intervention.","This framework can utilize the user-defined configuration parameters such as choice of maps, number and configurations of sensors, environmental and lighting conditions etc. to run the simulation in the background, collecting high-dimensional sensor data from diverse sensors such as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor, Semantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow Camera along with the ground-truths of the individual actors and storing the sensor data as well as ground-truth labels in a local or cloud-based database.","The framework uses multiple threads where a main thread runs the server, a worker thread deals with queue and frame number and the rest of the threads processes the sensor data.","The other way we derive speed up over the native implementation is by memory mapping the raw binary data into the disk and then converting the data into known formats at the end of data collection.","We show that using these techniques, we gain a significant speed up over frames, under an increasing set of sensors and over the number of spawned objects."],"url":"http://arxiv.org/abs/2503.03100v1"}
{"created":"2025-03-05 01:18:11","title":"MuCo-KGC: Multi-Context-Aware Knowledge Graph Completion","abstract":"Knowledge graph completion (KGC) seeks to predict missing entities (e.g., heads or tails) or relationships in knowledge graphs (KGs), which often contain incomplete data. Traditional embedding-based methods, such as TransE and ComplEx, have improved tail entity prediction but struggle to generalize to unseen entities during testing. Textual-based models mitigate this issue by leveraging additional semantic context; however, their reliance on negative triplet sampling introduces high computational overhead, semantic inconsistencies, and data imbalance. Recent approaches, like KG-BERT, show promise but depend heavily on entity descriptions, which are often unavailable in KGs. Critically, existing methods overlook valuable structural information in the KG related to the entities and relationships. To address these challenges, we propose Multi-Context-Aware Knowledge Graph Completion (MuCo-KGC), a novel model that utilizes contextual information from linked entities and relations within the graph to predict tail entities. MuCo-KGC eliminates the need for entity descriptions and negative triplet sampling, significantly reducing computational complexity while enhancing performance. Our experiments on standard datasets, including FB15k-237, WN18RR, CoDEx-S, and CoDEx-M, demonstrate that MuCo-KGC outperforms state-of-the-art methods on three datasets. Notably, MuCo-KGC improves MRR on WN18RR, and CoDEx-S and CoDEx-M datasets by $1.63\\%$, and $3.77\\%$ and $20.15\\%$ respectively, demonstrating its effectiveness for KGC tasks.","sentences":["Knowledge graph completion (KGC) seeks to predict missing entities (e.g., heads or tails) or relationships in knowledge graphs (KGs), which often contain incomplete data.","Traditional embedding-based methods, such as TransE and ComplEx, have improved tail entity prediction but struggle to generalize to unseen entities during testing.","Textual-based models mitigate this issue by leveraging additional semantic context; however, their reliance on negative triplet sampling introduces high computational overhead, semantic inconsistencies, and data imbalance.","Recent approaches, like KG-BERT, show promise but depend heavily on entity descriptions, which are often unavailable in KGs.","Critically, existing methods overlook valuable structural information in the KG related to the entities and relationships.","To address these challenges, we propose Multi-Context-Aware Knowledge Graph Completion (MuCo-KGC), a novel model that utilizes contextual information from linked entities and relations within the graph to predict tail entities.","MuCo-KGC eliminates the need for entity descriptions and negative triplet sampling, significantly reducing computational complexity while enhancing performance.","Our experiments on standard datasets, including FB15k-237, WN18RR, CoDEx-S, and CoDEx-M, demonstrate that MuCo-KGC outperforms state-of-the-art methods on three datasets.","Notably, MuCo-KGC improves MRR on WN18RR, and CoDEx-S and CoDEx-M datasets by $1.63\\%$, and $3.77\\%$ and $20.15\\%$ respectively, demonstrating its effectiveness for KGC tasks."],"url":"http://arxiv.org/abs/2503.03091v1"}
{"created":"2025-03-05 01:04:13","title":"\"Watch My Health, Not My Data\": Understanding Perceptions, Barriers, Emotional Impact, & Coping Strategies Pertaining to IoT Privacy and Security in Health Monitoring for Older Adults","abstract":"The proliferation of \"Internet of Things (IoT)\" provides older adults with critical support for \"health monitoring\" and independent living, yet significant concerns about security and privacy persist. In this paper, we report on these issues through a two-phase user study, including a survey (N = 22) and semi-structured interviews (n = 9) with adults aged 65+. We found that while 81.82% of our participants are aware of security features like \"two-factor authentication (2FA)\" and encryption, 63.64% express serious concerns about unauthorized access to sensitive health data. Only 13.64% feel confident in existing protections, citing confusion over \"data sharing policies\" and frustration with \"complex security settings\" which lead to distrust and anxiety. To cope, our participants adopt various strategies, such as relying on family or professional support and limiting feature usage leading to disengagement. Thus, we recommend \"adaptive security mechanisms,\" simplified interfaces, and real-time transparency notifications to foster trust and ensure \"privacy and security by design\" in IoT health systems for older adults.","sentences":["The proliferation of \"Internet of Things (IoT)\" provides older adults with critical support for \"health monitoring\" and independent living, yet significant concerns about security and privacy persist.","In this paper, we report on these issues through a two-phase user study, including a survey (N = 22) and semi-structured interviews (n = 9) with adults aged 65+.","We found that while 81.82% of our participants are aware of security features like \"two-factor authentication (2FA)\" and encryption, 63.64% express serious concerns about unauthorized access to sensitive health data.","Only 13.64% feel confident in existing protections, citing confusion over \"data sharing policies\" and frustration with \"complex security settings\" which lead to distrust and anxiety.","To cope, our participants adopt various strategies, such as relying on family or professional support and limiting feature usage leading to disengagement.","Thus, we recommend \"adaptive security mechanisms,\" simplified interfaces, and real-time transparency notifications to foster trust and ensure \"privacy and security by design\" in IoT health systems for older adults."],"url":"http://arxiv.org/abs/2503.03087v1"}
{"created":"2025-03-05 00:53:22","title":"Hopfield Networks Meet Big Data: A Brain-Inspired Deep Learning Framework for Semantic Data Linking","abstract":"The exponential rise in data generation has led to vast, heterogeneous datasets crucial for predictive analytics and decision-making. Ensuring data quality and semantic integrity remains a challenge. This paper presents a brain-inspired distributed cognitive framework that integrates deep learning with Hopfield networks to identify and link semantically related attributes across datasets. Modeled on the dual-hemisphere functionality of the human brain, the right hemisphere assimilates new information while the left retrieves learned representations for association. Our architecture, implemented on MapReduce with Hadoop Distributed File System (HDFS), leverages deep Hopfield networks as an associative memory mechanism to enhance recall of frequently co-occurring attributes and dynamically adjust relationships based on evolving data patterns. Experiments show that associative imprints in Hopfield memory are reinforced over time, ensuring linked datasets remain contextually meaningful and improving data disambiguation and integration accuracy. Our results indicate that combining deep Hopfield networks with distributed cognitive processing offers a scalable, biologically inspired approach to managing complex data relationships in large-scale environments.","sentences":["The exponential rise in data generation has led to vast, heterogeneous datasets crucial for predictive analytics and decision-making.","Ensuring data quality and semantic integrity remains a challenge.","This paper presents a brain-inspired distributed cognitive framework that integrates deep learning with Hopfield networks to identify and link semantically related attributes across datasets.","Modeled on the dual-hemisphere functionality of the human brain, the right hemisphere assimilates new information while the left retrieves learned representations for association.","Our architecture, implemented on MapReduce with Hadoop Distributed File System (HDFS), leverages deep Hopfield networks as an associative memory mechanism to enhance recall of frequently co-occurring attributes and dynamically adjust relationships based on evolving data patterns.","Experiments show that associative imprints in Hopfield memory are reinforced over time, ensuring linked datasets remain contextually meaningful and improving data disambiguation and integration accuracy.","Our results indicate that combining deep Hopfield networks with distributed cognitive processing offers a scalable, biologically inspired approach to managing complex data relationships in large-scale environments."],"url":"http://arxiv.org/abs/2503.03084v1"}
{"created":"2025-03-05 00:44:12","title":"AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons","abstract":"Scaling up imitation learning for real-world applications requires efficient and cost-effective demonstration collection methods. Current teleoperation approaches, though effective, are expensive and inefficient due to the dependency on physical robot platforms. Alternative data sources like in-the-wild demonstrations can eliminate the need for physical robots and offer more scalable solutions. However, existing in-the-wild data collection devices have limitations: handheld devices offer restricted in-hand camera observation, while whole-body devices often require fine-tuning with robot data due to action inaccuracies. In this paper, we propose AirExo-2, a low-cost exoskeleton system for large-scale in-the-wild demonstration collection. By introducing the demonstration adaptor to transform the collected in-the-wild demonstrations into pseudo-robot demonstrations, our system addresses key challenges in utilizing in-the-wild demonstrations for downstream imitation learning in real-world environments. Additionally, we present RISE-2, a generalizable policy that integrates 2D and 3D perceptions, outperforming previous imitation learning policies in both in-domain and out-of-domain tasks, even with limited demonstrations. By leveraging in-the-wild demonstrations collected and transformed by the AirExo-2 system, without the need for additional robot demonstrations, RISE-2 achieves comparable or superior performance to policies trained with teleoperated data, highlighting the potential of AirExo-2 for scalable and generalizable imitation learning. Project page: https://airexo.tech/airexo2","sentences":["Scaling up imitation learning for real-world applications requires efficient and cost-effective demonstration collection methods.","Current teleoperation approaches, though effective, are expensive and inefficient due to the dependency on physical robot platforms.","Alternative data sources like in-the-wild demonstrations can eliminate the need for physical robots and offer more scalable solutions.","However, existing in-the-wild data collection devices have limitations: handheld devices offer restricted in-hand camera observation, while whole-body devices often require fine-tuning with robot data due to action inaccuracies.","In this paper, we propose AirExo-2, a low-cost exoskeleton system for large-scale in-the-wild demonstration collection.","By introducing the demonstration adaptor to transform the collected in-the-wild demonstrations into pseudo-robot demonstrations, our system addresses key challenges in utilizing in-the-wild demonstrations for downstream imitation learning in real-world environments.","Additionally, we present RISE-2, a generalizable policy that integrates 2D and 3D perceptions, outperforming previous imitation learning policies in both in-domain and out-of-domain tasks, even with limited demonstrations.","By leveraging in-the-wild demonstrations collected and transformed by the AirExo-2 system, without the need for additional robot demonstrations, RISE-2 achieves comparable or superior performance to policies trained with teleoperated data, highlighting the potential of AirExo-2 for scalable and generalizable imitation learning.","Project page: https://airexo.tech/airexo2"],"url":"http://arxiv.org/abs/2503.03081v1"}
{"created":"2025-03-05 00:37:39","title":"Sublinear Data Structures for Nearest Neighbor in Ultra High Dimensions","abstract":"Geometric data structures have been extensively studied in the regime where the dimension is much smaller than the number of input points. But in many scenarios in Machine Learning, the dimension can be much higher than the number of points and can be so high that the data structure might be unable to read and store all coordinates of the input and query points.   Inspired by these scenarios and related studies in feature selection and explainable clustering, we initiate the study of geometric data structures in this ultra-high dimensional regime. Our focus is the {\\em approximate nearest neighbor} problem.   In this problem, we are given a set of $n$ points $C\\subseteq \\mathbb{R}^d$ and have to produce a {\\em small} data structure that can {\\em quickly} answer the following query: given $q\\in \\mathbb{R}^d$, return a point $c\\in C$ that is approximately nearest to $q$.   The main question in this paper is: {\\em Is there a data structure with sublinear ($o(nd)$) space and sublinear ($o(d)$) query time when $d\\gg n$?} In this paper, we answer this question affirmatively. We present $(1+\\epsilon)$-approximation data structures with the following guarantees. For $\\ell_1$- and $\\ell_2$-norm distances: $\\tilde O(n \\log(d)/\\mathrm{poly}(\\epsilon))$ space and $\\tilde O(n/\\mathrm{poly}(\\epsilon))$ query time. We show that these space and time bounds are tight up to $\\mathrm{poly}{(\\log n/\\epsilon)}$ factors. For $\\ell_p$-norm distances: $\\tilde O(n^2 \\log(d) (\\log\\log (n)/\\epsilon)^p)$ space and $\\tilde O\\left(n(\\log\\log (n)/\\epsilon)^p\\right)$ query time.   Via simple reductions, our data structures imply sublinear-in-$d$ data structures for some other geometric problems; e.g. approximate orthogonal range search, furthest neighbor, and give rise to a sublinear $O(1)$-approximate representation of $k$-median and $k$-means clustering.","sentences":["Geometric data structures have been extensively studied in the regime where the dimension is much smaller than the number of input points.","But in many scenarios in Machine Learning, the dimension can be much higher than the number of points and can be so high that the data structure might be unable to read and store all coordinates of the input and query points.   ","Inspired by these scenarios and related studies in feature selection and explainable clustering, we initiate the study of geometric data structures in this ultra-high dimensional regime.","Our focus is the {\\em approximate nearest neighbor} problem.   ","In this problem, we are given a set of $n$ points $C\\subseteq \\mathbb{R}^d$ and have to produce a {\\em small} data structure that can {\\em quickly} answer the following query: given $q\\in \\mathbb{R}^d$, return a point $c\\in C$ that is approximately nearest to $q$.   The main question in this paper is: {\\em Is there a data structure with sublinear ($o(nd)$) space and sublinear ($o(d)$) query time when $d\\gg n$?}","In this paper, we answer this question affirmatively.","We present $(1+\\epsilon)$-approximation data structures with the following guarantees.","For $\\ell_1$- and $\\ell_2$-norm distances: $\\tilde O(n \\log(d)/\\mathrm{poly}(\\epsilon))$ space and $\\tilde O(n/\\mathrm{poly}(\\epsilon))$ query time.","We show that these space and time bounds are tight up to $\\mathrm{poly}{(\\log n/\\epsilon)}$ factors.","For $\\ell_p$-norm distances: $\\tilde O(n^2 \\log(d) (\\log\\log (n)/\\epsilon)^p)$ space and $\\tilde O\\left(n(\\log\\log (n)/\\epsilon)^p\\right)$ query time.   ","Via simple reductions, our data structures imply sublinear-in-$d$ data structures for some other geometric problems; e.g. approximate orthogonal range search, furthest neighbor, and give rise to a sublinear $O(1)$-approximate representation of $k$-median and $k$-means clustering."],"url":"http://arxiv.org/abs/2503.03079v1"}
{"created":"2025-03-05 00:21:23","title":"Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion via Trajectory Optimization and Symbolic Repair","abstract":"We propose an integrated planning framework for quadrupedal locomotion over dynamically changing, unforeseen terrains. Existing approaches either rely on heuristics for instantaneous foothold selection--compromising safety and versatility--or solve expensive trajectory optimization problems with complex terrain features and long time horizons. In contrast, our framework leverages reactive synthesis to generate correct-by-construction controllers at the symbolic level, and mixed-integer convex programming (MICP) for dynamic and physically feasible footstep planning for each symbolic transition. We use a high-level manager to reduce the large state space in synthesis by incorporating local environment information, improving synthesis scalability. To handle specifications that cannot be met due to dynamic infeasibility, and to minimize costly MICP solves, we leverage a symbolic repair process to generate only necessary symbolic transitions. During online execution, re-running the MICP with real-world terrain data, along with runtime symbolic repair, bridges the gap between offline synthesis and online execution. We demonstrate, in simulation, our framework's capabilities to discover missing locomotion skills and react promptly in safety-critical environments, such as scattered stepping stones and rebars.","sentences":["We propose an integrated planning framework for quadrupedal locomotion over dynamically changing, unforeseen terrains.","Existing approaches either rely on heuristics for instantaneous foothold selection--compromising safety and versatility--or solve expensive trajectory optimization problems with complex terrain features and long time horizons.","In contrast, our framework leverages reactive synthesis to generate correct-by-construction controllers at the symbolic level, and mixed-integer convex programming (MICP) for dynamic and physically feasible footstep planning for each symbolic transition.","We use a high-level manager to reduce the large state space in synthesis by incorporating local environment information, improving synthesis scalability.","To handle specifications that cannot be met due to dynamic infeasibility, and to minimize costly MICP solves, we leverage a symbolic repair process to generate only necessary symbolic transitions.","During online execution, re-running the MICP with real-world terrain data, along with runtime symbolic repair, bridges the gap between offline synthesis and online execution.","We demonstrate, in simulation, our framework's capabilities to discover missing locomotion skills and react promptly in safety-critical environments, such as scattered stepping stones and rebars."],"url":"http://arxiv.org/abs/2503.03071v1"}
{"created":"2025-03-05 00:16:09","title":"Multi-View Depth Consistent Image Generation Using Generative AI Models: Application on Architectural Design of University Buildings","abstract":"In the early stages of architectural design, shoebox models are typically used as a simplified representation of building structures but require extensive operations to transform them into detailed designs. Generative artificial intelligence (AI) provides a promising solution to automate this transformation, but ensuring multi-view consistency remains a significant challenge. To solve this issue, we propose a novel three-stage consistent image generation framework using generative AI models to generate architectural designs from shoebox model representations. The proposed method enhances state-of-the-art image generation diffusion models to generate multi-view consistent architectural images. We employ ControlNet as the backbone and optimize it to accommodate multi-view inputs of architectural shoebox models captured from predefined perspectives. To ensure stylistic and structural consistency across multi-view images, we propose an image space loss module that incorporates style loss, structural loss and angle alignment loss. We then use depth estimation method to extract depth maps from the generated multi-view images. Finally, we use the paired data of the architectural images and depth maps as inputs to improve the multi-view consistency via the depth-aware 3D attention module. Experimental results demonstrate that the proposed framework can generate multi-view architectural images with consistent style and structural coherence from shoebox model inputs.","sentences":["In the early stages of architectural design, shoebox models are typically used as a simplified representation of building structures but require extensive operations to transform them into detailed designs.","Generative artificial intelligence (AI) provides a promising solution to automate this transformation, but ensuring multi-view consistency remains a significant challenge.","To solve this issue, we propose a novel three-stage consistent image generation framework using generative AI models to generate architectural designs from shoebox model representations.","The proposed method enhances state-of-the-art image generation diffusion models to generate multi-view consistent architectural images.","We employ ControlNet as the backbone and optimize it to accommodate multi-view inputs of architectural shoebox models captured from predefined perspectives.","To ensure stylistic and structural consistency across multi-view images, we propose an image space loss module that incorporates style loss, structural loss and angle alignment loss.","We then use depth estimation method to extract depth maps from the generated multi-view images.","Finally, we use the paired data of the architectural images and depth maps as inputs to improve the multi-view consistency via the depth-aware 3D attention module.","Experimental results demonstrate that the proposed framework can generate multi-view architectural images with consistent style and structural coherence from shoebox model inputs."],"url":"http://arxiv.org/abs/2503.03068v1"}
{"created":"2025-03-04 23:52:49","title":"Semi-Supervised In-Context Learning: A Baseline Study","abstract":"Most existing work in data selection for In-Context Learning (ICL) has focused on constructing demonstrations from ground truth annotations, with limited attention given to selecting reliable self-generated annotations. In this work, we propose a three-step semi-supervised ICL framework: annotation generation, demonstration selection, and semi-supervised inference. Our baseline, Naive-SemiICL, which prompts select high-confidence self-generated demonstrations for ICL prompting, outperforms a 16-shot baseline by an average of 9.94% across 16 datasets. We further introduce IterPSD, an annotation approach that refines pseudo-demonstrations iteratively, achieving up to 6.8% additional gains in classification tasks. Lastly, we reveal a scaling law for semi-supervised ICL, where models achieve optimal performance with over 1,000 demonstrations.","sentences":["Most existing work in data selection for In-Context Learning (ICL) has focused on constructing demonstrations from ground truth annotations, with limited attention given to selecting reliable self-generated annotations.","In this work, we propose a three-step semi-supervised ICL framework: annotation generation, demonstration selection, and semi-supervised inference.","Our baseline, Naive-SemiICL, which prompts select high-confidence self-generated demonstrations for ICL prompting, outperforms a 16-shot baseline by an average of 9.94% across 16 datasets.","We further introduce IterPSD, an annotation approach that refines pseudo-demonstrations iteratively, achieving up to 6.8% additional gains in classification tasks.","Lastly, we reveal a scaling law for semi-supervised ICL, where models achieve optimal performance with over 1,000 demonstrations."],"url":"http://arxiv.org/abs/2503.03062v1"}
{"created":"2025-03-04 23:49:02","title":"Generating Networks to Target Assortativity via Archimedean Copula Graphons","abstract":"We develop an approach to generate random graphs to a target level of assortativity by using copula structures in graphons. Unlike existing random graph generators, we do not use rewiring or binning approaches to generate the desired random graph. Instead, we connect Archimedean bivariate copulas to graphons in order to produce flexible models that can generate random graphs to target assortativity. We propose three models that use the copula distribution function, copula density function and their mixed tensor product to produce networks. We express the assortativity coefficient in terms of homomorphism densities. Establishing this relationship forges a connection between the parameter of the copula and the frequency of subgraphs in the generated network. Therefore, our method attains a desired the subgraph distribution as well as the target assortativity. We establish the homomorphism densities and assortativity coefficient for each of the models. Numerical examples demonstrate the ability of the proposed models to produce graphs with different levels of assortativity.","sentences":["We develop an approach to generate random graphs to a target level of assortativity by using copula structures in graphons.","Unlike existing random graph generators, we do not use rewiring or binning approaches to generate the desired random graph.","Instead, we connect Archimedean bivariate copulas to graphons in order to produce flexible models that can generate random graphs to target assortativity.","We propose three models that use the copula distribution function, copula density function and their mixed tensor product to produce networks.","We express the assortativity coefficient in terms of homomorphism densities.","Establishing this relationship forges a connection between the parameter of the copula and the frequency of subgraphs in the generated network.","Therefore, our method attains a desired the subgraph distribution as well as the target assortativity.","We establish the homomorphism densities and assortativity coefficient for each of the models.","Numerical examples demonstrate the ability of the proposed models to produce graphs with different levels of assortativity."],"url":"http://arxiv.org/abs/2503.03061v1"}
{"created":"2025-03-04 23:41:02","title":"A2Perf: Real-World Autonomous Agents Benchmark","abstract":"Autonomous agents and systems cover a number of application areas, from robotics and digital assistants to combinatorial optimization, all sharing common, unresolved research challenges. It is not sufficient for agents to merely solve a given task; they must generalize to out-of-distribution tasks, perform reliably, and use hardware resources efficiently during training and inference, among other requirements. Several methods, such as reinforcement learning and imitation learning, are commonly used to tackle these problems, each with different trade-offs. However, there is a lack of benchmarking suites that define the environments, datasets, and metrics which can be used to provide a meaningful way for the community to compare progress on applying these methods to real-world problems. We introduce A2Perf--a benchmark with three environments that closely resemble real-world domains: computer chip floorplanning, web navigation, and quadruped locomotion. A2Perf provides metrics that track task performance, generalization, system resource efficiency, and reliability, which are all critical to real-world applications. Using A2Perf, we demonstrate that web navigation agents can achieve latencies comparable to human reaction times on consumer hardware, reveal reliability trade-offs between algorithms for quadruped locomotion, and quantify the energy costs of different learning approaches for computer chip-design. In addition, we propose a data cost metric to account for the cost incurred acquiring offline data for imitation learning and hybrid algorithms, which allows us to better compare these approaches. A2Perf also contains several standard baselines, enabling apples-to-apples comparisons across methods and facilitating progress in real-world autonomy. As an open-source benchmark, A2Perf is designed to remain accessible, up-to-date, and useful to the research community over the long term.","sentences":["Autonomous agents and systems cover a number of application areas, from robotics and digital assistants to combinatorial optimization, all sharing common, unresolved research challenges.","It is not sufficient for agents to merely solve a given task; they must generalize to out-of-distribution tasks, perform reliably, and use hardware resources efficiently during training and inference, among other requirements.","Several methods, such as reinforcement learning and imitation learning, are commonly used to tackle these problems, each with different trade-offs.","However, there is a lack of benchmarking suites that define the environments, datasets, and metrics which can be used to provide a meaningful way for the community to compare progress on applying these methods to real-world problems.","We introduce A2Perf--a benchmark with three environments that closely resemble real-world domains: computer chip floorplanning, web navigation, and quadruped locomotion.","A2Perf provides metrics that track task performance, generalization, system resource efficiency, and reliability, which are all critical to real-world applications.","Using A2Perf, we demonstrate that web navigation agents can achieve latencies comparable to human reaction times on consumer hardware, reveal reliability trade-offs between algorithms for quadruped locomotion, and quantify the energy costs of different learning approaches for computer chip-design.","In addition, we propose a data cost metric to account for the cost incurred acquiring offline data for imitation learning and hybrid algorithms, which allows us to better compare these approaches.","A2Perf also contains several standard baselines, enabling apples-to-apples comparisons across methods and facilitating progress in real-world autonomy.","As an open-source benchmark, A2Perf is designed to remain accessible, up-to-date, and useful to the research community over the long term."],"url":"http://arxiv.org/abs/2503.03056v1"}
{"created":"2025-03-04 22:49:59","title":"Leveraging Randomness in Model and Data Partitioning for Privacy Amplification","abstract":"We study how inherent randomness in the training process -- where each sample (or client in federated learning) contributes only to a randomly selected portion of training -- can be leveraged for privacy amplification. This includes (1) data partitioning, where a sample participates in only a subset of training iterations, and (2) model partitioning, where a sample updates only a subset of the model parameters. We apply our framework to model parallelism in federated learning, where each client updates a randomly selected subnetwork to reduce memory and computational overhead, and show that existing methods, e.g. model splitting or dropout, provide a significant privacy amplification gain not captured by previous privacy analysis techniques. Additionally, we introduce Balanced Iteration Subsampling, a new data partitioning method where each sample (or client) participates in a fixed number of training iterations. We show that this method yields stronger privacy amplification than Poisson (i.i.d.) sampling of data (or clients). Our results demonstrate that randomness in the training process, which is structured rather than i.i.d. and interacts with data in complex ways, can be systematically leveraged for significant privacy amplification.","sentences":["We study how inherent randomness in the training process -- where each sample (or client in federated learning) contributes only to a randomly selected portion of training -- can be leveraged for privacy amplification.","This includes (1) data partitioning, where a sample participates in only a subset of training iterations, and (2) model partitioning, where a sample updates only a subset of the model parameters.","We apply our framework to model parallelism in federated learning, where each client updates a randomly selected subnetwork to reduce memory and computational overhead, and show that existing methods, e.g. model splitting or dropout, provide a significant privacy amplification gain not captured by previous privacy analysis techniques.","Additionally, we introduce Balanced Iteration Subsampling, a new data partitioning method where each sample (or client) participates in a fixed number of training iterations.","We show that this method yields stronger privacy amplification than Poisson (i.i.d.) sampling of data (or clients).","Our results demonstrate that randomness in the training process, which is structured rather than i.i.d. and interacts with data in complex ways, can be systematically leveraged for significant privacy amplification."],"url":"http://arxiv.org/abs/2503.03043v1"}
{"created":"2025-03-04 22:48:43","title":"Learning from Noisy Labels with Contrastive Co-Transformer","abstract":"Deep learning with noisy labels is an interesting challenge in weakly supervised learning. Despite their significant learning capacity, CNNs have a tendency to overfit in the presence of samples with noisy labels. Alleviating this issue, the well known Co-Training framework is used as a fundamental basis for our work. In this paper, we introduce a Contrastive Co-Transformer framework, which is simple and fast, yet able to improve the performance by a large margin compared to the state-of-the-art approaches. We argue the robustness of transformers when dealing with label noise. Our Contrastive Co-Transformer approach is able to utilize all samples in the dataset, irrespective of whether they are clean or noisy. Transformers are trained by a combination of contrastive loss and classification loss. Extensive experimental results on corrupted data from six standard benchmark datasets including Clothing1M, demonstrate that our Contrastive Co-Transformer is superior to existing state-of-the-art methods.","sentences":["Deep learning with noisy labels is an interesting challenge in weakly supervised learning.","Despite their significant learning capacity, CNNs have a tendency to overfit in the presence of samples with noisy labels.","Alleviating this issue, the well known Co-Training framework is used as a fundamental basis for our work.","In this paper, we introduce a Contrastive Co-Transformer framework, which is simple and fast, yet able to improve the performance by a large margin compared to the state-of-the-art approaches.","We argue the robustness of transformers when dealing with label noise.","Our Contrastive Co-Transformer approach is able to utilize all samples in the dataset, irrespective of whether they are clean or noisy.","Transformers are trained by a combination of contrastive loss and classification loss.","Extensive experimental results on corrupted data from six standard benchmark datasets including Clothing1M, demonstrate that our Contrastive Co-Transformer is superior to existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2503.03042v1"}
