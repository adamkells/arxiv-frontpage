{"created":"2023-12-26 18:56:49","title":"Social-Transmotion: Promptable Human Trajectory Prediction","abstract":"Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space. To address this, we introduce Social-Transmotion, a generic model that exploits the power of transformers to handle diverse and numerous visual cues, capturing the multi-modal nature of human behavior. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or body poses. This, in turn, augments trajectory data, leading to enhanced human trajectory prediction. Our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between pedestrians based on the available visual cues, whether they are poses, bounding boxes, or a combination thereof. By the masking technique, we ensure our model's effectiveness even when certain visual cues are unavailable, although performance is further boosted with the presence of comprehensive visual data. We delve into the merits of using 2d versus 3d poses, and a limited set of poses. Additionally, we investigate the spatial and temporal attention map to identify which keypoints and frames of poses are vital for optimizing human trajectory prediction. Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY. The code is publicly available: https://github.com/vita-epfl/social-transmotion","sentences":["Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems.","Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space.","To address this, we introduce Social-Transmotion, a generic model that exploits the power of transformers to handle diverse and numerous visual cues, capturing the multi-modal nature of human behavior.","We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or body poses.","This, in turn, augments trajectory data, leading to enhanced human trajectory prediction.","Our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between pedestrians based on the available visual cues, whether they are poses, bounding boxes, or a combination thereof.","By the masking technique, we ensure our model's effectiveness even when certain visual cues are unavailable, although performance is further boosted with the presence of comprehensive visual data.","We delve into the merits of using 2d versus 3d poses, and a limited set of poses.","Additionally, we investigate the spatial and temporal attention map to identify which keypoints and frames of poses are vital for optimizing human trajectory prediction.","Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY.","The code is publicly available: https://github.com/vita-epfl/social-transmotion"],"url":"http://arxiv.org/abs/2312.16168v1"}
{"created":"2023-12-26 18:36:01","title":"Association rule mining with earthquake data collected from Turkiye region","abstract":"Earthquakes are evaluated among the most destructive disasters for human beings, as also experienced for Turkiye region. Data science has the property of discovering hidden patterns in case a sufficient volume of data is supplied. Time dependency of events, specifically being defined by co-occurrence in a specific time window, may be handled as an associate rule mining task such as a market-basket analysis application. In this regard, we assumed each day's seismic activity as a single basket of events, leading to discovering the association patterns between these events. Consequently, this study presents the most prominent association rules for the earthquakes recorded in Turkiye region in the last 5 years, each year presented separately. Results indicate statistical inference with events recorded from regions of various distances, which could be further verified with geologic evidence from the field. As a result, we believe that the current study may form a statistical basis for the future works with the aid of machine learning algorithm performed for associate rule mining.","sentences":["Earthquakes are evaluated among the most destructive disasters for human beings, as also experienced for Turkiye region.","Data science has the property of discovering hidden patterns in case a sufficient volume of data is supplied.","Time dependency of events, specifically being defined by co-occurrence in a specific time window, may be handled as an associate rule mining task such as a market-basket analysis application.","In this regard, we assumed each day's seismic activity as a single basket of events, leading to discovering the association patterns between these events.","Consequently, this study presents the most prominent association rules for the earthquakes recorded in Turkiye region in the last 5 years, each year presented separately.","Results indicate statistical inference with events recorded from regions of various distances, which could be further verified with geologic evidence from the field.","As a result, we believe that the current study may form a statistical basis for the future works with the aid of machine learning algorithm performed for associate rule mining."],"url":"http://arxiv.org/abs/2312.16158v1"}
{"created":"2023-12-26 18:07:05","title":"JaColBERT and Hard Negatives, Towards Better Japanese-First Embeddings for Retrieval: Early Technical Report","abstract":"Document retrieval in many languages has been largely relying on multi-lingual models, and leveraging the vast wealth of English training data. In Japanese, the best performing deep-learning based retrieval approaches rely on multilingual dense embeddings. In this work, we introduce (1) a hard-negative augmented version of the Japanese MMARCO dataset and (2) JaColBERT, a document retrieval model built on the ColBERT model architecture, specifically for Japanese. JaColBERT vastly outperform all previous monolingual retrieval approaches and competes with the best multilingual methods, despite unfavourable evaluation settings (out-of-domain vs. in-domain for the multilingual models). JaColBERT reaches an average Recall@10 of 0.813, noticeably ahead of the previous monolingual best-performing model (0.716) and only slightly behind multilingual-e5-base (0.820), though more noticeably behind multilingual-e5-large (0.856). These results are achieved using only a limited, entirely Japanese, training set, more than two orders of magnitudes smaller than multilingual embedding models. We believe these results show great promise to support retrieval-enhanced application pipelines in a wide variety of domains.","sentences":["Document retrieval in many languages has been largely relying on multi-lingual models, and leveraging the vast wealth of English training data.","In Japanese, the best performing deep-learning based retrieval approaches rely on multilingual dense embeddings.","In this work, we introduce (1) a hard-negative augmented version of the Japanese MMARCO dataset and (2) JaColBERT, a document retrieval model built on the ColBERT model architecture, specifically for Japanese.","JaColBERT vastly outperform all previous monolingual retrieval approaches and competes with the best multilingual methods, despite unfavourable evaluation settings (out-of-domain vs. in-domain for the multilingual models).","JaColBERT reaches an average Recall@10 of 0.813, noticeably ahead of the previous monolingual best-performing model (0.716) and only slightly behind multilingual-e5-base (0.820), though more noticeably behind multilingual-e5-large (0.856).","These results are achieved using only a limited, entirely Japanese, training set, more than two orders of magnitudes smaller than multilingual embedding models.","We believe these results show great promise to support retrieval-enhanced application pipelines in a wide variety of domains."],"url":"http://arxiv.org/abs/2312.16144v1"}
{"created":"2023-12-26 18:04:49","title":"A Bayesian Framework of Deep Reinforcement Learning for Joint O-RAN/MEC Orchestration","abstract":"Multi-access Edge Computing (MEC) can be implemented together with Open Radio Access Network (O-RAN) over commodity platforms to offer low-cost deployment and bring the services closer to end-users. In this paper, a joint O-RAN/MEC orchestration using a Bayesian deep reinforcement learning (RL)-based framework is proposed that jointly controls the O-RAN functional splits, the allocated resources and hosting locations of the O-RAN/MEC services across geo-distributed platforms, and the routing for each O-RAN/MEC data flow. The goal is to minimize the long-term overall network operation cost and maximize the MEC performance criterion while adapting possibly time-varying O-RAN/MEC demands and resource availability. This orchestration problem is formulated as Markov decision process (MDP). However, the system consists of multiple BSs that share the same resources and serve heterogeneous demands, where their parameters have non-trivial relations. Consequently, finding the exact model of the underlying system is impractical, and the formulated MDP renders in a large state space with multi-dimensional discrete action. To address such modeling and dimensionality issues, a novel model-free RL agent is proposed for our solution framework. The agent is built from Double Deep Q-network (DDQN) that tackles the large state space and is then incorporated with action branching, an action decomposition method that effectively addresses the multi-dimensional discrete action with linear increase complexity. Further, an efficient exploration-exploitation strategy under a Bayesian framework using Thomson sampling is proposed to improve the learning performance and expedite its convergence. Trace-driven simulations are performed using an O-RAN-compliant model. The results show that our approach is data-efficient (i.e., converges faster) and increases the returned reward by 32\\% than its non-Bayesian version.","sentences":["Multi-access Edge Computing (MEC) can be implemented together with Open Radio Access Network (O-RAN) over commodity platforms to offer low-cost deployment and bring the services closer to end-users.","In this paper, a joint O-RAN/MEC orchestration using a Bayesian deep reinforcement learning (RL)-based framework is proposed that jointly controls the O-RAN functional splits, the allocated resources and hosting locations of the O-RAN/MEC services across geo-distributed platforms, and the routing for each O-RAN/MEC data flow.","The goal is to minimize the long-term overall network operation cost and maximize the MEC performance criterion while adapting possibly time-varying O-RAN/MEC demands and resource availability.","This orchestration problem is formulated as Markov decision process (MDP).","However, the system consists of multiple BSs that share the same resources and serve heterogeneous demands, where their parameters have non-trivial relations.","Consequently, finding the exact model of the underlying system is impractical, and the formulated MDP renders in a large state space with multi-dimensional discrete action.","To address such modeling and dimensionality issues, a novel model-free RL agent is proposed for our solution framework.","The agent is built from Double Deep Q-network (DDQN) that tackles the large state space and is then incorporated with action branching, an action decomposition method that effectively addresses the multi-dimensional discrete action with linear increase complexity.","Further, an efficient exploration-exploitation strategy under a Bayesian framework using Thomson sampling is proposed to improve the learning performance and expedite its convergence.","Trace-driven simulations are performed using an O-RAN-compliant model.","The results show that our approach is data-efficient (i.e., converges faster) and increases the returned reward by 32\\% than its non-Bayesian version."],"url":"http://arxiv.org/abs/2312.16142v1"}
{"created":"2023-12-26 18:03:05","title":"VirtualPainting: Addressing Sparsity with Virtual Points and Distance-Aware Data Augmentation for 3D Object Detection","abstract":"In recent times, there has been a notable surge in multimodal approaches that decorates raw LiDAR point clouds with camera-derived features to improve object detection performance. However, we found that these methods still grapple with the inherent sparsity of LiDAR point cloud data, primarily because fewer points are enriched with camera-derived features for sparsely distributed objects. We present an innovative approach that involves the generation of virtual LiDAR points using camera images and enhancing these virtual points with semantic labels obtained from image-based segmentation networks to tackle this issue and facilitate the detection of sparsely distributed objects, particularly those that are occluded or distant. Furthermore, we integrate a distance aware data augmentation (DADA) technique to enhance the models capability to recognize these sparsely distributed objects by generating specialized training samples. Our approach offers a versatile solution that can be seamlessly integrated into various 3D frameworks and 2D semantic segmentation methods, resulting in significantly improved overall detection accuracy. Evaluation on the KITTI and nuScenes datasets demonstrates substantial enhancements in both 3D and birds eye view (BEV) detection benchmarks","sentences":["In recent times, there has been a notable surge in multimodal approaches that decorates raw LiDAR point clouds with camera-derived features to improve object detection performance.","However, we found that these methods still grapple with the inherent sparsity of LiDAR point cloud data, primarily because fewer points are enriched with camera-derived features for sparsely distributed objects.","We present an innovative approach that involves the generation of virtual LiDAR points using camera images and enhancing these virtual points with semantic labels obtained from image-based segmentation networks to tackle this issue and facilitate the detection of sparsely distributed objects, particularly those that are occluded or distant.","Furthermore, we integrate a distance aware data augmentation (DADA) technique to enhance the models capability to recognize these sparsely distributed objects by generating specialized training samples.","Our approach offers a versatile solution that can be seamlessly integrated into various 3D frameworks and 2D semantic segmentation methods, resulting in significantly improved overall detection accuracy.","Evaluation on the KITTI and nuScenes datasets demonstrates substantial enhancements in both 3D and birds eye view (BEV) detection benchmarks"],"url":"http://arxiv.org/abs/2312.16141v1"}
{"created":"2023-12-26 17:19:09","title":"Large Language Model Situational Awareness Based Planning","abstract":"This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models. We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks. Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances. Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning. By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning for situated agents.","sentences":["This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models.","We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks.","Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances.","Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning.","By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning for situated agents."],"url":"http://arxiv.org/abs/2312.16127v1"}
{"created":"2023-12-26 14:43:26","title":"Event-based Shape from Polarization with Spiking Neural Networks","abstract":"Recent advances in event-based shape determination from polarization offer a transformative approach that tackles the trade-off between speed and accuracy in capturing surface geometries. In this paper, we investigate event-based shape from polarization using Spiking Neural Networks (SNNs), introducing the Single-Timestep and Multi-Timestep Spiking UNets for effective and efficient surface normal estimation. Specificially, the Single-Timestep model processes event-based shape as a non-temporal task, updating the membrane potential of each spiking neuron only once, thereby reducing computational and energy demands. In contrast, the Multi-Timestep model exploits temporal dynamics for enhanced data extraction. Extensive evaluations on synthetic and real-world datasets demonstrate that our models match the performance of state-of-the-art Artifical Neural Networks (ANNs) in estimating surface normals, with the added advantage of superior energy efficiency. Our work not only contributes to the advancement of SNNs in event-based sensing but also sets the stage for future explorations in optimizing SNN architectures, integrating multi-modal data, and scaling for applications on neuromorphic hardware.","sentences":["Recent advances in event-based shape determination from polarization offer a transformative approach that tackles the trade-off between speed and accuracy in capturing surface geometries.","In this paper, we investigate event-based shape from polarization using Spiking Neural Networks (SNNs), introducing the Single-Timestep and Multi-Timestep Spiking UNets for effective and efficient surface normal estimation.","Specificially, the Single-Timestep model processes event-based shape as a non-temporal task, updating the membrane potential of each spiking neuron only once, thereby reducing computational and energy demands.","In contrast, the Multi-Timestep model exploits temporal dynamics for enhanced data extraction.","Extensive evaluations on synthetic and real-world datasets demonstrate that our models match the performance of state-of-the-art Artifical Neural Networks (ANNs) in estimating surface normals, with the added advantage of superior energy efficiency.","Our work not only contributes to the advancement of SNNs in event-based sensing but also sets the stage for future explorations in optimizing SNN architectures, integrating multi-modal data, and scaling for applications on neuromorphic hardware."],"url":"http://arxiv.org/abs/2312.16071v1"}
{"created":"2023-12-26 14:15:19","title":"Error-free Training for Artificial Neural Network","abstract":"Conventional training methods for artificial neural network (ANN) models never achieve zero error rate systematically for large data. A new training method consists of three steps: first create an auxiliary data from conventionally trained parameters which correspond exactly to a global minimum for the loss function of the cloned data; second create a one-parameter homotopy (hybrid) of the auxiliary data and the original data; and third train the model for the hybrid data iteratively from the auxiliary data end of the homotopy parameter to the original data end while maintaining the zero-error training rate at every iteration. This continuationmethod is guaranteed to converge numerically by a theorem which converts the ANN training problem into a continuation problem for fixed points of a parameterized transformation in the training parameter space to which the Uniform Contraction Mapping Theorem from dynamical systems applies.","sentences":["Conventional training methods for artificial neural network (ANN) models never achieve zero error rate systematically for large data.","A new training method consists of three steps: first create an auxiliary data from conventionally trained parameters which correspond exactly to a global minimum for the loss function of the cloned data; second create a one-parameter homotopy (hybrid) of the auxiliary data and the original data; and third train the model for the hybrid data iteratively from the auxiliary data end of the homotopy parameter to the original data end while maintaining the zero-error training rate at every iteration.","This continuationmethod is guaranteed to converge numerically by a theorem which converts the ANN training problem into a continuation problem for fixed points of a parameterized transformation in the training parameter space to which the Uniform Contraction Mapping Theorem from dynamical systems applies."],"url":"http://arxiv.org/abs/2312.16060v1"}
{"created":"2023-12-26 13:54:00","title":"A Logically Consistent Chain-of-Thought Approach for Stance Detection","abstract":"Zero-shot stance detection (ZSSD) aims to detect stances toward unseen targets. Incorporating background knowledge to enhance transferability between seen and unseen targets constitutes the primary approach of ZSSD. However, these methods often struggle with a knowledge-task disconnect and lack logical consistency in their predictions. To address these issues, we introduce a novel approach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which improves stance detection by ensuring relevant and logically sound knowledge extraction. LC-CoT employs a three-step process. Initially, it assesses whether supplementary external knowledge is necessary. Subsequently, it uses API calls to retrieve this knowledge, which can be processed by a separate LLM. Finally, a manual exemplar guides the LLM to infer stance categories, using an if-then logical structure to maintain relevance and logical coherence. This structured approach to eliciting background knowledge enhances the model's capability, outperforming traditional supervised methods without relying on labeled data.","sentences":["Zero-shot stance detection (ZSSD) aims to detect stances toward unseen targets.","Incorporating background knowledge to enhance transferability between seen and unseen targets constitutes the primary approach of ZSSD.","However, these methods often struggle with a knowledge-task disconnect and lack logical consistency in their predictions.","To address these issues, we introduce a novel approach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which improves stance detection by ensuring relevant and logically sound knowledge extraction.","LC-CoT employs a three-step process.","Initially, it assesses whether supplementary external knowledge is necessary.","Subsequently, it uses API calls to retrieve this knowledge, which can be processed by a separate LLM.","Finally, a manual exemplar guides the LLM to infer stance categories, using an if-then logical structure to maintain relevance and logical coherence.","This structured approach to eliciting background knowledge enhances the model's capability, outperforming traditional supervised methods without relying on labeled data."],"url":"http://arxiv.org/abs/2312.16054v1"}
{"created":"2023-12-26 13:23:03","title":"AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts","abstract":"Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated. Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor. Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy. In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas. Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training. Validation experiments have been performed under the cases of \\emph{None}, \\emph{Light}, \\emph{Moderate}, \\emph{Heavy} and \\emph{Violent} on a large-scale precipitation benchmark named TIGGE. Finally, the average mean-absolute error (MAE) and average root-mean-square error (RMSE) of the proposed AdaNAS model are 0.98 and 2.04 mm/day, respectively. Additionally, the proposed AdaNAS model is compared with other neural architecture search methods and previous studies. Compared results reveal the satisfactory performance and superiority of the proposed AdaNAS model in terms of precipitation amount prediction and intensity classification. Concretely, the proposed AdaNAS model outperformed previous best-performing manual methods with MAE and RMSE improving by 80.5\\% and 80.3\\%, respectively.","sentences":["Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated.","Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor.","Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy.","In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas.","Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training.","Validation experiments have been performed under the cases of \\emph{None}, \\emph{Light}, \\emph{Moderate}, \\emph{Heavy} and \\emph{Violent} on a large-scale precipitation benchmark named TIGGE.","Finally, the average mean-absolute error (MAE) and average root-mean-square error (RMSE) of the proposed AdaNAS model are 0.98 and 2.04 mm/day, respectively.","Additionally, the proposed AdaNAS model is compared with other neural architecture search methods and previous studies.","Compared results reveal the satisfactory performance and superiority of the proposed AdaNAS model in terms of precipitation amount prediction and intensity classification.","Concretely, the proposed AdaNAS model outperformed previous best-performing manual methods with MAE and RMSE improving by 80.5\\% and 80.3\\%, respectively."],"url":"http://arxiv.org/abs/2312.16046v1"}
{"created":"2023-12-26 12:56:31","title":"Dual-scale Enhanced and Cross-generative Consistency Learning for Semi-supervised Polyp Segmentation","abstract":"Automatic polyp segmentation plays a crucial role in the early diagnosis and treatment of colorectal cancer (CRC). However, existing methods heavily rely on fully supervised training, which requires a large amount of labeled data with time-consuming pixel-wise annotations. Moreover, accurately segmenting polyps poses challenges due to variations in shape, size, and location. To address these issues, we propose a novel Dual-scale Enhanced and Cross-generative consistency learning framework for semi-supervised polyp Segmentation (DEC-Seg) from colonoscopy images. First, we propose a Cross-level Feature Aggregation (CFA) module that integrates cross-level adjacent layers to enhance the feature representation ability across different resolutions. To address scale variation, we present a scale-enhanced consistency constraint, which ensures consistency in the segmentation maps generated from the same input image at different scales. This constraint helps handle variations in polyp sizes and improves the robustness of the model. Additionally, we design a scale-aware perturbation consistency scheme to enhance the robustness of the mean teacher model. Furthermore, we propose a cross-generative consistency scheme, in which the original and perturbed images can be reconstructed using cross-segmentation maps. This consistency constraint allows us to mine effective feature representations and boost the segmentation performance. To produce more accurate segmentation maps, we propose a Dual-scale Complementary Fusion (DCF) module that integrates features from two scale-specific decoders operating at different scales. Extensive experimental results on five benchmark datasets demonstrate the effectiveness of our DEC-Seg against other state-of-the-art semi-supervised segmentation approaches. The implementation code will be released at https://github.com/taozh2017/DECSeg.","sentences":["Automatic polyp segmentation plays a crucial role in the early diagnosis and treatment of colorectal cancer (CRC).","However, existing methods heavily rely on fully supervised training, which requires a large amount of labeled data with time-consuming pixel-wise annotations.","Moreover, accurately segmenting polyps poses challenges due to variations in shape, size, and location.","To address these issues, we propose a novel Dual-scale Enhanced and Cross-generative consistency learning framework for semi-supervised polyp Segmentation (DEC-Seg) from colonoscopy images.","First, we propose a Cross-level Feature Aggregation (CFA) module that integrates cross-level adjacent layers to enhance the feature representation ability across different resolutions.","To address scale variation, we present a scale-enhanced consistency constraint, which ensures consistency in the segmentation maps generated from the same input image at different scales.","This constraint helps handle variations in polyp sizes and improves the robustness of the model.","Additionally, we design a scale-aware perturbation consistency scheme to enhance the robustness of the mean teacher model.","Furthermore, we propose a cross-generative consistency scheme, in which the original and perturbed images can be reconstructed using cross-segmentation maps.","This consistency constraint allows us to mine effective feature representations and boost the segmentation performance.","To produce more accurate segmentation maps, we propose a Dual-scale Complementary Fusion (DCF) module that integrates features from two scale-specific decoders operating at different scales.","Extensive experimental results on five benchmark datasets demonstrate the effectiveness of our DEC-Seg against other state-of-the-art semi-supervised segmentation approaches.","The implementation code will be released at https://github.com/taozh2017/DECSeg."],"url":"http://arxiv.org/abs/2312.16039v1"}
{"created":"2023-12-26 12:55:32","title":"Critical nonlinear aspects of hopping transport for reconfigurable logic in disordered dopant networks","abstract":"Nonlinear behavior in the hopping transport of interacting charges enables reconfigurable logic in disordered dopant network devices, where voltages applied at control electrodes tune the relation between voltages applied at input electrodes and the current measured at an output electrode. From kinetic Monte Carlo simulations we analyze the critical nonlinear aspects of variable-range hopping transport for realizing Boolean logic gates in these devices on three levels. First, we quantify the occurrence of individual gates for random choices of control voltages. We find that linearly inseparable gates such as the XOR gate are less likely to occur than linearly separable gates such as the AND gate, despite the fact that the number of different regions in the multidimensional control voltage space for which AND or XOR gates occur is comparable. Second, we use principal component analysis to characterize the distribution of the output current vectors for the (00,10,01,11) logic input combinations in terms of eigenvectors and eigenvalues of the output covariance matrix. This allows a simple and direct comparison of the behavior of different simulated devices and a comparison to experimental devices. Third, we quantify the nonlinearity in the distribution of the output current vectors necessary for realizing Boolean functionality by introducing three nonlinearity indicators. The analysis provides a physical interpretation of the effects of changing the hopping distance and temperature and is used in a comparison with data generated by a deep neural network trained on a physical device.","sentences":["Nonlinear behavior in the hopping transport of interacting charges enables reconfigurable logic in disordered dopant network devices, where voltages applied at control electrodes tune the relation between voltages applied at input electrodes and the current measured at an output electrode.","From kinetic Monte Carlo simulations we analyze the critical nonlinear aspects of variable-range hopping transport for realizing Boolean logic gates in these devices on three levels.","First, we quantify the occurrence of individual gates for random choices of control voltages.","We find that linearly inseparable gates such as the XOR gate are less likely to occur than linearly separable gates such as the AND gate, despite the fact that the number of different regions in the multidimensional control voltage space for which AND or XOR gates occur is comparable.","Second, we use principal component analysis to characterize the distribution of the output current vectors for the (00,10,01,11) logic input combinations in terms of eigenvectors and eigenvalues of the output covariance matrix.","This allows a simple and direct comparison of the behavior of different simulated devices and a comparison to experimental devices.","Third, we quantify the nonlinearity in the distribution of the output current vectors necessary for realizing Boolean functionality by introducing three nonlinearity indicators.","The analysis provides a physical interpretation of the effects of changing the hopping distance and temperature and is used in a comparison with data generated by a deep neural network trained on a physical device."],"url":"http://arxiv.org/abs/2312.16037v1"}
{"created":"2023-12-26 12:53:57","title":"Ensemble Learning to Assess Dynamics of Affective Experience Ratings and Physiological Change","abstract":"The congruence between affective experiences and physiological changes has been a debated topic for centuries. Recent technological advances in measurement and data analysis provide hope to solve this epic challenge. Open science and open data practices, together with data analysis challenges open to the academic community, are also promising tools for solving this problem. In this entry to the Emotion Physiology and Experience Collaboration (EPiC) challenge, we propose a data analysis solution that combines theoretical assumptions with data-driven methodologies. We used feature engineering and ensemble selection. Each predictor was trained on subsets of the training data that would maximize the information available for training. Late fusion was used with an averaging step. We chose to average considering a ``wisdom of crowds'' strategy. This strategy yielded an overall RMSE of 1.19 in the test set. Future work should carefully explore if our assumptions are correct and the potential of weighted fusion.","sentences":["The congruence between affective experiences and physiological changes has been a debated topic for centuries.","Recent technological advances in measurement and data analysis provide hope to solve this epic challenge.","Open science and open data practices, together with data analysis challenges open to the academic community, are also promising tools for solving this problem.","In this entry to the Emotion Physiology and Experience Collaboration (EPiC) challenge, we propose a data analysis solution that combines theoretical assumptions with data-driven methodologies.","We used feature engineering and ensemble selection.","Each predictor was trained on subsets of the training data that would maximize the information available for training.","Late fusion was used with an averaging step.","We chose to average considering a ``wisdom of crowds'' strategy.","This strategy yielded an overall RMSE of 1.19 in the test set.","Future work should carefully explore if our assumptions are correct and the potential of weighted fusion."],"url":"http://arxiv.org/abs/2312.16036v1"}
{"created":"2023-12-26 12:49:25","title":"On the Hardness of Minimum Embedded Order Dependency Validation","abstract":"Order Dependencies (ODs) have many applications, such as query optimization, data integration, and data cleaning. Although many works addressed the problem of discovering OD (and its variants), they do not consider datasets with missing values, a standard observation in real-world datasets. This paper introduces the novel notion of Embedded ODs (eODs) to deal with missing values. The intuition of eODs is to confirm ODs only on tuples with no missing values on a given embedding (a set of attributes). In this paper, we address the problem of validating a given eOD. If the eOD holds, we return true. Otherwise, we search for an updated embedding such that the updated eOD holds. If such embedding does not exist, we return false. A trivial requirement is to consider an embedding such that the number of ignored tuples is minimized. We show that it is NP-complete to compute such embedding. We therefore propose an efficient heuristic algorithm for validating embedded ODs. We conduct experiments on real-world datasets, and the results confirm the efficiency of our algorithm.","sentences":["Order Dependencies (ODs) have many applications, such as query optimization, data integration, and data cleaning.","Although many works addressed the problem of discovering OD (and its variants), they do not consider datasets with missing values, a standard observation in real-world datasets.","This paper introduces the novel notion of Embedded ODs (eODs) to deal with missing values.","The intuition of eODs is to confirm ODs only on tuples with no missing values on a given embedding (a set of attributes).","In this paper, we address the problem of validating a given eOD.","If the eOD holds, we return true.","Otherwise, we search for an updated embedding such that the updated eOD holds.","If such embedding does not exist, we return false.","A trivial requirement is to consider an embedding such that the number of ignored tuples is minimized.","We show that it is NP-complete to compute such embedding.","We therefore propose an efficient heuristic algorithm for validating embedded ODs.","We conduct experiments on real-world datasets, and the results confirm the efficiency of our algorithm."],"url":"http://arxiv.org/abs/2312.16033v1"}
{"created":"2023-12-26 12:12:58","title":"RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems. Numerous studies have employed specialized \\textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs. For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation. Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations. Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance. Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.   In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \\textbf{Ranker} for top-\\textit{k} \\textbf{Rec}ommendations. Specifically, we introduce importance-aware sampling, clustering-based sampling, and penalty for repetitive sampling for sampling high-quality, representative, and diverse training data. To enhance the prompt, we introduce position shifting strategy to mitigate position bias and augment the prompt with auxiliary information from conventional recommendation models, thereby enriching the contextual understanding of the LLM. Subsequently, we utilize the sampled data to assemble an instruction-tuning dataset with the augmented prompt comprising three distinct ranking tasks: pointwise, pairwise, and listwise rankings. We further propose a hybrid ranking method to enhance the model performance by ensembling these ranking tasks. Our empirical evaluations demonstrate the effectiveness of our proposed RecRanker in both direct and sequential recommendation scenarios.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems.","Numerous studies have employed specialized \\textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs.","For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation.","Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations.","Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance.","Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.   ","In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \\textbf{Ranker} for top-\\textit{k} \\textbf{Rec}ommendations.","Specifically, we introduce importance-aware sampling, clustering-based sampling, and penalty for repetitive sampling for sampling high-quality, representative, and diverse training data.","To enhance the prompt, we introduce position shifting strategy to mitigate position bias and augment the prompt with auxiliary information from conventional recommendation models, thereby enriching the contextual understanding of the LLM.","Subsequently, we utilize the sampled data to assemble an instruction-tuning dataset with the augmented prompt comprising three distinct ranking tasks: pointwise, pairwise, and listwise rankings.","We further propose a hybrid ranking method to enhance the model performance by ensembling these ranking tasks.","Our empirical evaluations demonstrate the effectiveness of our proposed RecRanker in both direct and sequential recommendation scenarios."],"url":"http://arxiv.org/abs/2312.16018v1"}
{"created":"2023-12-26 12:00:24","title":"V-STRONG: Visual Self-Supervised Traversability Learning for Off-road Navigation","abstract":"Reliable estimation of terrain traversability is critical for the successful deployment of autonomous systems in wild, outdoor environments. Given the lack of large-scale annotated datasets for off-road navigation, strictly-supervised learning approaches remain limited in their generalization ability. To this end, we introduce a novel, image-based self-supervised learning method for traversability prediction, leveraging a state-of-the-art vision foundation model for improved out-of-distribution performance. Our method employs contrastive representation learning using both human driving data and instance-based segmentation masks during training. We show that this simple, yet effective, technique drastically outperforms recent methods in predicting traversability for both on- and off-trail driving scenarios. We compare our method with recent baselines on both a common benchmark as well as our own datasets, covering a diverse range of outdoor environments and varied terrain types. We also demonstrate the compatibility of resulting costmap predictions with a model-predictive controller. Finally, we evaluate our approach on zero- and few-shot tasks, demonstrating unprecedented performance for generalization to new environments. Videos and additional material can be found here: \\url{https://sites.google.com/view/visual-traversability-learning}.","sentences":["Reliable estimation of terrain traversability is critical for the successful deployment of autonomous systems in wild, outdoor environments.","Given the lack of large-scale annotated datasets for off-road navigation, strictly-supervised learning approaches remain limited in their generalization ability.","To this end, we introduce a novel, image-based self-supervised learning method for traversability prediction, leveraging a state-of-the-art vision foundation model for improved out-of-distribution performance.","Our method employs contrastive representation learning using both human driving data and instance-based segmentation masks during training.","We show that this simple, yet effective, technique drastically outperforms recent methods in predicting traversability for both on- and off-trail driving scenarios.","We compare our method with recent baselines on both a common benchmark as well as our own datasets, covering a diverse range of outdoor environments and varied terrain types.","We also demonstrate the compatibility of resulting costmap predictions with a model-predictive controller.","Finally, we evaluate our approach on zero- and few-shot tasks, demonstrating unprecedented performance for generalization to new environments.","Videos and additional material can be found here: \\url{https://sites.google.com/view/visual-traversability-learning}."],"url":"http://arxiv.org/abs/2312.16016v1"}
{"created":"2023-12-26 11:38:36","title":"Achieving Fairness in DareFightingICE Agents Evaluation Through a Delay Mechanism","abstract":"This paper proposes a delay mechanism to mitigate the impact of latency differences in the gRPC framework--a high-performance, open-source universal remote procedure call (RPC) framework--between different programming languages on the performance of agents in DareFightingICE, a fighting game research platform. The study finds that gRPC latency differences between Java and Python can significantly impact real-time decision-making. Without a delay mechanism, Java-based agents outperform Python-based ones due to lower gRPC latency on the Java platform. However, with the proposed delay mechanism, both Java-based and Python-based agents exhibit similar performance, leading to a fair comparison between agents developed using different programming languages. Thus, this work underscores the crucial importance of considering gRPC latency when developing and evaluating agents in DareFightingICE, and the insights gained could potentially extend to other gRPC-based applications.","sentences":["This paper proposes a delay mechanism to mitigate the impact of latency differences in the gRPC framework--a high-performance, open-source universal remote procedure call (RPC) framework--between different programming languages on the performance of agents in DareFightingICE, a fighting game research platform.","The study finds that gRPC latency differences between Java and Python can significantly impact real-time decision-making.","Without a delay mechanism, Java-based agents outperform Python-based ones due to lower gRPC latency on the Java platform.","However, with the proposed delay mechanism, both Java-based and Python-based agents exhibit similar performance, leading to a fair comparison between agents developed using different programming languages.","Thus, this work underscores the crucial importance of considering gRPC latency when developing and evaluating agents in DareFightingICE, and the insights gained could potentially extend to other gRPC-based applications."],"url":"http://arxiv.org/abs/2312.16010v1"}
{"created":"2023-12-26 11:26:44","title":"A fully decentralized auditing approach for edge computing: A Game-Theoretic Perspective","abstract":"Edge storage presents a viable data storage alternative for application vendors (AV), offering benefits such as reduced bandwidth overhead and latency compared to cloud storage. However, data cached in edge computing systems is susceptible to intentional or accidental disturbances. This paper proposes a decentralized integrity auditing scheme to safeguard data integrity and counter the traditional reliance on centralized third-party auditors (TPA), which are unfit for distributed systems. Our novel approach employs edge servers (ES) as mutual auditors, eliminating the need for a centralized entity. This decentralization minimizes potential collusion with malicious auditors and biases in audit outcomes. Using a strategic game model, we demonstrate that ESs are more motivated to audit each other than TPAs. The auditing process is addressed as a Nash Equilibrium problem, assuring accurate integrity proof through incentives for ESs. Our scheme's security and performance are rigorously assessed, showing it is secure within the random oracle model, offers improved speed, and is cost-effective compared to existing methods.","sentences":["Edge storage presents a viable data storage alternative for application vendors (AV), offering benefits such as reduced bandwidth overhead and latency compared to cloud storage.","However, data cached in edge computing systems is susceptible to intentional or accidental disturbances.","This paper proposes a decentralized integrity auditing scheme to safeguard data integrity and counter the traditional reliance on centralized third-party auditors (TPA), which are unfit for distributed systems.","Our novel approach employs edge servers (ES) as mutual auditors, eliminating the need for a centralized entity.","This decentralization minimizes potential collusion with malicious auditors and biases in audit outcomes.","Using a strategic game model, we demonstrate that ESs are more motivated to audit each other than TPAs.","The auditing process is addressed as a Nash Equilibrium problem, assuring accurate integrity proof through incentives for ESs.","Our scheme's security and performance are rigorously assessed, showing it is secure within the random oracle model, offers improved speed, and is cost-effective compared to existing methods."],"url":"http://arxiv.org/abs/2312.16007v1"}
{"created":"2023-12-26 10:54:15","title":"Practical Bias Mitigation through Proxy Sensitive Attribute Label Generation","abstract":"Addressing bias in the trained machine learning system often requires access to sensitive attributes. In practice, these attributes are not available either due to legal and policy regulations or data unavailability for a given demographic. Existing bias mitigation algorithms are limited in their applicability to real-world scenarios as they require access to sensitive attributes to achieve fairness. In this research work, we aim to address this bottleneck through our proposed unsupervised proxy-sensitive attribute label generation technique. Towards this end, we propose a two-stage approach of unsupervised embedding generation followed by clustering to obtain proxy-sensitive labels. The efficacy of our work relies on the assumption that bias propagates through non-sensitive attributes that are correlated to the sensitive attributes and, when mapped to the high dimensional latent space, produces clusters of different demographic groups that exist in the data. Experimental results demonstrate that bias mitigation using existing algorithms such as Fair Mixup and Adversarial Debiasing yields comparable results on derived proxy labels when compared against using true sensitive attributes.","sentences":["Addressing bias in the trained machine learning system often requires access to sensitive attributes.","In practice, these attributes are not available either due to legal and policy regulations or data unavailability for a given demographic.","Existing bias mitigation algorithms are limited in their applicability to real-world scenarios as they require access to sensitive attributes to achieve fairness.","In this research work, we aim to address this bottleneck through our proposed unsupervised proxy-sensitive attribute label generation technique.","Towards this end, we propose a two-stage approach of unsupervised embedding generation followed by clustering to obtain proxy-sensitive labels.","The efficacy of our work relies on the assumption that bias propagates through non-sensitive attributes that are correlated to the sensitive attributes and, when mapped to the high dimensional latent space, produces clusters of different demographic groups that exist in the data.","Experimental results demonstrate that bias mitigation using existing algorithms such as Fair Mixup and Adversarial Debiasing yields comparable results on derived proxy labels when compared against using true sensitive attributes."],"url":"http://arxiv.org/abs/2312.15994v1"}
{"created":"2023-12-26 10:30:05","title":"Discrete Messages Improve Communication Efficiency among Isolated Intelligent Agents","abstract":"Individuals, despite having varied life experiences and learning processes, can communicate effectively through languages. This study aims to explore the efficiency of language as a communication medium. We put forth two specific hypotheses: First, discrete messages are more effective than continuous ones when agents have diverse personal experiences. Second, communications using multiple discrete tokens are more advantageous than those using a single token. To valdate these hypotheses, we designed multi-agent machine learning experiments to assess communication efficiency using various information transmission methods between speakers and listeners. Our empirical findings indicate that, in scenarios where agents are exposed to different data, communicating through sentences composed of discrete tokens offers the best inter-agent communication efficiency. The limitations of our finding include lack of systematic advantages over other more sophisticated encoder-decoder model such as variational autoencoder and lack of evluation on non-image dataset, which we will leave for future studies.","sentences":["Individuals, despite having varied life experiences and learning processes, can communicate effectively through languages.","This study aims to explore the efficiency of language as a communication medium.","We put forth two specific hypotheses:","First, discrete messages are more effective than continuous ones when agents have diverse personal experiences.","Second, communications using multiple discrete tokens are more advantageous than those using a single token.","To valdate these hypotheses, we designed multi-agent machine learning experiments to assess communication efficiency using various information transmission methods between speakers and listeners.","Our empirical findings indicate that, in scenarios where agents are exposed to different data, communicating through sentences composed of discrete tokens offers the best inter-agent communication efficiency.","The limitations of our finding include lack of systematic advantages over other more sophisticated encoder-decoder model such as variational autoencoder and lack of evluation on non-image dataset, which we will leave for future studies."],"url":"http://arxiv.org/abs/2312.15985v1"}
{"created":"2023-12-26 09:24:19","title":"Federated Hyperdimensional Computing","abstract":"Federated learning (FL) enables a loose set of participating clients to collaboratively learn a global model via coordination by a central server and with no need for data sharing. Existing FL approaches that rely on complex algorithms with massive models, such as deep neural networks (DNNs), suffer from computation and communication bottlenecks. In this paper, we first propose FedHDC, a federated learning framework based on hyperdimensional computing (HDC). FedHDC allows for fast and light-weight local training on clients, provides robust learning, and has smaller model communication overhead compared to learning with DNNs. However, current HDC algorithms get poor accuracy when classifying larger & more complex images, such as CIFAR10. To address this issue, we design FHDnn, which complements FedHDC with a self-supervised contrastive learning feature extractor. We avoid the transmission of the DNN and instead train only the HDC learner in a federated manner, which accelerates learning, reduces transmission cost, and utilizes the robustness of HDC to tackle network errors. We present a formal analysis of the algorithm and derive its convergence rate both theoretically, and show experimentally that FHDnn converges 3$\\times$ faster vs. DNNs. The strategies we propose to improve the communication efficiency enable our design to reduce communication costs by 66$\\times$ vs. DNNs, local client compute and energy consumption by ~1.5 - 6$\\times$, while being highly robust to network errors. Finally, our proposed strategies for improving the communication efficiency have up to 32$\\times$ lower communication costs with good accuracy.","sentences":["Federated learning (FL) enables a loose set of participating clients to collaboratively learn a global model via coordination by a central server and with no need for data sharing.","Existing FL approaches that rely on complex algorithms with massive models, such as deep neural networks (DNNs), suffer from computation and communication bottlenecks.","In this paper, we first propose FedHDC, a federated learning framework based on hyperdimensional computing (HDC).","FedHDC allows for fast and light-weight local training on clients, provides robust learning, and has smaller model communication overhead compared to learning with DNNs.","However, current HDC algorithms get poor accuracy when classifying larger & more complex images, such as CIFAR10.","To address this issue, we design FHDnn, which complements FedHDC with a self-supervised contrastive learning feature extractor.","We avoid the transmission of the DNN and instead train only the HDC learner in a federated manner, which accelerates learning, reduces transmission cost, and utilizes the robustness of HDC to tackle network errors.","We present a formal analysis of the algorithm and derive its convergence rate both theoretically, and show experimentally that FHDnn converges 3$\\times$ faster vs. DNNs.","The strategies we propose to improve the communication efficiency enable our design to reduce communication costs by 66$\\times$ vs. DNNs, local client compute and energy consumption by ~1.5 - 6$\\times$, while being highly robust to network errors.","Finally, our proposed strategies for improving the communication efficiency have up to 32$\\times$ lower communication costs with good accuracy."],"url":"http://arxiv.org/abs/2312.15966v1"}
{"created":"2023-12-26 08:47:24","title":"Range Entropy Queries and Partitioning","abstract":"Data partitioning that maximizes or minimizes Shannon entropy is a crucial subroutine in data compression, columnar storage, and cardinality estimation algorithms. These partition algorithms can be accelerated if we have a data structure to find the entropy in different subsets of data when the algorithm needs to decide what block to construct. While it is generally known how to compute the entropy of a discrete distribution efficiently, we want to efficiently derive the entropy among the data items that lie in a specific area. We solve this problem in a typical setting when we deal with real data, where data items are geometric points and each requested area is a query (hyper)rectangle. More specifically, we consider a set $P$ of $n$ weighted and colored points in $\\mathbb{R}^d$. The goal is to construct a low space data structure, such that given a query (hyper)rectangle $R$, it computes the entropy based on the colors of the points in $P\\cap R$, in sublinear time. We show a conditional lower bound for this problem proving that we cannot hope for data structures with near-linear space and near-constant query time. Then, we propose exact data structures for $d=1$ and $d>1$ with $o(n^{2d})$ space and $o(n)$ query time. We also provide a tune parameter $t$ that the user can choose to bound the asymptotic space and query time of the new data structures. Next, we propose near linear space data structures for returning either an additive or a multiplicative approximation of the entropy. Finally, we show how we can use the new data structures to efficiently partition time series and histograms with respect to entropy.","sentences":["Data partitioning that maximizes or minimizes Shannon entropy is a crucial subroutine in data compression, columnar storage, and cardinality estimation algorithms.","These partition algorithms can be accelerated if we have a data structure to find the entropy in different subsets of data when the algorithm needs to decide what block to construct.","While it is generally known how to compute the entropy of a discrete distribution efficiently, we want to efficiently derive the entropy among the data items that lie in a specific area.","We solve this problem in a typical setting when we deal with real data, where data items are geometric points and each requested area is a query (hyper)rectangle.","More specifically, we consider a set $P$ of $n$ weighted and colored points in $\\mathbb{R}^d$. The goal is to construct a low space data structure, such that given a query (hyper)rectangle $R$, it computes the entropy based on the colors of the points in $P\\cap R$, in sublinear time.","We show a conditional lower bound for this problem proving that we cannot hope for data structures with near-linear space and near-constant query time.","Then, we propose exact data structures for $d=1$ and $d>1$ with $o(n^{2d})$ space and $o(n)$ query time.","We also provide a tune parameter $t$ that the user can choose to bound the asymptotic space and query time of the new data structures.","Next, we propose near linear space data structures for returning either an additive or a multiplicative approximation of the entropy.","Finally, we show how we can use the new data structures to efficiently partition time series and histograms with respect to entropy."],"url":"http://arxiv.org/abs/2312.15959v1"}
{"created":"2023-12-26 08:19:10","title":"EnchantDance: Unveiling the Potential of Music-Driven Dance Movement","abstract":"The task of music-driven dance generation involves creating coherent dance movements that correspond to the given music. While existing methods can produce physically plausible dances, they often struggle to generalize to out-of-set data. The challenge arises from three aspects: 1) the high diversity of dance movements and significant differences in the distribution of music modalities, which make it difficult to generate music-aligned dance movements. 2) the lack of a large-scale music-dance dataset, which hinders the generation of generalized dance movements from music. 3) The protracted nature of dance movements poses a challenge to the maintenance of a consistent dance style. In this work, we introduce the EnchantDance framework, a state-of-the-art method for dance generation. Due to the redundancy of the original dance sequence along the time axis, EnchantDance first constructs a strong dance latent space and then trains a dance diffusion model on the dance latent space. To address the data gap, we construct a large-scale music-dance dataset, ChoreoSpectrum3D Dataset, which includes four dance genres and has a total duration of 70.32 hours, making it the largest reported music-dance dataset to date. To enhance consistency between music genre and dance style, we pre-train a music genre prediction network using transfer learning and incorporate music genre as extra conditional information in the training of the dance diffusion model. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance on dance quality, diversity, and consistency.","sentences":["The task of music-driven dance generation involves creating coherent dance movements that correspond to the given music.","While existing methods can produce physically plausible dances, they often struggle to generalize to out-of-set data.","The challenge arises from three aspects: 1) the high diversity of dance movements and significant differences in the distribution of music modalities, which make it difficult to generate music-aligned dance movements.","2) the lack of a large-scale music-dance dataset, which hinders the generation of generalized dance movements from music.","3)","The protracted nature of dance movements poses a challenge to the maintenance of a consistent dance style.","In this work, we introduce the EnchantDance framework, a state-of-the-art method for dance generation.","Due to the redundancy of the original dance sequence along the time axis, EnchantDance first constructs a strong dance latent space and then trains a dance diffusion model on the dance latent space.","To address the data gap, we construct a large-scale music-dance dataset, ChoreoSpectrum3D Dataset, which includes four dance genres and has a total duration of 70.32 hours, making it the largest reported music-dance dataset to date.","To enhance consistency between music genre and dance style, we pre-train a music genre prediction network using transfer learning and incorporate music genre as extra conditional information in the training of the dance diffusion model.","Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance on dance quality, diversity, and consistency."],"url":"http://arxiv.org/abs/2312.15946v1"}
{"created":"2023-12-26 08:14:46","title":"BAL: Balancing Diversity and Novelty for Active Learning","abstract":"The objective of Active Learning is to strategically label a subset of the dataset to maximize performance within a predetermined labeling budget. In this study, we harness features acquired through self-supervised learning. We introduce a straightforward yet potent metric, Cluster Distance Difference, to identify diverse data. Subsequently, we introduce a novel framework, Balancing Active Learning (BAL), which constructs adaptive sub-pools to balance diverse and uncertain data. Our approach outperforms all established active learning methods on widely recognized benchmarks by 1.20%. Moreover, we assess the efficacy of our proposed framework under extended settings, encompassing both larger and smaller labeling budgets. Experimental results demonstrate that, when labeling 80% of the samples, the performance of the current SOTA method declines by 0.74%, whereas our proposed BAL achieves performance comparable to the full dataset. Codes are available at https://github.com/JulietLJY/BAL.","sentences":["The objective of Active Learning is to strategically label a subset of the dataset to maximize performance within a predetermined labeling budget.","In this study, we harness features acquired through self-supervised learning.","We introduce a straightforward yet potent metric, Cluster Distance Difference, to identify diverse data.","Subsequently, we introduce a novel framework, Balancing Active Learning (BAL), which constructs adaptive sub-pools to balance diverse and uncertain data.","Our approach outperforms all established active learning methods on widely recognized benchmarks by 1.20%.","Moreover, we assess the efficacy of our proposed framework under extended settings, encompassing both larger and smaller labeling budgets.","Experimental results demonstrate that, when labeling 80% of the samples, the performance of the current SOTA method declines by 0.74%, whereas our proposed BAL achieves performance comparable to the full dataset.","Codes are available at https://github.com/JulietLJY/BAL."],"url":"http://arxiv.org/abs/2312.15944v1"}
{"created":"2023-12-26 08:10:22","title":"Pano-NeRF: Synthesizing High Dynamic Range Novel Views with Geometry from Sparse Low Dynamic Range Panoramic Images","abstract":"Panoramic imaging research on geometry recovery and High Dynamic Range (HDR) reconstruction becomes a trend with the development of Extended Reality (XR). Neural Radiance Fields (NeRF) provide a promising scene representation for both tasks without requiring extensive prior data. However, in the case of inputting sparse Low Dynamic Range (LDR) panoramic images, NeRF often degrades with under-constrained geometry and is unable to reconstruct HDR radiance from LDR inputs. We observe that the radiance from each pixel in panoramic images can be modeled as both a signal to convey scene lighting information and a light source to illuminate other pixels. Hence, we propose the irradiance fields from sparse LDR panoramic images, which increases the observation counts for faithful geometry recovery and leverages the irradiance-radiance attenuation for HDR reconstruction. Extensive experiments demonstrate that the irradiance fields outperform state-of-the-art methods on both geometry recovery and HDR reconstruction and validate their effectiveness. Furthermore, we show a promising byproduct of spatially-varying lighting estimation. The code is available at https://github.com/Lu-Zhan/Pano-NeRF.","sentences":["Panoramic imaging research on geometry recovery and High Dynamic Range (HDR) reconstruction becomes a trend with the development of Extended Reality (XR).","Neural Radiance Fields (NeRF) provide a promising scene representation for both tasks without requiring extensive prior data.","However, in the case of inputting sparse Low Dynamic Range (LDR) panoramic images, NeRF often degrades with under-constrained geometry and is unable to reconstruct HDR radiance from LDR inputs.","We observe that the radiance from each pixel in panoramic images can be modeled as both a signal to convey scene lighting information and a light source to illuminate other pixels.","Hence, we propose the irradiance fields from sparse LDR panoramic images, which increases the observation counts for faithful geometry recovery and leverages the irradiance-radiance attenuation for HDR reconstruction.","Extensive experiments demonstrate that the irradiance fields outperform state-of-the-art methods on both geometry recovery and HDR reconstruction and validate their effectiveness.","Furthermore, we show a promising byproduct of spatially-varying lighting estimation.","The code is available at https://github.com/Lu-Zhan/Pano-NeRF."],"url":"http://arxiv.org/abs/2312.15942v1"}
{"created":"2023-12-26 07:45:32","title":"ECHO: Efficient Dataset Condensation by Higher-Order Distribution Alignment","abstract":"In the era of deep learning, training deep neural networks often requires extensive data, leading to substantial costs. Dataset condensation addresses this by learning a small synthetic set that preserves essential information from the original large-scale dataset. Nowadays, optimization-oriented methods dominate dataset condensation for state-of-the-art (SOTA) results, but their computationally intensive bi-level optimization hinders practicality with large datasets. To enhance efficiency, as alternative solutions, Distribution-Matching (DM)-based methods reduce costs by aligning the representation distributions of real and synthetic examples. However, current DM-based methods still yield less comparable results to SOTA optimization-oriented methods. In this paper, we argue that existing DM-based methods overlook the higher-order alignment of the distributions, which may lead to sub-optimal matching results. Inspired by this, we propose a new DM-based method named as Efficient Dataset Condensation by Higher-Order Distribution Alignment (ECHO). Specifically, rather than only aligning the first-order moment of the representation distributions as previous methods, we learn synthetic examples via further aligning the higher-order moments of the representation distributions of real and synthetic examples based on the classical theory of reproducing kernel Hilbert space. Experiments demonstrate the proposed method achieves a significant performance boost while maintaining efficiency across various scenarios.","sentences":["In the era of deep learning, training deep neural networks often requires extensive data, leading to substantial costs.","Dataset condensation addresses this by learning a small synthetic set that preserves essential information from the original large-scale dataset.","Nowadays, optimization-oriented methods dominate dataset condensation for state-of-the-art (SOTA) results, but their computationally intensive bi-level optimization hinders practicality with large datasets.","To enhance efficiency, as alternative solutions, Distribution-Matching (DM)-based methods reduce costs by aligning the representation distributions of real and synthetic examples.","However, current DM-based methods still yield less comparable results to SOTA optimization-oriented methods.","In this paper, we argue that existing DM-based methods overlook the higher-order alignment of the distributions, which may lead to sub-optimal matching results.","Inspired by this, we propose a new DM-based method named as Efficient Dataset Condensation by Higher-Order Distribution Alignment (ECHO).","Specifically, rather than only aligning the first-order moment of the representation distributions as previous methods, we learn synthetic examples via further aligning the higher-order moments of the representation distributions of real and synthetic examples based on the classical theory of reproducing kernel Hilbert space.","Experiments demonstrate the proposed method achieves a significant performance boost while maintaining efficiency across various scenarios."],"url":"http://arxiv.org/abs/2312.15927v1"}
{"created":"2023-12-26 07:40:26","title":"FedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models","abstract":"Foundation models have shown great success in natural language processing, computer vision, and multimodal tasks. FMs have a large number of model parameters, thus requiring a substantial amount of data to help optimize the model during the training. Federated learning has revolutionized machine learning by enabling collaborative learning from decentralized data while still preserving the data privacy of clients. Despite the great benefits foundation models can have empowered by federated learning, they face severe computation, communication, and statistical challenges. In this paper, we propose a novel two-stage federated learning algorithm called FedMS. A global expert is trained in the first stage and a local expert is trained in the second stage to provide better personalization. We construct a Mixture of Foundation Models (MoFM) with these two experts and design a gate neural network with an inserted gate adapter that joins the aggregation every communication round in the second stage. To further adapt to edge computing scenarios with limited computational resources, we design a novel Sparsely Activated LoRA (SAL) algorithm that freezes the pre-trained foundation model parameters inserts low-rank adaptation matrices into transformer blocks and activates them progressively during the training. We employ extensive experiments to verify the effectiveness of FedMS, results show that FedMS outperforms other SOTA baselines by up to 55.25% in default settings.","sentences":["Foundation models have shown great success in natural language processing, computer vision, and multimodal tasks.","FMs have a large number of model parameters, thus requiring a substantial amount of data to help optimize the model during the training.","Federated learning has revolutionized machine learning by enabling collaborative learning from decentralized data while still preserving the data privacy of clients.","Despite the great benefits foundation models can have empowered by federated learning, they face severe computation, communication, and statistical challenges.","In this paper, we propose a novel two-stage federated learning algorithm called FedMS.","A global expert is trained in the first stage and a local expert is trained in the second stage to provide better personalization.","We construct a Mixture of Foundation Models (MoFM) with these two experts and design a gate neural network with an inserted gate adapter that joins the aggregation every communication round in the second stage.","To further adapt to edge computing scenarios with limited computational resources, we design a novel Sparsely Activated LoRA (SAL) algorithm that freezes the pre-trained foundation model parameters inserts low-rank adaptation matrices into transformer blocks and activates them progressively during the training.","We employ extensive experiments to verify the effectiveness of FedMS, results show that FedMS outperforms other SOTA baselines by up to 55.25% in default settings."],"url":"http://arxiv.org/abs/2312.15926v1"}
{"created":"2023-12-26 07:35:02","title":"Revealing the Proximate Long-Tail Distribution in Compositional Zero-Shot Learning","abstract":"Compositional Zero-Shot Learning (CZSL) aims to transfer knowledge from seen state-object pairs to novel unseen pairs. In this process, visual bias caused by the diverse interrelationship of state-object combinations blurs their visual features, hindering the learning of distinguishable class prototypes. Prevailing methods concentrate on disentangling states and objects directly from visual features, disregarding potential enhancements that could arise from a data viewpoint. Experimentally, we unveil the results caused by the above problem closely approximate the long-tailed distribution. As a solution, we transform CZSL into a proximate class imbalance problem. We mathematically deduce the role of class prior within the long-tailed distribution in CZSL. Building upon this insight, we incorporate visual bias caused by compositions into the classifier's training and inference by estimating it as a proximate class prior. This enhancement encourages the classifier to acquire more discernible class prototypes for each composition, thereby achieving more balanced predictions. Experimental results demonstrate that our approach elevates the model's performance to the state-of-the-art level, without introducing additional parameters. Our code is available at \\url{https://github.com/LanchJL/ProLT-CZSL}.","sentences":["Compositional Zero-Shot Learning (CZSL) aims to transfer knowledge from seen state-object pairs to novel unseen pairs.","In this process, visual bias caused by the diverse interrelationship of state-object combinations blurs their visual features, hindering the learning of distinguishable class prototypes.","Prevailing methods concentrate on disentangling states and objects directly from visual features, disregarding potential enhancements that could arise from a data viewpoint.","Experimentally, we unveil the results caused by the above problem closely approximate the long-tailed distribution.","As a solution, we transform CZSL into a proximate class imbalance problem.","We mathematically deduce the role of class prior within the long-tailed distribution in CZSL.","Building upon this insight, we incorporate visual bias caused by compositions into the classifier's training and inference by estimating it as a proximate class prior.","This enhancement encourages the classifier to acquire more discernible class prototypes for each composition, thereby achieving more balanced predictions.","Experimental results demonstrate that our approach elevates the model's performance to the state-of-the-art level, without introducing additional parameters.","Our code is available at \\url{https://github.com/LanchJL/ProLT-CZSL}."],"url":"http://arxiv.org/abs/2312.15923v1"}
{"created":"2023-12-26 07:25:12","title":"Review on Causality Detection Based on Empirical Dynamic Modeling","abstract":"In contemporary scientific research, understanding the distinction between correlation and causation is crucial. While correlation is a widely used analytical standard, it does not inherently imply causation. This paper addresses the potential for misinterpretation in relying solely on correlation, especially in the context of nonlinear dynamics. Despite the rapid development of various correlation research methodologies, including machine learning, the exploration into mining causal correlations between variables remains ongoing. Empirical Dynamic Modeling (EDM) emerges as a data-driven framework for modeling dynamic systems, distinguishing itself by eschewing traditional formulaic methods in data analysis. Instead, it reconstructs dynamic system behavior directly from time series data. The fundamental premise of EDM is that dynamic systems can be conceptualized as processes where a set of states, governed by specific rules, evolve over time in a high-dimensional space. By reconstructing these evolving states, dynamic systems can be effectively modeled. Using EDM, this paper explores the detection of causal relationships between variables within dynamic systems through their time series data. It posits that if variable X causes variable Y, then the information about X is inherent in Y and can be extracted from Y's data. This study begins by examining the dialectical relationship between correlation and causation, emphasizing that correlation does not equate to causation, and the absence of correlation does not necessarily indicate a lack of causation.","sentences":["In contemporary scientific research, understanding the distinction between correlation and causation is crucial.","While correlation is a widely used analytical standard, it does not inherently imply causation.","This paper addresses the potential for misinterpretation in relying solely on correlation, especially in the context of nonlinear dynamics.","Despite the rapid development of various correlation research methodologies, including machine learning, the exploration into mining causal correlations between variables remains ongoing.","Empirical Dynamic Modeling (EDM) emerges as a data-driven framework for modeling dynamic systems, distinguishing itself by eschewing traditional formulaic methods in data analysis.","Instead, it reconstructs dynamic system behavior directly from time series data.","The fundamental premise of EDM is that dynamic systems can be conceptualized as processes where a set of states, governed by specific rules, evolve over time in a high-dimensional space.","By reconstructing these evolving states, dynamic systems can be effectively modeled.","Using EDM, this paper explores the detection of causal relationships between variables within dynamic systems through their time series data.","It posits that if variable X causes variable Y, then the information about X is inherent in Y and can be extracted from Y's data.","This study begins by examining the dialectical relationship between correlation and causation, emphasizing that correlation does not equate to causation, and the absence of correlation does not necessarily indicate a lack of causation."],"url":"http://arxiv.org/abs/2312.15919v1"}
{"created":"2023-12-26 07:24:46","title":"Supervised Knowledge Makes Large Language Models Better In-context Learners","abstract":"Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.","sentences":["Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering.","The recent progress in large-scale generative models has further expanded their use in real-world language applications.","However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored.","While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage.","Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks.","Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality.","We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks.","Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs."],"url":"http://arxiv.org/abs/2312.15918v1"}
{"created":"2023-12-26 07:20:55","title":"ChartBench: A Benchmark for Complex Visual Reasoning in Charts","abstract":"Multimodal Large Language Models (MLLMs) have demonstrated remarkable multimodal understanding and generation capabilities. However, their understanding of synthetic charts is limited, while existing benchmarks are simplistic and the charts deviate significantly from real-world examples, making it challenging to accurately assess MLLMs' chart comprehension abilities. Hence, a challenging benchmark is essential for investigating progress and uncovering the limitations of current MLLMs on chart data. In this work, we propose to examine chart comprehension through more complex visual logic and introduce ChartBench, a comprehensive chart benchmark to accurately measure MLLMs' fundamental chart comprehension and data reliability. Specifically, ChartBench consists of \\textbf{41} categories, \\textbf{2K} charts, and \\textbf{16K} QA annotations. While significantly expanding chart types, ChartBench avoids direct labelling of data points, which requires MLLMs to infer values akin to humans by leveraging elements like color, legends, and coordinate systems. We also introduce an improved metric, \\textit{Acc+}, which accurately reflects MLLMs' chart comprehension abilities while avoiding labor-intensive manual evaluations or costly GPT-based evaluations. We conduct evaluations on \\textbf{12} mainstream open-source models and \\textbf{2} outstanding proprietary models. Through extensive experiments, we reveal the limitations of MLLMs on charts and provide insights to inspire the community to pay closer attention to MLLMs' chart comprehension abilities. The benchmark and code will be publicly available for research.","sentences":["Multimodal Large Language Models (MLLMs) have demonstrated remarkable multimodal understanding and generation capabilities.","However, their understanding of synthetic charts is limited, while existing benchmarks are simplistic and the charts deviate significantly from real-world examples, making it challenging to accurately assess MLLMs' chart comprehension abilities.","Hence, a challenging benchmark is essential for investigating progress and uncovering the limitations of current MLLMs on chart data.","In this work, we propose to examine chart comprehension through more complex visual logic and introduce ChartBench, a comprehensive chart benchmark to accurately measure MLLMs' fundamental chart comprehension and data reliability.","Specifically, ChartBench consists of \\textbf{41} categories, \\textbf{2K} charts, and \\textbf{16K} QA annotations.","While significantly expanding chart types, ChartBench avoids direct labelling of data points, which requires MLLMs to infer values akin to humans by leveraging elements like color, legends, and coordinate systems.","We also introduce an improved metric, \\textit{Acc+}, which accurately reflects MLLMs' chart comprehension abilities while avoiding labor-intensive manual evaluations or costly GPT-based evaluations.","We conduct evaluations on \\textbf{12} mainstream open-source models and \\textbf{2} outstanding proprietary models.","Through extensive experiments, we reveal the limitations of MLLMs on charts and provide insights to inspire the community to pay closer attention to MLLMs' chart comprehension abilities.","The benchmark and code will be publicly available for research."],"url":"http://arxiv.org/abs/2312.15915v1"}
{"created":"2023-12-26 07:04:39","title":"Reinforcement Unlearning","abstract":"Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \\emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes for environments; 2) how to avoid degrading the agent's performance in remaining environments; and 3) how to evaluate the effectiveness of unlearning. To tackle these challenges, we propose two reinforcement unlearning methods. The first method is based on decremental reinforcement learning, which aims to erase the agent's previously acquired knowledge gradually. The second method leverages environment poisoning attacks, which encourage the agent to learn new, albeit incorrect, knowledge to remove the unlearning environment. Particularly, to tackle the third challenge, we introduce the concept of ``environment inference attack'' to evaluate the unlearning outcomes. The source code is available at \\url{https://anonymous.4open.science/r/Reinforcement-Unlearning-D347}.","sentences":["Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners.","However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning.","Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards.","During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy.","As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \\emph{reinforcement unlearning}.","Reinforcement unlearning focuses on revoking entire environments rather than individual data samples.","This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes for environments; 2) how to avoid degrading the agent's performance in remaining environments; and 3) how to evaluate the effectiveness of unlearning.","To tackle these challenges, we propose two reinforcement unlearning methods.","The first method is based on decremental reinforcement learning, which aims to erase the agent's previously acquired knowledge gradually.","The second method leverages environment poisoning attacks, which encourage the agent to learn new, albeit incorrect, knowledge to remove the unlearning environment.","Particularly, to tackle the third challenge, we introduce the concept of ``environment inference attack'' to evaluate the unlearning outcomes.","The source code is available at \\url{https://anonymous.4open.science/r/Reinforcement-Unlearning-D347}."],"url":"http://arxiv.org/abs/2312.15910v1"}
{"created":"2023-12-26 07:02:12","title":"Generalizable Task Representation Learning for Offline Meta-Reinforcement Learning with Data Limitations","abstract":"Generalization and sample efficiency have been long-standing issues concerning reinforcement learning, and thus the field of Offline Meta-Reinforcement Learning~(OMRL) has gained increasing attention due to its potential of solving a wide range of problems with static and limited offline data. Existing OMRL methods often assume sufficient training tasks and data coverage to apply contrastive learning to extract task representations. However, such assumptions are not applicable in several real-world applications and thus undermine the generalization ability of the representations. In this paper, we consider OMRL with two types of data limitations: limited training tasks and limited behavior diversity and propose a novel algorithm called GENTLE for learning generalizable task representations in the face of data limitations. GENTLE employs Task Auto-Encoder~(TAE), which is an encoder-decoder architecture to extract the characteristics of the tasks. Unlike existing methods, TAE is optimized solely by reconstruction of the state transition and reward, which captures the generative structure of the task models and produces generalizable representations when training tasks are limited. To alleviate the effect of limited behavior diversity, we consistently construct pseudo-transitions to align the data distribution used to train TAE with the data distribution encountered during testing. Empirically, GENTLE significantly outperforms existing OMRL methods on both in-distribution tasks and out-of-distribution tasks across both the given-context protocol and the one-shot protocol.","sentences":["Generalization and sample efficiency have been long-standing issues concerning reinforcement learning, and thus the field of Offline Meta-Reinforcement Learning~(OMRL) has gained increasing attention due to its potential of solving a wide range of problems with static and limited offline data.","Existing OMRL methods often assume sufficient training tasks and data coverage to apply contrastive learning to extract task representations.","However, such assumptions are not applicable in several real-world applications and thus undermine the generalization ability of the representations.","In this paper, we consider OMRL with two types of data limitations: limited training tasks and limited behavior diversity and propose a novel algorithm called GENTLE for learning generalizable task representations in the face of data limitations.","GENTLE employs Task Auto-Encoder~(TAE), which is an encoder-decoder architecture to extract the characteristics of the tasks.","Unlike existing methods, TAE is optimized solely by reconstruction of the state transition and reward, which captures the generative structure of the task models and produces generalizable representations when training tasks are limited.","To alleviate the effect of limited behavior diversity, we consistently construct pseudo-transitions to align the data distribution used to train TAE with the data distribution encountered during testing.","Empirically, GENTLE significantly outperforms existing OMRL methods on both in-distribution tasks and out-of-distribution tasks across both the given-context protocol and the one-shot protocol."],"url":"http://arxiv.org/abs/2312.15909v1"}
{"created":"2023-12-26 06:51:09","title":"Align on the Fly: Adapting Chatbot Behavior to Established Norms","abstract":"In this paper, we aim to align large language models with the ever-changing, complex, and diverse human values (e.g., social norms) across time and locations. This presents a challenge to existing alignment techniques, such as supervised fine-tuning, which internalize values within model parameters. To overcome this, we propose an On-the-fly Preference Optimization (OPO) method, which is a real-time alignment that works in a streaming way. It employs an external memory to store established rules for alignment, which can constrain LLMs' behaviors without further training, allowing for convenient updates and customization of human values. We also introduce a scalable evaluation to assess the proposed method more effectively. Experimental results on both human-annotated and auto-generated questions from legal and moral domains indicate the effectiveness of the proposed OPO method. Our code and data are released at https://github.com/GAIR-NLP/OPO.","sentences":["In this paper, we aim to align large language models with the ever-changing, complex, and diverse human values (e.g., social norms) across time and locations.","This presents a challenge to existing alignment techniques, such as supervised fine-tuning, which internalize values within model parameters.","To overcome this, we propose an On-the-fly Preference Optimization (OPO) method, which is a real-time alignment that works in a streaming way.","It employs an external memory to store established rules for alignment, which can constrain LLMs' behaviors without further training, allowing for convenient updates and customization of human values.","We also introduce a scalable evaluation to assess the proposed method more effectively.","Experimental results on both human-annotated and auto-generated questions from legal and moral domains indicate the effectiveness of the proposed OPO method.","Our code and data are released at https://github.com/GAIR-NLP/OPO."],"url":"http://arxiv.org/abs/2312.15907v1"}
{"created":"2023-12-26 06:50:29","title":"Improving Transferability for Cross-domain Trajectory Prediction via Neural Stochastic Differential Equation","abstract":"Multi-agent trajectory prediction is crucial for various practical applications, spurring the construction of many large-scale trajectory datasets, including vehicles and pedestrians. However, discrepancies exist among datasets due to external factors and data acquisition strategies. External factors include geographical differences and driving styles, while data acquisition strategies include data acquisition rate, history/prediction length, and detector/tracker error. Consequently, the proficient performance of models trained on large-scale datasets has limited transferability on other small-size datasets, bounding the utilization of existing large-scale datasets. To address this limitation, we propose a method based on continuous and stochastic representations of Neural Stochastic Differential Equations (NSDE) for alleviating discrepancies due to data acquisition strategy. We utilize the benefits of continuous representation for handling arbitrary time steps and the use of stochastic representation for handling detector/tracker errors. Additionally, we propose a dataset-specific diffusion network and its training framework to handle dataset-specific detection/tracking errors. The effectiveness of our method is validated against state-of-the-art trajectory prediction models on the popular benchmark datasets: nuScenes, Argoverse, Lyft, INTERACTION, and Waymo Open Motion Dataset (WOMD). Improvement in performance gain on various source and target dataset configurations shows the generalized competence of our approach in addressing cross-dataset discrepancies.","sentences":["Multi-agent trajectory prediction is crucial for various practical applications, spurring the construction of many large-scale trajectory datasets, including vehicles and pedestrians.","However, discrepancies exist among datasets due to external factors and data acquisition strategies.","External factors include geographical differences and driving styles, while data acquisition strategies include data acquisition rate, history/prediction length, and detector/tracker error.","Consequently, the proficient performance of models trained on large-scale datasets has limited transferability on other small-size datasets, bounding the utilization of existing large-scale datasets.","To address this limitation, we propose a method based on continuous and stochastic representations of Neural Stochastic Differential Equations (NSDE) for alleviating discrepancies due to data acquisition strategy.","We utilize the benefits of continuous representation for handling arbitrary time steps and the use of stochastic representation for handling detector/tracker errors.","Additionally, we propose a dataset-specific diffusion network and its training framework to handle dataset-specific detection/tracking errors.","The effectiveness of our method is validated against state-of-the-art trajectory prediction models on the popular benchmark datasets: nuScenes, Argoverse, Lyft, INTERACTION, and Waymo Open Motion","Dataset (WOMD).","Improvement in performance gain on various source and target dataset configurations shows the generalized competence of our approach in addressing cross-dataset discrepancies."],"url":"http://arxiv.org/abs/2312.15906v1"}
{"created":"2023-12-26 06:39:21","title":"An Incremental Update Framework for Online Recommenders with Data-Driven Prior","abstract":"Online recommenders have attained growing interest and created great revenue for businesses. Given numerous users and items, incremental update becomes a mainstream paradigm for learning large-scale models in industrial scenarios, where only newly arrived data within a sliding window is fed into the model, meeting the strict requirements of quick response. However, this strategy would be prone to overfitting to newly arrived data. When there exists a significant drift of data distribution, the long-term information would be discarded, which harms the recommendation performance. Conventional methods address this issue through native model-based continual learning methods, without analyzing the data characteristics for online recommenders. To address the aforementioned issue, we propose an incremental update framework for online recommenders with Data-Driven Prior (DDP), which is composed of Feature Prior (FP) and Model Prior (MP). The FP performs the click estimation for each specific value to enhance the stability of the training process. The MP incorporates previous model output into the current update while strictly following the Bayes rules, resulting in a theoretically provable prior for the robust update. In this way, both the FP and MP are well integrated into the unified framework, which is model-agnostic and can accommodate various advanced interaction models. Extensive experiments on two publicly available datasets as well as an industrial dataset demonstrate the superior performance of the proposed framework.","sentences":["Online recommenders have attained growing interest and created great revenue for businesses.","Given numerous users and items, incremental update becomes a mainstream paradigm for learning large-scale models in industrial scenarios, where only newly arrived data within a sliding window is fed into the model, meeting the strict requirements of quick response.","However, this strategy would be prone to overfitting to newly arrived data.","When there exists a significant drift of data distribution, the long-term information would be discarded, which harms the recommendation performance.","Conventional methods address this issue through native model-based continual learning methods, without analyzing the data characteristics for online recommenders.","To address the aforementioned issue, we propose an incremental update framework for online recommenders with Data-Driven Prior (DDP), which is composed of Feature Prior (FP) and Model Prior (MP).","The FP performs the click estimation for each specific value to enhance the stability of the training process.","The MP incorporates previous model output into the current update while strictly following the Bayes rules, resulting in a theoretically provable prior for the robust update.","In this way, both the FP and MP are well integrated into the unified framework, which is model-agnostic and can accommodate various advanced interaction models.","Extensive experiments on two publicly available datasets as well as an industrial dataset demonstrate the superior performance of the proposed framework."],"url":"http://arxiv.org/abs/2312.15903v1"}
{"created":"2023-12-26 06:20:55","title":"Recursive Distillation for Open-Set Distributed Robot Localization","abstract":"A typical assumption in state-of-the-art self-localization models is that an annotated training dataset is available for the target workspace. However, this is not necessarily true when a robot travels around the general open world. This work introduces a novel training scheme for open-world distributed robot systems. In our scheme, a robot (``student\") can ask the other robots it meets at unfamiliar places (``teachers\") for guidance. Specifically, a pseudo-training dataset is reconstructed from the teacher model and then used for continual learning of the student model under domain, class, and vocabulary incremental setup. Unlike typical knowledge transfer schemes, our scheme introduces only minimal assumptions on the teacher model, so that it can handle various types of open-set teachers, including those uncooperative, untrainable (e.g., image retrieval engines), or black-box teachers (i.e., data privacy). In this paper, we investigate a ranking function as an instance of such generic models, using a challenging data-free recursive distillation scenario, where a student once trained can recursively join the next-generation open teacher set.","sentences":["A typical assumption in state-of-the-art self-localization models is that an annotated training dataset is available for the target workspace.","However, this is not necessarily true when a robot travels around the general open world.","This work introduces a novel training scheme for open-world distributed robot systems.","In our scheme, a robot (``student\") can ask the other robots it meets at unfamiliar places (``teachers\") for guidance.","Specifically, a pseudo-training dataset is reconstructed from the teacher model and then used for continual learning of the student model under domain, class, and vocabulary incremental setup.","Unlike typical knowledge transfer schemes, our scheme introduces only minimal assumptions on the teacher model, so that it can handle various types of open-set teachers, including those uncooperative, untrainable (e.g., image retrieval engines), or black-box teachers (i.e., data privacy).","In this paper, we investigate a ranking function as an instance of such generic models, using a challenging data-free recursive distillation scenario, where a student once trained can recursively join the next-generation open teacher set."],"url":"http://arxiv.org/abs/2312.15897v1"}
{"created":"2023-12-26 06:16:12","title":"WWW: What, When, Where to Compute-in-Memory","abstract":"Compute-in-memory (CiM) has emerged as a compelling solution to alleviate high data movement costs in von Neumann machines. CiM can perform massively parallel general matrix multiplication (GEMM) operations in memory, the dominant computation in Machine Learning (ML) inference. However, re-purposing memory for compute poses key questions on 1) What type of CiM to use: Given a multitude of analog and digital CiMs, determining their suitability from systems perspective is needed. 2) When to use CiM: ML inference includes workloads with a variety of memory and compute requirements, making it difficult to identify when CiM is more beneficial than standard processing cores. 3) Where to integrate CiM: Each memory level has different bandwidth and capacity, that affects the data movement and locality benefits of CiM integration.   In this paper, we explore answers to these questions regarding CiM integration for ML inference acceleration. We use Timeloop-Accelergy for early system-level evaluation of CiM prototypes, including both analog and digital primitives. We integrate CiM into different cache memory levels in an Nvidia A100-like baseline architecture and tailor the dataflow for various ML workloads. Our experiments show CiM architectures improve energy efficiency, achieving up to 0.12x lower energy than the established baseline with INT-8 precision, and upto 4x performance gains with weight interleaving and duplication. The proposed work provides insights into what type of CiM to use, and when and where to optimally integrate it in the cache hierarchy for GEMM acceleration.","sentences":["Compute-in-memory (CiM) has emerged as a compelling solution to alleviate high data movement costs in von Neumann machines.","CiM can perform massively parallel general matrix multiplication (GEMM) operations in memory, the dominant computation in Machine Learning (ML) inference.","However, re-purposing memory for compute poses key questions on 1) What type of CiM to use: Given a multitude of analog and digital CiMs, determining their suitability from systems perspective is needed.","2) When to use CiM: ML inference includes workloads with a variety of memory and compute requirements, making it difficult to identify when CiM is more beneficial than standard processing cores.","3) Where to integrate CiM: Each memory level has different bandwidth and capacity, that affects the data movement and locality benefits of CiM integration.   ","In this paper, we explore answers to these questions regarding CiM integration for ML inference acceleration.","We use Timeloop-Accelergy for early system-level evaluation of CiM prototypes, including both analog and digital primitives.","We integrate CiM into different cache memory levels in an Nvidia A100-like baseline architecture and tailor the dataflow for various ML workloads.","Our experiments show CiM architectures improve energy efficiency, achieving up to 0.12x lower energy than the established baseline with INT-8 precision, and upto 4x performance gains with weight interleaving and duplication.","The proposed work provides insights into what type of CiM to use, and when and where to optimally integrate it in the cache hierarchy for GEMM acceleration."],"url":"http://arxiv.org/abs/2312.15896v1"}
{"created":"2023-12-26 05:40:39","title":"ANN vs SNN: A case study for Neural Decoding in Implantable Brain-Machine Interfaces","abstract":"While it is important to make implantable brain-machine interfaces (iBMI) wireless to increase patient comfort and safety, the trend of increased channel count in recent neural probes poses a challenge due to the concomitant increase in the data rate. Extracting information from raw data at the source by using edge computing is a promising solution to this problem, with integrated intention decoders providing the best compression ratio. In this work, we compare different neural networks (NN) for motor decoding in terms of accuracy and implementation cost. We further show that combining traditional signal processing techniques with machine learning ones deliver surprisingly good performance even with simple NNs. Adding a block Bidirectional Bessel filter provided maximum gains of $\\approx 0.05$, $0.04$ and $0.03$ in $R^2$ for ANN\\_3d, SNN\\_3D and ANN models, while the gains were lower ($\\approx 0.02$ or less) for LSTM and SNN\\_streaming models. Increasing training data helped improve the $R^2$ of all models by $0.03-0.04$ indicating they have more capacity for future improvement. In general, LSTM and SNN\\_streaming models occupy the high and low ends of the pareto curves (for accuracy vs. memory/operations) respectively while SNN\\_3D and ANN\\_3D occupy intermediate positions. Our work presents state of the art results for this dataset and paves the way for decoder-integrated-implants of the future.","sentences":["While it is important to make implantable brain-machine interfaces (iBMI) wireless to increase patient comfort and safety, the trend of increased channel count in recent neural probes poses a challenge due to the concomitant increase in the data rate.","Extracting information from raw data at the source by using edge computing is a promising solution to this problem, with integrated intention decoders providing the best compression ratio.","In this work, we compare different neural networks (NN) for motor decoding in terms of accuracy and implementation cost.","We further show that combining traditional signal processing techniques with machine learning ones deliver surprisingly good performance even with simple NNs.","Adding a block Bidirectional Bessel filter provided maximum gains of $\\approx 0.05$, $0.04$ and $0.03$ in $R^2$ for ANN\\_3d, SNN\\_3D and ANN models, while the gains were lower ($\\approx 0.02$ or less) for LSTM and SNN\\_streaming models.","Increasing training data helped improve the $R^2$ of all models by $0.03-0.04$ indicating they have more capacity for future improvement.","In general, LSTM and SNN\\_streaming models occupy the high and low ends of the pareto curves (for accuracy vs. memory/operations) respectively while SNN\\_3D and ANN\\_3D occupy intermediate positions.","Our work presents state of the art results for this dataset and paves the way for decoder-integrated-implants of the future."],"url":"http://arxiv.org/abs/2312.15889v1"}
{"created":"2023-12-26 04:24:01","title":"Attention-aware Social Graph Transformer Networks for Stochastic Trajectory Prediction","abstract":"Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics. The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety. Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality. Especially, it still has limitations in long-time prediction. We propose Attention-aware Social Graph Transformer Networks for multi-modal trajectory prediction. We combine Graph Convolutional Networks and Transformer Networks by generating stable resolution pseudo-images from Spatio-temporal graphs through a designed stacking and interception method. Furthermore, we design the attention-aware module to handle social interaction information in scenarios involving mixed pedestrian-vehicle traffic. Thus, we maintain the advantages of the Graph and Transformer, i.e., the ability to aggregate information over an arbitrary number of neighbors and the ability to perform complex time-dependent data processing. We conduct experiments on datasets involving pedestrian, vehicle, and mixed trajectories, respectively. Our results demonstrate that our model minimizes displacement errors across various metrics and significantly reduces the likelihood of collisions. It is worth noting that our model effectively reduces the final displacement error, illustrating the ability of our model to predict for a long time.","sentences":["Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics.","The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety.","Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality.","Especially, it still has limitations in long-time prediction.","We propose Attention-aware Social Graph Transformer Networks for multi-modal trajectory prediction.","We combine Graph Convolutional Networks and Transformer Networks by generating stable resolution pseudo-images from Spatio-temporal graphs through a designed stacking and interception method.","Furthermore, we design the attention-aware module to handle social interaction information in scenarios involving mixed pedestrian-vehicle traffic.","Thus, we maintain the advantages of the Graph and Transformer, i.e., the ability to aggregate information over an arbitrary number of neighbors and the ability to perform complex time-dependent data processing.","We conduct experiments on datasets involving pedestrian, vehicle, and mixed trajectories, respectively.","Our results demonstrate that our model minimizes displacement errors across various metrics and significantly reduces the likelihood of collisions.","It is worth noting that our model effectively reduces the final displacement error, illustrating the ability of our model to predict for a long time."],"url":"http://arxiv.org/abs/2312.15881v1"}
{"created":"2023-12-26 03:33:48","title":"Medical Report Generation based on Segment-Enhanced Contrastive Representation Learning","abstract":"Automated radiology report generation has the potential to improve radiology reporting and alleviate the workload of radiologists. However, the medical report generation task poses unique challenges due to the limited availability of medical data and the presence of data bias. To maximize the utility of available data and reduce data bias, we propose MSCL (Medical image Segmentation with Contrastive Learning), a framework that utilizes the Segment Anything Model (SAM) to segment organs, abnormalities, bones, etc., and can pay more attention to the meaningful ROIs in the image to get better visual representations. Then we introduce a supervised contrastive loss that assigns more weight to reports that are semantically similar to the target while training. The design of this loss function aims to mitigate the impact of data bias and encourage the model to capture the essential features of a medical image and generate high-quality reports. Experimental results demonstrate the effectiveness of our proposed model, where we achieve state-of-the-art performance on the IU X-Ray public dataset.","sentences":["Automated radiology report generation has the potential to improve radiology reporting and alleviate the workload of radiologists.","However, the medical report generation task poses unique challenges due to the limited availability of medical data and the presence of data bias.","To maximize the utility of available data and reduce data bias, we propose MSCL (Medical image Segmentation with Contrastive Learning), a framework that utilizes the Segment Anything Model (SAM) to segment organs, abnormalities, bones, etc., and can pay more attention to the meaningful ROIs in the image to get better visual representations.","Then we introduce a supervised contrastive loss that assigns more weight to reports that are semantically similar to the target while training.","The design of this loss function aims to mitigate the impact of data bias and encourage the model to capture the essential features of a medical image and generate high-quality reports.","Experimental results demonstrate the effectiveness of our proposed model, where we achieve state-of-the-art performance on the IU X-Ray public dataset."],"url":"http://arxiv.org/abs/2312.15869v1"}
{"created":"2023-12-26 02:57:11","title":"Learning Online Policies for Person Tracking in Multi-View Environments","abstract":"In this paper, we introduce MVSparse, a novel and efficient framework for cooperative multi-person tracking across multiple synchronized cameras. The MVSparse system is comprised of a carefully orchestrated pipeline, combining edge server-based models with distributed lightweight Reinforcement Learning (RL) agents operating on individual cameras. These RL agents intelligently select informative blocks within each frame based on historical camera data and detection outcomes from neighboring cameras, significantly reducing computational load and communication overhead. The edge server aggregates multiple camera views to perform detection tasks and provides feedback to the individual agents. By projecting inputs from various perspectives onto a common ground plane and applying deep detection models, MVSparse optimally leverages temporal and spatial redundancy in multi-view videos. Notably, our contributions include an empirical analysis of multi-camera pedestrian tracking datasets, the development of a multi-camera, multi-person detection pipeline, and the implementation of MVSparse, yielding impressive results on both open datasets and real-world scenarios. Experimentally, MVSparse accelerates overall inference time by 1.88X and 1.60X compared to a baseline approach while only marginally compromising tracking accuracy by 2.27% and 3.17%, respectively, showcasing its promising potential for efficient multi-camera tracking applications.","sentences":["In this paper, we introduce MVSparse, a novel and efficient framework for cooperative multi-person tracking across multiple synchronized cameras.","The MVSparse system is comprised of a carefully orchestrated pipeline, combining edge server-based models with distributed lightweight Reinforcement Learning (RL) agents operating on individual cameras.","These RL agents intelligently select informative blocks within each frame based on historical camera data and detection outcomes from neighboring cameras, significantly reducing computational load and communication overhead.","The edge server aggregates multiple camera views to perform detection tasks and provides feedback to the individual agents.","By projecting inputs from various perspectives onto a common ground plane and applying deep detection models, MVSparse optimally leverages temporal and spatial redundancy in multi-view videos.","Notably, our contributions include an empirical analysis of multi-camera pedestrian tracking datasets, the development of a multi-camera, multi-person detection pipeline, and the implementation of MVSparse, yielding impressive results on both open datasets and real-world scenarios.","Experimentally, MVSparse accelerates overall inference time by 1.88X and 1.60X compared to a baseline approach while only marginally compromising tracking accuracy by 2.27% and 3.17%, respectively, showcasing its promising potential for efficient multi-camera tracking applications."],"url":"http://arxiv.org/abs/2312.15858v1"}
{"created":"2023-12-26 02:50:42","title":"SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance Fields","abstract":"Although significant progress has been made in the field of 2D-based interactive editing, fine-grained 3D-based interactive editing remains relatively unexplored. This limitation can be attributed to two main challenges: the lack of an efficient 3D representation robust to different modifications and the absence of an effective 3D interactive segmentation method. In this paper, we introduce a novel fine-grained interactive 3D segmentation and editing algorithm with radiance fields, which we refer to as SERF. Our method entails creating a neural mesh representation by integrating multi-view algorithms with pre-trained 2D models. Building upon this representation, we introduce a novel surface rendering technique that preserves local information and is robust to deformation. Moreover, this representation forms the basis for achieving accurate and interactive 3D segmentation without requiring 3D supervision. Harnessing this representation facilitates a range of interactive 3D editing operations, encompassing tasks such as interactive geometry editing and texture painting. Extensive experiments and visualization examples of editing on both real and synthetic data demonstrate the superiority of our method on representation quality and editing ability.","sentences":["Although significant progress has been made in the field of 2D-based interactive editing, fine-grained 3D-based interactive editing remains relatively unexplored.","This limitation can be attributed to two main challenges: the lack of an efficient 3D representation robust to different modifications and the absence of an effective 3D interactive segmentation method.","In this paper, we introduce a novel fine-grained interactive 3D segmentation and editing algorithm with radiance fields, which we refer to as SERF.","Our method entails creating a neural mesh representation by integrating multi-view algorithms with pre-trained 2D models.","Building upon this representation, we introduce a novel surface rendering technique that preserves local information and is robust to deformation.","Moreover, this representation forms the basis for achieving accurate and interactive 3D segmentation without requiring 3D supervision.","Harnessing this representation facilitates a range of interactive 3D editing operations, encompassing tasks such as interactive geometry editing and texture painting.","Extensive experiments and visualization examples of editing on both real and synthetic data demonstrate the superiority of our method on representation quality and editing ability."],"url":"http://arxiv.org/abs/2312.15856v1"}
{"created":"2023-12-26 02:12:21","title":"Hypergraph Enhanced Knowledge Tree Prompt Learning for Next-Basket Recommendation","abstract":"Next-basket recommendation (NBR) aims to infer the items in the next basket given the corresponding basket sequence. Existing NBR methods are mainly based on either message passing in a plain graph or transition modelling in a basket sequence. However, these methods only consider point-to-point binary item relations while item dependencies in real world scenarios are often in higher order. Additionally, the importance of the same item to different users varies due to variation of user preferences, and the relations between items usually involve various aspects. As pretrained language models (PLMs) excel in multiple tasks in natural language processing (NLP) and computer vision (CV), many researchers have made great efforts in utilizing PLMs to boost recommendation. However, existing PLM-based recommendation methods degrade when encountering Out-Of-Vocabulary (OOV) items. OOV items are those whose IDs are out of PLM's vocabulary and thus unintelligible to PLM. To settle the above challenges, we propose a novel method HEKP4NBR, which transforms the knowledge graph (KG) into prompts, namely Knowledge Tree Prompt (KTP), to help PLM encode the OOV item IDs in the user's basket sequence. A hypergraph convolutional module is designed to build a hypergraph based on item similarities measured by an MoE model from multiple aspects and then employ convolution on the hypergraph to model correlations among multiple items. Extensive experiments are conducted on HEKP4NBR on two datasets based on real company data and validate its effectiveness against multiple state-of-the-art methods.","sentences":["Next-basket recommendation (NBR) aims to infer the items in the next basket given the corresponding basket sequence.","Existing NBR methods are mainly based on either message passing in a plain graph or transition modelling in a basket sequence.","However, these methods only consider point-to-point binary item relations while item dependencies in real world scenarios are often in higher order.","Additionally, the importance of the same item to different users varies due to variation of user preferences, and the relations between items usually involve various aspects.","As pretrained language models (PLMs) excel in multiple tasks in natural language processing (NLP) and computer vision (CV), many researchers have made great efforts in utilizing PLMs to boost recommendation.","However, existing PLM-based recommendation methods degrade when encountering Out-Of-Vocabulary (OOV) items.","OOV items are those whose IDs are out of PLM's vocabulary and thus unintelligible to PLM.","To settle the above challenges, we propose a novel method HEKP4NBR, which transforms the knowledge graph (KG) into prompts, namely Knowledge Tree Prompt (KTP), to help PLM encode the OOV item IDs in the user's basket sequence.","A hypergraph convolutional module is designed to build a hypergraph based on item similarities measured by an MoE model from multiple aspects and then employ convolution on the hypergraph to model correlations among multiple items.","Extensive experiments are conducted on HEKP4NBR on two datasets based on real company data and validate its effectiveness against multiple state-of-the-art methods."],"url":"http://arxiv.org/abs/2312.15851v1"}
{"created":"2023-12-26 01:59:23","title":"Modality-Collaborative Transformer with Hybrid Feature Reconstruction for Robust Emotion Recognition","abstract":"As a vital aspect of affective computing, Multimodal Emotion Recognition has been an active research area in the multimedia community. Despite recent progress, this field still confronts two major challenges in real-world applications: 1) improving the efficiency of constructing joint representations from unaligned multimodal features, and 2) relieving the performance decline caused by random modality feature missing. In this paper, we propose a unified framework, Modality-Collaborative Transformer with Hybrid Feature Reconstruction (MCT-HFR), to address these issues. The crucial component of MCT is a novel attention-based encoder which concurrently extracts and dynamically balances the intra- and inter-modality relations for all associated modalities. With additional modality-wise parameter sharing, a more compact representation can be encoded with less time and space complexity. To improve the robustness of MCT, we further introduce HFR which consists of two modules: Local Feature Imagination (LFI) and Global Feature Alignment (GFA). During model training, LFI leverages complete features as supervisory signals to recover local missing features, while GFA is designed to reduce the global semantic gap between pairwise complete and incomplete representations. Experimental evaluations on two popular benchmark datasets demonstrate that our proposed method consistently outperforms advanced baselines in both complete and incomplete data scenarios.","sentences":["As a vital aspect of affective computing, Multimodal Emotion Recognition has been an active research area in the multimedia community.","Despite recent progress, this field still confronts two major challenges in real-world applications: 1) improving the efficiency of constructing joint representations from unaligned multimodal features, and 2) relieving the performance decline caused by random modality feature missing.","In this paper, we propose a unified framework, Modality-Collaborative Transformer with Hybrid Feature Reconstruction (MCT-HFR), to address these issues.","The crucial component of MCT is a novel attention-based encoder which concurrently extracts and dynamically balances the intra- and inter-modality relations for all associated modalities.","With additional modality-wise parameter sharing, a more compact representation can be encoded with less time and space complexity.","To improve the robustness of MCT, we further introduce HFR which consists of two modules: Local Feature Imagination (LFI) and Global Feature Alignment (GFA).","During model training, LFI leverages complete features as supervisory signals to recover local missing features, while GFA is designed to reduce the global semantic gap between pairwise complete and incomplete representations.","Experimental evaluations on two popular benchmark datasets demonstrate that our proposed method consistently outperforms advanced baselines in both complete and incomplete data scenarios."],"url":"http://arxiv.org/abs/2312.15848v1"}
{"created":"2023-12-26 01:14:10","title":"Masked Contrastive Reconstruction for Cross-modal Medical Image-Report Retrieval","abstract":"Cross-modal medical image-report retrieval task plays a significant role in clinical diagnosis and various medical generative tasks. Eliminating heterogeneity between different modalities to enhance semantic consistency is the key challenge of this task. The current Vision-Language Pretraining (VLP) models, with cross-modal contrastive learning and masked reconstruction as joint training tasks, can effectively enhance the performance of cross-modal retrieval. This framework typically employs dual-stream inputs, using unmasked data for cross-modal contrastive learning and masked data for reconstruction. However, due to task competition and information interference caused by significant differences between the inputs of the two proxy tasks, the effectiveness of representation learning for intra-modal and cross-modal features is limited. In this paper, we propose an efficient VLP framework named Masked Contrastive and Reconstruction (MCR), which takes masked data as the sole input for both tasks. This enhances task connections, reducing information interference and competition between them, while also substantially decreasing the required GPU memory and training time. Moreover, we introduce a new modality alignment strategy named Mapping before Aggregation (MbA). Unlike previous methods, MbA maps different modalities to a common feature space before conducting local feature aggregation, thereby reducing the loss of fine-grained semantic information necessary for improved modality alignment. Additionally, due to using only masked input, our method significantly reduces the gpu memory and time required for training. Qualitative and quantitative experiments conducted on the MIMIC-CXR dataset validate the effectiveness of our approach, demonstrating state-of-the-art performance in medical cross-modal retrieval tasks.","sentences":["Cross-modal medical image-report retrieval task plays a significant role in clinical diagnosis and various medical generative tasks.","Eliminating heterogeneity between different modalities to enhance semantic consistency is the key challenge of this task.","The current Vision-Language Pretraining (VLP) models, with cross-modal contrastive learning and masked reconstruction as joint training tasks, can effectively enhance the performance of cross-modal retrieval.","This framework typically employs dual-stream inputs, using unmasked data for cross-modal contrastive learning and masked data for reconstruction.","However, due to task competition and information interference caused by significant differences between the inputs of the two proxy tasks, the effectiveness of representation learning for intra-modal and cross-modal features is limited.","In this paper, we propose an efficient VLP framework named Masked Contrastive and Reconstruction (MCR), which takes masked data as the sole input for both tasks.","This enhances task connections, reducing information interference and competition between them, while also substantially decreasing the required GPU memory and training time.","Moreover, we introduce a new modality alignment strategy named Mapping before Aggregation (MbA).","Unlike previous methods, MbA maps different modalities to a common feature space before conducting local feature aggregation, thereby reducing the loss of fine-grained semantic information necessary for improved modality alignment.","Additionally, due to using only masked input, our method significantly reduces the gpu memory and time required for training.","Qualitative and quantitative experiments conducted on the MIMIC-CXR dataset validate the effectiveness of our approach, demonstrating state-of-the-art performance in medical cross-modal retrieval tasks."],"url":"http://arxiv.org/abs/2312.15840v1"}
{"created":"2023-12-25 22:49:03","title":"Comparative Analysis of Radiomic Features and Gene Expression Profiles in Histopathology Data Using Graph Neural Networks","abstract":"This study leverages graph neural networks to integrate MELC data with Radiomic-extracted features for melanoma classification, focusing on cell-wise analysis. It assesses the effectiveness of gene expression profiles and Radiomic features, revealing that Radiomic features, particularly when combined with UMAP for dimensionality reduction, significantly enhance classification performance. Notably, using Radiomics contributes to increased diagnostic accuracy and computational efficiency, as it allows for the extraction of critical data from fewer stains, thereby reducing operational costs. This methodology marks an advancement in computational dermatology for melanoma cell classification, setting the stage for future research and potential developments.","sentences":["This study leverages graph neural networks to integrate MELC data with Radiomic-extracted features for melanoma classification, focusing on cell-wise analysis.","It assesses the effectiveness of gene expression profiles and Radiomic features, revealing that Radiomic features, particularly when combined with UMAP for dimensionality reduction, significantly enhance classification performance.","Notably, using Radiomics contributes to increased diagnostic accuracy and computational efficiency, as it allows for the extraction of critical data from fewer stains, thereby reducing operational costs.","This methodology marks an advancement in computational dermatology for melanoma cell classification, setting the stage for future research and potential developments."],"url":"http://arxiv.org/abs/2312.15825v1"}
{"created":"2023-12-25 22:33:45","title":"Self-Supervised Learning for Few-Shot Bird Sound Classification","abstract":"Self-supervised learning (SSL) in audio holds significant potential across various domains, particularly in situations where abundant, unlabeled data is readily available at no cost. This is particularly pertinent in bioacoustics, where biologists routinely collect extensive sound datasets from the natural environment. In this study, we demonstrate that SSL is capable of acquiring meaningful representations of bird sounds from audio recordings without the need for annotations. Our experiments showcase that these learned representations exhibit the capacity to generalize to new bird species in few-shot learning (FSL) scenarios. Additionally, we show that selecting windows with high bird activation for self-supervised learning, using a pretrained audio neural network, significantly enhances the quality of the learned representations.","sentences":["Self-supervised learning (SSL) in audio holds significant potential across various domains, particularly in situations where abundant, unlabeled data is readily available at no cost.","This is particularly pertinent in bioacoustics, where biologists routinely collect extensive sound datasets from the natural environment.","In this study, we demonstrate that SSL is capable of acquiring meaningful representations of bird sounds from audio recordings without the need for annotations.","Our experiments showcase that these learned representations exhibit the capacity to generalize to new bird species in few-shot learning (FSL) scenarios.","Additionally, we show that selecting windows with high bird activation for self-supervised learning, using a pretrained audio neural network, significantly enhances the quality of the learned representations."],"url":"http://arxiv.org/abs/2312.15824v1"}
{"created":"2023-12-25 22:24:49","title":"Audiobox: Unified Audio Generation with Natural Language Prompts","abstract":"Audio is an essential part of our life, but creating it often requires expertise and is time-consuming. Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data. However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like \"a person speaking\" and would only generate mumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities. We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms. We allow transcript, vocal, and other audio styles to be controlled independently when generating speech. To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles. We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks. Our demo is available at https://audiobox.metademolab.com/","sentences":["Audio is an essential part of our life, but creating it often requires expertise and is time-consuming.","Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data.","However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like \"a person speaking\" and would only generate mumbling human voices.","This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities.","We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms.","We allow transcript, vocal, and other audio styles to be controlled independently when generating speech.","To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio.","Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles.","We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks.","Our demo is available at https://audiobox.metademolab.com/"],"url":"http://arxiv.org/abs/2312.15821v1"}
{"created":"2023-12-25 21:59:15","title":"Viral Marketing in Social Networks with Competing Products","abstract":"Consider a directed network where each node is either red (using the red product), blue (using the blue product), or uncolored (undecided). Then in each round, an uncolored node chooses red (resp. blue) with some probability proportional to the number of its red (resp. blue) out-neighbors. What is the best strategy to maximize the expected final number of red nodes given the budget to select $k$ red seed nodes? After proving that this problem is computationally hard, we provide a polynomial time approximation algorithm with the best possible approximation guarantee, building on the monotonicity and submodularity of the objective function and exploiting the Monte Carlo method. Furthermore, our experiments on various real-world and synthetic networks demonstrate that our proposed algorithm outperforms other algorithms. Additionally, we investigate the convergence time of the aforementioned process both theoretically and experimentally. In particular, we prove several tight bounds on the convergence time in terms of different graph parameters, such as the number of nodes/edges, maximum out-degree and diameter, by developing novel proof techniques.","sentences":["Consider a directed network where each node is either red (using the red product), blue (using the blue product), or uncolored (undecided).","Then in each round, an uncolored node chooses red (resp.","blue) with some probability proportional to the number of its red (resp.","blue) out-neighbors.","What is the best strategy to maximize the expected final number of red nodes given the budget to select $k$ red seed nodes?","After proving that this problem is computationally hard, we provide a polynomial time approximation algorithm with the best possible approximation guarantee, building on the monotonicity and submodularity of the objective function and exploiting the Monte Carlo method.","Furthermore, our experiments on various real-world and synthetic networks demonstrate that our proposed algorithm outperforms other algorithms.","Additionally, we investigate the convergence time of the aforementioned process both theoretically and experimentally.","In particular, we prove several tight bounds on the convergence time in terms of different graph parameters, such as the number of nodes/edges, maximum out-degree and diameter, by developing novel proof techniques."],"url":"http://arxiv.org/abs/2312.15819v1"}
{"created":"2023-12-25 21:55:00","title":"Contrastive Learning-Based Framework for Sim-to-Real Mapping of Lidar Point Clouds in Autonomous Driving Systems","abstract":"Perception sensor models are essential elements of automotive simulation environments; they also serve as powerful tools for creating synthetic datasets to train deep learning-based perception models. Developing realistic perception sensor models poses a significant challenge due to the large gap between simulated sensor data and real-world sensor outputs, known as the sim-to-real gap. To address this problem, learning-based models have emerged as promising solutions in recent years, with unparalleled potential to map low-fidelity simulated sensor data into highly realistic outputs. Motivated by this potential, this paper focuses on sim-to-real mapping of Lidar point clouds, a widely used perception sensor in automated driving systems. We introduce a novel Contrastive-Learning-based Sim-to-Real mapping framework, namely CLS2R, inspired by the recent advancements in image-to-image translation techniques. The proposed CLS2R framework employs a lossless representation of Lidar point clouds, considering all essential Lidar attributes such as depth, reflectance, and raydrop. We extensively evaluate the proposed framework, comparing it with state-of-the-art image-to-image translation methods using a diverse range of metrics to assess realness, faithfulness, and the impact on the performance of a downstream task. Our results show that CLS2R demonstrates superior performance across nearly all metrics. Source code is available at https://github.com/hamedhaghighi/CLS2R.git.","sentences":["Perception sensor models are essential elements of automotive simulation environments; they also serve as powerful tools for creating synthetic datasets to train deep learning-based perception models.","Developing realistic perception sensor models poses a significant challenge due to the large gap between simulated sensor data and real-world sensor outputs, known as the sim-to-real gap.","To address this problem, learning-based models have emerged as promising solutions in recent years, with unparalleled potential to map low-fidelity simulated sensor data into highly realistic outputs.","Motivated by this potential, this paper focuses on sim-to-real mapping of Lidar point clouds, a widely used perception sensor in automated driving systems.","We introduce a novel Contrastive-Learning-based Sim-to-Real mapping framework, namely CLS2R, inspired by the recent advancements in image-to-image translation techniques.","The proposed CLS2R framework employs a lossless representation of Lidar point clouds, considering all essential Lidar attributes such as depth, reflectance, and raydrop.","We extensively evaluate the proposed framework, comparing it with state-of-the-art image-to-image translation methods using a diverse range of metrics to assess realness, faithfulness, and the impact on the performance of a downstream task.","Our results show that CLS2R demonstrates superior performance across nearly all metrics.","Source code is available at https://github.com/hamedhaghighi/CLS2R.git."],"url":"http://arxiv.org/abs/2312.15817v1"}
{"created":"2023-12-25 21:46:06","title":"Compositional Generalization in Spoken Language Understanding","abstract":"State-of-the-art spoken language understanding (SLU) models have shown tremendous success in benchmark SLU datasets, yet they still fail in many practical scenario due to the lack of model compositionality when trained on limited training data. In this paper, we study two types of compositionality: (a) novel slot combination, and (b) length generalization. We first conduct in-depth analysis, and find that state-of-the-art SLU models often learn spurious slot correlations during training, which leads to poor performance in both compositional cases. To mitigate these limitations, we create the first compositional splits of benchmark SLU datasets and we propose the first compositional SLU model, including compositional loss and paired training that tackle each compositional case respectively. On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms (up to $5\\%$ F1 score) state-of-the-art BERT SLU model.","sentences":["State-of-the-art spoken language understanding (SLU) models have shown tremendous success in benchmark SLU datasets, yet they still fail in many practical scenario due to the lack of model compositionality when trained on limited training data.","In this paper, we study two types of compositionality: (a) novel slot combination, and (b) length generalization.","We first conduct in-depth analysis, and find that state-of-the-art SLU models often learn spurious slot correlations during training, which leads to poor performance in both compositional cases.","To mitigate these limitations, we create the first compositional splits of benchmark SLU datasets and we propose the first compositional SLU model, including compositional loss and paired training that tackle each compositional case respectively.","On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms (up to $5\\%$ F1 score) state-of-the-art BERT SLU model."],"url":"http://arxiv.org/abs/2312.15815v1"}
{"created":"2023-12-25 19:30:06","title":"GenCast: Diffusion-based ensemble forecasting for medium-range weather","abstract":"Probabilistic weather forecasting is critical for decision-making in high-impact domains such as flood forecasting, energy system planning or transportation routing, where quantifying the uncertainty of a forecast -- including probabilities of extreme events -- is essential to guide important cost-benefit trade-offs and mitigation measures. Traditional probabilistic approaches rely on producing ensembles from physics-based models, which sample from a joint distribution over spatio-temporally coherent weather trajectories, but are expensive to run. An efficient alternative is to use a machine learning (ML) forecast model to generate the ensemble, however state-of-the-art ML forecast models for medium-range weather are largely trained to produce deterministic forecasts which minimise mean-squared-error. Despite improving skills scores, they lack physical consistency, a limitation that grows at longer lead times and impacts their ability to characterize the joint distribution. We introduce GenCast, a ML-based generative model for ensemble weather forecasting, trained from reanalysis data. It forecasts ensembles of trajectories for 84 weather variables, for up to 15 days at 1 degree resolution globally, taking around a minute per ensemble member on a single Cloud TPU v4 device. We show that GenCast is more skillful than ENS, a top operational ensemble forecast, for more than 96\\% of all 1320 verification targets on CRPS and Ensemble-Mean RMSE, while maintaining good reliability and physically consistent power spectra. Together our results demonstrate that ML-based probabilistic weather forecasting can now outperform traditional ensemble systems at 1 degree, opening new doors to skillful, fast weather forecasts that are useful in key applications.","sentences":["Probabilistic weather forecasting is critical for decision-making in high-impact domains such as flood forecasting, energy system planning or transportation routing, where quantifying the uncertainty of a forecast -- including probabilities of extreme events -- is essential to guide important cost-benefit trade-offs and mitigation measures.","Traditional probabilistic approaches rely on producing ensembles from physics-based models, which sample from a joint distribution over spatio-temporally coherent weather trajectories, but are expensive to run.","An efficient alternative is to use a machine learning (ML) forecast model to generate the ensemble, however state-of-the-art ML forecast models for medium-range weather are largely trained to produce deterministic forecasts which minimise mean-squared-error.","Despite improving skills scores, they lack physical consistency, a limitation that grows at longer lead times and impacts their ability to characterize the joint distribution.","We introduce GenCast, a ML-based generative model for ensemble weather forecasting, trained from reanalysis data.","It forecasts ensembles of trajectories for 84 weather variables, for up to 15 days at 1 degree resolution globally, taking around a minute per ensemble member on a single Cloud TPU v4 device.","We show that GenCast is more skillful than ENS, a top operational ensemble forecast, for more than 96\\% of all 1320 verification targets on CRPS and Ensemble-Mean RMSE, while maintaining good reliability and physically consistent power spectra.","Together our results demonstrate that ML-based probabilistic weather forecasting can now outperform traditional ensemble systems at 1 degree, opening new doors to skillful, fast weather forecasts that are useful in key applications."],"url":"http://arxiv.org/abs/2312.15796v1"}
{"created":"2023-12-25 18:23:03","title":"AHAM: Adapt, Help, Ask, Model -- Harvesting LLMs for literature mining","abstract":"In an era marked by a rapid increase in scientific publications, researchers grapple with the challenge of keeping pace with field-specific advances. We present the `AHAM' methodology and a metric that guides the domain-specific \\textbf{adapt}ation of the BERTopic topic modeling framework to improve scientific text analysis. By utilizing the LLaMa2 generative language model, we generate topic definitions via one-shot learning by crafting prompts with the \\textbf{help} of domain experts to guide the LLM for literature mining by \\textbf{asking} it to model the topic names. For inter-topic similarity evaluation, we leverage metrics from language generation and translation processes to assess lexical and semantic similarity of the generated topics. Our system aims to reduce both the ratio of outlier topics to the total number of topics and the similarity between topic definitions. The methodology has been assessed on a newly gathered corpus of scientific papers on literature-based discovery. Through rigorous evaluation by domain experts, AHAM has been validated as effective in uncovering intriguing and novel insights within broad research areas. We explore the impact of domain adaptation of sentence-transformers for the task of topic \\textbf{model}ing using two datasets, each specialized to specific scientific domains within arXiv and medarxiv. We evaluate the impact of data size, the niche of adaptation, and the importance of domain adaptation. Our results suggest a strong interaction between domain adaptation and topic modeling precision in terms of outliers and topic definitions.","sentences":["In an era marked by a rapid increase in scientific publications, researchers grapple with the challenge of keeping pace with field-specific advances.","We present the `AHAM' methodology and a metric that guides the domain-specific \\textbf{adapt}ation of the BERTopic topic modeling framework to improve scientific text analysis.","By utilizing the LLaMa2 generative language model, we generate topic definitions via one-shot learning by crafting prompts with the \\textbf{help} of domain experts to guide the LLM for literature mining by \\textbf{asking} it to model the topic names.","For inter-topic similarity evaluation, we leverage metrics from language generation and translation processes to assess lexical and semantic similarity of the generated topics.","Our system aims to reduce both the ratio of outlier topics to the total number of topics and the similarity between topic definitions.","The methodology has been assessed on a newly gathered corpus of scientific papers on literature-based discovery.","Through rigorous evaluation by domain experts, AHAM has been validated as effective in uncovering intriguing and novel insights within broad research areas.","We explore the impact of domain adaptation of sentence-transformers for the task of topic \\textbf{model}ing using two datasets, each specialized to specific scientific domains within arXiv and medarxiv.","We evaluate the impact of data size, the niche of adaptation, and the importance of domain adaptation.","Our results suggest a strong interaction between domain adaptation and topic modeling precision in terms of outliers and topic definitions."],"url":"http://arxiv.org/abs/2312.15784v1"}
{"created":"2023-12-25 16:37:39","title":"A Recipe for Scaling up Text-to-Video Generation with Text-free Videos","abstract":"Diffusion-based text-to-video generation has witnessed impressive progress in the past year yet still falls behind text-to-image generation. One of the key reasons is the limited scale of publicly available data (e.g., 10M video-text pairs in WebVid10M vs. 5B image-text pairs in LAION), considering the high cost of video captioning. Instead, it could be far easier to collect unlabeled clips from video platforms like YouTube. Motivated by this, we come up with a novel text-to-video generation framework, termed TF-T2V, which can directly learn with text-free videos. The rationale behind is to separate the process of text decoding from that of temporal modeling. To this end, we employ a content branch and a motion branch, which are jointly optimized with weights shared. Following such a pipeline, we study the effect of doubling the scale of training set (i.e., video-only WebVid10M) with some randomly collected text-free videos and are encouraged to observe the performance improvement (FID from 9.67 to 8.19 and FVD from 484 to 441), demonstrating the scalability of our approach. We also find that our model could enjoy sustainable performance gain (FID from 8.19 to 7.64 and FVD from 441 to 366) after reintroducing some text labels for training. Finally, we validate the effectiveness and generalizability of our ideology on both native text-to-video generation and compositional video synthesis paradigms. Code and models will be publicly available at https://tf-t2v.github.io/.","sentences":["Diffusion-based text-to-video generation has witnessed impressive progress in the past year yet still falls behind text-to-image generation.","One of the key reasons is the limited scale of publicly available data (e.g., 10M video-text pairs in WebVid10M vs. 5B image-text pairs in LAION), considering the high cost of video captioning.","Instead, it could be far easier to collect unlabeled clips from video platforms like YouTube.","Motivated by this, we come up with a novel text-to-video generation framework, termed TF-T2V, which can directly learn with text-free videos.","The rationale behind is to separate the process of text decoding from that of temporal modeling.","To this end, we employ a content branch and a motion branch, which are jointly optimized with weights shared.","Following such a pipeline, we study the effect of doubling the scale of training set (i.e., video-only WebVid10M) with some randomly collected text-free videos and are encouraged to observe the performance improvement (FID from 9.67 to 8.19 and FVD from 484 to 441), demonstrating the scalability of our approach.","We also find that our model could enjoy sustainable performance gain (FID from 8.19 to 7.64 and FVD from 441 to 366) after reintroducing some text labels for training.","Finally, we validate the effectiveness and generalizability of our ideology on both native text-to-video generation and compositional video synthesis paradigms.","Code and models will be publicly available at https://tf-t2v.github.io/."],"url":"http://arxiv.org/abs/2312.15770v1"}
{"created":"2023-12-25 16:32:34","title":"Lp-Norm Constrained One-Class Classifier Combination","abstract":"Classifier fusion is established as an effective methodology for boosting performance in different settings and one-class classification is no exception. In this study, we consider the one-class classifier fusion problem by modelling the sparsity/uniformity of the ensemble. To this end, we formulate a convex objective function to learn the weights in a linear ensemble model and impose a variable Lp-norm constraint on the weight vector. The vector-norm constraint enables the model to adapt to the intrinsic uniformity/sparsity of the ensemble in the space of base learners and acts as a (soft) classifier selection mechanism by shaping the relative magnitudes of fusion weights. Drawing on the Frank-Wolfe algorithm, we then present an effective approach to solve the formulated convex constrained optimisation problem efficiently. We evaluate the proposed one-class classifier combination approach on multiple data sets from diverse application domains and illustrate its merits in comparison to the existing approaches.","sentences":["Classifier fusion is established as an effective methodology for boosting performance in different settings and one-class classification is no exception.","In this study, we consider the one-class classifier fusion problem by modelling the sparsity/uniformity of the ensemble.","To this end, we formulate a convex objective function to learn the weights in a linear ensemble model and impose a variable Lp-norm constraint on the weight vector.","The vector-norm constraint enables the model to adapt to the intrinsic uniformity/sparsity of the ensemble in the space of base learners and acts as a (soft) classifier selection mechanism by shaping the relative magnitudes of fusion weights.","Drawing on the Frank-Wolfe algorithm, we then present an effective approach to solve the formulated convex constrained optimisation problem efficiently.","We evaluate the proposed one-class classifier combination approach on multiple data sets from diverse application domains and illustrate its merits in comparison to the existing approaches."],"url":"http://arxiv.org/abs/2312.15769v1"}
{"created":"2023-12-25 16:20:32","title":"On Robust Wasserstein Barycenter: The Model and Algorithm","abstract":"The Wasserstein barycenter problem is to compute the average of $m$ given probability measures, which has been widely studied in many different areas; however, real-world data sets are often noisy and huge, which impedes its applications in practice. Hence, in this paper, we focus on improving the computational efficiency of two types of robust Wasserstein barycenter problem (RWB): fixed-support RWB (fixed-RWB) and free-support RWB (free-RWB); actually, the former is a subroutine of the latter. Firstly, we improve efficiency through model reducing; we reduce RWB as an augmented Wasserstein barycenter problem, which works for both fixed-RWB and free-RWB. Especially, fixed-RWB can be computed within $\\widetilde{O}(\\frac{mn^2}{\\epsilon_+})$ time by using an off-the-shelf solver, where $\\epsilon_+$ is the pre-specified additive error and $n$ is the size of locations of input measures. Then, for free-RWB, we leverage a quality guaranteed data compression technique, coreset, to accelerate computation by reducing the data set size $m$. It shows that running algorithms on the coreset is enough instead of on the original data set. Next, by combining the model reducing and coreset techniques above, we propose an algorithm for free-RWB by updating the weights and locations alternatively. Finally, our experiments demonstrate the efficiency of our techniques.","sentences":["The Wasserstein barycenter problem is to compute the average of $m$ given probability measures, which has been widely studied in many different areas; however, real-world data sets are often noisy and huge, which impedes its applications in practice.","Hence, in this paper, we focus on improving the computational efficiency of two types of robust Wasserstein barycenter problem (RWB): fixed-support RWB (fixed-RWB) and free-support RWB (free-RWB); actually, the former is a subroutine of the latter.","Firstly, we improve efficiency through model reducing; we reduce RWB as an augmented Wasserstein barycenter problem, which works for both fixed-RWB and free-RWB.","Especially, fixed-RWB can be computed within $\\widetilde{O}(\\frac{mn^2}{\\epsilon_+})$ time by using an off-the-shelf solver, where $\\epsilon_+$ is the pre-specified additive error and $n$ is the size of locations of input measures.","Then, for free-RWB, we leverage a quality guaranteed data compression technique, coreset, to accelerate computation by reducing the data set size $m$.","It shows that running algorithms on the coreset is enough instead of on the original data set.","Next, by combining the model reducing and coreset techniques above, we propose an algorithm for free-RWB by updating the weights and locations alternatively.","Finally, our experiments demonstrate the efficiency of our techniques."],"url":"http://arxiv.org/abs/2312.15762v1"}
{"created":"2023-12-25 15:41:01","title":"Dynamic MIMO Architecture Design for Near-Field Communications","abstract":"A novel dynamic hybrid beamforming architecture is proposed to achieve the spatial multiplexing-power consumption tradeoff for near-field multiple-input multiple-output (MIMO) networks, where each radio frequency (RF) chain is connected to each antenna using a couple of independent phase shifters to reduce the number of required RF chains. Based on this architecture, an optimization problem is formulated that maximizes the sum of achievable rates while minimizing the hardware power consumption. Both continuous and discrete phase shifters are considered. 1) For continuous phase shifters, a weighted minimum mean-square error-based two-stage (WMMSE-TS) algorithm is proposed, where the same performance as the optimal fully-digital beamformer can be achieved by the proposed hybrid beamformer even if the number of RF chains equals the number of data streams. 2) For discrete phase shifters, a penalty-based layered iterative (PLI) algorithm is proposed. The closed-form analog and baseband digital beamformers are derived in each iteration. Simulation results demonstrate that: 1) the proposed dynamic beamforming architecture outperforms the conventional fixed hybrid beamforming architecture in terms of spatial multiplexing-power consumption tradeoff, and 2) the proposed algorithms achieve better performance than the other baseline schemes.","sentences":["A novel dynamic hybrid beamforming architecture is proposed to achieve the spatial multiplexing-power consumption tradeoff for near-field multiple-input multiple-output (MIMO) networks, where each radio frequency (RF) chain is connected to each antenna using a couple of independent phase shifters to reduce the number of required RF chains.","Based on this architecture, an optimization problem is formulated that maximizes the sum of achievable rates while minimizing the hardware power consumption.","Both continuous and discrete phase shifters are considered.","1) For continuous phase shifters, a weighted minimum mean-square error-based two-stage (WMMSE-TS) algorithm is proposed, where the same performance as the optimal fully-digital beamformer can be achieved by the proposed hybrid beamformer even if the number of RF chains equals the number of data streams.","2) For discrete phase shifters, a penalty-based layered iterative (PLI) algorithm is proposed.","The closed-form analog and baseband digital beamformers are derived in each iteration.","Simulation results demonstrate that: 1) the proposed dynamic beamforming architecture outperforms the conventional fixed hybrid beamforming architecture in terms of spatial multiplexing-power consumption tradeoff, and 2) the proposed algorithms achieve better performance than the other baseline schemes."],"url":"http://arxiv.org/abs/2312.15757v1"}
{"created":"2023-12-25 15:24:41","title":"Solving Label Variation in Scientific Information Extraction via Multi-Task Learning","abstract":"Scientific Information Extraction (ScientificIE) is a critical task that involves the identification of scientific entities and their relationships. The complexity of this task is compounded by the necessity for domain-specific knowledge and the limited availability of annotated data. Two of the most popular datasets for ScientificIE are SemEval-2018 Task-7 and SciERC. They have overlapping samples and differ in their annotation schemes, which leads to conflicts. In this study, we first introduced a novel approach based on multi-task learning to address label variations. We then proposed a soft labeling technique that converts inconsistent labels into probabilistic distributions. The experimental results demonstrated that the proposed method can enhance the model robustness to label noise and improve the end-to-end performance in both ScientificIE tasks. The analysis revealed that label variations can be particularly effective in handling ambiguous instances. Furthermore, the richness of the information captured by label variations can potentially reduce data size requirements. The findings highlight the importance of releasing variation labels and promote future research on other tasks in other domains. Overall, this study demonstrates the effectiveness of multi-task learning and the potential of label variations to enhance the performance of ScientificIE.","sentences":["Scientific Information Extraction (ScientificIE) is a critical task that involves the identification of scientific entities and their relationships.","The complexity of this task is compounded by the necessity for domain-specific knowledge and the limited availability of annotated data.","Two of the most popular datasets for ScientificIE are SemEval-2018 Task-7 and SciERC.","They have overlapping samples and differ in their annotation schemes, which leads to conflicts.","In this study, we first introduced a novel approach based on multi-task learning to address label variations.","We then proposed a soft labeling technique that converts inconsistent labels into probabilistic distributions.","The experimental results demonstrated that the proposed method can enhance the model robustness to label noise and improve the end-to-end performance in both ScientificIE tasks.","The analysis revealed that label variations can be particularly effective in handling ambiguous instances.","Furthermore, the richness of the information captured by label variations can potentially reduce data size requirements.","The findings highlight the importance of releasing variation labels and promote future research on other tasks in other domains.","Overall, this study demonstrates the effectiveness of multi-task learning and the potential of label variations to enhance the performance of ScientificIE."],"url":"http://arxiv.org/abs/2312.15751v1"}
{"created":"2023-12-25 14:50:34","title":"An Intelligent Indoor Positioning Algorithm Based on Wi-Fi and Bluetooth Low Energy","abstract":"Indoor positioning plays a pivotal role in a wide range of applications, from smart homes to industrial automation. In this paper, we propose a comprehensive approach for accurate positioning in indoor environments through the integration of existing Wi-Fi and Bluetooth Low Energy (BLE) devices. The proposed algorithm involves acquiring the received signal strength indicator (RSSI) data from these devices and capturing the complex interactions between RSSI and positions. To enhance the accuracy of the collected data, we first use a Kalman filter for denoising RSSI values, then categorize them into distinct classes using the K-nearest neighbor (KNN) algorithm. Incorporating the filtered RSSI data and the class information obtained from KNN, we then introduce a recurrent neural network (RNN) architecture to estimate the positions with a high precision. We further evaluate the accuracy of our proposed algorithm through testbed experiments using ESP32 system on chip with integrated Wi-Fi and BLE. The results show that we can accurately estimate the positions with an average error of 61.29 cm, which demonstrates a 56\\% enhancement compared to the state-of-the-art existing works.","sentences":["Indoor positioning plays a pivotal role in a wide range of applications, from smart homes to industrial automation.","In this paper, we propose a comprehensive approach for accurate positioning in indoor environments through the integration of existing Wi-Fi and Bluetooth Low Energy (BLE) devices.","The proposed algorithm involves acquiring the received signal strength indicator (RSSI) data from these devices and capturing the complex interactions between RSSI and positions.","To enhance the accuracy of the collected data, we first use a Kalman filter for denoising RSSI values, then categorize them into distinct classes using the K-nearest neighbor (KNN) algorithm.","Incorporating the filtered RSSI data and the class information obtained from KNN, we then introduce a recurrent neural network (RNN) architecture to estimate the positions with a high precision.","We further evaluate the accuracy of our proposed algorithm through testbed experiments using ESP32 system on chip with integrated Wi-Fi and BLE.","The results show that we can accurately estimate the positions with an average error of 61.29 cm, which demonstrates a 56\\% enhancement compared to the state-of-the-art existing works."],"url":"http://arxiv.org/abs/2312.15744v1"}
{"created":"2023-12-25 14:40:46","title":"DI-V2X: Learning Domain-Invariant Representation for Vehicle-Infrastructure Collaborative 3D Object Detection","abstract":"Vehicle-to-Everything (V2X) collaborative perception has recently gained significant attention due to its capability to enhance scene understanding by integrating information from various agents, e.g., vehicles, and infrastructure. However, current works often treat the information from each agent equally, ignoring the inherent domain gap caused by the utilization of different LiDAR sensors of each agent, thus leading to suboptimal performance. In this paper, we propose DI-V2X, that aims to learn Domain-Invariant representations through a new distillation framework to mitigate the domain discrepancy in the context of V2X 3D object detection. DI-V2X comprises three essential components: a domain-mixing instance augmentation (DMA) module, a progressive domain-invariant distillation (PDD) module, and a domain-adaptive fusion (DAF) module. Specifically, DMA builds a domain-mixing 3D instance bank for the teacher and student models during training, resulting in aligned data representation. Next, PDD encourages the student models from different domains to gradually learn a domain-invariant feature representation towards the teacher, where the overlapping regions between agents are employed as guidance to facilitate the distillation process. Furthermore, DAF closes the domain gap between the students by incorporating calibration-aware domain-adaptive attention. Extensive experiments on the challenging DAIR-V2X and V2XSet benchmark datasets demonstrate DI-V2X achieves remarkable performance, outperforming all the previous V2X models. Code is available at https://github.com/Serenos/DI-V2X","sentences":["Vehicle-to-Everything (V2X) collaborative perception has recently gained significant attention due to its capability to enhance scene understanding by integrating information from various agents, e.g., vehicles, and infrastructure.","However, current works often treat the information from each agent equally, ignoring the inherent domain gap caused by the utilization of different LiDAR sensors of each agent, thus leading to suboptimal performance.","In this paper, we propose DI-V2X, that aims to learn Domain-Invariant representations through a new distillation framework to mitigate the domain discrepancy in the context of V2X 3D object detection.","DI-V2X comprises three essential components: a domain-mixing instance augmentation (DMA) module, a progressive domain-invariant distillation (PDD) module, and a domain-adaptive fusion (DAF) module.","Specifically, DMA builds a domain-mixing 3D instance bank for the teacher and student models during training, resulting in aligned data representation.","Next, PDD encourages the student models from different domains to gradually learn a domain-invariant feature representation towards the teacher, where the overlapping regions between agents are employed as guidance to facilitate the distillation process.","Furthermore, DAF closes the domain gap between the students by incorporating calibration-aware domain-adaptive attention.","Extensive experiments on the challenging DAIR-V2X and V2XSet benchmark datasets demonstrate DI-V2X achieves remarkable performance, outperforming all the previous V2X models.","Code is available at https://github.com/Serenos/DI-V2X"],"url":"http://arxiv.org/abs/2312.15742v1"}
{"created":"2023-12-25 14:23:46","title":"Enhanced Robot Motion Block of A-star Algorithm for Robotic Path Planning","abstract":"An efficient robot path-planning model is vulnerable to the number of search nodes, path cost, and time complexity. The conventional A-star (A*) algorithm outperforms other grid-based algorithms for its heuristic search. However it shows suboptimal performance for the time, space, and number of search nodes, depending on the robot motion block (RMB). To address this challenge, this study proposes an optimal RMB for the A* path-planning algorithm to enhance the performance, where the robot movement costs are calculated by the proposed adaptive cost function. Also, a selection process is proposed to select the optimal RMB size. In this proposed model, grid-based maps are used, where the robot's next move is determined based on the adaptive cost function by searching among surrounding octet neighborhood grid cells. The cumulative value from the output data arrays is used to determine the optimal motion block size, which is formulated based on parameters. The proposed RMB significantly affects the searching time complexity and number of search nodes of the A* algorithm while maintaining almost the same path cost to find the goal position by avoiding obstacles. For the experiment, a benchmarked online dataset is used and prepared three different dimensional maps. The proposed approach is validated using approximately 7000 different grid maps with various dimensions and obstacle environments. The proposed model with an optimal RMB demonstrated a remarkable improvement of 93.98% in the number of search cells and 98.94% in time complexity compared to the conventional A* algorithm. Path cost for the proposed model remained largely comparable to other state-of-the-art algorithms. Also, the proposed model outperforms other state-of-the-art algorithms.","sentences":["An efficient robot path-planning model is vulnerable to the number of search nodes, path cost, and time complexity.","The conventional A-star (A*) algorithm outperforms other grid-based algorithms for its heuristic search.","However it shows suboptimal performance for the time, space, and number of search nodes, depending on the robot motion block (RMB).","To address this challenge, this study proposes an optimal RMB for the A* path-planning algorithm to enhance the performance, where the robot movement costs are calculated by the proposed adaptive cost function.","Also, a selection process is proposed to select the optimal RMB size.","In this proposed model, grid-based maps are used, where the robot's next move is determined based on the adaptive cost function by searching among surrounding octet neighborhood grid cells.","The cumulative value from the output data arrays is used to determine the optimal motion block size, which is formulated based on parameters.","The proposed RMB significantly affects the searching time complexity and number of search nodes of the A* algorithm while maintaining almost the same path cost to find the goal position by avoiding obstacles.","For the experiment, a benchmarked online dataset is used and prepared three different dimensional maps.","The proposed approach is validated using approximately 7000 different grid maps with various dimensions and obstacle environments.","The proposed model with an optimal RMB demonstrated a remarkable improvement of 93.98% in the number of search cells and 98.94% in time complexity compared to the conventional A* algorithm.","Path cost for the proposed model remained largely comparable to other state-of-the-art algorithms.","Also, the proposed model outperforms other state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2312.15738v1"}
{"created":"2023-12-25 13:54:58","title":"Diversity-Based Recruitment in Crowdsensing By Combinatorial Multi-Armed Bandits","abstract":"This paper explores mobile crowdsensing, which leverages mobile devices and their users for collective sensing tasks under the coordination of a central requester. The primary challenge here is the variability in the sensing capabilities of individual workers, which are initially unknown and must be progressively learned. In each round of task assignment, the requester selects a group of workers to handle specific tasks. This process inherently leads to task overlaps in the same round and repetitions across rounds. We propose a novel model that enhances task diversity over the rounds by dynamically adjusting the weight of tasks in each round based on their frequency of assignment. Additionally, it accommodates the variability in task completion quality caused by overlaps in the same round, which can range from the maximum individual worker's quality to the summation of qualities of all assigned workers in the overlap. A significant constraint in this process is the requester's budget, which demands an efficient strategy for worker recruitment. Our solution is to maximize the overall weighted quality of tasks completed in each round. We employ a combinatorial multi-armed bandit framework with an upper confidence bound approach for this purpose. The paper further presents a regret analysis and simulations using realistic data to demonstrate the efficacy of our model.","sentences":["This paper explores mobile crowdsensing, which leverages mobile devices and their users for collective sensing tasks under the coordination of a central requester.","The primary challenge here is the variability in the sensing capabilities of individual workers, which are initially unknown and must be progressively learned.","In each round of task assignment, the requester selects a group of workers to handle specific tasks.","This process inherently leads to task overlaps in the same round and repetitions across rounds.","We propose a novel model that enhances task diversity over the rounds by dynamically adjusting the weight of tasks in each round based on their frequency of assignment.","Additionally, it accommodates the variability in task completion quality caused by overlaps in the same round, which can range from the maximum individual worker's quality to the summation of qualities of all assigned workers in the overlap.","A significant constraint in this process is the requester's budget, which demands an efficient strategy for worker recruitment.","Our solution is to maximize the overall weighted quality of tasks completed in each round.","We employ a combinatorial multi-armed bandit framework with an upper confidence bound approach for this purpose.","The paper further presents a regret analysis and simulations using realistic data to demonstrate the efficacy of our model."],"url":"http://arxiv.org/abs/2312.15729v1"}
{"created":"2023-12-25 13:24:22","title":"Notes on Information Propagation in Noisy Multichannel Data Models: Insights into Sensor Selection and Fusion in Multimodal Biomedical Applications","abstract":"Multimodality and multichannel monitoring have become increasingly popular and accessible in engineering, Internet of Things, wearable devices, and biomedical applications. In these contexts, given the diverse and complex nature of data modalities, the relevance of sensor fusion and sensor selection is heightened. In this note, we study the problem of channel/modality selection and fusion from an information theoretical perspective, focusing on linear and nonlinear signal mixtures corrupted by additive Gaussian noise. We revisit and extend well-known properties of linear noisy data models in estimation and information theory, providing practical insights that assist in the decision-making process between channel (modality) selection and fusion. Using the notion of multichannel signal-to-noise ratio, we derive conditions under which, selection or fusion of multimodal/multichannel data can be beneficial or redundant. This contributes to a better understanding of how to optimize sensor fusion and selection from a theoretical standpoint, aiming to enhance multimodal/multichannel system design, especially for biomedical multichannel/multimodal applications.","sentences":["Multimodality and multichannel monitoring have become increasingly popular and accessible in engineering, Internet of Things, wearable devices, and biomedical applications.","In these contexts, given the diverse and complex nature of data modalities, the relevance of sensor fusion and sensor selection is heightened.","In this note, we study the problem of channel/modality selection and fusion from an information theoretical perspective, focusing on linear and nonlinear signal mixtures corrupted by additive Gaussian noise.","We revisit and extend well-known properties of linear noisy data models in estimation and information theory, providing practical insights that assist in the decision-making process between channel (modality) selection and fusion.","Using the notion of multichannel signal-to-noise ratio, we derive conditions under which, selection or fusion of multimodal/multichannel data can be beneficial or redundant.","This contributes to a better understanding of how to optimize sensor fusion and selection from a theoretical standpoint, aiming to enhance multimodal/multichannel system design, especially for biomedical multichannel/multimodal applications."],"url":"http://arxiv.org/abs/2312.15725v1"}
{"created":"2023-12-25 13:00:05","title":"Spatial-Temporal Interplay in Human Mobility: A Hierarchical Reinforcement Learning Approach with Hypergraph Representation","abstract":"In the realm of human mobility, the decision-making process for selecting the next-visit location is intricately influenced by a trade-off between spatial and temporal constraints, which are reflective of individual needs and preferences. This trade-off, however, varies across individuals, making the modeling of these spatial-temporal dynamics a formidable challenge. To address the problem, in this work, we introduce the \"Spatial-temporal Induced Hierarchical Reinforcement Learning\" (STI-HRL) framework, for capturing the interplay between spatial and temporal factors in human mobility decision-making. Specifically, STI-HRL employs a two-tiered decision-making process: the low-level focuses on disentangling spatial and temporal preferences using dedicated agents, while the high-level integrates these considerations to finalize the decision. To complement the hierarchical decision setting, we construct a hypergraph to organize historical data, encapsulating the multi-aspect semantics of human mobility. We propose a cross-channel hypergraph embedding module to learn the representations as the states to facilitate the decision-making cycle. Our extensive experiments on two real-world datasets validate the superiority of STI-HRL over state-of-the-art methods in predicting users' next visits across various performance metrics.","sentences":["In the realm of human mobility, the decision-making process for selecting the next-visit location is intricately influenced by a trade-off between spatial and temporal constraints, which are reflective of individual needs and preferences.","This trade-off, however, varies across individuals, making the modeling of these spatial-temporal dynamics a formidable challenge.","To address the problem, in this work, we introduce the \"Spatial-temporal Induced Hierarchical Reinforcement Learning\" (STI-HRL) framework, for capturing the interplay between spatial and temporal factors in human mobility decision-making.","Specifically, STI-HRL employs a two-tiered decision-making process: the low-level focuses on disentangling spatial and temporal preferences using dedicated agents, while the high-level integrates these considerations to finalize the decision.","To complement the hierarchical decision setting, we construct a hypergraph to organize historical data, encapsulating the multi-aspect semantics of human mobility.","We propose a cross-channel hypergraph embedding module to learn the representations as the states to facilitate the decision-making cycle.","Our extensive experiments on two real-world datasets validate the superiority of STI-HRL over state-of-the-art methods in predicting users' next visits across various performance metrics."],"url":"http://arxiv.org/abs/2312.15717v1"}
{"created":"2023-12-25 12:48:55","title":"PersianLLaMA: Towards Building First Persian Large Language Model","abstract":"Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. This paper introduces the first large Persian language model, named PersianLLaMA, trained on a collection of Persian texts and datasets. This foundational model comes in two versions, with 7 and 13 billion parameters, trained on formal and colloquial Persian texts using two different approaches. PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics. The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian text. PersianLLaMA marks an important step in the development of Persian natural language processing and can be a valuable resource for the Persian-speaking community. This large language model can be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine translation, and text summarization","sentences":["Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language.","The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources.","Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian.","This paper introduces the first large Persian language model, named PersianLLaMA, trained on a collection of Persian texts and datasets.","This foundational model comes in two versions, with 7 and 13 billion parameters, trained on formal and colloquial Persian texts using two different approaches.","PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics.","The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian text.","PersianLLaMA marks an important step in the development of Persian natural language processing and can be a valuable resource for the Persian-speaking community.","This large language model can be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine translation, and text summarization"],"url":"http://arxiv.org/abs/2312.15713v1"}
{"created":"2023-12-25 11:54:07","title":"Three Heads Are Better Than One: Complementary Experts for Long-Tailed Semi-supervised Learning","abstract":"We address the challenging problem of Long-Tailed Semi-Supervised Learning (LTSSL) where labeled data exhibit imbalanced class distribution and unlabeled data follow an unknown distribution. Unlike in balanced SSL, the generated pseudo-labels are skewed towards head classes, intensifying the training bias. Such a phenomenon is even amplified as more unlabeled data will be mislabeled as head classes when the class distribution of labeled and unlabeled datasets are mismatched. To solve this problem, we propose a novel method named ComPlementary Experts (CPE). Specifically, we train multiple experts to model various class distributions, each of them yielding high-quality pseudo-labels within one form of class distribution. Besides, we introduce Classwise Batch Normalization for CPE to avoid performance degradation caused by feature distribution mismatch between head and non-head classes. CPE achieves state-of-the-art performances on CIFAR-10-LT, CIFAR-100-LT, and STL-10-LT dataset benchmarks. For instance, on CIFAR-10-LT, CPE improves test accuracy by over >2.22% compared to baselines. Code is available at https://github.com/machengcheng2016/CPE-LTSSL.","sentences":["We address the challenging problem of Long-Tailed Semi-Supervised Learning (LTSSL) where labeled data exhibit imbalanced class distribution and unlabeled data follow an unknown distribution.","Unlike in balanced SSL, the generated pseudo-labels are skewed towards head classes, intensifying the training bias.","Such a phenomenon is even amplified as more unlabeled data will be mislabeled as head classes when the class distribution of labeled and unlabeled datasets are mismatched.","To solve this problem, we propose a novel method named ComPlementary Experts (CPE).","Specifically, we train multiple experts to model various class distributions, each of them yielding high-quality pseudo-labels within one form of class distribution.","Besides, we introduce Classwise Batch Normalization for CPE to avoid performance degradation caused by feature distribution mismatch between head and non-head classes.","CPE achieves state-of-the-art performances on CIFAR-10-LT, CIFAR-100-LT, and STL-10-LT dataset benchmarks.","For instance, on CIFAR-10-LT, CPE improves test accuracy by over >2.22% compared to baselines.","Code is available at https://github.com/machengcheng2016/CPE-LTSSL."],"url":"http://arxiv.org/abs/2312.15702v1"}
{"created":"2023-12-25 11:39:46","title":"RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair","abstract":"Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tuning to converge and contributes to the effectiveness of the repair adapter to fix data-points outside the fine-tuning data distribution. Overall, RepairLLaMA correctly fixes 125 Defects4J v2 and 82 HumanEval-Java bugs, outperforming all baselines.","sentences":["Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs).","Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored.","Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs.","To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA.","This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models.","Our experiments demonstrate the validity of both concepts.","First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals.","Second, parameter-efficient fine-tuning helps fine-tuning to converge and contributes to the effectiveness of the repair adapter to fix data-points outside the fine-tuning data distribution.","Overall, RepairLLaMA correctly fixes 125 Defects4J v2 and 82 HumanEval-Java bugs, outperforming all baselines."],"url":"http://arxiv.org/abs/2312.15698v1"}
{"created":"2023-12-25 11:31:47","title":"EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data","abstract":"Large Language Models (LLMs) pre-trained on massive corpora have exhibited remarkable performance on various NLP tasks. However, applying these models to specific domains still poses significant challenges, such as lack of domain knowledge, limited capacity to leverage domain knowledge and inadequate adaptation to domain-specific data formats. Considering the exorbitant cost of training LLMs from scratch and the scarcity of annotated data within particular domains, in this work, we focus on domain-specific continual pre-training of LLMs using E-commerce domain as an exemplar. Specifically, we explore the impact of continual pre-training on LLMs employing unlabeled general and E-commercial corpora. Furthermore, we design a mixing strategy among different data sources to better leverage E-commercial semi-structured data. We construct multiple tasks to assess LLMs' few-shot In-context Learning ability and their zero-shot performance after instruction tuning in E-commerce domain. Experimental results demonstrate the effectiveness of continual pre-training of E-commerce LLMs and the efficacy of our devised data mixing strategy.","sentences":["Large Language Models (LLMs) pre-trained on massive corpora have exhibited remarkable performance on various NLP tasks.","However, applying these models to specific domains still poses significant challenges, such as lack of domain knowledge, limited capacity to leverage domain knowledge and inadequate adaptation to domain-specific data formats.","Considering the exorbitant cost of training LLMs from scratch and the scarcity of annotated data within particular domains, in this work, we focus on domain-specific continual pre-training of LLMs using E-commerce domain as an exemplar.","Specifically, we explore the impact of continual pre-training on LLMs employing unlabeled general and E-commercial corpora.","Furthermore, we design a mixing strategy among different data sources to better leverage E-commercial semi-structured data.","We construct multiple tasks to assess LLMs' few-shot In-context Learning ability and their zero-shot performance after instruction tuning in E-commerce domain.","Experimental results demonstrate the effectiveness of continual pre-training of E-commerce LLMs and the efficacy of our devised data mixing strategy."],"url":"http://arxiv.org/abs/2312.15696v1"}
{"created":"2023-12-25 10:46:20","title":"Word length-aware text spotting: Enhancing detection and recognition in dense text image","abstract":"Scene text spotting is essential in various computer vision applications, enabling extracting and interpreting textual information from images. However, existing methods often neglect the spatial semantics of word images, leading to suboptimal detection recall rates for long and short words within long-tailed word length distributions that exist prominently in dense scenes. In this paper, we present WordLenSpotter, a novel word length-aware spotter for scene text image detection and recognition, improving the spotting capabilities for long and short words, particularly in the tail data of dense text images. We first design an image encoder equipped with a dilated convolutional fusion module to integrate multiscale text image features effectively. Then, leveraging the Transformer framework, we synergistically optimize text detection and recognition accuracy after iteratively refining text region image features using the word length prior. Specially, we design a Spatial Length Predictor module (SLP) using character count prior tailored to different word lengths to constrain the regions of interest effectively. Furthermore, we introduce a specialized word Length-aware Segmentation (LenSeg) proposal head, enhancing the network's capacity to capture the distinctive features of long and short terms within categories characterized by long-tailed distributions. Comprehensive experiments on public datasets and our dense text spotting dataset DSTD1500 demonstrate the superiority of our proposed methods, particularly in dense text image detection and recognition tasks involving long-tailed word length distributions encompassing a range of long and short words.","sentences":["Scene text spotting is essential in various computer vision applications, enabling extracting and interpreting textual information from images.","However, existing methods often neglect the spatial semantics of word images, leading to suboptimal detection recall rates for long and short words within long-tailed word length distributions that exist prominently in dense scenes.","In this paper, we present WordLenSpotter, a novel word length-aware spotter for scene text image detection and recognition, improving the spotting capabilities for long and short words, particularly in the tail data of dense text images.","We first design an image encoder equipped with a dilated convolutional fusion module to integrate multiscale text image features effectively.","Then, leveraging the Transformer framework, we synergistically optimize text detection and recognition accuracy after iteratively refining text region image features using the word length prior.","Specially, we design a Spatial Length Predictor module (SLP) using character count prior tailored to different word lengths to constrain the regions of interest effectively.","Furthermore, we introduce a specialized word Length-aware Segmentation (LenSeg) proposal head, enhancing the network's capacity to capture the distinctive features of long and short terms within categories characterized by long-tailed distributions.","Comprehensive experiments on public datasets and our dense text spotting dataset DSTD1500 demonstrate the superiority of our proposed methods, particularly in dense text image detection and recognition tasks involving long-tailed word length distributions encompassing a range of long and short words."],"url":"http://arxiv.org/abs/2312.15690v1"}
{"created":"2023-12-25 10:31:22","title":"PULASki: Learning inter-rater variability using statistical distances to improve probabilistic segmentation","abstract":"In the domain of medical imaging, many supervised learning based methods for segmentation face several challenges such as high variability in annotations from multiple experts, paucity of labelled data and class imbalanced datasets. These issues may result in segmentations that lack the requisite precision for clinical analysis and can be misleadingly overconfident without associated uncertainty quantification. We propose the PULASki for biomedical image segmentation that accurately captures variability in expert annotations, even in small datasets. Our approach makes use of an improved loss function based on statistical distances in a conditional variational autoencoder structure (Probabilistic UNet), which improves learning of the conditional decoder compared to the standard cross-entropy particularly in class imbalanced problems. We analyse our method for two structurally different segmentation tasks (intracranial vessel and multiple sclerosis (MS) lesion) and compare our results to four well-established baselines in terms of quantitative metrics and qualitative output. Empirical results demonstrate the PULASKi method outperforms all baselines at the 5\\% significance level. The generated segmentations are shown to be much more anatomically plausible than in the 2D case, particularly for the vessel task. Our method can also be applied to a wide range of multi-label segmentation tasks and and is useful for downstream tasks such as hemodynamic modelling (computational fluid dynamics and data assimilation), clinical decision making, and treatment planning.","sentences":["In the domain of medical imaging, many supervised learning based methods for segmentation face several challenges such as high variability in annotations from multiple experts, paucity of labelled data and class imbalanced datasets.","These issues may result in segmentations that lack the requisite precision for clinical analysis and can be misleadingly overconfident without associated uncertainty quantification.","We propose the PULASki for biomedical image segmentation that accurately captures variability in expert annotations, even in small datasets.","Our approach makes use of an improved loss function based on statistical distances in a conditional variational autoencoder structure (Probabilistic UNet), which improves learning of the conditional decoder compared to the standard cross-entropy particularly in class imbalanced problems.","We analyse our method for two structurally different segmentation tasks (intracranial vessel and multiple sclerosis (MS) lesion) and compare our results to four well-established baselines in terms of quantitative metrics and qualitative output.","Empirical results demonstrate the PULASKi method outperforms all baselines at the 5\\% significance level.","The generated segmentations are shown to be much more anatomically plausible than in the 2D case, particularly for the vessel task.","Our method can also be applied to a wide range of multi-label segmentation tasks and and is useful for downstream tasks such as hemodynamic modelling (computational fluid dynamics and data assimilation), clinical decision making, and treatment planning."],"url":"http://arxiv.org/abs/2312.15686v1"}
{"created":"2023-12-25 10:29:28","title":"What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning","abstract":"Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach. Empirically, deita performs better or on par with the state-of-the-art open-source alignment models with only 6K SFT training data samples -- over 10x less than the data used in the baselines. When further trained with direct preference optimization (DPO), deita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55 MT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools on automatic data selection, facilitating data-efficient alignment. We release our models as well as the selected datasets for future researches to effectively align models more efficiently.","sentences":["Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase.","Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance.","However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively.","In this work, we delve deeply into automatic data selection strategies for alignment.","We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement.","Subsequently, we propose a simple strategy to select data samples based on the measurement.","We present deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach.","Empirically, deita performs better or on par with the state-of-the-art open-source alignment models with only 6K SFT training data samples -- over 10x less than the data used in the baselines.","When further trained with direct preference optimization (DPO), deita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55 MT-Bench and 90.06% AlpacaEval scores.","We anticipate this work to provide tools on automatic data selection, facilitating data-efficient alignment.","We release our models as well as the selected datasets for future researches to effectively align models more efficiently."],"url":"http://arxiv.org/abs/2312.15685v1"}
{"created":"2023-12-25 10:27:08","title":"Stochastic mean-shift clustering","abstract":"In this paper we presented a stochastic version mean-shift clustering algorithm. In the stochastic version the data points \"climb\" to the modes of the distribution collectively, while in the deterministic mean-shift, each datum \"climbs\" individually, while all other data points remains in their original coordinates. Stochastic version of the mean-shift clustering is comparison with a standard (deterministic) mean-shift clustering on synthesized 2- and 3-dimensional data distributed between several Gaussian component. The comparison performed in terms of cluster purity and class data purity. It was found the the stochastic mean-shift clustering outperformed in most of the cases the deterministic mean-shift.","sentences":["In this paper we presented a stochastic version mean-shift clustering algorithm.","In the stochastic version the data points \"climb\" to the modes of the distribution collectively, while in the deterministic mean-shift, each datum \"climbs\" individually, while all other data points remains in their original coordinates.","Stochastic version of the mean-shift clustering is comparison with a standard (deterministic) mean-shift clustering on synthesized 2- and 3-dimensional data distributed between several Gaussian component.","The comparison performed in terms of cluster purity and class data purity.","It was found the the stochastic mean-shift clustering outperformed in most of the cases the deterministic mean-shift."],"url":"http://arxiv.org/abs/2312.15684v1"}
{"created":"2023-12-25 10:07:37","title":"BDIS-SLAM: A lightweight CPU-based dense stereo SLAM for surgery","abstract":"Purpose: Common dense stereo Simultaneous Localization and Mapping (SLAM) approaches in Minimally Invasive Surgery (MIS) require high-end parallel computational resources for real-time implementation. Yet, it is not always feasible since the computational resources should be allocated to other tasks like segmentation, detection, and tracking. To solve the problem of limited parallel computational power, this research aims at a lightweight dense stereo SLAM system that works on a single-core CPU and achieves real-time performance (more than 30 Hz in typical scenarios). Methods: A new dense stereo mapping module is integrated with the ORB-SLAM2 system and named BDIS-SLAM. Our new dense stereo mapping module includes stereo matching and 3D dense depth mosaic methods. Stereo matching is achieved with the recently proposed CPU-level real-time matching algorithm Bayesian Dense Inverse Searching (BDIS). A BDIS-based shape recovery and a depth mosaic strategy are integrated as a new thread and coupled with the backbone ORB-SLAM2 system for real-time stereo shape recovery. Results: Experiments on in-vivo data sets show that BDIS-SLAM runs at over 30 Hz speed on modern single-core CPU in typical endoscopy/colonoscopy scenarios. BDIS-SLAM only consumes around an additional 12% time compared with the backbone ORB-SLAM2. Although our lightweight BDIS-SLAM simplifies the process by ignoring deformation and fusion procedures, it can provide a usable dense mapping for modern MIS on computationally constrained devices. Conclusion: The proposed BDIS-SLAM is a lightweight stereo dense SLAM system for MIS. It achieves 30 Hz on a modern single-core CPU in typical endoscopy/colonoscopy scenarios (image size around 640*480). BDIS-SLAM provides a low-cost solution for dense mapping in MIS and has the potential to be applied in surgical robots and AR systems.","sentences":["Purpose: Common dense stereo Simultaneous Localization and Mapping (SLAM) approaches in Minimally Invasive Surgery (MIS) require high-end parallel computational resources for real-time implementation.","Yet, it is not always feasible since the computational resources should be allocated to other tasks like segmentation, detection, and tracking.","To solve the problem of limited parallel computational power, this research aims at a lightweight dense stereo SLAM system that works on a single-core CPU and achieves real-time performance (more than 30 Hz in typical scenarios).","Methods: A new dense stereo mapping module is integrated with the ORB-SLAM2 system and named BDIS-SLAM.","Our new dense stereo mapping module includes stereo matching and 3D dense depth mosaic methods.","Stereo matching is achieved with the recently proposed CPU-level real-time matching algorithm Bayesian Dense Inverse Searching (BDIS).","A BDIS-based shape recovery and a depth mosaic strategy are integrated as a new thread and coupled with the backbone ORB-SLAM2 system for real-time stereo shape recovery.","Results:","Experiments on in-vivo data sets show that BDIS-SLAM runs at over 30 Hz speed on modern single-core CPU in typical endoscopy/colonoscopy scenarios.","BDIS-SLAM only consumes around an additional 12% time compared with the backbone ORB-SLAM2.","Although our lightweight BDIS-SLAM simplifies the process by ignoring deformation and fusion procedures, it can provide a usable dense mapping for modern MIS on computationally constrained devices.","Conclusion: The proposed BDIS-SLAM is a lightweight stereo dense SLAM system for MIS.","It achieves 30 Hz on a modern single-core CPU in typical endoscopy/colonoscopy scenarios (image size around 640*480).","BDIS-SLAM provides a low-cost solution for dense mapping in MIS and has the potential to be applied in surgical robots and AR systems."],"url":"http://arxiv.org/abs/2312.15679v1"}
{"created":"2023-12-25 08:20:50","title":"A graph-based multimodal framework to predict gentrification","abstract":"Gentrification--the transformation of a low-income urban area caused by the influx of affluent residents--has many revitalizing benefits. However, it also poses extremely concerning challenges to low-income residents. To help policymakers take targeted and early action in protecting low-income residents, researchers have recently proposed several machine learning models to predict gentrification using socioeconomic and image features. Building upon previous studies, we propose a novel graph-based multimodal deep learning framework to predict gentrification based on urban networks of tracts and essential facilities (e.g., schools, hospitals, and subway stations). We train and test the proposed framework using data from Chicago, New York City, and Los Angeles. The model successfully predicts census-tract level gentrification with 0.9 precision on average. Moreover, the framework discovers a previously unexamined strong relationship between schools and gentrification, which provides a basis for further exploration of social factors affecting gentrification.","sentences":["Gentrification--the transformation of a low-income urban area caused by the influx of affluent residents--has many revitalizing benefits.","However, it also poses extremely concerning challenges to low-income residents.","To help policymakers take targeted and early action in protecting low-income residents, researchers have recently proposed several machine learning models to predict gentrification using socioeconomic and image features.","Building upon previous studies, we propose a novel graph-based multimodal deep learning framework to predict gentrification based on urban networks of tracts and essential facilities (e.g., schools, hospitals, and subway stations).","We train and test the proposed framework using data from Chicago, New York City, and Los Angeles.","The model successfully predicts census-tract level gentrification with 0.9 precision on average.","Moreover, the framework discovers a previously unexamined strong relationship between schools and gentrification, which provides a basis for further exploration of social factors affecting gentrification."],"url":"http://arxiv.org/abs/2312.15646v1"}
{"created":"2023-12-25 08:13:28","title":"UVAGaze: Unsupervised 1-to-2 Views Adaptation for Gaze Estimation","abstract":"Gaze estimation has become a subject of growing interest in recent research. Most of the current methods rely on single-view facial images as input. Yet, it is hard for these approaches to handle large head angles, leading to potential inaccuracies in the estimation. To address this issue, adding a second-view camera can help better capture eye appearance. However, existing multi-view methods have two limitations. 1) They require multi-view annotations for training, which are expensive. 2) More importantly, during testing, the exact positions of the multiple cameras must be known and match those used in training, which limits the application scenario. To address these challenges, we propose a novel 1-view-to-2-views (1-to-2 views) adaptation solution in this paper, the Unsupervised 1-to-2 Views Adaptation framework for Gaze estimation (UVAGaze). Our method adapts a traditional single-view gaze estimator for flexibly placed dual cameras. Here, the \"flexibly\" means we place the dual cameras in arbitrary places regardless of the training data, without knowing their extrinsic parameters. Specifically, the UVAGaze builds a dual-view mutual supervision adaptation strategy, which takes advantage of the intrinsic consistency of gaze directions between both views. In this way, our method can not only benefit from common single-view pre-training, but also achieve more advanced dual-view gaze estimation. The experimental results show that a single-view estimator, when adapted for dual views, can achieve much higher accuracy, especially in cross-dataset settings, with a substantial improvement of 47.0%. Project page: https://github.com/MickeyLLG/UVAGaze.","sentences":["Gaze estimation has become a subject of growing interest in recent research.","Most of the current methods rely on single-view facial images as input.","Yet, it is hard for these approaches to handle large head angles, leading to potential inaccuracies in the estimation.","To address this issue, adding a second-view camera can help better capture eye appearance.","However, existing multi-view methods have two limitations.","1) They require multi-view annotations for training, which are expensive.","2) More importantly, during testing, the exact positions of the multiple cameras must be known and match those used in training, which limits the application scenario.","To address these challenges, we propose a novel 1-view-to-2-views (1-to-2 views) adaptation solution in this paper, the Unsupervised 1-to-2 Views Adaptation framework for Gaze estimation (UVAGaze).","Our method adapts a traditional single-view gaze estimator for flexibly placed dual cameras.","Here, the \"flexibly\" means we place the dual cameras in arbitrary places regardless of the training data, without knowing their extrinsic parameters.","Specifically, the UVAGaze builds a dual-view mutual supervision adaptation strategy, which takes advantage of the intrinsic consistency of gaze directions between both views.","In this way, our method can not only benefit from common single-view pre-training, but also achieve more advanced dual-view gaze estimation.","The experimental results show that a single-view estimator, when adapted for dual views, can achieve much higher accuracy, especially in cross-dataset settings, with a substantial improvement of 47.0%.","Project page: https://github.com/MickeyLLG/UVAGaze."],"url":"http://arxiv.org/abs/2312.15644v1"}
{"created":"2023-12-25 07:59:18","title":"Report of the DOE/NSF Workshop on Correctness in Scientific Computing, June 2023, Orlando, FL","abstract":"This report is a digest of the DOE/NSF Workshop on Correctness in Scientific Computing (CSC'23) held on June 17, 2023, as part of the Federated Computing Research Conference (FCRC) 2023. CSC was conceived by DOE and NSF to address the growing concerns about correctness among those who employ computational methods to perform large-scale scientific simulations. These concerns have escalated, given the complexity, scale, and heterogeneity of today's HPC software and hardware. If correctness is not proactively addressed, there is the risk of producing flawed science on top of unacceptable productivity losses faced by computational scientists and engineers. HPC systems are beginning to include data-driven methods, including machine learning and surrogate models, and their impact on overall HPC system correctness was also felt urgent to discuss.   Stakeholders of correctness in this space were identified to belong to several sub-disciplines of computer science; from computer architecture researchers who design special-purpose hardware that offers high energy efficiencies; numerical algorithm designers who develop efficient computational schemes based on reduced precision as well as reduced data movement; all the way to researchers in programming language and formal methods who seek methodologies for correct compilation and verification. To include attendees with such a diverse set of backgrounds, CSC was held during the Federated Computing Research Conference (FCRC) 2023.","sentences":["This report is a digest of the DOE/NSF Workshop on Correctness in Scientific Computing (CSC'23) held on June 17, 2023, as part of the Federated Computing Research Conference (FCRC) 2023.","CSC was conceived by DOE and NSF to address the growing concerns about correctness among those who employ computational methods to perform large-scale scientific simulations.","These concerns have escalated, given the complexity, scale, and heterogeneity of today's HPC software and hardware.","If correctness is not proactively addressed, there is the risk of producing flawed science on top of unacceptable productivity losses faced by computational scientists and engineers.","HPC systems are beginning to include data-driven methods, including machine learning and surrogate models, and their impact on overall HPC system correctness was also felt urgent to discuss.   ","Stakeholders of correctness in this space were identified to belong to several sub-disciplines of computer science; from computer architecture researchers who design special-purpose hardware that offers high energy efficiencies; numerical algorithm designers who develop efficient computational schemes based on reduced precision as well as reduced data movement; all the way to researchers in programming language and formal methods who seek methodologies for correct compilation and verification.","To include attendees with such a diverse set of backgrounds, CSC was held during the Federated Computing Research Conference (FCRC) 2023."],"url":"http://arxiv.org/abs/2312.15640v1"}
{"created":"2023-12-25 07:50:58","title":"Lifting by Image -- Leveraging Image Cues for Accurate 3D Human Pose Estimation","abstract":"The \"lifting from 2D pose\" method has been the dominant approach to 3D Human Pose Estimation (3DHPE) due to the powerful visual analysis ability of 2D pose estimators. Widely known, there exists a depth ambiguity problem when estimating solely from 2D pose, where one 2D pose can be mapped to multiple 3D poses. Intuitively, the rich semantic and texture information in images can contribute to a more accurate \"lifting\" procedure. Yet, existing research encounters two primary challenges. Firstly, the distribution of image data in 3D motion capture datasets is too narrow because of the laboratorial environment, which leads to poor generalization ability of methods trained with image information. Secondly, effective strategies for leveraging image information are lacking. In this paper, we give new insight into the cause of poor generalization problems and the effectiveness of image features. Based on that, we propose an advanced framework. Specifically, the framework consists of two stages. First, we enable the keypoints to query and select the beneficial features from all image patches. To reduce the keypoints attention to inconsequential background features, we design a novel Pose-guided Transformer Layer, which adaptively limits the updates to unimportant image patches. Then, through a designed Adaptive Feature Selection Module, we prune less significant image patches from the feature map. In the second stage, we allow the keypoints to further emphasize the retained critical image features. This progressive learning approach prevents further training on insignificant image features. Experimental results show that our model achieves state-of-the-art performance on both the Human3.6M dataset and the MPI-INF-3DHP dataset.","sentences":["The \"lifting from 2D pose\" method has been the dominant approach to 3D Human Pose Estimation (3DHPE) due to the powerful visual analysis ability of 2D pose estimators.","Widely known, there exists a depth ambiguity problem when estimating solely from 2D pose, where one 2D pose can be mapped to multiple 3D poses.","Intuitively, the rich semantic and texture information in images can contribute to a more accurate \"lifting\" procedure.","Yet, existing research encounters two primary challenges.","Firstly, the distribution of image data in 3D motion capture datasets is too narrow because of the laboratorial environment, which leads to poor generalization ability of methods trained with image information.","Secondly, effective strategies for leveraging image information are lacking.","In this paper, we give new insight into the cause of poor generalization problems and the effectiveness of image features.","Based on that, we propose an advanced framework.","Specifically, the framework consists of two stages.","First, we enable the keypoints to query and select the beneficial features from all image patches.","To reduce the keypoints attention to inconsequential background features, we design a novel Pose-guided Transformer Layer, which adaptively limits the updates to unimportant image patches.","Then, through a designed Adaptive Feature Selection Module, we prune less significant image patches from the feature map.","In the second stage, we allow the keypoints to further emphasize the retained critical image features.","This progressive learning approach prevents further training on insignificant image features.","Experimental results show that our model achieves state-of-the-art performance on both the Human3.6M dataset and the MPI-INF-3DHP dataset."],"url":"http://arxiv.org/abs/2312.15636v1"}
{"created":"2023-12-25 07:44:11","title":"Incorporating Feature Signal Transmission with Block-based Haptic Data Reduction for Time-delayed Teleoperation","abstract":"This paper presents an innovative feature signal transmission approach incorpo-rating block-based haptic data reduction to address time-delayed teleoperation. Numerous data reduction techniques rely on perceptual deadband (DB). In the preceding block-based approaches, the whole block within the DB is discarded. However, disregarding all signals within the DB loses too much information and hinders effective haptic signal tracking, as these signals contain valuable infor-mation for signal reconstruction. Consequently, we propose a feature signal transmission approach based on the block algorithm that aggregates samples as a unit, enabling high-quality haptic data reduction. In our proposed approach, we employ max-pooling to extract feature signals from the signals within the DB. These feature signals are then transmitted by adjusting the content of the trans-mission block. This methodology enables the transmission of more useful infor-mation without introducing additional delay, aside from the inherent algorithmic delay. Experimental results demonstrate the superiority of our approach over oth-er state-of-the-art (SOTA) methods on various assessment measures under dis-tinct channel delays.","sentences":["This paper presents an innovative feature signal transmission approach incorpo-rating block-based haptic data reduction to address time-delayed teleoperation.","Numerous data reduction techniques rely on perceptual deadband (DB).","In the preceding block-based approaches, the whole block within the DB is discarded.","However, disregarding all signals within the DB loses too much information and hinders effective haptic signal tracking, as these signals contain valuable infor-mation for signal reconstruction.","Consequently, we propose a feature signal transmission approach based on the block algorithm that aggregates samples as a unit, enabling high-quality haptic data reduction.","In our proposed approach, we employ max-pooling to extract feature signals from the signals within the DB.","These feature signals are then transmitted by adjusting the content of the trans-mission block.","This methodology enables the transmission of more useful infor-mation without introducing additional delay, aside from the inherent algorithmic delay.","Experimental results demonstrate the superiority of our approach over oth-er state-of-the-art (SOTA) methods on various assessment measures under dis-tinct channel delays."],"url":"http://arxiv.org/abs/2312.15634v1"}
{"created":"2023-12-25 07:03:01","title":"Mesh-LOAM: Real-time Mesh-Based LiDAR Odometry and Mapping","abstract":"Despite having achieved real-time performance in mesh construction, most of the current LiDAR odometry and meshing methods may struggle to deal with complex scenes due to relying on explicit meshing schemes. They are usually sensitive to noise. To overcome these limitations, we propose a real-time mesh-based LiDAR odometry and mapping approach for large-scale scenes via implicit reconstruction and a parallel spatial-hashing scheme. To efficiently reconstruct triangular meshes, we suggest an incremental voxel meshing method that updates every scan by traversing each point once and compresses space via a scalable partition module. By taking advantage of rapid accessing triangular meshes at any time, we design point-to-mesh odometry with location and feature-based data association to estimate the poses between the incoming point clouds and the recovered triangular meshes. The experimental results on four datasets demonstrate the effectiveness of our proposed approach in generating accurate motion trajectories and environmental mesh maps.","sentences":["Despite having achieved real-time performance in mesh construction, most of the current LiDAR odometry and meshing methods may struggle to deal with complex scenes due to relying on explicit meshing schemes.","They are usually sensitive to noise.","To overcome these limitations, we propose a real-time mesh-based LiDAR odometry and mapping approach for large-scale scenes via implicit reconstruction and a parallel spatial-hashing scheme.","To efficiently reconstruct triangular meshes, we suggest an incremental voxel meshing method that updates every scan by traversing each point once and compresses space via a scalable partition module.","By taking advantage of rapid accessing triangular meshes at any time, we design point-to-mesh odometry with location and feature-based data association to estimate the poses between the incoming point clouds and the recovered triangular meshes.","The experimental results on four datasets demonstrate the effectiveness of our proposed approach in generating accurate motion trajectories and environmental mesh maps."],"url":"http://arxiv.org/abs/2312.15630v1"}
{"created":"2023-12-25 06:32:14","title":"RDF-star2Vec: RDF-star Graph Embeddings for Data Mining","abstract":"Knowledge Graphs (KGs) such as Resource Description Framework (RDF) data represent relationships between various entities through the structure of triples (<subject, predicate, object>). Knowledge graph embedding (KGE) is crucial in machine learning applications, specifically in node classification and link prediction tasks. KGE remains a vital research topic within the semantic web community. RDF-star introduces the concept of a quoted triple (QT), a specific form of triple employed either as the subject or object within another triple. Moreover, RDF-star permits a QT to act as compositional entities within another QT, thereby enabling the representation of recursive, hyper-relational KGs with nested structures. However, existing KGE models fail to adequately learn the semantics of QTs and entities, primarily because they do not account for RDF-star graphs containing multi-leveled nested QTs and QT-QT relationships. This study introduces RDF-star2Vec, a novel KGE model specifically designed for RDF-star graphs. RDF-star2Vec introduces graph walk techniques that enable probabilistic transitions between a QT and its compositional entities. Feature vectors for QTs, entities, and relations are derived from generated sequences through the structured skip-gram model. Additionally, we provide a dataset and a benchmarking framework for data mining tasks focused on complex RDF-star graphs. Evaluative experiments demonstrated that RDF-star2Vec yielded superior performance compared to recent extensions of RDF2Vec in various tasks including classification, clustering, entity relatedness, and QT similarity.","sentences":["Knowledge Graphs (KGs) such as Resource Description Framework (RDF) data represent relationships between various entities through the structure of triples (<subject, predicate, object>).","Knowledge graph embedding (KGE) is crucial in machine learning applications, specifically in node classification and link prediction tasks.","KGE remains a vital research topic within the semantic web community.","RDF-star introduces the concept of a quoted triple (QT), a specific form of triple employed either as the subject or object within another triple.","Moreover, RDF-star permits a QT to act as compositional entities within another QT, thereby enabling the representation of recursive, hyper-relational KGs with nested structures.","However, existing KGE models fail to adequately learn the semantics of QTs and entities, primarily because they do not account for RDF-star graphs containing multi-leveled nested QTs and QT-QT relationships.","This study introduces RDF-star2Vec, a novel KGE model specifically designed for RDF-star graphs.","RDF-star2Vec introduces graph walk techniques that enable probabilistic transitions between a QT and its compositional entities.","Feature vectors for QTs, entities, and relations are derived from generated sequences through the structured skip-gram model.","Additionally, we provide a dataset and a benchmarking framework for data mining tasks focused on complex RDF-star graphs.","Evaluative experiments demonstrated that RDF-star2Vec yielded superior performance compared to recent extensions of RDF2Vec in various tasks including classification, clustering, entity relatedness, and QT similarity."],"url":"http://arxiv.org/abs/2312.15626v1"}
{"created":"2023-12-25 05:57:23","title":"Scalable Face Image Coding via StyleGAN Prior: Towards Compression for Human-Machine Collaborative Vision","abstract":"The accelerated proliferation of visual content and the rapid development of machine vision technologies bring significant challenges in delivering visual data on a gigantic scale, which shall be effectively represented to satisfy both human and machine requirements. In this work, we investigate how hierarchical representations derived from the advanced generative prior facilitate constructing an efficient scalable coding paradigm for human-machine collaborative vision. Our key insight is that by exploiting the StyleGAN prior, we can learn three-layered representations encoding hierarchical semantics, which are elaborately designed into the basic, middle, and enhanced layers, supporting machine intelligence and human visual perception in a progressive fashion. With the aim of achieving efficient compression, we propose the layer-wise scalable entropy transformer to reduce the redundancy between layers. Based on the multi-task scalable rate-distortion objective, the proposed scheme is jointly optimized to achieve optimal machine analysis performance, human perception experience, and compression ratio. We validate the proposed paradigm's feasibility in face image compression. Extensive qualitative and quantitative experimental results demonstrate the superiority of the proposed paradigm over the latest compression standard Versatile Video Coding (VVC) in terms of both machine analysis as well as human perception at extremely low bitrates ($<0.01$ bpp), offering new insights for human-machine collaborative compression.","sentences":["The accelerated proliferation of visual content and the rapid development of machine vision technologies bring significant challenges in delivering visual data on a gigantic scale, which shall be effectively represented to satisfy both human and machine requirements.","In this work, we investigate how hierarchical representations derived from the advanced generative prior facilitate constructing an efficient scalable coding paradigm for human-machine collaborative vision.","Our key insight is that by exploiting the StyleGAN prior, we can learn three-layered representations encoding hierarchical semantics, which are elaborately designed into the basic, middle, and enhanced layers, supporting machine intelligence and human visual perception in a progressive fashion.","With the aim of achieving efficient compression, we propose the layer-wise scalable entropy transformer to reduce the redundancy between layers.","Based on the multi-task scalable rate-distortion objective, the proposed scheme is jointly optimized to achieve optimal machine analysis performance, human perception experience, and compression ratio.","We validate the proposed paradigm's feasibility in face image compression.","Extensive qualitative and quantitative experimental results demonstrate the superiority of the proposed paradigm over the latest compression standard Versatile Video Coding (VVC) in terms of both machine analysis as well as human perception at extremely low bitrates ($<0.01$ bpp), offering new insights for human-machine collaborative compression."],"url":"http://arxiv.org/abs/2312.15622v1"}
{"created":"2023-12-25 05:35:57","title":"GanFinger: GAN-Based Fingerprint Generation for Deep Neural Network Ownership Verification","abstract":"Deep neural networks (DNNs) are extensively employed in a wide range of application scenarios. Generally, training a commercially viable neural network requires significant amounts of data and computing resources, and it is easy for unauthorized users to use the networks illegally. Therefore, network ownership verification has become one of the most crucial steps in safeguarding digital assets. To verify the ownership of networks, the existing network fingerprinting approaches perform poorly in the aspects of efficiency, stealthiness, and discriminability. To address these issues, we propose a network fingerprinting approach, named as GanFinger, to construct the network fingerprints based on the network behavior, which is characterized by network outputs of pairs of original examples and conferrable adversarial examples. Specifically, GanFinger leverages Generative Adversarial Networks (GANs) to effectively generate conferrable adversarial examples with imperceptible perturbations. These examples can exhibit identical outputs on copyrighted and pirated networks while producing different results on irrelevant networks. Moreover, to enhance the accuracy of fingerprint ownership verification, the network similarity is computed based on the accuracy-robustness distance of fingerprint examples'outputs. To evaluate the performance of GanFinger, we construct a comprehensive benchmark consisting of 186 networks with five network structures and four popular network post-processing techniques. The benchmark experiments demonstrate that GanFinger significantly outperforms the state-of-the-arts in efficiency, stealthiness, and discriminability. It achieves a remarkable 6.57 times faster in fingerprint generation and boosts the ARUC value by 0.175, resulting in a relative improvement of about 26%.","sentences":["Deep neural networks (DNNs) are extensively employed in a wide range of application scenarios.","Generally, training a commercially viable neural network requires significant amounts of data and computing resources, and it is easy for unauthorized users to use the networks illegally.","Therefore, network ownership verification has become one of the most crucial steps in safeguarding digital assets.","To verify the ownership of networks, the existing network fingerprinting approaches perform poorly in the aspects of efficiency, stealthiness, and discriminability.","To address these issues, we propose a network fingerprinting approach, named as GanFinger, to construct the network fingerprints based on the network behavior, which is characterized by network outputs of pairs of original examples and conferrable adversarial examples.","Specifically, GanFinger leverages Generative Adversarial Networks (GANs) to effectively generate conferrable adversarial examples with imperceptible perturbations.","These examples can exhibit identical outputs on copyrighted and pirated networks while producing different results on irrelevant networks.","Moreover, to enhance the accuracy of fingerprint ownership verification, the network similarity is computed based on the accuracy-robustness distance of fingerprint examples'outputs.","To evaluate the performance of GanFinger, we construct a comprehensive benchmark consisting of 186 networks with five network structures and four popular network post-processing techniques.","The benchmark experiments demonstrate that GanFinger significantly outperforms the state-of-the-arts in efficiency, stealthiness, and discriminability.","It achieves a remarkable 6.57 times faster in fingerprint generation and boosts the ARUC value by 0.175, resulting in a relative improvement of about 26%."],"url":"http://arxiv.org/abs/2312.15617v1"}
{"created":"2023-12-25 05:35:28","title":"Uncertainty as a Predictor: Leveraging Self-Supervised Learning for Zero-Shot MOS Prediction","abstract":"Predicting audio quality in voice synthesis and conversion systems is a critical yet challenging task, especially when traditional methods like Mean Opinion Scores (MOS) are cumbersome to collect at scale. This paper addresses the gap in efficient audio quality prediction, especially in low-resource settings where extensive MOS data from large-scale listening tests may be unavailable. We demonstrate that uncertainty measures derived from out-of-the-box pretrained self-supervised learning (SSL) models, such as wav2vec, correlate with MOS scores. These findings are based on data from the 2022 and 2023 VoiceMOS challenges. We explore the extent of this correlation across different models and language contexts, revealing insights into how inherent uncertainties in SSL models can serve as effective proxies for audio quality assessment. In particular, we show that the contrastive wav2vec models are the most performant in all settings.","sentences":["Predicting audio quality in voice synthesis and conversion systems is a critical yet challenging task, especially when traditional methods like Mean Opinion Scores (MOS) are cumbersome to collect at scale.","This paper addresses the gap in efficient audio quality prediction, especially in low-resource settings where extensive MOS data from large-scale listening tests may be unavailable.","We demonstrate that uncertainty measures derived from out-of-the-box pretrained self-supervised learning (SSL) models, such as wav2vec, correlate with MOS scores.","These findings are based on data from the 2022 and 2023 VoiceMOS challenges.","We explore the extent of this correlation across different models and language contexts, revealing insights into how inherent uncertainties in SSL models can serve as effective proxies for audio quality assessment.","In particular, we show that the contrastive wav2vec models are the most performant in all settings."],"url":"http://arxiv.org/abs/2312.15616v1"}
{"created":"2023-12-25 05:25:39","title":"A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on Software Engineering Tasks","abstract":"Pre-trained models (PTMs) have achieved great success in various Software Engineering (SE) downstream tasks following the ``pre-train then fine-tune'' paradigm. As fully fine-tuning all parameters of PTMs can be computationally expensive, a widely used solution is parameter-efficient fine-tuning (PEFT), which freezes PTMs while introducing extra parameters. Though work has been done to test PEFT methods in the SE field, a comprehensive evaluation is still lacking. This paper aims to fill in this gap by evaluating the effectiveness of five PEFT methods on eight PTMs and four SE downstream tasks. For different tasks and PEFT methods, we seek answers to the following research questions: 1) Is it more effective to use PTMs trained specifically on source code, or is it sufficient to use PTMs trained on natural language text? 2) What is the impact of varying model sizes? 3) How does the model architecture affect the performance? Besides effectiveness, we also discuss the efficiency of PEFT methods, concerning the costs of required training time and GPU resource consumption. We hope that our findings can provide a deeper understanding of PEFT methods on various PTMs and SE downstream tasks. All the codes and data are available at \\url{https://github.com/zwtnju/PEFT.git}.","sentences":["Pre-trained models (PTMs) have achieved great success in various Software Engineering (SE) downstream tasks following the ``pre-train then fine-tune'' paradigm.","As fully fine-tuning all parameters of PTMs can be computationally expensive, a widely used solution is parameter-efficient fine-tuning (PEFT), which freezes PTMs while introducing extra parameters.","Though work has been done to test PEFT methods in the SE field, a comprehensive evaluation is still lacking.","This paper aims to fill in this gap by evaluating the effectiveness of five PEFT methods on eight PTMs and four SE downstream tasks.","For different tasks and PEFT methods, we seek answers to the following research questions: 1) Is it more effective to use PTMs trained specifically on source code, or is it sufficient to use PTMs trained on natural language text?","2) What is the impact of varying model sizes?","3) How does the model architecture affect the performance?","Besides effectiveness, we also discuss the efficiency of PEFT methods, concerning the costs of required training time and GPU resource consumption.","We hope that our findings can provide a deeper understanding of PEFT methods on various PTMs and SE downstream tasks.","All the codes and data are available at \\url{https://github.com/zwtnju/PEFT.git}."],"url":"http://arxiv.org/abs/2312.15614v1"}
{"created":"2023-12-25 04:49:49","title":"APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond","abstract":"Animal Pose Estimation and Tracking (APT) is a critical task in detecting and monitoring the keypoints of animals across a series of video frames, which is essential for understanding animal behavior. Past works relating to animals have primarily focused on either animal tracking or single-frame animal pose estimation only, neglecting the integration of both aspects. The absence of comprehensive APT datasets inhibits the progression and evaluation of animal pose estimation and tracking methods based on videos, thereby constraining their real-world applications. To fill this gap, we introduce APTv2, the pioneering large-scale benchmark for animal pose estimation and tracking. APTv2 comprises 2,749 video clips filtered and collected from 30 distinct animal species. Each video clip includes 15 frames, culminating in a total of 41,235 frames. Following meticulous manual annotation and stringent verification, we provide high-quality keypoint and tracking annotations for a total of 84,611 animal instances, split into easy and hard subsets based on the number of instances that exists in the frame. With APTv2 as the foundation, we establish a simple baseline method named \\posetrackmethodname and provide benchmarks for representative models across three tracks: (1) single-frame animal pose estimation track to evaluate both intra- and inter-domain transfer learning performance, (2) low-data transfer and generalization track to evaluate the inter-species domain generalization performance, and (3) animal pose tracking track. Our experimental results deliver key empirical insights, demonstrating that APTv2 serves as a valuable benchmark for animal pose estimation and tracking. It also presents new challenges and opportunities for future research. The code and dataset are released at \\href{https://github.com/ViTAE-Transformer/APTv2}{https://github.com/ViTAE-Transformer/APTv2}.","sentences":["Animal Pose Estimation and Tracking (APT) is a critical task in detecting and monitoring the keypoints of animals across a series of video frames, which is essential for understanding animal behavior.","Past works relating to animals have primarily focused on either animal tracking or single-frame animal pose estimation only, neglecting the integration of both aspects.","The absence of comprehensive APT datasets inhibits the progression and evaluation of animal pose estimation and tracking methods based on videos, thereby constraining their real-world applications.","To fill this gap, we introduce APTv2, the pioneering large-scale benchmark for animal pose estimation and tracking.","APTv2 comprises 2,749 video clips filtered and collected from 30 distinct animal species.","Each video clip includes 15 frames, culminating in a total of 41,235 frames.","Following meticulous manual annotation and stringent verification, we provide high-quality keypoint and tracking annotations for a total of 84,611 animal instances, split into easy and hard subsets based on the number of instances that exists in the frame.","With APTv2 as the foundation, we establish a simple baseline method named \\posetrackmethodname and provide benchmarks for representative models across three tracks: (1) single-frame animal pose estimation track to evaluate both intra- and inter-domain transfer learning performance, (2) low-data transfer and generalization track to evaluate the inter-species domain generalization performance, and (3) animal pose tracking track.","Our experimental results deliver key empirical insights, demonstrating that APTv2 serves as a valuable benchmark for animal pose estimation and tracking.","It also presents new challenges and opportunities for future research.","The code and dataset are released at \\href{https://github.com/ViTAE-Transformer/APTv2}{https://github.com/ViTAE-Transformer/APTv2}."],"url":"http://arxiv.org/abs/2312.15612v1"}
{"created":"2023-12-25 04:41:52","title":"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks","abstract":"Some extremely low-dimensional yet crucial geometric eigen-lengths often determine the success of some geometric tasks. For example, the height of an object is important to measure to check if it can fit between the shelves of a cabinet, while the width of a couch is crucial when trying to move it through a doorway. Humans have materialized such crucial geometric eigen-lengths in common sense since they are very useful in serving as succinct yet effective, highly interpretable, and universal object representations. However, it remains obscure and underexplored if learning systems can be equipped with similar capabilities of automatically discovering such key geometric quantities from doing tasks. In this work, we therefore for the first time formulate and propose a novel learning problem on this question and set up a benchmark suite including tasks, data, and evaluation metrics for studying the problem. We focus on a family of common fitting tasks as the testbed for the proposed learning problem. We explore potential solutions and demonstrate the feasibility of learning eigen-lengths from simply observing successful and failed fitting trials. We also attempt geometric grounding for more accurate eigen-length measurement and study the reusability of the learned eigen-lengths across multiple tasks. Our work marks the first exploratory step toward learning crucial geometric eigen-lengths and we hope it can inspire future research in tackling this important yet underexplored problem.","sentences":["Some extremely low-dimensional yet crucial geometric eigen-lengths often determine the success of some geometric tasks.","For example, the height of an object is important to measure to check if it can fit between the shelves of a cabinet, while the width of a couch is crucial when trying to move it through a doorway.","Humans have materialized such crucial geometric eigen-lengths in common sense since they are very useful in serving as succinct yet effective, highly interpretable, and universal object representations.","However, it remains obscure and underexplored if learning systems can be equipped with similar capabilities of automatically discovering such key geometric quantities from doing tasks.","In this work, we therefore for the first time formulate and propose a novel learning problem on this question and set up a benchmark suite including tasks, data, and evaluation metrics for studying the problem.","We focus on a family of common fitting tasks as the testbed for the proposed learning problem.","We explore potential solutions and demonstrate the feasibility of learning eigen-lengths from simply observing successful and failed fitting trials.","We also attempt geometric grounding for more accurate eigen-length measurement and study the reusability of the learned eigen-lengths across multiple tasks.","Our work marks the first exploratory step toward learning crucial geometric eigen-lengths and we hope it can inspire future research in tackling this important yet underexplored problem."],"url":"http://arxiv.org/abs/2312.15610v1"}
{"created":"2023-12-25 04:29:05","title":"Federated learning-outcome prediction with multi-layer privacy protection","abstract":"Learning-outcome prediction (LOP) is a long-standing and critical problem in educational routes. Many studies have contributed to developing effective models while often suffering from data shortage and low generalization to various institutions due to the privacy-protection issue. To this end, this study proposes a distributed grade prediction model, dubbed FecMap, by exploiting the federated learning (FL) framework that preserves the private data of local clients and communicates with others through a global generalized model. FecMap considers local subspace learning (LSL), which explicitly learns the local features against the global features, and multi-layer privacy protection (MPP), which hierarchically protects the private features, including model-shareable features and not-allowably shared features, to achieve client-specific classifiers of high performance on LOP per institution. FecMap is then achieved in an iteration manner with all datasets distributed on clients by training a local neural network composed of a global part, a local part, and a classification head in clients and averaging the global parts from clients on the server. To evaluate the FecMap model, we collected three higher-educational datasets of student academic records from engineering majors. Experiment results manifest that FecMap benefits from the proposed LSL and MPP and achieves steady performance on the task of LOP, compared with the state-of-the-art models. This study makes a fresh attempt at the use of federated learning in the learning-analytical task, potentially paving the way to facilitating personalized education with privacy protection.","sentences":["Learning-outcome prediction (LOP) is a long-standing and critical problem in educational routes.","Many studies have contributed to developing effective models while often suffering from data shortage and low generalization to various institutions due to the privacy-protection issue.","To this end, this study proposes a distributed grade prediction model, dubbed FecMap, by exploiting the federated learning (FL) framework that preserves the private data of local clients and communicates with others through a global generalized model.","FecMap considers local subspace learning (LSL), which explicitly learns the local features against the global features, and multi-layer privacy protection (MPP), which hierarchically protects the private features, including model-shareable features and not-allowably shared features, to achieve client-specific classifiers of high performance on LOP per institution.","FecMap is then achieved in an iteration manner with all datasets distributed on clients by training a local neural network composed of a global part, a local part, and a classification head in clients and averaging the global parts from clients on the server.","To evaluate the FecMap model, we collected three higher-educational datasets of student academic records from engineering majors.","Experiment results manifest that FecMap benefits from the proposed LSL and MPP and achieves steady performance on the task of LOP, compared with the state-of-the-art models.","This study makes a fresh attempt at the use of federated learning in the learning-analytical task, potentially paving the way to facilitating personalized education with privacy protection."],"url":"http://arxiv.org/abs/2312.15608v1"}
